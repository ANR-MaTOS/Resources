"id_hal","abstract_en","title","author_names","Article_id","Translation_text","Translation_id","trans_sys_id","Postedit_id","Postedit_text","user_id","start_time","end_time","translation_id","severity","prob_faithfulness","prob_grammar","prob_terminology","prob_style","prob_spelling_punctuation","prob_document","prob_other","free_text","search_time","tech_error"
"3325101","The curation of a knowledge base is a crucial but costly task. In this work, we suggest to make use of the advances in neural network research to improve the automated correction of constraint violations. Our method is a deep learning refinement of ""Learning how to correct a knowledge base from the edit history"", and similarly uses the edits that solved some violations in the past to infer how to solve similar violations in the present. Our system makes use of the graph content, literal embeddings, and features extracted from Web pages to improve its performance. The experimental evaluation on Wikidata shows significant improvements over baselines.","Neural Knowledge Base Repairs","Thomas Pellissier Tanon, Fabian Suchanek","5479","Réparations de la base de connaissances neuronale

La curation d'une base de connaissances est une tâche cruciale mais coûteuse. Dans ce travail, nous proposons d'utiliser les avancées de la recherche sur les réseaux neuronaux pour améliorer la correction automatique des violations de contraintes. Notre méthode est un raffinement par apprentissage profond de ""Apprendre comment corriger une base de connaissances à partir de l'historique des modifications"", et utilise de manière similaire les modifications qui ont résolu certaines violations dans le passé pour déduire comment résoudre des violations similaires dans le présent. Notre système utilise le contenu du graphe, les enchâssements littéraux et les caractéristiques extraites des pages web pour améliorer ses performances. L'évaluation expérimentale sur Wikidata montre des améliorations significatives par rapport aux lignes de base.","19966","3","11","Réparations de la base de connaissances neuronale

La curation d&#039;une base de connaissances est une tâche cruciale, mais coûteuse. Dans cet article, nous proposons d&#039;utiliser les avancées de la recherche sur les réseaux neuronaux pour améliorer la correction automatique des violations de contraintes. Notre méthode consiste en un raffinement par apprentissage profond de &quot;Comment corriger une base de connaissances à partir de l&#039;historique des modifications&quot;, et utilise de manière similaire les modifications qui ont résolu certaines violations antérieures pour déduire comment résoudre des violations actuelles similaires. Notre système utilise des graphes, des plongements lexicaux et des caractéristiques extraites des pages web pour améliorer ses performances. L&#039;évaluation expérimentale sur Wikidata montre des améliorations significatives par rapport aux lignes de base.","2","2023-05-30 20:21:12","2023-05-30 20:48:23","19966",,"1","2","2","2","1","1",NULL,,"2023-05-30 08:20:50","0"
"3616400","This paper introduces the first fine-grained modular tool for automatic summarization. Open source and written in Java, it is designed to be as straightforward as possible for endusers. Its modular architecture is meant to ease its maintenance and the development and integration of new modules. We hope that it will ease the work of researchers in automatic summarization by providing a reliable baseline for future works as well as an easy way to evaluate methods on different corpora.","A Modular Tool for Automatic Summarization","Valentin Nyzam, Aurélien Bossard","797","Outil modulaire pour synthèse automatique

Cet article présente le premier outil modulaire à grain fin pour la synthèse automatique. Open source et écrit en Java, il est conçu pour être aussi simple que possible pour les utilisateurs finaux. Son architecture modulaire est destinée à faciliter sa maintenance et le développement et l'intégration de nouveaux modules. Nous espérons que cela facilitera le travail des chercheurs dans la synthèse automatique en fournissant une base fiable pour les travaux futurs ainsi qu'un moyen facile d'évaluer les méthodes sur différents corps.","4544","6","12","Outil modulaire de synthèse automatique

Cet article présente le premier outil modulaire de synthèse automatique à granulité fine. Open source et écrit en Java, il est conçu pour être aussi simple que possible pour les utilisateurs finaux. Son architecture modulaire est destinée à faciliter sa maintenance ainsi que le développement et l&#039;intégration de nouveaux modules. Nous espérons que cela facilitera le travail des chercheurs dans la synthèse automatique en fournissant une base fiable pour les travaux futurs ainsi qu&#039;un moyen facile d&#039;évaluer les méthodes sur différents corpus.","2","2023-05-31 11:23:19","2023-05-31 11:38:15","4544",,"2","3","2","1","2","1",NULL,,"2023-05-31 11:23:06","0"
"3344434","We propose in this paper a statistical model in the perspective of predicting listener's feedbacks in a conversation. The first contribution of the paper is a study of the prediction of all feedbacks, including those in overlap with the speaker with a good accuracy. Existing model are good at predicting feedbacks during a pause, but reach a very low success level for all feedbacks. We give in this paper a first step towards this complex problem. The second contribution is a model predicting precisely the type of the feedback (generic vs. specific) as well as other specific features (valence expectation) useful in particular for generating feedbacks in dialogue systems. This work relies on an original corpus.","A Multimodal Model for Predicting Conversational Feedbacks","Auriane Boudin, Roxane Bertrand, Stéphane Rauzy, Magalie Ochs, Philippe Blache","5427","Un modèle multimodal pour prédire les réactions conversationnelles

Nous proposons dans cet article un modèle statistique dans la perspective de prédire les réactions de l'auditeur dans une conversation. La première contribution de l'article est une étude de la prédiction de tous les feedbacks, y compris ceux qui se chevauchent avec le locuteur, avec une bonne précision. Les modèles existants sont efficaces pour prédire les feedbacks pendant une pause, mais atteignent un niveau de réussite très bas pour tous les feedbacks. Nous présentons dans cet article un premier pas vers ce problème complexe. La deuxième contribution est un modèle prédisant précisément le type de feedback (générique vs. spécifique) ainsi que d'autres caractéristiques spécifiques (attente de valence) utiles en particulier pour générer des feedbacks dans les systèmes de dialogue. Ce travail s'appuie sur un corpus original.","19914","3","24","Un modèle multimodal de prédiction des retours conversationnels

Nous proposons dans cet article un modèle statistique visant à prédire les retours de l&#039;auditeur lors d&#039;une conversation. La première contribution de l&#039;article est une étude de la prédiction de tous les retours, y compris ceux se chevauchant avec le locuteur, avec une bonne précision. Les modèles existants sont efficaces pour prédire les retours pendant une pause, mais atteignent un niveau de réussite très bas pour les retours dans leur ensemble. Cet article présente une première approche vers la résolution de ce problème complexe. La deuxième contribution est un modèle prédisant précisément le type de retour (générique ou spécifique) ainsi que d&#039;autres caractéristiques spécifiques (attente de valence) utiles en particulier pour générer des retours dans les systèmes de dialogue. Ce travail s&#039;appuie sur un corpus original.","2","2023-05-31 11:42:33","2023-05-31 11:57:09","19914",,"2","1","3","1","1","2",NULL,,"2023-05-31 11:41:34","0"
"3551922","False data injection is an attack in which an attacker injects fabricated data into a system with the objective to change the behaviour and the decision-making of the system. Many industrial data-based devices are vulnerable to such attacks, this work presents an approach for testing False Data Injection Attack. This approach uses a Domain-Specific Language to generate altered data with two objectives, to provide sophisticated attacks scenarios to increase the resilience of vulnerable systems against False Data Injection Attack and to train detection tools.","An Approach for Testing False Data Injection Attack on Data Dependent Industrial Devices","Mathieu Briland, Fabrice Bouquet","4938","Une approche pour tester les attaques par injection de fausses données sur les appareils industriels dépendant des données

L'injection de fausses données est une attaque dans laquelle un attaquant injecte des données fabriquées dans un système dans le but de modifier le comportement et la prise de décision du système. De nombreux dispositifs industriels basés sur des données sont vulnérables à de telles attaques. Ce travail présente une approche pour tester l'attaque par injection de fausses données. Cette approche utilise un langage spécifique au domaine pour générer des données altérées avec deux objectifs : fournir des scénarios d'attaques sophistiqués pour augmenter la résilience des systèmes vulnérables contre l'attaque par injection de fausses données et former les outils de détection.","17590","3","25","Approche de détection des attaques par injection de fausses données sur les dispositifs industriels s&#039;appuyant sur des données

L&#039;injection de fausses données est une attaque consistant à injecter des données fabriquées dans un système, dans le but de modifier le comportement et la prise de décision de celui-ci. De nombreux dispositifs industriels s&#039;appuyant sur des données sont vulnérables à de telles attaques. Cet article présente une approche pour détecter les attaques par injection de fausses données. Cette approche utilise un langage spécifique au domaine pour générer des données altérées avec deux objectifs : fournir des scénarios d&#039;attaques sophistiqués pour augmenter la résilience des systèmes vulnérables face aux attaques par injection de fausses données, et entraîner les outils de détection.","2","2023-05-31 14:12:27","2023-05-31 14:26:19","17590",,"1","2","2","3","2","1",NULL,,"2023-05-31 02:12:03","0"
"3769290","In this paper, our goal is to compare different recent time-frequency (TF) approaches to retrieve the modes of multicomponent signals (MCSs). While it is acknowledged that the synchrosqueezing transform (SST) improves the readability of the time-frequency representation (TFR) of the modes of MCSs, and that SST-based demodulation (DSST) is more efficient than SST itself for mode retrieval (MR), it is unclear whether DSST outperforms downsampled short-time Fourier transform (STFT) in that matter. The goal of the present paper is to answer this question and to propose a variant of DSST that reduces mode-mixing. The focus is put on the sensitivity of the different techniques to frequency modulation for the modes and frequency resolution.","On the use of short-time Fourier transform and synchrosqueezing-based demodulation for the retrieval of the modes of multicomponent signals","Sylvain Meignen, Duong-Hung Pham, Marcelo Colominas","598","Utilisation d'une transformée de Fourier de courte durée et démodulation basée sur la synchronisation pour la récupération des modes de signaux multicomposants

Dans cet article, notre objectif est de comparer différentes approches temporelles-fréquentielles (TF) récentes pour récupérer les modes de signaux multicomposants (MCS). Bien qu'il soit reconnu que la transformée de synchronisation (SST) améliore la lisibilité de la représentation temps-fréquence (TFR) des modes des MCS, et que la démodulation basée sur SST (DSST) est plus efficace que SST elle-même pour la récupération de mode (MR), il n'est pas clair si DSST surpasse la transformée de Fourier à court terme sous-échantillonnée (STFT) en la matière. Le but du présent document est de répondre à cette question et de proposer une variante de DSST qui réduit le mélange de modes. L'accent est mis sur la sensibilité des différentes techniques à la modulation de fréquence pour les modes et la résolution en fréquence.","4345","6","26","Utilisation d&#039;une transformée de Fourier de courte durée et d&#039;une démodulation basée sur le synchrosqueezing pour la récupération des modes de signaux multicomposants

Dans cet article, notre objectif est de comparer différentes approches temps-fréquence (TF) récentes pour récupérer les modes de signaux multicomposants (MCS). Bien qu&#039;il soit reconnu que la transformée de synchrosqueezing (SST) améliore la lisibilité de la représentation temps-fréquence (TFR) des modes des MCS, et que la démodulation basée sur SST (DSST) est plus efficace que SST elle-même pour la récupération de mode (MR), il n&#039;est pas clair si la DSST surpasse la transformée de Fourier à court terme sous-échantillonnée (STFT) en la matière. Le but du présent document est de répondre à cette question et de proposer une variante de la DSST qui réduit le mode-mixing. L&#039;accent est mis sur la sensibilité des différentes techniques à la modulation de fréquence pour les modes et la résolution en fréquence.","2","2023-05-31 14:51:13","2023-05-31 15:07:20","4345",,"2","2","4","2","2","3",NULL,,"2023-05-31 02:26:19","0"
"3179162","Most evolutionary algorithms concerned with a memory of evolution aim at memorizing and reusing the recipes of past successes (e.g. fruitful operators or fruitful mutation directions). The scheme proposed here follows the opp opposite track, and memorizes the past failures of evolution (unfit offspring) through a virtual individual termed the virtual loser. The underlying metaphor is that offspring should attempt to get further away from the virtual loser than their parents. This is done by a new evolution operator, termed flee-mutation, as an alternative to standard recombination and mutation. Special attention is paid to adjusting the flee-mutation step size. Experiments on large sized problems validate this approach and unexpectedly show that a constant flee-mutation step size over a population is desirable.","Toward civilized evolution: developping inhibitions","Michèle Sebag, Marc Schoenauer, Caroline Ravisé","1171","Vers une évolution civilisée : développer des inhibitions

La plupart des algorithmes évolutionnaires concernés par une mémoire de l'évolution visent à mémoriser et réutiliser les recettes des succès passés (par exemple opérateurs féconds ou directions de mutation fécondes). Le schéma proposé ici suit la piste opp opposée, et mémorise les échecs passés de l'évolution (descendance inapte) à travers un individu virtuel appelé le perdant virtuel. La métaphore sous-jacente est que la progéniture devrait essayer de s'éloigner plus du perdant virtuel que ses parents. Ceci est fait par un nouvel opérateur d'évolution, appelé fugue-mutation, comme alternative à la recombinaison et mutation standard. Une attention particulière est accordée à l'ajustement de la taille du pas de mutation de la puce. Des expériences sur des problèmes de grande taille valident cette approche et montrent de façon inattendue qu'une taille constante de mutation de fuite sur une population est souhaitable.","5416","6","27","Vers une évolution civilisée : développer des inhibitions

La plupart des algorithmes évolutionnaires utilisant une mémoire de l&#039;évolution visent à mémoriser et réutiliser les recettes des réussites précédentes (des opérateurs ou directions de mutation adaptés, par exemple). Le schéma proposé ici suit une approche contraire, et mémorise les échecs passés de l&#039;évolution (descendance inapte) à travers un individu virtuel appelé le perdant virtuel. La métaphore sous-jacente est que le descendant devrait être plus éloigné du perdant virtuel que ses parents. Cela se fait par un nouvel opérateur d&#039;évolution, appelé mutation de fuite, comme alternative à la recombinaison et mutation standard. Une attention particulière est accordée à l&#039;ajustement de la taille des étapes de mutation de fuite. Des expériences sur des problèmes à grande échelle valident cette approche et montrent de façon inattendue qu&#039;une taille constante des étapes de mutation de fuite sur une population est souhaitable.","2","2023-05-31 16:30:50","2023-05-31 16:55:14","5416",,"2","2","4","3","1","4",NULL,,"2023-05-31 04:30:40","0"
"3448724","Self-Supervised Learning (SSL) using huge unlabeled data has been successfully explored for image and natural language processing. Recent works also investigated SSL from speech. They were notably successful to improve performance on downstream tasks such as automatic speech recognition (ASR). While these works suggest it is possible to reduce dependence on labeled data for building efficient speech systems, their evaluation was mostly made on ASR and using multiple and heterogeneous experimental settings (most of them for English). This questions the objective comparison of SSL approaches and the evaluation of their impact on building speech systems. In this paper, we propose LeBenchmark: a reproducible framework for assessing SSL from speech. It not only includes ASR (high and low resource) tasks but also spoken language understanding, speech translation and emotion recognition. We also focus on speech technologies in a language different than English: French. SSL models of different sizes are trained from carefully sourced and documented datasets. Experiments show that SSL is beneficial for most but not all tasks which confirms the need for exhaustive and reliable benchmarks to evaluate its real impact. LeBenchmark is shared with the scientific community for reproducible research in SSL from speech.","LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech","Solène Evain, Ha Nguyen, Hang Le, Marcely Zanon Boito, Salima Mdhaffar, Sina Alisamir, Ziyi Tong, Natalia Tomashenko, Marco Dinarelli, Titouan Parcollet, Alexandre Allauzen, Yannick Estève, Benjamin Lecouteux, François Portet, Solange Rossato, Fabien Ringeval, Didier Schwab, Laurent Besacier","5183","LeBenchmark: Un cadre reproductible pour évaluer l’autosurveillance de la représentation Apprentissage de la parole

L’apprentissage autonome (SSL) utilisant d’énormes données non étiquetées a été exploré avec succès pour le traitement de l’image et du langage naturel. Des travaux récents ont également étudié SSL à partir de la parole. Ils ont notamment réussi à améliorer les performances sur les tâches en aval telles que la reconnaissance automatique de la parole (ASR). Bien que ces travaux suggèrent qu’il est possible de réduire la dépendance à l’égard des données étiquetées pour construire des systèmes de parole efficaces, leur évaluation a été principalement effectuée sur ASR et en utilisant des paramètres expérimentaux multiples et hétérogènes (la plupart d’entre eux pour l’anglais). Cela remet en question la comparaison objective des approches SSL et l’évaluation de leur impact sur la construction de systèmes de parole. Dans cet article, nous proposons LeBenchmark: un cadre reproductible pour évaluer SSL à partir de la parole. Il comprend non seulement les tâches ASR (haute et faible ressource), mais aussi la compréhension du langage parlé, la traduction de la parole et la reconnaissance des émotions. Nous nous concentrons également sur les technologies de la parole dans une langue différente de l’anglais: Français. Les modèles SSL de différentes tailles sont formés à partir d’ensembles de données soigneusement obtenus et documentés. Les expériences montrent que SSL est bénéfique pour la plupart des tâches, mais pas toutes, ce qui confirme la nécessité de benchmarks exhaustifs et fiables pour évaluer son impact réel. LeBenchmark est partagé avec la communauté scientifique pour la recherche reproductible en SSL à partir de la parole.","11970","4","69","LeBenchmark, un cadre reproductible pour l&#039;évaluation de l&#039;apprentissage de représentations autosupervisé par la parole
L&#039;exploration de l&#039;apprentissage auto-supervisé (SSL) utilisant une large quantité de données non étiquetées fut un succès pour le traitement des images et du langage naturel. Des travaux récents ont également étudié le SSL par la parole. Ils ont notamment réussi à améliorer les performances sur les tâches en aval telles que la reconnaissance automatique de la parole (RAP). Bien que ces travaux suggèrent qu&#039;il est possible de réduire la dépendance aux données étiquetées pour construire des systèmes vocaux efficaces, leur évaluation a été principalement faite sur la RAP et en utilisant des paramètres expérimentaux multiples et hétérogènes (la plupart d&#039;entre eux pour l&#039;anglais). Cela remet en question la comparaison objective des approches de SSL et l&#039;évaluation de leur impact sur la construction de systèmes vocaux. Dans ce document, nous proposons LeBenchmark : un cadre reproductible pour évaluer le SSL à partir de la parole. Il comprend non seulement les tâches de RAP (à données importantes et réduites), mais aussi la compréhension du langage parlé, la traduction vocale et la reconnaissance des émotions. Nous nous concentrons également sur les technologies vocales dans une langue différente de l&#039;anglais : le français. Les modèles de SSL de différentes tailles sont formés à partir d&#039;ensembles de données soigneusement sélectionnés et documentés. Des expériences montrent que le SSL est bénéfique pour la plupart des tâches, mais pas pour toutes, ce qui confirme la nécessité d&#039;avoir des références exhaustives et fiables pour évaluer son impact réel. LeBenchmark est partagé avec la communauté scientifique pour la recherche reproductible en SSL à partir de la parole.
","2","2023-06-06 15:15:27","2023-06-06 15:16:32","11970",,"1","4","3","1","4","1",NULL,,"2023-06-06 03:07:02","0"
"1789959","In this paper, we propose a new multi layer approach for automatic text summarization by extraction where the first layer constitute to use two techniques of extraction: scoring of phrases, and similarity that aims to eliminate redundant phrases without losing the theme of the text. While the second layer aims to optimize the results of the previous layer by the metaheuristic based on social spiders. the objective function of the optimization is to maximize the sum of similarity between phrases of the candidate summary in order to keep the theme of the text, minimize the sum of scores in order to increase the summarization rate, this optimization also will give a candidate’s summary where the order of the phrases changes compared to the original text.The third and final layer aims to choose the best summary from the candidate summaries generated by layer optimization, we opted for the technique of voting with a simple majority.","A New Multi-layered Approach for Automatic Text Summaries Mono-Document Based on Social Spiders","Mohamed Boudia, Reda Hamou, Abdelmalek Amine, Mohamed Rahmani, Amine Rahmani","3395","Une nouvelle approche multi-couches pour les résumés de texte automatiques mono-document basé sur les araignées sociales

Dans cet article, nous proposons une nouvelle approche multicouche pour la synthèse automatique du texte par extraction où la première couche constitue l’utilisation de deux techniques d’extraction: notation des phrases, et similitude qui vise à éliminer les phrases redondantes sans perdre le thème du texte. Alors que la deuxième couche vise à optimiser les résultats de la couche précédente par le métaheuristique basé sur les araignées sociales. la fonction objective de l’optimisation est de maximiser la somme des similitudes entre les phrases du résumé du candidat afin de conserver le thème du texte, de minimiser la somme des scores afin d’augmenter le taux de summarisation, cette optimisation donnera également le résumé d’un candidat où l’ordre des phrases change par rapport au texte original.La troisième et dernière couche vise à choisir le meilleur résumé des résumés des candidats générés par l’optimisation des couches, nous avons opté pour la technique de vote à la majorité simple.","2176","4","74","Une nouvelle approche multi-couches pour les résumés automatiques de texte mono-document s&#039;appuyant sur des social spiders

Dans cet article, nous proposons une nouvelle approche multi-couches pour les résumés automatique de texte par extraction, où la première couche consiste à utiliser deux techniques d’extraction : notation des phrases, et des similitudes visant à éliminer les phrases redondantes sans perdre le thème du texte. Alors que la deuxième couche vise à optimiser les résultats de la couche précédente par une métaheuristique s&#039;appuyant sur des social spiders, la fonction objective de l’optimisation est de maximiser la somme des similitudes entre les phrases du résumé du candidat afin de conserver le thème du texte, de minimiser la somme des scores afin d’augmenter le taux de synthèse, cette optimisation donnera également le résumé d’un candidat où l’ordre des phrases change par rapport au texte original. La troisième et dernière couche vise à choisir le meilleur résumé des résumés des candidats générés par l’optimisation des couches. Nous avons opté pour la technique de vote à la majorité simple.
","2","2023-06-06 17:16:03","2023-06-06 17:16:47","2176",,"2","3","4","3","3","2",NULL,,"2023-06-06 05:15:58","0"
"3925044","Islamophobia as a Form of Radicalisation discusses the scope and fragmented boundaries of Islamophobia as a concept and a sociopolitical reality. The fifteen chapters of this collection cover and connect interdisciplinary research, media content analyses, media discourse analysis, ethnographic research, intersectoral advocacy work, and action research conducted in Belgium, Canada, France, Germany, Poland, Portugal, and Spain. Confronted with an Islamophobia that is growing as a symptom of broader societal malaise in the West, a resistance against it is also arising. It is now a question of better understanding the foundations and mechanism of this metasolidarity and resistance. Islamophobia as a Form of Radicalisation offers recommendations for urgent consideration by Muslim citizens of Canada and Europe, media professionals, civil society and academic stakeholders, policymakers at the municipal, provincial and federal levels, and academics.","Islamophobia in the right-wing Portuguese opinion press (2015-2020)","Colin Robineau","3944","L'islamophobie dans la presse d'opinion portugaise de droite (2015-2020)

L'islamophobie en tant que forme de radicalisation examine la portée et les limites fragmentées de l'islamophobie en tant que concept et réalité sociopolitique. Les quinze chapitres de ce recueil couvrent et relient des recherches interdisciplinaires, des analyses du contenu des médias, des analyses du discours des médias, des recherches ethnographiques, des travaux de plaidoyer intersectoriels et des recherches-actions menées en Belgique, au Canada, en France, en Allemagne, en Pologne, au Portugal et en Espagne. Face à une islamophobie croissante, symptôme d'un malaise sociétal plus large en Occident, une résistance à cette islamophobie est également en train de naître. Il s'agit maintenant de mieux comprendre les fondements et les mécanismes de cette métasolidarité et de cette résistance. L'islamophobie comme forme de radicalisation propose des recommandations que les citoyens musulmans du Canada et d'Europe, les professionnels des médias, les acteurs de la société civile et du monde universitaire, les décideurs politiques aux niveaux municipal, provincial et fédéral, ainsi que les universitaires doivent examiner de toute urgence.","18760","3","98","L&#039;islamophobie dans la presse d&#039;opinion portugaise de droite (2015-2020)

L&#039;islamophobie en tant que forme de radicalisation examine la portée et les limites fragmentées de l&#039;islamophobie en tant que concept et réalité sociopolitique. Les quinze chapitres de ce recueil couvrent et mettent en rapport des recherches interdisciplinaires, des analyses du contenu médiatique, des analyses du discours médiatique, des recherches ethnographiques, des efforts intersectoriels de défense des droits et des recherches-actions menées en Belgique, au Canada, en France, en Allemagne, en Pologne, au Portugal et en Espagne. Face à une islamophobie croissante, symptôme d&#039;un malaise sociétal plus large en Occident, une résistance à cette islamophobie est également en train de naître. Il s&#039;agit maintenant de mieux comprendre les fondements et les mécanismes de cette métasolidarité et de cette résistance. L&#039;islamophobie comme forme de radicalisation propose des recommandations que les citoyens musulmans du Canada et d&#039;Europe, les professionnels des médias, les acteurs de la société civile et du monde universitaire, les décideurs politiques aux niveaux municipal, provincial et fédéral, ainsi que les universitaires doivent examiner de toute urgence.","2","2023-06-12 12:03:24","2023-06-12 12:23:49","18760",,"1","2","1","2","1","1",NULL,,"2023-06-12 11:36:47","0"
"3979568","RiC-O Converter is an open-source command-line tool to convert EAD finding aids and EAC-CPF authority records to RDF files conforming to ICA Records in Contexts ontology (RiC-O), in a robust manner. It was developed for the Archives nationales of France (ANF) but is aimed to be reused by other archival institutions, and to this aim is fully documented in English. It is based on XSLT stylesheets that take into account the variability of EAD content. It enabled the ANF to convert 15400 EAC-CPF files and 31000 EAD files into an homogeneous knowledge graph, and to start a more specific project aiming to provide end users with an intuitive search interface for a significant subset of this graph, opening new perspectives for navigating and linking from/to archival metadata.","RiC-O Converter: a Software to Convert EAC-CPF and EAD 2002 XML files to RDF Datasets Conforming to Records in Contexts Ontology","Florence Clavaud, Thomas Francart, Pauline Charbonnier","370","RiC-O Converter : un logiciel pour convertir les fichiers XML EAC-CPF et EAD 2002 en ensembles de données RDF conformes à l'ontologie Records in Contexts

RiC-O Converter est un outil open-source en ligne de commande pour convertir les instruments de recherche EAD et les notices d'autorité EAC-CPF en fichiers RDF conformes à l'ontologie Records in Contexts (RiC-O) de l'ICA, d'une manière robuste. Il a été développé pour les Archives nationales de France (ANF) mais est destiné à être réutilisé par d'autres institutions d'archives, et à cette fin est entièrement documenté en anglais. Il est basé sur des feuilles de style XSLT qui prennent en compte la variabilité du contenu EAD. Il a permis à l'ANF de convertir 15400 fichiers EAC-CPF et 31000 fichiers EAD en un graphe de connaissances homogène, et de lancer un projet plus spécifique visant à fournir aux utilisateurs finaux une interface de recherche intuitive pour un sous-ensemble significatif de ce graphe, ouvrant de nouvelles perspectives pour la navigation et la création de liens à partir de métadonnées d'archives.","10124","3","107","RiC-O Converter : un logiciel pour convertir les fichiers XML EAC-CPF et EAD 2002 en ensembles de données RDF conformes à l&#039;ontologie Records in Contexts

RiC-O Converter est un outil open-source en ligne de commande pour convertir de manière fiable les instruments de recherche EAD et les notices d&#039;autorité EAC-CPF en fichiers RDF conformes à l&#039;ontologie Records in Contexts (RiC-O) de l&#039;ICA. Il a été développé pour les Archives nationales de France (ANF) mais est destiné à être réutilisé par d&#039;autres institutions d&#039;archives, et à cette fin est entièrement documenté en anglais. Il est basé sur des feuilles de style XSLT qui prennent en compte la variabilité du contenu EAD. Il a permis à l&#039;ANF de convertir 15400 fichiers EAC-CPF et 31000 fichiers EAD en un graphe de connaissances homogène, et de lancer un projet plus spécifique visant à fournir aux utilisateurs finaux une interface de recherche intuitive pour un sous-ensemble significatif de ce graphe, ouvrant de nouvelles perspectives pour la navigation et la création de liens à partir de métadonnées d&#039;archives.","2","2023-06-13 12:11:53","2023-06-13 12:16:11","10124",,"1","1","1","2","1","1",NULL,,"2023-06-13 12:11:45","0"
"3978442","Background Public health surveillance relies on the collection of data, often in near-real time. Recent advances in natural language processing make it possible to envisage an automated system for extracting information from electronic health records. Objective To study the feasibility of setting up a national trauma observatory in France, we compared the performance of several automatic language processing methods in a multiclass classification task of unstructured clinical notes. Methods A total of 69,110 free-text clinical notes related to visits to the emergency departments of the University Hospital of Bordeaux, France, between 2012 and 2019 were manually annotated. Among these clinical notes, 32.5% (22,481/69,110) were traumas. We trained 4 transformer models (deep learning models that encompass attention mechanism) and compared them with the term frequency–inverse document frequency associated with the support vector machine method. Results The transformer models consistently performed better than the term frequency–inverse document frequency and a support vector machine. Among the transformers, the GPTanam model pretrained with a French corpus with an additional autosupervised learning step on 306,368 unlabeled clinical notes showed the best performance with a micro F1-score of 0.969. Conclusions The transformers proved efficient at the multiclass classification of narrative and medical data. Further steps for improvement should focus on the expansion of abbreviations and multioutput multiclass classification.","Deep Learning Transformer Models for Building a Comprehensive and Real-time Trauma Observatory: Development and Validation Study","Gabrielle Chenais, Cédric Gil-Jardiné, Hélène Touchais, Marta Avalos Fernandez, Benjamin Contrand, Eric Tellier, Xavier Combes, Loick Bourdois, Philippe Revel, Emmanuel Lagarde","3817","Modèles de transformation de l’apprentissage profond pour la construction d’un observatoire complet et en temps réel des traumatismes: Étude de développement et de validation

Contexte La surveillance de la santé publique repose sur la collecte de données, souvent en temps quasi réel. Les progrès récents dans le traitement du langage naturel permettent d’envisager un système automatisé d’extraction d’informations à partir de dossiers de santé électroniques. Objectif Pour étudier la faisabilité de la mise en place d’un observatoire national des traumatismes en France, nous avons comparé la performance de plusieurs méthodes de traitement automatique du langage dans une tâche de classification multiclasse de notes cliniques non structurées. Méthodes Un total de 69 110 notes cliniques en texte libre relatives aux visites aux services d’urgence de l’hôpital universitaire de Bordeaux, France, entre 2012 et 2019, ont été annotées manuellement. Parmi ces notes cliniques, 32,5 % (22,481/69,110) étaient des traumatismes. Nous avons formé 4 modèles de transformateurs (modèles d’apprentissage approfondi qui englobent le mécanisme d’attention) et les avons comparés avec le terme fréquence-inverse de document associé à la méthode de la machine vectorielle de support. Résultats Les modèles de transformateur ont toujours été meilleurs que le terme fréquence-inverse de document et une machine vectorielle de support. Parmi les transformateurs, le modèle GPTanam préformé avec un corpus français avec une étape d’apprentissage autosupervisée supplémentaire sur 306 368 notes cliniques non étiquetées a montré les meilleures performances avec un micro-score F1 de 0,969. Conclusions Les transformateurs se sont révélés efficaces dans la classification multiclasse des données narratives et médicales. D’autres mesures d’amélioration devraient être axées sur l’extension des abréviations et de la classification multiclasse multisorties.","12775","4","108","Modèles de transformation de l’apprentissage profond pour la construction d’un observatoire complet et en temps réel des traumatismes : Étude de développement et de validation

Contexte
La surveillance de la santé publique repose sur la collecte de données, souvent en temps quasi réel. Les progrès récents dans le traitement du langage naturel permettent d’envisager un système automatisé d’extraction d’informations à partir de dossiers de santé électroniques.
Objectif
Pour procéder à l&#039;étude de faisabilité de la mise en place d’un observatoire national des traumatismes en France, nous avons comparé la performance de plusieurs méthodes de traitement automatique des langues dans une tâche de classification multiclasse de notes cliniques non structurées.
Méthodes
Un total de 69 110 notes cliniques en texte libre relatives aux visites aux services d’urgence de l’hôpital universitaire de Bordeaux, entre 2012 et 2019, ont été annotées manuellement. Parmi ces notes cliniques, 32,5 % (22 481/69 110) étaient des traumatismes. Nous avons formé 4 modèles de transformateurs (modèles d’apprentissage approfondi englobant le mécanisme d’attention) et les avons comparés avec le TF-IDF associé à la méthode de la machine vectorielle de support.
Résultats
Les modèles de transformateur ont toujours montré une meilleure performance que le TF-IDF et une machine vectorielle de support. Parmi les transformateurs, le modèle GPTanam préformé avec un corpus français avec une étape d’apprentissage autosupervisée supplémentaire sur 306 368 notes cliniques non étiquetées a montré les meilleures performances avec un micro-score F1 de 0,969.
Conclusions
Les transformateurs se sont révélés efficaces dans la classification multiclasse des données narratives et médicales. D’autres mesures d’amélioration devraient être axées sur l’extension des abréviations et de la classification multiclasse multisorties.","2","2023-06-13 12:16:27","2023-06-13 12:45:00","12775",,"1","1","4","1","3","1",NULL,,"2023-06-13 12:16:23","0"
"3991530","In this short report we consider the possible manifestation of theory-of-mind skills by the recently proposed OpenAI's ChatGPT conversational agent. To tap into these skills, we used an indirect speech understanding task, the hinting task, and a new text version of a False Belief/False Photographs paradigm, and the Strange Stories paradigm. The hinting task is usually used to assess individuals with autism or schizophrenia by requesting them to infer hidden intentions from short conversations involving two characters. Our results show that the artificial model has quite limited performances on the Hinting task when either original scoring or revised SCOPE's rating scales are used. To better understand this limitation, we introduced slightly modified versions of the hinting task in which either cues about the presence of a communicative intention were added or a specific question about the character's intentions were asked. Only the latter demonstrated enhanced performances. In addition, the use of a False Belief/False Photographs paradigm to assess belief attribution skills demonstrates that ChatGPT keeps track of successive physical states of the world and may refer to a character's erroneous expectations about the world. No dissociation between the conditions was found. The Strange Stories were associated with correct performances but we could not be sure that the algorithm had no prior knowledge of it. These findings suggest that ChatGPT may answer about a character's intentions or beliefs when the question focuses on these mental states, but does not use such references spontaneously on a regular basis. This may guide AI designers to improve inference models by privileging mental states concepts in order to help chatbots having more natural conversations. This work offers an illustration of the possible application of psychological constructs and paradigms to a cognitive entity of a radically new nature, which leads to a reflection on the experimental methods that should in the future propose evaluation tools designed to allow the comparison of human performances and strategies with those of the machine.","Do conversational agents have a theory of mind? A single case study of ChatGPT with the Hinting, False Beliefs and False Photographs, and Strange Stories paradigms","Eric Brunet-Gouet, Nathan Vidal, Paul Roux","359","Les agents conversationnels ont-ils une théorie de l'esprit ? Une étude de cas unique de ChatGPT avec les paradigmes Hinting, False Beliefs and False Photographs, et Strange Stories.

Dans ce bref rapport, nous examinons la manifestation possible de compétences en théorie de l'esprit par l'agent conversationnel ChatGPT récemment proposé par l'OpenAI. Pour exploiter ces compétences, nous avons utilisé une tâche de compréhension indirecte de la parole, la tâche d'indication, et une nouvelle version textuelle du paradigme Fausses croyances/Fausses photographies, ainsi que le paradigme Histoires étranges. La tâche d'allusion est généralement utilisée pour évaluer les personnes atteintes d'autisme ou de schizophrénie en leur demandant d'inférer des intentions cachées à partir de courtes conversations impliquant deux personnages. Nos résultats montrent que le modèle artificiel a des performances assez limitées dans la tâche d'indication lorsque l'on utilise les échelles de notation originales ou les échelles de notation révisées de SCOPE. Pour mieux comprendre cette limitation, nous avons introduit des versions légèrement modifiées de la tâche d'indication dans lesquelles des indices sur la présence d'une intention de communication ont été ajoutés ou une question spécifique sur les intentions du personnage a été posée. Seule cette dernière version a permis d'améliorer les performances. En outre, l'utilisation d'un paradigme Fausses croyances/Fausses photographies pour évaluer les capacités d'attribution de croyances démontre que la ChatGPT suit les états physiques successifs du monde et peut se référer aux attentes erronées d'un personnage à propos du monde. Aucune dissociation entre les conditions n'a été constatée. Les histoires étranges ont été associées à des performances correctes, mais nous n'avons pas pu être sûrs que l'algorithme n'en avait aucune connaissance préalable. Ces résultats suggèrent que ChatGPT peut répondre sur les intentions ou les croyances d'un personnage lorsque la question porte sur ces états mentaux, mais qu'il n'utilise pas ces références de manière spontanée et régulière. Ceci peut guider les concepteurs d'IA à améliorer les modèles d'inférence en privilégiant les concepts d'états mentaux afin d'aider les chatbots à avoir des conversations plus naturelles. Ce travail offre une illustration de l'application possible de constructions et paradigmes psychologiques à une entité cognitive de nature radicalement nouvelle, ce qui conduit à une réflexion sur les méthodes expérimentales qui devraient à l'avenir proposer des outils d'évaluation conçus pour permettre la comparaison des performances et stratégies humaines avec celles de la machine.","10113","3","110","Les agents conversationnels ont-ils une théorie de l&#039;esprit ? Une étude de cas unique de ChatGPT avec les paradigmes Allusion, Fausses croyances et fausses photographies, et Histoires étranges.

Dans ce bref rapport, nous examinons la manifestation possible de compétences en théorie de l&#039;esprit par l&#039;agent conversationnel ChatGPT récemment mis à disposition par OpenAI. Pour exploiter ces compétences, nous avons utilisé une tâche de compréhension indirecte de la parole, la tâche d&#039;allusion, et une nouvelle version textuelle du paradigme Fausses croyances/Fausses photographies, ainsi que le paradigme Histoires étranges. La tâche d&#039;allusion est généralement utilisée pour évaluer les personnes autistes ou schizophrènes en leur demandant d&#039;inférer des intentions cachées à partir de courtes conversations impliquant deux personnages. Nos résultats montrent que le modèle artificiel a des performances assez limitées dans cette tâche d&#039;allusion lorsque l&#039;on utilise les échelles de notation originales ou les échelles de notation révisées de SCOPE. Pour mieux comprendre cette limitation, nous avons introduit des versions légèrement modifiées de la tâche d&#039;allusion; dans lesquelles des indices sur la présence d&#039;une intention de communication ont été ajoutés ou une question spécifique sur les intentions du personnage a été posée. Seule cette dernière version a permis d&#039;améliorer les performances. En outre, l&#039;utilisation d&#039;un paradigme Fausses croyances/Fausses photographies pour évaluer les capacités d&#039;attribution de croyances démontre que ChatGPT suit les états physiques successifs du monde et peut se référer aux attentes erronées d&#039;un personnage attentes vis-à-vis du monde. Aucune dissociation entre les conditions n&#039;a été constatée. Les Histoires étranges ont été associées à des performances correctes, mais nous n&#039;avons pas pu être sûrs que l&#039;algorithme n&#039;en avait aucune connaissance préalable. Ces résultats suggèrent que ChatGPT peut répondre à des questions sur les intentions ou les croyances d&#039;un personnage lorsque la question porte sur ces états mentaux, mais qu&#039;il n&#039;utilise pas ces références de manière spontanée et régulière. Ceci peut guider les concepteurs d&#039;IA à améliorer les modèles d&#039;inférence en privilégiant les concepts d&#039;états mentaux afin d&#039;aider les agents conversationnels à avoir des conversations plus naturelles. Ce rapport offre une illustration de l&#039;application possible de constructions et paradigmes psychologiques à une entité cognitive de nature radicalement nouvelle, ce qui conduit à une réflexion sur les méthodes expérimentales qui devraient à l&#039;avenir proposer des outils d&#039;évaluation conçus pour permettre la comparaison des performances et stratégies humaines avec celles de la machine.","2","2023-06-13 14:11:30","2023-06-13 14:46:22","10113",,"2","1","4","3","1","4",NULL,,"2023-06-13 02:11:24","0"
"3984528","In this study, we explore the value of using a recently proposed multimodal learning method as an initialization for anomaly detection in abdominal ultrasound images. The method efficiently learns visual concepts from radiological reports using natural language supervision and constrastive learning. The underlying requirement of the method is simply the availability of image and textual descriptions pairs. However, in abdominal ultrasound examinations, radiological reports are associated with several images and describe all organs observed during the examination. To address this shortcoming, we automatically construct image and text pairs using 1) deep clustering for abdominal organ classification on ultrasound images and 2) natural language processing tools to extract the corresponding description on the report. We show that pre-training the model with these constructed pairs yields representations that better separate normal classes from abnormal ones on ultrasound images for the kidneys, compared to ImageNet-based representations, with a 10% improvement in macro-average accuracy.","Joint representation learning from french radiological reports and ultrasound images","Hind Dadoun, Hervé Delingette, Anne-Laure Rousseau, Eric de Kerviler, Nicholas Ayache","3800","Apprentissage de représentations conjointes à partir de rapports radiologiques et d'images d'échographie en français

Dans cette étude, nous explorons la valeur de l'utilisation d'une méthode d'apprentissage multimodale récemment proposée comme initialisation pour la détection d'anomalies dans les images d'échographie abdominale. La méthode apprend efficacement des concepts visuels à partir de rapports radiologiques en utilisant la supervision du langage naturel et l'apprentissage constrastif. L'exigence sous-jacente de la méthode est simplement la disponibilité de paires d'images et de descriptions textuelles. Cependant, dans les examens d'échographie abdominale, les rapports radiologiques sont associés à plusieurs images et décrivent tous les organes observés pendant l'examen. Pour remédier à cette lacune, nous construisons automatiquement des paires d'images et de textes en utilisant 1) le clustering profond pour la classification des organes abdominaux sur les images d'échographie et 2) des outils de traitement du langage naturel pour extraire la description correspondante sur le rapport. Nous montrons que le pré-entraînement du modèle avec ces paires construites produit des représentations qui séparent mieux les classes normales des classes anormales sur les images échographiques des reins, par rapport aux représentations basées sur ImageNet, avec une amélioration de 10 % de la précision macro-moyenne.","18616","3","111","Apprentissage de représentations conjointes à partir de rapports radiologiques en français et d&#039;échographies

Dans cette étude, nous explorons la valeur de l&#039;utilisation d&#039;une méthode d&#039;apprentissage multimodale récemment proposée comme initialisation de la détection d&#039;anomalies dans les images d&#039;échographie abdominale. La méthode apprend efficacement des concepts visuels à partir de rapports radiologiques en utilisant la supervision du langage naturel et l&#039;apprentissage contrastif. L&#039;exigence sous-jacente de la méthode est simplement la disponibilité de paires d&#039;images et de descriptions textuelles. Cependant, dans les examens d&#039;échographie abdominale, les rapports radiologiques sont associés à plusieurs images et décrivent tous les organes observés pendant l&#039;examen. Pour remédier à cette lacune, nous construisons automatiquement des paires d&#039;images et de textes en utilisant 1) le regroupement profond pour la classification des organes abdominaux sur les échographies et 2) des outils de traitement du langage naturel pour extraire la description correspondante sur le rapport. Nous montrons que le pré-entraînement du modèle avec ces paires construites produit des représentations qui séparent mieux les classes normales des classes anormales sur les échographies des reins, par rapport aux représentations basées sur ImageNet, avec une amélioration de 10 % de la précision macro-moyenne.","2","2023-06-13 16:17:52","2023-06-13 16:28:35","18616",,"1","3","3","2","4","1",NULL,,"2023-06-13 04:17:44","0"
"3965003","Differential Linear Logic (DiLL) adds to Linear Logic (LL) a symmetrization of three out of the four exponential rules, and by doing so allows the expression of a natural notion of differentiation. In this paper, we introduce a codigging inference rule for DiLL and study the categorical semantics of DiLL with codigging using differential categories. The addition of codigging makes the rules of DiLL completely symmetrical. We will explain how codigging is interpreted thanks to the exponential function e^x , and in certain cases by the convolutional exponential. In a setting with codigging, every proof is equal to its Taylor series, which implies that every model of DiLL with codigging is quantitative. We provide examples of codigging in relational models, as well as models related to game logic and quantum programming. We also construct a graded model of DiLL with codigging in which the indices witness exponential growth. Since codigging makes the exponential of-course connective ! in LL into a monad, such that monad axioms enforce Taylor expansion, codigging opens the door to applications in programming languages, as well as further categorical generalizations.","Taylor Expansion as a Monad in Models of DiLL","Marie Kerjean, Jean-Simon Lemay","3849","Taylor Expansion en tant que Monade dans les Modèles de DiLL

La logique linéaire différentielle (DiLL) ajoute à la logique linéaire (LL) une symétrie de trois des quatre règles exponentielles, ce qui permet l’expression d’une notion naturelle de différenciation. Dans cet article, nous introduisons une règle d’inférence codigging pour DiLL et étudions la sémantique catégorique de DiLL avec codigging en utilisant des catégories différentielles. L’ajout de codigging rend les règles de DiLL complètement symétriques. Nous expliquerons comment le codigging est interprété grâce à la fonction exponentielle e^x, et dans certains cas par l’exponentiel convolutionnel. Dans un cadre de codigging, chaque preuve est égale à sa série Taylor, ce qui implique que chaque modèle de DiLL avec codigging est quantitatif. Nous fournissons des exemples de codigging dans des modèles relationnels, ainsi que des modèles liés à la logique du jeu et à la programmation quantique. Nous construisons également un modèle gradué de DiLL avec codigging dans lequel les indices témoignent d’une croissance exponentielle. Puisque le codigging rend la connective exponentielle du cours! dans LL en une monade, de sorte que les axiomes monades imposent l’expansion de Taylor, le codigging ouvre la porte aux applications dans les langages de programmation, ainsi qu’à d’autres généralisations catégoriques.","12807","4","112","La série de Taylor en tant que monade dans les modèles de DiLL

La logique linéaire différentielle (DiLL) ajoute à la logique linéaire (LL) une symétrie de trois des quatre règles exponentielles, ce qui permet l’expression d’une notion naturelle de différenciation. Dans cet article, nous introduisons une règle d’inférence codigging pour la DiLL et étudions la sémantique catégorique de la DiLL avec codigging en utilisant des catégories différentielles. L’ajout de codigging rend les règles de la DiLL complètement symétriques. Nous expliquerons comment le codigging est interprété grâce à la fonction exponentielle e^x, et dans certains cas par l’exponentiel convolutionnel. Dans un cadre de codigging, chaque preuve est égale à sa série de Taylor, ce qui implique que chaque modèle de la DiLL avec codigging est quantitatif. Nous fournissons des exemples de codigging dans des modèles relationnels, ainsi que des modèles liés à la logique du jeu et à la programmation quantique. Nous construisons également un modèle gradué de DiLL avec codigging dans lequel les indices témoignent d’une croissance exponentielle. Puisque le codigging rend la connective exponentielle du cours ! dans LL en une monade, de sorte que les axiomes monades imposent l’expansion de Taylor, le codigging ouvre la porte aux applications dans les langages de programmation, ainsi qu’à d’autres généralisations catégoriques.","2","2023-06-13 17:13:14","2023-06-13 17:19:57","12807",,"1","2","3","2","1","1",NULL,,"2023-06-13 05:13:07","0"
"3965397","This article proposes to improve an automatic speech recognition system by rescoring N-best recognition lists with models that could enhance the semantic consistency of the hypotheses. We believe that in noisy parts of speech, the semantic model can help remove acoustic ambiguities. We estimate a pairwise score for each pair of hypotheses by using BERT representations. The acoustic likelihood and LM scores are used as features in order to incorporate acoustic, language, and textual information together. In this research work, we investigate two new ideas: to use a fine-grained semantic representation at the word token level and to rely on the previously recognized sentences. On the TED-LIUM 3 dataset, in clean and noisy conditions, the best performance is obtained by leveraging context beyond the current utterance, which significantly outperforms the rescoring using the state-of-the-art GPT-2 model and the work of Fohr and Illina (2021).","Semantic Information Investigation for Transformer-based Rescoring of N-best Speech Recognition","Irina Illina, Dominique Fohr","3847","Recherche d'informations sémantiques pour la récupération basée sur transformateur de N meilleures Reconnaissance vocale

Cet article propose d'améliorer un système de reconnaissance automatique de la parole en retrouvant N meilleures listes de reconnaissance avec des modèles qui pourraient améliorer la cohérence sémantique des hypothèses. Nous croyons que dans les parties bruyantes de la parole, le modèle sémantique peut aider à supprimer les ambiguïtés acoustiques. Nous estimons un score par paire pour chaque paire d'hypothèses en utilisant des représentations BERT. La probabilité acoustique et les scores LM sont utilisés en tant que caractéristiques afin d'incorporer ensemble des informations acoustiques, linguistiques et textuelles. Dans ce travail de recherche, nous étudions deux nouvelles idées : utiliser une représentation sémantique fine au niveau du jeton de mot et s'appuyer sur les phrases précédemment reconnues. Sur l'ensemble de données TED-LIUM 3, dans des conditions propres et bruyantes, la meilleure performance est obtenue en tirant parti du contexte au-delà de l'énoncé actuel, ce qui surpasse de manière significative la notation en utilisant le modèle GPT-2 de pointe et les travaux de Fohr et Illina (2021).","15763","6","114","Enquête sur l’information sémantique pour la réévaluation de la meilleure reconnaissance vocale N-best s&#039;appuyant sur les Transformer

Cet article propose d’améliorer un système automatique de reconnaissance vocale en réévaluant les listes de reconnaissance N-best avec des modèles qui pourraient améliorer la cohérence sémantique des hypothèses. Nous croyons que dans les parties bruitées du discours, le modèle sémantique peut aider à éliminer les ambiguïtés acoustiques. Nous estimons un score par paire pour chaque paire d’hypothèses en utilisant les représentations BERT. La probabilité acoustique et les scores LM sont utilisés comme caractéristiques afin d’incorporer des informations acoustiques, linguistiques et textuelles ensemble. Dans ce travail de recherche, nous étudions deux nouvelles techniques : l&#039;utilisation une représentation sémantique à grain fin ancrée au niveau du token de mot, et l&#039;appui sur les phrases précédemment reconnues. Sur l’ensemble de données TED-LIUM 3, dans des conditions propres comme bruitées, les meilleures performances sont obtenues en tirant parti du contexte au-delà de l’énoncé actuel, ce qui surpasse considérablement la réévaluation utilisant le modèle GPT-2 de pointe et le travail de Fohr et Illina (2021).
","2","2023-06-14 14:32:20","2023-06-14 14:32:53","15763",,"3","1","4","3","1","4",NULL,,"2023-06-14 02:32:14","0"
"3951403","Coding practices are increasingly used by software companies. Their use promotes consistency, readability, and maintainability, which contribute to software quality. Coding practices were initially enforced by general-purpose linters, but companies now tend to design and adopt their own company-specific practices. However, these company-specific practices are often not automated, making it challenging to ensure they are shared and used by developers. Converting these practices into linter rules is a complex task that requires extensive static analysis and language engineering expertise. In this paper, we seek to answer the following question: can coding practices be learned automatically from examples manually tagged by developers? We conduct a feasibility study using CodeBERT, a state-of-the-art machine learning approach, to learn linter rules. Our results show that, although the resulting classifiers reach high precision and recall scores when evaluated on balanced synthetic datasets, their application on real-world, unbalanced codebases, while maintaining excellent recall, suffers from a severe drop in precision that hinders their usability.","MLinter: Learning Coding Practices from Examples-Dream or Reality?","Corentin Latappy, Quentin Perez, Thomas Degueule, Jean-Rémy Falleri, Christelle Urtado, Sylvain Vauttier, Xavier Blanc, Cédric Teyton","3872","MLinter : Apprendre les pratiques de codage à partir d'exemples - rêve ou réalité ?

Les pratiques de codage sont de plus en plus utilisées par les entreprises de logiciels. Leur utilisation favorise la cohérence, la lisibilité et la maintenabilité, ce qui contribue à la qualité des logiciels. Les pratiques de codage ont d'abord été appliquées par des linters à usage général, mais les entreprises ont désormais tendance à concevoir et à adopter des pratiques qui leur sont propres. Cependant, ces pratiques spécifiques à l'entreprise ne sont souvent pas automatisées, ce qui rend difficile de s'assurer qu'elles sont partagées et utilisées par les développeurs. La conversion de ces pratiques en règles de linter est une tâche complexe qui nécessite une analyse statique approfondie et une expertise en ingénierie linguistique. Dans cet article, nous cherchons à répondre à la question suivante : les pratiques de codage peuvent-elles être apprises automatiquement à partir d'exemples marqués manuellement par les développeurs ? Nous menons une étude de faisabilité en utilisant CodeBERT, une approche d'apprentissage automatique de pointe, pour apprendre les règles de codage. Nos résultats montrent que, bien que les classificateurs obtenus atteignent des scores de précision et de rappel élevés lorsqu'ils sont évalués sur des ensembles de données synthétiques équilibrés, leur application sur des bases de code réelles et déséquilibrées, tout en conservant un excellent rappel, souffre d'une chute importante de la précision qui entrave leur utilisabilité.","18688","3","116","MLinter : Apprendre les pratiques de programmation à partir d&#039;exemples, rêve ou réalité ?

Les pratiques de programmation sont de plus en plus utilisées par les entreprises de logiciels. Leur utilisation favorise la cohérence, la lisibilité et la maintenabilité, ce qui contribue à la qualité des logiciels. Les pratiques de programmation ont d&#039;abord été appliquées par des linters à usage général, mais les entreprises ont désormais tendance à concevoir et à adopter des pratiques qui leur sont propres. Cependant, ces pratiques spécifiques à l&#039;entreprise ne sont souvent pas automatisées, ce qui rend difficile de s&#039;assurer qu&#039;elles sont partagées et utilisées par les développeurs. La conversion de ces pratiques en règles de linter est une tâche complexe qui nécessite une analyse statique approfondie et une expertise en ingénierie linguistique. Dans cet article, nous cherchons à répondre à la question suivante : les pratiques de programmation peuvent-elles être apprises automatiquement à partir d&#039;exemples marqués manuellement par les développeurs ? Nous menons une étude de faisabilité en utilisant CodeBERT, une approche d&#039;apprentissage automatique de pointe, pour apprendre les règles de programmation. Nos résultats montrent que, bien que les classificateurs obtenus atteignent des scores de précision et de rappel élevés lorsqu&#039;ils sont évalués sur des ensembles de données synthétiques équilibrés, leur application sur des bases de code réelles et déséquilibrées, tout en conservant un excellent rappel, souffre d&#039;une chute importante de la précision qui entrave leur utilisabilité.","2","2023-06-14 14:39:57","2023-06-14 14:48:14","18688",,"1","1","1","2","2","1",NULL,,"2023-06-14 02:39:45","0"
"3933089","We present a new pre-training method, Multimodal Inverse Cloze Task, for Knowledge-based Visual Question Answering about named Entities (KVQAE). KVQAE is a recently introduced task that consists in answering questions about named entities grounded in a visual context using a Knowledge Base. Therefore, the interaction between the modalities is paramount to retrieve information and must be captured with complex fusion models. As these models require a lot of training data, we design this pre-training task from existing work in textual Question Answering. It consists in considering a sentence as a pseudo-question and its context as a pseudo-relevant passage and is extended by considering images near texts in multimodal documents. Our method is applicable to different neural network architectures and leads to a 9% relative-MRR and 15% relative-F1 gain for retrieval and reading comprehension, respectively, over a no-pre-training baseline.","Multimodal Inverse Cloze Task for Knowledge-based Visual Question Answering","Paul Lerner, Olivier Ferret, Camille Guinaudeau","410","Tâche multimodale de fermeture inversée pour la réponse aux questions visuelles basées sur les connaissances

Nous présentons une nouvelle méthode de préformation, Multimodal Inverse Cloze Task, pour répondre aux questions visuelles basées sur les connaissances sur les entités nommées (KVQAE). KVQAE est une tâche récemment introduite qui consiste à répondre à des questions sur des entités nommées fondées sur un contexte visuel à l’aide d’une base de connaissances. Par conséquent, l’interaction entre les modalités est primordiale pour récupérer des informations et doit être capturée avec des modèles de fusion complexes. Comme ces modèles nécessitent beaucoup de données de formation, nous concevons cette tâche de préformation à partir du travail existant en réponse aux questions textuelles. Il consiste à considérer une phrase comme une pseudo-question et son contexte comme un passage pseudo-pertinent et s’étend en considérant des images à proximité de textes dans des documents multimodals. Notre méthode est applicable à différentes architectures de réseau neuronal et conduit à un gain de 9 % relatif-MRR et 15 % relatif-F1 pour la récupération et la compréhension de lecture, respectivement, sur une base sans formation.","3164","4","120","Répondre aux questions visuelles s&#039;appuyant sur les connaissances avec Multimodal Inverse Cloze Task

Nous présentons une nouvelle méthode de préformation, Multimodal Inverse Cloze Task, pour répondre aux questions visuelles s&#039;appuyant sur les connaissances sur les entités nommées (KVQAE). KVQAE est une tâche récemment introduite qui consiste à répondre à des questions sur des entités nommées fondées sur un contexte visuel à l’aide d’une base de connaissances. Par conséquent, l’interaction entre les modalités est primordiale pour récupérer des informations et doit être capturée avec des modèles de fusion complexes. Comme ces modèles nécessitent beaucoup de données de formation, nous concevons cette tâche de préformation à partir du travail existant en réponse aux questions textuelles. Il consiste à considérer une phrase comme une pseudo-question et son contexte comme un passage pseudo-pertinent et s’étend en considérant des images à proximité de textes dans des documents multimodaux. Notre méthode est applicable à différentes architectures de réseau neuronal et conduit à un gain de 9% relatif au MRR et de 15% relatif au score F1 pour la récupération et la compréhension de lecture, respectivement, sur une base sans formation.","2","2023-06-14 14:53:26","2023-06-14 15:14:52","3164",,"3","4","3","3","1","4",NULL,,"2023-06-14 02:53:19","0"
"3980046","Open Information Extraction (OIE) is the task of extracting tuples of the form (subject, predicate, object), without any knowledge of the type and lexical form of the predicate, the subject, or the object. In this work, we focus on improving OIE quality by exploiting domain knowledge about the subject and object. More precisely, knowing that the subjects and objects in sentences are often named entities, we explore how to inject constraints in the extraction through constrained inference and constraint-aware training. Our work leverages the state-of-the-art OpenIE6 platform, which we adapt to our setting. Through a carefully constructed training dataset and constrained training, we obtain a 29.17% F1-score improvement in the CaRB metric and a 24.37% F1-score improvement in the WIRe57 metric. Our technique has important applications-one of them is investigative journalism, where automatically extracting conflict-of-interest between scientists and funding organizations helps understand the type of relations companies engage with the scientists. Our code and data are available at https: //github.com/prajnaupadhyay/ openie-with-entities","Open Information Extraction with Entity Focused Constraints","Prajna Upadhyay, Oana Balalau, Ioana Manolescu","3812","Extraction d'information ouverte avec des contraintes centrées sur les entités

L'extraction ouverte d'informations (OIE) consiste à extraire des tuples de la forme (sujet, prédicat, objet), sans aucune connaissance du type et de la forme lexicale du prédicat, du sujet ou de l'objet. Dans ce travail, nous nous concentrons sur l'amélioration de la qualité de l'OIE en exploitant les connaissances du domaine sur le sujet et l'objet. Plus précisément, sachant que les sujets et les objets dans les phrases sont souvent des entités nommées, nous explorons comment injecter des contraintes dans l'extraction par le biais de l'inférence contrainte et de la formation consciente des contraintes. Notre travail s'appuie sur la plateforme de pointe OpenIE6, que nous adaptons à notre contexte. Grâce à un ensemble de données d'entraînement soigneusement construit et à l'entraînement avec contraintes, nous obtenons une amélioration de 29,17 % du score F1 dans la métrique CaRB et une amélioration de 24,37 % du score F1 dans la métrique WIRe57. Notre technique a d'importantes applications, notamment dans le domaine du journalisme d'investigation, où l'extraction automatique des conflits d'intérêts entre les scientifiques et les organismes de financement permet de comprendre le type de relations que les entreprises entretiennent avec les scientifiques. Notre code et nos données sont disponibles à l'adresse https : //github.com/prajnaupadhyay/ openie-with-entities","18628","3","125","Extraction d&#039;information ouverte avec des contraintes centrées sur les entités

L&#039;extraction ouverte d&#039;informations (OIE) consiste à extraire des tuples de la forme (sujet, prédicat, objet), sans aucune connaissance du type et de la forme lexicale du prédicat, du sujet ou de l&#039;objet. Dans ce travail, nous nous concentrons sur l&#039;amélioration de la qualité de l&#039;OIE en exploitant les connaissances du domaine sur le sujet et l&#039;objet. Plus précisément, sachant que les sujets et les objets dans les phrases sont souvent des entités nommées, nous explorons comment injecter des contraintes dans l&#039;extraction par le biais de l&#039;inférence contrainte et de la formation consciente des contraintes. Notre travail s&#039;appuie sur la plateforme de pointe OpenIE6, que nous adaptons à notre contexte. Grâce à un ensemble de données d&#039;entraînement soigneusement construit et à l&#039;entraînement avec contraintes, nous obtenons une amélioration de 29,17 % du score F1 dans la métrique CaRB et une amélioration de 24,37 % du score F1 dans la métrique WIRe57. Notre technique a d&#039;importantes applications, notamment dans le domaine du journalisme d&#039;investigation, où l&#039;extraction automatique des conflits d&#039;intérêts entre les scientifiques et les organismes de financement permet de comprendre le type de relations que les entreprises entretiennent avec les scientifiques. Notre code et nos données sont disponibles à l&#039;adresse https : //github.com/prajnaupadhyay/ openie-with-entities","2","2023-06-14 15:17:31","2023-06-14 15:28:47","18628",,"1","1","1","1","1","1",NULL,,"2023-06-14 03:17:19","0"
"3980458","GitHub provides a distributed and collaborative platform to develop and maintain open-source projects. This social coding platform achieves this collaborative development, with or without coordination, using pull requests and issues artefacts. When the number of daily submitted issues rapidly grows up, especially in popular repositories, managing issues becomes more complicated. To help the repository’s developers in issues processing, there are external contributors who fix issues by submitting pull-requests. On GitHub, a pull-request is frequently linked with a submitted issue to show that a solution is in progress. Unfortunately, contributors might be lazy or forget to link the Pull-Requests with their corresponding Issues. Only a very small share of these links are established, whereas a large portion of links is missed in the development history. In spite of that, even for senior developers, manually recovering the links between Pull-Request and Issues from evolutionary development history is a time-consuming, challenging, and error-prone task. In this article, we propose to build ML models to recover links between pull-requests and their issues using two Machine Learning algorithms (KMeans and BIRCH) based on lexical and semantic weighting measurements. These models are evaluated using PI-Link ground-truth dataset. The obtained results show that pull-request and issue links can be recovered with an accuracy of 91.5% using BIRCH clustering algorithm.","ML-Augmented Automation for Recovering Links Between Pull-Requests and Issues on GitHub","Zakarea Alshara, Hamzeh Eyal-Salman, Anas Shatnawi, Abdelhak-Djamel Seriai","368","Automatisation renforcée par le ML pour récupérer les liens entre les Pull-Requests et les Issues sur GitHub

GitHub fournit une plateforme distribuée et collaborative pour développer et maintenir des projets open-source. Cette plateforme de codage social réalise ce développement collaboratif, avec ou sans coordination, en utilisant des artefacts de type ""pull requests"" (demandes d'extraction) et ""issues"" (problèmes). Lorsque le nombre de problèmes soumis quotidiennement augmente rapidement, en particulier dans les dépôts populaires, la gestion des problèmes devient plus compliquée. Pour aider les développeurs du dépôt à traiter les problèmes, il existe des contributeurs externes qui corrigent les problèmes en soumettant des demandes d'extraction. Sur GitHub, une pull-request est souvent liée à un problème soumis pour montrer qu'une solution est en cours. Malheureusement, les contributeurs peuvent être paresseux ou oublier de lier les Pull-Requests avec les Issues correspondantes. Seule une très petite partie de ces liens est établie, tandis qu'une grande partie des liens est oubliée dans l'historique de développement. Malgré cela, même pour les développeurs expérimentés, la récupération manuelle des liens entre les Pull-Requests et les Issues à partir de l'historique de développement évolutif est une tâche qui prend du temps, qui est difficile et qui est sujette à des erreurs. Dans cet article, nous proposons de construire des modèles d'apprentissage automatique pour récupérer les liens entre les demandes d'extraction et leurs problèmes en utilisant deux algorithmes d'apprentissage automatique (KMeans et BIRCH) basés sur des mesures de pondération lexicale et sémantique. Ces modèles sont évalués à l'aide de l'ensemble de données de référence PI-Link. Les résultats obtenus montrent que les liens entre les demandes et les problèmes peuvent être récupérés avec une précision de 91,5 % en utilisant l'algorithme de clustering BIRCH.","10122","3","135","Automatisation renforcée par le ML pour récupérer les liens entre les pull-requests et les issues sur GitHub

GitHub propose une plateforme décentralisée et collaborative pour développer et héberger des projets open-source. Cette plateforme de codage social réalise ce développement collaboratif, avec ou sans coordination, en utilisant des artefacts de type &quot;pull requests&quot; (demandes d&#039;extraction) et &quot;issues&quot; (problèmes). Lorsque le nombre de problèmes soumis quotidiennement augmente rapidement, en particulier dans les dépôts populaires, la gestion des problèmes devient plus compliquée. Pour aider les développeurs du dépôt à les traiter, des contributeurs externes corrigent ces problèmes en soumettant des demandes d&#039;extraction. Sur GitHub, une pull-request est souvent liée à un problème soumis pour montrer qu&#039;une solution est en cours. Malheureusement, les contributeurs peuvent être paresseux ou oublier de lier les pull-requests avec les issues correspondantes. Seule une très petite partie de ces liens est établie, tandis qu&#039;une grande partie des liens est oubliée dans l&#039;historique de développement. Malgré cela, même pour les développeurs expérimentés, la récupération manuelle des liens entre les pull-requests et les issues à partir de l&#039;historique de développement évolutif est une tâche qui prend du temps, qui est difficile et qui est sujette à des erreurs. Dans cet article, nous proposons de construire des modèles d&#039;apprentissage automatique pour récupérer les liens entre les demandes d&#039;extraction et leurs problèmes en utilisant deux algorithmes d&#039;apprentissage automatique (KMeans et BIRCH) basés sur des mesures de pondération lexicale et sémantique. Ces modèles sont évalués à l&#039;aide de l&#039;ensemble de données de référence PI-Link. Les résultats obtenus montrent que les liens entre les demandes et les problèmes peuvent être récupérés avec une précision de 91,5 % en utilisant l&#039;algorithme de clustering BIRCH.","2","2023-06-15 16:13:55","2023-06-15 16:28:21","10122",,"1","1","1","3","1","2",NULL,,"2023-06-15 04:13:42","0"
"3975399","In the digital world, more and more autonomous agents make automatic decisions to optimize a criterion by learning from their past decisions. Their rapid multiplication implies that these agents are going to interact with each other, whereas the algorithms they use were not necessarily designed for this. Unexpected emergent behaviors could therefore occur. In this thesis, we contribute to a first step towards the control of a large number of autonomous agents, which learn by interacting with an unknown environment, but also with other agents. We focus on bandit algorithms: an agent aims to minimize its regret with respect to the best possible policy by choosing the best actions. As the agent observes only the outcomes of the actions it has chosen, it must handle the exploration-exploitation dilemma: should one choose the action whose outcome is loosely estimated, or play the action whose result is empirically the best?Our contributions begin with the study of bandits in evolving environment, which is often the case in applications. We then propose contributions on the applications of contextual bandits in the digital world: dynamic allocation of resources for cloud computing, optimization of marketing campaigns, or automatic dialogue. We then state the problem of massively decentralized bandits to handle a very common problem in the digital world, A/B testing, but with the constraint of guarantying privacy. Finally, for optimizing communications in Internet of Things, we propose the massively multiplayer bandits.","On the relevance of bandit algorithms in digital world","Raphaël Feraud","3825","Sur la pertinence des algorithmes de bandit dans le monde numérique

Dans le monde numérique, de plus en plus d'agents autonomes prennent des décisions automatiques pour optimiser un critère en s'inspirant de leurs décisions passées. Leur multiplication rapide implique que ces agents vont interagir entre eux, alors que les algorithmes qu'ils utilisent n'ont pas forcément été conçus pour cela. Des comportements imprévus pourraient donc se produire. Dans cette thèse, nous contribuons à un premier pas vers le contrôle d'un grand nombre d'agents autonomes, qui apprennent en interagissant avec un environnement inconnu, mais aussi avec d'autres agents. Nous nous concentrons sur les algorithmes de bandit : un agent vise à minimiser ses regrets par rapport à la meilleure politique possible en choisissant les meilleures actions. Comme l'agent observe uniquement les résultats des actions qu'il a choisies, il doit gérer le dilemme de l'exploration-exploitation : faut-il choisir l'action dont le résultat est vaguement estimé, ou jouer l'action dont le résultat est empiriquement le meilleur ? Nos contributions commencent par l'étude des bandits dans un environnement en évolution, ce qui est souvent le cas dans les applications. Nous proposons ensuite des contributions sur les applications des bandits contextuels dans le monde numérique : allocation dynamique de ressources pour le cloud computing, optimisation de campagnes marketing, dialogue automatique. Nous posons ensuite le problème de bandits massivement décentralisés pour gérer un problème très commun dans le monde numérique, les tests A/B, mais avec la contrainte de garantir la vie privée. Enfin, pour optimiser les communications dans l'Internet des Objets, nous proposons les bandits massivement multijoueurs.","15741","6","137","Sur la pertinence des algorithmes de bandit dans le monde numérique

Dans le monde numérique, de plus en plus d&#039;agents autonomes prennent des décisions automatiques pour optimiser un critère en apprenant de leurs choix précédents. Leur multiplication rapide implique que ces agents vont interagir entre eux, alors que les algorithmes qu&#039;ils utilisent n&#039;ont pas forcément été conçus pour cela. Des comportements imprévus pourraient donc se produire. Dans cette thèse, nous contribuons à un premier pas vers le contrôle d&#039;un grand nombre d&#039;agents autonomes, qui apprennent en interagissant avec un environnement inconnu, mais aussi avec d&#039;autres agents. Nous nous concentrons sur les algorithmes de bandit : un agent vise à minimiser ses regrets par rapport à la meilleure politique possible en choisissant les meilleures actions. Comme l&#039;agent observe uniquement les résultats des actions qu&#039;il a choisies, il doit gérer le dilemme exploration-exploitation : faut-il choisir l&#039;action dont le résultat est vaguement estimé, ou jouer l&#039;action dont le résultat est empiriquement le meilleur ? Nos contributions commencent par l&#039;étude des bandits dans un environnement en évolution, ce qui est souvent le cas dans les applications. Nous proposons ensuite des contributions sur les applications des bandits contextuels dans le monde numérique : allocation dynamique de ressources pour le cloud computing, optimisation de campagnes marketing, dialogue automatique. Nous posons ensuite le problème de bandits massivement décentralisés pour gérer un problème très commun dans le monde numérique, les tests A/B, mais avec la contrainte de garantir la vie privée. Enfin, pour optimiser les communications dans l&#039;Internet des objets, nous proposons les bandits massivement multijoueurs.","2","2023-06-15 17:01:28","2023-06-15 17:18:30","15741",,"1","1","2","2","1","1",NULL,,"2023-06-15 05:01:18","0"
"3812715","Training of multi-speaker text-to-speech (TTS) systems relies on curated datasets based on high-quality recordings or audiobooks. Such datasets often lack speaker diversity and are expensive to collect. As an alternative, recent studies have leveraged the availability of large, crowdsourced automatic speech recognition (ASR) datasets. A major problem with such datasets is the presence of noisy and/or distorted samples, which degrade TTS quality. In this paper, we propose to automatically select high-quality training samples using a non-intrusive mean opinion score (MOS) estimator, WV-MOS. We show the viability of this approach for training a multi-speaker GlowTTS model on the Common Voice English dataset. Our approach improves the overall quality of generated utterances by 1.26 MOS point with respect to training on all the samples and by 0.35 MOS point with respect to training on the LibriTTS dataset. This opens the door to automatic TTS dataset curation for a wider range of languages.","Can we use Common Voice to train a Multi-Speaker TTS system?","Sewade Ogun, Vincent Colotte, Emmanuel Vincent","4263","Peut-on utiliser Common Voice pour former un système TTS multilocuteur ?

La formation des systèmes de synthèse vocale à plusieurs locuteurs repose sur des ensembles de données curatifs basés sur des enregistrements ou des livres audio de haute qualité. Ces ensembles de données manquent souvent de diversité dans les locuteurs et sont coûteux à collecter. En guise d'alternative, des études récentes ont exploité la disponibilité de vastes ensembles de données de reconnaissance automatique de la parole (ASR) provenant de la foule. L'un des principaux problèmes de ces ensembles de données est la présence d'échantillons bruyants et/ou déformés, qui dégradent la qualité du TTS. Dans cet article, nous proposons de sélectionner automatiquement des échantillons d'entraînement de haute qualité à l'aide d'un estimateur non intrusif du score d'opinion moyen (MOS), le WV-MOS. Nous démontrons la viabilité de cette approche pour l'entraînement d'un modèle GlowTTS à plusieurs locuteurs sur l'ensemble de données Common Voice English. Notre approche améliore la qualité globale des énoncés générés de 1,26 point MOS par rapport à l'entraînement sur tous les échantillons et de 0,35 point MOS par rapport à l'entraînement sur l'ensemble de données LibriTTS. Cela ouvre la voie à la curation automatique de jeux de données TTS pour un plus grand nombre de langues.","17274","3","139","Peut-on utiliser Common Voice pour former un système de synthèse vocale à locuteurs multiples ?

La formation des systèmes de synthèse vocale à locuteurs multiples repose sur des ensembles de données nettoyés et s&#039;appuyant sur des enregistrements ou des livres audio de haute qualité. Ces ensembles de données manquent souvent de diversité dans les locuteurs et sont coûteux à collecter. En guise d&#039;alternative, des études récentes ont exploité la disponibilité de vastes ensembles de données de reconnaissance automatique de la parole (RAP) utilisant une approche crowdsourcing. L&#039;un des principaux problèmes de ces ensembles de données est la présence d&#039;échantillons bruités et/ou déformés, qui dégradent la qualité de la synthèse vocale. Dans cet article, nous proposons de sélectionner automatiquement des échantillons d&#039;entraînement de haute qualité à l&#039;aide d&#039;un estimateur non intrusif du score d&#039;opinion moyen (MOS), le WV-MOS. Nous démontrons la viabilité de cette approche pour l&#039;entraînement d&#039;un modèle GlowTTS à plusieurs locuteurs sur l&#039;ensemble de données Common Voice English. Notre approche améliore la qualité globale des énoncés générés de 1,26 point MOS par rapport à l&#039;entraînement sur tous les échantillons et de 0,35 point MOS par rapport à l&#039;entraînement sur l&#039;ensemble de données LibriTTS. Cela ouvre la voie à la curation automatique de jeux de données de synthèse vocale pour un plus grand nombre de langues.","2","2023-06-15 17:37:55","2023-06-15 17:52:59","17274",,"1","1","3","3","1","3",NULL,,"2023-06-15 05:37:40","0"
"3998392","Stuttering is a neuro-developmental speech impairment characterized by uncontrolled utterances (interjections) and core behaviors (blocks, repetitions, and prolongations), and is caused by the failure of speech sensorimotors. Due to its complex nature, stuttering detection (SD) is a difficult task. If detected at an early stage, it could facilitate speech therapists to observe and rectify the speech patterns of persons who stutter (PWS). The stuttered speech of PWS is usually available in limited amounts and is highly imbalanced. To this end, we address the class imbalance problem in the SD domain via a multibranching (MB) scheme and by weighting the contribution of classes in the overall loss function, resulting in a huge improvement in stuttering classes on the SEP-28k dataset over the baseline (StutterNet). To tackle data scarcity, we investigate the effectiveness of data augmentation on top of a multi-branched training scheme. The augmented training outperforms the MB StutterNet (clean) by a relative margin of 4.18% in macro F1-score (F1). In addition, we propose a multi-contextual (MC) StutterNet, which exploits different contexts of the stuttered speech, resulting in an overall improvement of 4.48% in F 1 over the single context based MB StutterNet. Finally, we have shown that applying data augmentation in the cross-corpora scenario can improve the overall SD performance by a relative margin of 13.23% in F1 over the clean training.","Advancing Stuttering Detection via Data Augmentation, Class-Balanced Loss and Multi-Contextual Deep Learning","Shakeel Ahmad Sheikh, Md Sahidullah, Fabrice Hirsch, Slim Ouni","3769","Avancement de la détection du bégaiement via l'augmentation des données, la perte équilibrée en classes et l'apprentissage profond multicontexte

Le bégaiement est une altération neuro-développementale de la parole caractérisée par des énoncés non contrôlés (interjections) et des comportements de base (blocs, répétitions et prolongations), et est causée par la défaillance des moteurs sensorimoteurs de la parole. En raison de sa nature complexe, la détection du bégaiement (SD) est une tâche difficile. S'il est détecté à un stade précoce, il pourrait aider les orthophonistes à observer et à rectifier les modèles d'élocution des personnes qui bégaient (PWS). La parole bégayée de PWS est généralement disponible en quantités limitées et est très déséquilibrée. À cette fin, nous nous attaquons au problème de déséquilibre de classe dans le domaine SD via un schéma de multibranchement (MB) et en pondérant la contribution des classes dans la fonction de perte globale, ce qui entraîne une amélioration considérable des classes de bégaiement sur l'ensemble de données SEP-28k par rapport à la ligne de base (StutterNet). Pour lutter contre la rareté des données, nous étudions l'efficacité de l'augmentation des données en plus d'un programme de formation à plusieurs branches. La formation augmentée surpasse le MB StutterNet (propre) d'une marge relative de 4,18% dans la macro F1-score (F1). En outre, nous proposons un multi-contextuel (MC) StutterNet, qui exploite différents contextes de la parole bégayée, résultant en une amélioration globale de 4,48% en F 1 par rapport au contexte unique basé MB StutterNet. Enfin, nous avons montré que l'application de l'augmentation des données dans le scénario inter-corpora peut améliorer la performance globale de DD d'une marge relative de 13,23% en F1 par rapport à la formation propre.","15685","6","146","Avancement de la détection du bégaiement via l&#039;augmentation des données, la perte équilibrée en classes et l&#039;apprentissage profond multi-contextuel

Le bégaiement est une altération neuro-développementale de la parole caractérisée par des énoncés non contrôlés (interjections) et des comportements de base (blocages, répétitions et prolongations), et est causée par la défaillance des moteurs sensorimoteurs de la parole. En raison de sa nature complexe, la détection du bégaiement (SD, stuttering detection) n&#039;est pas une tâche facile. S&#039;il est détecté à un stade précoce, il pourrait aider les orthophonistes à observer et à rectifier les modèles d&#039;élocution des personnes bègues. Le discours des personnes bègues est généralement disponible en quantités limitées et est très déséquilibré. À cette fin, nous nous attaquons au problème de déséquilibre de classe dans le domaine de la détection du bégaiement via un schéma de multibranchement et en pondérant la contribution des classes dans la fonction de perte globale, ce qui entraîne une amélioration considérable des classes de bégaiement sur l&#039;ensemble de données SEP-28k par rapport à la ligne de base (StutterNet). Pour lutter contre la rareté des données, nous étudions l&#039;efficacité de l&#039;augmentation des données en plus d&#039;un programme de formation à plusieurs branches. La formation augmentée surpasse le StutterNet multibranchement (propre) d&#039;une marge relative de 4,18% dans la score F1 macro (F1). En outre, nous proposons un StutterNet multi-contextuel, qui exploite différents contextes de la parole bégayée, résultant en une amélioration globale de 4,48% en F 1 par rapport au StutterNet multibranchement s&#039;appuyant sur un contexte unique. Enfin, nous avons montré que l&#039;application de l&#039;augmentation des données dans le scénario inter-corpus peut améliorer la performance globale de la détection du bégaiement d&#039;une marge relative de 13,23% en F1 par rapport à la formation propre.","2","2023-06-15 23:33:21","2023-06-15 23:54:20","15685",,"2","2","4","4","1","3",NULL,,"2023-06-15 11:32:54","0"
"3986142","Recent work on predicting category structure with distributional models, using either static word embeddings (Heyman and Heyman, 2019) or contextualized language models (CLMs) (Misra et al., 2021), report low correlations with human ratings, thus calling into question their plausibility as models of human semantic memory. In this work, we revisit this question testing a wider array of methods for probing CLMs for predicting typicality scores. Our experiments, using BERT (Devlin et al., 2018), show the importance of using the right type of CLM probes, as our best BERT-based typicality prediction methods substantially improve over previous works. Second, our results highlight the importance of polysemy in this task: our best results are obtained when using a disambiguation mechanism. Finally, additional experiments reveal that Information Contentbased WordNet (Miller, 1995), also endowed with disambiguation, match the performance of the best BERT-based method, and in fact capture complementary information, which can be combined with BERT to achieve enhanced typicality predictions.","Exploring Category Structure with Contextual Language Models and Lexical Semantic Networks","Joseph Renner, Pascal Denis, Rémi Gilleron, Angèle Brunellière","3792","Exploration de la structure des catégories à l'aide de modèles linguistiques contextuels et de réseaux sémantiques lexicaux

Des travaux récents sur la prédiction de la structure des catégories avec des modèles distributionnels, utilisant soit des enchâssements de mots statiques (Heyman et Heyman, 2019), soit des modèles de langage contextualisés (CLM) (Misra et al., 2021), font état de faibles corrélations avec les évaluations humaines, remettant ainsi en question leur plausibilité en tant que modèles de la mémoire sémantique humaine. Dans ce travail, nous revisitons cette question en testant un plus large éventail de méthodes pour sonder les CLMs afin de prédire les scores de typicalité. Nos expériences, utilisant BERT (Devlin et al., 2018), montrent l'importance d'utiliser le bon type de sondes CLM, car nos meilleures méthodes de prédiction de typicalité basées sur BERT s'améliorent considérablement par rapport aux travaux précédents. Deuxièmement, nos résultats soulignent l'importance de la polysémie dans cette tâche : nos meilleurs résultats sont obtenus lors de l'utilisation d'un mécanisme de désambiguïsation. Enfin, des expériences supplémentaires révèlent que le WordNet basé sur le contenu de l'information (Miller, 1995), également doté d'un mécanisme de désambiguïsation, atteint les mêmes performances que la meilleure méthode basée sur l'ERTB et capture en fait des informations complémentaires, qui peuvent être combinées avec l'ERTB pour améliorer les prédictions de typicalité.","18608","3","153","Exploration de la structure syntaxique à l&#039;aide de modèles de langue contextuels et de réseaux sémantiques lexicaux

Des travaux récents sur la prédiction de la structure syntaxique avec des modèles distributionnels, utilisant soit des plongements lexicaux statiques (Heyman et Heyman, 2019), soit des modèles de langue contextualisés (CLM, contextualized language models) (Misra et al., 2021), font état de faibles corrélations avec les évaluations humaines, remettant ainsi en question leur plausibilité en tant que modèles de la mémoire sémantique humaine. Dans cet article, nous revisitons cette question en testant un plus large éventail de méthodes pour sonder les CLM afin de prédire les scores de typicalité. Nos expériences utilisant BERT (Devlin et al., 2018), montrent l&#039;importance d&#039;utiliser le bon type de sondes CLM, car nos meilleures méthodes de prédiction de typicalité s&#039;appuyant sur BERT s&#039;améliorent considérablement par rapport aux travaux précédents. Deuxièmement, nos résultats soulignent l&#039;importance de la polysémie dans cette tâche : nos meilleurs résultats sont obtenus lors de l&#039;utilisation d&#039;un mécanisme de désambiguïsation. Enfin, des expériences supplémentaires révèlent que le WordNet basé sur le contenu de l&#039;information (Miller, 1995), également doté d&#039;un mécanisme de désambiguïsation, atteint les mêmes performances que la meilleure méthode s&#039;appuyant sur BERT et capture en fait des informations complémentaires, qui peuvent être combinées avec BERT pour améliorer les prédictions de typicalité.","2","2023-06-16 18:06:07","2023-06-16 18:22:07","18608",,"3","2","4","2","2","4",NULL,,"2023-06-16 06:05:56","0"
"3862222","This work explores the use of constant-Q transform based modulation spectral features (CQT-MSF) for speech emotion recognition (SER). The human perception and analysis of sound comprise of two important cognitive parts: early auditory analysis and cortex-based processing. The early auditory analysis considers spectrogram-based representation whereas cortex-based analysis includes extraction of temporal modulations from the spectrogram. This temporal modulation representation of spectrogram is called modulation spectral feature (MSF). As the constant-Q transform (CQT) provides higher resolution at emotion salient low-frequency regions of speech, we find that CQTbased spectrogram, together with its temporal modulations, provides a representation enriched with emotion-specific information. We argue that CQT-MSF when used with a 2-dimensional convolutional network can provide a time-shift invariant and deformation insensitive representation for SER. Our results show that CQT-MSF outperforms standard mel-scale based spectrogram and its modulation features on two popular SER databases, Berlin EmoDB and RAVDESS. We also show that our proposed feature outperforms the shift and deformation invariant scattering transform coefficients, hence, showing the importance of joint hand-crafted and self-learned feature extraction instead of reliance on complete hand-crafted features. Finally, we perform Grad-CAM analysis to visually inspect the contribution of constant-Q modulation features over SER.","Modulation spectral features for speech emotion recognition using deep neural networks","Premjeet Singh, Md Sahidullah, Goutam Saha","4113","Fonctions spectrales de modulation pour la reconnaissance des émotions vocales à l’aide de réseaux neuronaux profonds

Ce travail explore l’utilisation de fonctions spectrales de modulation à base de transformation constante Q (CQT-MSF) pour la reconnaissance des émotions vocales (SER). La perception et l’analyse humaines du son se composent de deux parties cognitives importantes: analyse auditive précoce et traitement à base de cortex. L’analyse auditive précoce prend en compte la représentation basée sur le spectrogramme tandis que l’analyse basée sur le cortex comprend l’extraction des modulations temporelles du spectrogramme. Cette représentation de modulation temporelle du spectrogramme est appelée fonction spectrale de modulation (MSF). Comme la transformation constante-Q (CQT) fournit une résolution plus élevée aux régions de la parole à basse fréquence saillantes de l’émotion, nous constatons que le spectrogramme basé sur CQT, ainsi que ses modulations temporelles, fournissent une représentation enrichie d’informations spécifiques aux émotions. Nous soutenons que le CQT-MSF lorsqu’il est utilisé avec un réseau convolutionnel à 2 dimensions peut fournir une représentation invariante et insensible à la déformation pour SER. Nos résultats montrent que CQT-MSF surpasse le spectrogramme standard basé sur la mel et ses caractéristiques de modulation sur deux bases de données SER populaires, Berlin eMoDB et RAVDESS. Nous montrons également que notre caractéristique proposée surpasse les coefficients de transformation de la dispersion invariante du décalage et de la déformation, ce qui montre l’importance de l’extraction de caractéristiques fabriquées à la main et auto-apprenées à la main au lieu de dépendre de caractéristiques entièrement fabriquées à la main. Enfin, nous effectuons une analyse Grad-CAM pour inspecter visuellement la contribution des fonctionnalités de modulation constante-Q sur SER.","13071","4","154","Fonctions spectrales de modulation pour la reconnaissance des émotions dans la voix à l’aide de réseaux neuronaux profonds

Ce travail explore l’utilisation de fonctions spectrales de modulation à base de transformation constante Q (CQT-MSF) pour la reconnaissance des émotions dans la voix. La perception et l’analyse humaines du son se composent de deux parties cognitives importantes : analyse auditive précoce et traitement à base de cortex. L’analyse auditive précoce prend en compte la représentation basée sur le spectrogramme tandis que l’analyse basée sur le cortex comprend l’extraction des modulations temporelles du spectrogramme. Cette représentation de modulation temporelle du spectrogramme est appelée fonction spectrale de modulation (MSF). Comme la transformation constante-Q (CQT) fournit une résolution plus élevée aux régions de la parole à basse fréquence saillantes de l’émotion, nous constatons que le spectrogramme basé sur CQT, ainsi que ses modulations temporelles, fournissent une représentation enrichie d’informations spécifiques aux émotions. Nous affirmons que la CQT-MSF, lorsqu’il est utilisé avec un réseau neuronal convolutif à deux dimensions, peut fournir une représentation invariante et insensible à la déformation pour SER. Nos résultats montrent que CQT-MSF surpasse le spectrogramme standard basé sur la mel et ses caractéristiques de modulation sur deux bases de données SER populaires, Berlin eMoDB et RAVDESS. Nous montrons également que notre caractéristique proposée surpasse les coefficients de transformation de la dispersion invariante du décalage et de la déformation, ce qui montre l’importance de l’extraction de caractéristiques fabriquées et apprises à la main au lieu de dépendre de caractéristiques entièrement fabriquées à la main. Enfin, nous effectuons une analyse Grad-CAM pour inspecter visuellement la contribution des fonctionnalités de modulation constante-Q sur la reconnaissance des émotions dans la voix.","2","2023-06-16 18:23:15","2023-06-16 18:33:33","13071",,"2","3","4","2","3","3",NULL,,"2023-06-16 06:22:13","0"
"3835780","Human facial expressions change dynamically, so their recognition / analysis should be conducted by accounting for the temporal evolution of face deformations either in 2D or 3D. While abundant 2D video data do exist, this is not the case in 3D, where few 3D dynamic (4D) datasets were released for public use. The negative consequence of this scarcity of data is amplified by current deep learning based-methods for facial expression analysis that require large quantities of variegate samples to be effectively trained. With the aim of smoothing such limitations, in this paper we propose a large dataset, named Florence 4D, composed of dynamic sequences of 3D face models, where a combination of synthetic and real identities exhibit an unprecedented variety of 4D facial expressions, with variations that include the classical neutralapex transition, but generalize to expression-to-expression. All these characteristics are not exposed by any of the existing 4D datasets and they cannot even be obtained by combining more than one dataset. We strongly believe that making such a data corpora publicly available to the community will allow designing and experimenting new applications that were not possible to investigate till now. To show at some extent the difficulty of our data in terms of different identities and varying expressions, we also report a baseline experimentation on the proposed dataset that can be used as baseline.","The Florence 4D Facial Expression Dataset","Filippo Principi, Stefano Berrettini, Claudio Ferrari, Naima Otberdout, Mohamed Daoudi, Alberto del Bimbo","4192","Le jeu de données d’expression faciale Florence 4D

Les expressions faciales humaines changent dynamiquement, de sorte que leur reconnaissance/analyse doit être effectuée en tenant compte de l’évolution temporelle des déformations faciales en 2D ou en 3D. Bien que des données vidéo 2D abondantes existent, ce n’est pas le cas en 3D, où peu d’ensembles de données dynamiques 3D (4D) ont été publiés pour un usage public. La conséquence négative de cette rareté de données est amplifiée par les méthodes basées sur l’apprentissage profond pour l’analyse de l’expression faciale qui nécessitent une formation efficace de grandes quantités d’échantillons de variétégate. Dans le but de lisser de telles limitations, nous proposons dans cet article un grand ensemble de données, nommé Florence 4D, composé de séquences dynamiques de modèles de visage 3D, où une combinaison d’identités synthétiques et réelles présentent une variété sans précédent d’expressions faciales 4D, avec des variations qui incluent la transition neutralapex classique, mais se généralisent à l’expression à l’expression. Toutes ces caractéristiques ne sont exposées par aucun des ensembles de données 4D existants et elles ne peuvent même pas être obtenues en combinant plus d’un ensemble de données. Nous croyons fermement que la mise à la disposition du public d’un tel corpus de données permettra de concevoir et d’expérimenter de nouvelles applications qui n’ont pas été possibles jusqu’à présent. Pour montrer dans une certaine mesure la difficulté de nos données en termes d’identités différentes et d’expressions variables, nous rapportons également une expérimentation de base sur l’ensemble de données proposé qui peut être utilisé comme base de référence.","13150","4","172","Le jeu de données d’expression faciale Florence 4D

Les expressions faciales humaines changent de manière dynamique, de sorte que leur reconnaissance/analyse doit être effectuée en tenant compte de l’évolution temporelle des déformations faciales en 2D ou en 3D. Bien que des données vidéo 2D abondantes existent, ce n’est pas le cas en 3D, où peu d’ensembles de données dynamiques 3D (4D) ont été publiés pour un usage public. La conséquence négative de cette rareté de données est amplifiée par les méthodes fondées sur l’apprentissage profond pour l&#039;analyse automatique des expressions faciales qui nécessitent une formation efficace de grandes quantités d’échantillons variés. Dans le but d&#039;estomper ces limitations, nous proposons dans cet article un grand ensemble de données, nommé Florence 4D, composé de séquences dynamiques de modèles de visage 3D, où une combinaison d’identités synthétiques et réelles présentent une variété sans précédent d’expressions faciales 4D, avec des variations qui incluent la transition classique entre un visage neutre et l&#039;apex, mais se généralisent à l’expression à l’expression. Toutes ces caractéristiques ne sont exposées par aucun des ensembles de données 4D existants et elles ne peuvent même pas être obtenues en combinant plusieurs ensembles de données. Nous croyons fermement que la mise à la disposition du public d’un tel corpus de données permettra de concevoir et d’expérimenter de nouvelles applications qui n’ont pas été possibles jusqu’à présent. Pour montrer dans une certaine mesure la difficulté de nos données en termes d’identités différentes et d’expressions variables, nous rapportons également une expérimentation de base sur l’ensemble de données proposé qui peut être utilisé comme base de référence.","2","2023-06-19 17:22:47","2023-06-19 17:34:21","13150",,"4","3","4","3","1","4",NULL,,"2023-06-19 05:22:40","0"
"3957221","In this chapter, we present French Digital Discourse from historical and terminological perspectives and indicate how it is perceived specifically by the general French-speaking public, including a tendency towards protectionism and conservatism of the French language. We then provide a research overview of written language appearing in electronic messages (SMS, Facebook Messenger, Viber, WhatsApp, Twitter…) in the French-speaking world and also briefly suggest methodological approaches for data collection, corpora building and analysis. Furthermore, in order to understand the visible impacts and changes that digital discourse has brought in relation to standardized language, we highlight several Francophone projects as case studies. These include (socio-)linguistic, Natural Language Processing (NLP) and soci(et)al initiatives. Finally, we suggest future research directions for French Digital Discourse.","French digital discourse","Rachel Panckhurst, Louise-Amélie Cougnon, Cédrick Fairon","3861","Discours numérique en français

Dans ce chapitre, nous présentons le discours numérique français d'un point de vue historique et terminologique et indiquons comment il est perçu spécifiquement par le grand public francophone, y compris une tendance au protectionnisme et au conservatisme de la langue française. Nous donnons ensuite un aperçu de la recherche sur le langage écrit apparaissant dans les messages électroniques (SMS, Facebook Messenger, Viber, WhatsApp, Twitter...) dans le monde francophone et suggérons brièvement des approches méthodologiques pour la collecte de données, la constitution de corpus et l'analyse. En outre, afin de comprendre les impacts visibles et les changements que le discours numérique a apportés par rapport à la langue standardisée, nous mettons en évidence plusieurs projets francophones en tant qu'études de cas. Ceux-ci comprennent des initiatives (socio-)linguistiques, de traitement du langage naturel (NLP) et soci(et)ales. Enfin, nous suggérons des orientations de recherche futures pour le discours numérique français.","18677","3","174","Le discours numérique français

Dans ce chapitre, nous présentons le discours numérique français d&#039;un point de vue historique et terminologique et indiquons comment il est perçu spécifiquement par le grand public francophone, y compris une tendance au protectionnisme et au conservatisme de la langue française. Nous donnons ensuite un aperçu de la recherche sur le langage écrit apparaissant dans les messages électroniques (SMS, Facebook Messenger, Viber, WhatsApp, Twitter...) dans le monde francophone et suggérons brièvement des approches méthodologiques pour la collecte de données, la constitution de corpus et l&#039;analyse. En outre, afin de comprendre les impacts visibles et les changements que le discours numérique a apportés par rapport à la langue standardisée, nous mettons en évidence plusieurs projets francophones en tant qu&#039;études de cas. Ceux-ci comprennent des initiatives (socio-)linguistiques, de traitement automatique des langues (TAL) et soci(et)ales. Enfin, nous suggérons des orientations de recherche futures pour le discours numérique français.","2","2023-06-19 17:39:02","2023-06-19 17:44:05","18677",,"1","1","3","1","1","1",NULL,,"2023-06-19 05:38:53","0"
"3937780","Knowledge Bases (KB) are used in many fields, such as business intelligence or user assistance. They aggregate knowledge that can be exploited by computers to help decision making by providing better visualization or predicting new relations. However, their building remains complex for an expert who has to extract and link each new information. In this paper, we describe an entity-centric method for evaluating an end-to-end Knowledge Base Population system. This evaluation is applied to ELROND, a complete system designed as a workflow composed of 4 modules (Named Entity Recognition, Coreference Resolution, Relation Extraction and Entity Linking) and MERIT, a dynamic entity linking model made of a textual encoder to retrieve similar entities and a classifier.","Evaluating and Improving End-to-End Systems for Knowledge Base Population","Maxime Prieur, Cédric Du Mouza, Guillaume Gadek, Bruno Grilheres","3907","Évaluation et amélioration des systèmes de bout en bout pour la population de la base de connaissances

Les bases de connaissances (KB) sont utilisées dans de nombreux domaines, tels que la veille économique ou l'assistance utilisateur. Ils rassemblent des connaissances qui peuvent être exploitées par les ordinateurs pour aider à la prise de décision en fournissant une meilleure visualisation ou la prédiction de nouvelles relations. Cependant, leur construction reste complexe pour un expert qui doit extraire et lier chaque nouvelle information. Dans cet article, nous décrivons une méthode centrée sur l'entité pour évaluer un système de population de base de connaissances de bout en bout. Cette évaluation s'applique à ELROND, un système complet conçu comme un workflow composé de 4 modules (Named Entity Recognition, Corference Resolution, Relation Extraction and Entity Linking) et MERIT, un modèle dynamique de liaison d'entités constitué d'un codeur textuel pour récupérer des entités similaires et d'un classificateur.","15823","6","175","Évaluation et amélioration des systèmes de bout en bout pour le peuplement de base de connaissances

Les bases de connaissances sont utilisées dans de nombreux domaines, tels que la veille économique ou l&#039;assistance utilisateur. Elles rassemblent des connaissances qui peuvent être exploitées par les ordinateurs pour aider à la prise de décision en fournissant une meilleure visualisation ou la prédiction de nouvelles relations. Cependant, leur construction reste complexe pour un expert qui doit extraire et lier chaque nouvelle information. Dans cet article, nous décrivons une méthode centrée sur l&#039;entité pour évaluer un système de peuplement de base de connaissances de bout en bout. Cette évaluation s&#039;applique à ELROND, un système complet conçu comme un flux de travail composé de 4 modules (reconnaissance des entités nommées, résolution des coréférences, extraction de relations et liage d&#039;entités) et MERIT, un modèle dynamique de liage d&#039;entités constitué d&#039;un encodeur textuel pour récupérer des entités similaires et d&#039;un classificateur.","2","2023-06-19 17:49:20","2023-06-19 18:07:29","15823",,"2","4","4","1","1","2",NULL,,"2023-06-19 05:48:40","0"
"4010787","Signal inpainting is the task of restoring degraded or missing samples in a signal. In this paper we address signal inpainting when Fourier magnitudes are observed. We propose a mathematical formulation of the problem that highlights its connection with phase retrieval, and we introduce two methods for solving it. First, we derive an alternating minimization scheme, which shares similarities with the Gerchberg-Saxton algorithm, a classical phase retrieval method. Second, we propose a convex relaxation of the problem, which is inspired by recent approaches that reformulate phase retrieval into a semidefinite program. We assess the potential of these methods for the task of inpainting gaps in speech signals. Our methods exhibit both a high probability of recovering the original signals and robustness to magnitude noise.","Signal Inpainting from Fourier Magnitudes","Louis Bahrman, Marina Krémé, Paul Magron, Antoine Deleforge","3753","Peinture du signal à partir des magnitudes de Fourier

L'inpainting d'un signal consiste à restaurer les échantillons dégradés ou manquants d'un signal. Dans cet article, nous nous intéressons au repeignage de signaux lorsque les magnitudes de Fourier sont observées. Nous proposons une formulation mathématique du problème qui met en évidence son lien avec la récupération de phase, et nous introduisons deux méthodes pour le résoudre. Tout d'abord, nous dérivons un schéma de minimisation alternée, qui présente des similitudes avec l'algorithme de Gerchberg-Saxton, une méthode classique d'extraction de phase. Deuxièmement, nous proposons une relaxation convexe du problème, qui s'inspire d'approches récentes qui reformulent la recherche de phase en un programme semi-défini. Nous évaluons le potentiel de ces méthodes pour la tâche d'inpainting des lacunes dans les signaux vocaux. Nos méthodes présentent à la fois une probabilité élevée de récupération des signaux originaux et une robustesse au bruit de magnitude.","18569","3","180","Inpainting d&#039;un signal à partir des magnitudes de Fourier

L&#039;inpainting d&#039;un signal consiste à restaurer les échantillons dégradés ou manquants de ce signal. Dans cet article, nous nous intéressons à l&#039;inpainting de signaux lorsque les magnitudes de Fourier sont observées. Nous proposons une formulation mathématique du problème qui met en évidence son lien avec la récupération de phase, et nous introduisons deux méthodes pour le résoudre. Tout d&#039;abord, nous dérivons un schéma de minimisation alternée, qui présente des similitudes avec l&#039;algorithme de Gerchberg-Saxton, une méthode classique d&#039;extraction de phase. Deuxièmement, nous proposons une relaxation convexe du problème, qui s&#039;inspire d&#039;approches récentes qui reformulent la recherche de phase en un programme semi-défini. Nous évaluons le potentiel de ces méthodes pour la tâche d&#039;inpainting des pertes dans les signaux vocaux. Nos méthodes présentent à la fois une probabilité élevée de récupération des signaux originaux et une robustesse au bruit de grandeur.","2","2023-06-19 18:20:02","2023-06-19 18:30:40","18569",,"2","2","4","3","1","4",NULL,,"2023-06-19 06:19:56","0"
"3992297","Infants learn their native language(s) at an amazing speed. Before they even talk, their perception adapts to the language(s) they hear. However, the mechanisms responsible for this perceptual attunement still remain unclear. A long tradition in linguistics points to the importance of specialized language mechanisms that would allow us to quickly and effortlessly learn from the language(s) we are exposed to. However, the currently dominant explanation for perceptual attunement posits that infants apply a domain-general learning mechanism consisting in learning statistical regularities from the speech stream they hear, and which may be found in learning across domains and across species. Critically, the feasibil- ity of employing purely domain-general statistical learning mechanisms has only been demonstrated with computational models on unrealistic and simplified input. This paper presents the first attempt to study perceptual attunement with 2,000 hours of ecological child-centered recordings in American English and Metropolitan French. We show that, when applied on ecologically-valid data, generic learning mecha- nisms develop a language-relevant perceptual space but fail to show evidence for perceptual attunement. It is only when supplemented with domain-specific audio filtering and augmentation mechanisms that computational models show a significant attunement to the language they have been exposed to. Hence, we conclude that, when learning from ecological audio, domain-specific mechanisms may be necessary to guide early language learning in the wild even if the learning itself is done through generic mechanisms. We anticipate our work to be a starting point for ecologically-valid computational models of perceptual attunement in other domains and species.","Statistical learning models of early phonetic acquisition struggle with child-centered audio data","Marvin Lavechin, Maureen Seyssel, Marianne Métais, Florian Metze, Abdelrahman Mohamed, Hervé Bredin, Emmanuel Dupoux, Alejandrina Cristia","356","Les modèles d’apprentissage statistique de la lutte d’acquisition phonétique précoce avec des données audio centrées sur l’enfant

Les nourrissons apprennent leur(s) langue(s) maternelle(s) à une vitesse incroyable. Avant même de parler, leur perception s’adapte au(x) langage(s) qu’ils entendent. Cependant, les mécanismes responsables de cette attitude perceptuelle restent encore flous. Une longue tradition linguistique souligne l’importance de mécanismes linguistiques spécialisés qui nous permettraient d’apprendre rapidement et sans effort de la ou des langues auxquelles nous sommes exposés. Cependant, l’explication actuellement dominante de l’attitude perceptuelle postule que les nourrissons appliquent un mécanisme d’apprentissage général du domaine consistant à apprendre des régularités statistiques à partir du flux de parole qu’ils entendent, et que l’on peut trouver dans l’apprentissage à travers les domaines et entre les espèces. De manière critique, le succès de l’utilisation de mécanismes d’apprentissage statistique purement généraux n’a été démontré qu’avec des modèles de calcul sur des intrants irréalistes et simplifiés. Cet article présente la première tentative d’étude de l’attitude perceptuelle avec 2 000 heures d’enregistrements écologiques centrés sur l’enfant en anglais américain et en français métropolitain. Nous montrons que, lorsqu’ils sont appliqués sur des données écologiquement valides, les mécha-nismes d’apprentissage générique développent un espace perceptuel pertinent au langage, mais ne montrent pas de preuves d’une prise en compte perceptuelle. Ce n’est que lorsqu’il est complété par des mécanismes de filtrage et d’augmentation audio spécifiques à un domaine que les modèles informatiques montrent une adaptation significative au langage auquel ils ont été exposés. Par conséquent, nous concluons que, lors de l’apprentissage de l’audio écologique, des mécanismes spécifiques à un domaine peuvent être nécessaires pour guider l’apprentissage précoce des langues dans la nature même si l’apprentissage lui-même se fait par le biais de mécanismes génériques. Nous prévoyons que notre travail sera un point de départ pour des modèles informatiques écologiquement valides d’adaptation perceptuelle dans d’autres domaines et espèces.","3110","4","185","Modèles d’apprentissage statistique de troubles de l&#039;acquisition du langage oral avec des données audio centrées sur l’enfant

Les nourrissons apprennent leur(s) langue(s) maternelle(s) à une vitesse incroyable. Avant même de parler, leur perception s’adapte au(x) langage(s) qu’ils entendent. Cependant, les mécanismes responsables de cette harmonisation perceptuelle restent encore flous. Une longue tradition linguistique souligne l’importance de mécanismes linguistiques spécialisés qui nous permettraient d’apprendre rapidement et sans effort de la ou des langues auxquelles nous sommes exposés. Cependant, la théorie dominante actuelle de l’harmonisation perceptuelle avance que les nourrissons appliquent un mécanisme d’apprentissage général du domaine consistant à apprendre des régularités statistiques à partir du flux de parole qu’ils entendent, et que l’on peut trouver dans l’apprentissage à travers les domaines et entre les espèces. De manière critique, le succès de l’utilisation de mécanismes d’apprentissage statistique purement généraux n’a été démontré qu’avec des modèles de calcul sur des intrants irréalistes et simplifiés. Cet article présente la première tentative d’étude de l’attitude perceptuelle avec 2 000 heures d’enregistrements écologiques centrés sur l’enfant en anglais américain et en français métropolitain. Nous montrons que, lorsqu’ils sont appliqués sur des données respectant la validité écologique, les mécanismes d’apprentissage générique développent un espace perceptuel pertinent au langage, mais ne montrent pas de preuves d’une prise en compte perceptuelle. Ce n’est que lorsqu’il est complété par des mécanismes de filtrage et d’augmentation audio spécifiques à un domaine que les modèles informatiques montrent une adaptation significative au langage auquel ils ont été exposés. Par conséquent, nous concluons que, lors de l’acquisition par l’audio écologique, des mécanismes spécifiques à un domaine peuvent être nécessaires pour guider l’acquisition naturel précoce du langage, même si l’apprentissage lui-même se fait par le biais de mécanismes génériques. Nous pensons que notre travail sera un point de départ pour des modèles informatiques d’adaptation perceptuelle écologiquement valides dans d’autres domaines et espèces.","2","2023-06-21 16:06:30","2023-06-21 16:30:36","3110",,"4","2","4","3","4","3",NULL,,"2023-06-21 04:06:04","0"
"3934286","We present preliminary results about Legistix, a tool we are developing to automatically consolidate the French and European law. Legistix is based both on regular expressions used in several compound grammars, similar to the successive passes of a compiler, and on a new specialized language of functional type, allowing to describe the changes applied to the texts. Instead of creating manually a full consolidated version of a text at each modification date, Legistix generates automatically programs from legal documents written in natural language to automatically create the consolidated versions.","Towards an Automatic Consolidation of French Law","Georges-André Silber","3923","Vers une consolidation automatique du droit français

Nous présentons les premiers résultats concernant Legistix, un outil que nous développons pour consolider automatiquement le droit français et européen. Legistix est basé à la fois sur des expressions régulières utilisées dans plusieurs grammaires composées, similaires aux passes successives d'un compilateur, et sur un nouveau langage spécialisé de type fonctionnel, permettant de décrire les changements appliqués aux textes. Au lieu de créer manuellement une version consolidée complète d'un texte à chaque date de modification, Legistix génère automatiquement des programmes à partir de documents juridiques écrits en langage naturel pour créer automatiquement les versions consolidées.","15839","6","186","Vers une consolidation automatique du droit français

Nous présentons les premiers résultats concernant Legistix, un outil que nous développons pour consolider automatiquement le droit français et européen. Legistix est basé à la fois sur des expressions régulières utilisées dans plusieurs grammaires composées, similaires aux passes successives d&#039;un compilateur, et sur un nouveau langage spécialisé de type fonctionnel, permettant de décrire les changements appliqués aux textes. Au lieu de créer manuellement une version consolidée complète d&#039;un texte à chaque date de modification, Legistix génère automatiquement des programmes à partir de documents juridiques écrits en langage naturel pour créer automatiquement les versions consolidées.","2","2023-06-21 21:45:46","2023-06-21 21:49:41","15839",,"1","1","1","1","1","1",NULL,,"2023-06-21 09:45:40","0"
"3970122","Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.","Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning","Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer","3840","L'apprentissage par renforcement en ligne pour l'ancrage de grands modèles linguistiques dans des environnements interactifs

Des travaux récents ont permis d'exploiter avec succès les capacités des grands modèles de langage (LLM) à capturer des connaissances abstraites sur la physique du monde afin de résoudre des problèmes de prise de décision. Cependant, l'alignement entre les connaissances des LLM et l'environnement peut être erroné et limiter la compétence fonctionnelle en raison d'un manque d'ancrage. Dans cet article, nous étudions une approche permettant d'atteindre cet alignement grâce à l'ancrage fonctionnel : nous considérons un agent utilisant un LLM comme une politique qui est progressivement mise à jour au fur et à mesure que l'agent interagit avec l'environnement, en tirant parti de l'apprentissage par renforcement en ligne pour améliorer ses performances en vue de résoudre des problèmes. À l'aide d'un environnement textuel interactif conçu pour étudier les formes supérieures d'ancrage fonctionnel et d'un ensemble de tâches spatiales et de navigation, nous étudions plusieurs questions scientifiques : 1) Les LLM peuvent-ils améliorer l'efficacité de l'échantillon pour l'apprentissage en ligne de diverses tâches RL ? 2) Comment peuvent-ils stimuler différentes formes de généralisation ? 3) Quel est l'impact de l'apprentissage en ligne ? Nous étudions ces questions en fondant fonctionnellement plusieurs variantes (taille, architecture) de FLAN-T5.","18656","3","187","Apprentissage par renforcement en ligne pour l&#039;ancrage de grands modèles de langage dans des environnements interactifs

Des travaux récents ont permis d&#039;exploiter avec succès les capacités des grands modèles de langage (LLM) à capturer des connaissances abstraites sur la physique du monde afin de résoudre des problèmes de prise de décision. Cependant, l&#039;alignement entre les connaissances des LLM et l&#039;environnement peut être erroné et limiter la compétence fonctionnelle en raison d&#039;un manque d&#039;ancrage. Dans cet article, nous étudions une approche permettant d&#039;atteindre cet alignement grâce à l&#039;ancrage fonctionnel : nous considérons un agent utilisant un LLM comme une politique qui est progressivement mise à jour au fur et à mesure que l&#039;agent interagit avec l&#039;environnement, en tirant parti de l&#039;apprentissage par renforcement en ligne pour améliorer ses performances en vue de résoudre des problèmes. À l&#039;aide d&#039;un environnement textuel interactif conçu pour étudier les formes supérieures d&#039;ancrage fonctionnel et d&#039;un ensemble de tâches spatiales et de navigation, nous étudions plusieurs questions scientifiques : 1) Les LLM peuvent-ils améliorer l&#039;efficacité de l&#039;échantillon pour l&#039;apprentissage par renforcement en ligne de diverses tâches ? 2) Comment peuvent-ils stimuler différentes formes de généralisation ? 3) Quel est l&#039;impact de l&#039;apprentissage en ligne ? Nous étudions ces questions en ancrant fonctionnellement plusieurs variantes (taille, architecture) de FLAN-T5.","2","2023-06-21 21:50:00","2023-06-21 22:01:05","18656",,"2","1","2","2","1","4",NULL,,"2023-06-21 09:49:50","0"
"4009378","Senior online communities (SOCs) have become an important venue for older people to seek support and exchange information. While online community engagement has been well studied in the existing literature, few studies have explored how older adults behave in online communities.Therefore, drawing upon signaling theory, this study aims to investigate how different content-related and social-related signals influence users’ post replying behavior (i.e., reply to another user’s post) in SOCs. We collected 7486 health-related posts and 71,859 comments from one ofthe most popular Chinese SOCs, Keai (https://www.keai99.com). Information signals in the posts were operationalized using different techniques such as text mining and social network analysis. Results from negative binomial regression indicated that content-related signals (posts’ topic andlength) and social-related signals (authors’ position and centrality) were related to replying behavior. In addition, we revealed some differences between the effects of these signals on informational replies and emotional replies. More specifically, compared to posts mentioningtraditional Chinese medicine, posts mentioning western medicine received more informational replies, but less emotional replies. Original posts triggered more informational replies, whereas shared posts attracted more emotional replies. Average reply length was positively related toinformational replies, but negatively related to emotional replies. Considering the important role of SOCs in satisfying older adults’ social and informational needs, future research is needed to promote user social engagement in SOCs, thereby maintaining their sustainability.","Factors influencing users’ post replying behavior in a senior online community: An empirical investigation","Yuxing Qian, Zhenni Ni, Han Zheng, Zhenghao Liu, Feicheng Ma","3756","Facteurs influençant le comportement post-réponse des utilisateurs dans une communauté en ligne senior : Une étude empirique

Les communautés de seniors en ligne (SOCs) sont devenues un lieu important pour les personnes âgées pour rechercher du soutien et échanger des informations. Bien que l’engagement communautaire en ligne ait été bien étudié dans la documentation existante, peu d’études ont exploré la façon dont les personnes âgées se comportent dans les communautés en ligne. Par conséquent, en s’appuyant sur la théorie de la signalisation, cette étude vise à examiner comment différents signaux liés au contenu et sociaux influencent le comportement des utilisateurs après avoir répondu (c.-à-d. répondre à un message d’un autre utilisateur) dans les SOC. Nous avons collecté 7486 articles liés à la santé et 71 859 commentaires de l'un des SOC chinois les plus populaires, Keai (https://www.keai99.com). Les signaux d'information dans les postes ont été opérationnalisés en utilisant différentes techniques telles que l'exploration de texte et l'analyse des réseaux sociaux. Les résultats de régression binomiale négative indiquent que les signaux liés au contenu (sujet et longueur des articles) et les signaux sociaux (position et centralité des auteurs) sont liés au comportement de réponse. De plus, nous avons révélé des différences entre les effets de ces signaux sur les réponses informationnelles et émotionnelles. Plus précisément, par rapport aux billets mentionnant la médecine traditionnelle chinoise, les billets mentionnant la médecine occidentale ont reçu plus de réponses informatives, mais moins émotionnelles. Les billets originaux ont suscité plus de réponses informatives, tandis que les billets partagés ont suscité plus de réponses émotionnelles. La longueur moyenne des réponses était positivement liée aux réponses informationnelles, mais négativement liée aux réponses émotionnelles. Compte tenu du rôle important des SOC pour satisfaire les besoins sociaux et informationnels des personnes âgées, des recherches futures sont nécessaires pour promouvoir l’engagement social des utilisateurs dans les SOC, préservant ainsi leur durabilité.","15672","6","188","Etude empirique des facteurs influençant le comportement de réponse aux posts des utilisateurs dans une communauté de seniors en ligne

Les communautés de seniors en ligne (SOC, senior online communities) sont devenues un lieu important pour les personnes âgées pour rechercher du soutien et échanger des informations. Bien que l’engagement communautaire en ligne ait été bien étudié dans la documentation existante, peu d’études ont exploré la façon dont les personnes âgées se comportent dans les communautés en ligne. Par conséquent, en s’appuyant sur la théorie du signal, cette étude vise à examiner comment différents signaux liés au contenu et sociaux influencent le comportement de réponse aux posts des utilisateurs (c.-à-d. répondre à un message d’un autre utilisateur) dans les SOC. Nous avons collecté 7486 articles liés à la santé et 71 859 commentaires de l&#039;une des SOC chinoises les plus populaires, Keai (https://www.keai99.com). Les signaux d&#039;information dans les posts ont été opérationnalisés en utilisant différentes techniques telles que l&#039;exploration de texte et l&#039;analyse des réseaux sociaux. Les résultats de régression binomiale négative indiquent que les signaux liés au contenu (sujet et longueur des articles) et les signaux sociaux (position et centralité des auteurs) sont liés au comportement de réponse. De plus, nous avons révélé des différences entre les effets de ces signaux sur les réponses informationnelles et émotionnelles. Plus précisément, par rapport aux posts mentionnant la médecine traditionnelle chinoise, les posts mentionnant la médecine occidentale ont reçu plus de réponses informatives, mais moins émotionnelles. Les posts originaux ont suscité plus de réponses informatives, tandis que les posts partagés ont suscité plus de réponses émotionnelles. La longueur moyenne des réponses était positivement liée aux réponses informationnelles, mais négativement liée aux réponses émotionnelles. Compte tenu du rôle important des SOC pour satisfaire les besoins sociaux et informationnels des personnes âgées, des recherches futures sont nécessaires pour promouvoir l’engagement social des utilisateurs dans les SOC, préservant ainsi leur durabilité.","2","2023-06-22 12:21:10","2023-06-22 12:38:03","15672",,"1","1","4","2","1","4",NULL,,"2023-06-22 12:21:01","0"
"3964804","In the last decade, Deep neural networks (DNNs) have been proven to outperform conventional machine learning models in supervised learning tasks. Most of these models are typically optimized by minimizing the well-known Cross-Entropy objective function. The latter, however, has a number of drawbacks, including poor margins and instability. Taking inspiration from the recent selfsupervised Contrastive representation learning approaches, we introduce Supervised Contrastive learning framework for Textual representations (SuperConText) to address those issues. We pretrain a neural network by minimizing a novel fully-supervised contrastive loss. The goal is to increase both inter-class separability and intra-class compactness of the embeddings in the latent space. Examples belonging to the same class are regarded as positive pairs, while examples belonging to different classes are considered negatives. Further, we propose a simple yet effective method for selecting hard negatives during the training phase. In extensive series of experiments, we study the impact of a number of parameters on the quality of the learned representations (e.g. the batch size). Simulation results show that the proposed solution outperforms several competing approaches on various large-scale text classification benchmarks without requiring specialized architectures, data augmentations, memory banks, or additional unsupervised data. For instance, we achieved top-1 accuracy of 61.94% on the Amazon-F dataset, which is 3.54% above the best result obtained when using the cross-entropy with the same model architecture.","SuperConText: Supervised Contrastive Learning Framework for Textual representations","Youness Moukafih, Nada Sbihi, Mounir Ghogho, Kamel Smaïli","3850","SuperConText : Cadre d'apprentissage contrastif supervisé pour les représentations textuelles

Au cours de la dernière décennie, il a été prouvé que les réseaux neuronaux profonds (DNN) étaient plus performants que les modèles d'apprentissage automatique conventionnels dans les tâches d'apprentissage supervisé. La plupart de ces modèles sont généralement optimisés en minimisant la fonction objective bien connue de l'entropie croisée. Cette dernière présente toutefois un certain nombre d'inconvénients, notamment de faibles marges et une certaine instabilité. En s'inspirant des approches récentes d'apprentissage auto-supervisé de représentations contrastives, nous introduisons un cadre d'apprentissage contrastif supervisé pour les représentations textuelles (SuperConText) afin de résoudre ces problèmes. Nous pré-entraînons un réseau neuronal en minimisant une nouvelle perte contrastive entièrement supervisée. L'objectif est d'augmenter à la fois la séparabilité inter-classes et la compacité intra-classe des encastrements dans l'espace latent. Les exemples appartenant à la même classe sont considérés comme des paires positives, tandis que les exemples appartenant à des classes différentes sont considérés comme des paires négatives. En outre, nous proposons une méthode simple mais efficace pour sélectionner les négatifs durs pendant la phase d'apprentissage. Dans une série d'expériences approfondies, nous étudions l'impact d'un certain nombre de paramètres sur la qualité des représentations apprises (par exemple, la taille du lot). Les résultats de la simulation montrent que la solution proposée surpasse plusieurs approches concurrentes sur divers benchmarks de classification de textes à grande échelle sans nécessiter d'architectures spécialisées, d'augmentations de données, de banques de mémoire ou de données non supervisées supplémentaires. Par exemple, nous avons obtenu une précision de 61,94 % sur l'ensemble de données Amazon-F, soit 3,54 % de plus que le meilleur résultat obtenu en utilisant l'entropie croisée avec la même architecture de modèle.","18666","3","199","SuperConText : Cadre d&#039;apprentissage contrastif supervisé pour les représentations textuelles

Au cours de la dernière décennie, il a été prouvé que les réseaux de neurones profonds étaient plus performants que les modèles d&#039;apprentissage automatique conventionnels dans les tâches d&#039;apprentissage supervisé. La plupart de ces modèles sont généralement optimisés en minimisant la fonction objective bien connue de l&#039;entropie croisée. Cette dernière présente toutefois un certain nombre d&#039;inconvénients, notamment de faibles marges et une certaine instabilité. En s&#039;inspirant des approches récentes d&#039;apprentissage auto-supervisé de représentations contrastives, nous introduisons un cadre d&#039;apprentissage contrastif supervisé pour les représentations textuelles (SuperConText) afin de résoudre ces problèmes. Nous pré-entraînons un réseau neuronal en minimisant une nouvelle perte contrastive entièrement supervisée. L&#039;objectif est d&#039;augmenter à la fois la séparabilité interclasse et la compacité intraclasse des plongements dans l&#039;espace latent. Les exemples appartenant à la même classe sont considérés comme des paires positives, tandis que les exemples appartenant à des classes différentes sont considérés comme des paires négatives. En outre, nous proposons une méthode simple mais efficace pour sélectionner les négatifs durs pendant la phase d&#039;apprentissage. Dans une série d&#039;expériences approfondies, nous étudions l&#039;impact d&#039;un certain nombre de paramètres sur la qualité des représentations apprises (par exemple, la taille du lot). Les résultats de la simulation montrent que la solution proposée surpasse plusieurs approches concurrentes sur divers tests de performance de classification de textes à grande échelle sans nécessiter d&#039;architectures spécialisées, d&#039;augmentations de données, de banques de mémoire ou de données non supervisées supplémentaires. Par exemple, nous avons obtenu une précision de 61,94 % sur l&#039;ensemble de données Amazon-F, soit 3,54 % de plus que le meilleur résultat obtenu en utilisant l&#039;entropie croisée avec la même architecture de modèle.","2","2023-06-23 13:37:59","2023-06-23 13:43:08","18666",,"2","1","3","1","2","1",NULL,,"2023-06-23 01:37:55","0"
"3945796","With the amount of information shared on the internet increasing on a daily basis, we are prone to face more misinformation online. This is especially true on social media websites, where users have good amount of freedom to share their opinion. During the COVID-19 pandemic, numerous conspiracy theories were shared on Twitter. In this ""FakeNews Detection"" task, the goal is to detect COVID-19related conspiracy theories using tweet text and user interaction graph. We tackled this challenge using Transformer-based models (CT-BERT) and node embedding techniques (node2vec) with classification objective models. Our best model obtains a MCC score of 0.719 on the test data.","Detection of COVID-19-Related Conpiracy Theories in Tweets using Transformer-Based Models and Node Embedding Techniques","Youri Peskine, Paolo Papotti, Raphaël Troncy","3883","Détection des théories de la conpiratie liées à la COVID-19 dans les tweets à l’aide de modèles basés sur des transformateurs et de techniques d’intégration de nœuds

Avec l’augmentation quotidienne de la quantité d’informations partagées sur Internet, nous sommes enclins à faire face à davantage de désinformation en ligne. Cela est particulièrement vrai sur les sites de médias sociaux, où les utilisateurs ont une grande liberté de partager leur opinion. Pendant la pandémie de COVID-19, de nombreuses théories du complot ont été partagées sur Twitter. Dans cette tâche «FakeNews Detection», l’objectif est de détecter les théories du complot liées à la COVID-19 en utilisant le texte de tweet et le graphique d’interaction utilisateur. Nous avons relevé ce défi en utilisant des modèles basés sur Transformer (CT-BERT) et des techniques d’intégration de nœuds (node2vec) avec des modèles objectifs de classification. Notre meilleur modèle obtient un score MCC de 0,719 sur les données de test.","12841","4","202","Détection des théories du complot liées au COVID-19 dans les tweets à l’aide de modèles fondés sur Transformer et de techniques de plongement de nœuds

Avec l’augmentation quotidienne de la quantité d’informations partagées sur Internet, nous sommes enclins à faire face à davantage de désinformation en ligne. Cela est particulièrement vrai sur les réseaux sociaux, où les utilisateurs ont une grande liberté de partager leur opinion. Pendant la pandémie de COVID-19, de nombreuses théories du complot ont été partagées sur Twitter. Dans cette tâche « FakeNews Detection », l’objectif est de détecter les théories du complot liées au COVID-19 en utilisant le texte des tweets et le graphique d’interaction de l&#039;utilisateur. Nous avons relevé ce défi en utilisant des modèles basés sur Transformer (CT-BERT) et des techniques de plongement de nœuds (node2vec) avec des modèles objectifs de classification. Notre meilleur modèle obtient un score MCC de 0,719 sur les données de test.","2","2023-06-23 14:06:15","2023-06-23 14:21:54","12841",,"1","3","4","4","1","2",NULL,,"2023-06-23 02:06:10","0"
"3991406","The relationship between New Testament textual criticism (NTTC) and digital humanities (DH) is a successful one, as demonstrated by the analysis of five ERC grants in NTTC and DH, awarded in the last decade. Along with other features of the field, these projects highlight the fast evolution of digital editing of the NT and multilingualism, and the importance of domain centered research infrastructures. One DH approach has yet to be explored in NTTC: the automated recognition of handwritten characters. This chapter demonstrates that NTTC is boosting New Testament studies with access to new, digitized research material, while remaining grounded in a well-established tradition of close reading. It takes advantage of digital methodologies while honoring humanities core-skills.","New Testament Textual Criticism and Digital Humanities","Claire Clivaz","3786","Critique textuelle et humanités numériques du Nouveau Testament

La relation entre la critique textuelle du Nouveau Testament (NTTC) et les humanités numériques (DH) est une réussite, comme le démontre l'analyse de cinq subventions du CER en NTTC et DH, accordées au cours de la dernière décennie. Avec d'autres caractéristiques du domaine, ces projets mettent en évidence l'évolution rapide de l'édition numérique du NT et du multilinguisme, et l'importance d'infrastructures de recherche centrées sur le domaine. Une approche DH reste à explorer en NTTC : la reconnaissance automatique des caractères manuscrits. Ce chapitre démontre que le NTTC stimule les études du Nouveau Testament en donnant accès à de nouveaux documents de recherche numérisés, tout en restant ancré dans une tradition bien établie de lecture attentive. Il tire parti des méthodologies numériques tout en honorant les compétences de base des sciences humaines.","15702","6","203","Critique textuelle du Nouveau Testament et humanités numériques 

La relation entre la critique textuelle du Nouveau Testament (NTTC, New Testament textual criticism) et les humanités numériques (DH, digital humanities) est une réussite, comme le démontre l&#039;analyse de cinq subventions du Conseil européen de la recherche en NTTC et DH, accordées au cours de la dernière décennie. Avec d&#039;autres caractéristiques du domaine, ces projets mettent en évidence l&#039;évolution rapide de l&#039;édition numérique du Nouveau Testament et du multilinguisme, et l&#039;importance d&#039;infrastructures de recherche centrées sur le domaine. Une approche DH reste à explorer en NTTC : la reconnaissance automatique des caractères manuscrits. Ce chapitre démontre que le NTTC stimule les études du Nouveau Testament en donnant accès à de nouveaux documents de recherche numérisés, tout en restant ancré dans une tradition bien établie de lecture attentive. Il tire parti des méthodologies numériques tout en honorant les compétences de base des sciences humaines.","2","2023-06-23 14:24:42","2023-06-23 14:35:15","15702",,"4","1","2","3","2","1",NULL,,"2023-06-23 02:24:02","0"
"3942072","Suppose China has long adhered to the “tolerance and patience” foreign policy to ensure its “calm and low profile” in geopolitics. In that case, the rise of “wolf warrior public diplomacy (WWPD)” is analyzed by outsiders as Beijing’s one of critical strategic options for challenging the existing geopolitical pattern and order in the international public opinion field by exploiting the globalized crisis.Recent scholarly writing has turned its attention to Beijing’s aggressive diplomatic actions on social media. Nevertheless, these exploratory studies are often limited to a specific case or region. This study attempts to theorize the terminology of WWPD from both the perspective of international communication and the Chinese political institutions. We consider Wolf Warrior refers to a well-planned strategic communication option that has been rooted in the CPC’s propagandist doctrine. To defend this idea, first, we will discuss the root of the term WWPD and how the Chinese government has rationalized and legitimated it in institutional discourses. Second, the study will analyze the communication logics of China’s digital WWPD. Third, we aim to interpret the role and position of Chinese diplomats in their WWPD practices by analyzing their narratives and rhetoric during the pandemic time.","Theorizing Wolf Warrior Public Diplomacy: A Renewal or an Innovation of China’s Foreign Propaganda?","Zhao Alexandre Huang","3895","Théoriser la diplomatie publique des guerriers-loups : Un renouvellement ou une innovation de la propagande étrangère de la Chine ?

Supposons que la Chine adhère depuis longtemps à la politique étrangère de ""tolérance et de patience"" pour garantir son ""calme et sa discrétion"" en géopolitique. Dans ce cas, la montée en puissance de la ""wolf warrior public diplomacy (WWPD)"" est analysée par les observateurs extérieurs comme l'une des options stratégiques critiques de Pékin pour remettre en question le modèle et l'ordre géopolitiques existants dans le domaine de l'opinion publique internationale en exploitant la crise mondialisée.Des travaux universitaires récents se sont intéressés aux actions diplomatiques agressives de Pékin sur les médias sociaux. Néanmoins, ces études exploratoires se limitent souvent à un cas ou à une région spécifique. Cette étude tente de théoriser la terminologie du WWPD du point de vue de la communication internationale et des institutions politiques chinoises. Nous considérons que Wolf Warrior fait référence à une option de communication stratégique bien planifiée, ancrée dans la doctrine propagandiste du PCC. Pour défendre cette idée, nous examinerons tout d'abord l'origine du terme WWPD et la manière dont le gouvernement chinois l'a rationalisé et légitimé dans les discours institutionnels. Deuxièmement, l'étude analysera les logiques de communication de la WWPD numérique chinoise. Troisièmement, nous visons à interpréter le rôle et la position des diplomates chinois dans leurs pratiques de la WWPD en analysant leurs récits et leur rhétorique pendant la période de la pandémie.","18711","3","205","Théoriser la diplomatie du loup guerrier : Un renouvellement ou une innovation de la propagande étrangère de la Chine ?

Supposons que la Chine adhère depuis longtemps à la politique étrangère de &quot;tolérance et de patience&quot; pour garantir son &quot;calme et sa discrétion&quot; en géopolitique. Dans ce cas, la montée en puissance de la &quot;diplomatie du loup guerrier&quot;  est analysée par les observateurs extérieurs comme l&#039;une des options stratégiques critiques de Pékin pour remettre en question le modèle et l&#039;ordre géopolitiques existants dans le domaine de l&#039;opinion publique internationale en exploitant la crise mondiale. Des travaux universitaires récents se sont intéressés aux actions diplomatiques agressives de Pékin sur les médias sociaux. Néanmoins, ces études exploratoires se limitent souvent à un cas ou à une région spécifique. Cette étude tente de théoriser la terminologie de la diplomatie du loup guerrier du point de vue de la communication internationale et des institutions politiques chinoises. Nous considérons que cette notion fait référence à une option de communication stratégique bien planifiée, ancrée dans la doctrine propagandiste du PCC. Pour défendre cette idée, nous examinerons tout d&#039;abord l&#039;origine du terme &quot;diplomatie du loup guerrier&quot; et la manière dont le gouvernement chinois l&#039;a rationalisé et légitimé dans les discours institutionnels. Deuxièmement, l&#039;étude analysera les logiques de communication de la diplomatie du loup guerrier numérique chinoise. Troisièmement, nous visons à interpréter le rôle et la position des diplomates chinois dans leurs pratiques de la diplomatie du loup guerrier en analysant leurs récits et leur rhétorique durant la pandémie.","2","2023-06-23 14:56:55","2023-06-23 14:58:54","18711",,"2","3","4","3","3","3",NULL,,"2023-06-23 02:56:53","0"
"4004399","Voice conversion (VC) consists of digitally altering the voice of an individual to manipulate part of its content, primarily its identity, while maintaining the rest unchanged. Research in neural VC has accomplished considerable breakthroughs with the capacity to falsify a voice identity using a small amount of data with a highly realistic rendering. This paper goes beyond voice identity manipulation and presents an original neural architecture that allows the manipulation of voice attributes (e.g., gender and age). The proposed architecture is inspired by the fader network, transferring the same ideas to voice manipulation. The information conveyed by the speech signal is disentangled into interpretative voice attributes by means of minimizing adversarial loss to make the encoded information mutually independent while preserving the capacity to generate a speech signal from the disentangled codes. During inference for voice conversion, the disentangled voice attributes can be manipulated and the speech signal can be generated accordingly. For experimental evaluation, the proposed method is applied to the task of voice gender conversion using the freely available VCTK dataset. Quantitative measurements of mutual information between the variables of speaker identity and speaker gender show that the proposed architecture can learn gender-independent representation of speakers. Additional measurements of speaker recognition indicate that speaker identity can be recognized accurately from the gender-independent representation. Finally, a subjective experiment conducted on the task of voice gender manipulation shows that the proposed architecture can convert voice gender with very high efficiency and good naturalness.","Manipulating Voice Attributes by Adversarial Learning of Structured Disentangled Representations","Laurent Benaroya, Nicolas Obin, Axel Roebel","3762","Manipulation des attributs vocaux par apprentissage adversarial de représentations structurées et démêlées

La conversion vocale (CV) consiste à modifier numériquement la voix d'un individu pour manipuler une partie de son contenu, principalement son identité, tout en conservant le reste inchangé. La recherche sur la VC neuronale a réalisé des avancées considérables avec la capacité de falsifier une identité vocale à l'aide d'une petite quantité de données avec un rendu très réaliste. Cet article va au-delà de la manipulation de l'identité vocale et présente une architecture neuronale originale qui permet de manipuler les attributs de la voix (par exemple, le sexe et l'âge). L'architecture proposée s'inspire du réseau fader et transfère les mêmes idées à la manipulation de la voix. Les informations véhiculées par le signal vocal sont démêlées en attributs vocaux interprétatifs en minimisant la perte contradictoire afin de rendre les informations codées mutuellement indépendantes tout en préservant la capacité de générer un signal vocal à partir des codes démêlés. Pendant l'inférence pour la conversion vocale, les attributs vocaux démêlés peuvent être manipulés et le signal vocal peut être généré en conséquence. Pour l'évaluation expérimentale, la méthode proposée est appliquée à la tâche de conversion du genre de la voix en utilisant l'ensemble de données VCTK librement disponible. Les mesures quantitatives de l'information mutuelle entre les variables de l'identité et du sexe du locuteur montrent que l'architecture proposée peut apprendre une représentation indépendante du sexe des locuteurs. Des mesures supplémentaires de reconnaissance des locuteurs indiquent que l'identité du locuteur peut être reconnue avec précision à partir de la représentation indépendante du genre. Enfin, une expérience subjective menée sur la manipulation du genre de la voix montre que l'architecture proposée peut convertir le genre de la voix avec une très grande efficacité et un bon naturel.","18578","3","207","Manipulation des attributs vocaux par apprentissage antagoniste de représentations structurées et démêlées

La conversion de la voix consiste à modifier numériquement la voix d&#039;un individu pour manipuler une partie de son contenu, principalement son identité, tout en conservant le reste inchangé. La recherche sur la conversion de la voix neuronale a réalisé des avancées considérables avec la capacité de falsifier une identité vocale à l&#039;aide d&#039;une petite quantité de données avec un rendu très réaliste. Cet article va au-delà de la manipulation de l&#039;identité vocale et présente une architecture neuronale originale qui permet de manipuler les attributs de la voix (par exemple, le sexe et l&#039;âge). L&#039;architecture proposée s&#039;inspire du réseau fader et transfère les mêmes idées à la manipulation de la voix. Les informations véhiculées par le signal vocal sont démêlées en attributs vocaux interprétatifs en minimisant la perte contradictoire afin de rendre les informations codées mutuellement indépendantes tout en préservant la capacité de générer un signal vocal à partir des codes démêlés. Pendant l&#039;inférence pour la conversion vocale, les attributs vocaux démêlés peuvent être manipulés et le signal vocal peut être généré en conséquence. Pour l&#039;évaluation expérimentale, la méthode proposée est appliquée à la tâche de conversion du genre de la voix en utilisant l&#039;ensemble de données VCTK librement disponible. Les mesures quantitatives de l&#039;information mutuelle entre les variables de l&#039;identité et du sexe du locuteur montrent que l&#039;architecture proposée peut apprendre une représentation indépendante du sexe des locuteurs. Des mesures supplémentaires de reconnaissance des locuteurs indiquent que l&#039;identité du locuteur peut être reconnue avec précision à partir de la représentation indépendante du genre. Enfin, une expérience subjective menée sur la manipulation du genre de la voix montre que l&#039;architecture proposée peut convertir le genre de la voix de façon très efficace et avec un résultat réaliste.","2","2023-06-25 22:44:14","2023-06-25 23:06:44","18578",,"1","1","4","4","1","1",NULL,,"2023-06-25 10:44:10","0"
"3939456","Analogical proportions are 4-ary relations that read “A is to B as C is to D”. Recent works have highlighted the fact that such relations can support a specific form of inference, called analogical inference. This inference mechanism was empirically proved to be efficient in several reasoning and classification tasks. In the latter case, it relies on the notion of analogy preservation. In this paper, we explore this relation between formal models of analogy and the corresponding classes of analogy preserving functions, and we establish a Galois theory of analogical classifiers. We illustrate the usefulness of this Galois framework over Boolean domains, and we explicitly determine the closed sets of analogical classifiers, i.e., classifiers that are compatible with the analogical inference, for each pair of Boolean analogies.","Galois theory for analogical classifiers","Miguel Couceiro, Erkko Lehtonen","3902","Théorie Galois pour classificateurs analogiques

Les proportions analogiques sont des relations 4-ary qui se lisent «A est à B comme C est à D». Des travaux récents ont mis en évidence le fait que de telles relations peuvent soutenir une forme spécifique d’inférence, appelée inférence analogique. Ce mécanisme d’inférence s’est avéré empiriquement efficace dans plusieurs tâches de raisonnement et de classification. Dans ce dernier cas, elle s’appuie sur la notion de préservation par analogie. Dans cet article, nous explorons cette relation entre les modèles formels d’analogie et les classes correspondantes de préservation des fonctions d’analogie, et nous établissons une théorie galois des classificateurs analogiques. Nous illustrons l’utilité de ce framework Galois sur les domaines booléens, et nous déterminons explicitement les ensembles fermés de classificateurs analogiques, c’est-à-dire des classificateurs compatibles avec l’inférence analogique, pour chaque paire d’analogies booléennes.","12860","4","208","Théorie de Galois pour classificateurs analogiques

Les proportions analogiques sont des relations 4-ary qui se lisent « A est à B comme C est à D ». Des travaux récents ont mis en évidence le fait que de telles relations peuvent soutenir une forme spécifique d’inférence, appelée inférence analogique. Ce mécanisme d’inférence s’est avéré empiriquement efficace dans plusieurs tâches de raisonnement et de classification. Dans ce dernier cas, elle s’appuie sur la notion de préservation par analogie. Dans cet article, nous explorons cette relation entre les modèles formels d’analogie et les classes correspondantes de préservation des fonctions d’analogie, et nous établissons une théorie de Galois des classificateurs analogiques. Nous illustrons l’utilité de ce cadre de Galois sur les domaines booléens, et nous déterminons explicitement les ensembles fermés de classificateurs analogiques, c’est-à-dire des classificateurs compatibles avec l’inférence analogique, pour chaque paire d’analogies booléennes.","2","2023-06-26 10:08:50","2023-06-26 10:16:37","12860",,"1","1","4","2","3","2",NULL,,"2023-06-26 10:05:38","0"
"3957483","The very first moments of co-presence, during which a robot appears to a participant for the first time, are often “off-the-record” in the data collected from human-robot experiments (video recordings, motion tracking, methodology sections, etc.). Yet, this “pre-beginning” phase, well documented in the case of human-human interactions, is not an interactional vacuum: it is where interactional work from participants can take place so that the production of a first speaking turn (like greeting the robot) becomes relevant and expected. We base our analysis on an experiment which replicated the interaction opening delays sometimes observed in laboratory or ""in-the-wild"" human-robot interaction studies – where robots can require time before springing to life after they are in co-presence with a human. Using an ethnomethodological and multimodal conversation analytic methodology (EMCA), we identify which properties of the robot's behavior were oriented to by participants as creating the adequate conditions to produce a first greeting. Our findings highlight the importance of the state in which the robot originally appears to participants: as an immobile object or, instead, as an entity already involved in preexisting activity. Participants’ orientations to the very first behaviors manifested by the robot during this “pre-beginning” phase produced a priori unpredictable sequential trajectories, which configured the timing and the manner in which the robot emerged as a social agent. We suggest that these first instants of co-presence are not peripheral issues with respect to human-robot experiments but should be thought about and designed as an integral part of those.","From Inanimate Object to Agent: Impact of Pre-Beginnings on the Emergence of Greetings with a Robot","Damien Rudaz, Karen Tatarian, Rebecca Stower, Christian Licoppe","3858","De l'objet inanimé à l'agent : Impact des prémices sur l'émergence des salutations avec un robot

Les tout premiers moments de coprésence, au cours desquels un robot apparaît pour la première fois à un participant, sont souvent "" hors champ "" dans les données collectées lors des expérimentations homme-robot (enregistrements vidéo, suivi de mouvement, rubriques méthodologiques, etc.) Pourtant, cette phase de ""pré-début"", bien documentée dans le cas des interactions homme-homme, n'est pas un vide interactionnel : c'est là que le travail interactionnel des participants peut avoir lieu pour que la production d'un premier tour de parole (comme saluer le robot) devienne pertinente et attendue. Nous basons notre analyse sur une expérience qui reproduit les retards d'ouverture de l'interaction parfois observés dans les études d'interaction homme-robot en laboratoire ou ""dans la nature"" - où les robots peuvent avoir besoin de temps avant de s'animer après avoir été en coprésence avec un humain. En utilisant une méthodologie ethnométhodologique et multimodale d'analyse de conversation (EMCA), nous identifions quelles propriétés du comportement du robot ont été orientées par les participants comme créant les conditions adéquates pour produire une première salutation. Nos résultats soulignent l'importance de l'état dans lequel le robot apparaît initialement aux participants : comme un objet immobile ou, au contraire, comme une entité déjà impliquée dans une activité préexistante. Les orientations des participants vers les tout premiers comportements manifestés par le robot au cours de cette phase de ""pré-début"" ont produit des trajectoires séquentielles a priori imprévisibles, qui ont configuré le moment et la manière dont le robot a émergé en tant qu'agent social. Nous suggérons que ces premiers instants de coprésence ne sont pas des questions secondaires en ce qui concerne les expériences homme-robot, mais qu'ils devraient être pensés et conçus comme une partie intégrante de ces expériences.","18674","3","209","De l&#039;objet inanimé à l&#039;agent : Impact des prémices sur l&#039;émergence des salutations avec un robot

Les tout premiers moments de coprésence, au cours desquels un robot apparaît pour la première fois à un participant, sont souvent &quot;hors champ&quot; dans les données collectées lors des expérimentations homme-robot (enregistrements vidéo, suivi de mouvement, rubriques méthodologiques, etc.) Pourtant, cette phase de &quot;pré-début&quot;, bien documentée dans le cas des interactions humain-humain, n&#039;est pas un vide interactionnel : c&#039;est là que le travail interactionnel des participants peut avoir lieu pour que la production d&#039;un premier tour de parole (comme saluer le robot) devienne pertinente et attendue. Nous basons notre analyse sur une expérience qui reproduit les retards d&#039;ouverture de l&#039;interaction parfois observés dans les études d&#039;interaction homme-robot en laboratoire ou &quot;dans la nature&quot;, où les robots peuvent avoir besoin de temps avant de s&#039;animer après avoir été en coprésence avec un humain. En utilisant une méthodologie ethnométhodologique et multimodale d&#039;analyse de conversation (EMCA), nous identifions quelles propriétés du comportement du robot ont été orientées par les participants comme créant les conditions adéquates pour produire une première salutation. Nos résultats soulignent l&#039;importance de l&#039;état dans lequel le robot apparaît initialement aux participants : comme un objet immobile ou, au contraire, comme une entité déjà impliquée dans une activité préexistante. Les orientations des participants vers les tout premiers comportements manifestés par le robot au cours de cette phase de &quot;pré-début&quot; ont produit des trajectoires séquentielles a priori imprévisibles, qui ont configuré le moment et la manière dont le robot a émergé en tant qu&#039;agent social. Nous suggérons que ces premiers instants de coprésence ne sont pas des questions secondaires en ce qui concerne les expériences homme-robot, mais qu&#039;ils devraient être pensés et conçus comme une partie intégrante de ces expériences.","2","2023-06-26 10:50:47","2023-06-26 11:00:26","18674",,"1","1","1","1","2","1",NULL,,"2023-06-26 10:50:36","0"
"4002656","Data-to-text (D2T) and text-to-data (T2D) are dual tasks that convert structured data, such as graphs or tables into fluent text, and vice versa. These tasks are usually handled separately and use corpora extracted from a single source. Current systems leverage pre-trained language models fine-tuned on D2T or T2D tasks. This approach has two main limitations: first, a separate system has to be tuned for each task and source; second, learning is limited by the scarcity of available corpora. This paper considers a more general scenario where data are available from multiple heterogeneous sources. Each source, with its specific data format and semantic domain, provides a non-parallel corpus of text and structured data. We introduce a variational auto-encoder model with disentangled style and content variables that allows us to represent the diversity that stems from multiple sources of text and data. Our model is designed to handle the tasks of D2T and T2D jointly. We evaluate our model on several datasets, and show that by learning from multiple sources, our model closes the performance gap with its supervised single-source counterpart and outperforms it in some cases.","Learning from Multiple Sources for Data-to-Text and Text-to-Data","Song Duong, Alberto Lumbreras, Mike Gartrell, Patrick Gallinari","3765","Apprentissage à partir de sources multiples pour la conversion de données en texte et de texte en données

Data-to-text (D2T) et text-to-data (T2D) sont des tâches doubles qui convertissent des données structurées, telles que des graphiques ou des tableaux, en texte fluide, et vice versa. Ces tâches sont généralement gérées séparément et utilisent des corps extraits d'une source unique. Les systèmes actuels exploitent des modèles de langage pré-formés et adaptés aux tâches D2T ou T2D. Cette approche présente deux limites principales : d'abord, un système distinct doit être réglé pour chaque tâche et chaque source ; deuxièmement, l'apprentissage est limité par la rareté des corpus disponibles. Le présent document examine un scénario plus général où les données sont disponibles à partir de sources hétérogènes multiples. Chaque source, avec son format de données spécifique et son domaine sémantique, fournit un corpus non parallèle de texte et de données structurées. Nous introduisons un modèle d'auto-encodeur variationnel avec des variables de style et de contenu démêlées qui nous permet de représenter la diversité qui découle de multiples sources de texte et de données. Notre modèle est conçu pour gérer conjointement les tâches de D2T et de T2D. Nous évaluons notre modèle sur plusieurs ensembles de données et montrons qu'en apprenant de sources multiples, notre modèle comble l'écart de performance avec son homologue supervisé à source unique et le surpasse dans certains cas.","15681","6","210","Apprentissage à partir de sources multiples pour la conversion de données en texte et de texte en données

La conversion de données en texte (D2T, data-to-text) et de texte en données (T2D, text-to-data) sont des tâches doubles qui convertissent des données structurées, telles que des graphiques ou des tableaux, en texte fluide et vice versa. Ces tâches sont généralement gérées séparément et utilisent des corpus extraits d&#039;une source unique. Les systèmes actuels exploitent des modèles de langage pré-formés et adaptés aux tâches de D2T ou de T2D. Cette approche présente deux limites principales : d&#039;abord, un système distinct doit être réglé pour chaque tâche et chaque source. Deuxièmement, l&#039;apprentissage est limité par la rareté des corpus disponibles. Le présent document examine un scénario plus général où les données sont disponibles à partir de sources hétérogènes multiples. Chaque source, avec son format de données spécifique et son domaine sémantique, fournit un corpus non parallèle de texte et de données structurées. Nous introduisons un modèle d&#039;auto-encodeur variationnel avec des variables de style et de contenu démêlées nous permettant de représenter la diversité découlant de multiples sources de texte et de données. Notre modèle est conçu pour gérer conjointement les tâches de D2T et de T2D. Nous évaluons notre modèle sur plusieurs ensembles de données et montrons qu&#039;en apprenant de sources multiples, notre modèle comble l&#039;écart de performance avec son homologue supervisé à source unique et le surpasse dans certains cas.","2","2023-06-26 11:04:47","2023-06-26 11:14:23","15681",,"1","2","4","2","2","3",NULL,,"2023-06-26 11:04:40","0"
"3984643","This work addresses the cross-corpora generalization issue for the low-resourced spoken language identification (LID) problem. We have conducted the experiments in the context of Indian LID and identified strikingly poor cross-corpora generalization due to corpora-dependent nonlingual biases. Our contribution to this work is twofold. First, we propose domain diversification, which diversifies the limited training data using different audio data augmentation methods. We then propose the concept of maximally diversity-aware cascaded augmentations and optimize the augmentation fold-factor for effective diversification of the training data. Second, we introduce the idea of domain generalization considering the augmentation methods as pseudo-domains. Towards this, we investigate both domain-invariant and domain-aware approaches. Our LID system is based on the state-of-the-art emphasized channel attention, propagation, and aggregation based time delay neural network (ECAPA-TDNN) architecture. We have conducted extensive experiments with three widely used corpora for Indian LID research. In addition, we conduct a final blind evaluation of our proposed methods on the Indian subset of VoxLingua107 corpus collected in the wild. Our experiments demonstrate that the proposed domain diversification is more promising over commonly used simple augmentation methods. The study also reveals that domain generalization is a more effective solution than domain diversification. We also notice that domain-aware learning performs better for same-corpora LID, whereas domain-invariant learning is more suitable for cross-corpora generalization. Compared to basic ECAPA-TDNN, its proposed domain-invariant extensions improve the cross-corpora EER up to 5.23%. In contrast, the proposed domain-aware extensions also improve performance for same-corpora test scenarios.","Cross-corpora spoken language identification with domain diversification and generalization","Spandan Dey, Md Sahidullah, Goutam Saha","3799","Identification de langues parlées intercorporelles avec diversification et généralisation des domaines

Ce travail aborde la question de la généralisation inter-corporelle pour le problème de l'identification de la langue parlée (LID) à partir de ressources limitées. Nous avons mené des expériences dans le contexte de l'identification de la langue parlée indienne et identifié une généralisation inter-corporelle remarquablement faible due à des biais non linguistiques dépendant du corpus. Notre contribution à ce travail est double. Tout d'abord, nous proposons une diversification des domaines, qui diversifie les données d'apprentissage limitées en utilisant différentes méthodes d'augmentation des données audio. Nous proposons ensuite le concept d'augmentations en cascade tenant compte de la diversité maximale et optimisons le facteur de pliage de l'augmentation pour une diversification efficace des données d'apprentissage. Deuxièmement, nous introduisons l'idée de la généralisation du domaine en considérant les méthodes d'augmentation comme des pseudo-domaines. Pour ce faire, nous étudions les approches invariantes et sensibles au domaine. Notre système LID est basé sur l'architecture de pointe du réseau neuronal à retardement basé sur l'attention, la propagation et l'agrégation des canaux (ECAPA-TDNN). Nous avons mené des expériences approfondies avec trois corpus largement utilisés dans le cadre de la recherche indienne sur la LID. En outre, nous procédons à une évaluation finale en aveugle des méthodes proposées sur le sous-ensemble indien du corpus VoxLingua107 collecté dans la nature. Nos expériences démontrent que la diversification des domaines proposée est plus prometteuse que les méthodes d'augmentation simples couramment utilisées. L'étude révèle également que la généralisation des domaines est une solution plus efficace que la diversification des domaines. Nous remarquons également que l'apprentissage axé sur le domaine est plus performant pour la LID dans un même corpora, tandis que l'apprentissage invariant par domaine est plus adapté à la généralisation dans plusieurs corpora. Par rapport à l'ECAPA-TDNN de base, les extensions invariantes par domaine proposées améliorent l'EER inter-corpora jusqu'à 5,23 %. En revanche, les extensions proposées en fonction du domaine améliorent également les performances pour les scénarios d'essai dans un même contexte.","18615","3","211","Identification de la parole inter-corpus avec diversification et généralisation des domaines

Ce travail aborde la question de la généralisation inter-corpus pour le problème de l&#039;identification de la parole à partir de ressources limitées. Nous avons mené des expériences dans le contexte de l&#039;identification de la parole en langue indienne et identifié une généralisation inter-corpus remarquablement faible due à des biais non linguistiques dépendant du corpus. Notre contribution à ce travail est double. Tout d&#039;abord, nous proposons une diversification des domaines, qui diversifie les données d&#039;apprentissage limitées en utilisant différentes méthodes d&#039;augmentation des données audio. Nous proposons ensuite le concept d&#039;augmentations en cascade tenant compte de la diversité maximale et optimisons le facteur de pliage de l&#039;augmentation pour une diversification efficace des données d&#039;apprentissage. Deuxièmement, nous introduisons l&#039;idée de la généralisation du domaine en considérant les méthodes d&#039;augmentation comme des pseudo-domaines. Pour ce faire, nous étudions les approches invariantes et sensibles au domaine. Notre système d&#039;identification de la parole est fondé sur l&#039;architecture de pointe du réseau neuronal à retardement fondé sur l&#039;attention, la propagation et l&#039;agrégation des canaux (ECAPA-TDNN). Nous avons mené des expériences approfondies avec trois corpus largement utilisés dans le cadre de la recherche indienne sur l&#039;identification de la parole. En outre, nous procédons à une évaluation finale en aveugle des méthodes proposées sur le sous-ensemble indien du corpus VoxLingua107 collecté sur le terrain. Nos expériences démontrent que la diversification des domaines proposée est plus prometteuse que les méthodes d&#039;augmentation simples couramment utilisées. L&#039;étude révèle également que la généralisation des domaines est une solution plus efficace que la diversification des domaines. Nous remarquons également que l&#039;apprentissage axé sur le domaine est plus performant pour l&#039;identification de la parole dans un même corpus, tandis que l&#039;apprentissage invariant par domaine est plus adapté à la généralisation dans plusieurs corpus. Par rapport à l&#039;ECAPA-TDNN de base, les extensions invariantes par domaine proposées améliorent l&#039;EER inter-corpus jusqu&#039;à 5,23 %. En revanche, les extensions proposées en fonction du domaine améliorent également les performances pour les scénarios d&#039;essai dans un même contexte.","2","2023-06-26 11:16:59","2023-06-26 11:28:23","18615",,"2","3","4","4","3","4",NULL,,"2023-06-26 11:16:48","0"
"3905068","In this paper, we study the problem of learning Graph Neural Networks (GNNs) with Differential Privacy (DP). We propose a novel differentially private GNN based on Aggregation Perturbation (GAP), which adds stochastic noise to the GNN's aggregation function to statistically obfuscate the presence of a single edge (edge-level privacy) or a single node and all its adjacent edges (node-level privacy). Tailored to the specifics of private learning, GAP's new architecture is composed of three separate modules: (i) the encoder module, where we learn private node embeddings without relying on the edge information; (ii) the aggregation module, where we compute noisy aggregated node embeddings based on the graph structure; and (iii) the classification module, where we train a neural network on the private aggregations for node classification without further querying the graph edges. GAP's major advantage over previous approaches is that it can benefit from multi-hop neighborhood aggregations, and guarantees both edge-level and node-level DP not only for training, but also at inference with no additional costs beyond the training's privacy budget. We analyze GAP's formal privacy guarantees using Rényi DP and conduct empirical experiments over three real-world graph datasets. We demonstrate that GAP offers significantly better accuracy-privacy trade-offs than state-of-the-art DP-GNN approaches and naive MLP-based baselines. Our code is publicly available at https://github.com/sisaman/GAP.","GAP: Differentially Private Graph Neural Networks with Aggregation Perturbation","Sina Sajadmanesh, Ali Shahin Shamsabadi, Aurélien Bellet, Daniel Gatica-Perez","3998","ÉCART : Réseaux neuronaux de graphe privé différentiel avec perturbation d'agrégation

Dans cet article, nous étudions le problème de l'apprentissage des réseaux de neurones graphiques (GNNs) avec la confidentialité différentielle (DP). Nous proposons un nouveau GNN différentiellement privé basé sur la Perturbation d'Agrégation (GAP), qui ajoute un bruit stochastique à la fonction d'agrégation du GNN pour obscurcir statistiquement la présence d'un seul bord (confidentialité au niveau du bord) ou d'un seul noeud et de tous ses bords adjacents (confidentialité au niveau du noeud). Adaptée aux spécificités de l'apprentissage privé, la nouvelle architecture de GAP se compose de trois modules distincts : (i) le module encodeur, où nous apprenons l'intégration de noeuds privés sans dépendre des informations de périphérie ; (ii) le module d'agrégation, dans lequel nous calculons les noyaux agrégés bruités incorporés en fonction de la structure graphique ; et (iii) le module de classification, dans lequel on forme un réseau neuronal sur les agrégations privées pour la classification des noeuds sans interroger davantage les arêtes du graphe. Le principal avantage de GAP par rapport aux approches précédentes est qu'il peut bénéficier d'agrégations de voisinage multi-saut, et garantit à la fois le DP au niveau de la périphérie et au niveau du noeud, non seulement pour la formation, mais aussi en inférence, sans coûts supplémentaires au-delà du budget de confidentialité de la formation. Nous analysons les garanties formelles de confidentialité de GAP en utilisant Rényi DP et réalisons des expériences empiriques sur trois ensembles de données graphiques réels. Nous démontrons que GAP offre de bien meilleurs compromis entre précision et confidentialité que les approches DP-GNN de pointe et les bases de données naïves basées sur MLP. Notre code est disponible publiquement sur https://github.com/sisaman/GAP.","15914","6","220","GAP : Réseaux neuronaux de graphe privé différentiel avec perturbation d&#039;agrégation

Dans cet article, nous étudions le problème de l&#039;apprentissage des réseaux de neurones graphiques (GNN, Graph Neural Networks) avec la confidentialité différentielle (DP, Differential Privacy). Nous proposons un nouveau GNN différentiellement privé fondé sur la perturbation d&#039;agrégation (GAP, Aggregation Perturbation), qui ajoute un bruit stochastique à la fonction d&#039;agrégation du GNN pour obscurcir statistiquement la présence d&#039;un seul bord (confidentialité au niveau du bord) ou d&#039;un seul nœud et de tous ses bords adjacents (confidentialité au niveau du nœud). Adaptée aux spécificités de l&#039;apprentissage privé, la nouvelle architecture de GAP se compose de trois modules distincts : (i) le module encodeur, où nous apprenons l&#039;intégration de nœuds privés sans dépendre des informations de périphérie, (ii) le module d&#039;agrégation, dans lequel nous calculons les noyaux agrégés bruités incorporés en fonction de la structure graphique, et (iii) le module de classification, dans lequel on forme un réseau neuronal sur les agrégations privées pour la classification des nœuds sans interroger davantage les arêtes du graphe. Le principal avantage de GAP par rapport aux approches précédentes est qu&#039;il peut bénéficier d&#039;agrégations de voisinage multisaut, et garantit à la fois la confidentialité différentielle au niveau de la périphérie et au niveau du nœud, non seulement pour la formation, mais aussi en inférence, sans coûts supplémentaires au-delà du budget de confidentialité de la formation. Nous analysons les garanties formelles de confidentialité de GAP en utilisant Rényi DP et réalisons des expériences empiriques sur trois ensembles de données graphiques réels. Nous démontrons que GAP offre de bien meilleurs compromis entre précision et confidentialité que les approches DP-GNN de pointe et les bases de données naïves fondées sur MLP. Notre code est disponible publiquement sur https://github.com/sisaman/GAP.","2","2023-06-26 18:19:05","2023-06-26 18:26:23","15914",,"4","2","4","3","4","2",NULL,,"2023-06-26 06:18:59","0"
"3970122","Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functionally grounding several variants (size, architecture) of FLAN-T5.","Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning","Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer","3840","Mise à la terre de grands modèles linguistiques dans des environnements interactifs avec l’apprentissage du renforcement en ligne

Des travaux récents ont réussi à tirer parti des capacités de Large Language Models (LLM) pour capturer des connaissances abstraites sur la physique du monde pour résoudre des problèmes de prise de décision. Pourtant, l’alignement entre les connaissances des LLM et l’environnement peut être erroné et limiter les compétences fonctionnelles en raison d’un manque d’ancrage. Dans cet article, nous étudions une approche pour atteindre cet alignement par une mise à la terre fonctionnelle: nous considérons un agent utilisant un LLM comme une politique qui est progressivement mise à jour au fur et à mesure que l’agent interagit avec l’environnement, en tirant parti de l’apprentissage du renforcement en ligne pour améliorer ses performances pour résoudre des objectifs. En utilisant un environnement textuel interactif conçu pour étudier des formes de mise à la terre fonctionnelles de haut niveau, et un ensemble de tâches spatiales et de navigation, nous étudions plusieurs questions scientifiques: 1) Les LLM peuvent-ils améliorer l’efficacité de l’échantillon pour l’apprentissage en ligne de diverses tâches de RL? 2) Comment peut-il stimuler différentes formes de généralisation? 3) Quel est l’impact de l’apprentissage en ligne? Nous étudions ces questions en mettant à la base fonctionnellement plusieurs variantes (taille, architecture) de FLAN-T5.","12798","4","221","Ancrage de grands modèles de langage dans des environnements interactifs avec l’apprentissage par renforcement en ligne

Des travaux récents ont réussi à tirer parti des capacités de grands modèles de langage (LLM, Large Language Models) pour capturer des connaissances abstraites sur la physique du monde pour résoudre des problèmes de prise de décision. Pourtant, l’alignement entre les connaissances des LLM et l’environnement peut être erroné et limiter les compétences fonctionnelles en raison d’un manque d’ancrage. Dans cet article, nous étudions une approche pour atteindre cet alignement par un ancrage fonctionnel : nous considérons un agent utilisant un LLM comme une politique qui est progressivement mise à jour au fur et à mesure que l’agent interagit avec l’environnement, en tirant parti de l’apprentissage par renforcement en ligne pour améliorer ses performances pour résoudre des objectifs. En utilisant un environnement textuel interactif conçu pour étudier des formes d&#039;ancrage fonctionnel de haut niveau, et un ensemble de tâches spatiales et de navigation, nous étudions plusieurs questions scientifiques : 1) Les LLM peuvent-ils améliorer l’efficacité de l’échantillon pour l’apprentissage en ligne de diverses tâches d&#039;apprentissage par renforcement ? 2) Comment peut-il stimuler différentes formes de généralisation ? 3) Quel est l’impact de l’apprentissage en ligne ? Nous étudions ces questions en ancrant fonctionnellement plusieurs variantes (taille, architecture) de FLAN-T5.","2","2023-06-26 18:31:05","2023-06-26 18:41:30","12798",,"4","1","4","2","4","2",NULL,,"2023-06-26 06:30:53","0"
"3972560","We propose a multimodal speech driven approach to generate 2D upper-body gestures for virtual agents, in the communicative style of different speakers, seen or unseen by our model during training. Upper-body gestures of a source speaker are generated based on the content of his/her multimodal data-speech acoustics and text semantics. The synthesized source speaker's gestures are conditioned on the multimodal style representation of the target speaker. Our approach is zero-shot, and can generalize the style transfer to new unseen speakers, without any additional training. An objective evaluation is conducted to validate our approach.","Zero-Shot Style Transfer for Multimodal Data-Driven Gesture Synthesis","Mireille Fares, Nicolas Obin, Catherine Pelachaud","3833","Transfert de style Zero-Shot pour la synthèse de gesture multimodale

Nous proposons une approche multimodale axée sur la parole pour générer des gestes 2D du haut du corps pour les agents virtuels, dans le style communicatif de différents locuteurs, vus ou invisibles par notre modèle lors de l’entraînement. Les gestes du haut du corps d’un haut-parleur source sont générés en fonction du contenu de l’acoustique multimodale de la voix de données et de la sémantique textuelle. Les gestes du locuteur source synthétisé sont conditionnés à la représentation multimodale du locuteur cible. Notre approche est zéro-shot, et peut généraliser le transfert de style à de nouveaux haut-parleurs invisibles, sans aucune formation supplémentaire. Une évaluation objective est menée pour valider notre approche.","12791","4","222","Transfert de style zero-shot pour la synthèse de gestes multimodale axée sur les données

Nous proposons une approche multimodale axée sur la parole pour générer des gestes 2D du haut du corps pour les agents virtuels, dans le style communicatif de différents locuteurs, vus ou invisibles par notre modèle lors de l’entraînement. Les gestes du haut du corps d’un locuteur source sont générés en fonction du contenu de l’acoustique multimodale donnée-parole et de la sémantique textuelle. Les gestes du locuteur source synthétisé sont conditionnés à la représentation multimodale du locuteur cible. Nous utilisons une méthode d&#039;apprentissage zero-shot, pouvant généraliser le transfert de style à de nouveaux locuteurs invisibles, sans aucune formation supplémentaire. Une évaluation objective est menée pour valider notre approche.","2","2023-06-26 18:45:20","2023-06-26 18:54:29","12791",,"4","2","4","4","1","4",NULL,,"2023-06-26 06:45:07","0"
"4011927","Recently, engagement has emerged as a key variable explaining the success of conversation. In the perspective of human-machine interaction, an automatic assessment of engagement becomes crucial to better understand the dynamics of an interaction and to design socially-aware robots. This paper presents a predictive model of the level of engagement in conversations. It shows in particular the interest of using a rich multimodal set of features, outperforming the existing models in this domain. In terms of methodology, study is based on two audiovisual corpora of naturalistic face-to-face interactions. These resources have been enriched with various annotations of verbal and nonverbal behaviors, such as smiles, head nods, and feedbacks. In addition, we manually annotated gestures intensity. Based on a review of previous works in psychology and humanmachine interaction, we propose a new definition of the notion of engagement, adequate for the description of this phenomenon both in natural and mediated environments. This definition have been implemented in our annotation scheme. In our work, engagement is studied at the turn level, known to be crucial for the organization of the conversation. Even though there is still a lack of consensus around their precise definition, we have developed a turn detection tool. A multimodal characterization of engagement is performed using a multi-level classification of turns. We claim a set of multimodal cues, involving prosodic, mimo-gestural and morpho-syntactic information, is relevant to characterize the level of engagement of speakers in conversation. Our results significantly outperform the baseline and reach state-of-the-art level (. weighted F-score). The most contributing modalities are identified by testing the performance of a two-layer perceptron when trained on unimodal feature sets and on combinations of two to four modalities. These results support our claim about multimodality: combining features related to the speech fundamental frequency and energy with mimo-gestural features leads to the best performance.","A multimodal approach for modeling engagement in conversation","Arthur Pellet-Rostaing, Roxane Bertrand, Auriane Boudin, Stéphane Rauzy, Philippe Blache","3752","Une approche multimodale pour modéliser l'engagement dans une conversation

Récemment, l'engagement est apparu comme une variable clé expliquant le succès d'une conversation. Dans la perspective de l'interaction homme-machine, une évaluation automatique de l'engagement devient cruciale pour mieux comprendre la dynamique d'une interaction et pour concevoir des robots socialement conscients. Cet article présente un modèle prédictif du niveau d'engagement dans les conversations. Il montre en particulier l'intérêt d'utiliser un ensemble riche de caractéristiques multimodales, surpassant les modèles existants dans ce domaine. En termes de méthodologie, l'étude est basée sur deux corpus audiovisuels d'interactions naturalistes en face-à-face. Ces ressources ont été enrichies de diverses annotations de comportements verbaux et non verbaux, tels que les sourires, les hochements de tête et les commentaires. En outre, nous avons annoté manuellement l'intensité des gestes. Sur la base d'une revue des travaux antérieurs en psychologie et en interaction homme-machine, nous proposons une nouvelle définition de la notion d'engagement, adéquate pour la description de ce phénomène à la fois dans les environnements naturels et médiatisés. Cette définition a été implémentée dans notre système d'annotation. Dans notre travail, l'engagement est étudié au niveau du tour de parole, connu pour être crucial pour l'organisation de la conversation. Bien qu'il n'y ait pas encore de consensus sur leur définition précise, nous avons développé un outil de détection des tournures. Une caractérisation multimodale de l'engagement est réalisée à l'aide d'une classification multi-niveaux des tournures. Nous affirmons qu'un ensemble d'indices multimodaux, impliquant des informations prosodiques, mimo-gestuelles et morpho-syntaxiques, est pertinent pour caractériser le niveau d'engagement des locuteurs dans la conversation. Nos résultats dépassent de manière significative la ligne de base et atteignent le niveau de l'état de l'art (. weighted F-score). Les modalités les plus contributives sont identifiées en testant la performance d'un perceptron à deux couches lorsqu'il est entraîné sur des ensembles de caractéristiques unimodales et sur des combinaisons de deux à quatre modalités. Ces résultats confirment notre affirmation concernant la multimodalité : la combinaison de caractéristiques liées à la fréquence fondamentale et à l'énergie de la parole avec des caractéristiques mimo-gestuelles permet d'obtenir les meilleures performances.","18568","3","224","Une approche multimodale pour modéliser l&#039;engagement dans une conversation

Récemment, l&#039;engagement est apparu comme une variable clé expliquant le succès d&#039;une conversation. Dans la perspective de l&#039;interaction homme-machine, une évaluation automatique de l&#039;engagement devient cruciale pour mieux comprendre la dynamique d&#039;une interaction et pour concevoir des robots socialement conscients. Cet article présente un modèle prédictif du niveau d&#039;engagement dans les conversations. Il montre en particulier l&#039;intérêt d&#039;utiliser un ensemble riche de caractéristiques multimodales, surpassant les modèles existants dans ce domaine. En termes de méthodologie, l&#039;étude s&#039;appuie sur deux corpus audiovisuels d&#039;interactions naturalistes en face-à-face. Ces ressources ont été enrichies de diverses annotations de comportements verbaux et non verbaux, tels que les sourires, les hochements de tête et les commentaires. En outre, nous avons annoté manuellement l&#039;intensité des gestes. Sur la base d&#039;une revue des travaux antérieurs en psychologie et en interaction homme-machine, nous proposons une nouvelle définition de la notion d&#039;engagement, adéquate pour la description de ce phénomène à la fois dans les environnements naturels et médiatisés. Cette définition a été implémentée dans notre système d&#039;annotation. Dans notre travail, l&#039;engagement est étudié au niveau du tour de parole, connu pour être crucial pour l&#039;organisation de la conversation. Bien qu&#039;il n&#039;y ait pas encore de consensus sur leur définition précise, nous avons développé un outil de détection des tours de parole. Une caractérisation multimodale de l&#039;engagement est réalisée à l&#039;aide d&#039;une classification multiniveau des tours de parole. Nous affirmons qu&#039;un ensemble d&#039;indices multimodaux, impliquant des informations prosodiques, mimo-gestuelles et morphosyntaxiques, est pertinent pour caractériser le niveau d&#039;engagement des locuteurs dans la conversation. Nos résultats dépassent de manière significative la ligne de base et atteignent le niveau de l&#039;état de l&#039;art (score F pondéré). Les modalités les plus contributives sont identifiées en testant la performance d&#039;une perception à deux couches lorsqu&#039;il est entraîné sur des ensembles de caractéristiques unimodales et sur des combinaisons de deux à quatre modalités. Ces résultats confirment notre affirmation concernant la multimodalité : la combinaison de caractéristiques liées à la fréquence fondamentale et à l&#039;énergie de la parole avec des caractéristiques mimo-gestuelles permet d&#039;obtenir les meilleures performances.","2","2023-06-27 10:37:01","2023-06-27 10:41:51","18568",,"4","2","4","2","3","2",NULL,,"2023-06-27 10:36:52","0"
"3957265","Many studies have shown that transformers are able to predict subject-verb agreement, demonstrating their ability to uncover an abstract representation of the sentence in an unsupervised way. Recently, Li et al. (2021) found that transformers were also able to predict the object-past participle agreement in French, the modeling of which in formal grammar is fundamentally different from that of subject-verb agreement and relies on a movement and an anaphora resolution. To better understand transformers’ internal working, we propose to contrast how they handle these two kinds of agreement. Using probing and counterfactual analysis methods, our experiments on French agreements show that (i) the agreement task suffers from several confounders that partially question the conclusions drawn so far and (ii) transformers handle subject-verb and object-past participle agreements in a way that is consistent with their modeling in theoretical linguistics.","Assessing the Capacity of Transformer to Abstract Syntactic Representations: A Contrastive Analysis Based on Long-distance Agreement","Bingzhi Li, Guillaume Wisniewski, Benoît Crabbé","3860","Évaluation de la capacité du transformateur à résumer les représentations synthétiques: Une analyse contradictoire fondée sur un accord à longue distance

De nombreuses études ont montré que les transformateurs sont capables de prédire l’accord sujet-verbe, démontrant leur capacité à découvrir une représentation abstraite de la phrase d’une manière non supervisée. Récemment, Li et al. (2021) a constaté que les transformateurs étaient également capables de prédire l’accord de participation passé objet en français, dont la modélisation dans la grammaire formelle est fondamentalement différente de celle de l’accord subject-verbe et repose sur un mouvement et une résolution anaphora. Pour mieux comprendre le fonctionnement interne des transformateurs, nous proposons de contraster la manière dont ils gèrent ces deux types d’accords. En utilisant des méthodes de sondage et d’analyse contrefactuelle, nos expériences sur les accords français montrent que (i) la tâche de l’accord souffre de plusieurs facteurs de confusion qui remettent partiellement en question les conclusions tirées jusqu’à présent et (ii) les transformateurs manipulent des accords subject-verbe et participle objet-passé d’une manière compatible avec leur modélisation en linguistique théorique.","12818","4","225","Évaluation de la capacité des transformateurs à résumer les représentations synthétiques : Une analyse contradictoire fondée sur un accord à longue distance

De nombreuses études ont montré que les transformateurs sont capables de prédire l’accord sujet-verbe, démontrant leur capacité à découvrir une représentation abstraite de la phrase d’une manière non supervisée. Récemment, Li et al. (2021) ont constaté que les transformateurs étaient également capables de prédire l’accord objet-participe passé en français, dont la modélisation dans la grammaire formelle est fondamentalement différente de celle de l’accord sujet-verbe et repose sur un mouvement et une résolution des anaphores. Pour mieux comprendre le fonctionnement interne des transformateurs, nous proposons de contraster la manière dont ils gèrent ces deux types d’accords. En utilisant des méthodes de sondage et d’analyse contrefactuelle, nos expériences sur les accords français montrent que (i) la tâche de l’accord souffre de plusieurs facteurs de confusion qui remettent partiellement en question les conclusions tirées jusqu’à présent et (ii) les transformateurs manipulent des accords sujet-verbe et objet-participe passé d’une manière compatible avec leur modélisation en linguistique théorique.","2","2023-06-27 10:53:54","2023-06-27 11:00:52","12818",,"4","4","4","4","4","4",NULL,,"2023-06-27 10:53:48","0"
"3958642","Despite — or perhaps because of — their simplicity, n-grams, or contiguous sequences of tokens, have been used with great success in computational linguistics since their introduction in the late 20th century. Recast as k-mers, or contiguous sequences of monomers, they have also found applications in computational biology. When applied to the analysis of texts, n-grams usually take the form of sequences of words. But if we try to apply this model to the analysis of Sanskrit texts, we are faced with the arduous task of, firstly, resolving sandhi to split a phrase into words, and, secondly, splitting long compounds into their components. This paper presents a simpler method of tokenizing a Sanskrit text for n-grams, by using n-akṣaras, or contiguous sequences of akṣaras. This model reduces the need for sandhi resolution, making it much easier to use on raw text. It is also possible to use this model on Sanskrit-adjacent texts, e.g., a Tamil commentary on a Sanskrit text. As a test case, the commentaries on Amarakoṣa 1.0.1 have been modelled as n-akṣaras, showing patterns of text reuse across ten centuries and nine languages. Some initial observations are made concerning Buddhist commentarial practices.","Using n-akṣaras to model Sanskrit & Sanskrit-adjacent texts","Charles Li","383","Utilisation des n-akṣaras pour modéliser les textes sanskrits et sanskrits adjacents

Malgré - ou peut-être à cause de - leur simplicité, les n-grammes, ou séquences contiguës de tokens, ont été utilisés avec beaucoup de succès en linguistique informatique depuis leur introduction à la fin du 20e siècle. Refondus en k-mères, ou séquences contiguës de monomères, ils ont également trouvé des applications en biologie informatique. Appliqués à l'analyse de textes, les n-grammes prennent généralement la forme de séquences de mots. Mais si nous essayons d'appliquer ce modèle à l'analyse des textes sanskrits, nous sommes confrontés à la tâche ardue qui consiste, d'une part, à résoudre les sandhi pour diviser une phrase en mots et, d'autre part, à diviser les longs composés en leurs éléments constitutifs. Cet article présente une méthode plus simple de tokenisation d'un texte sanskrit pour les n-grammes, en utilisant des n-akṣaras, ou des séquences contiguës d'akṣaras. Ce modèle réduit le besoin de résolution de sandhi, ce qui le rend beaucoup plus facile à utiliser sur un texte brut. Il est également possible d'utiliser ce modèle sur des textes adjacents au sanskrit, par exemple un commentaire tamoul sur un texte sanskrit. À titre de test, les commentaires de l'Amarakoṣa 1.0.1 ont été modélisés comme des n-akṣaras, montrant des schémas de réutilisation de textes à travers dix siècles et neuf langues. Quelques observations initiales sont faites concernant les pratiques bouddhistes en matière de commentaires.","10137","3","228","Utilisation des n-akṣaras pour modéliser les textes sanskrits et adjacents au sanskrit

Malgré (ou peut-être à cause de) leur simplicité, les n-grammes, ou séquences contiguës de tokens, ont été utilisés avec beaucoup de succès en linguistique informatique depuis leur introduction à la fin du 20e siècle. Refondus en k-mers, ou séquences contiguës de monomères, ils ont également trouvé des applications en biologie informatique. Appliqués à l&#039;analyse de textes, les n-grammes prennent généralement la forme de séquences de mots. Mais si nous essayons d&#039;appliquer ce modèle à l&#039;analyse des textes sanskrits, nous sommes confrontés à la tâche ardue qui consiste, d&#039;une part, à résoudre les sandhis pour diviser une phrase en mots et, d&#039;autre part, à diviser les longs composés en leurs éléments constitutifs. Cet article présente une méthode plus simple de tokenisation d&#039;un texte sanskrit pour les n-grammes, en utilisant des n-akṣaras, ou des séquences contiguës d&#039;akṣaras. Ce modèle réduit le besoin de résolution de sandhis, ce qui le rend beaucoup plus facile à utiliser sur un texte brut. Il est également possible d&#039;utiliser ce modèle sur des textes adjacents au sanskrit, par exemple un commentaire tamoul sur un texte sanskrit. À titre de test, les commentaires de l&#039;Amarakoṣa 1.0.1 ont été modélisés comme des n-akṣaras, montrant des schémas de réutilisation de textes à travers dix siècles et neuf langues. Quelques observations initiales sont faites concernant les pratiques bouddhistes en matière de commentaires.","2","2023-06-27 15:21:10","2023-06-27 15:29:28","10137",,"2","4","3","2","3","3",NULL,,"2023-06-27 03:21:06","0"
"3976592","This paper presents a novel approach for phone-level pronunciation scoring. The proposed method relies on the two usual stages of pronunciation scoring: an acoustic model transcribes the spoken utterance into a phoneme sequence and then, Weighted-Dynamic Time Warping (W-DTW) is used to compare the predicted phoneme sequence against the reference one. Our approach alters the comparison process by considering Phonetic PosteriorGrams (PPG) rather than only the most probable sequence of phonemes. This led us to pro- pose a modified W-DTW algorithm that considers the probabilities of the predicted phonemes, as well as the use of articulatory features as a proxy of phonetic similarity. The results achieved are satisfactory considering the content of the adult speech database and are comparable to well-known state-of- the-art methods.","Phone-Level Pronunciation Scoring for L1 Using Weighted-Dynamic Time Warping","Aghilas Sini, Antoine Perquin, Damien Lolive, Arnaud Delhay","3822","Poinçonnage de la prononciation par téléphone pour L1 en utilisant le réchauffement temporel dynamique pondéré

Cet article présente une nouvelle approche pour le score de prononciation au niveau du téléphone. La méthode proposée repose sur les deux étapes habituelles de la notation de prononciation: un modèle acoustique transcrit l’énoncé parlé en séquence phonème puis, le Warping temporel pondéré-dynamique (W-DTW) est utilisé pour comparer la séquence phonème prédite avec la séquence de référence. Notre approche modifie le processus de comparaison en considérant les phonèmes postérieurs phonétiques (PPG) plutôt que la séquence la plus probable des phonèmes. Cela nous a amenés à poser un algorithme W-DTW modifié qui prend en compte les probabilités des phonèmes prédits, ainsi que l’utilisation de caractéristiques articulatoires en tant que proxy de la similitude phonétique. Les résultats obtenus sont satisfaisants compte tenu du contenu de la base de données sur la parole des adultes et sont comparables à des méthodes de pointe bien connues.","12780","4","230","Évaluation de la prononciation par téléphone de la L1 en utilisant la déformation temporelle dynamique pondérée

Cet article présente une nouvelle approche pour le score de prononciation au téléphone. La méthode proposée repose sur les deux étapes habituelles de la notation de prononciation : un modèle acoustique transcrit l’énoncé parlé en séquence phonème, puis la déformation temporelle dynamique pondérée (W-DTW, Weighted-Dynamic Time Warping) est utilisée pour comparer la séquence phonème prédite avec la séquence de référence. Notre approche modifie le processus de comparaison en considérant les phonèmes postérieurs phonétiques (PPG) plutôt que la séquence la plus probable des phonèmes. Cela nous a amenés à poser un algorithme W-DTW modifié qui prend en compte les probabilités des phonèmes prédits, ainsi que l’utilisation de caractéristiques articulatoires en tant que proxy de la similitude phonétique. Les résultats obtenus sont satisfaisants compte tenu du contenu de la base de données sur la parole des adultes et sont comparables à des méthodes de pointe bien connues.","2","2023-06-27 15:31:59","2023-06-27 15:43:00","12780",,"4","3","4","3","3","3",NULL,,"2023-06-27 03:31:52","0"
"3930045","This paper presents an annotated dataset used in the MOOD Antimicrobial Resistance (AMR) hackathon, hosted in Montpellier, June 2022. The collected data concerns unstructured data from news items, scientific publications and national or international reports, collected from four eventbased surveillance (EBS) Systems, i.e. ProMED, PADI-web, HealthMap and MedISys. Data was annotated by relevance for epidemic intelligence (EI) purposes with the help of AMR experts and an annotation guideline. Extracted data were intended to include relevant events on the emergence and spread of AMR such as reports on AMR trends, discovery of new drug-bug resistances, or new AMR genes in human, animal or environmental reservoirs. This dataset can be used to train or evaluate classification approaches to automatically identify written text on AMR events across the different reservoirs and sectors of One Health (i.e. human, animal,","An annotated dataset for event-based surveillance of antimicrobial resistance","Nejat Arınık, Wim van Bortel, Bahdja Boudoua, Luca Busani, Rémy Decoupes, Roberto Interdonato, Rodrique Kafando, Esther van Kleef, Mathieu Roche, Mehtab Alam Syed, Maguelonne Teisseire","3937","Ensemble de données annoté pour la surveillance de la résistance aux antimicrobiens basée sur les événements

Cet article présente un ensemble de données annoté utilisé dans le hackathon MOOD Antimicrobial Resistance (AMR), organisé à Montpellier, Juin 2022. Les données collectées concernent des données non structurées issues d'articles de presse, de publications scientifiques et de rapports nationaux ou internationaux, collectées à partir de quatre systèmes de surveillance événementielle (EBS), à savoir ProMED, PADI-web, HealthMap et MedISys. Les données ont été annotées en fonction de leur pertinence aux fins de l'intelligence épidémique (IE) avec l'aide d'experts en RAM et d'une ligne directrice d'annotation. Les données extraites visaient à inclure des événements pertinents sur l'émergence et la propagation de la RAM, tels que des rapports sur les tendances de la RAM, la découverte de nouvelles résistances aux punaises des médicaments ou de nouveaux gènes de la RAM dans des réservoirs humains, animaux ou environnementaux. Cet ensemble de données peut être utilisé pour former ou évaluer des approches de classification afin d'identifier automatiquement un texte écrit sur les événements de RAM dans les différents réservoirs et secteurs d'une seule santé (c.-à-d. humain, animal,","15853","6","231","Ensemble de données annoté pour la surveillance de la résistance aux antimicrobiens fondée sur les événements

Cet article présente un ensemble de données annoté utilisé dans le hackathon MOOD Antimicrobial Resistance (AMR), organisé à Montpellier en juin 2022. Les données collectées concernent des données non structurées issues d&#039;articles de presse, de publications scientifiques et de rapports nationaux ou internationaux, collectées à partir de quatre systèmes de surveillance événementielle (EBS), à savoir ProMED, PADI-web, HealthMap et MedISys. Les données ont été annotées en fonction de leur pertinence aux fins de l&#039;intelligence épidémique (IE) avec l&#039;aide d&#039;experts en AMR et d&#039;une ligne directrice d&#039;annotation. Les données extraites visaient à inclure des événements pertinents sur l&#039;émergence et la propagation de l&#039;AMR, tels que des rapports sur les tendances de l&#039;AMR, la découverte de nouvelles résistances aux punaises des médicaments ou de nouveaux gènes de l&#039;AMR dans des réservoirs humains, animaux ou environnementaux. Cet ensemble de données peut être utilisé pour former ou évaluer des approches de classification afin d&#039;identifier automatiquement un texte écrit sur les événements d&#039;AMR dans les différents réservoirs et secteurs d&#039;une seule santé (c.-à-d. humain, animal,","2","2023-06-27 15:44:58","2023-06-27 15:55:32","15853",,"4","2","4","3","2","4",NULL,,"2023-06-27 03:44:52","0"
"3995318","Recent years have seen a remarkable development of deep neural network techniques for data analysis, along with their increasing application in scientific research across different disciplines. The field of mathematics has not been exempted from this general trend. The present paper proposes a philosophical assessment of the epistemological claims and conditions of such attempts. After a quick survey of recent applications of neural models to mathematical knowledge, we address the philosophical significance of those results, focusing on the specific problem of mathematical textuality and the somewhat surprising circumstance that semantic aspects of mathematical knowledge can be inferred from pure syntax. We then analyze the renewed role of distributionalism in neural models and propose an alternative understanding of its relation to meaning. Finally, we present an illustration, based on empirical evidence, of how aspects of arithmetical content such as recursive structure and total order could be inferred through explicit and interpretable means from the distributional properties of a natural language corpus.","Content from Expressions","Juan Luis Gastaldi","344","Contenu des expressions

Ces dernières années ont vu un développement remarquable des techniques de réseaux neuronaux profonds pour l’analyse des données, ainsi que leur application croissante dans la recherche scientifique dans différentes disciplines. Le domaine des mathématiques n’a pas été exempté de cette tendance générale. Le présent article propose une évaluation philosophique des revendications épistémologiques et des conditions de telles tentatives. Après une étude rapide des applications récentes des modèles neuronaux à la connaissance mathématique, nous abordons la signification philosophique de ces résultats, en nous concentrant sur le problème spécifique de la textualité mathématique et la circonstance quelque peu surprenante que les aspects sémantiques de la connaissance mathématique peuvent être déduits de la syntaxe pure. Nous analysons ensuite le rôle renouvelé du distributionalisme dans les modèles neuronaux et proposons une compréhension alternative de son rapport au sens. Enfin, nous présentons une illustration, basée sur des preuves empiriques, de la façon dont des aspects du contenu arithmétique tels que la structure récursive et l’ordre total pourraient être déduits par des moyens explicites et interprétables des propriétés de distribution d’un corpus de langage naturel.","3098","4","235","Contenu des expressions

Ces dernières années ont vu un développement remarquable des techniques de réseau de neurones profonds pour l’analyse des données, ainsi que leur application croissante dans la recherche scientifique dans différentes disciplines. Le domaine des mathématiques n’a pas été exempté de cette tendance générale. Le présent article propose une évaluation philosophique des revendications épistémologiques et des conditions de telles tentatives. Après une étude rapide des applications récentes des modèles neuronaux à la connaissance mathématique, nous abordons la signification philosophique de ces résultats, en nous concentrant sur le problème spécifique de la textualité mathématique et la circonstance quelque peu surprenante que les aspects sémantiques de la connaissance mathématique peuvent être déduits de la syntaxe pure. Nous analysons ensuite le rôle renouvelé du distributionalisme dans les modèles neuronaux et proposons une compréhension alternative de son rapport au sens. Enfin, nous présentons une illustration, basée sur des preuves empiriques, de la façon dont des aspects du contenu arithmétique tels que la structure récursive et l’ordre total pourraient être déduits par des moyens explicites et interprétables des propriétés de distribution d’un corpus de langage naturel.","2","2023-06-27 16:47:53","2023-06-27 16:50:41","3098",,"1","1","2","1","1","1",NULL,,"2023-06-27 04:47:47","0"
"3889057","The ability to ensure that a classifier gives reliable confidence scores is essential to ensure informed decision-making. To this end, recent work has focused on miscalibration, i.e., the over or under confidence of model scores. Yet calibration is not enough: even a perfectly calibrated classifier with the best possible accuracy can have confidence scores that are far from the true posterior probabilities. This is due to the grouping loss, created by samples with the same confidence scores but different true posterior probabilities. Proper scoring rule theory shows that given the calibration loss, the missing piece to characterize individual errors is the grouping loss. While there are many estimators of the calibration loss, none exists for the grouping loss in standard settings. Here, we propose an estimator to approximate the grouping loss. We show that modern neural network architectures in vision and NLP exhibit grouping loss, notably in distribution shifts settings, which highlights the importance of pre-production validation.","Beyond calibration: estimating the grouping loss of modern neural networks","Alexandre Perez-Lebel, Marine Morvan, Gaël Varoquaux","4039","Au-delà du calibrage : estimation de la perte de groupement de reseaux neuronaux modernes

La capacité de s'assurer qu'un classificateur donne des scores de confiance fiables est essentielle pour assurer une prise de décision éclairée. À cette fin, des travaux récents ont porté sur les erreurs d'étalonnage, c'est-à-dire sur la confiance excessive ou insuffisante des scores des modèles. Mais l'étalonnage ne suffit pas : même un classificateur parfaitement calibré avec la meilleure précision possible peut avoir des scores de confiance qui sont loin des vraies probabilités postérieures. Ceci est dû à la perte de regroupement, créée par des échantillons ayant les mêmes scores de confiance mais des probabilités postérieures vraies différentes. La théorie des règles de notation appropriées montre que, compte tenu de la perte d'étalonnage, la pièce manquante pour caractériser les erreurs individuelles est la perte de regroupement. Bien qu'il existe de nombreux estimateurs de la perte d'étalonnage, il n'en existe pas pour la perte de regroupement dans les réglages standard. Ici, nous proposons un estimateur pour estimer la perte de groupement. Nous montrons que les architectures de réseaux neuronaux modernes en vision et en PNL présentent une perte de groupement, notamment dans les paramètres de distribution, ce qui souligne l'importance de la validation de pré-production.","15955","6","236","Au-delà du calibrage : estimation de la perte par regroupement des réseaux neuronaux modernes

La capacité de s&#039;assurer qu&#039;un classificateur donne des scores de confiance fiables est essentielle pour assurer une prise de décision éclairée. À cette fin, des travaux récents ont porté sur les erreurs d&#039;étalonnage, c&#039;est-à-dire sur la confiance excessive ou insuffisante des scores des modèles. Mais l&#039;étalonnage ne suffit pas : même un classificateur parfaitement calibré avec la meilleure précision possible peut avoir des scores de confiance qui sont loin des vraies probabilités postérieures. Ceci est dû à la perte de regroupement (grouping loss), créée par des échantillons ayant les mêmes scores de confiance mais des probabilités postérieures réelles différentes. La théorie des règles de notation appropriées montre que, compte tenu de la perte d&#039;étalonnage, la pièce manquante pour caractériser les erreurs individuelles est la perte par regroupement. Bien qu&#039;il existe de nombreux estimateurs de la perte d&#039;étalonnage, il n&#039;en existe pas pour la perte par regroupement dans les réglages standard. Ici, nous proposons un estimateur pour estimer la perte par regroupement. Nous montrons que les architectures de réseaux neuronaux modernes en vision et en TAL présentent une perte par regroupement, notamment dans les paramètres de distribution, ce qui souligne l&#039;importance de la validation de pré-production.","2","2023-06-27 16:51:49","2023-06-27 17:00:48","15955",,"4","1","4","2","1","4",NULL,,"2023-06-27 04:51:42","0"
"4004568","This paper implements a method for sampling from the d-dimensional Von Mises Fisher distribution using NumPy, focusing on speed and readability. The complexity of the algorithm is O(nd) for n samples, which is theoretically optimal taking into account that nd is the output size.","Fast Python sampler of the von Mises Fisher distribution","Carlos Pinzón","338","Échantillonneur Fast Python de la distribution de von Mises Fisher

Cet article met en oeuvre une méthode d'échantillonnage à partir de la distribution de Von Mises Fisher en d à l'aide de NumPy, en se concentrant sur la vitesse et la lisibilité. La complexité de l'algorithme est O(nd) pour n échantillons, ce qui est théoriquement optimal compte tenu que nd est la taille de sortie.","4085","6","247","Échantillonneur Python rapide de la loi de von Mises-Fisher

Cet article met en œuvre une méthode d&#039;échantillonnage à partir de la loi de Von Mises-Fisher d-dimensionnelle à l&#039;aide de NumPy, en se concentrant sur la vitesse et la lisibilité. La complexité de l&#039;algorithme est O(nd) pour n échantillons, ce qui est théoriquement optimal compte tenu que nd est la taille de sortie.","2","2023-06-27 19:17:39","2023-06-27 19:23:33","4085",,"3","2","4","2","2","3",NULL,,"2023-06-27 07:17:31","0"
"3979667","The ""Holistic Evaluation of Audio Representations"" (HEAR) is an emerging research program towards statistical models that can transfer to diverse machine listening tasks. The originality of HEAR is to conduct a fair, ""apples-to-apples"" comparison of many deep learning models over many datasets, resulting in multitask evaluation metrics that are readily interpretable by practitioners. On the flip side, this comparison incurs a neural architecture search: as such, it is not directly interpretable in terms of audio signal processing. In this paper, we propose a complementary viewpoint on the HEAR benchmark, which we name GEAR: Generative Evaluation of Audio Representations. The key idea behind GEAR is to generate a dataset of sounds with few independent factors of variability, analyze it with HEAR embeddings, and visualize it with an unsupervised manifold learning algorithm. Visual inspection reveals stark contrasts in the global structure of the nearest-neighbor graphs associated to logmelspec, Open-L 3 , BYOL, CREPE, wav2vec2, GURA, and YAMNet. Although GEAR currently lacks mathematical refinement, we intend it as a proof of concept to show the potential of parametric audio synthesis in general-purpose machine listening research.","From HEAR to GEAR: Generative Evaluation of Audio Representations","Vincent Lostanlen, Lingyao Yan, Xianyi Yang","3816","De HEAR à GEAR : évaluation générative des représentations audio

L'""évaluation holistique des représentations audio"" (HEAR) est un programme de recherche émergent vers des modèles statistiques qui peuvent être transférés à diverses tâches d'écoute automatique. L'originalité de HEAR est d'effectuer une comparaison équitable, ""pommes à pommes"", de nombreux modèles d'apprentissage profond sur de nombreux ensembles de données, ce qui permet d'obtenir des mesures d'évaluation multitâches facilement interprétables par les praticiens. En revanche, cette comparaison implique une recherche d'architecture neuronale : en tant que telle, elle n'est pas directement interprétable en termes de traitement du signal audio. Dans cet article, nous proposons un point de vue complémentaire sur le benchmark HEAR, que nous appelons GEAR : Generative Evaluation of Audio Representations (évaluation générative des représentations audio). L'idée clé de GEAR est de générer un ensemble de données sonores avec peu de facteurs indépendants de variabilité, de l'analyser avec les encastrements HEAR et de le visualiser à l'aide d'un algorithme d'apprentissage manifold non supervisé. L'inspection visuelle révèle des contrastes marqués dans la structure globale des graphes des plus proches voisins associés à logmelspec, Open-L 3 , BYOL, CREPE, wav2vec2, GURA et YAMNet. Bien que GEAR manque actuellement de raffinement mathématique, nous l'envisageons comme une preuve de concept pour montrer le potentiel de la synthèse audio paramétrique dans la recherche sur l'écoute automatique à usage général.","18632","3","248","De HEAR à GEAR : évaluation générative des représentations audio

L&#039;&quot;évaluation holistique des représentations audio&quot; (HEAR, Holistic Evaluation of Audio Representations) est un programme de recherche émergent vers des modèles statistiques qui peuvent être transférés à diverses tâches d&#039;écoute automatique. L&#039;originalité de HEAR est d&#039;effectuer une comparaison équitable de nombreux modèles d&#039;apprentissage profond sur de nombreux ensembles de données, ce qui permet d&#039;obtenir des mesures d&#039;évaluation multitâches facilement interprétables par les praticiens. En revanche, cette comparaison implique une recherche d&#039;architecture neuronale : en tant que telle, elle n&#039;est pas directement interprétable en termes de traitement du signal audio. Dans cet article, nous proposons un point de vue complémentaire sur le benchmark HEAR, que nous appelons GEAR : Generative Evaluation of Audio Representations (évaluation générative des représentations audio). L&#039;idée clé de GEAR est de générer un ensemble de données sonores avec peu de facteurs indépendants de variabilité, de l&#039;analyser avec les plongements HEAR et de le visualiser à l&#039;aide d&#039;un algorithme d&#039;apprentissage varié et non supervisé. L&#039;inspection visuelle révèle des contrastes marqués dans la structure globale des graphes des plus proches voisins associés à logmelspec, Open-L 3 , BYOL, CREPE, wav2vec2, GURA et YAMNet. Bien que GEAR manque actuellement de raffinement mathématique, nous l&#039;envisageons comme une preuve de concept pour montrer le potentiel de la synthèse audio paramétrique dans la recherche sur l&#039;écoute automatique à usage général.","2","2023-06-27 19:29:50","2023-06-27 19:41:45","18632",,"1","1","4","3","1","3",NULL,,"2023-06-27 07:29:44","0"
"3994702","When extracting structured data from repetitively organized documents, such as dictionaries, directories, or even newspapers, a key challenge is to correctly segment what constitutes the basic text regions for the target database. Traditionally, such a problem was tackled as part of the layout analysis and was mostly based on visual clues for dividing (top-down) approaches. Some agglomerating (bottom-up) approaches started to consider textual information to link similar contents, but they required a proper over-segmentation of ne-grained units. In this work, we propose a new pragmatic approach whose eciency is demonstrated on 19 th century French Trade Directories. We propose to consider two sub-problems: coarse layout detection (text columns and reading order), which is assumed to be eective and not detailed here, and a ne-grained entry separation stage for which we propose to adapt a state-of-the-art Named Entity Recognition (NER) approach. By injecting special visual tokens, coding, for instance, indentation or breaks, into the token stream of the language model used for NER purpose, we can leverage both textual and visual knowledge simultaneously. Code, data, results and models are available at https://github.com/soduco/ paper-entryseg-icdar23-code, https://huggingface.co/HueyNemud/ (icdar23-entrydetector* variants).","Entry Separation using a Mixed Visual and Textual Language Model: Application to 19th century French Trade Directories","Bertrand Duménieu, Edwin Carlinet, Nathalie Abadie, Joseph Chazalon","350","Séparation des entrées à l'aide d'un modèle de langage mixte : Application aux répertoires commerciaux français du XIXe siècle

Lors de l'extraction de données structurées à partir de documents organisés de façon répétitive, tels que des dictionnaires, des répertoires ou même des journaux, un défi majeur consiste à segmenter correctement les zones de texte de base de la base de données cible. Traditionnellement, un tel problème était abordé dans le cadre de l'analyse de la mise en page et était principalement basé sur des indices visuels pour diviser (top-down) approches. Certaines approches d'agglomération (ascendante) ont commencé à considérer l'information textuelle pour relier des contenus similaires, mais elles exigeaient une sursegmentation appropriée des unités à grains nouveaux. Dans ce travail, nous proposons une nouvelle approche pragmatique dont la compétence est démontrée sur les répertoires français du 19ème siècle. Nous proposons d'examiner deux sous-problèmes : détection d'une mise en page grossière (colonnes de texte et ordre de lecture), supposée efficace et non détaillée ici, et une étape de séparation d'entrée à grain unique pour laquelle nous proposons d'adapter une approche NER (Named Entity Recognition) de pointe. En injectant des jetons visuels spéciaux, en codant, par exemple, en indentant ou en cassant, dans le flux de jetons du modèle de langage utilisé pour les besoins du NER, nous pouvons exploiter simultanément la connaissance textuelle et visuelle. Le code, les données, les résultats et les modèles sont disponibles à l'adresse suivante : https://github.com/soduco/ paper-entryseg-icdar23-code, https://huggingface.co/HueyNemud/ (icdar23-entrydetecteurs* variantes).","4097","6","249","Séparation des entrées à l&#039;aide d&#039;un modèle de langue visuel et textuel : Application aux répertoires commerciaux français du XIXᵉ siècle

Lors de l&#039;extraction de données structurées à partir de documents organisés de façon répétitive, tels que des dictionnaires, des répertoires ou même des journaux, un défi majeur consiste à segmenter correctement les zones de texte de base pour la base de données cible. Traditionnellement, un tel problème était abordé dans le cadre de l&#039;analyse de la mise en page et était principalement basé sur des indices visuels pour des approches de division (descendante). Certaines approches d&#039;agglomération (ascendante) ont commencé à considérer l&#039;information textuelle pour relier des contenus similaires, mais elles exigeaient une sursegmentation appropriée des unités fines. Dans ce travail, nous proposons une nouvelle approche pragmatique dont la compétence est démontrée sur les répertoires français du XIXᵉ siècle. Nous proposons d&#039;examiner deux sous-problèmes : détection d&#039;une mise en page grossière (colonnes de texte et ordre de lecture), supposée efficace et non détaillée ici, et une étape de séparation d&#039;entrée à grain fin pour laquelle nous proposons d&#039;adapter une approche de reconnaissance des entités nommées (NER, Named Entity Recognition) de pointe. En injectant des jetons visuels spéciaux, en codant, par exemple, des indentations ou des instructions break, dans le flux de jetons du modèle de langage utilisé pour les besoins du NER, nous pouvons exploiter simultanément la connaissance textuelle et visuelle. Le code, les données, les résultats et les modèles sont disponibles à l&#039;adresse suivante : https://github.com/soduco/ paper-entryseg-icdar23-code, https://huggingface.co/HueyNemud/ (icdar23-entrydetecteurs* variantes).","2","2023-06-28 15:32:48","2023-06-28 15:46:53","4097",,"3","2","4","3","4","2",NULL,,"2023-06-28 03:32:34","0"
"3997713","In Virtual Reality (VR), a growing number of applications involve verbal communications with avatars, such as for teleconference, entertainment, virtual training, social networks, etc. In this context, our paper aims to investigate how tactile feedback consisting in vibrations synchronized with speech could influence aspects related to VR social interactions such as persuasion, co-presence and leadership. We conducted two experiments where participants embody a first-person avatar attending a virtual meeting in immersive VR. In the first experiment, participants were listening to two speaking virtual agents and the speech of one agent was augmented with vibrotactile feedback. Interestingly, the results show that such vibrotactile feedback could significantly improve the perceived co-presence but also the persuasiveness and leadership of the haptically-augmented agent. In the second experiment, the participants were asked to speak to two agents, and their own speech was augmented or not with vibrotactile feedback. The results show that vibrotactile feedback had again a positive effect on co-presence, and that participants perceive their speech as more persuasive in presence of haptic feedback. Taken together, our results demonstrate the strong potential of haptic feedback for supporting social interactions in VR, and pave the way to novel usages of vibrations in a wide range of applications in which verbal communication plays a prominent role.","Persuasive Vibrations: Effects of Speech-Based Vibrations on Persuasion, Leadership, and Co-Presence During Verbal Communication in VR","Justine Saint-Aubert, Ferran Argelaguet, Marc J.-M. Macé, Claudio Pacchierotti, Amir Amedi, Anatole Lécuyer","3771","Vibrations persuasives: Effets des vibrations basées sur la parole sur la persuasion, le leadership et la co-présence pendant la communication verbale en VR

Dans la réalité virtuelle (VR), un nombre croissant d’applications impliquent des communications verbales avec des avatars, comme pour la téléconférence, le divertissement, la formation virtuelle, les réseaux sociaux, etc. Dans ce contexte, notre article vise à étudier comment la rétroaction tactile composée de vibrations synchronisées avec la parole pourrait influencer des aspects liés aux interactions sociales de la réalité virtuelle telles que la persuasion, la coprésence et le leadership. Nous avons mené deux expériences où les participants incarnent un avatar à la première personne assistant à une réunion virtuelle en VR immersive. Dans la première expérience, les participants écoutaient deux agents virtuels parlants et le discours d’un agent a été augmenté avec la rétroaction vibrotactile. Fait intéressant, les résultats montrent qu’une telle rétroaction vibrotactile pourrait améliorer considérablement la coprésence perçue, mais aussi la persuasion et le leadership de l’agent haptiquement augmenté. Dans la deuxième expérience, les participants ont été invités à parler à deux agents, et leur propre discours a été augmenté ou non avec la rétroaction vibrotactile. Les résultats montrent que la rétroaction vibrotactile a de nouveau eu un effet positif sur la coprésence, et que les participants perçoivent leur discours comme plus persuasif en présence de rétroaction haptique. Ensemble, nos résultats démontrent le fort potentiel de rétroaction haptique pour soutenir les interactions sociales en RV, et ouvrent la voie à de nouvelles utilisations des vibrations dans un large éventail d’applications dans lesquelles la communication verbale joue un rôle de premier plan.","12729","4","253","Vibrations persuasives : Effets des vibrations synchronisées avec la parole sur la persuasion, le leadership et la coprésence pendant la communication verbale en RV

Dans la réalité virtuelle (RV), un nombre croissant d’applications impliquent des communications verbales avec des avatars, comme pour la téléconférence, le divertissement, la formation virtuelle, les réseaux sociaux, etc. Dans ce contexte, notre article vise à étudier comment la rétroaction tactile composée de vibrations synchronisées avec la parole pourrait influencer des aspects liés aux interactions sociales de la réalité virtuelle telles que la persuasion, la coprésence et le leadership. Nous avons mené deux expériences où les participants incarnent un avatar à la première personne assistant à une réunion virtuelle en RV immersive. Dans la première expérience, les participants écoutaient deux agents virtuels parlants et le discours d’un agent a été augmenté avec la rétroaction vibrotactile. Fait intéressant, les résultats montrent qu’une telle rétroaction vibrotactile pourrait améliorer considérablement la coprésence perçue, mais aussi la persuasion et le leadership de l’agent haptiquement augmenté. Dans la deuxième expérience, les participants ont été invités à parler à deux agents, et leur propre discours a été augmenté ou non avec la rétroaction vibrotactile. Les résultats montrent que la rétroaction vibrotactile a de nouveau eu un effet positif sur la coprésence, et que les participants perçoivent leur discours comme plus persuasif en présence de rétroaction haptique. Ensemble, nos résultats démontrent le fort potentiel de rétroaction haptique pour soutenir les interactions sociales en RV, et ouvrent la voie à de nouvelles utilisations des vibrations dans un large éventail d’applications dans lesquelles la communication verbale joue un rôle de premier plan.","2","2023-06-28 17:30:06","2023-06-28 17:38:34","12729",,"1","2","3","2","1","4",NULL,,"2023-06-28 05:29:45","0"
"3936036","As a web search engine can find relevant sources for a keyword query, the web of data needs a source selection engine to find relevant endpoints for a SPARQL query. However, source selection requires getting information about the content of endpoints and currently, it remains difficult to automatically explore the content of endpoints as web robots explore the content of web servers. Thanks to the web preemption principle, we propose to automatically build RDF summaries of endpoints through SPARQL queries. We propose an approach to compute the source selection of a query Q by evaluating a rewriting of Q on summaries. As all queries terminate under the web preemption paradigm,DeKaloG can provide a web automated source selection service relying on preemptable SPARQL servers. We empirically demonstrate that various summaries can be extracted with a data transfer proportional to the size of the summary, and highlight the trade-off between the size of the summaries, the accuracy of source selection, and the execution time of source selection.","DeKaloG: Source Selection for Decentralized Knowledge Graphs","Julien Aimonier-Davat, Minh Dang, Hala Skaf-Molli, Pascal Molli","3916","Dekalog: Sélection des sources pour les graphiques de connaissances décentralisés

Comme un moteur de recherche Web peut trouver des sources pertinentes pour une requête par mot-clé, le web de données a besoin d’un moteur de sélection de source pour trouver des points de terminaison pertinents pour une requête SPARQL. Cependant, la sélection des sources nécessite d’obtenir des informations sur le contenu des points de terminaison et il reste actuellement difficile d’explorer automatiquement le contenu des points de terminaison lorsque les robots Web explorent le contenu des serveurs Web. Grâce au principe de préemption web, nous proposons de créer automatiquement des résumés RDF des points de terminaison via des requêtes SPARQL. Nous proposons une approche pour calculer la sélection source d’une requête Q en évaluant une réécriture de Q sur les résumés. Comme toutes les requêtes se terminent sous le paradigme de la préemption Web, Dekalog peut fournir un service de sélection de sources automatisée en s’appuyant sur des serveurs SPARQL préemptables. Nous démontrons empiriquement que divers résumés peuvent être extraits avec un transfert de données proportionnel à la taille du résumé, et mettre en évidence le compromis entre la taille des résumés, l’exactitude de la sélection de la source et le temps d’exécution de la sélection de la source.","12874","4","255","DeKaLog : Sélection des sources pour les graphes de connaissances décentralisés

Tout comme un moteur de recherche Web peut trouver des sources pertinentes pour une requête par mot-clé, le web des données a besoin d’un moteur de sélection de source pour trouver des point d&#039;accès pertinents lors d&#039;une requête SPARQL. Cependant, la sélection des sources nécessite d’obtenir des informations sur le contenu des points d&#039;accès, et il reste pour l&#039;instant difficile d’explorer automatiquement ce contenu lorsque les robots d&#039;indexation explorent le contenu des serveurs Web. Grâce au principe de préemption web, nous proposons de créer automatiquement des résumés RDF des points d&#039;accès via des requêtes SPARQL. Nous proposons une approche pour calculer la sélection source d’une requête R en évaluant une réécriture de R sur les résumés. Comme toutes les requêtes se terminent sous le paradigme de la préemption Web, DeKaLog peut fournir un service de sélection de sources automatisée en s’appuyant sur des serveurs SPARQL préemptables. Nous démontrons empiriquement que divers résumés peuvent être extraits avec un transfert de données proportionnel à la taille du résumé, et mettre en évidence le compromis entre la taille des résumés, l’exactitude de la sélection de la source et le temps d’exécution de la sélection de la source.","2","2023-06-28 17:42:26","2023-06-28 17:54:00","12874",,"3","1","4","4","1","4",NULL,,"2023-06-28 05:42:22","0"
"3973654","In the Humanities, the emergence of digital methods has opened up research questions to quantitative analysis. This is why HTR technology is increasingly involved in humanities research projects following precursors such as the Himanis project. However, many research teams have limited resources, either financially or in terms of their expertise in artificial intelligence. It may therefore be difficult to integrate handwritten text recognition into their project pipeline if they need to train a model or to create data from scratch. The goal here is not to explain how to build or improve a new HTR engine, nor to find a way to automatically align a preexisting corpus with an image to quickly create ground truths for training. This paper aims to help humanists easily develop an HTR model for medieval manuscripts, create and gather training data by knowing the issues underlying their choices. The objective is also to show the importance of the constitution of consistent data as a prerequisite to allow their gathering and to train efficient HTR models. We will present an overview of our work and experiment in the CREMMALab project (2021-2022), showing first how we ensure the consistency of the data and then how we have developed a generic model for medieval French manuscripts from the 13 th to the 15 th century, ready to be shared (more than 94% accuracy) and/or fine-tuned by other projects.","Generic HTR Models for Medieval Manuscripts The CREMMALab Project","Ariane Pinche","375","Modèles HTR génériques pour les manuscrits médiévaux Le projet CREMMALab

Dans les sciences humaines, l’émergence des méthodes numériques a ouvert des questions de recherche à l’analyse quantitative. C’est pourquoi la technologie HTR est de plus en plus impliquée dans des projets de recherche en sciences humaines à la suite de précurseurs tels que le projet Himanis. Cependant, de nombreuses équipes de recherche disposent de ressources limitées, que ce soit financièrement ou en termes d’expertise en intelligence artificielle. Il peut donc être difficile d’intégrer la reconnaissance manuscrite de texte dans leur portefeuille de projets s’ils ont besoin de former un modèle ou de créer des données à partir de zéro. L’objectif ici n’est pas d’expliquer comment construire ou améliorer un nouveau moteur HTR, ni de trouver un moyen d’aligner automatiquement un corpus préexistant avec une image pour créer rapidement des vérités de terrain pour l’entraînement. Cet article vise à aider les humanistes à développer facilement un modèle HTR pour les manuscrits médiévaux, à créer et à recueillir des données de formation en connaissant les enjeux sous-jacents à leurs choix. L’objectif est également de montrer l’importance de la constitution de données cohérentes comme condition préalable à leur collecte et à la formation de modèles HTR efficaces. Nous présenterons un aperçu de notre travail et de notre expérience dans le projet CREMMALab (2021-2022), montrant d’abord comment nous assurons la cohérence des données, puis comment nous avons développé un modèle générique pour les manuscrits français médiévaux du 13ème au 15ème siècle, prêts à être partagés (plus de 94 % de précision) et/ou affinés par d’autres projets.","3129","4","257","Modèles HTR génériques pour les manuscrits médiévaux : Le projet CREMMALab

Dans les sciences humaines, l’émergence des méthodes numériques a ouvert des questions de recherche à l’analyse quantitative. C’est pourquoi la technologie de reconnaissance de l’écriture manuscrite est de plus en plus impliquée dans des projets de recherche en sciences humaines à la suite de précurseurs tels que le projet Himanis. Cependant, de nombreuses équipes de recherche disposent de ressources limitées, que ce soit financièrement ou en termes d’expertise en intelligence artificielle. Il peut donc être difficile d’intégrer la reconnaissance de l’écriture manuscrite dans leur portefeuille de projets s’ils ont besoin de former un modèle ou de créer des données à partir de zéro. L’objectif ici n’est pas d’expliquer comment construire ou améliorer un nouveau moteur de reconnaissance de l’écriture manuscrite, ni de trouver un moyen d’aligner automatiquement un corpus préexistant sur une image pour créer rapidement des vérités terrain pour l’entraînement. Cet article vise à aider les humanistes à développer facilement un modèle de reconnaissance de l’écriture manuscrite pour les manuscrits médiévaux, à créer et à recueillir des données de formation en connaissant les enjeux sous-jacents à leurs choix. L’objectif est également de montrer l’importance de la constitution de données cohérentes comme condition préalable à leur collecte et à la formation de modèles de reconnaissance de l’écriture manuscrite efficaces. Nous présenterons un aperçu de notre travail et de notre expérience dans le projet CREMMALab (2021-2022), montrant d’abord comment nous assurons la cohérence des données, puis comment nous avons développé un modèle générique pour les manuscrits français médiévaux du XIIIᵉ au XVᵉ siècle, prêts à être partagés (plus de 94 % de précision) et/ou affinés par d’autres projets.","2","2023-06-28 17:58:24","2023-06-28 18:04:39","3129",,"2","1","4","3","3","3",NULL,,"2023-06-28 05:58:11","0"
"3995339","Non-autoregressive machine translation (NAT) has recently made great progress. However, most works to date have focused on standard translation tasks, even though some edit-based NAT models, such as the Levenshtein Transformer (LevT), seem well suited to translate with a Translation Memory (TM). This is the scenario considered here. We first analyze the vanilla LevT model and explain why it does not do well in this setting. We then propose a new variant, TM-LevT, and show how to effectively train this model. By modifying the data presentation and introducing an extra deletion operation, we obtain performance that are on par with an autoregressive approach, while reducing the decoding load. We also show that incorporating TMs during training dispenses to use knowledge distillation, a well-known trick used to mitigate the multimodality issue.","Integrating Translation Memories into Non-Autoregressive Machine Translation","Jitao Xu, Josep Crego, François Yvon","343","Intégration des mémoires de traduction dans la traduction automatique non-autorégressive

La traduction automatique non autorégressive (NAT) a récemment fait de grands progrès. Cependant, la plupart des travaux à ce jour se sont concentrés sur des tâches de traduction standard, même si certains modèles NAT basés sur l’édition, tels que le Levenshtein Transformer (LevT), semblent bien adaptés à la traduction avec une mémoire de traduction (TM). C’est le scénario considéré ici. Nous analysons d’abord le modèle de vanille LevT et expliquons pourquoi il ne fonctionne pas bien dans ce cadre. Nous proposons ensuite une nouvelle variante, TM-LevT, et montrons comment former efficacement ce modèle. En modifiant la présentation des données et en introduisant une opération de suppression supplémentaire, nous obtenons des performances équivalentes à une approche autorégressive, tout en réduisant la charge de décodage. Nous montrons également que l’intégration des MT lors des dispenses de formation permet d’utiliser la distillation des connaissances, une astuce bien connue utilisée pour atténuer la question de la multimodalité.","3097","4","260","Intégration des mémoires de traduction dans la traduction automatique non-autorégressive

La traduction automatique non autorégressive (NAT, non-autoregressive machine translation) a récemment fait de grands progrès. Cependant, la plupart des travaux à ce jour se sont concentrés sur des tâches de traduction standard, même si certains modèles de NAT basés sur l’édition, tels que le Levenshtein Transformer (LevT), semblent bien adaptés à la traduction avec une mémoire de traduction (MT). C’est le scénario considéré ici. Nous analysons d’abord le modèle de base LevT et expliquons pourquoi il ne fonctionne pas bien dans ce cadre. Nous proposons ensuite une nouvelle variante, TM-LevT, et montrons comment former efficacement ce modèle. En modifiant la présentation des données et en introduisant une opération de suppression supplémentaire, nous obtenons des performances équivalentes à une approche autorégressive, tout en réduisant la charge de décodage. Nous montrons également que l’intégration des MT lors des dispenses de formation permet d’utiliser la distillation des connaissances, une astuce bien connue utilisée pour atténuer la question de la multimodalité.","2","2023-06-28 21:48:39","2023-06-28 21:53:49","3097",,"1","1","3","4","1","2",NULL,,"2023-06-28 09:48:19","0"
"3929913","Over recent years, we witnessed an astonishing growth in production and consumption of Linked Data (LD), which contains valuable information to support decision-making processes in various application domains. In this context, data visualization plays a decisive role in making sense of the large volumes of data created every day and in effectively communicating structures, processes, and trends in data in an accessible way. In this paper, we present LDViz, a visualization tool designed to support the exploration of knowledge graphs via multiple perspectives: (i) RDF graph/vocabulary inspection, (ii) RDF summarization, and (iii) exploratory search. We demonstrate the usage and feasibility of our approach through a set of use case scenarios showing how users can perform searches through SPARQL queries and explore multiple perspectives of the resulting data through multiple complementary visualization techniques. We also demonstrate the reach and generic aspects of our tool through an evaluation that tests the support of 419 different SPARQL endpoints.","LDViz: a tool to assist the multidimensional exploration of SPARQL endpoints","Aline Menin, Pierre Maillot, Catherine Faron, Olivier Corby, Carla Dal Sasso Freitas, Fabien Gandon, Marco Winckler","3938","LDViz: un outil pour faciliter l’exploration multidimensionnelle des points de terminaison SPARQL

Au cours des dernières années, nous avons assisté à une croissance étonnante de la production et de la consommation de données liées (LD), qui contient des informations précieuses pour soutenir les processus décisionnels dans divers domaines d’application. Dans ce contexte, la visualisation des données joue un rôle décisif en donnant un sens aux grands volumes de données créés chaque jour et en communiquant efficacement les structures, les processus et les tendances des données de manière accessible. Dans cet article, nous présentons LDViz, un outil de visualisation conçu pour soutenir l’exploration des graphes de connaissances via de multiples perspectives: (I) l’inspection graphique/vocabulaire RDF, (ii) la synthèse de RDF, et (iii) la recherche exploratoire. Nous démontrons l’utilisation et la faisabilité de notre approche à travers un ensemble de scénarios de cas d’utilisation montrant comment les utilisateurs peuvent effectuer des recherches par le biais de requêtes SPARQL et explorer de multiples perspectives des données résultantes grâce à de multiples techniques de visualisation complémentaires. Nous démontrons également la portée et les aspects génériques de notre outil à travers une évaluation qui teste le support de 419 paramètres SPARQL différents.","12896","4","261","LDViz : un outil pour faciliter l’exploration multidimensionnelle des points d&#039;accès SPARQL

Au cours des dernières années, nous avons assisté à une croissance étonnante de la production et de la consommation de données liées (LD, Linked Data), qui contiennent des informations précieuses pour soutenir les processus décisionnels dans divers domaines d’application. Dans ce contexte, la visualisation des données joue un rôle décisif en donnant un sens aux grands volumes de données créés chaque jour et en communiquant efficacement les structures, les processus et les tendances des données de manière accessible. Dans cet article, nous présentons LDViz, un outil de visualisation conçu pour soutenir l’exploration des graphes de connaissances via de multiples perspectives : (i) l’inspection graphique/vocabulaire RDF, (ii) la synthèse de RDF, et (iii) la recherche exploratoire. Nous démontrons l’utilisation et la faisabilité de notre approche à travers un ensemble de scénarios de cas d’utilisation montrant comment les utilisateurs peuvent effectuer des recherches par le biais de requêtes SPARQL et explorer de multiples perspectives des données résultantes grâce à de multiples techniques de visualisation complémentaires. Nous démontrons également la portée et les aspects génériques de notre outil à travers une évaluation qui teste le support de 419 paramètres SPARQL différents.","2","2023-06-28 22:05:37","2023-06-28 22:08:49","12896",,"1","4","3","1","4","1",NULL,,"2023-06-28 10:05:26","0"
"3946680","In recent years, a large number of RDF datasets have been built and published on the Web in fields as diverse as linguistics or life sciences, as well as general datasets such as DBpedia or Wikidata. The joint exploitation of these datasets requires specific knowledge about their content, access points, and commonalities. However, not all datasets contain a self-description, and not all access points can handle the complex queries used to generate such a description. In this article, we provide a standard-based approach to generate the description of a dataset. The generated descriptions as well as the process of their computation are expressed using standard vocabularies and languages. We implemented our approach into a framework, called IndeGx, where each indexing feature and its computation is collaboratively and declaratively defined in a GitHub repository. We have experimented IndeGx on a set of 339 RDF datasets with endpoints listed in public catalogs, over 8 months. The results show that we can collect, as much as possible, important characteristics of the datasets depending on their availability and capacities. The resulting index captures the commonalities, variety and disparity in the offered content and services and it provides an important support to any application designed to query RDF datasets.","IndeGx: A Model and a Framework for Indexing RDF Knowledge Graphs with SPARQL-based Test Suits","Pierre Maillot, Olivier Corby, Catherine Faron, Fabien Gandon, Franck Michel","3877","IndexGx : Modèle et cadre d'indexation de graphiques de connaissances RDF avec des combinaisons de test SPARQL

Ces dernières années, un grand nombre d'ensembles de données RDF ont été construits et publiés sur le Web dans des domaines aussi variés que la linguistique ou les sciences de la vie, ainsi que des ensembles de données généraux tels que DBpedia ou Wikidata. L'exploitation conjointe de ces ensembles de données nécessite une connaissance spécifique de leur contenu, de leurs points d'accès et de leurs points communs. Cependant, tous les jeux de données ne contiennent pas d'auto-description et tous les points d'accès ne peuvent pas gérer les requêtes complexes utilisées pour générer une telle description. Dans cet article, nous proposons une approche standard pour générer la description d'un ensemble de données. Les descriptions générées ainsi que le processus de leur calcul sont exprimés à l'aide de vocabulaires et de langues standard. Nous avons implémenté notre approche dans un framework, appelé IndexGx, où chaque fonction d'indexation et son calcul sont définis de manière collaborative et déclarative dans un dépôt GitHub. Nous avons expérimenté IndeGx sur un ensemble de 339 ensembles de données RDF avec des points finaux listés dans les catalogues publics, sur 8 mois. Les résultats montrent que nous pouvons recueillir, autant que possible, des caractéristiques importantes des ensembles de données en fonction de leur disponibilité et de leurs capacités. L'index qui en résulte capture les points communs, la variété et la disparité dans le contenu et les services offerts, et il fournit un support important à toute application conçue pour interroger des ensembles de données RDF.","15793","6","262","IndexGx : Modèle et cadre d&#039;indexation de graphes de connaissances RDF avec des combinaisons de test SPARQL

Ces dernières années, un grand nombre d&#039;ensembles de données RDF ont été construits et publiés sur le Web dans des domaines aussi variés que la linguistique ou les sciences de la vie, ainsi que des ensembles de données généraux tels que DBpedia ou Wikidata. L&#039;exploitation conjointe de ces ensembles de données nécessite une connaissance spécifique de leur contenu, de leurs points d&#039;accès et de leurs points communs. Cependant, tous les jeux de données ne contiennent pas d&#039;autodescription et tous les points d&#039;accès ne peuvent pas gérer les requêtes complexes utilisées pour générer une telle description. Dans cet article, nous proposons une approche standard pour générer la description d&#039;un ensemble de données. Les descriptions générées ainsi que le processus de leur calcul sont exprimés à l&#039;aide de vocabulaires et de langues standard. Nous avons implémenté notre approche dans un framework, appelé IndexGx, où chaque fonction d&#039;indexation et son calcul sont définis de manière collaborative et déclarative dans un dépôt GitHub. Nous avons expérimenté IndeGx sur un ensemble de 339 ensembles de données RDF avec des points finaux listés dans les catalogues publics, sur 8 mois. Les résultats montrent que nous pouvons recueillir, autant que possible, des caractéristiques importantes des ensembles de données en fonction de leur disponibilité et de leurs capacités. L&#039;index qui en résulte capture les points communs, la variété et la disparité dans le contenu et les services offerts, et il fournit un support important à toute application conçue pour interroger des ensembles de données RDF.","2","2023-06-28 22:09:31","2023-06-28 22:11:35","15793",,"1","1","3","1","1","1",NULL,,"2023-06-28 10:09:20","0"
"3972415","Modeling virtual agents with behavior style is one factor for personalizing human-agent interaction. In this paper, we propose an efficient yet effective machine learning approach to synthesize gestures driven by prosodic features and text in the style of different speakers including those unseen during training. Our model performs zero-shot multimodal style transfer driven by multimodal data from the PATS database containing videos of various speakers. We view style as being pervasive while speaking; it colors the communicative behaviors expressivity while speech content is carried by multimodal signals and text. This disentanglement scheme of content and style allows us to directly infer the style embedding even of speaker whose data are not part of the training phase, without requiring any further training or fine-tuning. The first goal of our model is to generate the gestures of a source speaker based on the content of two input modalities-Mel spectrogram and text semantics. The second goal is to condition the source speaker's predicted gestures on the multimodal behavior style embedding of a target speaker. The third goal is to allow zero-shot style transfer of speakers unseen during training without retraining the model. Our system consists of two main components: (1) a speaker style encoder network that learns to generate a fixed-dimensional speaker embedding style from a target speaker multimodal data (mel-spectrogram, pose, and text); and (2) a sequence-to-sequence synthesis network that synthesizes gestures based on the content of the input modalities-text and mel-spectrogram-of a source speaker, and conditioned on the speaker style embedding. We evaluate that our model is able to synthesize gestures of a source speaker given the two input modalities, and transfer the knowledge of target speaker style variability learned by the speaker style encoder to the gesture generation task in a zero-shot setup, indicating that the model has learned a high quality speaker representation. For our evaluation we convert the 2D generated gestures to 3D poses, and produce 3D animations of the generated gestures. We conduct objective and subjective evaluations to validate our approach and compare it with baselines.","Zero-Shot Style Transfer for Gesture Animation driven by Text and Speech using Adversarial Disentanglement of Multimodal Style Encoding","Mireille Fares, Michele Grimaldi, Catherine Pelachaud, Nicolas Obin","3834","Transfert de style zéro instantané pour l’animation de gesture pilotée par le texte et la parole à l’aide d’un désanglement contradictoire du codage de style multimodal

La modélisation des agents virtuels avec le style de comportement est un facteur de personnalisation de l’interaction homme-agent. Dans cet article, nous proposons une approche d’apprentissage automatique efficace et efficace pour synthétiser des gestes animés par des caractéristiques prosodiques et du texte dans le style de différents locuteurs, y compris ceux qui n’ont pas été vus pendant la formation. Notre modèle effectue un transfert de style multimodal zéro-shot piloté par les données multimodales de la base de données PATS contenant des vidéos de différents haut-parleurs. Nous considérons le style comme étant omniprésent en parlant; il colore l’expressivité des comportements communicatifs tandis que le contenu de la parole est porté par des signaux multimodals et du texte. Ce schéma de démêlage de contenu et de style nous permet de déduire directement le style d’intégration même d’un locuteur dont les données ne font pas partie de la phase de formation, sans nécessiter de formation complémentaire ou de réglage fin. Le premier objectif de notre modèle est de générer les gestes d’un haut-parleur source basé sur le contenu de deux modalités d’entrée — le spectrogramme Mel et la sémantique textuelle. Le deuxième objectif est de conditionner les gestes prédits de l’orateur source sur le style de comportement multimodal d’un haut-parleur cible. Le troisième objectif est de permettre le transfert de style zéro-shot des haut-parleurs invisibles pendant l’entraînement sans recycler le modèle. Notre système se compose de deux composants principaux: (1) un réseau de codeurs de style de haut-parleur qui apprend à générer un locuteur de dimension fixe intégrant le style à partir d’une donnée multimodale de l’enceinte cible (mel-spectrogramme, pose et texte); et (2) un réseau de synthèse séquence-séquence qui synthétise des gestes basés sur le contenu des modalités d’entrée — texte et mel-spectrogramme — d’un haut-parleur source, et conditionné sur l’incorporation du style de l’enceinte. Nous évaluons que notre modèle est capable de synthétiser les gestes d’un haut-parleur source compte tenu des deux modalités d’entrée, et de transférer la connaissance de la variabilité du style de l’enceinte cible apprise par le codeur de style de l’enceinte à la tâche de génération de gestes dans une configuration zéro-shot, indiquant que le modèle a appris une représentation de haut-parleur de haute qualité. Pour notre évaluation, nous convertissons les gestes générés en 2D en poses 3D et produisons des animations 3D des gestes générés. Nous effectuons des évaluations objectives et subjectives pour valider notre approche et la comparer aux niveaux de référence.","12792","4","266","Transfert de style zero-shot pour l’animation de gestes pilotée par le texte et la parole à l’aide d’un désenchevêtrement adversarial de l&#039;encodage de style multimodal

La modélisation des agents virtuels avec le style de comportement est un facteur de personnalisation de l’interaction homme-agent. Dans cet article, nous proposons une approche d’apprentissage automatique efficace et efficace pour synthétiser des gestes animés par des caractéristiques prosodiques et du texte dans le style de différents locuteurs, y compris ceux qui n’ont pas été vus pendant la formation. Notre modèle effectue un transfert de style multimodal zero-shot piloté par les données multimodales de la base de données PATS contenant des vidéos de différents locuteurs. Nous considérons le style comme étant omniprésent dans le discours : il colore l’expressivité des comportements communicatifs, tandis que le contenu de la parole est porté par des signaux multimodaux et du texte. Ce schéma de désenchevêtrement de contenu et de style nous permet de déduire directement le style d’intégration même d’un locuteur dont les données ne font pas partie de la phase de formation, sans nécessiter de formation complémentaire ou de réglage fin. Le premier objectif de notre modèle est de générer les gestes d’un haut-parleur source basé sur le contenu de deux modalités d’entrée — le spectrogramme Mel et la sémantique textuelle. Le deuxième objectif est de conditionner les gestes prédits du locuteur source sur le style de comportement multimodal d’un haut-parleur cible. Le troisième objectif est de permettre le transfert de style zero-shot des locuteurs invisibles pendant l’entraînement sans recycler le modèle. Notre système se compose de deux composants principaux : (1) un réseau de codeurs de style de haut-parleur qui apprend à générer un locuteur de dimension fixe intégrant le style à partir d’une donnée multimodale du locuteur cible (mel-spectrogramme, pose et texte); et (2) un réseau de synthèse séquence-séquence qui synthétise des gestes basés sur le contenu des modalités d’entrée — texte et mel-spectrogramme — d’un haut-parleur source, et conditionné sur l’incorporation du style du locuteur. Nous évaluons que notre modèle est capable de synthétiser les gestes d’un locuteur source compte tenu des deux modalités d’entrée, et de transférer la connaissance de la variabilité du style du locuteur cible apprise par l&#039;encodeur de style du locuteur à la tâche de génération de gestes dans une configuration zero-shot, indiquant que le modèle a appris une représentation de locuteur de haute qualité. Pour notre évaluation, nous convertissons les gestes générés en 2D en poses 3D et produisons des animations 3D des gestes générés. Nous effectuons des évaluations objectives et subjectives pour valider notre approche et la comparer aux niveaux de référence.","2","2023-06-29 15:25:41","2023-06-29 15:46:16","12792",,"4","4","4","2","3","4",NULL,,"2023-06-29 03:25:23","0"
"3984703","The dramatic increase in the number of microbe descriptions in databases, reports, and papers presents a two-fold challenge for accessing the information: integration of heterogeneous data in a standard ontology-based representation and normalization of the textual descriptions by semantic analysis. Recent text mining methods offer powerful ways to extract textual information and generate ontology-based representation. This paper describes the design of the Omnicrobe application that gathers comprehensive information on habitats, phenotypes, and usages of microbes from scientific sources of high interest to the microbiology community. The Omnicrobe database contains around 1 million descriptions of microbe properties. These descriptions are created by analyzing and combining six information sources of various kinds, i.e. biological resource catalogs, sequence databases and scientific literature. The microbe properties are indexed by the Ontobiotope ontology and their taxa are indexed by an extended version of the taxonomy maintained by the National Center for Biotechnology Information. The Omnicrobe application covers all domains of microbiology. With simple or rich ontology-based queries, it provides easy-to-use support in the resolution of scientific questions related to the habitats, phenotypes, and uses of microbes. We illustrate the potential of Omnicrobe with a use case from the food innovation domain.","Omnicrobe, an open-access database of microbial habitats and phenotypes using a comprehensive text mining and data fusion approach","Sandra Dérozier, Robert Bossy, Louise Deléger, Mouhamadou Ba, Estelle Chaix, Olivier Harlé, Valentin Loux, Hélène Falentin, Claire Nédellec","365","Omnicrobe, une base de données en libre accès sur les habitats et les phénotypes microbiens utilisant une approche complète d'exploration de texte et de fusion de données.

L'augmentation spectaculaire du nombre de descriptions de microbes dans les bases de données, les rapports et les articles présente un double défi pour l'accès à l'information : l'intégration de données hétérogènes dans une représentation standard basée sur une ontologie et la normalisation des descriptions textuelles par l'analyse sémantique. Les méthodes récentes d'exploration de texte offrent des moyens puissants d'extraire des informations textuelles et de générer une représentation basée sur une ontologie. Cet article décrit la conception de l'application Omnicrobe qui rassemble des informations complètes sur les habitats, les phénotypes et les utilisations des microbes à partir de sources scientifiques présentant un grand intérêt pour la communauté microbiologique. La base de données Omnicrobe contient environ 1 million de descriptions des propriétés des microbes. Ces descriptions sont le fruit de l'analyse et de la combinaison de six sources d'information de nature différente, à savoir des catalogues de ressources biologiques, des bases de données de séquences et de la littérature scientifique. Les propriétés des microbes sont indexées par l'ontologie Ontobiotope et leurs taxons sont indexés par une version étendue de la taxonomie maintenue par le National Center for Biotechnology Information. L'application Omnicrobe couvre tous les domaines de la microbiologie. Avec des requêtes simples ou riches basées sur l'ontologie, elle fournit un support facile à utiliser dans la résolution de questions scientifiques liées aux habitats, aux phénotypes et aux utilisations des microbes. Nous illustrons le potentiel d'Omnicrobe à l'aide d'un cas d'utilisation dans le domaine de l'innovation alimentaire.","10119","3","268","Omnicrobe, une base de données en libre accès sur les habitats et les phénotypes microbiens utilisant une approche complète d&#039;exploration de texte et de fusion de données.

L&#039;augmentation spectaculaire du nombre de descriptions de microbes dans les bases de données, les rapports et les articles présente un double défi pour l&#039;accès à l&#039;information : l&#039;intégration de données hétérogènes dans une représentation standard basée sur une ontologie et la normalisation des descriptions textuelles par l&#039;analyse sémantique. Les méthodes récentes d&#039;exploration de texte offrent des moyens puissants d&#039;extraire des informations textuelles et de générer une représentation basée sur une ontologie. Cet article décrit la conception de l&#039;application Omnicrobe, qui rassemble des informations complètes sur les habitats, les phénotypes et les utilisations des microbes à partir de sources scientifiques présentant un grand intérêt pour la communauté microbiologique. La base de données Omnicrobe contient environ 1 million de descriptions des propriétés des microbes. Ces descriptions sont le fruit de l&#039;analyse et de la combinaison de six sources d&#039;information de nature différente, à savoir des catalogues de ressources biologiques, des bases de données de séquences et de la littérature scientifique. Les propriétés des microbes sont indexées par l&#039;ontologie Ontobiotope et leurs taxons sont indexés par une version étendue de la taxonomie maintenue par le National Center for Biotechnology Information. L&#039;application Omnicrobe couvre tous les domaines de la microbiologie. Avec des requêtes simples ou riches basées sur l&#039;ontologie, elle fournit un support facile à utiliser dans la résolution de questions scientifiques liées aux habitats, aux phénotypes et aux utilisations des microbes. Nous illustrons le potentiel d&#039;Omnicrobe à l&#039;aide d&#039;un cas d&#039;utilisation dans le domaine de l&#039;innovation alimentaire.","2","2023-06-29 15:49:17","2023-06-29 15:55:55","10119",,"1","1","1","1","2","1",NULL,,"2023-06-29 03:49:07","0"
"3980630","GitHub hosts Git repositories and provides issues-tracking services to provide a better collaboration environment for software developers. Issues and Pull-Requests are frequently used in GitHub to discuss and review the software requirements (new features, bugs, etc.) and software solutions (source code, test cases, etc.) respectively. The links between Issues and their corresponding Pull-Requests comprise valuable information to keep tracking current development as well as documenting knowledge for future development. Considering a large number of links, such information can be used to train machine learning models for several purposes such as feature location, bug prediction and localization, recommendation systems and documentation generation. To the best of our knowledge, no dataset has been proposed as a ground-truth of links between Issues and Pull-Requests. In this paper, we propose, PI-Link, a new significant and reliable ground-truth dataset composed of 50369 links that explicitly connect 34732 Issues with 50369 Pull-Requests. These links are automatically extracted from all (907,139) Android projects in GitHub created between January 1, 2011 and January 1, 2021. To better organize and store the collected data, we propose a metamodel based on the concepts of Issues and Pull Requests. Moreover, we analyze the relationships between Issues and their linked Pull Requests based on four features related to their titles, bodies, labels and comments. The selected features are analyzed in terms of their lengths and similarities based on three lexical and one semantic similarity metrics. The results showed promising similarities between Issues and their linked PRs at the lexical and semantic levels. In addition, some feature similarities are sensitive to the text length, whereas other feature similarities are sensitive to the term frequency.","PI-Link: A Ground-Truth Dataset of Links Between Pull-Requests and Issues in GitHub","Zakarea Alshara, Anas Shatnawi, Hamzeh Eyal-Salman, Abdelhak-Djamel Seriai, Maad Shatnawi","367","Pi-Link: Un ensemble de données de vérité au sol des liens entre les demandes de Pull et les problèmes dans GitHub

GitHub héberge des dépôts Git et fournit des services de suivi des problèmes pour fournir un meilleur environnement de collaboration aux développeurs de logiciels. Les problèmes et les demandes sont fréquemment utilisés dans GitHub pour discuter et examiner les exigences logicielles (nouvelles fonctionnalités, bogues, etc.) et les solutions logicielles (code source, cas de test, etc.) respectivement. Les liens entre les Problèmes et leurs Pull-Requests correspondants constituent des informations précieuses pour continuer à suivre l’évolution actuelle ainsi que documenter les connaissances pour le développement futur. Compte tenu d’un grand nombre de liens, ces informations peuvent être utilisées pour former des modèles d’apprentissage automatique à plusieurs fins telles que l’emplacement des fonctionnalités, la prédiction et la localisation des bugs, les systèmes de recommandation et la génération de documentation. À notre connaissance, aucun ensemble de données n’a été proposé comme une vérité de base des liens entre les enjeux et les demandes d’appels d’offres. Dans cet article, nous proposons, PI-Link, un nouvel ensemble de données significatif et fiable composé de 50369 liens qui relient explicitement 34732 Issues à 50369 Pull-Requests. Ces liens sont automatiquement extraits de tous les projets Android (907 139) dans GitHub créés entre le 1er janvier 2011 et le 1er janvier 2021. Pour mieux organiser et stocker les données collectées, nous proposons un métamodèle basé sur les concepts de Problèmes et de Demandes de Pull. De plus, nous analysons les relations entre les Problèmes et leurs Demandes de Pull liées en fonction de quatre caractéristiques liées à leurs titres, corps, étiquettes et commentaires. Les caractéristiques sélectionnées sont analysées en termes de longueurs et de similitudes sur la base de trois métriques de similitude lexicale et sémantique. Les résultats ont montré des similitudes prometteuses entre les enjeux et leurs relations publiques liées aux niveaux lexical et sémantique. En outre, certaines similitudes de caractéristiques sont sensibles à la longueur du texte, tandis que d’autres similitudes sont sensibles au terme fréquence.","3121","4","271","Pi-Link : Un ensemble de données de vérité terrain des liens entre les pull-requests et les issues sur GitHub

GitHub héberge des dépôts Git et fournit des services de suivi des problèmes pour fournir un meilleur environnement de collaboration aux développeurs de logiciels. Les issues et les pull-requests sont fréquemment utilisézs sur GitHub pour discuter et examiner les exigences logicielles (nouvelles fonctionnalités, bugs, etc.) et les solutions logicielles (code source, cas de test, etc.) respectivement. Les liens entre les issues et leurs pull-requests correspondantes constituent des informations précieuses pour continuer à suivre l’évolution actuelle ainsi que documenter les connaissances pour le développement à venir. Compte tenu d’un grand nombre de liens, ces informations peuvent être utilisées pour former des modèles d’apprentissage automatique à plusieurs fins telles que l’emplacement des fonctionnalités, la prédiction et la localisation des bugs, les systèmes de recommandation et la génération de documentation. À notre connaissance, aucun ensemble de données n’a été proposé comme une vérité terrain des liens entre les issues et les pull-requests. Dans cet article, nous proposons, PI-Link, un nouvel ensemble de données significatif et fiable composé de 50369 liens qui relient explicitement 34732 issues à 50369 pull-requests. Ces liens sont automatiquement extraits de tous les projets Android (907139) dans GitHub créés entre le 1ᵉʳ janvier 2011 et le 1ᵉʳ janvier 2021. Pour mieux organiser et stocker les données collectées, nous proposons un métamodèle fondé sur les concepts d&#039;issues et de pull-resquests. De plus, nous analysons les relations entre les issues et leurs pull-requests liées en fonction de quatre caractéristiques liées à leurs titres, corps, étiquettes et commentaires. Les caractéristiques sélectionnées sont analysées en termes de longueurs et de similitudes sur la base de trois métriques de similitude lexicale et sémantique. Les résultats ont montré des similitudes prometteuses entre les issues et leurs pull-requests liées aux niveaux lexical et sémantique. En outre, certaines similitudes de caractéristiques sont sensibles à la longueur du texte, tandis que d’autres similitudes sont sensibles au terme fréquence.","2","2023-06-29 16:03:06","2023-06-29 16:15:09","3121",,"4","2","3","3","2","4",NULL,,"2023-06-29 04:02:03","0"
"4002207","The automatic generation of explanations to improve the transparency of machine predictions is a major challenge in Artificial Intelligence. Such explanations may also be effectively applied to other decision making processes where it is crucial to improve critical thinking in human beings. An example of that consists in the clinical cases proposed to medical residents together with a set of possible diseases to be diagnosed, where only one correct answer exists. The main goal is not to identify the correct answer, but to be able to explain why one is the correct answer and the others are not. In this paper, we propose a novel approach to generate argument-based natural language explanations for the correct and incorrect answers of standardized medical exams. By combining information extraction methods from heterogeneous medical knowledge bases, we propose an automatic approach where the symptoms relevant to the correct diagnosis are automatically extracted from the case, to build a natural language explanation. To do so, we annotated a new resource of 314 clinical cases, where 1843 different symptoms are identified. Results in retrieving and matching the relevant symptoms for the clinical cases to support the correct diagnosis and contrast incorrect ones outperform standard baselines.","Natural Language Explanatory Arguments for Correct and Incorrect Diagnoses of Clinical Cases","Santiago Marro, Benjamin Molinet, Elena Cabrio, Serena Villata","3766","Arguments explicatifs en langage naturel pour des diagnostics corrects et incorrects de cas cliniques

La génération automatique d'explications pour améliorer la transparence des prédictions machine est un défi majeur de l'intelligence artificielle. De telles explications peuvent également être appliquées efficacement à d'autres processus décisionnels où il est crucial d'améliorer la pensée critique chez les êtres humains. Un exemple en est les cas cliniques proposés aux résidents médicaux, ainsi qu'un ensemble de maladies possibles à diagnostiquer, pour lesquelles il n'existe qu'une seule réponse correcte. L'objectif principal n'est pas d'identifier la bonne réponse, mais d'être en mesure d'expliquer pourquoi l'une est la bonne réponse et les autres ne le sont pas. Dans cet article, nous proposons une nouvelle approche pour générer des explications en langage naturel basées sur des arguments pour les réponses correctes et incorrectes des examens médicaux standardisés. En combinant des méthodes d'extraction d'informations à partir de bases de connaissances médicales hétérogènes, nous proposons une approche automatique où les symptômes pertinents au diagnostic correct sont automatiquement extraits du cas, pour construire une explication en langage naturel. Pour ce faire, nous avons annoté une nouvelle ressource de 314 cas cliniques, où 1843 symptômes différents sont identifiés. Obtient et compare les symptômes pertinents pour les cas cliniques afin d'appuyer le diagnostic correct et contraste les symptômes incorrects surpassent les valeurs de base standard.","15682","6","280","Arguments explicatifs en langue naturelle pour des diagnostics corrects et incorrects de cas cliniques

La génération automatique d&#039;explications pour améliorer la transparence des prédictions machine est un défi majeur de l&#039;intelligence artificielle. De telles explications peuvent également être appliquées efficacement à d&#039;autres processus décisionnels où il est crucial d&#039;améliorer la pensée critique chez les êtres humains. Un exemple en est les cas cliniques proposés aux médecins résidents, ainsi qu&#039;un ensemble de maladies possibles à diagnostiquer, pour lesquelles il n&#039;existe qu&#039;une seule réponse correcte. L&#039;objectif principal n&#039;est pas d&#039;identifier la bonne réponse, mais d&#039;être en mesure d&#039;expliquer pourquoi l&#039;une est la bonne réponse et les autres ne le sont pas. Dans cet article, nous proposons une nouvelle approche pour générer des explications en langue naturelle basées sur des arguments pour les réponses correctes et incorrectes des examens médicaux standardisés. En combinant des méthodes d&#039;extraction d&#039;informations à partir de bases de connaissances médicales hétérogènes, nous proposons une approche automatique où les symptômes pertinents au diagnostic correct sont automatiquement extraits du cas, pour construire une explication en langue naturelle. Pour ce faire, nous avons annoté une nouvelle ressource de 314 cas cliniques, où 1843 symptômes différents sont identifiés. Les résultats obtenus pour l&#039;extraction et la comparaison des symptômes pertinents pour les cas cliniques afin d&#039;appuyer le diagnostic correct et contraster les symptômes incorrects surpassent les valeurs de base standard.","2","2023-06-29 17:10:49","2023-06-29 17:24:47","15682",,"4","1","4","4","1","4",NULL,,"2023-06-29 05:10:44","0"
"3946223","Background In head and neck cancer, many tools exist to measure speech impairment, but few evaluate the impact on communication abilities. Some self-administered questionnaires are available to assess general activity limitations including communication. Others are not validated in oncology. These different tools result in scores that does not provide an accurate measure of the communication limitations perceived by the patients. Aim To develop a holistic score measuring the functional impact of speech disorders on communication in patients treated for oral or oropharyngeal cancer, in two steps: its construction and its validation.Methods & Procedures Patients treated for oral/oropharyngeal cancer filled six self-questionnaires: two about communicative dynamics (ECVB and DIP), two assessing speech function (PHI and CHI), and two relating to quality of life (EORTC QLQ-C30 and EORTC QLQ-H&N35). One hundred and seventy-four items were initially collected. A dimensionality reduction methodology was then applied. Face validity analysis led to eliminate non-relevant items by surveying a panel of nine experts from communication-related disciplines (linguistics, medicine, speech pathology, computer science). Construct validity analysis led to eliminate redundant and insufficiently variable items. Finally, the holistic communication score was elaborated by PCF (principal-component factor) and validated using cross-validation and latent profile analysis.Outcomes & Results Twenty-five patients filled the questionnaires (median age 67 years, EIQ 12; 15 men, 10 women; oral cavity 14, oropharynx 10, two locations 1). After face validity analysis, 44 items were retained (Kappa>0.80). Four additional items were excluded because of a very high correlation (r>0.90) with other items presenting a better dispersion. Forty items were finally included in the factor analysis. A post-analysis score prediction was performed (mean=100; standard deviation=10). Twenty-four items are finally retained for the construction of the holistic communication score (HoCoS): 19 items from questionnaires assessing communicative dynamics (13 from the ECVB and six from the DIP), four items from a perceived speech impairment questionnaire (PHI) and one from a quality-of-life questionnaire (EORTC QLQ-H&N35). The reliability is good (five-fold cross-validation: rs=0.91) and the complementary latent profile analysis shows a good validity of the HoCoS, clustering subjects by level of communication performance.Conclusions & Implications A global score allowing a measure of the impact of the speech disorder on communication was developed. It fills the lack of this type of score in head and neck oncology and allows to better understand the functional and psychosocial consequences of the pathology in the patients’ follow-up.","Development of a holistic communication score (HoCoS) in patients treated for oral or oropharyngeal cancer: Preliminary validation","Mathieu Balaguer, Julien Pinquier, Jérôme Farinas, Virginie Woisard","393","Développement d'un score de communication holistique (HoCoS) chez les patients traités pour un cancer de la bouche ou de l'oropharynx : Validation préliminaire

Contexte Dans le cas du cancer de la tête et du cou, il existe de nombreux outils pour mesurer les troubles de la parole, mais peu d'entre eux évaluent l'impact sur les capacités de communication. Certains questionnaires auto-administrés sont disponibles pour évaluer les limitations générales d'activité, y compris la communication. D'autres ne sont pas validés en oncologie. Ces différents outils aboutissent à des scores qui ne donnent pas une mesure précise des limitations de communication perçues par les patients. Objectif Développer un score holistique mesurant l'impact fonctionnel des troubles de la parole sur la communication chez les patients traités pour un cancer de la bouche ou de l'oropharynx, en deux étapes : sa construction et sa validation.Méthodes & Procédures Les patients traités pour un cancer de la bouche ou de l'oropharynx ont rempli six auto-questionnaires : deux sur la dynamique communicative (ECVB et DIP), deux évaluant la fonction de la parole (PHI et CHI), et deux relatifs à la qualité de vie (EORTC QLQ-C30 et EORTC QLQ-H&N35). Cent soixante-quatorze items ont été initialement collectés. Une méthodologie de réduction de la dimensionnalité a ensuite été appliquée. L'analyse de la validité apparente a permis d'éliminer les éléments non pertinents en interrogeant un panel de neuf experts issus de disciplines liées à la communication (linguistique, médecine, orthophonie, informatique). L'analyse de la validité de construction a permis d'éliminer les éléments redondants et insuffisamment variables. Enfin, le score de communication holistique a été élaboré par PCF (facteur à composante principale) et validé par validation croisée et analyse de profil latent.Résultats Vingt-cinq patients ont rempli les questionnaires (âge médian 67 ans, EIQ 12 ; 15 hommes, 10 femmes ; cavité buccale 14, oropharynx 10, deux localisations 1). Après analyse de la validité apparente, 44 items ont été retenus (Kappa>0,80). Quatre items supplémentaires ont été exclus en raison d'une corrélation très élevée (r>0,90) avec d'autres items présentant une meilleure dispersion. Quarante items ont finalement été inclus dans l'analyse factorielle. Une prédiction du score post-analyse a été réalisée (moyenne=100 ; écart-type=10). Vingt-quatre items sont finalement retenus pour la construction du score de communication holistique (HoCoS) : 19 items issus de questionnaires évaluant la dynamique communicative (13 de l'ECVB et six du DIP), quatre items issus d'un questionnaire sur la perception des troubles de la parole (PHI) et un item issu d'un questionnaire sur la qualité de vie (EORTC QLQ-H&N35). La fiabilité est bonne (validation croisée cinq fois : rs=0,91) et l'analyse complémentaire du profil latent montre une bonne validité du HoCoS, regroupant les sujets par niveau de performance de communication.Conclusions & Implications Un score global permettant de mesurer l'impact du trouble de la parole sur la communication a été développé. Il comble le manque de ce type de score en oncologie de la tête et du cou et permet de mieux comprendre les conséquences fonctionnelles et psychosociales de la pathologie dans le suivi des patients.","10147","3","281","Développement d&#039;un score de communication holistique (HoCoS) chez les patients traités pour un cancer de la bouche ou de l&#039;oropharynx : Validation préliminaire

Contexte
Dans le cas du cancer de la tête et du cou, il existe de nombreux outils pour mesurer les troubles de la parole, mais peu d&#039;entre eux évaluent l&#039;impact sur les capacités de communication. Certains questionnaires autoadministrés sont disponibles pour évaluer les limitations générales d&#039;activité, y compris la communication. D&#039;autres ne sont pas validés en oncologie. Ces différents outils aboutissent à des scores qui ne donnent pas une mesure précise des limitations de communication perçues par les patients.
Objectif
Développer un score holistique mesurant l&#039;impact fonctionnel des troubles de la parole sur la communication chez les patients traités pour un cancer de la bouche ou de l&#039;oropharynx, en deux étapes : sa construction et sa validation.
Méthodes &amp; Procédures
Les patients traités pour un cancer de la bouche ou de l&#039;oropharynx ont rempli six auto-uestionnaires : deux sur la dynamique communicative (ECVB et DIP), deux évaluant la fonction de la parole (PHI et CHI), et deux relatifs à la qualité de vie (EORTC QLQ-C30 et EORTC QLQ-H&amp;N35). Cent soixante-quatorze items ont été initialement collectés. Une méthodologie de réduction de la dimensionnalité a ensuite été appliquée. L&#039;analyse de la validité apparente a permis d&#039;éliminer les éléments non pertinents en interrogeant un panel de neuf experts issus de disciplines liées à la communication (linguistique, médecine, orthophonie, informatique). L&#039;analyse de la validité de construction a permis d&#039;éliminer les éléments redondants et insuffisamment variables. Enfin, le score de communication holistique a été élaboré par PCF (facteur à composante principale) et validé par validation croisée et analyse de profil latent.
Résultats
Vingt-cinq patients ont rempli les questionnaires (âge médian 67 ans, EIQ 12 ; 15 hommes, 10 femmes ; cavité buccale 14, oropharynx 10, deux localisations 1). Après analyse de la validité apparente, 44 items ont été retenus (Kappa&gt;0,80). Quatre items supplémentaires ont été exclus en raison d&#039;une corrélation très élevée (r&gt;0,90) avec d&#039;autres items présentant une meilleure dispersion. Quarante items ont finalement été inclus dans l&#039;analyse factorielle. Une prédiction du score post-analyse a été réalisée (moyenne=100 ; écart-type=10). Vingt-quatre items sont finalement retenus pour la construction du score de communication holistique (HoCoS) : 19 items issus de questionnaires évaluant la dynamique communicative (13 de l&#039;ECVB et six du DIP), quatre items issus d&#039;un questionnaire sur la perception des troubles de la parole (PHI) et un item issu d&#039;un questionnaire sur la qualité de vie (EORTC QLQ-H&amp;N35). La fiabilité est bonne (validation croisée cinq fois : rs=0,91) et l&#039;analyse complémentaire du profil latent montre une bonne validité du HoCoS, regroupant les sujets par niveau de performance de communication.
Conclusions &amp; Conséquences 
Un score global permettant de mesurer l&#039;impact du trouble de la parole sur la communication a été développé. Il comble le manque de ce type de score en oncologie de la tête et du cou et permet de mieux comprendre les conséquences fonctionnelles et psychosociales de la pathologie dans le suivi des patients.","2","2023-06-29 17:38:03","2023-06-29 17:41:45","10147",,"1","1","1","3","2","1",NULL,,"2023-06-29 05:37:41","0"
"3961592","This paper introduces a novel, completely automatic, audio-subtitle synchronization algorithm designed to increase the accessibility and comprehension of the hearing-impaired people over the video documents. The major contribution of the paper involves the anchor words matching strategy that can reliably put in correspondence the subtitle document generated by a human transcriber and the automatic speech recognition (ASR) textual file. In addition, we propose a subtitle positioning strategy that automatically determines the optimal location for each phrase on the user screen, with respect to the video semantic content. The experimental evaluation validates the proposed method with average accuracy scores superior to 90%.","Automatic alignment of human generated transcripts to speech signals","Bogdan Cosmin Mocanu, Ruxandra Tapu","3853","Alignement automatique de transcriptions générées par l'homme sur des signaux vocaux

Cet article présente un nouvel algorithme de synchronisation de sous-titres audio entièrement automatique conçu pour accroître l'accessibilité et la compréhension des personnes malentendantes sur les documents vidéo. La contribution majeure de l'article implique la stratégie de correspondance des mots d'ancrage qui peut mettre en correspondance de manière fiable le document de sous-titre généré par un transcripteur humain et le fichier textuel de reconnaissance vocale automatique (ASR). En outre, nous proposons une stratégie de positionnement des sous-titres qui détermine automatiquement l'emplacement optimal de chaque phrase sur l'écran utilisateur, par rapport au contenu sémantique vidéo. L'évaluation expérimentale valide la méthode proposée avec une précision moyenne supérieure à 90 %.","15769","6","282","Alignement automatique de transcriptions générées par l&#039;homme sur des signaux vocaux

Cet article présente un nouvel algorithme de synchronisation de sous-titres audio entièrement automatique conçu pour accroître l&#039;accessibilité et la compréhension des personnes malentendantes sur les documents vidéo. La contribution majeure de l&#039;article implique la stratégie de correspondance des mots d&#039;ancrage qui peut mettre en correspondance de manière fiable le document de sous-titre généré par un transcripteur humain et le fichier textuel de reconnaissance automatique de la parole (RAP). En outre, nous proposons une stratégie de positionnement des sous-titres qui détermine automatiquement l&#039;emplacement optimal de chaque phrase sur l&#039;écran utilisateur, par rapport au contenu sémantique vidéo. L&#039;évaluation expérimentale valide la méthode proposée avec une précision moyenne supérieure à 90 %.","2","2023-06-29 17:43:36","2023-06-29 17:47:00","15769",,"1","1","4","1","1","1",NULL,,"2023-06-29 05:43:28","0"
"3963729","Recent advances in NLP have significantly improved the performance of language models on a variety of tasks. While these advances are largely driven by the availability of large amounts of data and compute, they also benefit from the development of better training methods and architectures. In this paper, we introduce CAMEMBERTA, a French DeBERTa model that builds upon the DeBERTaV3 architecture and training objective. We evaluate our model's performance on a variety of French downstream tasks and datasets, including question answering, part-of-speech tagging, dependency parsing, named entity recognition, and the FLUE benchmark, and compare against CamemBERT, the state-of-the-art monolingual model for French. Our results show that, given the same amount of training tokens, our model outperforms BERT-based models trained with MLM on most tasks. Furthermore, our new model reaches similar or superior performance on downstream tasks compared to Camem-BERT, despite being trained on only 30% of its total number of input tokens. In addition to our experimental results, we also publicly release the weights and code implementation of CAMEMBERTA, making it the first publicly available DeBERTaV3 model outside of the original paper and the first openly available implementation of a DeBERTaV3 training objective.","Data-Efficient French Language Modeling with CamemBERTa","Wissam Antoun, Benoît Sagot, Djamé Seddah","3851","Modélisation de la langue française efficace en termes de données avec CamemBERTa

Les progrès récents dans le domaine du NLP ont permis d'améliorer de manière significative les performances des modèles de langage sur un grand nombre de tâches. Bien que ces progrès soient en grande partie dus à la disponibilité de grandes quantités de données et de calculs, ils bénéficient également du développement de meilleures méthodes et architectures d'apprentissage. Dans cet article, nous présentons CAMEMBERTA, un modèle DeBERTa français qui s'appuie sur l'architecture et l'objectif de formation de DeBERTaV3. Nous évaluons la performance de notre modèle sur une variété de tâches et d'ensembles de données en aval du français, y compris la réponse aux questions, l'étiquetage de la partie du discours, l'analyse syntaxique des dépendances, la reconnaissance des entités nommées et le benchmark FLUE, et nous le comparons à CamemBERT, le modèle monolingue de pointe pour le français. Nos résultats montrent que, pour une même quantité de jetons d'entraînement, notre modèle est plus performant que les modèles basés sur BERT entraînés avec MLM dans la plupart des tâches. De plus, notre nouveau modèle atteint des performances similaires ou supérieures sur les tâches en aval par rapport à Camem-BERT, bien qu'il ait été entraîné sur seulement 30% de son nombre total de tokens d'entrée. En plus de nos résultats expérimentaux, nous publions également les poids et l'implémentation du code de CAMEMBERTA, ce qui en fait le premier modèle DeBERTaV3 accessible au public en dehors de l'article original et la première implémentation accessible au public d'un objectif d'entraînement DeBERTaV3.","18667","3","283","Modélisation de la langue française efficace en termes de données avec CamemBERTa

Les progrès récents dans le domaine du TAL ont permis d&#039;améliorer de manière significative les performances des modèles de langage sur un grand nombre de tâches. Bien que ces progrès soient en grande partie dus à la disponibilité de grandes quantités de données et de calculs, ils bénéficient également du développement de meilleures méthodes et architectures d&#039;apprentissage. Dans cet article, nous présentons CAMEMBERTA, un modèle DeBERTa français qui s&#039;appuie sur l&#039;architecture et l&#039;objectif de formation de DeBERTaV3. Nous évaluons la performance de notre modèle sur une variété de tâches et d&#039;ensembles de données en aval du français, y compris la réponse aux questions, l&#039;étiquetage des partie du discours, l&#039;analyse syntaxique des dépendances, la reconnaissance des entités nommées et le benchmark FLUE, et nous le comparons à CamemBERT, le modèle monolingue de pointe pour le français. Nos résultats montrent que, pour une même quantité de jetons d&#039;entraînement, notre modèle est plus performant que les modèles basés sur BERT entraînés avec MLM dans la plupart des tâches. De plus, notre nouveau modèle atteint des performances similaires ou supérieures sur les tâches en aval par rapport à Camem-BERT, bien qu&#039;il ait été entraîné sur seulement 30% de son nombre total de tokens d&#039;entrée. En plus de nos résultats expérimentaux, nous publions également les poids et l&#039;implémentation du code de CAMEMBERTA, ce qui en fait le premier modèle DeBERTaV3 accessible au public en dehors de l&#039;article original et la première implémentation accessible au public d&#039;un objectif d&#039;entraînement DeBERTaV3.","2","2023-06-29 17:52:42","2023-06-29 17:57:30","18667",,"2","1","4","1","1","1",NULL,,"2023-06-29 05:52:23","0"
"3976515","We present a knowledge graph that models cooking recipes to manage human-cobot interaction (collaborative robot) during the preparation of meals. We also present tools for the enrichment and exploration of the graph.","Graphe de connaissances pour l’aide à la réalisation de recettes de cuisine","Farida Saïd, Jeanne Villaneau, Samia Benferhat, Arnaud Biger, Thibault Celeste, Valentin Cadou, Kevin Philippe","371","Graphe de connaissances pour l'aide à la réalisation de recettes de cuisine

Nous présentons un graphe de connaissances qui modélise les recettes de cuisine pour gérer l'interaction homme-cobot (robot collaboratif) lors de la préparation des repas. Nous présentons également des outils pour l'enrichissement et l'exploration du graphe.","10125","3","295","Graphe de connaissances pour l&#039;aide à la réalisation de recettes de cuisine

Nous présentons un graphe de connaissances qui modélise les recettes de cuisine pour gérer l&#039;interaction homme-cobot (robot collaboratif) lors de la préparation des repas. Nous présentons également des outils pour l&#039;enrichissement et l&#039;exploration du graphe.","2","2023-06-30 10:37:59","2023-06-30 10:39:29","10125",,"1","1","1","1","1","1",NULL,,"2023-06-30 10:37:37","0"
"3547413","Can language models learn grounded representations from text distribution alone? This question is both central and recurrent in natural language processing; authors generally agree that grounding requires more than textual distribution. We propose to experimentally test this claim: if any two words have different meanings and yet cannot be distinguished from distribution alone, then grounding is out of the reach of textbased models. To that end, we present early work on an online game for the collection of human judgments on the distributional similarity of word pairs in five languages. We further report early results of our data collection campaign.","A Game Interface to Study Semantic Grounding in Text-Based Models","Timothee Mickus, Mathieu Constant, Denis Paperno","4950","Une interface de jeu pour étudier l'ancrage sémantique dans les modèles textuels

Les modèles de langage peuvent-ils apprendre des représentations ancrées à partir de la seule distribution du texte ? Cette question est à la fois centrale et récurrente dans le traitement du langage naturel ; les auteurs s'accordent généralement à dire que l'ancrage nécessite plus qu'une distribution textuelle. Nous proposons de tester expérimentalement cette affirmation : si deux mots ont des significations différentes et ne peuvent pourtant pas être distingués à partir de la seule distribution, alors l'ancrage est hors de portée des modèles basés sur le texte. À cette fin, nous présentons les premiers travaux sur un jeu en ligne permettant de recueillir des jugements humains sur la similarité distributionnelle de paires de mots dans cinq langues. Nous présentons également les premiers résultats de notre campagne de collecte de données.","17602","3","298","Une interface de jeu pour étudier l&#039;ancrage sémantique dans les modèles textuels

Les modèles de langage peuvent-ils apprendre des représentations ancrées à partir de la seule distribution du texte ? Cette question est à la fois centrale et récurrente dans le traitement du langage naturel : les auteurs s&#039;accordent généralement à dire que l&#039;ancrage nécessite plus qu&#039;une distribution textuelle. Nous proposons de tester expérimentalement cette affirmation : si deux mots ont des significations différentes et ne peuvent pourtant pas être distingués à partir de la seule distribution, alors l&#039;ancrage est hors de portée des modèles basés sur le texte. À cette fin, nous présentons les premiers travaux sur un jeu en ligne permettant de recueillir des jugements humains sur la similarité distributionnelle de paires de mots dans cinq langues. Nous présentons également les premiers résultats de notre campagne de collecte de données.","2","2023-06-30 10:43:44","2023-06-30 10:46:50","17602",,"1","1","1","1","3","1",NULL,,"2023-06-30 10:43:39","0"
"3369206","This paper investigates different approaches in order to improve the performance of a speech recognition system for a given speaker by using no more than 5 min of speech from this speaker, and without exchanging data from other users/speakers. Inspired by the federated learning paradigm, we consider speakers that have access to a personalized database of their own speech, learn an acoustic model and collaborate with other speakers in a network to improve their model. Several local personalizations are explored depending on how aggregation mechanisms are performed. We study the impact of selecting, in an adaptive way, a subset of speakers's models based on a notion of similarity. We also investigate the effect of weighted averaging of fine-tuned and global models. In our approach, only neural acoustic model parameters are exchanged and no audio data is exchanged. By avoiding communicating their personal data, the proposed approach tends to preserve the privacy of speakers. Experiments conducted on the TEDLIUM 3 dataset show that the best improvement is given by averaging a subset of different acoustic models fine-tuned on several user datasets. Our approach applied to HMM/TDNN acoustic models improves quickly and significantly the ASR performance in terms of WER (for instance in one of our two evaluation datasets, from 14.84% to 13.45% with less than 5 min of speech per speaker).","Study on Acoustic Model Personalization in a Context of Collaborative Learning Constrained by Privacy Preservation","Salima Mdhaffar, Marc Tommasi, Yannick Estève","5336","Étude sur la personnalisation du modèle acoustique dans un contexte d'apprentissage collaboratif contraint par la préservation de la vie privée

Cet article étudie différentes approches afin d'améliorer les performances d'un système de reconnaissance vocale pour un locuteur donné en utilisant au maximum 5 minutes de parole de ce locuteur, et sans échanger de données avec d'autres utilisateurs/locuteurs. Inspirés par le paradigme de l'apprentissage fédéré, nous considérons des locuteurs qui ont accès à une base de données personnalisée de leur propre parole, apprennent un modèle acoustique et collaborent avec d'autres locuteurs dans un réseau pour améliorer leur modèle. Plusieurs personnalisations locales sont explorées en fonction de la manière dont les mécanismes d'agrégation sont exécutés. Nous étudions l'impact de la sélection, de manière adaptative, d'un sous-ensemble de modèles de locuteurs sur la base d'une notion de similarité. Nous étudions également l'effet de la moyenne pondérée des modèles globaux et des modèles finement ajustés. Dans notre approche, seuls les paramètres du modèle acoustique neuronal sont échangés et aucune donnée audio n'est échangée. En évitant de communiquer leurs données personnelles, l'approche proposée tend à préserver la vie privée des locuteurs. Les expériences menées sur l'ensemble de données TEDLIUM 3 montrent que la meilleure amélioration est obtenue en faisant la moyenne d'un sous-ensemble de différents modèles acoustiques affinés sur plusieurs ensembles de données d'utilisateurs. Notre approche appliquée aux modèles acoustiques HMM/TDNN améliore rapidement et significativement la performance ASR en termes de WER (par exemple, dans l'un de nos deux ensembles de données d'évaluation, de 14,84% à 13,45% avec moins de 5 minutes de discours par locuteur).","19823","3","301","Étude sur la personnalisation du modèle acoustique dans un contexte d&#039;apprentissage collaboratif contraint par la protection de la vie privée

Cet article étudie différentes approches afin d&#039;améliorer les performances d&#039;un système de reconnaissance vocale pour un locuteur donné en utilisant au maximum 5 minutes de parole de ce locuteur, et sans échanger de données avec d&#039;autres utilisateurs/locuteurs. Inspirés par le paradigme de l&#039;apprentissage fédéré, nous considérons des locuteurs qui ont accès à une base de données personnalisée de leur propre parole, apprennent un modèle acoustique et collaborent avec d&#039;autres locuteurs dans un réseau pour améliorer leur modèle. Plusieurs personnalisations locales sont explorées en fonction de la manière dont les mécanismes d&#039;agrégation sont exécutés. Nous étudions l&#039;impact de la sélection, de manière adaptative, d&#039;un sous-ensemble de modèles de locuteurs sur la base d&#039;une notion de similarité. Nous étudions également l&#039;effet de la moyenne pondérée des modèles globaux et des modèles finement ajustés. Dans notre approche, seuls les paramètres du modèle acoustique neuronal sont échangés et aucune donnée audio n&#039;est échangée. En évitant de communiquer leurs données personnelles, l&#039;approche proposée tend à protéger la vie privée des locuteurs. Les expériences menées sur l&#039;ensemble de données TEDLIUM 3 montrent que la meilleure amélioration est obtenue en faisant la moyenne d&#039;un sous-ensemble de différents modèles acoustiques affinés sur plusieurs ensembles de données d&#039;utilisateurs. Notre approche appliquée aux modèles acoustiques HMM/TDNN améliore rapidement et significativement la performance de RAP en termes de taux d&#039;erreur mot (par exemple, dans l&#039;un de nos deux ensembles de données d&#039;évaluation, de 14,84% à 13,45% avec moins de 5 minutes de discours par locuteur).","2","2023-06-30 17:27:09","2023-06-30 17:38:05","19823",,"1","1","4","3","1","1",NULL,,"2023-06-30 05:27:02","0"
"2021809","We describe our system for the translation task of WMT 2010. This system, developed for the English-French and French-English directions, is based on Moses and was trained using only the resources supplied for the workshop. We report experiments to enhance it with out-of-domain parallel corpora sub-sampling, N-best list post-processing and a French grammatical checker.","The RALI Machine Translation System for WMT 2010","Stéphane Huet, Julien Bourdaillet, Alexandre Patry, Philippe Langlais","2028","Le système de traduction automatique RALI pour WMT 2010

Nous décrivons notre système pour la tâche de traduction de WMT 2010. Ce système, développé pour les directions anglais-français et français-anglais, est basé sur Moïse et a été formé en utilisant uniquement les ressources fournies pour l’atelier. Nous rapportons des expériences pour l’améliorer avec le sous-échantillonnage parallèle des corpus hors domaine, le post-traitement N-best list et un vérificateur grammatical français.","2301","4","309","Le système de traduction automatique RALI pour WMT 2010

Nous décrivons notre système pour la tâche de traduction de WMT 2010. Ce système, développé pour la traduction anglais-français et français-anglais, est fondé sur Moses et a été formé en utilisant uniquement les ressources fournies pour l’atelier. Nous rapportons des expériences pour l’améliorer avec le sous-échantillonnage parallèle des corpus hors domaine, le post-traitement des listes N-best et un vérificateur grammatical français.","2","2023-07-03 13:40:42","2023-07-03 13:48:58","2301",,"2","1","4","3","1","1",NULL,,"2023-07-03 01:40:04","0"
"3360794","ASVspoof 2021 is the forth edition in the series of biannual challenges which aim to promote the study of spoofing and the design of countermeasures to protect automatic speaker verification systems from manipulation. In addition to a continued focus upon logical and physical access tasks in which there are a number of advances compared to previous editions, ASVspoof 2021 introduces a new task involving deepfake speech detection. This paper describes all three tasks, the new databases for each of them, the evaluation metrics, four challenge baselines, the evaluation platform and a summary of challenge results. Despite the introduction of channel and compression variability which compound the difficulty, results for the logical access and deepfake tasks are close to those from previous ASVspoof editions. Results for the physical access task show the difficulty in detecting attacks in real, variable physical spaces. With ASVspoof 2021 being the first edition for which participants were not provided with any matched training or development data and with this reflecting real conditions in which the nature of spoofed and deepfake speech can never be predicated with confidence, the results are extremely encouraging and demonstrate the substantial progress made in the field in recent years.","ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection","Junichi Yamagishi, Xin Wang, Massimiliano Todisco, Md Sahidullah, Jose Patino, Andreas Nautsch, Xuechen Liu, Kong Aik Lee, Tomi Kinnunen, Nicholas Evans, Héctor Delgado","5361","ASVspoof 2021 : accélération de la progression de la détection de parole usurpée et de parole contrefaite

ASVspoof 2021 est la quatrième édition de la série de défis semestriels qui visent à promouvoir l'étude de l'usurpation d'identité et la conception de contre-mesures pour protéger les systèmes de vérification automatique des haut-parleurs de la manipulation. En plus d'un accent continu sur les tâches d'accès logique et physique dans lesquelles il ya un certain nombre d'avancées par rapport aux éditions précédentes, ASVspoof 2021 introduit une nouvelle tâche impliquant la détection de la parole deep fake. Le présent document décrit les trois tâches, les nouvelles bases de données pour chacune d'elles, les paramètres d'évaluation, les quatre niveaux de référence, la plate-forme d'évaluation et un résumé des résultats des défis. Malgré l'introduction de la variabilité des canaux et de la compression qui aggrave la difficulté, les résultats pour les tâches d'accès logique et de deep fake sont proches de ceux des éditions précédentes d'ASVspoof. Les résultats de la tâche d'accès physique montrent la difficulté de détecter des attaques dans des espaces physiques réels et variables. Étant donné que ASVspoof 2021 est la première édition pour laquelle les participants n'ont reçu aucune donnée de formation ou de développement correspondante et que cela reflète des conditions réelles dans lesquelles la nature des discours usurpés et des discours truqués ne peuvent jamais être prédits avec confiance, les résultats sont extrêmement encourageants et démontrent les progrès substantiels accomplis dans ce domaine au cours des dernières années.","15055","6","313","ASVspoof 2021 : accélération de la progression de la détection de discours usurpés et deepfake

ASVspoof 2021 est la quatrième édition de la série de défis semestriels qui visent à promouvoir l&#039;étude de l&#039;usurpation d&#039;identité et la conception de contre-mesures pour empêcher la manipulation des systèmes de reconnaissance automatique du locuteur. En plus d&#039;un accent continu sur les tâches d&#039;accès logique et physique dans lesquelles il y a un certain nombre d&#039;avancées par rapport aux éditions précédentes, ASVspoof 2021 introduit une nouvelle tâche impliquant la détection des discours deepfake. Le présent document décrit les trois tâches, les nouvelles bases de données pour chacune d&#039;elles, les paramètres d&#039;évaluation, les quatre niveaux de référence, la plate-forme d&#039;évaluation et un résumé des résultats des défis. Malgré l&#039;introduction de la variabilité des canaux et de la compression qui aggrave la difficulté, les résultats pour les tâches d&#039;accès logique et de deepfake sont proches de ceux des éditions précédentes d&#039;ASVspoof. Les résultats de la tâche d&#039;accès physique montrent la difficulté de détecter des attaques dans des espaces physiques réels et variables. Étant donné que ASVspoof 2021 est la première édition pour laquelle les participants n&#039;ont reçu aucune donnée de formation ou de développement correspondante et que cela reflète des conditions réelles dans lesquelles la nature des discours usurpés et des discours deepfakes ne peuvent jamais être prédits avec confiance, les résultats sont extrêmement encourageants et démontrent les progrès substantiels accomplis dans ce domaine au cours des dernières années.","2","2023-07-03 14:03:48","2023-07-03 14:06:55","15055",,"3","1","4","3","4","3",NULL,,"2023-07-03 02:03:43","0"
"3429859","Dialogue is fundamental to argumentation, providing a dialectical basis for establishing which arguments are acceptable. Argumentation can also be used as the basis for dialogue. In such ""argumentationbased"" dialogues, participants take part in an exchange of arguments, and the mechanisms of argumentation are used to establish what participants take to be acceptable at the end of the exchange. This chapter considers such dialogues, discussing the elements that are required in order to carry out argumentation-based dialogues, giving examples, and discussing open issues.","Argumentation-based Dialogue","Elizabeth Black, Nicolas Maudet, Simon Parsons","5216","Dialogue basé sur l'argumentation

Le dialogue est fondamental pour l'argumentation, car il fournit une base dialectique pour déterminer quels arguments sont acceptables. L'argumentation peut également servir de base au dialogue. Dans ces dialogues ""basés sur l'argumentation"", les participants prennent part à un échange d'arguments et les mécanismes de l'argumentation sont utilisés pour établir ce que les participants considèrent comme acceptable à la fin de l'échange. Ce chapitre examine ce type de dialogue, en discutant des éléments nécessaires pour mener à bien des dialogues basés sur l'argumentation, en donnant des exemples et en discutant des questions en suspens.","19702","3","316","Dialogue fondé sur l&#039;argumentation

Le dialogue est fondamental pour l&#039;argumentation, car il fournit une base dialectique pour déterminer quels arguments sont acceptables. L&#039;argumentation peut également servir de base au dialogue. Dans ces dialogues &quot;fondés sur l&#039;argumentation&quot;, les participants prennent part à un échange d&#039;arguments et les mécanismes de l&#039;argumentation sont utilisés pour établir ce que les participants considèrent comme acceptable à la fin de l&#039;échange. Ce chapitre examine ce type de dialogue, en discutant des éléments nécessaires pour mener à bien des dialogues fondés sur l&#039;argumentation, en donnant des exemples et en discutant des questions en suspens.","2","2023-07-03 14:22:22","2023-07-03 14:27:47","19702",,"1","1","1","3","1","1",NULL,,"2023-07-03 02:22:17","0"
"3325568","Here, we introduce ITEXT-BIO, an intelligent process for biomedical domain terminology extraction from textual documents and subsequent analysis. The proposed methodology consists of two complementary approaches, including free and driven term extraction. The first is based on term extraction with statistical measures, while the second considers morphosyntactic variation rules to extract term variants from the corpus. The combination of two term extraction and analysis strategies is the keystone of ITEXT-BIO. These include combined intra-corpus strategies that enable term extraction and analysis either from a single corpus (intra), or from corpora (inter). We assessed the two approaches, the corpus or corpora to be analysed and the type of statistical measures used. Our experimental findings revealed that the proposed methodology could be used: (1) to efficiently extract representative, discriminant and new terms from a given corpus or corpora, and (2) to provide quantitative and qualitative analyses on these terms regarding the study domain.","ITEXT-BIO: Intelligent Term EXTraction for BIOmedical Analysis","Rodrique Kafando, Rémy Decoupes, Sarah Valentin, Lucile Sautot, Maguelonne Teisseire, Mathieu Roche","1039","ITEXT-BIO : Extraction intelligente de termes pour l'analyse biomédicale

Nous présentons ici ITEXT-BIO, un processus intelligent d'extraction de terminologie dans le domaine biomédical à partir de documents textuels et d'analyse ultérieure. La méthodologie proposée consiste en deux approches complémentaires, incluant l'extraction de termes libre et pilotée. La première est basée sur l'extraction de termes à l'aide de mesures statistiques, tandis que la seconde prend en compte les règles de variation morphosyntaxique pour extraire des variantes de termes du corpus. La combinaison de deux stratégies d'extraction et d'analyse de termes est la clé de voûte d'ITEXT-BIO. Il s'agit de stratégies combinées intra-corpus qui permettent l'extraction et l'analyse de termes soit à partir d'un seul corpus (intra), soit à partir de corpus (inter). Nous avons évalué les deux approches, le ou les corpus à analyser et le type de mesures statistiques utilisées. Nos résultats expérimentaux ont révélé que la méthodologie proposée pouvait être utilisée : (1) pour extraire efficacement des termes représentatifs, discriminants et nouveaux à partir d'un ou de plusieurs corpus donnés, et (2) pour fournir des analyses quantitatives et qualitatives sur ces termes concernant le domaine d'étude.","9307","3","318","ITEXT-BIO : Extraction intelligente de termes pour l&#039;analyse biomédicale

Nous présentons ici ITEXT-BIO, un processus intelligent d&#039;extraction terminologique dans le domaine biomédical à partir de documents textuels et d&#039;analyse ultérieure. La méthodologie proposée consiste en deux approches complémentaires, incluant l&#039;extraction de termes libre et pilotée. La première est basée sur l&#039;extraction de termes à l&#039;aide de mesures statistiques, tandis que la seconde prend en compte les règles de variation morphosyntaxique pour extraire des variantes de termes du corpus. La combinaison de deux stratégies d&#039;extraction et d&#039;analyse de termes est la clé de voûte d&#039;ITEXT-BIO. Il s&#039;agit de stratégies combinées intracorpus qui permettent l&#039;extraction et l&#039;analyse de termes soit à partir d&#039;un seul corpus (intra), soit à partir de plusieurs corpus (inter). Nous avons évalué les deux approches, le ou les corpus à analyser et le type de mesures statistiques utilisées. Nos résultats expérimentaux ont révélé que la méthodologie proposée pouvait être utilisée : (1) pour extraire efficacement des termes représentatifs, discriminants et nouveaux à partir d&#039;un ou de plusieurs corpus donnés, et (2) pour fournir des analyses quantitatives et qualitatives sur ces termes concernant le domaine d&#039;étude.","2","2023-07-03 14:33:49","2023-07-03 14:41:53","9307",,"4","2","3","1","1","1",NULL,,"2023-07-03 02:33:41","0"
"3142170","A lot of effort is currently made to provide methods to analyze and understand deep neural network impressive performances for tasks such as image or text classification. These methods are mainly based on visualizing the important input features taken into account by the network to build a decision. However these techniques, let us cite LIME, SHAP, Grad-CAM, or TDS, require extra effort to interpret the visualization with respect to expert knowledge. In this paper, we propose a novel approach to inspect the hidden layers of a fitted CNN in order to extract interpretable linguistic objects from texts exploiting classification process. In particular, we detail a weighted extension of the Text Deconvolution Saliency (wTDS) measure which can be used to highlight the relevant features used by the CNN to perform the classification task. We empirically demonstrate the efficiency of our approach on corpora from two different languages: English and French. On all datasets, wTDS automatically encodes complex linguistic objects based on co-occurrences and possibly on grammatical and syntax analysis.","From text saliency to linguistic objects: learning linguistic interpretable markers with a multi-channels convolutional architecture","Laurent Vanni, Marco Corneli, Damon Mayaffre, Frédéric Precioso","5848","De la saillance du texte aux objets linguistiques : apprentissage de marqueurs linguistiques interprétables avec une architecture convolutionnelle multi-canaux

De nombreux efforts sont actuellement déployés pour fournir des méthodes permettant d'analyser et de comprendre les performances impressionnantes des réseaux de neurones profonds pour des tâches telles que la classification d'images ou de textes. Ces méthodes sont principalement basées sur la visualisation des caractéristiques d'entrée importantes prises en compte par le réseau pour construire une décision. Cependant ces techniques, citons LIME, SHAP, Grad-CAM, ou TDS, nécessitent un effort supplémentaire pour interpréter la visualisation au regard de la connaissance experte. Dans cet article, nous proposons une nouvelle approche pour inspecter les couches cachées d'un CNN ajusté afin d'extraire des objets linguistiques interprétables à partir de textes en exploitant le processus de classification. En particulier, nous détaillons une extension pondérée de la mesure de la saillance par déconvolution de texte (wTDS) qui peut être utilisée pour mettre en évidence les caractéristiques pertinentes utilisées par le CNN pour effectuer la tâche de classification. Nous démontrons empiriquement l'efficacité de notre approche sur des corpus de deux langues différentes : anglais et français. Sur tous les jeux de données, wTDS encode automatiquement les objets linguistiques complexes en se basant sur les cooccurrences et éventuellement sur l'analyse grammaticale et syntaxique.","18442","3","349","De la saillance linguistique aux objets linguistiques : apprentissage de marqueurs linguistiques interprétables avec une architecture convolutionnelle multicanaux

De nombreux efforts sont actuellement déployés pour fournir des méthodes permettant d&#039;analyser et de comprendre les performances impressionnantes des réseaux de neurones profonds pour des tâches telles que la classification d&#039;images ou de textes. Ces méthodes sont principalement basées sur la visualisation des caractéristiques d&#039;entrée importantes prises en compte par le réseau pour construire une décision. Cependant, ces techniques, par exemple LIME, SHAP, Grad-CAM, ou TDS, nécessitent un effort supplémentaire pour interpréter la visualisation au regard de la connaissance experte. Dans cet article, nous proposons une nouvelle approche pour inspecter les couches cachées d&#039;un réseau de neurones convolutifs ajusté afin d&#039;extraire des objets linguistiques interprétables à partir de textes en exploitant le processus de classification. En particulier, nous détaillons une extension pondérée de la mesure de la saillance par déconvolution de texte (wTDS) qui peut être utilisée pour mettre en évidence les caractéristiques pertinentes utilisées par le réseau de neurones convolutifs pour effectuer la tâche de classification. Nous démontrons empiriquement l&#039;efficacité de notre approche sur des corpus de deux langues différentes : l&#039;anglais et le français. Sur tous les jeux de données, wTDS encode automatiquement les objets linguistiques complexes en se basant sur les cooccurrences et éventuellement sur l&#039;analyse grammaticale et syntaxique.","2","2023-07-04 12:28:19","2023-07-04 12:47:35","18442",,"1","1","4","4","3","1",NULL,,"2023-07-04 12:28:14","0"
"3463628","This article analyzes the spread of unreliable information on Twitter during the 2017 French presidential campaign, focusing on the use of mobile phones with regard to information-sharing behavior. The corpus is composed of 38,346,765 tweets, posted by 2,163,812 supporters of the five main French political parties, from November 25, 2016 to May 12, 2017. We examine more precisely a sub-corpus of tweets (13,044,619) containing links to external information sources, in order to evaluate the different types of information sources and their reliability. Our research shows that information-sharing behavior within Twitter in France is generally based on reliable information sources, produced by journalists and professional media. However, we highlight that smartphone users tended to share a greater amount of user-generated content, as well as articles from a wider range of alternative political information sources (blogs, activists’ websites); such sources were most likely to publish unreliable information. Thus it appears that users of mobile phones tend to share more unreliable news than those who use Twitter from a computer web browser. Further, we show that this “device effect” on the spread of unreliable information is primarily amplified among the practices of one political community—namely, the far-right party and its network of supporters—which is more likely to organize debate around a larger number of unreliable references. We are claiming here that the design-based interoperability of these unreliable political news and social media applications helps to understand why the French far-right community shared more unreliable information from the Twitter application.","Mobile phones in the spread of unreliable information on Twitter: evidence from the 2017 French presidential campaign","Julien Figeac, Pierre Ratinaud, Nikos Smyrnaios, Guillaume Cabanac, Ophélie Fraisier, Tristan Salord, Fanny Seffusatti","5151","Les téléphones portables diffusent des informations peu fiables sur Twitter : les preuves de la campagne présidentielle française de 2017

Cet article analyse la diffusion d'informations peu fiables sur Twitter pendant la campagne présidentielle française de 2017, en mettant l'accent sur l'utilisation des téléphones mobiles en ce qui concerne le partage d'informations. Le corpus est composé de 38 346 765 tweets, postés par 2 163 812 sympathisants des cinq principaux partis politiques français, du 25 novembre 2016 au 12 mai 2017. Nous examinons plus précisément un sous-corpus de tweets (13 044 619) contenant des liens vers des sources d'informations externes, afin d'évaluer les différents types de sources d'informations et leur fiabilité. Notre recherche montre que le comportement de partage d'informations au sein de Twitter en France est généralement basé sur des sources d'informations fiables, produites par des journalistes et des médias professionnels. Cependant, nous soulignons que les utilisateurs de smartphones ont tendance à partager une plus grande quantité de contenu généré par les utilisateurs, ainsi que des articles provenant d'un plus large éventail de sources d'information politique alternatives (blogs, sites web des militants); de telles sources étaient plus susceptibles de publier des informations non fiables. Ainsi, il semble que les utilisateurs de téléphones mobiles ont tendance à partager des nouvelles moins fiables que ceux qui utilisent Twitter à partir d'un navigateur Web ordinateur. En outre, nous montrons que cet « effet de dispositif » sur la diffusion d'informations non fiables est principalement amplifié parmi les pratiques d'une communauté politique, à savoir, le parti d'extrême droite et son réseau de partisans, qui est plus susceptible d'organiser un débat autour d'un plus grand nombre de références non fiables. Nous affirmons ici que l'interopérabilité, basée sur le design, de ces applications d'informations politiques et de médias sociaux peu fiables aide à comprendre pourquoi la communauté française d'extrême droite a partagé plus d'informations peu fiables de l'application Twitter.","17067","6","355","Les téléphones portables dans la diffusion d&#039;informations peu fiables sur Twitter : preuves tirées de la campagne présidentielle française de 2017

Cet article analyse la diffusion d&#039;informations peu fiables sur Twitter pendant la campagne présidentielle française de 2017, en mettant l&#039;accent sur l&#039;utilisation des téléphones portables en ce qui concerne le partage d&#039;informations. Le corpus est composé de 38 346 765 tweets, postés par 2 163 812 sympathisants des cinq principaux partis politiques français, du 25 novembre 2016 au 12 mai 2017. Nous examinons plus précisément un sous-corpus de tweets (13 044 619) contenant des liens vers des sources d&#039;informations externes, afin d&#039;évaluer les différents types de sources d&#039;informations et leur fiabilité. Notre recherche montre que le comportement de partage d&#039;informations au sein de Twitter en France est généralement basé sur des sources d&#039;informations fiables, produites par des journalistes et des médias professionnels. Cependant, nous soulignons que les utilisateurs de smartphones ont tendance à partager une plus grande quantité de contenu généré par les utilisateurs, ainsi que des articles provenant d&#039;un plus large éventail de sources d&#039;information politique alternatives (blogs, sites web de militants) ; de telles sources étaient plus susceptibles de publier des informations non fiables. Ainsi, il semble que les utilisateurs de téléphones portables ont tendance à partager des informations moins fiables que ceux qui utilisent Twitter à partir d&#039;un navigateur Web pour ordinateur. En outre, nous montrons que cet « effet d&#039;appareil » sur la diffusion d&#039;informations non fiables est principalement amplifié parmi les pratiques d&#039;une communauté politique, à savoir, le parti d&#039;extrême droite et son réseau de partisans, qui est plus susceptible d&#039;organiser un débat autour d&#039;un plus grand nombre de références non fiables. Nous affirmons ici que l&#039;interopérabilité, basée sur le design, de ces applications d&#039;informations politiques et de médias sociaux peu fiables aide à comprendre pourquoi la communauté française d&#039;extrême droite a partagé plus d&#039;informations peu fiables de l&#039;application Twitter.","2","2023-07-04 14:38:00","2023-07-04 14:42:50","17067",,"1","1","3","4","1","1",NULL,,"2023-07-04 02:37:02","0"
"3373624","We present the Zero Resource Speech Challenge 2021, which asks participants to learn a language model directly from audio, without any text or labels. The challenge is based on the Libri-light dataset, which provides up to 60k hours of audio from English audio books without any associated text. We provide a pipeline baseline system consisting on an encoder based on contrastive predictive coding (CPC), a quantizer ($k$-means) and a standard language model (BERT or LSTM). The metrics evaluate the learned representations at the acoustic (ABX discrimination), lexical (spot-the-word), syntactic (acceptability judgment) and semantic levels (similarity judgment). We present an overview of the eight submitted systems from four groups and discuss the main results.","The Zero Resource Speech Challenge 2021: Spoken language modelling","Ewan Dunbar, Mathieu Bernard, Nicolas Hamilakis, Tu Anh Nguyen, Maureen de Seyssel, Patricia Rozé, Morgane Rivière, Eugene Kharitonov, Emmanuel Dupoux","5316","Le défi du discours zéro ressource 2021: Modélisation du langage parlé

Nous présentons le Zero Resource Speech Challenge 2021, qui demande aux participants d’apprendre un modèle de langue directement à partir de l’audio, sans texte ni étiquettes. Le défi est basé sur l’ensemble de données Libri-light, qui fournit jusqu’à 60 000 heures d’audio à partir de livres audio anglais sans aucun texte associé. Nous fournissons un système de base de pipeline composé d’un codeur basé sur le codage prédictif contrastif (CPC), d’un quantificateur (k$-moans) et d’un modèle linguistique standard (BERT ou LSTM). Les métriques évaluent les représentations apprises au niveau acoustique (discrimination ABX), lexical (spot-the-word), syntaxique (jugement d’acceptabilité) et sémantique (jugement de similarité). Nous présentons un aperçu des huit systèmes soumis par quatre groupes et discutons des principaux résultats.","13525","4","356","Le Zero Resource Speech Challenge 2021 : Modélisation de la parole

Nous présentons le Zero Resource Speech Challenge 2021, qui demande aux participants d’apprendre un modèle de langue directement à partir de l’audio, sans texte ni étiquettes. Le défi est fondé sur l’ensemble de données Libri-light, qui fournit jusqu’à 60 000 heures d’audio à partir de livres audio anglais sans aucun texte associé. Nous fournissons un système de base de pipeline composé d’un codeur basé sur le codage prédictif contrastif (CPC), d’un quantificateur (k$-moyennes) et d’un modèle de langue standard (BERT ou LSTM). Les métriques évaluent les représentations apprises au niveau acoustique (discrimination ABX), lexical (spot-the-word), syntaxique (jugement d’acceptabilité) et sémantique (jugement de similarité). Nous présentons un aperçu des huit systèmes soumis par quatre groupes et discutons des principaux résultats.","2","2023-07-04 14:43:29","2023-07-04 14:51:16","13525",,"1","1","4","3","1","1",NULL,,"2023-07-04 02:42:54","0"
"3138351","NLP Interpretability aims to increase trust in model predictions. This makes evaluating interpretability approaches a pressing issue. There are multiple datasets for evaluating NLP Interpretability, but their dependence on human provided ground truths raises questions about their unbiasedness. In this work, we take a different approach and formulate a specific classification task by diverting question-answering datasets. For this custom classification task, the interpretability ground-truth arises directly from the definition of the classification problem. We use this method to propose a benchmark and lay the groundwork for future research in NLP interpretability by evaluating a wide range of current state of the art methods.","QUACKIE: A NLP Classification Task With Ground Truth Explanations","Yves Rychener, Xavier Renard, Djamé Seddah, Pascal Frossard, Marcin Detyniecki","5858","QUACKIE : Tâche de classification NLP avec explications de la vérité de base

Interprétabilité PNL vise à accroître la confiance dans les prédictions de modèle. Cela fait de l'évaluation de l'interprétabilité une question urgente. Il existe de multiples ensembles de données pour évaluer l'interprétabilité de la PNL, mais leur dépendance aux vérités de base fournies par l'homme soulève des questions sur leur impartialité. Dans ce travail, nous adoptons une approche différente et formulons une tâche de classification spécifique en détournant les ensembles de données de questions-réponses. Pour cette tâche de classification personnalisée, le fondement de l'interprétabilité découle directement de la définition du problème de classification. Nous utilisons cette méthode pour proposer un point de repère et jeter les bases de recherches futures sur l'interprétabilité de PNL en évaluant un large éventail de méthodes actuelles de pointe.","15552","6","358","QUACKIE : Tâche de classification TAL avec explications de vérité terrain

L&#039;interprétabilité en TAL vise à accroître la confiance dans les prédictions de modèle. Cela fait de l&#039;évaluation de l&#039;interprétabilité une question urgente. Il existe de multiples ensembles de données pour évaluer l&#039;interprétabilité en TAL, mais leur dépendance aux vérités terrain fournies par l&#039;homme soulève des questions sur leur impartialité. Dans ce travail, nous adoptons une approche différente et formulons une tâche de classification spécifique en détournant les ensembles de données de questions-réponses. Pour cette tâche de classification personnalisée, le fondement de l&#039;interprétabilité découle directement de la définition du problème de classification. Nous utilisons cette méthode pour proposer un point de repère et jeter les bases de recherches futures sur l&#039;interprétabilité en TAL en évaluant un large éventail de méthodes de pointe actuelles.","2","2023-07-04 14:52:02","2023-07-04 14:59:13","15552",,"4","1","4","4","1","4",NULL,,"2023-07-04 02:51:50","0"
"3447745","Area under the ROC curve (AUC) optimisation techniques developed for neural networks have recently demonstrated their capabilities in different audio and speech related tasks. However, due to its intrinsic nature, AUC optimisation has focused only on binary tasks so far. In this paper, we introduce an extension to the AUC optimisation framework so that it can be easily applied to an arbitrary number of classes, aiming to overcome the issues derived from training data limitations in deep learning solutions. Building upon the multiclass definitions of the AUC metric found in the literature, we define two new training objectives using a one-versus-one and a one-versus-rest approach. In order to demonstrate its potential, we apply them in an audio segmentation task with limited training data that aims to differentiate 3 classes: foreground music, background music and no music. Experimental results show that our proposal can improve the performance of audio segmentation systems significantly compared to traditional training criteria such as cross entropy.","Generalising AUC Optimisation to Multiclass Classification for Audio Segmentation with Limited Training Data","Pablo Gimeno, Victoria Mingote, Alfonso Ortega, Antonio Miguel, Eduardo Lleida","5187","Généraliser l'optimisation de l'AUC à la classification multiclasse pour la segmentation audio avec des données d'entraînement limitées

Les techniques d'optimisation de l'aire sous la courbe ROC (AUC) développées pour les réseaux neuronaux ont récemment démontré leurs capacités dans différentes tâches liées à l'audio et à la parole. Cependant, en raison de sa nature intrinsèque, l'optimisation de la surface sous la courbe ROC ne s'est concentrée jusqu'à présent que sur des tâches binaires. Dans cet article, nous introduisons une extension du cadre d'optimisation AUC afin qu'il puisse être facilement appliqué à un nombre arbitraire de classes, dans le but de surmonter les problèmes dérivés des limites des données d'entraînement dans les solutions d'apprentissage profond. En se basant sur les définitions multi-classes de la métrique AUC trouvées dans la littérature, nous définissons deux nouveaux objectifs de formation en utilisant une approche un contre un et un contre un repos. Afin de démontrer son potentiel, nous les appliquons à une tâche de segmentation audio avec des données d'entraînement limitées qui vise à différencier 3 classes : musique de premier plan, musique d'arrière-plan et pas de musique. Les résultats expérimentaux montrent que notre proposition peut améliorer les performances des systèmes de segmentation audio de manière significative par rapport aux critères d'apprentissage traditionnels tels que l'entropie croisée.","17839","3","359","Généraliser l&#039;optimisation de la surface sous la courbe à la classification multiclasse pour la segmentation audio avec des données d&#039;entraînement limitées

Les techniques d&#039;optimisation de la surface sous la courbe ROC (SSC) développées pour les réseaux neuronaux ont récemment démontré leurs capacités dans différentes tâches liées à l&#039;audio et à la parole. Cependant, en raison de sa nature intrinsèque, l&#039;optimisation de la surface sous la courbe ROC ne s&#039;est concentrée jusqu&#039;à présent que sur des tâches binaires. Dans cet article, nous introduisons une extension du cadre d&#039;optimisation de SSC afin qu&#039;il puisse être facilement appliqué à un nombre arbitraire de classes, dans le but de surmonter les problèmes dérivés des limites des données d&#039;entraînement dans les solutions d&#039;apprentissage profond. En se basant sur les définitions multiclasse de la métrique SSC trouvées dans la littérature, nous définissons deux nouveaux objectifs de formation en utilisant une approche un contre un et un contre un repos. Afin de démontrer son potentiel, nous les appliquons à une tâche de segmentation audio avec des données d&#039;entraînement limitées qui vise à différencier 3 classes : musique de premier plan, musique d&#039;arrière-plan et pas de musique. Les résultats expérimentaux montrent que notre proposition peut améliorer les performances des systèmes de segmentation audio de manière significative par rapport aux critères d&#039;apprentissage traditionnels tels que l&#039;entropie croisée.","2","2023-07-04 15:11:27","2023-07-04 15:22:45","17839",,"3","1","4","2","1","4",NULL,,"2023-07-04 03:11:18","0"
"3097523","In this paper, we present the method we have designed and implemented for identifying lists and sentences in PDF documents while participating to FinSBD-2 Financial Document Analysis Shared Task. We propose a model-driven approach for the French and English datasets. It relies on a top-down process from the PDF itself in order to keep control of the workflow. Our objective is to use PDF structure extraction to improve text segment boundaries detection in an end-to-end fashion.","Daniel at the FinSBD-2 task : Extracting Lists and Sentences from PDF Documents: a model-driven end-to-end approach to PDF document analysis","Emmanuel Giguet, Gaël Lejeune","5964","Daniel à la tâche FinSBD-2: Extraction des listes et des peines à partir de documents PDF: une approche de bout en bout axée sur le modèle pour l’analyse des documents PDF

Dans cet article, nous présentons la méthode que nous avons conçue et mise en œuvre pour identifier les listes et les phrases dans les documents PDF tout en participant à la tâche partagée FinSBD-2 Analyse des documents financiers. Nous proposons une approche basée sur des modèles pour les ensembles de données français et anglais. Il s’appuie sur un processus descendant du PDF lui-même afin de garder le contrôle du flux de travail. Notre objectif est d’utiliser l’extraction de structure PDF pour améliorer la détection des limites de segment de texte de bout en bout.","14173","4","364","DAnIEL à la tâche FinSBD-2 : Extraction de listes et de phrases à partir de documents PDF : une approche de bout en bout axée sur le modèle pour l’analyse des documents PDF

Dans cet article, nous présentons la méthode que nous avons conçue et mise en œuvre pour identifier les listes et les phrases dans les documents PDF tout en participant à la tâche partagée d&#039;analyse des documents financiers FinSBD-2. Nous proposons une approche fondée sur des modèles pour les ensembles de données français et anglais. Il s’appuie sur un processus descendant du PDF lui-même afin de garder le contrôle du flux de travail. Notre objectif est d’utiliser l’extraction de structure PDF pour améliorer la détection des limites de segment de texte de bout en bout.","2","2023-07-04 15:41:55","2023-07-04 15:48:31","14173",,"3","1","3","4","1","2",NULL,,"2023-07-04 03:41:47","0"
"3366097","We study the extraction and reorganization of event-related information in texts regarding industrial pollution. The object is to build a memory of polluted sites that gathers the information about industrial events from various databases and corpora. An industrial event is described through several features as the event trigger, the industrial activity, the institution, the pollutant, etc. In order to efficiently collect information from a large corpus, it is necessary to automatize the information extraction process. To this end, we manually annotated a part of a corpus about soil industrial pollution, then we used it to train information extraction models with deep learning methods. The models we trained achieve 0.76 F-score on event feature extraction. We intend to improve the models and then use them on other text resources to enrich the polluted sites memory with extracted information about industrial events.","Extracting Event-related Information from a Corpus Regarding Soil Industrial Pollution","Chuanming Dong, Philippe Gambette, Catherine Dominguès","5345","Extraction d'un corpus d'informations relatives à des événements concernant la pollution industrielle des sols

Nous étudions l'extraction et la réorganisation des informations liées à l'événement dans les textes concernant la pollution industrielle. L'objectif est de construire une mémoire de sites pollués qui rassemble les informations sur les événements industriels à partir de différentes bases de données et corpora. Un événement industriel est décrit à travers plusieurs caractéristiques comme le déclencheur de l'événement, l'activité industrielle, l'institution, le polluant, etc. Afin de collecter efficacement des informations à partir d'un grand corpus, il est nécessaire d'automatiser le processus d'extraction d'informations. A cette fin, nous avons annoté manuellement une partie d'un corpus sur la pollution industrielle des sols, puis nous l'avons utilisé pour former des modèles d'extraction d'informations avec des méthodes d'apprentissage profond. Les modèles que nous avons formés atteignent 0,76 F-score sur l'extraction de caractéristiques d'événement. Nous avons l'intention d'améliorer les modèles, puis de les utiliser sur d'autres ressources de texte pour enrichir la mémoire des sites pollués avec des informations extraites sur les événements industriels.","15039","6","367","Extraction d&#039;un corpus d&#039;informations relatives à des événements concernant la pollution industrielle des sols

Nous étudions l&#039;extraction et la réorganisation des informations relatives à des événements dans les textes concernant la pollution industrielle. L&#039;objectif est de construire une mémoire de sites pollués qui rassemble les informations sur les événements industriels à partir de différentes bases de données et corpus. Un événement industriel est décrit à travers plusieurs caractéristiques comme le déclencheur de l&#039;événement, l&#039;activité industrielle, l&#039;institution, le polluant, etc. Afin de collecter efficacement des informations à partir d&#039;un grand corpus, il est nécessaire d&#039;automatiser le processus d&#039;extraction d&#039;informations. À cette fin, nous avons annoté manuellement une partie d&#039;un corpus sur la pollution industrielle des sols, puis nous l&#039;avons utilisé pour former des modèles d&#039;extraction d&#039;informations avec des méthodes d&#039;apprentissage profond. Les modèles que nous avons formés atteignent un score F de 0,76 sur l&#039;extraction de caractéristiques d&#039;événement. Nous avons l&#039;intention d&#039;améliorer les modèles, puis de les utiliser sur d&#039;autres ressources de texte pour enrichir la mémoire des sites pollués avec des informations extraites sur les événements industriels.","2","2023-07-04 15:53:31","2023-07-04 15:58:08","15039",,"4","1","4","2","2","2",NULL,,"2023-07-04 03:53:15","0"
"1618388","While recent changes in Machine Translation state-of-the-art brought translation quality a step further, it is regularly acknowledged that the standard automatic metrics do not provide enough insights to fully measure the impact of neural models. This paper proposes a new type of evaluation focused specifically on the morphological competence of a system with respect to various grammatical phenomena. Our approach uses automatically generated pairs of source sentences, where each pair tests one morphological contrast. This methodology is used to compare several systems submitted at WMT'17 for English into Czech and Latvian.","Evaluating the morphological competence of Machine Translation Systems","Franck Burlot, François Yvon","1835","Évaluation de la compétence morphologique des systèmes de traduction automatique

Bien que les récents changements dans l'état de l'art de la traduction automatique aient fait progresser la qualité de la traduction, il est régulièrement reconnu que les mesures automatiques standard ne fournissent pas suffisamment d'informations pour mesurer pleinement l'impact des modèles neuronaux. Cet article propose un nouveau type d'évaluation axé spécifiquement sur la compétence morphologique d'un système par rapport à divers phénomènes grammaticaux. Notre approche utilise des paires de phrases sources générées automatiquement, où chaque paire teste un contraste morphologique. Cette méthodologie est utilisée pour comparer plusieurs systèmes soumis à WMT'17 pour la traduction de l'anglais vers le tchèque et le letton.","10599","3","370","Évaluation de la compétence morphologique des systèmes de traduction automatique

Bien que les récents changements dans l&#039;état de l&#039;art de la traduction automatique aient fait progresser la qualité de la traduction, il est régulièrement reconnu que les mesures automatiques standard ne fournissent pas suffisamment d&#039;informations pour mesurer pleinement l&#039;impact des modèles neuronaux. Cet article propose un nouveau type d&#039;évaluation axé spécifiquement sur la compétence morphologique d&#039;un système par rapport à divers phénomènes grammaticaux. Notre approche utilise des paires de phrases sources générées automatiquement, où chaque paire teste un contraste morphologique. Cette méthodologie est utilisée pour comparer plusieurs systèmes soumis à WMT&#039;17 pour la traduction de l&#039;anglais vers le tchèque et le letton.","2","2023-07-05 13:16:57","2023-07-05 13:21:11","10599",,"1","1","1","1","1","1",NULL,,"2023-07-05 01:16:55","0"
"2343408","Word alignments identify translational correspondences between words in a parallel sentence pair and is used, for instance, to learn bilingual dictionaries, to train statistical machine translation systems, or to perform quality estimation. In most areas of natural language processing, neural network models nowadays constitute the preferred approach, a situation that might also apply to word alignment models.In this work, we study and comprehensively evaluate neural models for unsupervised word alignment for four language pairs, contrasting several variants of neural models. We show that in most settings, neural versions of the IBM-1 and hidden Markov models vastly outperform their discrete counterparts. We also analyze typical alignment errors of the baselines that our models overcome to illustrate the benefits --- and the limitations --- of these new models for morphologically rich languages.","Neural Baselines for Word Alignments","Anh Khoa Ngo Ho, François Yvon","2109","Lignes de base neuronales pour les alignements de mots

Les alignements de mots identifient les correspondances traductionnelles entre les mots d'une paire de phrases parallèles et sont utilisés, par exemple, pour apprendre des dictionnaires bilingues, pour entraîner des systèmes de traduction automatique statistique ou pour effectuer une estimation de la qualité. Dans la plupart des domaines du traitement du langage naturel, les modèles de réseaux neuronaux constituent aujourd'hui l'approche privilégiée, une situation qui pourrait également s'appliquer aux modèles d'alignement de mots. Dans ce travail, nous étudions et évaluons de manière exhaustive les modèles neuronaux pour l'alignement de mots non supervisé pour quatre paires de langues, en comparant plusieurs variantes de modèles neuronaux. Nous montrons que dans la plupart des cas, les versions neuronales des modèles IBM-1 et des modèles de Markov cachés sont nettement plus performantes que leurs équivalents discrets. Nous analysons également les erreurs d'alignement typiques des lignes de base que nos modèles surmontent pour illustrer les avantages --- et les limites --- de ces nouveaux modèles pour les langues morphologiquement riches.","7619","3","371","Système neuronal de base pour les alignements de mots

Les alignements de mots identifient les correspondances traductionnelles entre les mots d&#039;une paire de phrases parallèles et sont utilisés, par exemple, pour apprendre des dictionnaires bilingues, pour entraîner des systèmes de traduction automatique statistique ou pour effectuer une estimation de la qualité. Dans la plupart des domaines du traitement du langage naturel, les modèles de réseaux neuronaux constituent aujourd&#039;hui l&#039;approche privilégiée, une situation qui pourrait également s&#039;appliquer aux modèles d&#039;alignement de mots. Dans ce travail, nous étudions et évaluons de manière exhaustive les modèles neuronaux pour l&#039;alignement de mots non supervisé pour quatre paires de langues, en comparant plusieurs variantes de modèles neuronaux. Nous montrons que dans la plupart des cas, les versions neuronales des modèles IBM-1 et des modèles de Markov cachés sont nettement plus performantes que leurs équivalents discrets. Nous analysons également les erreurs d&#039;alignement typiques des systèmes de base que nos modèles surmontent pour illustrer les avantages (et les limites) de ces nouveaux modèles pour les langues morphologiquement riches.","2","2023-07-05 13:21:28","2023-07-05 13:32:07","7619",,"1","1","4","1","3","1",NULL,,"2023-07-05 01:21:21","0"
"3396226","Resources for evaluating sentence-level and word-level alignment algorithms are unsatisfactory. Regarding sentence alignments, the existing data is too scarce, especially when it comes to difficult bitexts, containing instances of non-literal translations. Regarding word-level alignments, most available hand-aligned data provide a complete annotation at the level of words that is difficult to exploit, for lack of a clear semantics for alignment links. In this study, we propose new methodologies for collecting human judgements on alignment links, which have been used to annotate 4 new data sets, at the sentence and at the word level. These will be released online, with the hope that they will prove useful to evaluate alignment software and quality estimation tools for automatic alignment.","Novel elicitation and annotation schemes for sentential and sub-sentential alignments of bitexts","Yong Xu, François Yvon","971","Nouveaux schémas d'élicitation et d'annotation pour les alignements sententiels et sous-sententiels de bitextes

Les ressources permettant d'évaluer les algorithmes d'alignement au niveau des phrases et des mots ne sont pas satisfaisantes. En ce qui concerne les alignements de phrases, les données existantes sont trop rares, en particulier lorsqu'il s'agit de bitextes difficiles, contenant des exemples de traductions non littérales. En ce qui concerne les alignements au niveau des mots, la plupart des données d'alignement manuel disponibles fournissent une annotation complète au niveau des mots qui est difficile à exploiter, faute d'une sémantique claire pour les liens d'alignement. Dans cette étude, nous proposons de nouvelles méthodologies pour recueillir des jugements humains sur les liens d'alignement, qui ont été utilisés pour annoter 4 nouveaux ensembles de données, au niveau de la phrase et du mot. Ces données seront publiées en ligne, dans l'espoir qu'elles s'avéreront utiles pour évaluer les logiciels d'alignement et les outils d'estimation de la qualité pour l'alignement automatique.","11223","3","377","Nouveaux schémas d&#039;élicitation et d&#039;annotation pour les alignements phrastiques et sous-phrastiques de bi-textes

Les ressources permettant d&#039;évaluer les algorithmes d&#039;alignement au niveau des phrases et des mots ne sont pas satisfaisantes. En ce qui concerne les alignements de phrases, les données existantes sont trop rares, en particulier lorsqu&#039;il s&#039;agit de bi-textes difficiles, contenant des exemples de traductions non littérales. En ce qui concerne les alignements au niveau des mots, la plupart des données d&#039;alignement manuel disponibles fournissent une annotation complète au niveau des mots qui est difficile à exploiter, faute d&#039;une sémantique claire pour les liens d&#039;alignement. Dans cette étude, nous proposons de nouvelles méthodologies pour recueillir des jugements humains sur les liens d&#039;alignement, qui ont été utilisés pour annoter 4 nouveaux ensembles de données, au niveau de la phrase et du mot. Ces données seront publiées en ligne, dans l&#039;espoir qu&#039;elles s&#039;avéreront utiles pour évaluer les logiciels d&#039;alignement et les outils d&#039;estimation de la qualité pour l&#039;alignement automatique.","2","2023-07-05 15:15:47","2023-07-05 15:20:12","11223",,"1","1","4","1","2","1",NULL,,"2023-07-05 03:15:42","0"
"2572041","Sequence-to-sequence models have lead to significant progress in keyphrase generation, butit  remains  unknown  whether  they  are  reli-able enough to be beneficial for document re-trieval.This  study  provides  empirical  evi-dence  that  such  models  can  significantly  improve  retrieval  performance,  and  introducesa new extrinsic evaluation framework that al-lows  for  a  better  understanding  of  the  limi-tations  of  keyphrase  generation  models.   Using  this  framework,  we  point  out  and  dis-cuss  the  difficulties  encountered  with  supplementing documents with –not present in text– keyphrases,  and  generalizing  models  acrossdomains.   Our  code  is  available  at https://github.com/boudinfl/ir-using-kg","Keyphrase Generation for Scientific Document Retrieval","Florian Boudin, Ygor Gallina, Akiko Aizawa","1445","Génération de phrases-clés pour la recherche de documents scientifiques

Cette étude fournit des preuves empiriques que de tels modèles peuvent améliorer de manière significative les performances de recherche, et introduit un nouveau cadre d'évaluation extrinsèque qui permet de mieux comprendre les limites des modèles de génération de phrases-clés. En utilisant ce cadre, nous soulignons et discutons les difficultés rencontrées pour compléter les documents avec des phrases-clés non présentes dans le texte, et pour généraliser les modèles à travers les domaines. Notre code est disponible à l'adresse suivante : https://github.com/boudinfl/ir-using-kg","9713","3","378","Génération de termes-clés pour la recherche de documents scientifiques

Les modèles séquence à séquence ont permis de réaliser des progrès significatifs dans la génération de termes-clés. Cependant, on ne sait toujours pas s&#039;ils sont suffisamment fiables pour être utiles à la recherche de documents. Cette étude fournit des preuves empiriques que de tels modèles peuvent améliorer de manière notable les performances de recherche, et introduit un nouveau cadre d&#039;évaluation extrinsèque qui permet de mieux comprendre les limites des modèles de génération de termes-clés. En utilisant ce cadre, nous soulignons et discutons les difficultés rencontrées pour compléter les documents avec des termes-clés non présents dans le texte, et pour généraliser les modèles à travers les domaines. Notre code est disponible à l&#039;adresse suivante : https://github.com/boudinfl/ir-using-kg","2","2023-07-05 15:20:35","2023-07-05 15:28:17","9713",,"4","1","4","2","1","4",NULL,"Il manque la première phrase dans la TA","2023-07-05 03:20:33","0"
"1960634","Electronic versions of literary works abound on the Internet and the rapid dissemination of electronic readers will make electronic books more and more common. It is often the case thatliterary works exist in more than one language, suggesting that, if properly aligned, they could be turned into useful resources for many practical applications, such as writing and language learning aids, translation studies, or data-based machine translation. To be of any use, these bilingual works need to be aligned as precisely as possible, a notoriously difficult task. In this paper, we revisit the problem of sentence alignment for literary works and explore the performance of a new, multi-pass, approach based on a combination of systems. Experiments conducted on excerpts of ten masterpieces of the French and English literature show that our approach significantly outperforms two open source tools.","Aligning Bilingual Literary Works: a Pilot Study","Qian Yu, Aurélien Max, François Yvon","3529","Alignement des œuvres littéraires bilingues : une étude pilote

Les versions électroniques d'œuvres littéraires abondent sur l'internet et la diffusion rapide des lecteurs électroniques rendra les livres électroniques de plus en plus courants. Il arrive souvent que des œuvres littéraires existent dans plus d'une langue, ce qui suggère que, si elles sont correctement alignées, elles pourraient être transformées en ressources utiles pour de nombreuses applications pratiques, telles que les aides à l'écriture et à l'apprentissage des langues, les études de traduction ou la traduction automatique basée sur des données. Pour être utiles, ces œuvres bilingues doivent être alignées aussi précisément que possible, une tâche notoirement difficile. Dans cet article, nous revisitons le problème de l'alignement des phrases pour les œuvres littéraires et explorons les performances d'une nouvelle approche, à plusieurs passages, basée sur une combinaison de systèmes. Des expériences menées sur des extraits de dix chefs-d'œuvre de la littérature française et anglaise montrent que notre approche surpasse de manière significative deux outils open source.","8543","3","379","Alignement des œuvres littéraires bilingues : une étude pilote

Les versions électroniques d&#039;œuvres littéraires abondent sur Internet, et la diffusion rapide des lecteurs électroniques rendra les livres électroniques de plus en plus communs. Il arrive souvent que des œuvres littéraires existent dans plus d&#039;une langue, ce qui suggère que, si elles sont correctement alignées, elles pourraient être transformées en ressources utiles pour de nombreuses applications pratiques, telles que les aides à l&#039;écriture et à l&#039;apprentissage des langues, les études de traduction ou la traduction automatique basée sur des données. Pour être utiles, ces œuvres bilingues doivent être alignées aussi précisément que possible, une tâche indéniablement difficile. Dans cet article, nous revisitons le problème de l&#039;alignement des phrases pour les œuvres littéraires et explorons les performances d&#039;une nouvelle approche, à plusieurs passages, basée sur une combinaison de systèmes. Des expériences menées sur des extraits de dix chefs-d&#039;œuvre de la littérature française et anglaise montrent que notre approche surpasse de manière significative deux outils open source.","2","2023-07-05 15:29:29","2023-07-05 15:48:53","8543",,"1","1","1","3","1","1",NULL,,"2023-07-05 03:29:27","0"
"1425724","The paper deals with the generation of ReadME files from an ontology-based description of NLP tool. ReadME files are structured and organised according to properties defined in the ontology. One of the problem is being able to deal with multilingual generation of texts. To do so, we propose to map the ontol-ogy elements to multilingual knowledge defined in a SKOS ontology.","ReadME generation from an OWL ontology describing NLP tools","Driss Sadoun, Satenik Mkhitaryan, Damien Nouvel, Mathieu Valette","1698","Génération ReadME à partir d'une ontologie OWL décrivant les outils NLP

L'article traite de la génération de fichiers ReadME à partir d'une description ontologique de l'outil NLP. Les fichiers ReadME sont structurés et organisés en fonction des propriétés définies dans l'ontologie. L'un des problèmes est de pouvoir traiter la génération multilingue de textes. Pour ce faire, nous proposons de mettre en correspondance les éléments d'ontologie avec des connaissances multilingues définies dans une ontologie SKOS.","4948","6","381","Génération ReadME à partir d&#039;une ontologie OWL décrivant les outils TAL

L&#039;article traite de la génération de fichiers ReadME à partir d&#039;une description ontologique de l&#039;outil TAL. Les fichiers ReadME sont structurés et organisés en fonction des propriétés définies dans l&#039;ontologie. L&#039;un des problèmes principaux est de pouvoir traiter la génération multilingue de textes. Pour ce faire, nous proposons de mettre en correspondance les éléments d&#039;ontologie avec des connaissances multilingues définies dans une ontologie SKOS.","2","2023-07-05 15:55:19","2023-07-05 15:58:20","4948",,"1","1","4","3","1","1",NULL,,"2023-07-05 03:54:59","0"
"3930344","A straightforward approach to context-aware neural machine translation consists in feeding the standard encoder-decoder architecture with a window of consecutive sentences, formed by the current sentence and a number of sentences from its context concatenated to it. In this work, we propose an improved concatenation approach that encourages the model to focus on the translation of the current sentence, discounting the loss generated by target context. We also propose an additional improvement that strengthen the notion of sentence boundaries and of relative sentence distance, facilitating model compliance to the context-discounted objective. We evaluate our approach with both average-translation quality metrics and contrastive test sets for the translation of inter-sentential discourse phenomena, proving its superiority to the vanilla concatenation approach and other sophisticated context-aware systems.","Focused Concatenation for Context-Aware Neural Machine Translation","Lorenzo Lupo, Marco Dinarelli, Laurent Besacier","415","Concaténation focalisée pour la traduction automatique neuronale consciente du contexte

Une approche simple de la traduction automatique neuronale sensible au contexte consiste à alimenter l'architecture codeur-décodeur standard avec une fenêtre de phrases consécutives, formée par la phrase actuelle et un certain nombre de phrases de son contexte concaténées à celle-ci. Dans ce travail, nous proposons une approche de concaténation améliorée qui encourage le modèle à se concentrer sur la traduction de la phrase courante, en écartant la perte générée par le contexte cible. Nous proposons également une amélioration supplémentaire qui renforce la notion de limites de phrases et de distance relative entre les phrases, facilitant la conformité du modèle à l'objectif d'actualisation du contexte. Nous évaluons notre approche à l'aide de mesures de la qualité moyenne de la traduction et d'ensembles de tests contrastifs pour la traduction de phénomènes discursifs inter-sententiels, prouvant sa supériorité par rapport à l'approche de concaténation vanille et à d'autres systèmes sophistiqués tenant compte du contexte.","10169","3","383","Concaténation focalisée pour la traduction automatique neuronale sensible au contexte

Une approche simple de la traduction automatique neuronale sensible au contexte consiste à alimenter l&#039;architecture codeur-décodeur standard avec une fenêtre de phrases consécutives, formée par la phrase actuelle et un certain nombre de phrases de son contexte concaténées à celle-ci. Dans ce travail, nous proposons une approche de concaténation améliorée qui encourage le modèle à se concentrer sur la traduction de la phrase courante, en écartant la perte générée par le contexte cible. Nous proposons également une amélioration supplémentaire qui renforce la notion de limites de phrases et de distance relative entre les phrases, facilitant la conformité du modèle à l&#039;objectif d&#039;actualisation du contexte. Nous évaluons notre approche à l&#039;aide de mesures de la qualité moyenne de la traduction et d&#039;ensembles de tests contrastifs pour la traduction de phénomènes discursifs interphrastiques, prouvant sa supériorité par rapport à l&#039;approche de concaténation de base et à d&#039;autres systèmes sophistiqués tenant sensibles au contexte.","2","2023-07-05 16:15:04","2023-07-05 16:22:20","10169",,"2","1","4","2","1","4",NULL,,"2023-07-05 04:15:02","0"
"1704769","Translation into a morphologically rich language  requires a large output vocabulary to model various  morphological phenomena, which is a challenge for  neural machine translation architectures. To address this issue,  the present paper investigates the impact of having  two output factors with a system able to generate  separately two distinct representations of the target  words. Within this framework, we investigate  several word representations that correspond to  different distributions of morpho-syntactic information  across both factors. We report experiments for translation  from English into two morphologically rich languages,  Czech and Latvian, and show the importance of explicitly  modeling target morphology.","Word Representations in Factored Neural Machine Translation","Franck Burlot, Mercedes Garcia-Martinez, Loïc Barrault, Fethi Bougares, François Yvon","1873","Représentations des mots dans la traduction automatique neuronale factorisée

La traduction dans une langue morphologiquement riche nécessite un large vocabulaire de sortie pour modéliser les différents phénomènes morphologiques, ce qui constitue un défi pour les architectures neuronales de traduction automatique. Pour résoudre ce problème, le présent article étudie l'impact de l'existence de deux facteurs de sortie avec un système capable de générer séparément deux représentations distinctes des mots cibles. Dans ce cadre, nous étudions plusieurs représentations de mots qui correspondent à différentes distributions d'informations morpho-syntaxiques dans les deux facteurs. Nous présentons des expériences de traduction de l'anglais vers deux langues morphologiquement riches, le tchèque et le letton, et montrons l'importance de la modélisation explicite de la morphologie de la cible.","10637","3","387","Représentations des mots dans la traduction automatique neuronale factorisée

La traduction dans une langue morphologiquement riche nécessite un large vocabulaire de sortie pour modéliser les différents phénomènes morphologiques, ce qui constitue un défi pour les architectures neuronales de traduction automatique. Pour résoudre ce problème, le présent article étudie l&#039;impact de l&#039;existence de deux facteurs de sortie avec un système capable de générer séparément deux représentations distinctes des mots cibles. Dans ce cadre, nous étudions plusieurs représentations de mots qui correspondent à différentes distributions d&#039;informations morphosyntaxiques dans les deux facteurs. Nous présentons des expériences de traduction de l&#039;anglais vers deux langues morphologiquement riches, le tchèque et le letton, et montrons l&#039;importance de la modélisation explicite de la morphologie de la cible.","2","2023-07-06 15:28:43","2023-07-06 15:35:27","10637",,"1","1","1","1","3","1",NULL,,"2023-07-06 03:28:41","0"
"1742378","This paper describes LIUM submissions to WMT17 News Translation Task for English↔German, English↔Turkish, English→Czech and English→Latvian language pairs. We train BPE-based attentive Neural Machine Translation systems with and without factored outputs using the open source nmtpy framework. Competitive scores were obtained by en-sembling various systems and exploiting the availability of target monolingual corpora for back-translation. The impact of back-translation quantity and quality is also analyzed for English→Turkish where our post-deadline submission surpassed the best entry by +1.6 BLEU.","LIUM Machine Translation Systems for WMT17 News Translation Task","Mercedes Garcia-Martinez, Ozan Caglayan, Walid Aransa, Adrien Bardet, Fethi Bougares, Loïc Barrault","1887","Systèmes de traduction automatique Lium pour WMT17 Nouvelles Tâche de Traduction

Cet article décrit les soumissions de LIUM à WMT17 News Translation Task pour l’anglais, l’allemand, l’anglais, l’anglais, le tchèque et l’anglais→langue latine. Nous formons des systèmes de traduction automatique neuronales attentifs basés sur BPE avec et sans sorties factorisées en utilisant le framework nmtpy open source. Des scores compétitifs ont été obtenus par l’assemblage de divers systèmes et l’exploitation de la disponibilité des corpus monolingues cibles pour la traduction en arrière. L’impact de la rétro-traduction quantitative et de la qualité est également analysé pour English→Turkish où notre soumission post-date a dépassé la meilleure entrée de + 1,6 BLEU.","1661","4","388","Systèmes de traduction automatique LIUM pour WMT17 News Translation Task 

Cet article décrit les soumissions de LIUM à WMT17 News Translation Task pour les paires de langue anglais ↔ allemand, anglais ↔ turc, anglais ↔ tchèque et anglais ↔ letton. Nous formons des systèmes de traduction automatique neuronales attentifs fondés sur BPE avec et sans sorties factorisées en utilisant le framework nmtpy open source. Des scores compétitifs ont été obtenus par l’assemblage de divers systèmes et l’exploitation de la disponibilité des corpus monolingues cibles pour la traduction en arrière. L’impact de la rétro-traduction quantitative et de la qualité est également analysé pour la paire anglais → turc où notre soumission post-date a dépassé la meilleure entrée de + 1,6 BLEU.","2","2023-07-06 15:38:37","2023-07-06 15:46:25","1661",,"4","1","4","4","4","4",NULL,,"2023-07-06 03:38:21","0"
"1800739","For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models' ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50% accuracy on our coreference test set and 53.5% for coherence/cohesion (compared to a non-contextual baseline of 50%). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5% for corefer-ence and 57% for coherence/cohesion), highlighting the importance of target-side context.","Evaluating Discourse Phenomena in Neural Machine Translation","Rachel Bawden, Rico Sennrich, Alexandra Birch, Barry Haddow","1906","Evaluation de Phénomènes de Discours en Traduction Automatique Neuronale

Pour que la traduction automatique s'attaque aux phénomènes de discours, les modèles doivent avoir accès à un contexte linguistique extrasententiel. Récemment, on s'est intéressé au contexte de modélisation dans la traduction automatique neuronale (NMT), mais les modèles ont été principalement évalués avec des métriques automatiques standard, mal adaptées à l'évaluation des phénomènes de discours. Dans cet article, nous présentons des tests de discours faits à la main, conçus pour tester la capacité des modèles à exploiter les phrases source et cible précédentes. Nous étudions les performances des modèles NMT multi-encodeurs récemment proposés, formés sur les sous-titres anglais vers français. Nous explorons également une nouvelle façon d'exploiter le contexte de la phrase précédente. Malgré les progrès réalisés avec BLEU, les modèles multi-encodeurs apportent des améliorations limitées dans la gestion des phénomènes de discours : Précision de 50 % sur notre ensemble de tests de correspondance et de 53,5 % pour la cohérence/cohésion (par rapport à une base non contextuelle de 50 %). Une simple stratégie de décodage de la concaténation de la phrase précédente et de la phrase actuelle conduit à de bonnes performances, et notre nouvelle stratégie de multi-codage et décodage de deux phrases conduit à la meilleure performance (72,5% pour la corrélation et 57% pour la cohérence/cohésion), soulignant l'importance du contexte du côté cible.","5156","6","389","Évaluer les phénomènes de discours en traduction automatique neuronale

Pour que la traduction automatique s&#039;attaque aux phénomènes de discours, les modèles doivent avoir accès à un contexte linguistique extraphrastique. Récemment, on s&#039;est intéressé au contexte de modélisation dans la traduction automatique neuronale (NMT, Neural Machine Translation), mais les modèles ont été principalement évalués avec des métriques automatiques standard, mal adaptées à l&#039;évaluation des phénomènes de discours. Dans cet article, nous présentons des tests de discours faits à la main, conçus pour tester la capacité des modèles à exploiter les phrases source et cible précédentes. Nous étudions les performances des modèles NMT multiencodeurs récemment proposés, formés sur les sous-titres anglais vers français. Nous explorons également une nouvelle façon d&#039;exploiter le contexte de la phrase précédente. Malgré les progrès réalisés avec BLEU, les modèles multiencodeurs apportent des améliorations limitées dans la gestion des phénomènes de discours : précision de 50 % sur notre ensemble de tests de correspondance et de 53,5 % pour la cohérence/cohésion (par rapport à une base non contextuelle de 50 %). Une simple stratégie de décodage de la concaténation de la phrase précédente et de la phrase actuelle conduit à de bonnes performances, et notre nouvelle stratégie de multicodage et décodage de deux phrases conduit à la meilleure performance (72,5% pour la corrélation et 57% pour la cohérence/cohésion), soulignant l&#039;importance du contexte du côté cible.","2","2023-07-06 15:47:21","2023-07-06 15:51:12","5156",,"2","1","4","1","2","1",NULL,,"2023-07-06 03:47:18","0"
"1821299","Nowadays machine translation is widely used, but the required data for training, tuning and testing a machine translation engine is often not sufficient or not useful. The automatic selection of data that are qualitatively appropriate for building translation models can help improve translation accuracy. In this paper, we used a large parallel corpus of educational video lecture subtitles as well as text posted by students and lecturers on the course fora. The text is quite challenging to translate due to the scientific domains involved and its informal genre. We applied a random forest classification schema on the output of three machine translation models (one based on statistical machine translation and two on neural machine translation) in order to automatically identify the best output. The unorthodox language phenomena observed as well as the rich-in-terminology scientific domains addressed in the educational video lectures, the language-independent nature of the approach, and the tackled three-class classification problem constitute innovative challenges of the work described herein.","Automatic Selection of Parallel Data for Machine Translation","Despoina Mouratidis, Katia Kermanidis","3433","Sélection automatique de données parallèles pour la traduction automatique

De nos jours, la traduction automatique est largement utilisée, mais les données requises pour la formation, le réglage et le test d’un moteur de traduction automatique ne sont souvent pas suffisantes ou inutiles. La sélection automatique de données qualitativement appropriées pour construire des modèles de traduction peut aider à améliorer la précision de la traduction. Dans cet article, nous avons utilisé un vaste corpus parallèle de sous-titres de conférences vidéo éducatives ainsi que du texte posté par les étudiants et les conférenciers sur les forums de cours. Le texte est assez difficile à traduire en raison des domaines scientifiques impliqués et de son genre informel. Nous avons appliqué un schéma de classification forestière aléatoire sur la sortie de trois modèles de traduction automatique (l’un basé sur la traduction automatique statistique et deux sur la traduction automatique neuronale) afin d’identifier automatiquement la meilleure sortie. Les phénomènes linguistiques peu orthodoxes observés ainsi que les domaines scientifiques riches en terminologie abordés dans les vidéoconférences éducatives, le caractère indépendant de la langue de l’approche et le problème de classification en trois classes abordé constituent des défis novateurs du travail décrit ici.","2214","4","390","Sélection automatique de données parallèles pour la traduction automatique

De nos jours, la traduction automatique est largement utilisée, mais les données requises pour la formation, le réglage et le test d’un moteur de traduction automatique sont souvent insuffisantes ou inutiles. La sélection automatique de données qualitativement appropriées pour construire des modèles de traduction peut aider à améliorer la précision de la traduction. Dans cet article, nous avons utilisé un vaste corpus parallèle de sous-titres de conférences vidéo éducatives ainsi que du texte posté par les étudiants et les conférenciers sur les forums de cours. Le texte est assez difficile à traduire en raison des domaines scientifiques impliqués et de son genre informel. Nous avons appliqué un schéma de classification de forêt aléatoire (random forest) sur la sortie de trois modèles de traduction automatique (l’un basé sur la traduction automatique statistique et deux sur la traduction automatique neuronale) afin d’identifier automatiquement la meilleure sortie. Les phénomènes linguistiques peu orthodoxes observés ainsi que les domaines scientifiques riches en terminologie abordés dans les vidéoconférences éducatives, le caractère indépendant de la langue de l’approche et le problème de classification en trois classes abordé constituent des défis novateurs du travail décrit ici.","2","2023-07-06 15:52:01","2023-07-06 15:59:07","2214",,"1","1","3","4","1","1",NULL,,"2023-07-06 03:51:58","0"
"2460000","In this paper, we propose an approach for semi-automatically creating a data-to-text (D2T) corpus for Russian that can be used to learn a D2T natural language generation model. An error analysis of the output of an English-to-Russian neural machine translation system shows that 80% of the automatically translated sentences contain an error and that 53% of all translation errors bear on named entities (NE). We therefore focus on named entities and introduce two post-editing techniques for correcting wrongly translated NEs.","Creating a Corpus for Russian Data-to-Text Generation Using Neural Machine Translation and Post-Editing","Anastasia Shimorina, Elena Khasanova, Claire Gardent","1531","Création d'un corpus pour la génération de textes à partir de données en russe à l'aide de la traduction automatique neuronale et de la post-édition

Dans cet article, nous proposons une approche pour la création semi-automatique d'un corpus de données vers texte (D2T) pour le russe qui peut être utilisé pour apprendre un modèle de génération de langage naturel D2T. Une analyse d'erreur de la sortie d'un système de traduction automatique neuronal anglais-russe montre que 80% des phrases traduites automatiquement contiennent une erreur et que 53% de toutes les erreurs de traduction portent sur des entités nommées (NE). Nous nous concentrons donc sur les entités nommées et introduisons deux techniques de post-édition pour corriger les EN mal traduites.","10295","3","393","Création d&#039;un corpus pour la génération data-to-text en russe à l&#039;aide de la traduction automatique neuronale et de la post-édition

Dans cet article, nous proposons une approche pour la création semi-automatique d&#039;un corpus de data-to-text (D2T) pour le russe qui peut être utilisé pour apprendre un modèle de génération de langage naturel D2T. Une analyse d&#039;erreur de la sortie d&#039;un système de traduction automatique neuronal anglais-russe montre que 80% des phrases traduites automatiquement contiennent une erreur et que 53% de toutes les erreurs de traduction portent sur des entités nommées (EN). Nous nous concentrons donc sur les entités nommées et introduisons deux techniques de post-édition pour corriger les EN mal traduites.","2","2023-07-06 17:02:09","2023-07-06 17:04:34","10295",,"1","1","4","2","1","1",NULL,,"2023-07-06 05:02:07","0"
"3505095","In recent years, there has been a resurgence in research on empirical methods for machine translation. Most of this research has been focused on high-resource, European languages. Despite the fact that around 30% of all languages spoken worldwide are African, the latter have been heavily under investigated and this, partly due to the lack of public parallel corpora online. Furthermore, despite their large number (more than 2,000) and the similarities between them, there is currently no publicly available study on how to use this multilingualism (and associated similarities) to improve machine translation systems performance on African languages. So as to address these issues, we propose a new dataset (from a source that allows us to use and release) for African languages that provides parallel data for vernaculars not present in commonly used dataset like JW300. To exploit multilingualism, we first use a historical approach based on migrations of population to identify similar vernaculars. We also propose a new metric to automatically evaluate similarities between languages. This new metric does not require word level parallelism like traditional methods but only paragraph level parallelism. We then show that performing Masked Language Modelling and Translation Language Modeling in addition to multi-task learning on a cluster of similar languages leads to a strong boost of performance in translating individual pairs inside this cluster. In particular, we record an improvement of 29 BLEU on the pair Bafia-Ewondo using our approaches compared to previous work methods that did not exploit multilingualism in any way. Finally, we release the dataset and code of this work to ensure reproducibility and accelerate research in this domain.","On the use of linguistic similarities to improve Neural Machine Translation for African Languages","Pascal Junior Tikeng Notsawo, Brice Nanda, James Assiene","5068","Sur l’utilisation des similitudes linguistiques pour améliorer la traduction automatique neuronale pour les langues africaines

Ces dernières années, il y a eu une recrudescence de la recherche sur les méthodes empiriques de traduction automatique. La plupart de ces recherches se sont concentrées sur les langues européennes à hautes ressources. Malgré le fait qu’environ 30 % de toutes les langues parlées dans le monde sont africaines, ces dernières ont fait l’objet d’une enquête approfondie, en partie en raison de l’absence de corpus publics parallèles en ligne. En outre, malgré leur grand nombre (plus de 2 000) et leurs similitudes, il n’existe actuellement aucune étude publique sur la manière d’utiliser ce multilinguisme (et les similitudes associées) pour améliorer la performance des systèmes de traduction automatique dans les langues africaines. Afin de résoudre ces problèmes, nous proposons un nouvel ensemble de données (à partir d’une source qui nous permet d’utiliser et de publier) pour les langues africaines qui fournit des données parallèles pour les vernaculaires non présents dans les jeux de données couramment utilisés comme JW300. Pour exploiter le multilinguisme, nous utilisons d’abord une approche historique basée sur les migrations de population pour identifier des vernaculaires similaires. Nous proposons également une nouvelle métrique pour évaluer automatiquement les similitudes entre les langues. Cette nouvelle métrique ne nécessite pas de parallélisme au niveau des mots comme les méthodes traditionnelles, mais seulement le parallélisme au niveau des paragraphes. Nous montrons ensuite que l’exécution de la modélisation du langage masqué et de la modélisation du langage de traduction en plus de l’apprentissage multi-tâches sur un cluster de langues similaires conduit à une forte augmentation des performances dans la traduction de paires individuelles à l’intérieur de ce cluster. En particulier, nous observons une amélioration de 29 BLEU sur la paire Bafia-Ewondo en utilisant nos approches par rapport aux méthodes de travail précédentes qui n’exploitaient en aucune façon le multilinguisme. Enfin, nous libérons l’ensemble de données et le code de ce travail pour assurer la reproductibilité et accélérer la recherche dans ce domaine.","11855","4","394","Sur l’utilisation des similitudes linguistiques pour améliorer la traduction automatique neuronale pour les langues africaines

Ces dernières années, il y a eu une recrudescence de la recherche sur les méthodes empiriques de traduction automatique. La plupart de ces recherches se sont concentrées sur les langues européennes bien dotées. Malgré le fait qu’environ 30 % de toutes les langues parlées dans le monde sont africaines, ces dernières sont loin d&#039;avoir fait l’objet d’études approfondies, en partie en raison de l’absence de corpus publics parallèles en ligne. En outre, malgré leur grand nombre (plus de 2 000) et leurs similitudes, il n’existe actuellement aucune étude publique sur la manière d’utiliser ce multilinguisme (et les similitudes associées) pour améliorer la performance des systèmes de traduction automatique dans les langues africaines. Afin de résoudre ces problèmes, nous proposons un nouvel ensemble de données (à partir d’une source qui nous permet d’utiliser et de publier) pour les langues africaines qui fournit des données parallèles pour les vernaculaires non présents dans les jeux de données couramment utilisés comme JW300. Pour exploiter le multilinguisme, nous utilisons d’abord une approche historique basée sur les migrations de population pour identifier des vernaculaires similaires. Nous proposons également une nouvelle métrique pour évaluer automatiquement les similitudes entre les langues. Cette nouvelle métrique ne nécessite pas de parallélisme au niveau des mots comme les méthodes traditionnelles, mais seulement le parallélisme au niveau des paragraphes. Nous montrons ensuite que l’exécution de la modélisation du langage masqué et de la modélisation du langage de traduction en plus de l’apprentissage multitâches sur un cluster de langues similaires conduit à une forte augmentation des performances dans la traduction de paires individuelles à l’intérieur de ce cluster. En particulier, nous observons une amélioration de 29 BLEU sur la paire Bafia-Ewondo en utilisant nos approches par rapport aux méthodes de travail précédentes qui n’exploitaient en aucune façon le multilinguisme. Enfin, nous libérons l’ensemble de données et le code de ce travail pour assurer la reproductibilité et accélérer la recherche dans ce domaine.","2","2023-07-06 17:10:02","2023-07-06 17:13:48","11855",,"4","1","4","2","1","1",NULL,,"2023-07-06 05:09:59","0"
"3977982","One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as an image. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of building effective cross-modal representations but also by the lack of specific evaluation and training data. We present a new MMT approach based on a strong text-only MT model, which uses neural adapters and a novel guided self-attention mechanism and which is jointly trained on both visual masking and MMT. We also release CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation dataset, composed of ambiguous sentences and their possible translations, accompanied by disambiguating images corresponding to each translation. Our approach obtains competitive results over strong text-only models on standard English-to-French benchmarks and outperforms these baselines and state-of-the-art MMT systems with a large margin on our contrastive test set.","Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation","Matthieu Futeral, Cordelia Schmid, Ivan Laptev, Benoît Sagot, Rachel Bawden","3818","S'attaquer à l'ambiguïté avec des images : Traduction automatique multimodale améliorée et évaluation contrastive

L'un des principaux défis de la traduction automatique (TA) est l'ambiguïté, qui peut dans certains cas être résolue par l'accompagnement d'un contexte tel qu'une image. Cependant, des travaux récents dans le domaine de la traduction automatique multimodale ont montré qu'il était difficile d'obtenir des améliorations à partir d'images, limitées non seulement par la difficulté de construire des représentations multimodales efficaces, mais aussi par le manque de données d'évaluation et d'entraînement spécifiques. Nous présentons une nouvelle approche de la MT multimodale basée sur un modèle de MT textuelle solide, qui utilise des adaptateurs neuronaux et un nouveau mécanisme d'auto-attention guidée et qui est entraîné conjointement sur le masquage visuel et la MT multimodale. Nous publions également CoMMuTE, un ensemble de données d'évaluation de la traduction multimodale multilingue contrastive, composé de phrases ambiguës et de leurs traductions possibles, accompagnées d'images désambiguïsantes correspondant à chaque traduction. Notre approche obtient des résultats compétitifs par rapport à des modèles textuels forts sur des benchmarks anglais-français standards et surpasse ces baselines et les systèmes MMT de pointe avec une grande marge sur notre ensemble de tests contrastifs.","18634","3","395","S&#039;attaquer à l&#039;ambiguïté avec des images : Traduction automatique multimodale améliorée et évaluation contrastive

L&#039;un des principaux défis de la traduction automatique (TA) est l&#039;ambiguïté, qui peut dans certains cas être résolue par l&#039;accompagnement d&#039;un contexte tel qu&#039;une image. Cependant, des travaux récents dans le domaine de la traduction automatique multimodale ont montré qu&#039;il était difficile d&#039;obtenir des améliorations à partir d&#039;images, limitées non seulement par la difficulté de construire des représentations multimodales efficaces, mais aussi par le manque de données d&#039;évaluation et d&#039;entraînement spécifiques. Nous présentons une nouvelle approche de la MT multimodale basée sur un modèle de MT textuelle solide, qui utilise des adaptateurs neuronaux et un nouveau mécanisme d&#039;auto-attention guidée et qui est entraîné conjointement sur le masquage visuel et la MT multimodale. Nous publions également CoMMuTE, un ensemble de données d&#039;évaluation de la traduction multimodale multilingue contrastive, composé de phrases ambiguës et de leurs traductions possibles, accompagnées d&#039;images désambiguïsantes correspondant à chaque traduction. Notre approche obtient des résultats compétitifs par rapport à des modèles textuels forts sur des benchmarks anglais-français standards et surpasse ces modèles de base et les systèmes de traduction automatique multimodale de pointe avec une grande marge sur notre ensemble de tests contrastifs.","2","2023-07-06 17:14:46","2023-07-06 17:17:02","18634",,"1","1","3","1","1","1",NULL,,"2023-07-06 05:14:45","0"
"3686763","Building effective Neural Machine Translation models often implies accommodating diverse sets of heterogeneous data so as to optimize performance for the domain(s) of interest. Such multi-source / multi-domain adaptation problems are typically approached through instance selection or reweighting strategies, based on a static assessment of the relevance of training instances with respect to the task at hand. In this paper, we study dynamic data selection strategies that are able to automatically re-evaluate the usefulness of data samples in the course of training. Based on the results of multiple experiments, we show that our method offer a generic framework to automatically handle several real-world situations, from multi-source or unsupervised domain adaptation to multidomain learning.","Multi-Domain Adaptation in Neural Machine Translation with Dynamic Sampling Strategies","Minh-Quang Pham, Josep Crego, François Yvon","725","Adaptation multi-domaine en traduction automatique neuronale avec stratégies d'échantillonnage dynamique

La construction de modèles efficaces de traduction automatique neuronale implique souvent de prendre en compte divers ensembles de données hétérogènes afin d'optimiser les performances pour le ou les domaines d'intérêt. De tels problèmes d'adaptation multi-sources / multi-domaines sont généralement abordés par le biais de stratégies de sélection ou de repondération d'instances, basées sur une évaluation statique de la pertinence des instances d'apprentissage par rapport à la tâche en cours. Dans cet article, nous étudions les stratégies de sélection dynamique des données qui sont capables de réévaluer automatiquement l'utilité des échantillons de données au cours de la formation. Sur la base des résultats de multiples expériences, nous montrons que notre méthode offre un cadre générique pour gérer automatiquement plusieurs situations réelles, de l'adaptation de domaine multi-source ou non supervisée à l'apprentissage multidomaine.","4472","6","405","Adaptation multidomaine en traduction automatique neuronale avec stratégies d&#039;échantillonnage dynamique

La construction de modèles de traduction automatique neuronale efficaces implique souvent de prendre en compte divers ensembles de données hétérogènes afin d&#039;optimiser les performances pour le ou les domaines d&#039;intérêt. De tels problèmes d&#039;adaptation multisource/multidomaine sont généralement abordés par le biais de stratégies de sélection ou de repondération d&#039;instances, basées sur une évaluation statique de la pertinence des instances d&#039;apprentissage par rapport à la tâche en cours. Dans cet article, nous étudions les stratégies de sélection dynamique des données qui sont capables de réévaluer automatiquement l&#039;utilité des échantillons de données au cours de la formation. Sur la base des résultats de multiples expériences, nous montrons que notre méthode offre un cadre générique pour gérer automatiquement plusieurs situations réelles, de l&#039;adaptation de domaine multisource ou non supervisée à l&#039;apprentissage multidomaine.","2","2023-07-07 14:25:25","2023-07-07 14:31:10","4472",,"1","1","1","3","3","4",NULL,,"2023-07-07 02:25:23","0"
"3697442","Many types of distributional word embeddings (weakly) encode linguistic regularities as directions (the difference between jump and jumped will be in a similar direction to that of walk and walked, and so on). Several attempts have been made to explain this fact. We respond to Allen and Hospedales' recent (ICML, 2019) theoretical explanation, which claims that word2vec and GloVe will encode linguistic regularities whenever a specific relation of paraphrase holds between the four words involved in the regularity. We demonstrate that the explanation does not go through: the paraphrase relations needed under this explanation do not hold empirically.","Paraphrases do not explain word analogies","Louis Fournier, Ewan Dunbar","709","Les paraphrases n’expliquent pas les analogies de mots

De nombreux types d’incorporations de mots distributionnels (faiblement) codent les régularités linguistiques comme directions (la différence entre sauter et sauter sera dans une direction similaire à celle de la marche et de la marche, et ainsi de suite). Plusieurs tentatives ont été faites pour expliquer ce fait. Nous répondons à l’explication théorique récente d’Allen et Hospedales (ICML, 2019), qui affirme que word2vec et GloVe coderont les régularités linguistiques chaque fois qu’une relation spécifique de paraphrase entre les quatre mots impliqués dans la régularité. Nous démontrons que l’explication ne passe pas par: les relations de paraphrase nécessaires sous cette explication ne tiennent pas empiriquement.","980","4","406","Les paraphrases n’expliquent pas les analogies de mots

De nombreux types de plongements de mots distributionnels codent (faiblement) les régularités linguistiques comme directions (la différence entre &quot;jump&quot; et &quot;jumped&quot; sera dans une direction similaire à celle de&quot;walk&quot; et &quot;walked&quot;, et ainsi de suite). Plusieurs tentatives ont été faites pour expliquer ce fait. Nous répondons à l’explication théorique récente d’Allen et Hospedales (ICML, 2019), qui affirme que word2vec et GloVe coderont les régularités linguistiques chaque fois qu’une relation spécifique de paraphrase entre les quatre mots impliqués dans la régularité. Nous démontrons que cette explication ne tient pas debout : les relations de paraphrase nécessaires à cette explication ne sont pas vérifiées empiriquement.","2","2023-07-07 14:31:35","2023-07-07 14:36:02","980",,"3","1","4","4","1","3",NULL,,"2023-07-07 02:31:25","0"
"3248881","This work aims to improve automatic speech recognition (ASR) by modeling long-term semantic relations. We propose to perform this through rescoring the ASR N-best hypotheses list. To achieve this, we propose two deep neural network (DNN) models and combine semantic, acoustic, and linguistic information. Our DNN rescoring models are aimed at selecting hypotheses that have better semantic consistency and therefore lower WER. We investigate a powerful representation as part of input features to our DNN model: dynamic contextual embeddings from Transformer-based BERT. Acoustic and linguistic features are also included. We perform experiments on the publicly available dataset TED-LIUM. We evaluate in clean and in noisy conditions, with n-gram and Recurrent Neural Network Language Model (RNNLM), more precisely Long Short-Term Memory (LSTM) model. The proposed rescoring approaches give significant WER improvements over the ASR system without rescoring models. Furthermore, the combination of rescoring methods based on BERT and GPT-2 scores achieves the best results.","BERT-based Semantic Model for Rescoring N-best Speech Recognition List","Dominique Fohr, Irina Illina","5640","Modèle sémantique basé sur BERT pour le recalage de la N meilleure liste de reconnaissance de la parole

Ce travail vise à améliorer la reconnaissance automatique de la parole (ASR) en modélisant les relations sémantiques à long terme. Nous proposons d'y parvenir en recalant la liste des N meilleures hypothèses de la reconnaissance automatique de la parole. Pour ce faire, nous proposons deux modèles de réseaux neuronaux profonds (DNN) et combinons des informations sémantiques, acoustiques et linguistiques. Nos modèles de recalage DNN visent à sélectionner des hypothèses qui ont une meilleure cohérence sémantique et donc un WER plus faible. Nous étudions une représentation puissante dans le cadre des caractéristiques d'entrée de notre modèle DNN : les enchâssements contextuels dynamiques du BERT basé sur les transformateurs. Des caractéristiques acoustiques et linguistiques sont également incluses. Nous réalisons des expériences sur l'ensemble de données publiques TED-LIUM. Nous évaluons dans des conditions propres et bruyantes, avec des n-grammes et un modèle linguistique de réseau neuronal récurrent (RNNLM), plus précisément un modèle de mémoire à long terme (LSTM). Les approches de rescoring proposées apportent des améliorations significatives du WER par rapport au système ASR sans modèle de rescoring. En outre, la combinaison des méthodes de recadrage basées sur les scores BERT et GPT-2 donne les meilleurs résultats.","18234","3","409","Modèle sémantique fondé sur BERT pour la réévaluation de la liste N-best de reconnaissance de la parole

Ce travail vise à améliorer la reconnaissance automatique de la parole (RAP) en modélisant les relations sémantiques à long terme. Nous proposons d&#039;y parvenir en réévaluant la liste des hypothèses N-best de la RAP. Pour ce faire, nous proposons deux modèles de réseaux neuronaux profonds (DNN) et combinons des informations sémantiques, acoustiques et linguistiques. Nos modèles de réévaluation DNN visent à sélectionner des hypothèses qui ont une meilleure cohérence sémantique et donc un taux d&#039;erreur mot plus faible. Nous étudions une représentation puissante dans le cadre des caractéristiques d&#039;entrée de notre modèle DNN : les enchâssements contextuels dynamiques du BERT basé sur les transformateurs. Des caractéristiques acoustiques et linguistiques sont également incluses. Nous réalisons des expériences sur l&#039;ensemble de données publiques TED-LIUM. Nous évaluons dans des conditions propres et bruitées, avec des n-grammes et un modèle linguistique de réseau neuronal récurrent (RNNLM), plus précisément un modèle de type Long Short-Term Memory (LSTM). Les approches de réévaluation proposées apportent des améliorations significatives du taux d&#039;erreur mot par rapport au système RAP sans modèle de réévaluation. En outre, la combinaison des méthodes de recadrage basées sur les scores BERT et GPT-2 donne les meilleurs résultats.","2","2023-07-07 14:39:03","2023-07-07 14:44:32","18234",,"2","1","4","4","1","2",NULL,,"2023-07-07 02:38:25","0"
"3256324","In this paper, we present the different methods proposed for the FinSIM-2 Shared Task 2021 on Learning Semantic Similarities for the Financial domain. The main focus of this task is to evaluate the classification of financial terms into corresponding top-level concepts (also known as hypernyms) that were extracted from an external ontology. We approached the task as a semantic textual similarity problem. By relying on a siamese network with pre-trained language model encoders, we derived semantically meaningful term embeddings and computed similarity scores between them in a ranked manner. Additionally, we exhibit the results of different baselines in which the task is tackled as a multi-class classification problem. The proposed methods outperformed our baselines and proved the robustness of the models based on textual similarity siamese network. CCS CONCEPTS • Computing methodologies → Lexical semantics; Neural networks.","L3i_LBPAM at the FinSim-2 task: Learning Financial Semantic Similarities with Siamese Transformers","Nhu Khoa Nguyen, Emanuela Boros, Gaël Lejeune, Antoine Doucet, Thierry Delahaut","5630","L3i_LBPAM à la tâche FinSim-2: Apprendre les similitudes sémantiques financières avec les transformateurs siamois

Dans cet article, nous présentons les différentes méthodes proposées pour la tâche partagée FinSim-2 2021 sur l’apprentissage des similitudes sémantiques pour le domaine financier. L’objectif principal de cette tâche est d’évaluer la classification des termes financiers dans des concepts de haut niveau correspondants (également appelés hypernymes) qui ont été extraits d’une ontologie externe. Nous avons abordé la tâche comme un problème de similitude textuelle sémantique. En nous appuyant sur un réseau siamois avec des codeurs de modèles de langage pré-formés, nous avons dérivé des intégrateurs de termes sémantiquement significatifs et avons calculé des scores de similitude entre eux d’une manière classée. En outre, nous présentons les résultats de différentes lignes de base dans lesquelles la tâche est traitée comme un problème de classification multi-classes. Les méthodes proposées ont surpassé nos lignes de base et ont prouvé la robustesse des modèles basés sur le réseau siamese de similitude textuelle. CCS CONCEPTS • Méthodologies informatiques → sémantique lexique; Réseaux neuronaux.","13839","4","426","L3i_LBPAM à la tâche FinSim-2 : Apprendre les similitudes sémantiques financières avec les transformeurs siamois

Dans cet article, nous présentons les différentes méthodes proposées pour la tâche partagée FinSim-2 2021 sur l’apprentissage des similitudes sémantiques pour le domaine financier. L’objectif principal de cette tâche est d’évaluer la classification des termes financiers dans des concepts de haut niveau correspondants (également appelés hypernymes) qui ont été extraits d’une ontologie externe. Nous avons abordé la tâche comme un problème de similitude textuelle sémantique. En nous appuyant sur un réseau siamois avec des codeurs de modèles de langage pré-formés, nous avons dérivé des intégrateurs de termes sémantiquement significatifs et avons calculé des scores de similitude entre eux d’une manière classée. En outre, nous présentons les résultats de différentes lignes de base dans lesquelles la tâche est traitée comme un problème de classification multiclasse. Les méthodes proposées ont surpassé nos systèmes de base et ont prouvé la robustesse des modèles basés sur le réseau siamois de similitude textuelle. CCS CONCEPTS • Méthodologies informatiques → Sémantique lexicale ; Réseaux neuronaux.","2","2023-07-10 15:20:04","2023-07-10 15:25:42","13839",,"1","3","4","2","4","1",NULL,,"2023-07-10 03:06:26","0"
"3786135","Large pre-trained language models (LLM) have shown remarkable Zero-Shot Learning performances in many Natural Language Processing tasks. However, designing effective prompts is still very difficult for some tasks, in particular for dialogue act recognition. We propose an alternative way to leverage pretrained LLM for such tasks that replace manual prompts with simple rules, which are more intuitive and easier to design for some tasks. We demonstrate this approach on the question type recognition task, and show that our zero-shot model obtains competitive performances both with a supervised LSTM trained on the full training corpus, and another supervised model from previously published works on the MRDA corpus. We further analyze the limits of the proposed approach, which can not be applied on any task, but may advantageously complement prompt programming for specific classes.","Weak supervision for Question Type Detection with large language models","Jiří Martínek, Christophe Cerisara, Pavel Král, Ladislav Lenc, Josef Baloun","4339","Surveillance faible pour la détection de type de question avec des modèles de langue volumineux

De grands modèles de langage pré-formés (LLM) ont montré des performances remarquables d'apprentissage automatique dans de nombreuses tâches de traitement du langage naturel. Cependant, la conception de messages efficaces reste très difficile pour certaines tâches, en particulier pour la reconnaissance des actes de dialogue. Nous proposons un moyen alternatif d'exploiter la gestion du cycle de vie des documents préformés pour de telles tâches, qui remplacent les invites manuelles par des règles simples, plus intuitives et plus faciles à concevoir pour certaines tâches. Nous démontrons cette approche sur la tâche de reconnaissance du type de question, et montrons que notre modèle zéro-tir obtient des performances compétitives à la fois avec un LSTM supervisé formé sur le corpus complet de formation, et un autre modèle supervisé à partir de travaux publiés précédemment sur le corpus MRDA. Nous analysons en outre les limites de l'approche proposée, qui ne peut être appliquée à aucune tâche, mais peut avantageusement compléter la programmation rapide pour des classes spécifiques.","16255","6","427","Supervision faible pour la détection de type de question avec des grands modèles de langue 

De grands modèles de langue pré-formés ont montré des performances remarquables en matière d&#039;apprentissage automatique dans de nombreuses tâches de traitement du langage naturel. Cependant, la conception de messages efficaces reste très difficile pour certaines tâches, en particulier pour la reconnaissance des actes de dialogue. Nous proposons un moyen alternatif d&#039;exploiter la gestion du cycle de vie des documents préformés pour de telles tâches, qui remplacent les invites manuelles par des règles simples, plus intuitives et plus faciles à concevoir pour certaines tâches. Nous démontrons cette approche sur la tâche de reconnaissance du type de question, et montrons que notre modèle zero-shot obtient des performances compétitives à la fois avec un LSTM supervisé formé sur le corpus complet de formation, et un autre modèle supervisé à partir de travaux publiés précédemment sur le corpus MRDA. Nous analysons en outre les limites de l&#039;approche proposée, qui ne peut être appliquée à aucune tâche, mais peut avantageusement compléter la programmation rapide pour des classes spécifiques.","2","2023-07-10 15:27:55","2023-07-10 15:36:21","16255",,"1","1","4","3","1","4",NULL,,"2023-07-10 03:25:43","0"
"3794029","Deep learning models such as Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) models have recently emerged as effective solutions to various NLP tasks with comparatively remarkable results. The CNN model efficiently extracts higher level features using convolutional layers and max-pooling layers while the LSTM model allows capturing long-term dependencies between word sequences. In this paper, we propose a hybrid CNN-LSTM model taking advantage of both models to overcome the sentiment analysis problem on Twitter data. We employ a multi-channel CNN that extracts local n-gram features in different sizes using several filters of different lengths. We also employ a weighted average word embeddings method which incorporates sentiment information in the continuous representation of words based on an adapted version of the delta TFIDF measure. These word representations will be the input for the CNN-LSTM model. Experiment results substantiate our intuition by reaching good macro average recall and accuracy scores beating several existing models as well as our same model using individual CNN and LSTM.","Deep Hybrid Neural Networks with Improved Weighted Word Embeddings for Sentiment Analysis","Rania Othman, Rim Faiz, Youcef Abdelsadek, Kamel Chelghoum, Imed Kacem","4307","Réseaux neuronaux hybrides profonds avec intégration améliorée de mots pondérés pour l'analyse de sentiment

Des modèles d'apprentissage profond tels que les modèles de réseau neuronal convolutionnel (CNN) et de mémoire à long terme (LSTM) sont récemment apparus comme des solutions efficaces à diverses tâches de PNL avec des résultats relativement remarquables. Le modèle CNN extrait efficacement des caractéristiques de niveau supérieur à l'aide de couches convolutionnelles et de couches de regroupement maximal, tandis que le modèle LSTM permet de capturer des dépendances à long terme entre des séquences de mots. Dans cet article, nous proposons un modèle hybride CNN-LSTM qui tire parti des deux modèles pour surmonter le problème de l'analyse de sentiment sur les données Twitter. Nous utilisons un CNN multicanal qui extrait les caractéristiques locales de n-grammes dans différentes tailles en utilisant plusieurs filtres de différentes longueurs. Nous utilisons également un procédé d'incorporation de mots moyens pondérés qui incorpore des informations de sentiment dans la représentation continue de mots basée sur une version adaptée de la mesure TFIDF delta. Ces représentations de mots seront l'entrée pour le modèle CNN-LSTM. Les résultats d'expériences corroborent notre intuition en atteignant de bons scores de rappel et d'exactitude macro-moyens, battant plusieurs modèles existants ainsi que notre même modèle utilisant CNN et LSTM individuels.","16223","6","428","Réseaux neuronaux hybrides profonds avec intégration améliorée de mots pondérés pour l&#039;analyse de sentiment

Les modèles d&#039;apprentissage profond, tels que les modèles de réseau de neurones convolutifs (CNN) et de type Long Short-Term Memory (LSTM) sont récemment apparus comme des solutions efficaces à diverses tâches de TAL avec des résultats relativement remarquables. Le modèle CNN extrait efficacement des caractéristiques de niveau supérieur à l&#039;aide de couches convolutionnelles et de couches de regroupement maximal, tandis que le modèle LSTM permet de capturer des dépendances à long terme entre des séquences de mots. Dans cet article, nous proposons un modèle hybride CNN-LSTM qui tire parti des deux modèles pour surmonter le problème de l&#039;analyse de sentiment sur les données Twitter. Nous utilisons un CNN multicanal qui extrait les caractéristiques locales de n-grammes dans différentes tailles en utilisant plusieurs filtres de différentes longueurs. Nous utilisons également un procédé d&#039;incorporation de mots moyens pondérés qui incorpore des informations de sentiment dans la représentation continue de mots basée sur une version adaptée de la mesure TFIDF delta. Ces représentations de mots seront l&#039;entrée pour le modèle CNN-LSTM. Les résultats d&#039;expériences corroborent notre intuition en atteignant de bons scores de rappel et d&#039;exactitude macro-moyens, battant plusieurs modèles existants ainsi que notre même modèle utilisant CNN et LSTM individuels.","2","2023-07-10 17:19:07","2023-07-10 17:25:09","16223",,"1","1","4","1","1","1",NULL,,"2023-07-10 05:18:55","0"
"3646770","The context of medical conditions is an important feature to consider when processing clinical narratives. NegEx and its extension ConText became the most well-known rule-based systems that allow determining whether a medical condition is negated, historical or experienced by someone other than the patient in English clinical text. In this paper, we present a French adaptation and enrichment of FastContext which is the most recent, n-trie engine-based implementation of the ConText algorithm. We compiled an extensive list of French lexical cues by automatic and manual translation and enrichment. To evaluate French FastContext, we manually annotated the context of medical conditions present in two types of clinical narratives: (i) death certificates and (ii) electronic health records. Results show good performance across different context values on both types of clinical notes (on average 0.93 and 0.86 F1, respectively). Furthermore, French FastContext outperforms previously reported French systems for negation detection when compared on the same datasets and it is the first implementation of contextual temporality and experiencer identification reported for French. Finally, French FastContext has been implemented within the SIFR Annotator: a publicly accessible Web service to annotate French biomedical text data (http://bioportal.lirmm.fr/annotator). To our knowledge, this is the first implementation of a Web-based ConText-like system in a publicly accessible platform allowing non-natural-language-processing experts to both annotate and contextualize medical conditions in clinical notes.","French FastContext: a Publicly Accessible System for Detecting Negation, Temporality and Experiencer in French Clinical Notes","Mehdi Mirzapour, Amine Abdaoui, Andon Tchechmedjiev, William Digan, Sandra Bringay, Clement Jonquet","4717","FastContext en français : un système accessible au public pour détecter la négation, la temporalité et l'expérience dans les notes cliniques en français

Le contexte des pathologies est une caractéristique importante à prendre en compte lors du traitement des récits cliniques. NegEx et son extension ConText sont devenus les systèmes à base de règles les plus connus qui permettent de déterminer si une condition médicale est niée, historique ou vécue par quelqu'un d'autre que le patient dans un texte clinique anglais. Dans cet article, nous présentons une adaptation et un enrichissement en français de FastContext qui est l'implémentation la plus récente de l'algorithme ConText basée sur le moteur n-trie. Nous avons compilé une liste exhaustive d'indices lexicaux français par traduction automatique et manuelle et par enrichissement. Pour évaluer FastContext en français, nous avons annoté manuellement le contexte des conditions médicales présentes dans deux types de récits cliniques : (i) les certificats de décès et (ii) les dossiers médicaux électroniques. Les résultats montrent une bonne performance pour différentes valeurs de contexte sur les deux types de notes cliniques (en moyenne 0,93 et 0,86 F1, respectivement). De plus, French FastContext surpasse les systèmes français précédemment rapportés pour la détection de la négation lorsqu'ils sont comparés sur les mêmes ensembles de données et il s'agit de la première implémentation de la temporalité contextuelle et de l'identification de l'expérimentateur rapportée pour le français. Enfin, FastContext français a été implémenté dans l'annotateur SIFR : un service Web accessible au public pour annoter les données textuelles biomédicales françaises (http://bioportal.lirmm.fr/annotator). À notre connaissance, il s'agit de la première mise en œuvre d'un système de type ConText basé sur le Web dans une plateforme accessible au public, permettant à des experts en traitement du langage non naturel d'annoter et de contextualiser des conditions médicales dans des notes cliniques.","19187","3","429","French FastContext : un système accessible au public de détection de la négation, la temporalité et l&#039;expérient dans les notes cliniques en français

Le contexte des pathologies est une caractéristique importante à prendre en compte lors du traitement des récits cliniques. NegEx et son extension ConText sont devenus les systèmes régis par des règles les plus connus qui permettent de déterminer si une condition médicale est niée, historique ou vécue par quelqu&#039;un d&#039;autre que le patient dans un texte clinique anglais. Dans cet article, nous présentons une adaptation et un enrichissement en français de FastContext qui est l&#039;implémentation la plus récente de l&#039;algorithme ConText basée sur le moteur n-trie. Nous avons compilé une liste exhaustive d&#039;indices lexicaux français par traduction automatique et manuelle et par enrichissement. Pour évaluer FastContext en français, nous avons annoté manuellement le contexte des conditions médicales présentes dans deux types de récits cliniques : (i) les certificats de décès et (ii) les dossiers médicaux électroniques. Les résultats montrent une bonne performance pour différentes valeurs de contexte sur les deux types de notes cliniques (en moyenne 0,93 et 0,86 F1, respectivement). De plus, French FastContext surpasse les systèmes français précédemment rapportés pour la détection de la négation lorsqu&#039;ils sont comparés sur les mêmes ensembles de données et il s&#039;agit de la première implémentation de la temporalité contextuelle et de l&#039;identification de l&#039;expérimentateur rapportée pour le français. Enfin, FastContext français a été implémenté dans l&#039;annotateur SIFR : un service Web accessible au public pour annoter les données textuelles biomédicales françaises (http://bioportal.lirmm.fr/annotator). À notre connaissance, il s&#039;agit de la première mise en œuvre d&#039;un système de type ConText basé sur le Web dans une plateforme accessible au public, permettant à des experts en traitement du langage non naturel d&#039;annoter et de contextualiser des conditions médicales dans des notes cliniques.","2","2023-07-10 17:30:14","2023-07-10 17:38:19","19187",,"1","1","4","3","1","3",NULL,,"2023-07-10 05:29:44","0"
"3712469","We describe CARTOLABE, a web-based multi-scale system for visualizing and exploring large textual corpora based on topics, introducing a novel mechanism for the progressive visualization of filtering queries. Initially designed to represent and navigate through scientific publications in different disciplines, CARTOLABE has evolved to become a generic system and accommodate various corpora, ranging from Wikipedia (4.5M entries) to the French National Debate (4.3M entries). CARTOLABE is made of two modules: the first relies on Natural Language Processing methods, converting a corpus and its entities (documents, authors, concepts) into high-dimensional vectors, computing their projection on the 2D plane, and extracting meaningful labels for regions of the plane. The second module is a Web-based visualization, displaying tiles computed from the multidimensional projection of the corpus using the UMAP projection method. This visualization module aims at enabling users with no expertise in visualization and data analysis to get an overview of their corpus, and to interact with it: exploring, querying, filtering, panning and zooming on regions of semantic interest. Three use cases are discussed to illustrate CARTOLABE versatility and ability to bring large scale textual corpus visualization and exploration to a wide audience.","Cartolabe: A Web-Based Scalable Visualization of Large Document Collections","Philippe Caillou, Jonas Renault, Jean-Daniel Fekete, Anne-Catherine Letournel, Michèle Sebag","4538","Cartolabe: Une visualisation évolutive basée sur le Web des grandes collections de documents

Nous décrivons CARTOLABE, un système multi-échelle basé sur le Web pour visualiser et explorer de grands corpus textuels basés sur des sujets, introduisant un nouveau mécanisme pour la visualisation progressive des requêtes filtrantes. Initialement conçu pour représenter et naviguer à travers des publications scientifiques dans différentes disciplines, CARTOLABE a évolué pour devenir un système générique et accueillir divers corpus, allant de Wikipédia (4.5M entrées) au débat national français (4,3 millions d’entrées). CARTOLABE est composé de deux modules: la première s’appuie sur les méthodes de traitement du langage naturel, convertissant un corpus et ses entités (documents, auteurs, concepts) en vecteurs de haute dimension, calculant leur projection sur le plan 2D et en extrayant des étiquettes significatives pour les régions du plan. Le deuxième module est une visualisation Web, affichant des tuiles calculées à partir de la projection multidimensionnelle du corpus à l’aide de la méthode de projection UMAP. Ce module de visualisation vise à permettre aux utilisateurs sans expertise en visualisation et en analyse de données d’avoir une vue d’ensemble de leur corpus et d’interagir avec lui: explorer, interroger, filtrer, panoramiquer et zoomer sur les régions d’intérêt sémantique. Trois cas d’utilisation sont discutés pour illustrer la polyvalence et la capacité de CARTOLABE à apporter la visualisation et l’exploration textuelles à grande échelle à un large public.","11325","4","430","Cartolabe : Une visualisation de grandes collections de documents évolutive et disponible sur le web 

Nous décrivons CARTOLABE, un système multi-échelle disponible sur le web pour visualiser et explorer de grands corpus textuels consacrés à des sujets spécifiques, introduisant un nouveau mécanisme pour la visualisation progressive des requêtes filtrantes. Initialement conçu pour représenter et naviguer à travers des publications scientifiques dans différentes disciplines, CARTOLABE a évolué pour devenir un système générique et accueillir divers corpus, allant de Wikipédia (4.5M entrées) au débat national français (4,3 millions d’entrées). CARTOLABE est composé de deux modules : la première s’appuie sur les méthodes de traitement du langage naturel, convertissant un corpus et ses entités (documents, auteurs, concepts) en vecteurs de haute dimension, calculant leur projection sur le plan 2D et en extrayant des étiquettes significatives pour les régions du plan. Le deuxième module est une visualisation Web, affichant des tuiles calculées à partir de la projection multidimensionnelle du corpus à l’aide de la méthode de projection UMAP. Ce module de visualisation vise à permettre aux utilisateurs sans expertise en visualisation et en analyse de données d’avoir une vue d’ensemble de leur corpus et d’interagir avec lui : explorer, interroger, filtrer, panoramiquer et zoomer sur les régions d’intérêt sémantique. Trois cas d’utilisation sont discutés pour illustrer la polyvalence et la capacité de CARTOLABE à apporter la visualisation et l’exploration textuelles à grande échelle à un large public.","2","2023-07-10 18:01:46","2023-07-10 18:05:10","11325",,"1","1","1","4","4","2",NULL,,"2023-07-10 06:01:39","0"
"3361421","The activations of language transformers like GPT-2 have been shown to linearly map onto brain activity during speech comprehension. However, the nature of these activations remains largely unknown and presumably conflate distinct linguistic classes. Here, we propose a taxonomy to factorize the high-dimensional activations of language models into four combinatorial classes: lexical, compositional, syntactic, and semantic representations. We then introduce a statistical method to decompose, through the lens of GPT-2's activations, the brain activity of 345 subjects recorded with functional magnetic resonance imaging (fMRI) during the listening of 4.6 hours of narrated text. The results highlight two findings. First, compositional representations recruit a more widespread cortical network than lexical ones, and encompass the bilateral temporal, parietal and prefrontal cortices. Second, contrary to previous claims, syntax and semantics are not associated with separated modules, but, instead, appear to share a common and distributed neural substrate. Overall, this study introduces a versatile framework to isolate, in the brain activity, the distributed representations of linguistic constructs.","Disentangling Syntax and Semantics in the Brain with Deep Networks","Charlotte Caucheteux, Alexandre Gramfort, Jean-Remi King","5358","Démêler la syntaxe et la sémantique dans le cerveau avec des réseaux profonds

Il a été démontré que les activations de transformateurs de langage comme GPT-2 se projettent linéairement sur l'activité cérébrale pendant la compréhension vocale. Cependant, la nature de ces activations reste largement inconnue et confond vraisemblablement des classes linguistiques distinctes. Ici, nous proposons une taxonomie pour factoriser les activations à haute dimension des modèles langagiers en quatre classes combinatoires : les représentations lexicales, compositionnelles, syntaxiques et sémantiques. Nous introduisons ensuite une méthode statistique pour décomposer, à travers le prisme des activations du GPT-2, l'activité cérébrale de 345 sujets enregistrés par imagerie par résonance magnétique fonctionnelle (IRMf) pendant l'écoute de 4,6 heures de texte narré. Les résultats mettent en évidence deux constatations. Premièrement, les représentations compositionnelles recrutent un réseau cortical plus étendu que les corticales lexicales, et englobent les cortex temporal, pariétal et préfrontal bilatéraux. Deuxièmement, contrairement aux revendications précédentes, la syntaxe et la sémantique ne sont pas associées à des modules séparés, mais semblent plutôt partager un substrat neuronal commun et distribué. Globalement, cette étude introduit un cadre polyvalent pour isoler, dans l'activité cérébrale, les représentations distribuées des constructions linguistiques.","15052","6","457","Démêler la syntaxe et la sémantique dans le cerveau avec des réseaux profonds

Il a été démontré que les activations de transformateurs de langage comme GPT-2 se projettent linéairement sur l&#039;activité cérébrale pendant la compréhension vocale. Cependant, la nature de ces activations reste largement inconnue et confond vraisemblablement des classes linguistiques distinctes. Ici, nous proposons une taxonomie pour factoriser les activations à haute dimension des modèles de langue en quatre classes combinatoires : les représentations lexicales, compositionnelles, syntaxiques et sémantiques. Nous introduisons ensuite une méthode statistique pour décomposer, à travers le prisme des activations du GPT-2, l&#039;activité cérébrale de 345 sujets enregistrés par imagerie par résonance magnétique fonctionnelle (IRMf) pendant l&#039;écoute de 4,6 heures de texte narré. Les résultats mettent en évidence deux constatations. Premièrement, les représentations compositionnelles recrutent un réseau cortical plus étendu que les corticales lexicales, et englobent les cortex temporal, pariétal et préfrontal bilatéraux. Deuxièmement, contrairement aux revendications précédentes, la syntaxe et la sémantique ne sont pas associées à des modules séparés, mais semblent plutôt partager un substrat neuronal commun et distribué. Globalement, cette étude introduit un cadre polyvalent pour isoler, dans l&#039;activité cérébrale, les représentations distribuées des constructions linguistiques.","2","2023-07-12 13:46:47","2023-07-12 13:49:16","15052",,"1","1","4","1","1","1",NULL,,"2023-07-12 01:46:42","0"
"3269361","Texts written in Old Literary Finnish represent the first literary work ever written in Finnish starting from the 16th century. There have been several projects in Finland that have digitized old publications and made them available for research use. However, using modern NLP methods in such data poses great challenges. In this paper we propose an approach for simultaneously normalizing and lemmatizing Old Literary Finnish into modern spelling. Our best model reaches to 96.3% accuracy in texts written by Agricola and 87.7% accuracy in other contemporary out-of-domain text. Our method has been made freely available on Zenodo and Github.","Lemmatization of Historical Old Literary Finnish Texts in Modern Orthography","Mika Hämäläinen, Niko Partanen, Khalid Alnajjar","1098","Lemmatisation de textes historiques finnois littéraires dans l'orthographe moderne

Les textes écrits en vieux finnois littéraire représentent la première oeuvre littéraire jamais écrite en finnois à partir du XVIe siècle. En Finlande, plusieurs projets ont numérisé d'anciennes publications et les ont mises à la disposition des chercheurs. Cependant, l'utilisation des méthodes modernes de PNL dans de telles données pose de grands défis. Dans cet article, nous proposons une approche pour normaliser et lemmatiser simultanément le vieux finnois littéraire en orthographe moderne. Notre meilleur modèle atteint 96,3% de précision dans les textes écrits par Agricola et 87,7% de précision dans d'autres textes hors domaine contemporains. Notre méthode est disponible gratuitement sur Zenodo et Github.","5343","6","458","Lemmatisation de textes littéraires historiques finnois dans l&#039;orthographe moderne

Les textes écrits en vieux finnois littéraire représentent les premières œuvres littéraires jamais écrites en finnois à partir du XVIe siècle. En Finlande, plusieurs projets ont numérisé d&#039;anciennes publications et les ont mises à la disposition des chercheurs. Cependant, l&#039;utilisation des méthodes modernes de TAL dans de telles données pose de grands défis. Dans cet article, nous proposons une approche pour normaliser et lemmatiser simultanément le vieux finnois littéraire en orthographe moderne. Notre meilleur modèle atteint 96,3% de précision dans les textes écrits par Agricola et 87,7% de précision dans d&#039;autres textes hors domaine contemporains. Notre méthode est disponible gratuitement sur Zenodo et Github.","2","2023-07-12 13:50:02","2023-07-12 13:52:26","5343",,"1","1","4","3","1","1",NULL,,"2023-07-12 01:49:44","0"
"3380268","We study properties and relationship between three classes of quantitative language models computing over infinite input alphabets: Symbolic Weighted Automata (swA) at the joint between Symbolic Automata (sA) and Weighted Automata (wA), as well as Transducers (swT) and Visibly Pushdown (sw-VPA) variants. Like sA, swA deal with large or infinite input alphabets, and like wA, they output a weight value in a semiring domain. The transitions of swA are labeled by functions from an infinite alphabet into the weight domain. This generalizes sA, whose transitions are guarded by Boolean predicates overs symbols in an infinite alphabet, and also wA, whose transitions are labeled by constant weight values, and which deal only with finite alphabets. We present a Bar-Hillel Perles Shamir construction of a swA computing a swT-defined distance between a swA input language and a word, some closure results and a polynomial best-search algorithm for sw-VPA. These results are applied to solve a variant of parsing over infinite alphabets.","Symbolic Weighted Language Models, Quantitative Parsing and Verification over Infinite Alphabets","Florent Jacquemard, Philippe Rigaux, Lydia Rodriguez de La Nava","5295","Modèles symboliques pondérés de langage, analyse quantitative et vérification sur des alphabets infinis

Nous étudions les propriétés et les relations entre trois classes de modèles quantitatifs de langage calculant sur des alphabets d'entrée infinis : Les Automates Symboliques Pondérés (swA), à la jonction entre les Automates Symboliques (sA) et les Automates Pondérés (wA), ainsi que les Transducteurs (swT) et les variantes Visibly Pushdown (sw-VPA). Comme les sA, les swA traitent des alphabets d'entrée grands ou infinis, et comme les wA, ils produisent une valeur de poids dans un domaine de semiring. Les transitions de swA sont étiquetées par des fonctions d'un alphabet infini dans le domaine de poids. Cela généralise sA, dont les transitions sont protégées par des prédicats booléens sur des symboles dans un alphabet infini, et aussi wA, dont les transitions sont étiquetées par des valeurs de poids constantes, et qui ne traitent que des alphabets finis. Nous présentons une construction de Bar-Hillel Perles Shamir d'un swA calculant une distance définie par swT entre un langage d'entrée swA et un mot, quelques résultats de fermeture et un algorithme polynomial de meilleure recherche pour sw-VPA. Ces résultats sont appliqués pour résoudre une variante de l'analyse syntaxique sur des alphabets infinis.","19781","3","459","Modèles de langue symboliques pondérés, analyse quantitative et vérification sur des alphabets infinis

Nous étudions les propriétés et les relations entre trois classes de modèles de langue quantitatifs calculant sur des alphabets d&#039;entrée infinis : Les Automates Symboliques Pondérés (swA, Symbolic Weighted Automata), à la jonction entre les Automates Symboliques (sA) et les Automates Pondérés (wA), ainsi que les Transducteurs (swT) et les variantes Visibly Pushdown (sw-VPA). Comme les sA, les swA traitent des alphabets d&#039;entrée grands ou infinis, et comme les wA, ils produisent une valeur de poids dans un domaine de semi-anneau. Les transitions de swA sont étiquetées par des fonctions d&#039;un alphabet infini dans le domaine de poids. Cela généralise les sA, dont les transitions sont protégées par des prédicats booléens sur des symboles dans un alphabet infini, et aussi les wA, dont les transitions sont étiquetées par des valeurs de poids constantes, et qui ne traitent que des alphabets finis. Nous présentons une construction de Bar-Hillel Perles Shamir d&#039;un swA calculant une distance définie par swT entre un langage d&#039;entrée swA et un mot, quelques résultats de fermeture et un algorithme polynomial de meilleure recherche pour sw-VPA. Ces résultats sont appliqués pour résoudre une variante de l&#039;analyse syntaxique sur des alphabets infinis.","2","2023-07-12 13:53:53","2023-07-12 13:57:57","19781",,"1","1","4","3","1","3",NULL,,"2023-07-12 01:52:40","0"
"3245310","It has been observed that there has been increased demand in real-world event detection using publicly accessible data through various social platforms such as Twitter, YouTube, or Facebook. By applying social media intelligence for detection and identification of any civil or secular turmoil-oriented ultimatum is an important field of interest for researchers for the last few decades. Several researchers have come up with various integrated event detection structures. It mainly consisted of data collection, pre-processing, classification. Based on their respective training data they have obtained accuracy. Many researchers have proposed their framework to address such civil agitation by techniques such as semi-supervised machine learning, Neural Network, Decision tree, Naive Bayes.","A Short Survey on Riot Prediction","Koushik Deb, Arpita Konar, Kousheya Das, Mrityunjoy Sen","5643","Une courte enquête sur la prévision des émeutes

Il a été observé qu’il y a eu une demande accrue de détection d’événements dans le monde réel en utilisant des données accessibles au public via diverses plateformes sociales telles que Twitter, YouTube ou Facebook. En appliquant l’intelligence des médias sociaux pour la détection et l’identification de tout ultimatum civil ou laïque axé sur les troubles est un domaine d’intérêt important pour les chercheurs au cours des dernières décennies. Plusieurs chercheurs ont mis au point diverses structures intégrées de détection d’événements. Il s’agissait principalement de la collecte de données, du prétraitement, de la classification. Sur la base de leurs données de formation respectives, ils ont obtenu l’exactitude. De nombreux chercheurs ont proposé leur cadre pour aborder cette agitation civile par des techniques telles que l’apprentissage automatique semi-supervisé, le réseau neuronal, l’arbre de décision, Naive Bayes.","13852","4","460","Une courte enquête sur la prévision des émeutes

Il a été observé qu’il y a eu une demande accrue de détection d’événements dans le monde réel en utilisant des données accessibles au public via diverses plateformes sociales telles que Twitter, YouTube ou Facebook. En appliquant l’intelligence des médias sociaux pour la détection et l’identification de tout ultimatum civil ou profane axé sur les troubles est un domaine d’intérêt important pour les chercheurs au cours des dernières décennies. Plusieurs chercheurs ont mis au point diverses structures intégrées de détection d’événements. Il s’agissait principalement de la collecte de données, du prétraitement, de la classification. Sur la base de leurs données de formation respectives, ils ont obtenu des résultats précis. De nombreux chercheurs ont proposé leur cadre pour aborder cette agitation civile par des techniques telles que l’apprentissage automatique semi-supervisé, les réseaux neuronaux, les arbres de décision, et la classification naïve bayésienne.","2","2023-07-12 13:58:11","2023-07-12 14:03:21","13852",,"1","2","4","4","1","1",NULL,,"2023-07-12 01:58:04","0"
"3504691","We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer ""What is the color of the sky"" with ""blue"" by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on our subset of CounterExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https://github.com/cdancette/detect-shortcuts.","Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering","Corentin Dancette, Remi Cadene, Damien Teney, Matthieu Cord","5071","Au-delà des biases fondées sur des questions: Évaluation de l’apprentissage par raccourcis multimodaux dans la réponse aux questions visuelles

Nous introduisons une méthodologie d’évaluation pour la réponse aux questions visuelles (VQA) afin de mieux diagnostiquer les cas d’apprentissage par raccourci. Ces cas se produisent lorsqu’un modèle exploite des régularités statistiques fallacieuses pour produire des réponses correctes, mais ne déploie pas réellement le comportement souhaité. Il est nécessaire d’identifier les raccourcis possibles dans un ensemble de données et d’évaluer leur utilisation avant de déployer un modèle dans le monde réel. La communauté de recherche de VQA s’est concentrée exclusivement sur les raccourcis basés sur des questions, où un modèle pourrait, par exemple, répondre «Quelle est la couleur du ciel» avec «bleu» en s’appuyant principalement sur l’entraînement conditionnel des questions préalables et donner peu de poids aux preuves visuelles. Nous allons plus loin et considérons les raccourcis multimodals qui impliquent à la fois des questions et des images. Nous identifions d’abord les raccourcis potentiels dans la formation VQA v2 populaire définie par l’extraction de règles prédictives triviales telles que des co-occurrences de mots et d’éléments visuels. Nous introduisons ensuite VQA-counterexamples (VQA-CE), un protocole d’évaluation basé sur notre sous-ensemble de contre-exemples, c’est-à-dire des triplets de réponse image-question-réponse où nos règles conduisent à des réponses incorrectes. Nous utilisons cette nouvelle évaluation dans une étude à grande échelle des approches existantes pour la VQA. Nous démontrons que même les modèles de pointe fonctionnent mal et que les techniques existantes pour réduire les biais sont largement inefficaces dans ce contexte. Nos résultats suggèrent que les travaux antérieurs sur les biais fondés sur des questions dans l’AQV n’ont abordé qu’une facette d’un problème complexe. Le code de notre méthode est disponible à l’adresse https://github.com/cdancette/detect-shortcuts.","11858","4","462","Au-delà des biais fondés sur des questions : Évaluation de l’apprentissage par raccourcis multimodaux dans la réponse aux questions visuelles

Nous introduisons une méthodologie d’évaluation pour la réponse aux questions visuelles (VQA) afin de mieux diagnostiquer les cas d’apprentissage par raccourci. Ces cas se produisent lorsqu’un modèle exploite des régularités statistiques fallacieuses pour produire des réponses correctes, mais ne déploie pas réellement le comportement souhaité. Il est nécessaire d’identifier les raccourcis possibles dans un ensemble de données et d’évaluer leur utilisation avant de déployer un modèle dans le monde réel. La communauté de recherche de VQA s’est concentrée exclusivement sur les raccourcis basés sur des questions, où un modèle pourrait, par exemple, répondre à « Quelle est la couleur du ciel » par « bleu » en s’appuyant principalement sur l’entraînement conditionnel des questions préalables et donner peu de poids aux preuves visuelles. Nous allons plus loin et considérons les raccourcis multimodaux qui impliquent à la fois des questions et des images. Nous identifions d’abord les raccourcis potentiels dans la formation VQA v2 populaire définie par l’extraction de règles prédictives triviales telles que des cooccurrences de mots et d’éléments visuels. Nous introduisons ensuite VQA-CounterExamples (VQA-CE), un protocole d’évaluation fondé sur notre sous-ensemble de contre-exemples, c’est-à-dire des triplets de réponse image-question-réponse où nos règles conduisent à des réponses incorrectes. Nous utilisons cette nouvelle évaluation dans une étude à grande échelle des approches existantes pour la VQA. Nous démontrons que même les modèles de pointe fonctionnent mal et que les techniques existantes pour réduire les biais sont largement inefficaces dans ce contexte. Nos résultats suggèrent que les travaux antérieurs sur les biais fondés sur des questions dans l’AQV n’ont abordé qu’une facette d’un problème complexe. Le code de notre méthode est disponible à l’adresse https://github.com/cdancette/detect-shortcuts.","2","2023-07-12 14:04:09","2023-07-12 14:07:15","11858",,"1","3","3","4","1","1",NULL,,"2023-07-12 02:04:02","0"
"2021882","Computer Assisted Translation tools remain the preferred solution of human translators when publication quality is of concern. In this paper, we present our ongoing efforts conducted within TS3, a project which aims at improving the commercial bilingual concordancer TransSearch. The core technology of this Web-based service mainly relies on sentence-level alignment. In this study, we discuss and evaluate the embedding of statistical word-level alignment.","TS3: an Improved Version of the Bilingual Concordancer TransSearch","Stéphane Huet, Julien Bourdaillet, Philippe Langlais","2033","TS3 : une version améliorée de l'outil de concordance bilingue TransSearch

Les outils de traduction assistée par ordinateur restent la solution préférée des traducteurs humains lorsque la qualité de publication est préoccupante. Dans cet article, nous présentons nos efforts continus menés au sein de TS3, un projet qui vise à améliorer le concordancier commercial bilingue TransSearch. La technologie de base de ce service Web repose principalement sur l'alignement au niveau de la phrase. Dans cette étude, nous discutons et évaluons l'intégration de l'alignement statistique au niveau des mots.","7046","6","467","TS3 : une version améliorée du concordancier bilingue TransSearch

Les outils de traduction assistée par ordinateur restent la solution préférée des traducteurs humains lorsque la qualité de publication est importante. Dans cet article, nous présentons nos efforts continus menés au sein de TS3, un projet qui vise à améliorer le concordancier commercial bilingue TransSearch. La technologie de base de ce service Web repose principalement sur l&#039;alignement au niveau de la phrase. Dans cette étude, nous discutons et évaluons l&#039;intégration de l&#039;alignement statistique au niveau des mots.","2","2023-07-18 15:33:06","2023-07-18 15:35:38","7046",,"3","1","4","3","1","1",NULL,,"2023-07-18 03:32:59","0"
"3049809","In this work, we present the system description of the UIAI entry for the short-duration speaker verification (SdSV) challenge 2020. Our focus is on Task 1 dedicated to text-dependent speaker verification. We investigate different feature extraction and modeling approaches for automatic speaker verification (ASV) and utterance verification (UV). We have also studied different fusion strategies for combining UV and ASV modules. Our primary submission to the challenge is the fusion of seven subsystems which yields a normalized minimum detection cost function (minDCF) of 0.072 and an equal error rate (EER) of 2.14% on the evaluation set. The single system consisting of a pass-phrase identification based model with phone-discriminative bottleneck features gives a normalized minDCF of 0.118 and achieves 19% relative improvement over the state-of-the-art challenge baseline.","UIAI System for Short-Duration Speaker Verification Challenge 2020","Md Sahidullah, Achintya Kumar Sarkar, Ville Vestman, Xuechen Liu, Romain Serizel, Tomi Kinnunen, Zheng-Hua Tan, Emmanuel Vincent","6065","Système UIAI pour la vérification des locuteurs de courte durée Défi 2020

Dans ce travail, nous présentons la description du système de l'entrée de l'UIAI pour le défi 2020 de la vérification du locuteur de courte durée (SdSV). Nous nous concentrons sur la tâche 1 dédiée à la vérification du locuteur dépendant du texte. Nous étudions différentes approches d'extraction de caractéristiques et de modélisation pour la vérification automatique du locuteur (ASV) et la vérification de l'énoncé (UV). Nous avons également étudié différentes stratégies de fusion pour combiner les modules UV et ASV. Notre principale contribution au défi est la fusion de sept sous-systèmes qui produit une fonction de coût de détection minimale normalisée (minDCF) de 0,072 et un taux d'erreur égal (EER) de 2,14 % sur l'ensemble d'évaluation. Le système unique consistant en un modèle basé sur l'identification des phrases de passe avec des caractéristiques de goulot d'étranglement discriminant les téléphones donne une fonction de coût de détection minimum normalisée de 0,118 et réalise une amélioration relative de 19 % par rapport à l'état de l'art de la base du défi.","19377","3","468","Système UIAI pour le Short-Duration Speaker Verification Challenge 2020

Dans ce travail, nous présentons la description du système de l&#039;entrée de l&#039;UIAI pour Short-Duration Speaker Verification (SdSV) Challenge 2020. Nous nous concentrons sur la tâche 1 dédiée à la vérification du locuteur dépendant du texte. Nous étudions différentes approches d&#039;extraction de caractéristiques et de modélisation pour la reconnaissance automatique du locuteur (ASV) et la vérification de l&#039;énoncé (UV). Nous avons également étudié différentes stratégies de fusion pour combiner les modules UV et ASV. Notre principale contribution au défi est la fusion de sept sous-systèmes qui produit une fonction de coût de détection minimale normalisée (minDCF) de 0,072 et un taux d&#039;erreur égal (EER) de 2,14 % sur l&#039;ensemble d&#039;évaluation. Le système unique consistant en un modèle basé sur l&#039;identification des phrases de passe avec des caractéristiques de goulot d&#039;étranglement discriminant les téléphones donne une fonction de coût de détection minimum normalisée de 0,118 et réalise une amélioration relative de 19 % par rapport à l&#039;état de l&#039;art de la base du défi.","2","2023-07-18 15:36:11","2023-07-18 15:39:45","19377",,"1","2","3","1","1","1",NULL,,"2023-07-18 03:36:04","0"
"3225377","Sentiment analysis aims to automatically classify the subject’s sentiment (e.g., positive, negative, or neutral) towards a particular aspect such as a topic, product, movie, news, etc. Deep learning has recently emerged as a powerful machine learning technique to tackle the growing demand for accurate sentiment analysis. However, the majority of research efforts are devoted to English-language only, while information of great importance is also available in other languages. This paper presents a novel, context-aware, deep-learning-driven, Persian sentiment analysis approach. Specifically, the proposed deep-learning-driven automated feature-engineering approach classifies Persian movie reviews as having positive or negative sentiments. Two deep learning algorithms, convolutional neural networks (CNN) and long-short-term memory (LSTM), are applied and compared with our previously proposed manual-feature-engineering-driven, SVM-based approach. Simulation results demonstrate that LSTM obtained a better performance as compared to multilayer perceptron (MLP), autoencoder, support vector machine (SVM), logistic regression and CNN algorithms.","Sentiment Analysis of Persian Movie Reviews Using Deep Learning","Kia Dashtipour, Mandar Gogate, Ahsan Adeel, Hadi Larijani, Amir Hussain","5695","Analyse du sentiment des critiques de films perses en utilisant l’apprentissage profond

L’analyse des sentiments vise à classer automatiquement le sentiment du sujet (par exemple, positif, négatif ou neutre) vers un aspect particulier tel qu’un sujet, un produit, un film, une nouvelle, etc. L’apprentissage approfondi est récemment apparu comme une technique d’apprentissage automatique puissante pour répondre à la demande croissante d’analyses de sentiments précises. Cependant, la majorité des efforts de recherche sont consacrés uniquement à la langue anglaise, tandis que des informations d’une grande importance sont également disponibles dans d’autres langues. Cet article présente une approche d’analyse du sentiment persane, nouvelle, consciente du contexte, axée sur l’apprentissage profond. Plus précisément, l’approche automatisée d’ingénierie de fonctionnalités basée sur l’apprentissage profond classe les critiques de films perses comme ayant des sentiments positifs ou négatifs. Deux algorithmes d’apprentissage profond, les réseaux neuronaux convolutionaux (CNN) et la mémoire à long terme (LSTM), sont appliqués et comparés à notre approche SVM basée sur les fonctionnalités manuelles précédemment proposée. Les résultats de simulation démontrent que LSTM a obtenu de meilleures performances par rapport au perceptron multicouche (MLP), à l’autoencodeur, à la machine vectorielle de support (SVM), à la régression logistique et aux algorithmes CNN.","13904","4","469","Analyse des sentiments des critiques de films perses en utilisant l’apprentissage profond

L’analyse des sentiments vise à classer automatiquement le sentiment du sujet (par exemple, positif, négatif ou neutre) vers un aspect particulier tel qu’un sujet, un produit, un film, une nouvelle, etc. L’apprentissage profond est récemment apparu comme une technique d’apprentissage automatique puissante pour répondre à la demande croissante d’analyses de sentiments précises. Cependant, la majorité des efforts de recherche sont consacrés uniquement à la langue anglaise, tandis que des informations d’une grande importance sont également disponibles dans d’autres langues. Cet article présente une approche d’analyse du sentiment persane, nouvelle, consciente du contexte, axée sur l’apprentissage profond. Plus précisément, l’approche automatisée d’ingénierie de fonctionnalités basée sur l’apprentissage profond classe les critiques de films perses comme ayant des sentiments positifs ou négatifs. Deux algorithmes d’apprentissage profond, les réseaux de neurones convolutifs (CNN) et le Long Short Term Memory (LSTM), sont appliqués et comparés à notre approche SVM basée sur les fonctionnalités manuelles précédemment proposée. Les résultats de simulation démontrent que LSTM a obtenu de meilleures performances par rapport au perceptron multicouche (MLP), à l’autoencodeur, à la machine vectorielle de support (SVM), à la régression logistique et aux algorithmes CNN.","2","2023-07-18 15:40:51","2023-07-18 15:46:15","13904",,"1","1","4","2","1","1",NULL,,"2023-07-18 03:40:44","0"
"3221439","With advancement of social media network, there are lots of unlabelled reviews available online, therefore its necessarily to develop an automatic tools to classify these types of reviews. For utilising these reviews for user perception, there is a need for automated tools that can process online user data for optimising user perception. In this paper, a sentiment analysis framework has been proposed to identify people's perception towards mobile networks. The proposed framework consists of three basic steps: preprocessing, feature selection and applying different machine learning algorithms. The performance of the framework has taken into account different feature combinations. The simulation results show that the best performance is by integrating unigram, bigram and trigram features.","Public Perception towards fifth generation of cellular networks (5G) on social media","Kia Dashtipour, William Taylor, Shuja Ansari, Mandar Gogate, Adnan Zahid, Yusuf Sambo, Amir Hussain, Qammer Abbasi, Muhammad Imran","5703","Perception du public vers la cinquième génération de réseaux cellulaires (5G) sur les médias sociaux

Avec l’avancement des réseaux de médias sociaux, il y a beaucoup d’avis non étiquetés disponibles en ligne, donc il est nécessairement de développer un outil automatique pour classer ces types d’avis. Pour utiliser ces avis pour la perception des utilisateurs, il est nécessaire d’utiliser des outils automatisés qui peuvent traiter les données des utilisateurs en ligne pour optimiser la perception des utilisateurs. Dans cet article, un cadre d’analyse des sentiments a été proposé pour identifier la perception des personnes vis-à-vis des réseaux mobiles. Le cadre proposé se compose de trois étapes fondamentales: prétraitement, sélection de fonctionnalités et application de différents algorithmes d’apprentissage automatique. La performance du cadre a pris en compte différentes combinaisons de caractéristiques. Les résultats de la simulation montrent que la meilleure performance est l’intégration des caractéristiques unigramme, bigram et trigramme.","13912","4","470","Perception du public vers la cinquième génération de réseaux de téléphonie mobile (5G) sur les médias sociaux

Avec l’avancement des réseaux de médias sociaux, il y a beaucoup d’avis non étiquetés disponibles en ligne, donc il est nécessairement de développer un outil automatique pour classer ces types d’avis. Pour utiliser ces avis pour la perception des utilisateurs, il est nécessaire d’utiliser des outils automatisés qui peuvent traiter les données des utilisateurs en ligne pour optimiser la perception des utilisateurs. Dans cet article, un cadre d’analyse des sentiments a été proposé pour identifier la perception des personnes vis-à-vis des réseaux mobiles. Le cadre proposé se compose de trois étapes fondamentales : prétraitement, sélection de fonctionnalités et application de différents algorithmes d’apprentissage automatique. La performance du cadre a pris en compte différentes combinaisons de caractéristiques. Les résultats de la simulation montrent que la meilleure performance est l’intégration des caractéristiques unigramme, bigramme et trigramme.","2","2023-07-18 15:46:50","2023-07-18 15:49:48","13912",,"1","1","3","1","1","1",NULL,,"2023-07-18 03:46:30","0"
"3231047","The task of automatically detecting hate speech in social media is gaining more and more attention. Given the enormous volume of content posted daily, human monitoring of hate speech is unfeasible. In this work, we propose new word-level features for automatic hate speech detection (HSD): multiword expressions (MWEs). MWEs are lexical units greater than a word that have idiomatic and compositional meanings. We propose to integrate MWE features in a deep neural network-based HSD framework. Our baseline HSD system relies on Universal Sentence Encoder (USE). To incorporate MWE features, we create a three-branch deep neural network: one branch for USE, one for MWE categories, and one for MWE embeddings. We conduct experiments on two hate speech tweet corpora with different MWE categories and with two types of MWE embeddings, word2vec and BERT. Our experiments demonstrate that the proposed HSD system with MWE features significantly outperforms the baseline system in terms of macro-F1.","Multiword Expression Features for Automatic Hate Speech Detection","Nicolas Zampieri, Irina Illina, Dominique Fohr","5679","Caractéristiques d'expression multi-mots pour la détection automatique de discours haineux

La détection automatique des discours haineux dans les médias sociaux suscite de plus en plus d'intérêt. Compte tenu de l'énorme volume de contenu publié chaque jour, la surveillance humaine des discours haineux est irréalisable. Dans ce travail, nous proposons de nouvelles caractéristiques au niveau des mots pour la détection automatique des discours haineux (HSD) : les expressions multi-mots (MWE). Les expressions multi-mots sont des unités lexicales supérieures à un mot qui ont des significations idiomatiques et compositionnelles. Nous proposons d'intégrer les caractéristiques des expressions multi-mots dans un cadre de détection automatique des discours haineux basé sur un réseau neuronal profond. Notre système HSD de base repose sur l'encodeur universel de phrases (USE). Pour intégrer les caractéristiques MWE, nous créons un réseau neuronal profond à trois branches : une branche pour USE, une pour les catégories MWE et une pour les enchâssements MWE. Nous menons des expériences sur deux corpus de tweets haineux avec différentes catégories MWE et avec deux types d'encastrements MWE, word2vec et BERT. Nos expériences démontrent que le système HSD proposé avec des caractéristiques MWE est nettement plus performant que le système de référence en termes de macro-F1.","18273","3","471","Caractéristiques des termes complexes pour la détection automatique de discours haineux

La détection automatique des discours haineux dans les médias sociaux suscite de plus en plus d&#039;intérêt. Compte tenu de l&#039;énorme volume de contenu publié chaque jour, la surveillance humaine des discours haineux est irréalisable. Dans ce travail, nous proposons de nouvelles caractéristiques au niveau des mots pour la détection automatique des discours haineux (HSD) : les expressions multi-mots (MWE). Les termes complexes sont des unités lexicales supérieures à un mot qui ont des significations idiomatiques et compositionnelles. Nous proposons d&#039;intégrer les caractéristiques des termes complexes dans un cadre de détection automatique des discours haineux basé sur un réseau neuronal profond. Notre système HSD de base repose sur l&#039;encodeur universel de phrases (USE). Pour intégrer les caractéristiques MWE, nous créons un réseau neuronal profond à trois branches : une branche pour USE, une pour les catégories MWE et une pour les enchâssements MWE. Nous menons des expériences sur deux corpus de tweets haineux avec différentes catégories MWE et avec deux types d&#039;encastrements MWE, word2vec et BERT. Nos expériences démontrent que le système HSD proposé avec des caractéristiques MWE est nettement plus performant que le système de référence en termes de macro-F1.","2","2023-07-18 15:54:44","2023-07-18 15:57:23","18273",,"1","1","4","1","1","3",NULL,,"2023-07-18 03:54:35","0"
"3109299","Progress in Sentence Simplification has been hindered by the lack of supervised data, particularly in languages other than English. Previous work has aligned sentences from original and simplified corpora such as English Wikipedia and Simple English Wikipedia, but this limits corpus size, domain, and language. In this work, we propose using unsupervised mining techniques to automatically create training corpora for simplification in multiple languages from raw Common Crawl web data. When coupled with a controllable generation mechanism that can flexibly adjust attributes such as length and lexical complexity, these mined paraphrase corpora can be used to train simplification systems in any language. We further incorporate multilingual unsupervised pretraining methods to create even stronger models and show that by training on mined data rather than supervised corpora, we outperform the previous best results. We evaluate our approach on English, French, and Spanish simplification benchmarks and reach state-of-the-art performance with a totally unsupervised approach. We will release our models and code to mine the data in any language included in Common Crawl.","Multilingual Unsupervised Sentence Simplification","Louis Martin, Angela Fan, Eric Villemonte de La Clergerie, Antoine Bordes, Benoît Sagot","5937","Simplification de la peine multilingue non supervisée

La simplification de la peine a été entravée par le manque de données supervisées, en particulier dans des langues autres que l’anglais. Les travaux précédents ont aligné des phrases de corpus originaux et simplifiés tels que Wikipédia en anglais et Wikipédia en anglais simple, mais cela limite la taille du corpus, le domaine et la langue. Dans ce travail, nous proposons d’utiliser des techniques de minage non supervisées pour créer automatiquement des corpus de formation pour la simplification dans plusieurs langues à partir de données Web brutes Common Crawl. Lorsqu’ils sont associés à un mécanisme de génération contrôlable qui peut ajuster de manière flexible des attributs tels que la longueur et la complexité lexicale, ces corpus de paraphrase minés peuvent être utilisés pour former des systèmes de simplification dans n’importe quel langage. Nous intégrons également des méthodes de préformation multilingues non supervisées pour créer des modèles encore plus solides et montrer qu’en formant sur des données minées plutôt que sur des corpus supervisés, nous surperformons les meilleurs résultats précédents. Nous évaluons notre approche sur les critères de simplification de l’anglais, du français et de l’espagnol et nous atteignons des performances de pointe avec une approche totalement non supervisée. Nous publierons nos modèles et code pour exploiter les données dans n’importe quelle langue incluse dans Common Crawl.","14146","4","472","Simplification de phrases multilingue non supervisée

La simplification de phrases a été entravée par le manque de données supervisées, en particulier dans des langues autres que l’anglais. Les travaux précédents ont aligné des phrases de corpus originaux et simplifiés tels que Wikipédia en anglais et Wikipédia en anglais simple, mais cela limite la taille du corpus, le domaine et la langue. Dans ce travail, nous proposons d’utiliser des techniques de fouille non supervisées pour créer automatiquement des corpus d&#039;entrainement pour la simplification dans plusieurs langues à partir de données Web brutes Common Crawl. Lorsqu’ils sont associés à un mécanisme de génération contrôlable qui peut ajuster de manière flexible des attributs tels que la longueur et la complexité lexicale, ces corpus de paraphrase fouillés peuvent être utilisés pour former des systèmes de simplification dans n’importe quel langage. Nous intégrons également des méthodes de préformation multilingues non supervisées pour créer des modèles encore plus solides et montrer qu’en formant sur des données fouillées plutôt que sur des corpus supervisés, nous surperformons les meilleurs résultats précédents. Nous évaluons notre approche sur les critères de simplification de l’anglais, du français et de l’espagnol et nous atteignons des performances de pointe avec une approche totalement non supervisée. Nous publierons nos modèles et code pour exploiter les données dans n’importe quelle langue incluse dans Common Crawl.","2","2023-07-18 16:28:57","2023-07-18 16:34:30","14146",,"1","1","4","2","1","2",NULL,,"2023-07-18 04:28:49","0"
"1847314","A vast amount of biomedical information is available in the form of scientific literature and government-authored patient information documents. While English is the most widely used language in many of these sources, there is a need to provide access to health information in languages other than English. Parallel corpora can be leveraged to implement cross-lingual information retrievalor machine translation tools. Herein, we review the extent of parallel corpus coverage in the biomedical domain. Specifically, we perform a scoping review of existing resources and we describe the recent development of new datasets for scientific literature (the EDP dataset and an extension of the Scielo corpus) and clinical trials (the ReBEC corpus). These corpora are currently beingused in the biomedical task in the Conference on Machine Translation (WMT16 and WMT17), which illustrates their potential for improving and evaluating biomedical machine translation systems. Furthermore, we suggest additional applications for multilingual natural language processing using these resources, and plan to extend resource coverage to additional text genres and language pairs.","Parallel Corpora for the Biomedical Domain","Aurélie Névéol, Antonio Jimeno Yepes, L Neves, Karin Verspoor","1965","Corpora parallèles pour le domaine biomédical

Une grande quantité d'informations biomédicales est disponible sous la forme de littérature scientifique et de documents d'information pour les patients rédigés par les pouvoirs publics. Bien que l'anglais soit la langue la plus utilisée dans bon nombre de ces sources, il est nécessaire de fournir un accès à l'information sur la santé dans des langues autres que l'anglais. Les corpus parallèles peuvent être utilisés pour mettre en œuvre des outils de recherche d'informations ou de traduction automatique multilingues. Nous examinons ici l'étendue de la couverture des corpus parallèles dans le domaine biomédical. Plus précisément, nous procédons à une analyse des ressources existantes et nous décrivons le développement récent de nouveaux ensembles de données pour la littérature scientifique (l'ensemble de données EDP et une extension du corpus Scielo) et les essais cliniques (le corpus ReBEC). Ces corpus sont actuellement utilisés dans la tâche biomédicale de la Conférence sur la traduction automatique (WMT16 et WMT17), ce qui illustre leur potentiel pour l'amélioration et l'évaluation des systèmes de traduction automatique biomédicale. En outre, nous suggérons d'autres applications pour le traitement multilingue du langage naturel en utilisant ces ressources, et nous prévoyons d'étendre la couverture des ressources à d'autres genres de textes et paires de langues.","10729","3","474","Corpus parallèles pour le domaine biomédical

Une grande quantité d&#039;informations biomédicales est disponible dans la littérature scientifique et dans les documents d&#039;information à l&#039;intention des patients rédigés par les pouvoirs publics. Bien que l&#039;anglais soit la langue la plus utilisée dans bon nombre de ces sources, il est nécessaire de fournir un accès à l&#039;information sur la santé dans des langues autres que l&#039;anglais. Les corpus parallèles peuvent être utilisés pour mettre en œuvre des outils multilingues de recherche d&#039;information ou de traduction automatique. Nous examinons ici l&#039;étendue de la couverture des corpus parallèles dans le domaine biomédical. Plus précisément, nous passons en revue les ressources existantes et nous décrivons le développement récent de nouveaux corpus comportant de la littérature scientifique (l&#039;ensemble de données EDP et une extension du corpus Scielo) et des essais cliniques (le corpus ReBEC). Ces corpus sont actuellement utilisés dans la tâche biomédicale de la Conférence sur la traduction automatique (WMT16 et WMT17), ce qui illustre leur potentiel pour l&#039;amélioration et l&#039;évaluation des systèmes de traduction automatique dans le domaine biomédical. En outre, nous suggérons d&#039;autres applications pour le traitement multilingue du langage naturel en utilisant ces ressources, et nous prévoyons d&#039;étendre la couverture des ressources à d&#039;autres genres de textes et paires de langues.","3","2023-08-24 16:31:13","2023-08-24 16:52:11","10729",,"2","2","1","1","1","1",NULL,,"2023-08-24 04:31:01","0"
"1990502","Many applications in biomedical natural language processing rely on sequence tagging as an initial step to perform more complex analysis. To support text analysis in the biomedical domain, we introduce Yet Another SEquence Tagger (YASET), an open-source multi purpose sequence tagger that implements state-of-the-art deep learning algorithms for sequence tagging. Herein, we evaluate YASET on part-of-speech tagging and named entity recognition in a variety of text genres including articles from the biomedical literature in English and clinical narratives in French. Tofurther characterize performance, we report distributions over 30 runs and different sizes of training datasets. YASET provides state-of-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts.","Evaluation of a Sequence Tagging Tool for Biomedical Texts","Julien Tourille, Matthieu Doutreligne, Olivier Ferret, Nicolas Paris, Aurélie Névéol, Xavier Tannier","2021","Évaluation d'un outil de marquage de séquences pour les textes biomédicaux

De nombreuses applications dans le traitement du langage naturel biomédical s'appuient sur l'étiquetage des séquences comme étape initiale pour effectuer des analyses plus complexes. Pour soutenir l'analyse de texte dans le domaine biomédical, nous présentons Yet Another SEquence Tagger (YASET), un étiqueteur de séquences polyvalent open-source qui met en œuvre des algorithmes d'apprentissage profond de pointe pour l'étiquetage de séquences. Ici, nous évaluons YASET sur l'étiquetage de la partie du discours et la reconnaissance des entités nommées dans une variété de genres de textes, y compris des articles de la littérature biomédicale en anglais et des récits cliniques en français. Pour mieux caractériser les performances, nous rapportons les distributions sur 30 exécutions et différentes tailles d'ensembles de données d'entraînement. YASET fournit des performances de pointe sur l'ensemble de données NER CoNLL 2003 (F1=0,87), le corpus MEDPOST (F1=0,97), le corpus MERLoT (F1=0,99) et le corpus de maladies NCBI (F1=0,81). Nous pensons que YASET est un outil polyvalent et efficace qui peut être utilisé pour le marquage de séquences dans les textes biomédicaux et cliniques.","7531","3","476","Évaluation d&#039;un outil de étiquetage de séquences pour les textes biomédicaux

De nombreuses applications dans le traitement du langage naturel dans le domaine biomédical s&#039;appuient sur l&#039;étiquetage des séquences comme étape initiale pour effectuer des analyses plus complexes. Pour soutenir l&#039;analyse de texte dans le domaine biomédical, nous présentons Yet Another SEquence Tagger (YASET), un étiqueteur de séquences open-source polyvalent qui met en œuvre des algorithmes d&#039;apprentissage profond de pointe pour l&#039;étiquetage de séquences. Ici, nous évaluons YASET sur l&#039;étiquetage morpho-syntaxique et la reconnaissance des entités nommées dans une variété de genres textuels, y compris des articles de la littérature biomédicale en anglais et des études cliniques en français. Pour mieux caractériser les performances de YASET, nous rapportons les distributions des résultats pour 30 itérations et pour différentes tailles des données d&#039;entraînement. YASET fournit des performances à l&#039;état de l&#039;art sur la tâche de reconnaissance d&#039;entités nommées CoNLL 2003 (F1=0,87), le corpus MEDPOST (F1=0,97), le corpus MERLoT (F1=0,99) et le corpus NCBI disease (F1=0,81). Nous pensons que YASET est un outil polyvalent et efficace qui peut être utilisé pour l&#039;étiquetage de séquences dans les textes biomédicaux et cliniques.","3","2023-08-24 16:55:42","2023-08-24 17:13:59","7531",,"1","1","3","1","1","1",NULL,,"2023-08-24 04:55:38","0"
"3812319","Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work. We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages. We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies.","You Reap What You Sow: On the Challenges of Bias Evaluation Under Multilingual Settings","Zeerak Talat, Aurélie Névéol, Stella Biderman, Miruna Clinciu, Manan Dey, Shayne Longpre, Sasha Luccioni, Maraim Masoud, Margaret Mitchell, Dragomir Radev, Shanya Sharma, Arjun Subramonian, Jaesung Tae, Samson Tan, Deepak Tunuguntla, Oskar van der Wal","542","Vous récoltez ce que vous semez: Sur les défis de l’évaluation des risques dans des paramètres multilingues

L’évaluation des préjugés, de l’équité et de l’impact social dans les modèles linguistiques monolingues est une tâche difficile. Ce défi est encore aggravé lorsque la modélisation linguistique se produit dans un contexte multilingue. Compte tenu de l’implication des biais d’évaluation pour les grands modèles linguistiques multilingues, nous situons la discussion de l’évaluation des biais dans un contexte plus large de recherche scientifique sociale avec des travaux de calcul. Nous soulignons trois dimensions de l’élaboration de cadres d’évaluation des biais multilingues: 1) accroître la transparence grâce à la documentation, 2) élargir les cibles de préjugés au-delà du sexe, et (3) traiter les différences culturelles qui existent entre les langues. Nous discutons en outre de la dynamique de puissance et des conséquences de la formation de grands modèles linguistiques et recommandons aux chercheurs de rester conscients des ramifications du développement de telles technologies.","813","4","480","On récolte ce que l&#039;on sème : défis de l’évaluation des biais dans des contextes multilingues

L’évaluation des biais, de l’équité et de l’impact sociétal dans les modèles de langue monolingues est une tâche difficile. Ce défi est encore plus important dans le cas de la modélisation linguistique multilingue. Compte tenu de l&#039;enjeu de l&#039;évaluation des biais des grands modèles de langue multilingues, nous situons la discussion de l’évaluation des biais dans un contexte plus large de la manière dont la recherche scientifique impacte la société grâce au numérique. Nous soulignons trois dimensions de l’élaboration de cadres d’évaluation des biais en dans un contexte multilingue : 1) accroître la transparence grâce à la documentation, 2) élargir les cibles des stéréotypes étudiés au-delà du genre, et 3) aborder les différences culturelles qui existent entre les langues. Nous discutons en outre de la dynamique de pouvoir et des conséquences de l&#039;entraînement de grands modèles de langue et recommandons aux chercheurs de rester conscients des ramifications du développement de telles technologies.","3","2023-08-24 17:23:54","2023-08-24 17:40:13","813",,"2","1","3","2","2","2",NULL,"within a wider context of social scientific research with computational work --&gt; pas très clairement rédigé en anglais. ","2023-08-24 05:23:52","0"
"3901369","Cognate prediction is the task of generating, in a given language, the likely cognates of words in a related language, where cognates are words in related languages that have evolved from a common ancestor word. It is a task for which little data exists and which can aid linguists in the discovery of previously undiscovered relations. Previous work has applied machine translation (MT) techniques to this task, based on the tasks' similarities, without, however, studying their numerous differences or optimising architectural choices and hyper-parameters. In this paper, we investigate whether cognate prediction can benefit from insights from low-resource MT. We first compare statistical MT (SMT) and neural MT (NMT) architectures in a bilingual setup. We then study the impact of employing data augmentation techniques commonly seen to give gains in low-resource MT: monolingual pretraining, backtranslation and multilinguality. Our experiments on several Romance languages show that cognate prediction behaves only to a certain extent like a standard lowresource MT task. In particular, MT architectures, both statistical and neural, can be successfully used for the task, but using supplementary monolingual data is not always as beneficial as using additional language data, contrarily to what is observed for MT.","Can Cognate Prediction Be Modelled as a Low-Resource Machine Translation Task?","Clémentine Fourrier, Rachel Bawden, Benoît Sagot","437","La prévision apparentée peut-elle être modélisée en tant que tâche de traduction automatique à faibles ressources ?

La prédiction des mots apparentés est la tâche de générer, dans une langue donnée, les mots apparentés probables dans une langue apparentée, où les mots apparentés sont des mots dans des langues apparentées qui ont évolué à partir d'un mot ancêtre commun. C'est une tâche pour laquelle peu de données existent et qui peut aider les linguistes dans la découverte de relations précédemment non découvertes. Les travaux précédents ont appliqué des techniques de traduction automatique (MT) à cette tâche, en fonction des similitudes des tâches, sans toutefois étudier leurs nombreuses différences ou optimiser les choix architecturaux et les hyper-paramètres. Dans cet article, nous examinons si la prédiction congénitale peut bénéficier des informations provenant de MT à faibles ressources. Nous comparons d'abord les architectures MT statistiques (SMT) et MT neuronales (NMT) dans une configuration bilingue. Nous étudions ensuite l'impact de l'utilisation de techniques d'augmentation des données communément vues pour donner des gains en MT à faibles ressources : préformation monolingue, rétrotraduction et multilinguisme. Nos expériences sur plusieurs langages romains montrent que la prédiction apparentée ne se comporte que dans une certaine mesure comme une tâche MT standard à faibles ressources. En particulier, les architectures MT, tant statistiques que neuronales, peuvent être utilisées avec succès pour la tâche, mais l'utilisation de données monolingues supplémentaires n'est pas toujours aussi bénéfique que l'utilisation de données linguistiques supplémentaires, contrairement à ce qui est observé pour MT.","4184","6","486","La prévision des cognats peut-elle être modélisée comme une tâche de traduction automatique peu dotée ?

La prédiction des cognats est la tâche qui consiste à générer, à partir d&#039;une une langue donnée, les cognats probables dans une langue apparentée, où les cognats sont des mots apparentés dans des langues proches qui ont évolué à partir d&#039;un mot ancêtre commun. C&#039;est une tâche pour laquelle peu de données existent et qui peut aider les linguistes dans la découverte de relations précédemment non découvertes. Les travaux précédents ont appliqué des techniques de traduction automatique (TA) à cette tâche, en se basant sur la similarité des tâches, sans toutefois étudier leurs nombreuses différences ou optimiser les choix architecturaux et les hyper-paramètres. Dans cet article, nous examinons si la prédiction des cognats peut bénéficier des méthodes de TA en contexte peu doté. Nous comparons d&#039;abord les architectures de TA statistiques (TAS) et neuronales (TAN) dans une configuration bilingue. Nous étudions ensuite l&#039;impact de l&#039;utilisation de techniques d&#039;augmentation des données communément employées pour améliorer les performances de la TA peu dotée : pré-entraînement monolingue, rétrotraduction et multilinguisme. Nos expériences sur plusieurs langages romanes montrent que la prédiction des cognats ne se comporte que dans une certaine mesure comme une tâche standard de TA peu dotée. En particulier, les architectures de TA, tant statistiques que neuronales, peuvent être utilisées avec succès pour la tâche, mais l&#039;utilisation de données monolingues supplémentaires n&#039;est pas toujours aussi bénéfique que l&#039;utilisation de données linguistiques supplémentaires, contrairement à ce qui est observé pour la TA.","3","2023-08-24 17:42:23","2023-08-24 18:00:15","4184",,"2","1","4","1","1","2",NULL,"La dernière phrase n&#039;est pas très claire en anglais - car les données monolingues sont un type particulier de donnée linguistique, non ? ","2023-08-24 05:42:21","0"
"1011059","This article presents the methods, results, and precision of the syntactic annotation process of the Rhapsodie Treebank of spoken French. The Rhapsodie Treebank is an 33,000 word corpus annotated for prosody and syntax, licensed in its entirety under Creative Commons. The syntactic annotation contains two levels: a macro-syntactic level, containing a segmentation into illocutionary units (including discourse markers, parentheses ...) and a micro-syntactic level including dependency relations and various paradigmatic structures, called pile constructions, the latter being particularly frequent and diverse in spoken language. The micro-syntactic annotation process, presented in this paper, includes a semi-automatic preparation of the transcription, the application of a syntactic dependency parser, transcoding of the parsing results to the Rhapsodie annotation scheme, manual correction by multiple annotators followed by a validation process, and finally the application of coherence rules that check common errors. The good inter-annotator agreement scores are presented and analyzed in greater detail. The article also includes the list of functions used in the dependency annotation and for the distinction of various pile constructions and presents the ideas underlying these choices.","Correcting and Validating Syntactic Dependency in the Spoken French Treebank Rhapsodie","Rachel Bawden, Marie-Amélie Bottala, Kim Gerdes, Sylvain Kahane","2160","Correction et validation de la dépendance syntaxique dans le Rhapsodie de la banque d’arbres française parlée

Cet article présente les méthodes, les résultats et la précision du processus d’annotation syntaxique de la Rhapsodie Treebank de français parlé. Le Rhapsodie Treebank est un corpus de 33 000 mots annoté pour la prosodie et la syntaxe, sous licence dans son intégralité sous Creative Commons. L’annotation syntaxique contient deux niveaux: un niveau macro-syntactique, contenant une segmentation en unités illocutionnaires (y compris des marqueurs de discours, des parenthèses...) et un niveau micro-syntactique comprenant des relations de dépendance et diverses structures paradigmatiques, appelées constructions de pile, ces dernières étant particulièrement fréquentes et diversifiées dans le langage parlé. Le processus d’annotation micro-syntactique, présenté dans cet article, comprend une préparation semi-automatique de la transcription, l’application d’un analyseur de dépendance syntaxique, le transcodage des résultats d’analyse au schéma d’annotation Rhapsodie, la correction manuelle par plusieurs annotateurs suivie d’un processus de validation, et enfin l’application de règles de cohérence qui vérifient les erreurs courantes. Les bons scores d’accord inter-annotateurs sont présentés et analysés plus en détail. L’article comprend également la liste des fonctions utilisées dans l’annotation de dépendance et pour la distinction des différentes constructions de pieux et présente les idées qui sous-tendent ces choix.","2433","4","492","Correction et validation de la dépendance syntaxique dans le corpus arboré du français parlé Rhapsodie 

Cet article présente les méthodes, les résultats et la précision du processus d’annotation syntaxique du corpus arboré du français parlé Rhapsodie. Le corpus arboré Rhapsodie est un corpus de 33 000 mots annoté pour la prosodie et la syntaxe, sous licence Creative Commons. L’annotation syntaxique contient deux niveaux: un niveau macro-syntaxique, comportant une segmentation en unités illocutoires (y compris des marqueurs de discours, des parenthèses...) et un niveau micro-syntaxique comprenant des relations de dépendance et diverses structures paradigmatiques, appelées constructions de pile, ces dernières étant particulièrement fréquentes et diversifiées dans le langage parlé. Le processus d’annotation micro-syntaxique, présenté dans cet article, comprend une préparation semi-automatique de la transcription, l’application d’un analyseur syntaxique en dépendance, le transcodage des résultats d’analyse au schéma d’annotation Rhapsodie, la correction manuelle par plusieurs annotateurs suivie d’un processus de validation, et enfin l’application des règles de cohérence qui vérifient les erreurs courantes. Les bons scores d’accord inter-annotateurs sont présentés et analysés plus en détail. L’article comprend également la liste des fonctions utilisées dans l’annotation en dépendances et pour la distinction des différentes constructions de pile et présente les idées qui sous-tendent ces choix.","3","2023-08-24 20:04:32","2023-08-24 20:10:39","2433",,"2","2","3","2","1","1",NULL,,"2023-08-24 08:04:31","0"
"1842475","In  this  paper,  we  present  a  method  for temporal relation extraction from clinical narratives  in  French  and  in  English.   We experiment  on  two  comparable  corpora, the  MERLOT  corpus  for  French  and  the THYME corpus for English, and show that a common approach can be used for both languages.","Temporal information extraction from clinical text","Julien Tourille, Olivier Ferret, Xavier Tannier, Aurélie Névéol","1958","Extraction d'informations temporelles à partir de textes cliniques

Dans cet article, nous présentons une méthode d'extraction de relations temporelles à partir de récits cliniques en français et en anglais. Nous expérimentons sur deux corpus comparables, le corpus MERLOT pour le français et le corpus THYME pour l'anglais, et montrons qu'une approche commune peut être utilisée pour les deux langues.","5208","6","493","Extraction d&#039;informations temporelles à partir de textes cliniques

Dans cet article, nous présentons une méthode d&#039;extraction de relations temporelles à partir de documents cliniques en français et en anglais. Nous expérimentons sur deux corpus comparables, le corpus MERLOT pour le français et le corpus THYME pour l&#039;anglais, et montrons qu&#039;une approche commune peut être utilisée pour les deux langues.","3","2023-08-24 20:11:21","2023-08-24 20:13:02","5208",,"1","1","2","1","1","1",NULL,,"2023-08-24 08:11:08","0"
"3014123","Domain adaptation is an old and vexing problem for machine translation systems. The most common and successful approach to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the model unchanged. This has the additional merit to leave the baseline model intact and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea using two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model and open perspective to also make adapted models more robust to label domain errors.","A Study of Residual Adapters for Multi-Domain Neural Machine Translation","Minh Quang Pham, Josep-Maria Crego, François Yvon, Jean Senellart","1269","Une étude des adaptateurs résiduels pour la traduction automatique de neurones multi-domaines

L’adaptation de domaine est un problème ancien et vexant pour les systèmes de traduction automatique. L’approche la plus courante et la plus réussie en matière d’adaptation supervisée consiste à affiner un système de base avec des données parallèles dans le domaine. Le réglage standard modifie cependant tous les paramètres du réseau, ce qui rend cette approche coûteuse et sujette à un surajustement. Une approche récente et légère augmente plutôt un modèle de base avec des couches d’adaptateur supplémentaires (petites), gardant le reste du modèle inchangé. Cela a le mérite supplémentaire de laisser le modèle de base intact et adaptable à plusieurs domaines. Dans cet article, nous effectuons une analyse approfondie du modèle d’adaptateur dans le cadre d’une tâche de traduction automatique multidomaine. Nous contrastons plusieurs implémentations de cette idée en utilisant deux paires de langages. Nos principales conclusions sont que les adaptateurs résiduels fournissent une méthode rapide et bon marché pour l’adaptation multidomaine supervisée; nos deux variantes s’avèrent aussi efficaces que le modèle d’adaptateur d’origine et une perspective ouverte pour rendre également les modèles adaptés plus robustes pour étiqueter les erreurs de domaine.","3530","4","503","Une étude des adaptateurs résiduels pour la traduction automatique neuronale multi-domaines

L’adaptation au domaine est un problème ancien et délicat pour les systèmes de traduction automatique. L’approche la plus courante et la plus efficace en matière d’adaptation supervisée consiste à affiner un système de base avec des données parallèles du domaine. L&#039;affinage standard modifie cependant tous les paramètres du réseau, ce qui rend cette approche coûteuse et sujette au sur-apprentissage. Une approche récente et moins coûteuse augmente plutôt un modèle de base avec des (petites) couches d’adaptateurs supplémentaires, gardant le reste du modèle inchangé. Cela a l&#039;avantage supplémentaire de laisser le modèle de base intact et adaptable à plusieurs domaines. Dans cet article, nous effectuons une analyse approfondie du modèle avec adaptation dans le cadre d’une tâche de traduction automatique multi-domaine. Nous comparons plusieurs implémentations de cette idée en utilisant deux paires de langages. Nos principales conclusions sont que les adaptateurs résiduels fournissent une méthode rapide et bon marché pour l’adaptation multi-domaine supervisée ; nos deux variantes s’avèrent aussi efficaces que le modèle adapté d’origine et ouvrent également des perspectives prometteuses pour rendre les modèles adaptés plus robustes aux erreurs d&#039;étiquetage des domaines.","3","2023-09-18 15:42:37","2023-09-18 16:26:58","3530",,"3","2","3","1","2","2",NULL,,"2023-09-18 03:42:20","0"
"1674140","The dream of a universal translation device goes back many decades, long before Douglas Adams’s fictional Babel fish provided this service in The Hitchhiker’s Guide to the Galaxy. Since the advent of computers, research has focused on the design of digital machine translation tools—computer programs capable of automatically translating a text from a source language to a target language. This has become one of the most fundamental tasks of artificial intelligence. This volume in the MIT Press Essential Knowledge series offers a concise, nontechnical overview of the development of machine translation, including the different approaches, evaluation issues, and market potential. The main approaches are presented from a largely historical perspective and in an intuitive manner, allowing the reader to understand the main principles without knowing the mathematical details. The book begins by discussing problems that must be solved during the development of a machine translation system and offering a brief overview of the evolution of the field. It then takes up the history of machine translation in more detail, describing its pre-digital beginnings, rule-based approaches, the 1966 ALPAC (Automatic Language Processing Advisory Committee) report and its consequences, the advent of parallel corpora, the example-based paradigm, the statistical paradigm, the segment-based approach, the introduction of more linguistic knowledge into the systems, and the latest approaches based on deep learning. Finally, it considers evaluation challenges and the commercial status of the field, including activities by such major players as Google and Systran.","Machine Translation","Thierry Poibeau","3257","Traduction automatique

Le rêve d’un dispositif de traduction universel remonte à de nombreuses décennies, bien avant que le poisson fictif Babel de Douglas Adams fournisse ce service dans The Hitchhiker’s Guide to the Galaxy. Depuis l’avènement des ordinateurs, la recherche s’est concentrée sur la conception d’outils numériques de traduction automatique — des programmes informatiques capables de traduire automatiquement un texte d’une langue source vers une langue cible. C’est devenu l’une des tâches les plus fondamentales de l’intelligence artificielle. Ce volume de la série MIT Press Essential Knowledge offre un aperçu concis et non technique du développement de la traduction automatique, y compris les différentes approches, les questions d’évaluation et le potentiel du marché. Les approches principales sont présentées d’un point de vue largement historique et d’une manière intuitive, permettant au lecteur de comprendre les principes principaux sans connaître les détails mathématiques. Le livre commence par discuter des problèmes qui doivent être résolus lors du développement d’un système de traduction automatique et offrir un bref aperçu de l’évolution du domaine. Il reprend ensuite plus en détail l’histoire de la traduction automatique, décrivant ses débuts prénumériques, ses approches fondées sur des règles, le rapport de 1966 ALPAC (Automatic Language Processing Advisory Committee) et ses conséquences, l’avènement de corpus parallèles, le paradigme basé sur l’exemple, le paradigme statistique, l’approche par segment, l’introduction de plus de connaissances linguistiques dans les systèmes, et les dernières approches basées sur l’apprentissage profond. Enfin, il examine les enjeux d’évaluation et le statut commercial du terrain, y compris les activités d’acteurs majeurs tels que Google et Systran.","2038","4","505","La traduction automatique 

Le rêve d’un dispositif de traduction universel remonte à de nombreuses décennies, bien avant que le poisson Babel imaginaire de Douglas Adams ne fournisse ce service dans Le Guide du voyageur galactique. Depuis l’avènement des ordinateurs, la recherche s’est concentrée sur la conception d’outils numériques de traduction automatique — des programmes informatiques capables de traduire automatiquement un texte d’une langue source vers une langue cible. C’est devenu l’une des tâches les plus fondamentales de l’intelligence artificielle. Ce volume de la série MIT Press Essential Knowledge offre un aperçu concis et non technique du développement de la traduction automatique, portant sur les différentes approches à la traduction automatique, les questions d’évaluation et le potentiel commercial. Les approches principales sont présentées dans une perspective largement historique et d’une manière intuitive, permettant au lecteur d&#039;en comprendre les principes fondamentaux sans en connaître les détails mathématiques. Le livre commence par discuter des problèmes qui doivent être résolus lors du développement d’un système de traduction automatique et offrir un bref aperçu de l’évolution du domaine. Il reprend ensuite plus en détail l’histoire de la traduction automatique, décrivant ses débuts pré-numériques, ses approches fondées sur des règles, le rapport de ALPAC (Automatic Language Processing Advisory Committee) de 1966 et ses conséquences, l’avènement de corpus parallèles, le paradigme basé sur l’exemple, le paradigme statistique, l’approche par segment, l’introduction de plus de connaissances linguistiques dans les systèmes, et les dernières approches basées sur l’apprentissage profond. Enfin, il examine les enjeux d’évaluation et le statut commercial du domaine, y compris les activités d’acteurs majeurs tels que Google et Systran.","3","2023-09-18 17:02:11","2023-09-18 17:10:51","2038",,"3","1","3","1","1","1",NULL,,"2023-09-18 05:02:06","0"
"3029255","We treat projective dependency trees as latent variables in our probabilistic model and induce them in such a way as to be beneficial for a downstream task, without relying on any direct tree supervision. Our approach relies on Gumbel perturbations and differentiable dynamic programming. Unlike previous approaches to latent tree learning, we stochastically sample global structures and our parser is fully differentiable. We illustrate its effectiveness on sentiment analysis and natural language inference tasks. We also study its properties on a synthetic structure induction task. Ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees.","Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming","Caio Corro, Ivan Titov","1253","Apprentissage d'arbres latents avec perturbations stochastiques et programmation dynamique différentiable

Nous traitons les arbres de dépendance projectifs comme des variables latentes dans notre modèle probabiliste et les induisons de manière à ce qu'ils soient bénéfiques pour une tâche en aval, sans dépendre d'une supervision directe de l'arbre. Notre approche repose sur les perturbations de Gumbel et la programmation dynamique différentiable. Contrairement aux approches précédentes de l'apprentissage des arbres latents, nous échantillonnons stochastiquement les structures globales et notre analyseur syntaxique est entièrement différentiable. Nous illustrons son efficacité sur des tâches d'analyse de sentiments et d'inférence du langage naturel. Nous étudions également ses propriétés dans le cadre d'une tâche d'induction de structures synthétiques. Les études d'ablation soulignent l'importance de la stochasticité et de l'obligation pour les structures latentes d'être des arbres projectifs.","9521","3","517","Apprentissage d&#039;arbres latents avec perturbations aléatoires et programmation dynamique différentiable

Dans notre modèle probabiliste, nous traitons les arbres de dépendance projectifs comme des variables latentes et nous les induisons de manière à ce qu&#039;ils soient bénéfiques pour une tâche en aval, sans dépendre d&#039;une supervision directe de l&#039;arbre. Notre approche repose sur les perturbations de Gumbel et la programmation dynamique différentiable. Contrairement aux approches précédentes de l&#039;apprentissage des arbres latents, nous échantillonnons aléatoirement les structures globales et notre analyseur syntaxique est entièrement différentiable. Nous illustrons son efficacité sur des tâches d&#039;analyse des sentiments et d&#039;inférence en langue naturelle. Nous étudions également ses propriétés dans le cadre d&#039;une tâche d&#039;induction sur des structures synthétiques. Les études d&#039;ablation soulignent l&#039;importance des perturbations aléatoires et de la contrainte pour les structures latentes d&#039;être des arbres projectifs.","3","2023-09-20 14:26:32","2023-09-20 14:49:58","9521",,"2","2","2","1","1","1",NULL,,"2023-09-20 02:26:29","0"
"3029253","We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures. In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(n^6) down to O(n^3). The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers. We evaluate our approach on German and English treebanks (Negra, Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting. We also experiment with pre-trained word embeddings and Bertbased neural networks.","Span-based discontinuous constituency parsing: a family of exact chart-based algorithms with time complexities from O(n^6) down to O(n^3)","Caio Corro","6099","Analyse de constituants discontinus basée sur l'étendue : une famille d'algorithmes exacts basés sur des diagrammes avec des complexités temporelles de O(n^6) à O(n^3)

Nous présentons un nouvel algorithme basé sur les diagrammes pour l'analyse syntaxique basée sur l'étendue des arbres de circonscription discontinus de degré de bloc deux, y compris les structures mal imbriquées. En particulier, nous montrons que nous pouvons construire des variantes de notre analyseur syntaxique avec des espaces de recherche plus petits et des complexités temporelles allant de O(n^6) à O(n^3). La variante à temps cubique couvre 98% des constituants observés dans les banques de données linguistiques tout en ayant la même complexité que les analyseurs de constituants continus. Nous évaluons notre approche sur des banques de données allemandes et anglaises (Negra, Tiger et DPTB) et rapportons des résultats de pointe dans un cadre entièrement supervisé. Nous expérimentons également avec des word embeddings pré-entraînés et des réseaux neuronaux basés sur Bert.","19411","3","521","Analyse de constituants discontinus basée sur les empans : une famille d&#039;algorithmes tabulaires exacts basés avec des complexités temporelles allant de O(n^6) à O(n^3)

Nous présentons un nouvel algorithme tabulaire pour l&#039;analyse syntaxique basée sur les empans des arbres de constituants discontinus de degré de bloc deux, y compris les structures mal imbriquées. En particulier, nous montrons que nous pouvons produire des variantes de notre analyseur syntaxique avec des espaces de recherche plus réduits et des complexités temporelles allant de O(n^6) à O(n^3). La variante à temps cubique couvre 98% des constituants observés dans les corpus arborés tout en ayant la même complexité que les analyseurs de constituants continus. Nous évaluons notre approche sur des corpus arborés allemands et anglais (Negra, Tiger et DPTB) et rapportons des résultats à l&#039;état de l&#039;art dans un cadre entièrement supervisé. Nous expérimentons également avec des plongements lexicaux pré-entraînés et des réseaux neuronaux basés sur Bert.","3","2023-09-21 10:59:27","2023-09-21 11:22:08","19411",,"3","2","3","1","1","2",NULL,,"2023-09-21 10:59:17","0"
"3434803","Semantic relations exist two concepts present in the text. Semantic relation extraction becomes an essential part of building an efficient Natural Language Processing (NLP) applications such as Question Answering (QA) and Information Retrieval (IR) system. Automatic semantic relation extraction from text increases the efficiency of these systems by aiding in retrieving more accurate information to the user query. In this research work, we have proposed a framework that extracts agricultural entities and finds the semantic relation exist between entities. Entity extraction is done using a Parts Of Speech (POS) tagger, Word Suffixes and Thesaurus without using any of the external domain-specific knowledge bases, such as Ontology and WordNet. Semantic relation exists between entities are done by using Multinomial Naïve Bayes (MNB) classifier. This paper extracts two entities, namely disease and treatment and focuses on two semantic relations namely “Cure” and “Prevent”. The “Cure” semantic relation expresses the remedial measure for the diseases that prevail in the crops, and the “Prevent” semantic relation shows the precautionary measures that could prevent the crop from being affected. The proposed approach has been trained with 2281 sentences and tested against 553 sentences and then evaluated using standard metrics.","A Thesaurus Based Semantic Relation Extraction for Agricultural Corpora","R. Srinivasan, C. Subalalitha","922","Extraction de relations sémantiques basée sur un thésaurus pour des corps agricoles

Les relations sémantiques existent deux concepts présents dans le texte. L'extraction de relations sémantiques devient un élément essentiel de la construction d'applications efficaces de traitement du langage naturel (PNL), telles que le système de réponse aux questions (QA) et de récupération d'informations (IR). L'extraction automatique de relations sémantiques à partir de texte augmente l'efficacité de ces systèmes en aidant à récupérer des informations plus précises pour la requête de l'utilisateur. Dans ce travail de recherche, nous avons proposé un cadre qui extrait des entités agricoles et trouve la relation sémantique existe entre les entités. L'extraction d'entité est effectuée à l'aide d'un marqueur de parties de la parole (POS), de suffixes de mots et de thésaurus sans utiliser aucune des bases de connaissances externes spécifiques au domaine, telles que Ontology et WordNet. Les relations sémantiques entre les entités sont réalisées en utilisant le classificateur Multinomial Naïve Bayes (MNB). Cet article extrait deux entités, à savoir la maladie et le traitement et se concentre sur deux relations sémantiques à savoir «Cure» et «Prevent». La relation sémantique « Cure » exprime la mesure curative pour les maladies qui prévalent dans les cultures, et la relation sémantique « Prevent » montre les mesures de précaution qui pourraient empêcher la culture d'être affectée. L'approche proposée a été formée avec 2 281 phrases et testée par rapport à 553 phrases, puis évaluée à l'aide de mesures standard.","4669","6","1","Extraction de relations sémantiques basée sur un thésaurus pour des corpus agricoles

Les relations sémantiques regroupe deux concepts présents dans les textes. L&#039;extraction de relations sémantiques devient un élément essentiel de la construction d&#039;applications efficaces de traitement automatique des langues (TAL), telles que le système de questions-réponses et de récupération d&#039;informations. L&#039;extraction automatique de relations sémantiques à partir de texte augmente l&#039;efficacité de ces systèmes en aidant à récupérer des informations plus précises pour la requête de l&#039;utilisateur. Dans ce travail de recherche, nous avons proposé un cadre qui extrait des entités agricoles et trouve la relation sémantique qui existe entre elles. L&#039;extraction d&#039;entité est effectuée à l&#039;aide d&#039;un étiqueteur morphosyntaxique, de suffixes de mots et de thésaurus sans utiliser aucune des bases de connaissances externes spécifiques au domaine, telles que Ontology ou WordNet. Les relations sémantiques entre les entités sont déterminées en utilisant le classificateur Multinomial Naïve Bayes (MNB). Cet article extrait deux entités, à savoir la maladie et le traitement et se concentre sur deux relations sémantiques à savoir « Cure » et « Prevent ». La relation sémantique « Cure » exprime la mesure curative pour les maladies qui prévalent dans les cultures, et la relation sémantique « Prevent » montre les mesures de précaution qui pourraient empêcher la culture d&#039;être affectée. L&#039;approche proposée a été formée avec 2 281 phrases et testée par rapport à 553 phrases, puis évaluée à l&#039;aide de mesures standard.","4","2023-05-30 10:59:36","2023-05-30 11:04:10","4669",,"2","1","4","3","2","1",NULL,"À chaque fois que le texte source emploie le verbe &quot;exist&quot;, il y a un problème grammatical rendant la phrase au mieux incorrecte, au pire inintelligible. Dans ces conditions, la TA n’est pas en mesure de produire une traduction convenable étant donné qu’elle ne peut améliorer le texte source.","2023-05-30 10:58:50","0"
"3762935","We present the AGODA (Analyse semantique et Graphes relationnels pour l’Ouverture des Débats à l’Assemblée nationale) project, which aims to create a platform for consulting and exploring digitised French parliamentary debates (1881-1940) available in the digital library of the National Library of France. This project brings together historians and NLP specialists: parliamentary debates are indeed an essential source for French history of the contemporary period, but also for linguistics. This project therefore aims to produce a corpus of texts that can be easily exploited with computational methods, and that respect the TEI standard. Ancient parliamentary debates are also an excellent case study for the development and application of tools for publishing and exploring large historical corpora. In this paper, we present the steps necessary to produce such a corpus. We detail the processing and publication chain of these documents, in particular by mentioning the problems linked to the extraction of texts from digitised images. We also introduce the first analyses that we have carried out on this corpus with “bag-of-words” techniques not too sensitive to OCR quality (namely topic modelling and word embedding).","Between History and Natural Language Processing: Study, Enrichment and Online Publication of French Parliamentary Debates of the Early Third Republic (1881–1899)","Marie Puren, Aurélien Pellet, Nicolas Bourgeois, Pierre Vernus, Fanny Lebreton","4406","Entre l'histoire et le traitement du langage naturel : Étude, enrichissement et publication en ligne des débats parlementaires français de la Troisième République (1881-1899)

Nous présentons le projet AGODA (Analyze semantique et Graphes relationnels pour l’Ouverture des Débats à l’Assemblée nationale), qui vise à créer une plateforme de consultation et d’exploration des débats parlementaires français numérisés (1881-1940) disponible dans la bibliothèque numérique de la Bibliothèque nationale de France. Ce projet réunit des historiens et des spécialistes de la PNL : les débats parlementaires sont en effet une source essentielle pour l'histoire française de l'époque contemporaine, mais aussi pour la linguistique. Ce projet vise donc à produire un corpus de textes facilement exploitables par des méthodes de calcul, et respectant la norme TEI. Les débats parlementaires anciens sont également une excellente étude de cas pour le développement et l'application d'outils pour la publication et l'exploration de grands corpus historiques. Dans cet article, nous présentons les étapes nécessaires pour produire un tel corpus. Nous détaillons la chaîne de traitement et de publication de ces documents, notamment en mentionnant les problèmes liés à l'extraction de textes à partir d'images numérisées. Nous présentons également les premières analyses que nous avons réalisées sur ce corpus avec des techniques « bag-of-words » peu sensibles à la qualité OCR (à savoir modélisation thématique et intégration de mots).","16322","6","2","Entre Histoire et le traitement automatique des langues : étude, enrichissement et publication en ligne des débats parlementaires français de la Troisième République (1881-1899)

Nous présentons le projet AGODA (Analyze semantique et Graphes relationnels pour l’Ouverture des Débats à l’Assemblée nationale), qui vise à créer une plateforme de consultation et d’exploration des débats parlementaires français numérisés (1881-1940) disponible dans la bibliothèque numérique de la Bibliothèque Nationale de France. Ce projet réunit des historiens et des spécialistes du TAL : les débats parlementaires sont en effet une source essentielle pour l&#039;histoire française de l&#039;époque contemporaine, mais aussi pour la linguistique. Ce projet vise donc à produire un corpus de textes facilement exploitables par des méthodes de calcul, et respectant la norme TEI. Les débats parlementaires anciens sont également une excellente étude de cas pour le développement et l&#039;application d&#039;outils pour la publication et l&#039;exploration de grands corpus historiques. Dans cet article, nous présentons les étapes nécessaires pour produire un tel corpus. Nous détaillons la chaîne de traitement et de publication de ces documents, notamment en mentionnant les problèmes liés à l&#039;extraction de textes à partir d&#039;images numérisées. Nous présentons également les premières analyses que nous avons réalisées sur ce corpus avec des techniques « sac-de-mots » peu sensibles à la qualité OCR (à savoir la modélisation thématique et le plongement lexical).","4","2023-05-30 11:04:55","2023-05-30 11:15:29","16322",,"1","1","3","1","2","1",NULL,,"2023-05-30 11:04:10","0"
"3466721","The Artificial Intelligence for Media and Humanities laboratory (AIMH) has the mission to investigate and advance the state of the art in the Artificial Intelligence field, specifically addressing applications to digital media and digital humanities, and taking also into account issues related to scalability. This report summarize the 2020 activities of the research group.","AIMH Research Activities 2020","Nicola Aloia, Giuseppe Amato, Valentina Bartalesi, Filippo Benedetti, Paolo Bolettieri, Fabio Carrara, Vittore Casarosa, Luca Ciampi, Cesare Concordia, Silvia Corbara, Marco Benedetto, Andrea Esuli, Fabrizio Falchi, Claudio Gennaro, Gabriele Lagani, Fabio Valerio Massoli, Carlo Meghini, Nicola Messina, Daniele Metilli, Alessio Molinari, Alejandro Moreo, Alessandro Nardi, Andrea Pedrotti, Nicolò Pratelli, Fausto Rabitti, Pasquale Savino, Fabrizio Sebastiani, Costantino Thanos, Luca Trupiano, Lucia Vadicamo, Claudio Vairo","5142","Activités de recherche de l’AIMH 2020

Le laboratoire d’intelligence artificielle pour les médias et les sciences humaines (AIMH) a pour mission d’étudier et de faire progresser l’état de la technique dans le domaine de l’intelligence artificielle, en abordant spécifiquement les applications aux médias numériques et aux humanités numériques, ainsi qu’en tenant compte des questions liées à l’évolutivité. Ce rapport résume les activités 2020 du groupe de recherche.","11929","4","5","Activités de recherche de l’AIMH 2020

Le laboratoire d’intelligence artificielle pour les médias et les sciences humaines (AIMH) a pour mission d’étudier et de faire progresser la technique dans le domaine de l’intelligence artificielle, en abordant spécifiquement les applications aux médias numériques et aux humanités numériques, ainsi qu’en tenant compte des questions liées à l’évolutivité. Ce rapport résume les activités 2020 du groupe de recherche.","4","2023-05-30 11:29:30","2023-05-30 11:33:16","11929",,"1","1","1","2","1","1",NULL,,"2023-05-30 11:29:07","0"
"1899826","In this paper, we propose a two-step method to normalize multi-word terms with concepts from a domain-specific ontology. Normalization is a critical step of information extraction. The method uses vector representations of terms computed with word embedding information and hierarchical information among ontology concepts. A training dataset and a first result dataset with high precision and low recall are generated by using the ToMap unsupervised normalization method. It is based on the similarities between the form of the term to normalize and the form of concept labels. Then, a projection of the space of terms towards the space of concepts is learned by globally minimizing the distances between vectors of terms and vectors of concepts. It applies multivariate linear regression using the previously generated training dataset. Finally, a distance calculation is carried out between the projections of term vectors and the concept vectors, providing a prediction of normalization by a concept for each term. This method was evaluated through the categorization task of bacterial habitats of BioNLP Shared Task 2016. Our results largely outperform all existing systems on this task, opening up very encouraging prospects.","Combining rule-based and embedding-based approaches to normalize textual entities with an ontology","Arnaud Ferré, Louise Deléger, Pierre Zweigenbaum, Claire Nédellec","1981","Combiner les approches basées sur les règles et sur l'intégration pour normaliser les entités textuelles avec une ontologie

Dans cet article, nous proposons une méthode en deux étapes pour normaliser les termes multi-mots avec les concepts d'une ontologie spécifique à un domaine. La normalisation est une étape critique de l'extraction d'informations. La méthode utilise des représentations vectorielles des termes calculées à l'aide d'informations sur l'intégration des mots et d'informations hiérarchiques sur les concepts de l'ontologie. La méthode de normalisation non supervisée ToMap permet de générer un ensemble de données de formation et un ensemble de données de premier résultat avec une précision élevée et un faible rappel. Cette méthode est basée sur les similitudes entre la forme du terme à normaliser et la forme des étiquettes de concepts. Ensuite, une projection de l'espace des termes vers l'espace des concepts est apprise en minimisant globalement les distances entre les vecteurs de termes et les vecteurs de concepts. La régression linéaire multivariée est appliquée à l'aide de l'ensemble de données d'apprentissage généré précédemment. Enfin, un calcul de distance est effectué entre les projections des vecteurs de termes et les vecteurs de concepts, ce qui permet de prédire la normalisation par un concept pour chaque terme. Cette méthode a été évaluée à travers la tâche de catégorisation des habitats bactériens de BioNLP Shared Task 2016. Nos résultats surpassent largement tous les systèmes existants sur cette tâche, ouvrant des perspectives très encourageantes.","10745","3","6","Combiner les approches basées sur les règles et sur le plongement pour normaliser des entités textuelles avec une ontologie

Dans cet article, nous proposons une méthode en deux étapes pour normaliser les termes complexes avec les concepts d&#039;une ontologie spécifique à un domaine. La normalisation est une étape critique de l&#039;extraction d&#039;informations. La méthode utilise des représentations vectorielles des termes, calculées à l&#039;aide d&#039;informations sur l&#039;intégration des mots et d&#039;informations hiérarchiques sur les concepts de l&#039;ontologie. La méthode de normalisation non supervisée ToMap permet de générer un ensemble de données de formation et un ensemble de données de premier résultat avec une précision élevée et un faible rappel. Cette méthode est basée sur les similitudes entre la forme du terme à normaliser et la forme des étiquettes de concepts. Ensuite, une projection de l&#039;espace des termes vers l&#039;espace des concepts est apprise en minimisant globalement les distances entre les vecteurs de termes et les vecteurs de concepts. La régression linéaire multivariée est appliquée à l&#039;aide de l&#039;ensemble de données d&#039;apprentissage généré précédemment. Enfin, un calcul de distance est effectué entre les projections des vecteurs de termes et les vecteurs de concepts, ce qui permet de prédire la normalisation par un concept pour chaque terme. Cette méthode a été évaluée à travers la tâche de catégorisation des habitats bactériens de BioNLP Shared Task 2016. Nos résultats surpassent largement tous les systèmes existants sur cette tâche, ouvrant des perspectives très encourageantes.","4","2023-05-30 11:33:25","2023-05-30 11:46:50","10745",,"1","2","2","2","2","2",NULL,,"2023-05-30 11:33:16","0"
"1445141","In this paper, we investigate some language acquisition facets of an auto-adaptative system that can automatically acquire most of the relevant lexical knowledge and authoring practices for an application in a given domain. This is the LELIO project: producing customized LELIE solutions. Our goal, within the framework of LELIE (a system that tags language uses that do not follow the Constrained Natural Language principles), is to automate the long, costly and error prone lexical customization of LELIE to a given application domain. Technical texts being relatively restricted in terms of syntax and lexicon, results obtained show that this approach is feasible and relatively reliable. By auto-adaptative, we mean that the system learns from a sample of the application corpus the various lexical terms and uses crucial for LELIE to work properly (e.g. verb uses, fuzzy terms, business terms, stylistic patterns). A technical writer validation method is developed at each step of the acquisition.","LELIO: an auto-adaptative system to acquire domain lexical knowledge in technical texts","Patrick Saint Dizier","1739","LELIO: un système auto-adaptatif pour acquérir des connaissances lexicales de domaine dans les textes techniques

Dans cet article, nous étudions certaines facettes de l’acquisition de langage d’un système auto-adaptatif qui peut automatiquement acquérir la plupart des connaissances lexicales pertinentes et des pratiques de création pour une application dans un domaine donné. Voici le projet LELIO: produire des solutions LELIE personnalisées. Notre objectif, dans le cadre de LELIE (un système qui balise les langages qui ne suivent pas les principes du langage naturel limité), est d’automatiser la personnalisation lexicale longue, coûteuse et sujette aux erreurs de LELIE sur un domaine d’application donné. Les textes techniques étant relativement limités en termes de syntaxe et de lexique, les résultats obtenus montrent que cette approche est réalisable et relativement fiable. Par auto-adaptatif, nous voulons dire que le système apprend à partir d’un échantillon de corpus d’applications les différents termes lexicals et utilise des termes essentiels pour que LELIE fonctionne correctement (par exemple, usages de verbes, termes flous, termes commerciaux, motifs stylistiques). Une méthode de validation technique est développée à chaque étape de l’acquisition.","1513","4","7","LELIO : un système auto-adaptatif pour acquérir des connaissances lexicales de domaine dans les textes techniques

Dans cet article, nous étudions certaines facettes de l’acquisition du langage par un système auto-adaptatif qui peut automatiquement acquérir la plupart des connaissances lexicales pertinentes et des pratiques de création pour une application dans un domaine donné. Voici le projet LELIO : produire des solutions LELIE personnalisées. Notre objectif, dans le cadre de LELIE (un système qui étiquette les utilisations du langage qui ne suivent pas les principes du langage naturel limité), est d’automatiser la personnalisation lexicale longue, coûteuse et sujette aux erreurs de LELIE sur un domaine d’application donné. Les textes techniques étant relativement limités en termes de syntaxe et de lexique, les résultats obtenus montrent que cette approche est réalisable et relativement fiable. Par auto-adaptatif, nous voulons dire que le système apprend à partir d’un échantillon de corpus d’applications les différents termes lexicaux et utilise des termes essentiels pour que LELIE fonctionne correctement (par exemple, utilisation des verbes, termes flous, termes commerciaux, motifs stylistiques). Une méthode de validation technique est développée à chaque étape de l’acquisition.","4","2023-05-30 11:47:20","2023-05-30 12:00:01","1513",,"1","1","2","1","2","1",NULL,,"2023-05-30 11:46:50","0"
"2144675","Boolean networks (BNs) are widely used to model the qualitative dynamics of biological systems. Besides the logical rules determining the evolution of each component with respect to the state of its regulators, the scheduling of component updates can have a dramatic impact on the predicted behaviours. In this paper, we explore the use of Read (contextual) Petri Nets (RPNs) to study dynamics of BNs from a concurrency theory perspective. After showing bi-directional translations between RPNs and BNs and analogies between results on synchronism sensitivity, we illustrate that usual updating modes for BNs can miss plausible behaviours, i.e., incorrectly conclude on the absence/impossibility of reaching specific configurations. We propose an encoding of BNs capitalizing on the RPN semantics enabling more behaviour than the generalized asynchronous updating mode. The proposed encoding ensures a correct abstraction of any multivalued refinement, as one may expect to achieve when modelling biological systems with no assumption on its time features.","Concurrency in Boolean networks","Thomas Chatain, Stefan Haar, Juraj Kolčák, Loïc Paulevé, Aalok Thakkar","6657","Concurrence dans les réseaux booléens

Les réseaux booléens (BN) sont largement utilisés pour modéliser la dynamique qualitative des systèmes biologiques. Outre les règles logiques déterminant l'évolution de chaque composant par rapport à l'état de ses régulateurs, la programmation des mises à jour des composants peut avoir un impact dramatique sur les comportements prédits. Dans cet article, nous explorons l'utilisation des réseaux de Pétri (RPN) Read (contextuel) pour étudier la dynamique des BN dans une perspective de théorie de concurrence. Après avoir montré des traductions bidirectionnelles entre RPN et BNs et des analogies entre les résultats sur la sensibilité au synchronisme, nous illustrons que les modes de mise à jour habituels pour BNs peuvent manquer des comportements plausibles, c'est-à-dire, conclure à tort sur l'absence/impossibilité d'atteindre des configurations spécifiques. Nous proposons un codage de BN capitalisant sur la sémantique RPN permettant plus de comportement que le mode de mise à jour asynchrone généralisée. Le codage proposé assure une abstraction correcte de tout raffinement à valeurs multiples, comme on peut s'attendre à le faire lors de la modélisation de systèmes biologiques sans hypothèse sur ses caractéristiques temporelles.","14866","6","9","Concurrence dans les réseaux booléens

Les réseaux booléens (RB) sont largement utilisés pour modéliser la dynamique qualitative des systèmes biologiques. Outre les règles logiques déterminant l&#039;évolution de chaque composant par rapport à l&#039;état de ses régulateurs, la programmation des mises à jour des composants peut avoir un impact dramatique sur les comportements prédits. Dans cet article, nous explorons l&#039;utilisation des réseaux de Pétri Read (RPN) pour étudier la dynamique des RB dans une perspective de théorie de concurrence. Après avoir montré des traductions bidirectionnelles entre RPN et RB et des analogies entre les résultats sur la sensibilité au synchronisme, nous illustrons que les modes de mise à jour habituels pour les RB peuvent manquer des comportements plausibles, c&#039;est-à-dire, conclure à tort sur l&#039;absence ou l&#039;impossibilité d&#039;atteindre des configurations spécifiques. Nous proposons un codage de RB capitalisant sur la sémantique RPN permettant plus de comportement que le mode de mise à jour asynchrone généralisée. Le codage proposé assure une abstraction correcte de tout raffinement à valeurs multiples, comme on peut s&#039;attendre à le faire lors de la modélisation de systèmes biologiques sans hypothèse sur ses caractéristiques temporelles.","4","2023-05-30 12:00:15","2023-05-30 12:13:37","14866",,"2","2","3","3","2","3",NULL,,"2023-05-30 12:00:01","0"
"3754898","Adverse drug reactions (ADRs) are statistically characterized within randomized clinical trials or by postmarketing pharmacovigilance. However, the molecular mechanisms causing ADRs remain unknown in most cases. This is true even for common toxicities that are classically monitored during trials such as hepatic or skin toxicities. Interestingly, many elements of knowledge about drugs and drug ingredients are available beside clinical trials. In particular, open-access knowledge graphs describe their properties, interactions, and involvements in pathways. Expert classifications have also been manually established by experts and label drugs either as causative or not for several types of ADRs. In our paper, we propose to mine biomedical knowledge graphs to identify biomolecular features that enable to automatically reproduce such expert classifications, distinguishing drugs causative or not for a given type of ADR. In an Explainable AI perspective, we explore simple classification techniques such as Decision Trees and Classification Rules because they provide human-readable models which explain the classification itself. We also evaluate the assumption that biomolecular features mined from knowledge graphs might provide elements of explanation for the molecular mechanisms behind ADRs.","Investigating ADR mechanisms with Explainable AI: a feasibility study with knowledge graph mining","Emmanuel Bresso, Pierre Monnin, Cédric Bousquet, François-Élie Calvier, Ndeye Coumba Ndiaye, Nadine Petitpain, Malika Smaïl-Tabbone, Adrien Coulet","4424","Enquêter sur les mécanismes de RED avec l'IA explicable : une étude de faisabilité avec une exploration graphique des connaissances

Les effets indésirables des médicaments (EIM) sont statistiquement caractérisés dans le cadre d'essais cliniques randomisés ou par pharmacovigilance après commercialisation. Cependant, les mécanismes moléculaires à l'origine des EIM demeurent inconnus dans la plupart des cas. Cela est vrai même pour les toxicités courantes qui sont surveillées de façon classique au cours d'essais, comme les toxicités hépatiques ou cutanées. Fait intéressant, de nombreux éléments de connaissance sur les médicaments et les ingrédients médicamenteux sont disponibles en dehors des essais cliniques. En particulier, les graphiques de connaissances en accès libre décrivent leurs propriétés, leurs interactions et leurs implications dans les voies d'accès. Des experts ont également établi manuellement des classifications d'experts et étiqueté les médicaments comme causals ou non pour plusieurs types d'EIM. Dans notre article, nous proposons d'explorer les graphiques de connaissances biomédicales pour identifier les caractéristiques biomoléculaires qui permettent de reproduire automatiquement ces classifications d'experts, en distinguant les médicaments causals ou non pour un type donné d'EIM. Dans une perspective d'IA explicable, nous explorons des techniques de classification simples telles que les arbres de décision et les règles de classification, car elles fournissent des modèles lisibles par l'homme qui expliquent la classification elle-même. Nous évaluons également l'hypothèse selon laquelle les caractéristiques biomoléculaires extraites des graphiques des connaissances pourraient fournir des éléments d'explication pour les mécanismes moléculaires derrière les EIM.","16340","6","30","Enquête sur les mécanismes d&#039;EIM avec une IA explicable : une étude de faisabilité avec une exploration graphique des connaissances

Les effets indésirables des médicaments (EIM) sont statistiquement caractérisés dans le cadre d&#039;essais cliniques randomisés ou par pharmacovigilance après commercialisation. Cependant, les mécanismes moléculaires à l&#039;origine des EIM demeurent inconnus dans la plupart des cas. Cela est vrai même pour les toxicités courantes qui sont surveillées de façon classique au cours d&#039;essais, comme les hépatotoxicités ou toxicités cutanées. Fait intéressant, de nombreux éléments de connaissance sur les médicaments et les ingrédients médicamenteux sont disponibles en dehors des essais cliniques. En particulier, les graphiques de connaissances en accès libre décrivent leurs propriétés, leurs interactions et leurs implications dans les voies d&#039;accès. Des experts ont également établi manuellement des classifications d&#039;experts et étiqueté les médicaments comme causals ou non pour plusieurs types d&#039;EIM. Dans notre article, nous proposons d&#039;explorer les graphiques de connaissances biomédicales pour identifier les caractéristiques biomoléculaires qui permettent de reproduire automatiquement ces classifications d&#039;experts, en distinguant les médicaments causals ou non pour un type donné d&#039;EIM. Dans la perspective d&#039;une IA explicable, nous explorons des techniques de classification simples telles que les arbres de décision et les règles de classification, car elles fournissent des modèles lisibles par l&#039;homme qui expliquent la classification elle-même. Nous évaluons également l&#039;hypothèse selon laquelle les caractéristiques biomoléculaires extraites des graphiques de connaissances pourraient fournir des éléments d&#039;explication pour les mécanismes moléculaires derrière les EIM.","4","2023-05-31 22:37:47","2023-05-31 22:48:07","16340",,"1","1","2","1","1","2",NULL,,"2023-05-31 10:33:35","0"
"1276045","Organisations that operate in a global environment can be subject to potentially diverse and complex regulatory requirements. This paper explains some of the key issues that corporate governance faces related to privacy and some mechanisms for addressing these.","Privacy Management and Accountability in Global Organisations","Siani Pearson","2373","Gestion de la confidentialité et responsabilité dans les organisations internationales

Les organisations qui opèrent dans un environnement global peuvent être soumises à des exigences réglementaires potentiellement diverses et complexes. Le présent document explique certains des principaux problèmes auxquels la gouvernance d'entreprise fait face en matière de protection de la vie privée et certains mécanismes pour y remédier.","7386","6","31","Gestion de la confidentialité et de la responsabilité dans les organisations mondiales

Les organisations qui opèrent dans un environnement mondialisé peuvent être soumises à des exigences réglementaires potentiellement diverses et complexes. Le présent document explique certains des principaux problèmes auxquels la gouvernance d&#039;entreprise fait face en matière de protection de la vie privée et certains mécanismes pour y remédier.","4","2023-05-31 22:51:43","2023-05-31 22:54:51","7386",,"1","1","2","1","1","2",NULL,,"2023-05-31 10:48:07","0"
"3855924","With around 300 millions people worldwide suffering from depression, the detection of this disorder is crucial and a challenge for individual and public health. As with many diseases, early detection means better medical management; the use of social media messages as potential clues to depression is an opportunity to assist in this early detection by automatic means. This chapter is based on the participation of the CNRS IRIT laboratory in the early detection of depressive people (eRisk) task at the CLEF evaluation forum. Early depression detection differs from depression detection in that it considers temporality; the system must make its decision about a user’s possible depression with as little data as possible. In this chapter we re-evaluate the models we have developed for our participation at eRisk over the years on the different collections, to obtain a more robust comparison. We also add new models. We use well-established classification methods, such as Logistic regression, Random forest, and Support Vector Machine (SVM). The users’ data from which the system should detect if they are depressed, are represented as vectors composed of (a) various task-oriented features including depression related lexicons and (b) word and document embeddings, extracted from the users’ posts. We perform an ablation study to analyze the most important features for our models. We also use BERT deep learning architecture for comparison purposes, both for depression detection and early depression detection. According to our results, well-established machine learning models are still better than more modern models for -early- detection of depression.","Comparison of machine learning models for early depression detection from users’ posts","Josiane Mothe, Faneva Ramiandrisoa, Md Zia Ullah","4132","Comparaison de modèles d'apprentissage automatique pour la détection précoce de dépression à partir des publications des utilisateurs

Avec environ 300 millions de personnes dans le monde souffrant de dépression, la détection de ce trouble est cruciale et un défi pour la santé individuelle et publique. Comme pour de nombreuses maladies, la détection précoce signifie une meilleure prise en charge médicale ; l'utilisation des messages des médias sociaux comme indices potentiels de dépression est une occasion d'aider à cette détection précoce par des moyens automatiques. Ce chapitre est basé sur la participation du laboratoire IRIT du CNRS à la tâche de détection précoce des personnes dépressives (eRisk) au forum d'évaluation de la CLEF. La détection précoce de la dépression diffère de la détection de la dépression en ce qu'elle considère la temporalité ; le système doit prendre sa décision sur l’éventuelle dépression d’un utilisateur avec le moins de données possible. Dans ce chapitre, nous réévaluons les modèles que nous avons développés pour notre participation à eRisk au fil des ans sur les différentes collections, pour obtenir une comparaison plus robuste. Nous ajoutons également de nouveaux modèles. Nous utilisons des méthodes de classification bien établies, telles que la régression logistique, la forêt aléatoire et la machine à vecteurs de support (SVM). Les données des utilisateurs, à partir desquelles le système devrait détecter si elles sont déprimées, sont représentées sous forme de vecteurs composés (a) de diverses caractéristiques axées sur les tâches, y compris des lexiques liés à la dépression et (b) d’intégrations de mots et de documents, extraites des publications des utilisateurs. Nous effectuons une étude d'ablation pour analyser les caractéristiques les plus importantes pour nos modèles. Nous utilisons également l'architecture d'apprentissage profond BERT à des fins de comparaison, à la fois pour la détection de dépression et la détection précoce de dépression. Selon nos résultats, les modèles d'apprentissage automatique bien établis sont encore meilleurs que les modèles plus modernes pour la détection précoce de la dépression.","16048","6","33","Comparaison de modèles d&#039;apprentissage automatique pour le dépistage précoce de dépression à partir des publications des utilisateurs

Environ 300 millions de personnes dans le monde souffrent de dépression, ce qui fait de la détection de ce trouble un défi crucial pour la santé individuelle et publique. Comme pour de nombreuses maladies, le dépistage précoce implique une meilleure prise en charge médicale et l&#039;utilisation des messages des médias sociaux comme indices potentiels de dépression est une occasion d&#039;aider à ce dépistage précoce par des moyens automatiques. Ce chapitre est basé sur la participation du laboratoire IRIT du CNRS à la tâche de dépistage précoce des personnes dépressives (eRisk) au forum d&#039;évaluation de la CLEF. Le dépistage précoce de la dépression se distingue du simple dépistage de la dépression, parce que la temporalité est importante. Le système doit prendre sa décision sur l’éventuelle dépression d’un utilisateur avec le moins de données possible. Dans ce chapitre, nous réévaluons les modèles que nous avons développés pour notre participation à eRisk au fil des ans sur les différentes collections, pour obtenir une comparaison plus fiable. Nous ajoutons également de nouveaux modèles. Nous utilisons des méthodes de classification bien établies, telles que la régression logistique, la forêt aléatoire et la machine à vecteurs de support (SVM). Les données des utilisateurs, à partir desquelles le système devrait détecter si ces derniers sont dépressifs, sont représentées sous forme de vecteurs composés (a) de diverses caractéristiques axées sur les tâches, y compris des lexiques liés à la dépression et (b) d’intégrations de mots et de documents, extraites des publications des utilisateurs. Nous effectuons une étude par ablation pour analyser les caractéristiques les plus importantes pour nos modèles. Nous utilisons également l&#039;architecture d&#039;apprentissage profond BERT à des fins de comparaison, à la fois pour le dépistage de la dépression et le dépistage précoce de la dépression. Selon nos résultats, les modèles d&#039;apprentissage automatique bien établis sont encore meilleurs que les modèles plus modernes pour le dépistage précoce de la dépression.","4","2023-05-31 23:50:40","2023-05-31 23:57:30","16048",,"2","2","3","3","2","1",NULL,"Néologisme : &quot;ablation&quot;. Après consultation d&#039;un expert en IA, il semble que le terme n&#039;ait pas encore de traduction qui fasse consensus, mais que les experts emploient plutôt le terme anglais. Sachant ce dont il est question et l&#039;origine du terme anglais (emprunt au domaine de la biologie), nous avons choisi de calquer l&#039;anglais.","2023-05-31 11:50:20","0"
"3810198","Benchmarking initiatives support the meaningful comparison of competing solutions to prominent problems in speech and language processing. Successive benchmarking evaluations typically reflect a progressive evolution from ideal lab conditions towards to those encountered in the wild. ASVspoof, the spoofing and deepfake detection initiative and challenge series, has followed the same trend. This article provides a summary of the ASVspoof 2021 challenge and the results of 37 participating teams. For the logical access task, results indicate that countermeasures solutions are robust to newly introduced encoding and transmission effects. Results for the physical access task indicate the potential to detect replay attacks in real, as opposed to simulated physical spaces, but a lack of robustness to variations between simulated and real acoustic environments. The DF task, new to the 2021 edition, targets solutions to the detection of manipulated, compressed speech data posted online. While detection solutions offer some resilience to compression effects, they lack generalization across different source datasets. In addition to a summary of the top-performing systems for each task, new analyses of influential data factors and results for hidden data subsets, the article includes a review of post-challenge results, an outline of the principal challenge limitations and a road-map for the future of ASVspoof. Link to the ASVspoof challenge and related resources: https://www.asvspoof.org/index2021.html","ASVspoof 2021: Towards Spoofed and Deepfake Speech Detection in the Wild","Xuechen Liu, Xin Wang, Md Sahidullah, Jose Patino, Héctor Delgado, Tomi Kinnunen, Massimiliano Todisco, Junichi Yamagishi, Nicholas Evans, Andreas Nautsch, Kong Aik Lee","4266","ASVspoof 2021: Vers la détection d’éponges et de faux discours dans la nature

Les initiatives d’analyse comparative appuient la comparaison significative des solutions concurrentes à des problèmes importants dans le traitement de la parole et du langage. Les évaluations successives d’analyses comparatives reflètent généralement une évolution progressive des conditions de laboratoire idéales vers celles rencontrées dans la nature. ASVspoof, l’initiative d’usurpation et de détection des contrefaçons profondes et la série de défis, a suivi la même tendance. Cet article résume le défi ASVspoof 2021 et les résultats de 37 équipes participantes. Pour la tâche d’accès logique, les résultats indiquent que les solutions de contre-mesures sont robustes aux effets de codage et de transmission nouvellement introduits. Les résultats de la tâche d’accès physique indiquent le potentiel de détecter les attaques de replay en réel, par opposition aux espaces physiques simulés, mais un manque de robustesse aux variations entre les environnements acoustiques simulés et réels. La tâche DF, nouvelle dans l’édition 2021, vise des solutions à la détection des données vocales manipulées et compressées mises en ligne. Bien que les solutions de détection offrent une certaine résilience aux effets de compression, elles manquent de généralisation dans différents ensembles de données sources. En plus d’un résumé des systèmes les plus performants pour chaque tâche, de nouvelles analyses des facteurs de données influents et des résultats pour les sous-ensembles de données cachés, l’article comprend un examen des résultats post-défi, un aperçu des principales limites des défis et une feuille de route pour l’avenir de ASVspoof. Lien vers le défi ASVspoof et les ressources connexes: https://www.asvspoof.org/index2021.html","13224","4","63","ASVspoof 2021 : Vers la détection des discours falsifiés et hypertruqués dans la nature

Les initiatives d&#039;analyse comparative soutiennent la comparaison significative de solutions concurrentes à des problèmes importants dans le traitement de la parole et de la langue. Les évaluations comparatives successives reflètent généralement une évolution progressive des conditions de laboratoire idéales vers celles rencontrées dans la nature. ASVspoof, initiative et série de défis de détection d&#039;usurpation d&#039;identité et d&#039;hypertrucages (ou deepfakes), a suivi la même tendance. Cet article présente un résumé du défi ASVspoof 2021 et les résultats de 37 équipes participantes. Pour la tâche d&#039;accès logique, les résultats indiquent que les solutions de contre-mesures résistent aux effets de codage et de transmission nouvellement introduits. Les résultats de la tâche d&#039;accès physique indiquent la possibilité de détecter des attaques de réexécution dans des espaces réels, par opposition à des espaces physiques simulés, mais un manque de résistance aux variations entre les environnements acoustiques simulés et réels. La tâche DF, nouvelle édition 2021, cible des solutions pour la détection de données vocales manipulées et compressées postées en ligne. Bien que les solutions de détection offrent une certaine résilience aux effets de compression, elles ne sont pas généralisées entre différents ensembles de données sources. En plus d&#039;un résumé des systèmes les plus performants pour chaque tâche, de nouvelles analyses des facteurs de données influents et des résultats pour les sous-ensembles de données cachées, l&#039;article comprend un examen des résultats postérieurs au défi, un aperçu des principales limitations du défi et une feuille de route pour l&#039;avenir de ASVspoof. Lien vers le défi ASVspoof et les ressources connexes : https://www.asvspoof.org/index2021.html","4","2023-06-06 12:16:03","2023-06-06 12:16:33","13224",,"3","2","3","4","2","4",NULL,"problèmes terminologiques avec la traduction de &quot;deepfake&quot; notamment, manque de cohérence au contenu d&#039;origine","2023-06-06 12:14:44","0"
"1629154","We present our two separate tools for data protection measurement and evaluation of websites. The first tool does a generic check on a single website and is openly available for any web user to use when evaluating data protection measures implemented on a website. The second tool was used to perform a more exhaustive evaluation of Swedish municipalities. The work focuses on leakages of personally identifiable information to third parties when a web visitor goes to a website, and in our accompanying website we have also identified measures that web developers could undertake, or that web visitors could request, to improve the data protection of their visitors.","Evaluating Websites and Their Adherence to Data Protection Principles: Tools and Experiences","Amelia Andersdotter, Anders Jensen-Urstad","3167","Évaluation des sites Web et de leur adhésion aux principes de protection des données: Outils et expériences

Nous présentons nos deux outils distincts pour la mesure de la protection des données et l’évaluation des sites Web. Le premier outil effectue une vérification générique sur un seul site Web et est ouvertement disponible pour tout utilisateur du Web à utiliser lors de l’évaluation des mesures de protection des données mises en œuvre sur un site Web. Le deuxième outil a été utilisé pour réaliser une évaluation plus exhaustive des municipalités suédoises. Le travail se concentre sur les fuites d’informations personnellement identifiables à des tiers lorsqu’un visiteur se rend sur un site Web, et dans notre site Web d’accompagnement, nous avons également identifié les mesures que les développeurs Web pourraient entreprendre, ou que les visiteurs pourraient demander, pour améliorer la protection des données de leurs visiteurs.","1948","4","64","Évaluation des sites Web et de leur respect des principes de protection des données : outils et expériences

Nous présentons nos deux outils distincts pour la mesure de la protection des données et l&#039;évaluation des sites Web. Le premier outil effectue un contrôle générique sur un site unique et est ouvert à tout internaute pour l&#039;évaluation des mesures de protection des données mises en œuvre sur un site. Le deuxième outil a été utilisé pour réaliser une évaluation plus exhaustive des municipalités suédoises. Le travail se concentre sur les fuites d&#039;informations personnellement identifiables à des tiers quand un utilisateur accède à un site Web, et dans notre site Web d&#039;accompagnement, nous avons également identifié des mesures que les développeurs pourraient entreprendre, ou que les visiteurs du Web pourraient demander pour améliorer la protection des données de leurs visiteurs.","4","2023-06-06 12:16:59","2023-06-06 12:17:24","1948",,"1","2","2","1","2","1",NULL,,"2023-06-06 12:16:35","0"
"1883632","The increased popularity of interconnected devices, which we rely on when performing day-to-day activities expose people to various privacy harms. This paper presents findings from the empirical investigation of privacy concerns. The study revealed that people, regardless of their diversity, perceive privacy harms as generic and simplified models, not individually as suggested in Solove’s framework. Additionally, the results identified differences in privacy concerns related to information disclosure, protection behavior, and demographics. The findings may benefit privacy and system designers, ensuring that policies and digital systems match people’s privacy expectations, decreasing risks and harms.","Is It Harmful? Re-examining Privacy Concerns","Agnieszka Kitkowska, Erik Wästlund, Joachim Meyer, Leonardo Martucci","3501","Est-Ce Nocif ? Réexaminer les préoccupations en matière de vie privée

La popularité accrue des appareils interconnectés, sur lesquels nous comptons dans l'exécution des activités quotidiennes exposent les gens à divers dommages à la vie privée. Le présent document présente les conclusions de l'enquête empirique sur les préoccupations en matière de protection des renseignements personnels. L’étude a révélé que les gens, quelle que soit leur diversité, perçoivent les atteintes à la vie privée comme des modèles génériques et simplifiés, et non individuellement comme le suggère le cadre de Solove. De plus, les résultats ont révélé des différences dans les préoccupations en matière de protection de la vie privée liées à la divulgation de renseignements, au comportement de protection et aux données démographiques. Les conclusions peuvent être bénéfiques pour la protection de la vie privée et les concepteurs de systèmes, en veillant à ce que les politiques et les systèmes numériques correspondent aux attentes des gens en matière de protection de la vie privée, réduisant ainsi les risques et les préjudices.","6255","6","65","Est-ce nocif ? Réexaminer les préoccupations en matière de confidentialité

La popularité accrue des appareils interconnectés sur lesquels nous comptons pour exécuter nos activités quotidiennes exposent les gens à diverses atteintes à la vie privée. Le présent document présente les conclusions de l&#039;enquête empirique sur les préoccupations en matière de protection des données personnelles. L’étude a révélé que les gens, quelle que soit leur diversité, perçoivent les atteintes à la vie privée comme des modèles génériques et simplifiés, et non individuellement comme le suggèrent les travaux de Solove. De plus, les résultats ont révélé des différences dans les préoccupations en matière de protection de la vie privée liées à la divulgation de renseignements, au comportement de protection et aux données démographiques. Les conclusions peuvent être bénéfiques pour la protection de la vie privée et les concepteurs de systèmes, en veillant à ce que les politiques et les systèmes numériques correspondent aux attentes des gens en matière de protection de la vie privée, réduisant ainsi les risques et les préjudices.","4","2023-06-06 12:17:35","2023-06-06 12:58:49","6255",,"2","2","3","2","1","1",NULL,"https://swissprivacy.law/108/ : traduction de &quot;privacy&quot; problématique. Nous avons choisi de traduire par &quot;confidentialité&quot; et &quot;vie privée&quot;, selon les contextes","2023-06-06 12:17:25","0"
"1386311","We aim at showing that lexical descriptions based on multifactorial and continuous models can be used by linguists and lexicographers (and not only by machines) so long as they are provided with a way to efficiently navigate data collections. We propose to demonstrate such a system.","Exploring a Continuous and Flexible Representation of the Lexicon","Pierre Marchal, Thierry Poibeau","1680","Explorer une représentation continue et flexible du lexique

Nous voulons montrer que les descriptions lexicales basées sur des modèles multifactoriels et continus peuvent être utilisées par les linguistes et les lexicographes (et pas seulement par les machines) tant qu'elles sont fournies avec un moyen efficace de naviguer dans les collections de données. Nous proposons de démontrer un tel système.","4930","6","66","Explorer une représentation continue et flexible du lexique

Nous voulons montrer que les descriptions lexicales basées sur des modèles multifactoriels et continus peuvent être utilisées par les linguistes et les lexicographes (et pas seulement par les machines) tant qu&#039;elles sont fournies avec un moyen efficace de naviguer dans les collections de données. Nous proposons de démontrer un tel système.","4","2023-06-06 12:58:57","2023-06-06 13:07:06","4930",,"1","1","1","1","1","1",NULL,,"2023-06-06 12:58:50","0"
"3613392","This article describes the participation of ERTIM (INALCO) to the French NER task of CLEF HIPE 2020 lab with the mXS system, a combination of pattern mining and machine learning, implemented in 2010-2013. Due to multiple reasons, almost no upgrades or improvements were achieved since then, only a minimal linking module and some lexical entries were added. No training and almost no adaptation were implemented for this lab. Results on historical data show severe degradations, in particular concerning the recognition of organisations.","Adapting a Pre-Neural Named Entity Recognizer and Linker to Historical Data","Damien Nouvel, Jean-Claude Zagabe Seruti","4801","Adaptation d'un reconnaisseur pré-neuronal d'entités nommées et d'un éditeur de liens à des données historiques

Cet article décrit la participation d'ERTIM (INALCO) à la tâche NER française du laboratoire CLEF HIPE 2020 avec le système mXS, une combinaison de fouille de formes et d'apprentissage automatique, mis en œuvre en 2010-2013. Pour de multiples raisons, presque aucune mise à jour ou amélioration n'a été réalisée depuis lors, seuls un module de liaison minimal et quelques entrées lexicales ont été ajoutés. Aucune formation et presque aucune adaptation n'ont été mises en œuvre pour ce laboratoire. Les résultats sur les données historiques montrent des dégradations sévères, en particulier en ce qui concerne la reconnaissance des organisations.","19271","3","67","Adaptation d&#039;un reconnaisseur pré-neuronal d&#039;entités nommées et d&#039;un éditeur de liens à des données historiques

Cet article décrit la participation d&#039;ERTIM (INALCO) à la tâche NER française du laboratoire CLEF HIPE 2020 avec le système mXS, qui combine la fouille de motifs et l&#039;apprentissage automatique, mis en œuvre entre 2010 et 2013. Pour de multiples raisons, presque aucune mise à jour ou amélioration n&#039;a été réalisée depuis lors ; seuls un module de liaison minimal et quelques entrées lexicales ont été ajoutés. Aucune formation et presque aucune adaptation n&#039;ont été mises en œuvre pour ce laboratoire. Les résultats sur les données historiques présentent des dégradations sévères, en particulier en ce qui concerne la reconnaissance des organisations.","4","2023-06-06 13:07:15","2023-06-06 13:15:51","19271",,"1","1","2","2","1","1",NULL,,"2023-06-06 01:07:07","0"
"1653512","Involving stakeholders in enterprise modeling, besides rendering valid models, also helps stakeholders articulate and align their views on their organization. This requires that stakeholders are able to understand and actively perform conceptual modeling for representing their views on enterprise structure and behavior. The specific skills required for this should not be taken for granted and need to be developed explicitly. Scaffolding is an educational concept that allows to embed learning support mechanisms in operative modeling processes. The present article introduces a framework that makes it possible to view scaffolding as an integral part of stakeholder-centric modeling activities. The framework is validated with respect to its descriptive and discriminatory power by an ex-post analysis of the design and application of an existing modeling method.","Scaffolding Stakeholder-Centric Enterprise Model Articulation","Stefan Oppl, Stijn Hoppenbrouwers","3235","Articulation De Modèle D'Entreprise Centré Sur Les Parties Prenantes

Associer les parties prenantes à la modélisation d'entreprise, en plus de rendre des modèles valides, aide également les parties prenantes à articuler et aligner leurs points de vue sur leur organisation. Cela exige que les parties prenantes soient en mesure de comprendre et d'effectuer activement la modélisation conceptuelle pour représenter leurs vues sur la structure et le comportement de l'entreprise. Les compétences spécifiques requises à cet effet ne devraient pas être considérées comme acquises et doivent être développées explicitement. L'échafaudage est un concept éducatif qui permet d'intégrer des mécanismes d'aide à l'apprentissage dans des processus de modélisation opérationnels. Le présent article introduit un cadre qui permet de considérer l'échafaudage comme faisant partie intégrante des activités de modélisation centrées sur les parties prenantes. Le cadre est validé quant à son pouvoir descriptif et discriminatoire par une analyse ex post de la conception et de l'application d'une méthode de modélisation existante.","5989","6","72","Articulation de modèle d&#039;entreprise en échafaudage centré sur les parties prenantes

Associer les parties prenantes à la modélisation d&#039;entreprise, en plus de rendre des modèles valides, aide également les parties prenantes à articuler et aligner leurs points de vue sur leur organisation. Cela exige que les parties prenantes soient en mesure de comprendre et d&#039;effectuer activement la modélisation conceptuelle pour représenter leurs vues sur la structure et le comportement de l&#039;entreprise. Les compétences spécifiques requises à cet effet ne devraient pas être considérées comme acquises et doivent être développées explicitement. L&#039;échafaudage est un concept éducatif qui permet d&#039;intégrer des mécanismes d&#039;aide à l&#039;apprentissage dans des processus de modélisation opérationnels. Le présent article introduit un cadre qui permet de considérer l&#039;échafaudage comme faisant partie intégrante des activités de modélisation centrées sur les parties prenantes. Le cadre est validé quant à son pouvoir descriptif et discriminatoire par une analyse ex-post de la conception et de l&#039;application d&#039;une méthode de modélisation existante.","4","2023-06-06 17:02:48","2023-06-06 17:34:52","5989",,"3","2","1","1","2","1",NULL,,"2023-06-06 05:00:26","0"
"2774325","Transcribing structured data into natural language descriptions has emerged as a challenging task, referred to as “data-to-text”. These structures generally regroup multiple elements, as well as their attributes. Most attempts rely on translation encoder-decoder methods which linearize elements into a sequence. This however loses most of the structure contained in the data. In this work, we propose to overpass this limitation with a hierarchical model that encodes the data-structure at the element-level and the structure level. Evaluations on RotoWire show the effectiveness of our model w.r.t. qualitative and quantitative metrics.","A Hierarchical Model for Data-to-Text Generation","Clément Rebuffel, Laure Soulier, Geoffrey Scoutheeten, Patrick Gallinari","6490","Modèle hiérarchique pour la génération de données en texte

Transcrire des données structurées en descriptions en langage naturel est apparu comme une tâche difficile, appelée « données-texte ». Ces structures regroupent généralement plusieurs éléments, ainsi que leurs attributs. La plupart des tentatives reposent sur des méthodes de codage-décodage de traduction qui linéarisent les éléments en une séquence. Ceci perd cependant la majeure partie de la structure contenue dans les données. Dans ce travail, nous proposons de dépasser cette limitation avec un modèle hiérarchique qui code la structure de données au niveau élément et au niveau structure. Les évaluations sur RotoWire montrent l'efficacité de notre modèle w.r.t. mesures qualitatives et quantitatives.","14699","6","77","Modèle hiérarchique pour la génération de texte à partir de données

Transcrire des données structurées en descriptions en langage naturel est apparu comme une tâche difficile, appelée « data-to-text ». Ces structures regroupent généralement plusieurs éléments, ainsi que leurs attributs. La plupart des tentatives reposent sur des méthodes de codage-décodage de traduction qui linéarisent les éléments en une séquence. Cependant, la majeure partie de la structure contenue dans les données se perd. Dans ce travail, nous proposons de dépasser cette limitation avec un modèle hiérarchique qui code la structure de données au niveau de l&#039;élément et au niveau de la structure. Les évaluations sur RotoWire montrent l&#039;efficacité des mesures qualitatives et quantitatives de notre modèle w.r.t.","4","2023-06-06 18:03:31","2023-06-06 18:38:59","14699",,"3","3","3","3","2","3",NULL,,"2023-06-06 05:34:54","0"
"1523700","The Windows Registry often contains key data that help determine the activities performed on a computer. While some forensic tools format Registry data for common questions that are required to be answered in digital investigations, their output is geared for standalone use, not for indexable content.This paper describes RegXML, an XML syntax designed to represent Windows Registry hive files. RegXML captures the logical structure of a hive and notes the locations of found data within hive files. The paper also describes a Python library designed to be used with RegXML and the results obtained upon applying the library to analyze two forensic corpora. Experimental results are presented based on hundreds of disk images, thousands of hive files and tens of millions of Registry cells.","XML Conversion of the Windows Registry for Forensic Processing and Distribution","Alex Nelson","2823","Conversion en XML du registre Windows pour le traitement et la distribution de données médico-légales

Le registre de Windows contient souvent des données clés qui aident à déterminer les activités effectuées sur un ordinateur. Bien que certains outils de criminalistique formatent les données du registre en fonction des questions courantes auxquelles il faut répondre dans les enquêtes numériques, leurs résultats sont destinés à une utilisation autonome, et non à un contenu indexable. RegXML capture la structure logique d'un répertoire de stockage et note l'emplacement des données trouvées dans les fichiers du répertoire de stockage. L'article décrit également une bibliothèque Python conçue pour être utilisée avec RegXML et les résultats obtenus lors de l'application de la bibliothèque à l'analyse de deux corpus médico-légaux. Les résultats expérimentaux sont présentés sur la base de centaines d'images de disques, de milliers de fichiers de la ruche et de dizaines de millions de cellules du registre.","9103","3","88","Conversion en XML du Registre Windows pour le traitement et la distribution de données médico-légales

Le Registre Windows contient souvent des données clés qui aident à déterminer les activités effectuées sur un ordinateur. Bien que certains outils de criminalistique formatent les données du registre en fonction des questions courantes auxquelles il faut répondre dans les enquêtes numériques, leurs résultats sont destinés à une utilisation autonome, et non à un contenu indexable. RegXML collecte la structure logique d&#039;un répertoire de stockage et note l&#039;emplacement des données trouvées dans les fichiers du répertoire de stockage. L&#039;article décrit également une bibliothèque Python conçue pour être utilisée avec RegXML et les résultats obtenus lors de l&#039;application de la bibliothèque à l&#039;analyse de deux corpus médico-légaux. Les résultats expérimentaux sont présentés sur la base de centaines d&#039;images de disques, de milliers de fichiers hive et de dizaines de millions de cellules du registre.","4","2023-06-09 10:29:56","2023-06-09 11:17:09","9103",,"2","1","3","1","1","2",NULL,"https://learn.microsoft.com/fr-fr/windows/win32/sysinfo/registry-hives","2023-06-09 10:29:40","0"
"1463380","In this paper, we describe a design science framework for the use of interactive, sensor-intensive prototypes to develop interactive greenhouse climate management systems. We emphasize the ways in which design science, and in particular what we call micro information systems, may enhance the human-computer interaction (HCI) aspects of emission control performance through better interactive control interfaces and the utilization of sensor network technology. By applying guidelines suggested in design science to the case studied, we identify a number of interactive prototypes that successively address core issues in this particular setting. Starting from a simple human-computer interaction with a one-sensor-one-output prototype made in Lego Mindstorms NXT, we end up with a custom made sensor that addresses interaction with our particular user profile: the gardener. Thus, we provide a reference platform for combining micro information systems and human-computer interaction in design science research into environmental sustainability research.","A Design Science Approach to Interactive Greenhouse Climate Control Using Lego Mindstorms for Sensor-Intensive Prototyping","Rasmus Pedersen, Torkil Clemmensen","2682","Une approche de la science de la conception pour le contrôle interactif du climat de la serre en utilisant Lego Mindstorms pour un prototypage intensif de capteurs

Dans cet article, nous décrivons un cadre de science de la conception pour l'utilisation de prototypes interactifs à forte intensité de capteurs afin de développer des systèmes interactifs de gestion du climat des serres. Nous mettons l'accent sur la façon dont la science de la conception, et en particulier ce que nous appelons les microsystèmes d'information, peut améliorer les aspects de l'interaction homme-machine (IHM) de la performance du contrôle des émissions grâce à de meilleures interfaces de contrôle interactives et à l'utilisation de la technologie des réseaux de capteurs. En appliquant les lignes directrices suggérées par la science de la conception au cas étudié, nous identifions un certain nombre de prototypes interactifs qui abordent successivement des questions fondamentales dans ce contexte particulier. En partant d'une simple interaction homme-machine avec un prototype à un capteur et une sortie réalisé en Lego Mindstorms NXT, nous aboutissons à un capteur sur mesure qui permet d'interagir avec notre profil d'utilisateur particulier : le jardinier. Ainsi, nous fournissons une plateforme de référence pour combiner les microsystèmes d'information et l'interaction homme-machine dans la recherche en sciences de la conception pour la recherche sur la durabilité environnementale.","8962","3","89","Une approche de la science de la conception pour le contrôle interactif du climat de la serre en utilisant Lego Mindstorms pour un prototypage intensif de capteurs

Dans cet article, nous décrivons un cadre scientifique de conception pour l&#039;utilisation de prototypes interactifs à forte intensité de capteurs afin de développer des systèmes interactifs de gestion du climat des serres. Nous mettons l&#039;accent sur la façon dont la science de la conception, et en particulier ce que nous appelons les microsystèmes d&#039;information, peut améliorer les aspects de l&#039;interaction homme-machine concernant la performance du contrôle des émissions grâce à de meilleures interfaces de contrôle interactives et à l&#039;utilisation de la technologie des réseaux de capteurs. En appliquant les lignes directrices suggérées par la science de la conception au cas étudié, nous identifions un certain nombre de prototypes interactifs qui abordent successivement des questions fondamentales dans ce contexte particulier. En partant d&#039;une simple interaction homme-machine avec un prototype doté de capteur et une sortie réalisé en Lego Mindstorms NXT, nous aboutissons à un capteur sur mesure qui permet d&#039;interagir avec notre profil d&#039;utilisateur particulier : le jardinier. Ainsi, nous fournissons une plateforme de référence pour combiner les microsystèmes d&#039;information et l&#039;interaction homme-machine dans la recherche en sciences de la conception pour la recherche sur la durabilité environnementale.","4","2023-06-09 11:17:19","2023-06-09 11:40:25","8962",,"3","4","2","3","3","1",NULL,,"2023-06-09 11:17:10","0"
"3098752","Many edge bundling techniques (i.e., data simplification as a support for data visualization and decision making) exist but they are not directly applicable to any kind of dataset and their parameters are often too abstract and difficult to set up. As a result, this hinders the user ability to create efficient aggregated visualizations. To address these issues, we investigated a novel way of handling visual aggregation with a task-driven and user-centered approach. Given a graph, our approach produces a decluttered view as follows: first, the user investigates different edge bundling results and specifies areas, where certain edge bundling techniques would provide user-desired results. Second, our system then computes a smooth and structural preserving transition between these specified areas. Lastly, the user can further fine-tune the global visualization with a direct manipulation technique to remove the local ambiguity and to apply different visual deformations. In this paper, we provide details for our design rationale and implementation. Also, we show how our algorithm gives more suitable results compared to current edge bundling techniques, and in the end, we provide concrete instances of usages, where the algorithm combines various edge bundling results to support diverse data exploration and visualizations.","Interactive Structure-aware Blending of Diverse Edge Bundling Visualizations","Yunhai Wang, Mingliang Xue, Yanyan Wang, Xinyuan Yan, Baoquan Chen, Chi-Wing Fu, Christophe Hurter","5963","Mélange interactif et conscient de la structure de diverses visualisations de regroupement d'arêtes

Il existe de nombreuses techniques de regroupement des bords (c'est-à-dire de simplification des données pour faciliter la visualisation des données et la prise de décision), mais elles ne sont pas directement applicables à tout type d'ensemble de données et leurs paramètres sont souvent trop abstraits et difficiles à définir. Par conséquent, cela entrave la capacité de l'utilisateur à créer des visualisations agrégées efficaces. Pour résoudre ces problèmes, nous avons étudié une nouvelle façon de traiter l'agrégation visuelle avec une approche axée sur la tâche et centrée sur l'utilisateur. À partir d'un graphique, notre approche produit une vue épurée de la manière suivante : tout d'abord, l'utilisateur étudie différents résultats de regroupement d'arêtes et spécifie les zones où certaines techniques de regroupement d'arêtes fourniraient les résultats souhaités par l'utilisateur. Ensuite, notre système calcule une transition douce et préservant la structure entre les zones spécifiées. Enfin, l'utilisateur peut affiner la visualisation globale à l'aide d'une technique de manipulation directe pour supprimer l'ambiguïté locale et appliquer différentes déformations visuelles. Dans cet article, nous fournissons des détails sur notre conception et notre mise en œuvre. Nous montrons également comment notre algorithme donne des résultats plus appropriés que les techniques actuelles de regroupement des bords et, pour finir, nous fournissons des exemples concrets d'utilisation, où l'algorithme combine divers résultats de regroupement des bords pour faciliter l'exploration et la visualisation de diverses données.","18557","3","90","Mélange interactif et conscient de la structure de diverses visualisations de regroupement de bords

Il existe de nombreuses techniques de regroupement des bords (c&#039;est-à-dire de simplification des données pour faciliter la visualisation des données et la prise de décision), mais elles ne sont pas directement applicables à tout type d&#039;ensemble de données et leurs paramètres sont souvent trop abstraits et difficiles à définir. Par conséquent, cela entrave la capacité de l&#039;utilisateur à créer des visualisations agrégées efficaces. Pour résoudre ces problèmes, nous avons étudié une nouvelle façon de traiter l&#039;agrégation visuelle avec une approche axée sur la tâche et centrée sur l&#039;utilisateur. À partir d&#039;un graphique, notre approche produit une vue épurée de la manière suivante : tout d&#039;abord, l&#039;utilisateur étudie différents résultats de regroupement d&#039;arêtes et spécifie les zones où certaines techniques de regroupement d&#039;arêtes fourniraient les résultats souhaités par l&#039;utilisateur. Ensuite, notre système calcule une transition douce et préservant la structure entre les zones spécifiées. Enfin, l&#039;utilisateur peut affiner la visualisation globale à l&#039;aide d&#039;une technique de manipulation directe pour supprimer l&#039;ambiguïté locale et appliquer différentes déformations visuelles. Dans cet article, nous fournissons des détails sur notre conception et notre mise en œuvre. Nous montrons également comment notre algorithme donne des résultats plus appropriés que les techniques actuelles de regroupement des bords et, pour finir, nous fournissons des exemples concrets d&#039;utilisation, où l&#039;algorithme combine divers résultats de regroupement des bords pour faciliter l&#039;exploration et la visualisation de diverses données.","4","2023-06-09 11:40:40","2023-06-09 12:22:15","18557",,"1","1","2","1","1","1",NULL,,"2023-06-09 11:40:26","0"
"3694873","Transformer-based Language Models are widely used in Natural Language Processing related tasks. Thanks to their pre-training, they have been successfully adapted to Information Extraction in business documents. However, most pre-training tasks proposed in the literature for business documents are too generic and not sufficient to learn more complex structures. In this paper, we use LayoutLM, a language model pre-trained on a collection of business documents, and introduce two new pre-training tasks that further improve its capacity to extract relevant information. The first is aimed at better understanding the complex layout of documents, and the second focuses on numeric values and their order of magnitude. These tasks force the model to learn bettercontextualized representations of the scanned documents. We further introduce a new post-processing algorithm to decode BIESO tags in Information Extraction that performs better with complex entities. Our method significantly improves extraction performance on both public (from 93.88 to 95.50 F1 score) and private (from 84.35 to 84.84 F1 score) datasets composed of expense receipts, invoices, and purchase orders.","Improving Information Extraction on Business Documents with Specific Pre-Training Tasks","Thibault Douzon, Stefan Duffner, Christophe Garcia, Jérémy Espinas","4587","Amélioration de l'extraction d'information sur des documents commerciaux avec des tâches de pré-entraînement spécifiques

Les modèles de langage basés sur des transformateurs sont largement utilisés dans les tâches liées au traitement du langage naturel. Grâce à leur pré-entraînement, ils ont été adaptés avec succès à l'extraction d'informations dans les documents commerciaux. Cependant, la plupart des tâches de pré-entraînement proposées dans la littérature pour les documents commerciaux sont trop génériques et ne permettent pas d'apprendre des structures plus complexes. Dans cet article, nous utilisons LayoutLM, un modèle de langage pré-entraîné sur une collection de documents commerciaux, et introduisons deux nouvelles tâches de pré-entraînement qui améliorent encore sa capacité à extraire des informations pertinentes. La première vise à mieux comprendre la mise en page complexe des documents, et la seconde se concentre sur les valeurs numériques et leur ordre de grandeur. Ces tâches obligent le modèle à apprendre des représentations mieux contextualisées des documents scannés. Nous introduisons également un nouvel algorithme de post-traitement pour décoder les étiquettes BIESO dans l'extraction d'informations, qui donne de meilleurs résultats avec les entités complexes. Notre méthode améliore considérablement les performances d'extraction sur les ensembles de données publics (de 93,88 à 95,50 pour le score F1) et privés (de 84,35 à 84,84 pour le score F1) composés de reçus de dépenses, de factures et de bons de commande.","19057","3","95","Amélioration de l&#039;extraction d&#039;information sur des documents commerciaux avec des tâches de pré-entraînement spécifiques

Les modèles de langue basés sur des transformateurs sont largement utilisés dans les tâches liées au traitement automatique des langues. Grâce à leur pré-entraînement, ils ont été adaptés avec succès à l&#039;extraction d&#039;informations dans les documents commerciaux. Cependant, la plupart des tâches de pré-entraînement proposées dans la littérature pour les documents commerciaux sont trop génériques et ne permettent pas d&#039;apprendre des structures plus complexes. Dans cet article, nous utilisons LayoutLM, un modèle de langue pré-entraîné sur une collection de documents commerciaux, et introduisons deux nouvelles tâches de pré-entraînement qui améliorent encore sa capacité à extraire des informations pertinentes. La première vise à mieux comprendre la mise en page complexe des documents, et la seconde se concentre sur les valeurs numériques et leur ordre de grandeur. Ces tâches obligent le modèle à apprendre des représentations mieux contextualisées des documents scannés. Nous introduisons également un nouvel algorithme de post-traitement pour décoder les étiquettes BIESO dans l&#039;extraction d&#039;informations, qui donne de meilleurs résultats avec les entités complexes. Notre méthode améliore considérablement les performances d&#039;extraction sur les ensembles de données publics (de 93,88 à 95,50 pour le score F1) et privés (de 84,35 à 84,84 pour le score F1) composés de reçus de dépenses, de factures et de bons de commande.","4","2023-06-11 22:07:20","2023-06-11 22:32:30","19057",,"1","1","2","1","1","1",NULL,,"2023-06-11 10:06:13","0"
"3688032","ok","Screen & Relax: Accelerating The Resolution Of Elastic-Net By Safe Identification of The Solution Support","Theo Guyard, Cedric Herzet, Clement Elvira","4617","Ecran et Détendez-vous: Accélérer la résolution de Elastic-Net en identifiant en toute sécurité le support de la solution

OK","11404","4","100","Ecran et Détendez-vous: Accélérer la résolution de Elastic-Net en identifiant en toute sécurité le support de la solution

OK","4","2023-06-12 13:39:28","2023-06-12 13:40:59","11404",,NULL,NULL,NULL,NULL,NULL,"1",NULL,,"2023-06-12 01:39:24","1"
"3603538","Probabilistic text generators have been used to produce fake scientific papers for more than a decade. Now more complex AI-powered generation techniques produce texts indistinguishable from that of humans and the generation of scientific texts starting from a few keywords used as input has been documented. Our study introduces the concept of tortured phrases: unexpected weird phrases in lieu of established ones, such as ‘counterfeit consciousness’ instead of ‘artificial intelligence.’ Hypothesising the use of advanced language models we ran a detector on the abstracts of recent articles and on several control sets.","Flagging suspect publications and crowdsourcing post-publication reassessments: the ‘Problematic Paper Screener’","Guillaume Cabanac, Cyril Labbé, Alexander Magazinov","806","Signalement des publications suspectes et réévaluations post-publication par le crowdsourcing : le ""Problematic Paper Screener"" (Filtre à articles problématiques)

Les générateurs de textes probabilistes sont utilisés depuis plus de dix ans pour produire de faux articles scientifiques. Aujourd'hui, des techniques de génération plus complexes alimentées par l'IA produisent des textes impossibles à distinguer de ceux des humains, et la génération de textes scientifiques à partir de quelques mots-clés utilisés en entrée a été documentée. Notre étude introduit le concept de phrases torturées : des phrases bizarres inattendues à la place de phrases établies, telles que ""conscience contrefaite"" au lieu d'""intelligence artificielle"". En supposant l'utilisation de modèles linguistiques avancés, nous avons lancé un détecteur sur les résumés d'articles récents et sur plusieurs ensembles de contrôle.","11058","3","103","Signalement des publications suspectes et réévaluations post-publication par le crowdsourcing : le &quot;Problematic Paper Screener&quot; (Filtre à articles problématiques)

Les générateurs automatiques de textes sont utilisés depuis plus de dix ans pour produire de faux articles scientifiques. Aujourd&#039;hui, des techniques de génération plus complexes alimentées par l&#039;IA produisent des textes impossibles à distinguer de ceux des humains, et la génération de textes scientifiques à partir de quelques mots-clés utilisés en entrée a été documentée. Notre étude introduit le concept de « tortured phrases » : des expressions étranges inattendues à la place d&#039;expressions établies, telles que « conscience contrefaite » au lieu de « intelligence artificielle ». En faisant l&#039;hypothèse de l&#039;utilisation de modèles de langage avancés, nous avons lancé un détecteur sur les résumés d&#039;articles récents et sur plusieurs ensembles de contrôle.","4","2023-06-12 15:00:58","2023-06-12 15:04:43","11058",,"2","1","3","2","1","1",NULL,"&quot;tortured phrase&quot; : terme sans traduction, introduit dans cet article. &quot;Phrase torturée&quot; serait trop étrange, j&#039;ai donc choisi d&#039;emprunter le terme anglais.","2023-06-12 03:00:53","0"
"3516008","Public mood can affect individual behavior. In this paper, I explores the relationship between public mood and trend of stock market. In this paper, I collected one-month Tweets in terms of several aspects such as happy, sad. I train a labeled movie reviews as the training data using Recurrent Neural Network (RNN). Then, I classify the Tweets based on the trained model in real time. After the sentiment analysis on Tweets, I use the linear and polynomial models to predict the stock prices of SPY. My results show that RNN is one of promising methods to do sentiment analysis on Tweets and the predictions made by the classification of RNN are good resources to predict stock prices.","Predicting Stock Market Indicators Through Sentiment Analysis On Twitter","Andy Fuller","5045","Prédire les indicateurs boursiers grâce à l'analyse des sentiments sur Twitter

L'humeur du public peut affecter le comportement individuel. Dans cet article, j'explore la relation entre l'humeur du public et la tendance du marché boursier. Dans cet article, j'ai collecté des Tweets d'un mois en fonction de plusieurs aspects tels que la joie, la tristesse. J'entraîne des critiques de films étiquetées comme données d'entraînement à l'aide d'un réseau neuronal récurrent (RNN). Ensuite, je classe les tweets sur la base du modèle formé en temps réel. Après l'analyse des sentiments sur les Tweets, j'utilise les modèles linéaires et polynomiaux pour prédire les prix des actions de SPY. Mes résultats montrent que le RNN est l'une des méthodes prometteuses pour effectuer l'analyse des sentiments sur les Tweets et que les prédictions faites par la classification du RNN sont de bonnes ressources pour prédire les prix des actions.","17697","3","105","Prédire les indicateurs boursiers grâce à l&#039;analyse des sentiments sur Twitter

L&#039;humeur du public peut affecter le comportement individuel. Dans cet article, nous explorons la relation entre l&#039;humeur du public et la tendance du marché boursier. Nous avons collecté des tweets sur un mois en fonction de plusieurs métriques telles que la joie ou la tristesse. Nous avons utilisé des critiques de films étiquetées comme données d&#039;entraînement à l&#039;aide d&#039;un réseau de neurones récurrent. Ensuite, nous avons classé les tweets sur la base du modèle formé en temps réel. Après l&#039;analyse des sentiments dans les tweets, nous avons utilisé les modèles linéaires et polynomiaux pour prédire les prix des actions de SPY. Les résultats montrent que le réseau de neurones récurrent est une méthode prometteuse pour effectuer l&#039;analyse des sentiments dans les tweets et que les prédictions ainsi faites sont de bonnes ressources pour prédire le prix des actions.","4","2023-06-13 10:53:28","2023-06-13 11:20:00","17697",,"2","2","3","4","3","2",NULL,,"2023-06-13 10:53:25","0"
"3808925","We present a locking-based design-for-security methodology to prevent piracy of RF transceiver integrated circuits. The solution is called SyncLock as it locks the synchronization of the transmitter with the receiver. If a key other than the secret key is applied, synchronization and, thereby, communication fail. SyncLock is implemented using a novel locking concept consisting of two spatially separated mechanisms. A hard-coded error is hidden into the design to break synchronization while error correction, i.e., unlocking, takes place in another part of the design by applying the secret key. SyncLock offers several advantages: the secret key is unique, i.e., any incorrect key causes a denial-of-service, there is no performance penalty, it can be seemingly integrated into the digital design flow, area and power overheads are negligible, and it achieves maximum provable security thwarting all known counterattacks. SyncLock is demonstrated with hardware measurements.","Anti-Piracy Design of RF Transceivers","Alán Rodrigo Díaz-Rizo, Hassan Aboushady, Haralampos-G. Stratigopoulos","4267","Conception anti-piratage des émetteurs-récepteurs RF

Nous présentons une méthodologie de conception pour la sécurité basée sur le verrouillage pour prévenir le piratage des circuits intégrés d’émetteur-récepteur RF. La solution s’appelle SyncLock car elle verrouille la synchronisation de l’émetteur avec le récepteur. Si une clé autre que la clé secrète est appliquée, la synchronisation et, par conséquent, la communication échoue. SyncLock est implémenté en utilisant un nouveau concept de verrouillage composé de deux mécanismes spatialement séparés. Une erreur codée en dur est cachée dans la conception pour briser la synchronisation tandis que la correction d’erreur, c’est-à-dire le déverrouillage, a lieu dans une autre partie de la conception en appliquant la clé secrète. SyncLock offre plusieurs avantages: la clé secrète est unique, c’est-à-dire que toute clé incorrecte cause un déni de service, il n’y a pas de pénalité de performance, elle peut apparemment être intégrée dans le flux de conception numérique, la zone et les frais généraux de puissance sont négligeables, et il atteint une sécurité prouvable maximale contre toutes les contre-attaques connues. SyncLock est démontré avec des mesures matérielles.","13225","4","123","Conception anti-piratage des émetteurs-récepteurs RF

Nous présentons une méthodologie de conception pour la sécurité basée sur le verrouillage pour lutter contre le piratage des circuits intégrés d’émetteur-récepteur RF. Cette solution s’appelle SyncLock car elle verrouille la synchronisation de l’émetteur avec le récepteur. Si un code autre que le code secret est saisi, la synchronisation et donc la communication échouent. SyncLock est implémenté en utilisant un nouveau concept de verrouillage composé de deux mécanismes spatialement séparés. Une erreur codée en dur est cachée dans la conception pour interrompre la synchronisation tandis que la correction d’erreur, c’est-à-dire le déverrouillage, a lieu dans une autre partie de la conception en saisissant le code secret. SyncLock offre plusieurs avantages : le code secret est unique, c’est-à-dire que tout code incorrect entraîne un déni de service, il n’y a pas de pénalité de performance, elle peut apparemment être intégrée dans le flux de conception numérique, la zone et les frais généraux de puissance sont négligeables, et il atteint une preuve de sécurité maximale contre toutes les contre-attaques connues. SyncLock est démontré avec des mesures matérielles.","4","2023-06-14 15:11:05","2023-06-14 15:11:59","13225",,"2","1","3","4","3","1",NULL,"provable security : preuve de sécurité
","2023-06-14 03:11:02","0"
"3761937","To embed multi-core COTS processors in an avionic product, the platform must be thoroughly analyzed from two perspectives: the worst case real-time behaviours and the safety impact of internal failures. Both activities are very complex and error-prone for large size systems. Moreover, the frameworks for both perspectives (real-time and safety) are completely decoupled, leading to independent and possibly incoherent analyses. Our purpose is to unify both worlds and help designers in their certification process. To this end, we have formalized and unified as much as possible the different perspectives of multi-core analysis. We have also proposed a simple description language for the platform, which contains the minimal concepts needed by both perspectives, as well as an automatic translation to the two analysis frameworks.","Modelling and analyzing multi-core COTS processors","Frédéric Boniol, Julien Brunel, Kevin Delmas, Claire Pagetti, Victor Jegu","4412","Modélisation et analyse des processeurs multi-cœurs COTS

Pour intégrer des processeurs COTS multicœurs dans un produit avionique, la plate-forme doit être analysée en profondeur sous deux angles : les comportements en temps réel les plus défavorables et l'impact des défaillances internes sur la sécurité. Ces deux activités sont très complexes et sujettes aux erreurs pour les systèmes de grande taille. De plus, les cadres des deux perspectives (temps réel et sécurité) sont complètement découplés, ce qui conduit à des analyses indépendantes et éventuellement incohérentes. Notre objectif est d'unifier les deux mondes et d'aider les concepteurs dans leur processus de certification. A cette fin, nous avons formalisé et unifié autant que possible les différentes perspectives de l'analyse multicœur. Nous avons également proposé un langage de description simple pour la plateforme, qui contient les concepts minimaux nécessaires aux deux perspectives, ainsi qu'une traduction automatique vers les deux cadres d'analyse.","17423","3","124","Modélisation et analyse des processeurs multi-cœurs COTS

Pour intégrer des processeurs COTS multicœur dans un produit avionique, la plate-forme doit être analysée en profondeur sous deux angles : les comportements en temps réel les plus défavorables et l&#039;impact des défaillances internes sur la sécurité. Ces deux activités sont très complexes et sujettes aux erreurs pour les systèmes de grande taille. De plus, les cadres des deux angles (temps réel et sécurité) sont complètement découplés, ce qui conduit à des analyses indépendantes et possiblement incohérentes. Notre objectif est d&#039;unifier les deux mondes et d&#039;aider les concepteurs dans leur processus de certification. À cette fin, nous avons formalisé et unifié autant que possible les différentes perspectives de l&#039;analyse multicœur. Nous avons également proposé un langage de description simple pour la plateforme, qui contient les concepts minimaux nécessaires aux deux perspectives, ainsi qu&#039;une traduction automatique vers les deux cadres d&#039;analyse.","4","2023-06-14 15:12:09","2023-06-14 15:27:17","17423",,"1","2","2","1","1","1",NULL,,"2023-06-14 03:12:05","0"
"3826652","With the emergence of deep perceptual image features, style transfer has become a popular application that repaints a picture while preserving the geometric patterns and textures from a sample image. Our work is devoted to the combination of perceptual features from multiple style images, taken at different scales, e.g. to mix large-scale structures of a style image with fine-scale textures. Surprisingly, this turns out to be difficult, as most deep neural representations are learned to be robust to scale modifications, so that large structures tend to be tangled with smaller scales. Here a multi-scale convolutional architecture is proposed for bi-scale style transfer. Our solution is based on a modular auto-encoder composed of two lightweight modules that are trained independently to transfer style at specific scales, with control over styles and colors.","Modular and Lightweight Networks for Bi-scale Style Transfer","Thibault Durand, Julien Rabin, David Tschumperlé","4221","Réseaux modulaires et légers pour transfert de style bi-échelle

Avec l'émergence de caractéristiques d'image perceptuelles profondes, le transfert de style est devenu une application populaire qui redessine une image tout en préservant les motifs géométriques et les textures d'une image échantillon. Notre travail est consacré à la combinaison de caractéristiques perceptuelles de plusieurs images de style, prises à différentes échelles, par exemple pour mélanger des structures à grande échelle d'une image de style avec des textures à petite échelle. Étonnamment, cela s'avère difficile, car la plupart des représentations neuronales profondes sont apprises pour être robustes aux modifications d'échelle, de sorte que les grandes structures ont tendance à être emmêlées avec des échelles plus petites. Ici, une architecture convolutionnelle multi-échelle est proposée pour un transfert de style bi-échelle. Notre solution est basée sur un codeur automatique modulaire composé de deux modules légers qui sont formés indépendamment pour transférer le style à des échelles spécifiques, avec contrôle sur les styles et les couleurs.","16137","6","126","Réseaux modulaires et légers pour transfert de style à double échelle

Avec l&#039;émergence des fonctionnalités d&#039;image perceptuelles profondes, le transfert de style est devenu une application populaire qui redessine une image tout en préservant les motifs géométriques et les textures d&#039;un échantillon d&#039;image. Notre travail est consacré à la combinaison de caractéristiques perceptuelles de plusieurs images de style, prises à différentes échelles, par exemple pour mélanger des structures à grande échelle d&#039;une image de style avec des textures à petite échelle. Étonnamment, cela s&#039;avère difficile, car la plupart des représentations neuronales profondes sont faites pour résister aux modifications d&#039;échelle, de sorte que les grandes structures ont tendance à être mêlées avec des échelles plus petites. Ici, une architecture convolutionnelle multi-échelle est proposée pour un transfert de style à double échelle. Notre solution est basée sur un codeur automatique modulaire composé de deux modules légers qui sont formés indépendamment pour transférer le style à des échelles spécifiques, avec contrôle sur les styles et les couleurs.","4","2023-06-14 16:32:47","2023-06-14 17:09:29","16137",,"3","2","2","4","3","2",NULL,,"2023-06-14 04:32:34","0"
"3923473","Graded modal types systems and coeffects are becoming a standard formalism to deal with context-dependent, usage-sensitive computations, especially when combined with computational effects. From a semantic perspective, effectful and coeffectful languages have been studied mostly by means of denotational semantics and almost nothing has been done from the point of view of relational reasoning. This gap in the literature is quite surprising, since many cornerstone results — such as non-interference , metric preservation , and proof irrelevance — on concrete coeffects are inherently relational. In this paper, we fill this gap by developing a general theory and calculus of program relations for higher-order languages with combined effects and coeffects. The relational calculus builds upon the novel notion of a corelator (or comonadic lax extension ) to handle coeffects relationally. Inside such a calculus, we define three notions of effectful and coeffectful program refinements: contextual approximation , logical preorder , and applicative similarity . These are the first operationally-based notions of program refinement (and, consequently, equivalence) for languages with combined effects and coeffects appearing in the literature. We show that the axiomatics of a corelator (together with the one of a relator) is precisely what is needed to prove all the aforementioned program refinements to be precongruences, this way obtaining compositional relational techniques for reasoning about combined effects and coeffects.","A relational theory of effects and coeffects","Ugo Dal Lago, Francesco Gavazzo","3951","Une théorie relationnelle des effets et des coeffets

Les systèmes de types modals et les coeffets sont en train de devenir un formalisme standard pour traiter les calculs liés au contexte et sensibles à l’utilisation, en particulier lorsqu’ils sont combinés avec des effets de calcul. D’un point de vue sémantique, des langages efficaces et coefficaces ont été étudiés principalement au moyen de la sémantique dénotationnelle et presque rien n’a été fait du point de vue du raisonnement relationnel. Cette lacune dans la littérature est assez surprenante, car de nombreux résultats fondamentaux — tels que la non-ingérence, la préservation métrique et l’absence de pertinence de la preuve — sur les coeffets concrets sont intrinsèquement relationnels. Dans cet article, nous comblons cette lacune en développant une théorie générale et un calcul des relations de programme pour les langages d’ordre supérieur avec des effets combinés et des coeffets. Le calcul relationnel s’appuie sur la nouvelle notion d’un corelateur (ou extension laxique comonadique) pour gérer les coeffets relationnels. À l’intérieur d’un tel calcul, nous définissons trois notions de raffinements efficaces et coefficaces du programme: approximation contextuelle, précommande logique et similarité applicative. Ce sont les premières notions opérationnelles de raffinement des programmes (et, par conséquent, d’équivalence) pour les langues avec effets combinés et coeffets apparaissant dans la littérature. Nous montrons que l’axiomatique d’un corelateur (avec celle d’un relateur) est précisément ce qui est nécessaire pour prouver que tous les raffinements du programme susmentionnés sont des précongruences, obtenant ainsi des techniques relationnelles de composition pour le raisonnement sur les effets combinés et les coeffets.","12909","4","127","Une théorie relationnelle des effets et des co-effets

Les systèmes de types modaux et les co-effets sont en train de devenir un formalisme standard pour traiter les calculs liés au contexte et sensibles à l’usage, en particulier lorsqu’ils sont combinés avec des effets de calcul. D’un point de vue sémantique, des langages efficaces et co-efficaces ont été étudiés principalement au moyen de la sémantique dénotationnelle et presque rien n’a été fait du point de vue du raisonnement relationnel. Cette lacune dans la littérature est assez surprenante, car de nombreux résultats fondamentaux — tels que la non-ingérence, la préservation métrique et l’absence de pertinence de la preuve — sur les co-effets concrets sont intrinsèquement relationnels. Dans cet article, nous comblons cette lacune en développant une théorie générale et un calcul des relations de programme pour les langages d’ordre supérieur avec des effets combinés et des co-effets. Le calcul relationnel s’appuie sur la nouvelle notion d’un co-relateur (ou extension laxiste comonadique) pour gérer les co-effets relationnels. À l’intérieur d’un tel calcul, nous définissons trois notions de raffinements efficaces et co-efficaces du programme : approximation contextuelle, précommande logique et similarité applicative. Ce sont les premières notions opérationnelles de raffinement des programmes (et, par conséquent, d’équivalence) pour les langues avec effets combinés et co-effets apparaissant dans la littérature. Nous montrons que l’axiomatique d’un co-relateur (avec celle d’un relateur) est précisément ce qui est nécessaire pour prouver que tous les raffinements du programme susmentionnés sont des précongruences, obtenant ainsi des techniques relationnelles de composition pour le raisonnement sur les effets combinés et les co-effets.","4","2023-06-14 17:09:39","2023-06-14 17:29:46","12909",,"2","3","2","2","3","1",NULL,,"2023-06-14 05:09:35","0"
"3694961","Kernel discriminant analysis (KDA), the nonlinear extension of linear Discriminant Analysis (LDA), is a popular tool for learning one or multiple categories in nonlinear data sets. However, in most modern pattern recognition applications such as video surveillance, data are collected in flow and require sequential processing. In this context, KDA is faced two critical issues: an original formulation unsuited to the dynamic nature of the data and an increasing memory requirement for the kernel matrix storage. Motivated by the state-of-the-art performance reported by the null KDA, we propose in this paper a new solution to solve the null KDA (NKDA) in the context of data streams. Compared to previous works, our contribution is based on three points: first, we develop an exact incremental scheme which guarantees accurate solutions. Secondly, we develop a compression mechanism based on the following observation: rger the size of the training data set more the distances in the null space contract This property of the null space leads to formulate an indicator of redundancy in the training data set. This criterion is the cornerstone of our incremental KNDA because it authorizes incremental learning on large-scale data sets. Third, the problem of novelty detection in multi-class and one-class scenarios is addressed. More precisely, the fact that distances in the null space change over the training period leads us to define adjustable novelty thresholds. Lastly, numerous experiments based on various publicly available data sets and state-of-the-art classifiers show that the proposed method is effective both for multi-class and one-class real applications.","Incremental and compressible kernel null discriminant analysis","Franck Dufrenois","4583","Analyse discriminante du noyau incrémental et compressible

L’analyse discriminante du noyau (KDA), l’extension non linéaire de l’analyse discriminante linéaire (LDA), est un outil populaire pour apprendre une ou plusieurs catégories dans des ensembles de données non linéaires. Cependant, dans la plupart des applications modernes de reconnaissance de motifs telles que la vidéosurveillance, les données sont collectées en flux et nécessitent un traitement séquentiel. Dans ce contexte, KDA est confrontée à deux problèmes critiques: une formulation originale inadaptée à la nature dynamique des données et à un besoin croissant de mémoire pour le stockage de la matrice du noyau. Motivés par les performances à la pointe de la technologie rapportées par la KDA nulle, nous proposons dans cet article une nouvelle solution pour résoudre le KDA nul (NKDA) dans le contexte des flux de données. Par rapport aux travaux précédents, notre contribution est basée sur trois points: tout d’abord, nous développons un schéma incrémental exact qui garantit des solutions précises. Deuxièmement, nous développons un mécanisme de compression basé sur l’observation suivante: rger la taille des données d’entraînement plus les distances dans le contrat d’espace nul Cette propriété de l’espace nul conduit à formuler un indicateur de redondance dans l’ensemble de données d’entraînement. Ce critère est la pierre angulaire de notre KNDA incrémental car il autorise l’apprentissage progressif sur des ensembles de données à grande échelle. Troisièmement, le problème de la détection de la nouveauté dans les scénarios multi-classes et d’une classe est abordé. Plus précisément, le fait que les distances dans l’espace nul changent au cours de la période d’entraînement nous amène à définir des seuils de nouveauté ajustables. Enfin, de nombreuses expériences basées sur divers ensembles de données accessibles au public et des classificateurs de pointe montrent que la méthode proposée est efficace à la fois pour des applications réelles multi-classes et une seule classe.","11370","4","128","Analyse discriminante du noyau incrémental et compressible

L’analyse discriminante du noyau (KDA), extension non linéaire de l’analyse discriminante linéaire (LDA), est un outil populaire pour apprendre une ou plusieurs catégories dans des ensembles de données non linéaires. Cependant, dans la plupart des applications modernes de reconnaissance de motifs telles que la vidéosurveillance, les données sont collectées en flux et nécessitent un traitement séquentiel. Dans ce contexte, la KDA est confrontée à deux problèmes cruciaux : une formulation originale inadaptée à la nature dynamique des données et un besoin croissant de mémoire pour le stockage de la matrice du noyau. Motivés par les performances à la pointe de la technologie rapportées par la KDA nulle, nous proposons dans cet article une nouvelle solution pour résoudre le KDA nulle (NKDA) dans le contexte des flux de données. Par rapport aux travaux précédents, notre contribution est basée sur trois points : tout d’abord, nous développons un schéma incrémental exact qui garantit des solutions précises. Deuxièmement, nous développons un mécanisme de compression basé sur l’observation suivante: régler la taille des données d’entraînement plus les distances dans le contrat d’espace nul. Cette propriété de l’espace nul conduit à formuler un indicateur de redondance dans l’ensemble de données d’entraînement. Ce critère est la pierre angulaire de notre NKDA incrémental car il autorise l’apprentissage progressif sur des ensembles de données à grande échelle. Troisièmement, le problème de la détection de la nouveauté dans les scénarios multi-classes et d’une classe est abordé. Plus précisément, le fait que les distances dans l’espace nul changent au cours de la période d’entraînement nous amène à définir des seuils de nouveauté ajustables. Enfin, de nombreuses expériences basées sur divers ensembles de données accessibles au public et des classificateurs de pointe montrent que la méthode proposée est efficace à la fois pour des applications réelles multi-classes et une seule classe.","4","2023-06-14 17:30:23","2023-06-14 18:05:10","11370",,"2","3","2","2","3","3",NULL,,"2023-06-14 05:30:17","0"
"3827010","Machine Translation (MT) is usually viewed as a one-shot process that generates the target language equivalent of some source text from scratch. We consider here a more general setting which assumes an initial target sequence, that must be transformed into a valid translation of the source, thereby restoring parallelism between source and target. For this bilingual synchronization task, we consider several architectures (both autoregressive and non-autoregressive) and training regimes, and experiment with multiple practical settings such as simulated interactive MT, translating with Translation Memory (TM) and TM cleaning. Our results suggest that one single generic edit-based system, once fine-tuned, can compare with, or even outperform, dedicated systems specifically trained for these tasks.","Bilingual Synchronization: Restoring Translational Relationships with Editing Operations","Jitao Xu, Josep Crego, François Yvon","517","Synchronisation bilingue : Restauration des relations de traduction avec les opérations de modification

La traduction automatique (MT) est généralement considérée comme un processus en une seule étape qui génère l'équivalent linguistique cible d'un texte source à partir de zéro. Nous considérons ici un paramètre plus général qui suppose une séquence cible initiale, qui doit être transformée en une traduction valide de la source, ce qui rétablit le parallélisme entre la source et la cible. Pour cette tâche de synchronisation bilingue, nous considérons plusieurs architectures (à la fois autorégressives et non autorégressives) et régimes d'entraînement, et expérimentons avec de multiples paramètres pratiques tels que MT interactif simulé, traduction avec mémoire de traduction (TM) et TM nettoyage. Nos résultats suggèrent qu'un seul système générique basé sur l'édition, une fois ajusté, peut comparer, voire surpasser, les systèmes dédiés spécialement formés pour ces tâches.","4264","6","132","Synchronisation bilingue : Restauration des relations de traduction avec les opérations de modification

La traduction automatique (MT) est généralement considérée comme un processus en une seule étape qui génère l&#039;équivalent en langue cible d&#039;un texte source à partir de zéro. Nous considérons ici un paramètre plus général qui suppose une séquence cible initiale, qui doit être transformée en une traduction valide de la source, ce qui rétablit le parallélisme entre la source et la cible. Pour cette tâche de synchronisation bilingue, nous considérons plusieurs architectures (à la fois autorégressives et non autorégressives) et régimes d&#039;entraînement, et expérimentons avec de multiples paramètres pratiques tels que MT interactif simulé, traduction avec mémoire de traduction (TM) et TM nettoyage. Nos résultats suggèrent qu&#039;un seul système générique basé sur l&#039;édition, une fois ajusté, peut comparer, voire surpasser, les systèmes dédiés spécialement formés pour ces tâches.","4","2023-06-15 10:29:58","2023-06-15 10:59:52","4264",,"1","1","2","1","1","1",NULL,,"2023-06-15 10:29:53","0"
"3606724","Research-focused information systems harvest and promote the scientific output of researchers. Disambiguating author identities is key when disentangling homonyms to avoid merging several persons' records. ORCID offers an identifier to link one's identity, affiliations, and bibliography. While funding agencies and scholarly publishers promote ORCID, little is known about its adoption rate. We introduce a method to quantify ORCID adoption according to researchers' discipline and occupation in a higher-education organisation. We semi-automatically matched the 6,607 staff members affiliated to the 145 labs of the Toulouse scientific area with the 7.3 million profiles at orcid.org. The observed ORCID adoption of 41.8% comes with disciplinewise disparities. Unexpectedly, only 48.3% of all profiles listed at least one work and profiles with no works might just have been created to get an identifier. Those 'empty' profiles are of little interest for the entity disambiguation task. To our knowledge, this is the first study of ORCID adoption at the scale of a multidisciplinary scientific metropole. This method is replicable and future studies can target other cases to contrast the dynamics of ORCID adoption worldwide.","ORCID growth and field‐wise dynamics of adoption: A case study of the Toulouse scientific area","Heusse M.-D., Guillaume Cabanac","805","Croissance de l’ORCID et dynamique d’adoption sur le terrain: Une étude de cas de l’espace scientifique toulousain

Les systèmes d’information axés sur la recherche recueillent et promeuvent les résultats scientifiques des chercheurs. Il est essentiel de désambiguiser les identités des auteurs lorsqu’il s’agit de démanteler des homonymes afin d’éviter de fusionner les dossiers de plusieurs personnes. ORCID offre un identifiant pour relier son identité, ses affiliations et sa bibliographie. Alors que les organismes de financement et les éditeurs universitaires font la promotion de l’ORCID, on sait peu de choses sur son taux d’adoption. Nous introduisons une méthode pour quantifier l’adoption de l’ORCID en fonction de la discipline et de la profession des chercheurs dans une organisation d’enseignement supérieur. Nous avons comparé semi-automatiquement les 6 607 collaborateurs affiliés aux 145 laboratoires de la zone scientifique toulousaine avec les 7,3 millions de profils sur orcid.org. L’adoption par l’ORCID de 41,8 % est accompagnée de disparités disciplinaires. De façon inattendue, seulement 48,3 % de tous les profils répertoriés au moins un travail et des profils sans travaux pourraient avoir été créés pour obtenir un identifiant. Ces profils «vides» présentent peu d’intérêt pour la tâche de désambiguation de l’entité. À notre connaissance, il s’agit de la première étude de l’adoption de l’ORCID à l’échelle d’une métropole scientifique pluridisciplinaire. Cette méthode est reproductible et les études futures peuvent cibler d’autres cas pour contraster la dynamique de l’adoption de l’ORCID dans le monde entier.","1076","4","133","Croissance d’ORCID et dynamique d’adoption sur le terrain: Une étude de cas de l’espace scientifique toulousain

Les systèmes d’information axés sur la recherche recueillent et promeuvent les résultats scientifiques des chercheurs. Il est essentiel de désambiguïser les identités des auteurs lorsqu’il s’agit de distinguer des homonymes afin d’éviter de fusionner les dossiers de plusieurs personnes. ORCID offre un identifiant pour relier son identité, ses affiliations et sa bibliographie. Alors que les organismes de financement et les éditeurs universitaires font la promotion d’ORCID, on sait peu de choses sur son taux d’adoption. Nous présentons une méthode pour quantifier l’adoption d’ORCID en fonction de la discipline et de la profession des chercheurs dans une organisation d’enseignement supérieur. Nous avons comparé semi-automatiquement les 6 607 collaborateurs affiliés aux 145 laboratoires de la zone scientifique toulousaine avec les 7,3 millions de profils sur orcid.org. L’adoption d’ORCID à 41,8 % est accompagnée de disparités disciplinaires. De façon inattendue, seulement 48,3 % de tous les profils répertoriés au moins un travail et des profils sans travaux pourraient avoir été créés pour obtenir un identifiant. Ces profils « vides » présentent peu d’intérêt pour la tâche de désambiguation de l’entité. À notre connaissance, il s’agit de la première étude de l’adoption d’ORCID à l’échelle d’une métropole scientifique pluridisciplinaire. Cette méthode est reproductible et les études futures peuvent cibler d’autres cas pour contraster la dynamique de l’adoption d’ORCID dans le monde entier.","4","2023-06-15 11:00:06","2023-06-15 11:37:53","1076",,"1","2","1","3","2","2",NULL,,"2023-06-15 10:59:58","0"
"3563308","In information retrieval (IR) systems, trends and users' interests may change over time, altering either the distribution of requests or contents to be recommended. Since neural ranking approaches heavily depend on the training data, it is crucial to understand the transfer capacity of recent IR approaches to address new domains in the long term. In this paper, we first propose a dataset based upon the MSMarco corpus aiming at modeling a long stream of topics as well as IR property-driven controlled settings. We then in-depth analyze the ability of recent neural IR models while continually learning those streams. Our empirical study highlights in which particular cases catastrophic forgetting occurs (e.g., level of similarity between tasks, peculiarities on text length, and ways of learning models) to provide future directions in terms of model design.","Continual Learning of Long Topic Sequences in Neural Information Retrieval","Thomas Gerald, Laure Soulier","4891","Apprentissage continu de longues séquences thématiques dans la recherche d'informations neuronales

Dans les systèmes de recherche d'informations, les tendances et les intérêts des utilisateurs peuvent changer au fil du temps, modifiant la distribution des demandes ou du contenu à recommander. Étant donné que les approches de classement neuronal dépendent fortement des données de formation, il est crucial de comprendre la capacité de transfert des approches IR récentes pour aborder de nouveaux domaines à long terme. Dans cet article, nous proposons d'abord un ensemble de données basé sur le corpus MSMarco visant à modéliser un long flux de sujets ainsi que des paramètres contrôlés par propriété IR. Nous analysons ensuite en profondeur la capacité des modèles récents d'IR neuronal tout en apprenant continuellement ces flux. Notre étude empirique met en évidence dans quels cas particuliers se produit l'oubli catastrophique (par exemple, niveau de similitude entre les tâches, particularités sur la longueur du texte, et façons d'apprendre les modèles) pour fournir des orientations futures en termes de conception de modèle.","16807","6","134","Apprentissage continu de longues séquences thématiques dans la recherche d&#039;informations neuronales

Dans les systèmes de recherche d&#039;information, les tendances et les intérêts des utilisateurs peuvent changer au fil du temps, modifiant la distribution des demandes ou du contenu à recommander. Étant donné que les approches de classement neuronal dépendent fortement des données de formation, il est crucial de comprendre la capacité de transfert des approches récentes de recherche d&#039;information pour aborder de nouveaux domaines à long terme. Dans cet article, nous proposons d&#039;abord un ensemble de données basé sur le corpus MSMarco visant à modéliser un long flux de sujets ainsi que des paramètres contrôlés par propriété de recherche d&#039;information. Nous analysons ensuite en profondeur la capacité des modèles neuronaux récents de recherche d&#039;information tout en apprenant continuellement ces flux. Notre étude empirique met en évidence dans quels cas particuliers se produit l&#039;oubli catastrophique (par exemple, niveau de similitude entre les tâches, particularités sur la longueur du texte, et façons d&#039;apprendre les modèles) pour fournir des orientations futures en termes de conception de modèle.","4","2023-06-15 11:59:00","2023-06-15 12:09:02","16807",,"1","1","3","2","1","2",NULL,,"2023-06-15 11:58:52","0"
"3943404","Topics play an important role in coherence in dialogue, as what is currently discussed constrains the possible contributions of the participants, and initiating a topic while the previous one is still under discussion may be confusing without appropriate signals. However, how to actually define the notion of topic is debated in linguistics and not sufficiently discussed in dialogue modelling.A precise description of topics and topic shifts in conversation would contribute to understanding what it is we perceive when we judge a sequence of utterances to be coherent. In particular, having more understanding of the notion of topic is important to account for language use which does not conform to our intuitions about topical coherence, such as can be found in interactions involving for example patients with schizophrenia. In patients interaction we often see difficulties to produce a coherent speech from the ordinary listener’s point of view. These difficulties can manifest at topic level where patients would use fewer markers to indicate a topic shift, or use unusual topical links such as sound similarity between two different concepts. Formalising topic transitions, and in particular more unusual ones, would thus contribute to identifying cues to support diagnosing schizophrenia.In this thesis we analyse several pieces of conversation containing topic shifts in order to understand the different mechanisms that license topic shifts in dialogue and the way participants acknowledge them. In particular, we investigate topic shifts based on extra-linguistic content and more unconventional topic shifts where speakers take advantage of word similarities to introduce a new topic with no semantic relation with the previous one. We apply two classical discourse modelling theories to our examples, Rhetorical Structure Theory and Segmented Discourse Representation Theory, and show their limits when it comes to modelling topics and topic shifts. In particular, the unique representation for a conversation that they produce does not make it possible to precisely understand the mechanisms that lead to a disruption. Hence, we use a more complex framework, Conversation Oriented Semantics, which is based on Type Theory with Records and enables us to represent each participant's representation of the conversation, as well as different forms of context. We suggest a way to add topics to these representations and account for topic shifts, including unconventional ones.","Topic Shifts: Preserving Comprehension in Conversation","Amandine Decker","400","Changement de sujet : Préserver la compréhension dans la conversation

Les sujets jouent un rôle important dans la cohérence du dialogue, car ce qui est discuté actuellement limite les contributions possibles des participants, et le fait de lancer un sujet alors que le précédent est toujours en cours de discussion peut être source de confusion en l'absence de signaux appropriés. Une description précise des sujets et des changements de sujet dans une conversation permettrait de mieux comprendre ce que nous percevons lorsque nous jugeons qu'une séquence d'énoncés est cohérente. En particulier, il est important de mieux comprendre la notion de sujet pour rendre compte de l'utilisation du langage qui n'est pas conforme à nos intuitions sur la cohérence topique, comme on peut le trouver dans les interactions impliquant par exemple des patients atteints de schizophrénie. Dans les interactions entre patients, on observe souvent des difficultés à produire un discours cohérent du point de vue de l'auditeur ordinaire. Ces difficultés peuvent se manifester au niveau du sujet, lorsque les patients utilisent moins de marqueurs pour indiquer un changement de sujet, ou utilisent des liens topiques inhabituels tels que la similarité sonore entre deux concepts différents. Dans cette thèse, nous analysons plusieurs morceaux de conversation contenant des changements de sujet afin de comprendre les différents mécanismes qui autorisent les changements de sujet dans le dialogue et la façon dont les participants les reconnaissent. En particulier, nous étudions les changements de sujet basés sur le contenu extra-linguistique et les changements de sujet moins conventionnels où les locuteurs profitent des similitudes de mots pour introduire un nouveau sujet sans relation sémantique avec le précédent. Nous appliquons deux théories classiques de modélisation du discours à nos exemples, la théorie de la structure rhétorique et la théorie de la représentation segmentée du discours, et nous montrons leurs limites lorsqu'il s'agit de modéliser les sujets et les changements de sujet. En particulier, la représentation unique d'une conversation qu'elles produisent ne permet pas de comprendre précisément les mécanismes qui conduisent à une rupture. Nous utilisons donc un cadre plus complexe, la Sémantique Orientée Conversation, qui s'appuie sur la Théorie des Types avec Enregistrements et permet de représenter la représentation de la conversation par chaque participant, ainsi que différentes formes de contexte. Nous proposons un moyen d'ajouter des sujets à ces représentations et de prendre en compte les changements de sujets, y compris ceux qui ne sont pas conventionnels.","10154","3","182","Changement de sujet : Préserver la compréhension dans la conversation

Les sujets jouent un rôle important dans la cohérence du dialogue, car ce qui est discuté à un moment donné limite les contributions possibles des participants, et le fait de lancer un sujet alors que le précédent est toujours en cours de discussion peut être source de confusion en l&#039;absence de signaux appropriés. Une description précise des sujets et des changements de sujet dans une conversation permettrait de mieux comprendre ce que nous percevons lorsque nous jugeons qu&#039;une séquence d&#039;énoncés est cohérente. En particulier, il est important de mieux comprendre la notion de sujet pour rendre compte de l&#039;utilisation du langage qui n&#039;est pas conforme à nos intuitions sur la cohérence topique, comme on peut le trouver dans les interactions impliquant par exemple des patients atteints de schizophrénie. Dans les interactions entre patients, on observe souvent des difficultés à produire un discours cohérent du point de vue de l&#039;auditeur ordinaire. Ces difficultés peuvent se manifester au niveau du sujet, lorsque les patients utilisent moins de marqueurs pour indiquer un changement de sujet, ou utilisent des liens topiques inhabituels tels que la similarité sonore entre deux concepts différents. Dans cette thèse, nous analysons plusieurs extraits de conversation contenant des changements de sujet afin de comprendre les différents mécanismes qui autorisent les changements de sujet dans le dialogue et la façon dont les participants les reconnaissent. En particulier, nous étudions les changements de sujet basés sur le contenu extra-linguistique et les changements de sujet moins conventionnels où les locuteurs profitent des similitudes entre les mots pour introduire un nouveau sujet sans relation sémantique avec le précédent. Nous appliquons deux théories classiques de modélisation du discours à nos exemples, la théorie de la structure rhétorique et la théorie de la représentation segmentée du discours, et nous montrons leurs limites lorsqu&#039;il s&#039;agit de modéliser les sujets et les changements de sujet. En particulier, la représentation unique d&#039;une conversation qu&#039;elles produisent ne permet pas de comprendre précisément les mécanismes qui conduisent à une rupture. Nous utilisons donc un cadre plus complexe, la Sémantique Orientée Conversation, qui s&#039;appuie sur la Théorie des Types avec Enregistrements et permet de représenter la représentation de la conversation par chaque participant, ainsi que différentes formes de contexte. Nous proposons un moyen d&#039;ajouter des sujets à ces représentations et de prendre en compte les changements de sujets, y compris ceux qui ne sont pas conventionnels.","4","2023-06-21 08:58:34","2023-06-21 09:26:59","10154",,"1","1","2","2","1","1",NULL,,"2023-06-21 08:58:22","0"
"3840304","Facebook reviews contain reviews and reviewers' information and include a set of likes, comments, sharing, and reactions called Facebook Behaviors (FBs). We extend existing research on review helpfulness to fit Facebook reviews by demonstrating that Facebook behaviors can impact review helpfulness. This study proposes a theoretical model that explains reviews' helpfulness based on FBs and baseline features. The model is empirically validated using a real Facebook data set and different feature selection methods (FS) to determine the importance level of such features to maximize the helpfulness prediction. Consequently, a combination of the impactful features is identified based on a robust and effective model. In this context, the like and love behaviors deliver the best predictive performance. Furthermore, we employ different classification techniques and a set of influencer features. The results showed the performance of the proposed model by 0.925 of accuracy.The outcomes of the current study can be applied to develop a smart review ranking system for Facebook product pages.","The effect of Facebook behaviors on the prediction of review helpfulness","Emna Ben-Abdallah, Khouloud Boukadi","4178","L'effet des comportements sur Facebook sur la prédiction de l'utilité des critiques

Les évaluations Facebook contiennent des évaluations et des informations sur les évaluateurs, ainsi qu'un ensemble d'appréciations, de commentaires, de partages et de réactions appelés ""comportements Facebook"" (FB). Nous étendons les recherches existantes sur l'utilité des évaluations aux évaluations sur Facebook en démontrant que les comportements sur Facebook peuvent avoir un impact sur l'utilité des évaluations. Cette étude propose un modèle théorique qui explique l'utilité des critiques en fonction des comportements Facebook et des caractéristiques de base. Le modèle est validé empiriquement à l'aide d'un ensemble de données Facebook réelles et de différentes méthodes de sélection des caractéristiques (FS) afin de déterminer le niveau d'importance de ces caractéristiques pour maximiser la prédiction de l'utilité. Par conséquent, une combinaison des caractéristiques ayant un impact est identifiée sur la base d'un modèle robuste et efficace. Dans ce contexte, les comportements ""like"" et ""love"" offrent les meilleures performances prédictives. En outre, nous utilisons différentes techniques de classification et un ensemble de caractéristiques d'influence. Les résultats de cette étude peuvent être appliqués au développement d'un système intelligent de classement des critiques pour les pages de produits sur Facebook.","17189","3","183","L&#039;effet des comportements Facebook sur la prédiction de l&#039;utilité des critiques

Les évaluations Facebook contiennent des évaluations et des informations sur les évaluateurs, ainsi qu&#039;un ensemble d&#039;appréciations, de commentaires, de partages et de réactions appelés &quot;comportements Facebook&quot;. Nous étendons les recherches existantes sur l&#039;utilité des évaluations en les appliquant aux évaluations sur Facebook, en démontrant que les comportements Facebook peuvent avoir un impact sur l&#039;utilité des évaluations. Cette étude propose un modèle théorique qui explique l&#039;utilité des critiques en fonction des comportements Facebook et des caractéristiques de base. Le modèle est validé empiriquement à l&#039;aide d&#039;un ensemble de données Facebook réelles et de différentes méthodes de sélection des caractéristiques afin de déterminer le niveau d&#039;importance de ces caractéristiques pour maximiser la prédiction de l&#039;utilité. Par conséquent, une combinaison des caractéristiques ayant un impact est identifiée sur la base d&#039;un modèle robuste et efficace. Dans ce contexte, les comportements &quot;like&quot; et &quot;love&quot; offrent les meilleures performances prédictives. En outre, nous utilisons différentes techniques de classification et un ensemble de caractéristiques d&#039;influence. Les résultats de cette étude peuvent être appliqués au développement d&#039;un système intelligent de classement des critiques pour les pages de produits sur Facebook.","4","2023-06-21 11:39:16","2023-06-21 12:13:48","17189",,"2","2","2","3","1","3",NULL,,"2023-06-21 11:39:11","0"
"3666567","As the amount of audiovisual content increases, the need to develop automatic captioning and subtitling solutions to match the expectations of a growing international audience appears as the only viable way to boost throughput and lower the related postproduction costs. Automatic captioning and subtitling often need to be tightly intertwined to achieve an appropriate level of consistency and synchronization with each other and with the video signal. In this work, we assess a dual decoding scheme to achieve a strong coupling between these two tasks and show how adequacy and consistency are increased, with virtually no additional cost in terms of model size and training complexity.","Joint Generation of Captions and Subtitles with Dual Decoding","Jitao Xu, François Buet, Josep Crego, Elise Bertin-Lemée, François Yvon","746","Génération conjointe de sous-titres et de sous-titres avec double décoration

À mesure que la quantité de contenu audiovisuel augmente, la nécessité de développer des solutions de sous-titrage et de sous-titrage automatiques pour répondre aux attentes d’un public international croissant apparaît comme le seul moyen viable d’augmenter le débit et de réduire les coûts de postproduction associés. Le sous-titrage automatique et le sous-titrage doivent souvent être étroitement liés pour atteindre un niveau approprié de cohérence et de synchronisation les uns avec les autres et avec le signal vidéo. Dans ce travail, nous évaluons un double système de décodage pour parvenir à un couplage fort entre ces deux tâches et montrons comment l’adéquation et la cohérence sont accrues, avec pratiquement aucun coût supplémentaire en termes de taille du modèle et de complexité de la formation.","1017","4","191","Génération conjointe de sous-titres classiques et de sous-titres pour sourds et malentendants avec double décodage

À mesure que la quantité de contenu audiovisuel augmente, la nécessité de développer des solutions de sous-titrage et de sous-titrage pour sourds et malentendants automatiques pour répondre aux attentes d’un public international croissant apparaît comme le seul moyen viable d’augmenter le débit et de réduire les coûts de postproduction associés. Le sous-titrage classique et le sous-titrage pour sourds et malentendants automatiques doivent souvent être étroitement liés pour atteindre un niveau approprié de cohérence et de synchronisation les uns avec les autres et avec le signal vidéo. Dans ce travail, nous évaluons un double système de décodage pour parvenir à un couplage fort entre ces deux tâches et montrons comment l’adéquation et la cohérence sont accrues, avec pratiquement aucun coût supplémentaire en termes de taille du modèle et de complexité de la formation.","4","2023-06-22 15:59:20","2023-06-22 16:24:20","1017",,"3","1","4","2","1","2",NULL,,"2023-06-22 03:59:14","0"
"3363795","This work combines semantic reasoning and machine learning to create tools that allow curators of the visual art collections to identify and correct the annotations of the artwork as well as to improve the relevance of the content-based search results in these collections. The research is based on the Joconde database maintained by French Ministry of Culture that contains illustrated artwork records from main French public and private museums representing archeological objects, decorative arts, fine arts, historical and scientific documents, etc. The Joconde database includes semantic metadata that describes properties of the artworks and their content. The developed methods create a data pipeline that processes metadata, trains a Convolutional Neural Network image classification model, makes prediction for the entire collection and expands the metadata to be the base for the SPARQL search queries. We developed a set of such queries to identify noise and silence in the human annotations and to search image content with results ranked according to the relevance of the objects quantified by the prediction score provided by the deep learning model. We also developed methods to discover new contextual relationships between the concepts in the metadata by analyzing the contrast between the concepts similarities in the Joconde's semantic model and other vocabularies and we tried to improve the model prediction scores based on the semantic relations. Our results show that cross-fertilization between symbolic AI and machine learning can indeed provide the tools to address the challenges of the museum curators work describing the artwork pieces and searching for the relevant images.","Learning and Reasoning for Cultural Metadata Quality","Anna Bobasheva, Fabien Gandon, Frédéric Precioso","5350","Apprentissage et raisonnement pour la qualité des métadonnées culturelles

Ce travail combine le raisonnement sémantique et l'apprentissage automatique pour créer des outils qui permettent aux conservateurs des collections d'art visuel d'identifier et de corriger les annotations des œuvres d'art ainsi que d'améliorer la pertinence des résultats de recherche basés sur le contenu dans ces collections. La recherche s'appuie sur la base de données Joconde, gérée par le ministère français de la culture, qui contient des notices d'œuvres d'art illustrées provenant des principaux musées publics et privés français et représentant des objets archéologiques, des arts décoratifs, des beaux-arts, des documents historiques et scientifiques, etc. La base de données Joconde comprend des métadonnées sémantiques qui décrivent les propriétés des œuvres d'art et leur contenu. Les méthodes développées créent un pipeline de données qui traite les métadonnées, entraîne un modèle de classification d'images par réseau neuronal convolutif, fait des prédictions pour l'ensemble de la collection et développe les métadonnées pour servir de base aux requêtes de recherche SPARQL. Nous avons développé un ensemble de requêtes de ce type pour identifier le bruit et le silence dans les annotations humaines et pour rechercher le contenu des images avec des résultats classés en fonction de la pertinence des objets quantifiée par le score de prédiction fourni par le modèle d'apprentissage profond. Nous avons également développé des méthodes pour découvrir de nouvelles relations contextuelles entre les concepts dans les métadonnées en analysant le contraste entre les similitudes des concepts dans le modèle sémantique de Joconde et d'autres vocabulaires et nous avons essayé d'améliorer les scores de prédiction du modèle sur la base des relations sémantiques. Nos résultats montrent que la fertilisation croisée entre l'IA symbolique et l'apprentissage automatique peut effectivement fournir les outils nécessaires pour relever les défis du travail des conservateurs de musée en décrivant les œuvres d'art et en recherchant les images pertinentes.","19837","3","192","Apprentissage et raisonnement pour la qualité des métadonnées culturelles

Cet article combine le raisonnement sémantique et l&#039;apprentissage automatique pour créer des outils qui permettent aux conservateurs des collections d&#039;art visuel d&#039;identifier et de corriger les annotations des œuvres d&#039;art ainsi que d&#039;améliorer la pertinence des résultats de recherche basés sur le contenu dans ces collections. La recherche s&#039;appuie sur la base de données Joconde, gérée par le ministère de la Culture français, qui contient des notices d&#039;œuvres d&#039;art illustrées provenant des principaux musées publics et privés français et représentant des objets archéologiques, des arts décoratifs, des beaux-arts, des documents historiques et scientifiques, etc. La base de données Joconde comprend des métadonnées sémantiques qui décrivent les propriétés des œuvres d&#039;art et leur contenu. Les méthodes développées créent un pipeline de données qui traite les métadonnées, entraîne un modèle de classification d&#039;images par réseau de neurones convolutifs, fait des prédictions pour l&#039;ensemble de la collection et développe les métadonnées pour servir de base aux requêtes de recherche SPARQL. Nous avons développé un ensemble de requêtes de ce type pour identifier le bruit et le silence dans les annotations humaines et pour rechercher le contenu des images avec des résultats classés en fonction de la pertinence des objets quantifiée par le score de prédiction fourni par le modèle d&#039;apprentissage profond. Nous avons également développé des méthodes pour découvrir de nouvelles relations contextuelles entre les concepts dans les métadonnées en analysant le contraste entre les similitudes des concepts dans le modèle sémantique de Joconde et d&#039;autres vocabulaires et nous avons essayé d&#039;améliorer les scores de prédiction du modèle sur la base des relations sémantiques. Nos résultats montrent que la fertilisation croisée entre l&#039;IA symbolique et l&#039;apprentissage automatique peut effectivement fournir les outils nécessaires pour relever les défis du travail des conservateurs de musée en décrivant les œuvres d&#039;art et en recherchant les images pertinentes.","4","2023-06-22 16:50:37","2023-06-22 17:36:01","19837",,"1","1","2","1","1","1",NULL,,"2023-06-22 04:50:20","0"
"3621550","In emergency medical procedures, positive and trusting interactions between followers and leaders are imperative. That interaction is even more important when a virtual agent assumes the leader role and a human assumes the follower role. In order to manage the human-computer interaction, situational leadership is employed to match the human follower to an appropriate leadership style embodied by the agent. Situational leadership was used to create 33 utterances indicative of the four different leadership styles. A participant evaluation was then carried out in order to examine (1) whether perceptions of leader trust and motivation vary dependent on both readiness level and utterance syntax and (2) whether follower ability and willingness are affected by the leader’s speech. We found that general perceptions of leadership behavior influenced follower performance and that the leader’s speech influences followers’ ability. Finally, we demonstrate how the results of this study are implemented in a virtual agent system.","Speech Perception and Implementation in a Virtual Medical Assistant","Aryana Collins Jackson, Yann Glémarec, Elisabetta Bevacqua, Pierre de Loor, Ronan Querrec","4781","Perception de la parole et implémentation dans un assistant médical virtuel

Dans les procédures médicales d'urgence, les interactions positives et de confiance entre les suiveurs et les dirigeants sont impératives. Cette interaction est encore plus importante lorsqu'un agent virtuel joue le rôle de leader et qu'un humain joue le rôle de suiveur. Afin de gérer l'interaction homme-ordinateur, le leadership situationnel est utilisé pour faire correspondre le suiveur humain à un style de leadership approprié incarné par l'agent. Le leadership situationnel a été utilisé pour créer 33 énoncés indiquant les quatre styles de leadership différents. Une évaluation des participants a ensuite été réalisée afin d'examiner (1) si les perceptions de la confiance et de la motivation du leader varient en fonction du niveau de préparation et de la syntaxe de l'énoncé et (2) si la capacité et la volonté du suiveur sont affectées par le discours du leader. Nous avons constaté que les perceptions générales du comportement du leader influençaient la performance des suiveurs et que le discours du leader influençait la capacité des suiveurs. Enfin, nous démontrons comment les résultats de cette étude sont mis en œuvre dans un système d'agent virtuel.","19251","3","193","Perception de la parole et implémentation dans un assistant médical virtuel

Dans les procédures médicales d&#039;urgence, les interactions positives et de confiance entre les followers et les leaders sont impératives. Cette interaction est encore plus importante lorsqu&#039;un agent virtuel joue le rôle de leader et qu&#039;un humain joue le rôle de follower. Afin de gérer l&#039;interaction homme-ordinateur, le leadership situationnel est utilisé pour faire correspondre le follower humain à un style de leadership approprié incarné par l&#039;agent. Le leadership situationnel a été utilisé pour créer 33 énoncés indiquant les quatre styles de leadership différents. Une évaluation des participants a ensuite été réalisée afin d&#039;examiner (1) si les perceptions de la confiance et de la motivation du leader varient en fonction du niveau de préparation et de la syntaxe de l&#039;énoncé et (2) si la capacité et la volonté du follower sont affectées par le discours du leader. Nous avons constaté que les perceptions générales du comportement du leader influençaient la performance des followers et que le discours du leader influençait la capacité des followers. Enfin, nous démontrons comment les résultats de cette étude sont mis en œuvre dans un système d&#039;agent virtuel.","4","2023-06-23 12:17:09","2023-06-23 13:10:35","19251",,"1","1","3","1","1","1",NULL,"en RH, emprunts à l&#039;anglais : leader et follower","2023-06-23 12:17:04","0"
"3539754","In this paper, we aim at analyzing the differences between three families of divergences used to compare probability density functions of Gaussian random vectors storing k consecutive samples of wide-sense stationary ARMA processes. There may be various applications: signal classification, statistical change detection, etc. Among the families that are studied, we propose to look at the α-divergence, the β-divergence and the γ-divergence. We first provide the expression of the divergences in the Gaussian case and then express their divergence increments, i.e. the differences between the divergences computed for k + 1 and k consecutive samples. Finally, we analyze how these divergence increments evolve when k increases and tends to infinity.","Studying Three Families Of Divergences To Compare Wide-Sense Stationary Gaussian ARMA Processes","Eric Grivel","4973","Etude de trois familles de divergences pour comparer des processus ARMA gaussiens stationnaires à sens large

Dans cet article, nous visons à analyser les différences entre trois familles de divergences utilisées pour comparer les fonctions de densité de probabilité de vecteurs aléatoires gaussiens stockant k échantillons consécutifs de processus ARMA stationnaires à sens large. Les applications peuvent être diverses : classification des signaux, détection statistique des changements, etc. Parmi les familles étudiées, nous proposons d'examiner la α-divergence, la β-divergence et la γ-divergence. Nous fournissons d'abord l'expression des divergences dans le cas gaussien, puis nous exprimons leurs incréments de divergence, c'est-à-dire les différences entre les divergences calculées pour k + 1 et k échantillons consécutifs. Enfin, nous analysons l'évolution de ces incréments de divergence lorsque k augmente et tend vers l'infini.","17625","3","194","Etude de trois familles de divergences pour comparer des processus ARMA gaussiens stationnaires à sens large

Dans cet article, nous visons à analyser les différences entre trois familles de divergences utilisées pour comparer les fonctions de densité de probabilité de vecteurs aléatoires gaussiens stockant k échantillons consécutifs de processus ARMA stationnaires à sens large. Les applications peuvent être diverses : classification des signaux, détection statistique des changements, etc. Parmi les familles étudiées, nous proposons d&#039;examiner la divergence α, la divergence β et la γ-divergence γ. Nous fournissons d&#039;abord l&#039;expression des divergences dans le cas gaussien, puis nous exprimons leurs incréments de divergence, c&#039;est-à-dire les différences entre les divergences calculées pour k + 1 et k échantillons consécutifs. Enfin, nous analysons l&#039;évolution de ces incréments de divergence lorsque k augmente et tend vers l&#039;infini.","4","2023-06-23 13:10:50","2023-06-23 13:17:37","17625",,"1","1","1","1","2","1",NULL,,"2023-06-23 01:10:40","0"
"3679646","While fuzzy methods, and in particular fuzzy rule based methods, have been pointed out as explainable, it is not always easy to attach a linguistic label to the conclusion provided by a rule-based system for a given observation. In this paper, we focus on the case of sparse rules, with impreciseor linguistic premises and conclusions, and their use with imprecise or linguistic observations. We explore fuzzy solutions of interpolative reasoning based on analogies, with regard to desirable mathematical properties and explainability criteria. We first recall such criteria existing in the state of the art and we analyse them in the light of explainable Artificial Intelligence (AI) requirements. We then propose a new method making easierto explain both the result of the fuzzy interpolative reasoning and the approach used to construct it. A set of experimental comparisons with some existing fuzzy interpolative reasoning approaches is presented.","Explainable Fuzzy Interpolative Reasoning","Christophe Marsala, Bernadette Bouchon-Meunier","4636","Raisonnement interpolatif flou explicable

Bien que les méthodes floues, et en particulier les méthodes fondées sur des règles floues, aient été considérées comme explicables, il n'est pas toujours facile d'attacher une étiquette linguistique à la conclusion fournie par un système fondé sur des règles pour une observation donnée. Dans cet article, nous nous concentrons sur le cas des règles peu nombreuses, avec des prémisses et des conclusions imprécises ou linguistiques, et sur leur utilisation avec des observations imprécises ou linguistiques. Nous explorons des solutions floues de raisonnement interpolatif basées sur des analogies, en ce qui concerne les propriétés mathématiques souhaitables et les critères d'explicabilité. Nous rappelons d'abord ces critères existant dans l'état de l'art et nous les analysons à la lumière des exigences de l'Intelligence Artificielle (IA) explicable. Nous proposons ensuite une nouvelle méthode qui facilite l'explication du résultat du raisonnement interpolatif flou et de l'approche utilisée pour le construire. Une série de comparaisons expérimentales avec certaines approches de raisonnement interpolatif flou existantes est présentée.","19106","3","195","Raisonnement par interpolation flou explicable

Bien que les méthodes floues, et en particulier les méthodes fondées sur des règles floues, aient été considérées comme explicables, il n&#039;est pas toujours facile d&#039;attacher une étiquette linguistique à la conclusion fournie par un système fondé sur des règles pour une observation donnée. Dans cet article, nous nous concentrons sur le cas des règles peu nombreuses, avec des prémisses et des conclusions imprécises ou linguistiques, et sur leur utilisation avec des observations imprécises ou linguistiques. Nous explorons des solutions floues de raisonnement par interpolation basées sur des analogies, en ce qui concerne les propriétés mathématiques souhaitables et les critères d&#039;explicabilité. Nous rappelons d&#039;abord ces critères existant dans l&#039;état de l&#039;art et nous les analysons à la lumière des exigences de l&#039;Intelligence Artificielle (IA) explicable. Nous proposons ensuite une nouvelle méthode qui facilite l&#039;explication du résultat du raisonnement par interpolation flou et de l&#039;approche utilisée pour le construire. Une série de comparaisons expérimentales avec certaines approches de raisonnement par interpolation flou existantes est présentée.","4","2023-06-23 13:17:58","2023-06-23 13:30:15","19106",,"1","1","3","3","1","2",NULL,"interpolative : corpus TALN &quot;par interpolation&quot;. Reverso donne des contextes traduits par &quot;interpolatif&quot; (rare et semble être traduit automatiquement) et &quot;à interpolation&quot; mais il semble que ce soit des noms de composants électriques (&quot;modulateur à interpolation&quot;)","2023-06-23 01:17:41","0"
"3911564","Over the past few years, domain specific pretrained language models have been investigated and have shown remarkable achievements in different downstream tasks, especially in biomedical domain. These achievements stem on the well known BERT architecture which uses an attention based selfsupervision for context learning of textual documents. However, these domain specific biomedical pretrained language models mainly use English corpora. Therefore, non-English, domain-specific pretrained models remain quite rare, both of these requirements being hard to achieve. In this work, we proposed AliBERT, a biomedical pretrained language model for French and investigated different learning strategies. AliBERT is trained using regularized Unigram based tokenizer trained for this purpose. AliBERT has achieved state of the art F1 and accuracy scores in different downstream biomedical tasks. Our pretrained model manages to outperform some French non domain-specific models such as CamemBERT and FlauBERT on diverse downstream tasks, with less pretraining and training time and with much less corpora.","ALIBERT: A PRETRAINED LANGUAGE MODEL FOR FRENCH BIOMEDICAL TEXT A PREPRINT","Aman Berhe, Guillaume Draznieks, Vincent Martenot, Valentin Masdeu, Lucas Davy, Jean-Daniel Zucker","3982","ALIBERT: UN MODÈLE LINGUISTIQUE PRÉ-FORMÉ POUR LE TEXTE BIOMÉDICAL FRANÇAIS

Au cours des dernières années, des modèles linguistiques préformés spécifiques à un domaine ont été étudiés et ont montré des réalisations remarquables dans différentes tâches en aval, en particulier dans le domaine biomédical. Ces réalisations découlent de l’architecture BERT bien connue qui utilise une autosurveillance basée sur l’attention pour l’apprentissage contextuel des documents textuels. Cependant, ces modèles de langues biomédicales préformées spécifiques à un domaine utilisent principalement des corpus anglais. Par conséquent, les modèles préformés non anglais et spécifiques à un domaine restent assez rares, ces deux exigences étant difficiles à atteindre. Dans ce travail, nous avons proposé Alibert, un modèle de langue pré-formée biomédicale pour le français et nous avons étudié différentes stratégies d’apprentissage. Alibert est formé à l’aide de tokenizer basé sur Unigram régularisé formé à cet effet. Alibert a atteint l’état de l’art de la F1 et des scores de précision dans différentes tâches biomédicales en aval. Notre modèle préformé parvient à surpasser certains modèles français non spécifiques au domaine tels que Camembert et FlauBERT sur diverses tâches en aval, avec moins de temps de préformation et de formation et avec beaucoup moins de corpus.","12940","4","197","ALIBERT : UN MODÈLE LINGUISTIQUE PRÉ-ENTRAÎNÉ POUR LE TEXTE BIOMÉDICAL FRANÇAIS - PRÉ-PUBLICATION

Au cours des dernières années, des modèles linguistiques pré-entraînés spécifiques à un domaine ont été étudiés et ont montré des réalisations remarquables dans différentes tâches en aval, en particulier dans le domaine biomédical. Ces réalisations découlent de l’architecture BERT bien connue qui utilise une autosurveillance basée sur l’attention pour l’apprentissage contextuel des documents textuels. Cependant, ces modèles de langues biomédicales pré-entraînées spécifiques à un domaine utilisent principalement des corpus anglais. Par conséquent, les modèles pré-entrapînés non anglais et spécifiques à un domaine restent assez rares, ces deux exigences étant difficiles à atteindre. Dans ce travail, nous avons proposé Alibert, un modèle pré-entraîné de langue biomédicale pour le français et nous avons étudié différentes stratégies d’apprentissage. Alibert est formé à l’aide de tokeniseur basé sur Unigram régularisé formé à cette fin. Alibert a atteint l’état de l’art de la F1 et des scores de précision dans différentes tâches biomédicales en aval. Notre modèle pré-entraîné parvient à surpasser certains modèles français non spécifiques au domaine tels que Camembert et FlauBERT sur diverses tâches en aval, avec moins de temps de préformation et de formation et avec beaucoup moins de corpus.","4","2023-06-23 13:30:32","2023-06-23 13:41:48","12940",,"3","2","3","2","1","2",NULL,,"2023-06-23 01:30:20","0"
"3547539","The purpose of this study was to investigate the effect of corpus augmentation on the quality of English-Amharic Machine Translation (MT). In fact, trigram and four-gram Statistical Machine Translation (SMT) language models, as well as Neural Machine Translation (NMT) models based on Gated Recurrent Units (GRU) were used. They were trained independently using both the original and augmented corpus to see how the augmentation of the corpus affects the translation quality of these models. These two corpora (original and augmented) contain 225,304 and 463,796 English-Amharic parallel sentences respectively. To complete the corpus augmentation challenge, an offline token level tokenization technique was used. This technique (corpus augmentation) was used before any other MT processes were started. Among several token-level tokenization mechanisms, random insertion, replacement, deletion, and swapping approaches were chosen and implemented. After both models had been trained, the Bilingual Evaluation Understudy (BLEU) ratings were collected and analyzed. Our results demonstrate that the models trained with the augmented corpus outperform their corresponding models (models trained with the original corpus) in terms of BLEU scores. As a result, we can conclude that corpus augmentation did indeed help in the improvement of the performance of both SMT and NMT translation systems.","Offline Corpus Augmentation for English-Amharic Machine Translation","Yohannes Biadgligne, Kamel Smaïli","4948","Augmentation du corpus hors ligne pour la traduction automatique anglais-amharique

Le but de cette étude était d'étudier l'effet de l'augmentation du corpus sur la qualité de la traduction automatique anglais-amharique (MT). En fait, des modèles de langage de traduction automatique statistique (SMT) de trigramme et de quatre grammes, ainsi que des modèles de traduction automatique neuronale (NMT) basés sur des unités récurrentes à portes (GRU) ont été utilisés. Ils ont été formés indépendamment en utilisant à la fois le corpus original et augmenté pour voir comment l'augmentation du corpus affecte la qualité de traduction de ces modèles. Ces deux corpus (original et augmenté) contiennent respectivement 225 304 et 463 796 phrases anglaises-amhariques parallèles. Pour effectuer la stimulation d'augmentation du corpus, une technique de tokenisation de niveau jeton hors connexion a été utilisée. Cette technique (augmentation du corpus) a été utilisée avant le démarrage de tout autre processus MT. Parmi plusieurs mécanismes de tokenisation au niveau jeton, des approches d'insertion, de remplacement, de suppression et de permutation aléatoires ont été choisies et mises en oeuvre. Une fois les deux modèles formés, les cotes de la Bilingual Evaluation Understudy (BLEU) ont été recueillies et analysées. Nos résultats montrent que les modèles formés avec le corpus augmenté surpassent leurs modèles correspondants (modèles formés avec le corpus original) en termes de scores BLEU. Par conséquent, nous pouvons conclure que l'augmentation du corpus a effectivement aidé à améliorer les performances des systèmes de traduction SMT et NMT.","16864","6","201","Augmentation du corpus hors ligne pour la traduction automatique anglais-amharique

Le but de cette étude est d&#039;étudier l&#039;effet de l&#039;augmentation du corpus sur la qualité de la traduction automatique (MT) anglais-amharique. En fait, des modèles de langage de traduction automatique statistique (SMT) de trigramme et de quadrigrammes, ainsi que des modèles de traduction automatique neuronale (NMT) basés sur des Gated Recurrent Units (GRU) ont été utilisés. Ils ont été formés indépendamment en utilisant à la fois le corpus original et augmenté pour voir comment l&#039;augmentation du corpus affecte la qualité de traduction de ces modèles. Ces deux corpus (original et augmenté) contiennent respectivement 225 304 et 463 796 segments anglais-amhariques parallèles. Pour effectuer le défi d&#039;augmentation du corpus, une technique de tokenisation de niveau token hors connexion a été utilisée. Cette technique (augmentation du corpus) a été utilisée avant le démarrage de tout autre processus de MT. Parmi plusieurs mécanismes de tokenisation au niveau token, des approches d&#039;insertion, de remplacement, de suppression et de permutation aléatoires ont été choisies et mises en œuvre. Une fois les deux modèles formés, les cotes de la Bilingual Evaluation Understudy (BLEU) ont été recueillies et analysées. Nos résultats montrent que les modèles formés avec le corpus augmenté surpassent leurs modèles correspondants (modèles formés avec le corpus original) en termes de scores BLEU. Par conséquent, nous pouvons conclure que l&#039;augmentation du corpus a effectivement aidé à améliorer les performances des systèmes de traduction SMT et NMT.","4","2023-06-23 13:47:47","2023-06-23 14:26:41","16864",,"1","2","3","2","2","1",NULL,,"2023-06-23 01:43:17","0"
"3796641","This paper proposes a visuo-auditory substitution method to assist visually impaired people in scene understanding. Our approach focuses on person localisation in the user's vicinity in order to ease urban walking. Since a real-time and low-latency is required in this context for user's security, we propose an embedded system. The processing is based on a lightweight convolutional neural network to perform an efficient 2D person localisation. This measurement is enhanced with the corresponding person depth information, and is then transcribed into a stereophonic signal via a head-related transfer function. A GPU-based implementation is presented that enables a real-time processing to be reached at 23 frames/s on a 640x480 video stream. We show with an experiment that this method allows for a real-time accurate audio-based localization.","Low-Latency Human-Computer Auditory Interface Based on Real-Time Vision Analysis","Florian Scalvini, Camille Bordeau, Maxime Ambard, Cyrille Migniot, Julien Dubois","4295","Interface auditive homme-ordinateur à faible latence basée sur une analyse de la vision en temps réel

Cet article propose une méthode de substitution visuo-auditive pour aider les personnes malvoyantes à comprendre la scène. Notre approche se concentre sur la localisation de la personne dans le voisinage de l'utilisateur afin de faciliter la marche urbaine. Etant donné qu'un temps réel et une faible latence sont nécessaires dans ce contexte pour la sécurité de l'utilisateur, nous proposons un système embarqué. Le traitement est basé sur un réseau neuronal convolutif léger pour effectuer une localisation de personne en 2D efficace. Cette mesure est améliorée avec l'information de profondeur de personne correspondante, puis est transcrite en un signal stéréophonique via une fonction de transfert associée à la tête. L'invention concerne une implémentation basée sur un processeur graphique qui permet d'atteindre un traitement en temps réel à 23 images/s sur un flux vidéo 640x480. Nous montrons avec une expérience que cette méthode permet une localisation audio précise en temps réel.","16211","6","206","Interface auditive homme-ordinateur à faible latence basée sur une analyse de la vision en temps réel

Cet article propose une méthode de substitution visuo-auditive pour aider les personnes malvoyantes à comprendre la scène. Notre approche se concentre sur la localisation de la personne dans le voisinage de l&#039;utilisateur afin de faciliter la marche urbaine. Étant donné qu&#039;un temps réel et une faible latence sont nécessaires dans ce contexte pour la sécurité de l&#039;utilisateur, nous proposons un système embarqué. Le traitement est basé sur un réseau de neurones convolutifs léger pour effectuer une localisation en 2D de personne efficace. Cette mesure est améliorée avec l&#039;information de profondeur de personne correspondante, puis est transcrite en un signal stéréophonique via une fonction de transfert associée à la tête. L&#039;invention concerne une implémentation basée sur un processeur graphique qui permet d&#039;atteindre un traitement en temps réel à 23 images par seconde sur un flux vidéo 640x480. Nous montrons avec une expérience que cette méthode permet une localisation audio précise en temps réel.","4","2023-06-24 10:01:52","2023-06-24 10:05:52","16211",,"1","1","2","1","2","1",NULL,,"2023-06-24 10:00:51","0"
"3787501","Education is a complex domain where students must make their curricula choices carefully. To model the intricacies of the educational domain, ontologies have successfully been leveraged in the past. However, no available ontology or dataset directly addresses the critical transition from high school to university from a decision-making perspective. Therefore, the contribution of our ongoing work is twofold. Firstly, we introduce EducOnto - an ontology that aims at modeling university curricula and students' profiles. Secondly, we introduce EduKG - a knowledge graph inheriting the semantics of EducOnto and instantiated with data about French students and curricula.","New Ontology and Knowledge Graph for University Curriculum Recommendation","Nicolas Hubert, Armelle Brun, Davy Monticolo","4337","Nouveau graphique de l’ontologie et des connaissances pour la recommandation du programme d’études universitaires

L’éducation est un domaine complexe où les étudiants doivent faire leurs choix de programme avec soin. Pour modéliser les subtilités du domaine éducatif, les ontologies ont été exploitées avec succès dans le passé. Cependant, aucune ontologie ou ensemble de données disponible n’aborde directement la transition critique de l’école secondaire à l’université du point de vue de la prise de décision. Par conséquent, la contribution de nos travaux en cours est double. Tout d’abord, nous présentons EducOnto — une ontologie qui vise à modéliser les programmes d’études universitaires et les profils des étudiants. Deuxièmement, nous présentons EduKG — un graphique de connaissances héritant de la sémantique d’EducOnto et instancié de données sur les étudiants et les programmes d’études français.","13295","4","213","Nouveau graphique de l’ontologie et des connaissances pour la recommandation du programme d’études universitaires

L’éducation est un domaine complexe où les étudiants doivent choisir leur programme avec soin. Pour modéliser les subtilités du domaine éducatif, les ontologies ont été exploitées avec succès dans le passé. Cependant, aucune ontologie ou ensemble de données disponible n’aborde directement la transition critique de l’école secondaire à l’université du point de vue de la prise de décision. Par conséquent, la contribution de nos travaux en cours est double. Tout d’abord, nous présentons EducOnto — une ontologie qui vise à modéliser les programmes d’études universitaires et les profils des étudiants. Deuxièmement, nous présentons EduKG — un graphique de connaissances héritant de la sémantique d’EducOnto et instancié de données sur les étudiants et les programmes d’études français.","4","2023-06-26 15:26:47","2023-06-26 15:48:01","13295",,"1","1","1","2","1","1",NULL,,"2023-06-26 03:24:28","0"
"3890035","The recent trend of adopting linked-data principles to integrate and publish semantically described open data using W3C standards has led to a large amount of available resources. In particular, meteorological sensor data have been uplifted into public weather-focused RDF graphs, such as WeKG-MF which offers access to a large set of meteorological variables described through spatial and temporal dimensions. Nevertheless, these resources include huge numbers of raw observations that are tedious to explore by lay users. In this article, we aim at providing them with visual exploratory ""tours"", benefiting from RDF data cubes to present high-level aggregated views together with on-demand fine-grained details through a unified Web interface.","Multi-Level Visual Tours of Weather Linked Data","Nadia Yacoubi, Damien Graux, Catherine Faron","4038","Visites visuelles à plusieurs niveaux de données liées à la météo

La tendance récente à adopter des principes de données liées pour intégrer et publier des données ouvertes décrites sémantiquement en utilisant les normes du W3C a conduit à une grande quantité de ressources disponibles. En particulier, les données des capteurs météorologiques ont été rehaussées dans des graphiques RDF axés sur la météo publique, tels que WEKG-MF qui offre l’accès à un grand ensemble de variables météorologiques décrites à travers les dimensions spatiales et temporelles. Néanmoins, ces ressources comprennent un grand nombre d’observations brutes qui sont fastidieuses à explorer par les utilisateurs profanes. Dans cet article, nous visons à leur fournir des «tours visuels» exploratoires, bénéficiant de cubes de données RDF pour présenter des vues agrégées de haut niveau ainsi que des détails à la demande à grains fins via une interface Web unifiée.","12996","4","214","Visites visuelles à plusieurs niveaux de données liées à la météo

La tendance récente à adopter des principes de données liées pour intégrer et publier des données ouvertes décrites sémantiquement en utilisant les normes du W3C a rendu disponible une grande quantité de ressources. En particulier, les données des capteurs météorologiques ont été rehaussées dans des graphiques RDF axés sur la météo publique, tels que WEKG-MF qui offre l’accès à un grand ensemble de variables météorologiques décrites à travers les dimensions spatiales et temporelles. Néanmoins, ces ressources comprennent un grand nombre d’observations brutes qui sont fastidieuses à explorer par les utilisateurs profanes. Dans cet article, nous visons à leur fournir des « visites » visuelles d&#039;exploration, bénéficiant de cubes de données RDF pour présenter des vues agrégées de haut niveau ainsi que des détails infimes à la demande via une interface Web unifiée.","4","2023-06-26 15:48:18","2023-06-26 16:18:48","12996",,"2","1","3","3","2","1",NULL,,"2023-06-26 03:48:06","0"
"3878174","The 2022 Multilingual Representation Learning (MRL) Shared Task was dedicated to clause-level morphology. As the first ever benchmark that defines and evaluates morphology outside its traditional lexical boundaries, the shared task on multilingual clause-level morphology sets the scene for competition across different approaches to morphological modeling, with 3 clause-level sub-tasks: morphological inflection, reinflection and analysis, where systems are required to generate, manipulate or analyze simple sentences centered around a single content lexeme and a set of morphological features characterizing its syntactic clause. This year's tasks covered eight typologically distinct languages: English, French, German, Hebrew, Russian, Spanish, Swahili and Turkish. The tasks has received submissions of four systems from three teams which were compared to two baselines implementing prominent multilingual learning methods. The results show that modern NLP models are effective in solving morphological tasks even at the clause level. However, there is still room for improvement, especially in the task of morphological analysis.","The MRL 2022 Shared Task on Multilingual Clause-level Morphology","Omer Goldman, Francesco Tinner, Hila Gonen, Benjamin Muller, Victoria Basmov, Shadrack Kirimi, Lydia Nishimwe, Benoît Sagot, Djamé Seddah, Reut Tsarfaty, Duygu Ataman","450","La tâche partagée de MRL 2022 sur la morphologie multilingue au niveau des clauses

La tâche partagée de l'apprentissage par représentation multilingue (MRL) 2022 a été consacrée à la morphologie au niveau des clauses. En tant que tout premier benchmark qui définit et évalue la morphologie en dehors de ses frontières lexicales traditionnelles, la tâche partagée sur la morphologie multilingue au niveau des clauses pose le décor de la compétition entre différentes approches de la modélisation morphologique, avec 3 sous-tâches au niveau des clauses : l'inflexion, la réinflexion et l'analyse morphologiques, lorsque des systèmes sont nécessaires pour générer, manipuler ou analyser des phrases simples centrées autour d'un seul lexème de contenu et d'un ensemble de caractéristiques morphologiques caractérisant sa clause syntaxique. Les tâches de cette année couvraient huit langues typologiquement distinctes : anglais, français, allemand, hébreu, russe, espagnol, swahili et turc. Quatre systèmes ont été soumis par trois équipes, comparés à deux systèmes de référence mettant en oeuvre des méthodes d'apprentissage multilingues de premier plan. Les résultats montrent que les modèles NLP modernes sont efficaces pour résoudre les tâches morphologiques, même au niveau des clauses. Toutefois, des améliorations sont encore possibles, notamment en ce qui concerne l'analyse morphologique.","4197","6","215","La tâche partagée de MRL 2022 sur la morphologie multilingue au niveau des clauses

La tâche partagée de l&#039;apprentissage par représentation multilingue (MRL, Multilingual Representation Learning) 2022 a été consacrée à la morphologie au niveau des clauses. En tant que tout premier benchmark qui définit et évalue la morphologie en dehors de ses frontières lexicales traditionnelles, la tâche partagée sur la morphologie multilingue au niveau des clauses pose le décor de la compétition entre différentes approches de la modélisation morphologique, avec 3 sous-tâches au niveau des clauses : l&#039;inflexion, la réinflexion et l&#039;analyse morphologiques, lorsque des systèmes sont nécessaires pour générer, manipuler ou analyser des phrases simples centrées autour d&#039;un seul lexème de contenu et d&#039;un ensemble de caractéristiques morphologiques caractérisant sa clause syntaxique. Les tâches de cette année couvraient huit langues typologiquement distinctes : anglais, français, allemand, hébreu, russe, espagnol, swahili et turc. Quatre systèmes ont été soumis par trois équipes, comparés à deux systèmes de référence mettant en œuvre des méthodes d&#039;apprentissage multilingues de premier plan. Les résultats montrent que les modèles de TAL modernes sont efficaces pour résoudre les tâches morphologiques, même au niveau des clauses. Toutefois, des améliorations sont encore possibles, notamment en ce qui concerne l&#039;analyse morphologique.","4","2023-06-26 16:19:09","2023-06-26 16:45:58","4197",,"1","1","1","1","2","1",NULL,,"2023-06-26 04:19:03","0"
"3683695","Meteorological institutions produce a valuable amount of data as a direct or side product of their activities, which can be potentially explored in diverse applications. However, making this data fully reusable requires considerable efforts in order to guarantee compliance to the FAIR principles. While most efforts in data FAIRification are limited to describing data with semantic metadata, such a description is not enough to fully address interoperability and reusability. We tackle this weakness by proposing a rich ontological model to represent both metadata and data schema of meteorological data. We apply the proposed model on a largely used meteorological dataset, the ""SYNOP"" dataset of Météo-France and show how the proposed model improves FAIRness.","Towards the FAIRification of Meteorological Data: a Meteorological Semantic Model","Amina Annane, Mouna Kamel, Cassia Trojahn, Nathalie Aussenac-Gilles, Catherine Comparot, Christophe Baehr","4631","Vers la FAIRification des données météorologiques : un modèle sémantique météorologique

Les institutions météorologiques produisent une quantité précieuse de données en tant que produit direct ou secondaire de leurs activités, qui peuvent être potentiellement exploitées dans diverses applications. Cependant, rendre ces données entièrement réutilisables nécessite des efforts considérables afin de garantir la conformité aux principes FAIR. Alors que la plupart des efforts de FAIRification des données se limitent à décrire les données avec des métadonnées sémantiques, une telle description n'est pas suffisante pour aborder pleinement l'interopérabilité et la réutilisabilité. Nous nous attaquons à cette faiblesse en proposant un modèle ontologique riche pour représenter à la fois les métadonnées et le schéma des données météorologiques. Nous appliquons le modèle proposé à un ensemble de données météorologiques largement utilisé, l'ensemble de données ""SYNOP"" de Météo-France, et nous montrons comment le modèle proposé améliore la FAIRness.","19101","3","216","Vers la FAIRification des données météorologiques : un modèle sémantique météorologique

Les institutions météorologiques produisent une quantité précieuse de données en tant que produit direct ou secondaire de leurs activités, qui peuvent être potentiellement exploitées dans diverses applications. Cependant, rendre ces données entièrement réutilisables requiert des efforts considérables afin de garantir la conformité aux principes FAIR. Alors que la plupart des efforts de FAIRification des données se limitent à décrire les données avec des métadonnées sémantiques, une telle description n&#039;est pas suffisante pour aborder pleinement l&#039;interopérabilité et la réutilisabilité. Nous nous attaquons à cette faiblesse en proposant un modèle ontologique riche pour représenter à la fois les métadonnées et le schéma des données météorologiques. Nous appliquons le modèle proposé à un ensemble de données météorologiques largement utilisé, l&#039;ensemble de données &quot;SYNOP&quot; de Météo-France, et nous montrons comment le modèle proposé améliore la FAIRness.","4","2023-06-26 16:46:29","2023-06-26 16:50:14","19101",,"1","1","1","2","1","1",NULL,,"2023-06-26 04:46:23","0"
"3599076","The use of the mel spectrogram as a signal parameterization for voice generation is quite recent and linked to the development of neural vocoders. These are deep neural networks that allow reconstructing high-quality speech from a given mel spectrogram. While initially developed for speech synthesis, now neural vocoders have also been studied in the context of voice attribute manipulation, opening new means for voice processing in audio production. However, to be able to apply neural vocoders in real-world applications, two problems need to be addressed: (1) To support use in professional audio workstations, the computational complexity should be small, (2) the vocoder needs to support a large variety of speakers, differences in voice qualities, and a wide range of intensities potentially encountered during audio production. In this context, the present study will provide a detailed description of the Multi-band Excited WaveNet, a fully convolutional neural vocoder built around signal processing blocks. It will evaluate the performance of the vocoder when trained on a variety of multi-speaker and multi-singer databases, including an experimental evaluation of the neural vocoder trained on speech and singing voices. Addressing the problem of intensity variation, the study will introduce a new adaptive signal normalization scheme that allows for robust compensation for dynamic and static gain variations. Evaluations are performed using objective measures and a number of perceptual tests including different neural vocoder algorithms known from the literature. The results confirm that the proposed vocoder compares favorably to the state-of-the-art in its capacity to generalize to unseen voices and voice qualities. The remaining challenges will be discussed.","Neural Vocoding for Singing and Speaking Voices with the Multi-Band Excited WaveNet","Axel Roebel, Frederik Bous","4830","Vocoding neuronal pour chanter et parler avec le multi-bande excité WaveNet

L'utilisation du spectrogramme mel comme paramétrisation de signal pour la génération vocale est assez récente et liée au développement de vocodeurs neuronaux. Ce sont des réseaux neuronaux profonds qui permettent de reconstruire la parole de haute qualité à partir d'un spectrogramme de Mel donné. Alors qu'initialement développé pour la synthèse vocale, maintenant les vocodeurs neuronaux ont également été étudiés dans le contexte de la manipulation des attributs vocaux, ouvrant de nouveaux moyens pour le traitement de la voix dans la production audio. Cependant, pour pouvoir appliquer des vocodeurs neuronaux dans des applications réelles, deux problèmes doivent être abordés : (1) Pour prendre en charge l'utilisation dans des stations de travail audio professionnelles, la complexité de calcul doit être faible, (2) le vocodeur doit prendre en charge une grande variété de haut-parleurs, des différences dans les qualités vocales, et une large gamme d'intensités potentiellement rencontrées pendant la production audio. Dans ce contexte, la présente étude fournira une description détaillée du Multi-band Exited WaveNet, un vocodeur neuronal entièrement convolutionnel construit autour de blocs de traitement de signal. Il évaluera la performance du vocodeur lorsqu'il est formé sur une variété de bases de données multi-haut-parleurs et multi-chanteurs, y compris une évaluation expérimentale du vocodeur neuronal formé sur la parole et les voix chantantes. Abordant le problème de la variation d'intensité, l'étude introduira un nouveau schéma de normalisation adaptative du signal qui permettra une compensation robuste des variations de gain dynamiques et statiques. Les évaluations sont effectuées à l'aide de mesures objectives et d'un certain nombre de tests perceptuels, y compris différents algorithmes de vocodeur neuronal connus de la littérature. Les résultats confirment que le vocodeur proposé se compare favorablement à l'état de l'art dans sa capacité de généraliser aux voix invisibles et qualités vocales. Les défis restants seront examinés.","16746","6","226","Vocodeur neuronal pour voix chantées et parlées avec le Multi-Band Excited WaveNet

L&#039;utilisation du spectrogramme en mel comme paramétrisation de signal pour la synthèse vocale est assez récente et liée au développement de vocodeurs neuronaux. Ce sont des réseaux de neurones profonds qui permettent de reconstruire la parole de haute qualité à partir d&#039;un spectrogramme en mel donné. Bien qu&#039;ils aient été initialement développés pour la synthèse vocale, les vocodeurs neuronaux ont également été étudiés dans le contexte de la manipulation des attributs vocaux, ouvrant de nouveaux moyens pour le traitement de la voix dans la production audio. Cependant, pour pouvoir appliquer des vocodeurs neuronaux dans des applications réelles, deux problèmes doivent être abordés : (1) Pour prendre en charge l&#039;utilisation dans des stations de travail audio professionnelles, la complexité de calcul doit être faible, (2) le vocodeur doit prendre en charge une grande variété de haut-parleurs, des différences dans les qualités vocales, et une large gamme d&#039;intensités potentiellement rencontrées pendant la production audio. Dans ce contexte, la présente étude fournira une description détaillée du Multi-band Excited WaveNet, un vocodeur neuronal entièrement convolutionnel construit autour de blocs de traitement de signal. Il évaluera la performance du vocodeur lorsqu&#039;il est formé sur une variété de bases de données formée d&#039;extraits de plusieurs locuteurs et chanteurs, y compris une évaluation expérimentale du vocodeur neuronal formé les voix parlées et chantées. Abordant le problème de la variation d&#039;intensité, l&#039;étude introduira un nouveau schéma de normalisation adaptative du signal qui permettra une compensation robuste des variations de gain dynamiques et statiques. Les évaluations sont effectuées à l&#039;aide de mesures objectives et d&#039;un certain nombre de tests perceptuels, y compris différents algorithmes de vocodeur neuronal connus de la littérature. Les résultats confirment que le vocodeur proposé se compare favorablement à l&#039;état de l&#039;art dans sa capacité à se généraliser aux voix invisibles et qualités vocales. Les défis restants seront examinés.","4","2023-06-27 14:29:44","2023-06-27 15:26:21","16746",,"3","2","2","4","4","4",NULL,"synthèse vocale
spectrogramme en mel (mel = unité de fréquence)","2023-06-27 02:29:35","0"
"3911555","In today's rapidly advancing software industry, we experience exciting technological growth every year in an extensive range of fields such as innovative AI-powered development, cloud and edge computing, machine learning, progressive and lightweight web and mobile applications etc. However, it is not quite relevant to the Software Testing industry. Most companies still rely on the outdated manual testing process despite the availability of Automation testing procedures. It may work for small teams testing the software using a limited number of test cases. Although, with the increase in team size and the number of test cases over time, the cost, effort and time needed to manually validate and review the test cases increase tenfold. It often leads to recurrent and unclear test cases in the test suite, delivered by different employees who often work across teams. These test cases are represented in Natural Language. Also, the redundant test cases can impact the manual testing process by testing the same feature multiple times and can reduce the possibility of writing multiple methods to test the same feature when automating tests in the future. Hence, in this project, we propose to address the problem of similar test cases using an unsupervised learning approach. Additionally, after removing redundancy in the test suite, we intend to identify key features in the software to be tested based on the description of the test cases in the suite, and group multiple test cases into a software feature. This feature can be directly assigned to a Quality Assurance engineer for testing.","Identifying Similar Test Cases That Are Specified in Natural Language","Amey Joshi, Ranit Ganguly, Ritik Gandhi","3983","Identification de cas de test similaires spécifiés en langage naturel

Dans l'industrie des logiciels, qui progresse rapidement aujourd'hui, nous connaissons une croissance technologique passionnante chaque année dans un large éventail de domaines tels que le développement innovant alimenté par l'IA, le cloud et l'informatique de pointe, l'apprentissage automatique, le web progressif et léger et les applications mobiles, etc. Cependant, il n'est pas tout à fait pertinent pour l'industrie du test de logiciels. La plupart des entreprises continuent à utiliser le processus de test manuel obsolète malgré la disponibilité des procédures de test Automation. Il peut fonctionner pour de petites équipes testant le logiciel en utilisant un nombre limité de cas de test. Cependant, avec l'augmentation de la taille de l'équipe et du nombre de cas de test au fil du temps, le coût, l'effort et le temps nécessaires pour valider et examiner manuellement les cas de test augmentent de dix fois. Elle conduit souvent à des cas tests récurrents et peu clairs dans la suite de tests, fournis par différents employés qui travaillent souvent entre les équipes. Ces cas tests sont représentés en langage naturel. En outre, les cas de test redondants peuvent avoir un impact sur le processus de test manuel en testant la même fonctionnalité plusieurs fois et peuvent réduire la possibilité d'écrire plusieurs méthodes pour tester la même fonctionnalité lors de l'automatisation de tests futurs. Par conséquent, dans le cadre de ce projet, nous proposons de traiter le problème des cas de test similaires en utilisant une approche d'apprentissage non supervisé. En outre, après suppression de la redondance dans la suite de tests, nous avons l'intention d'identifier les principales caractéristiques du logiciel à tester sur la base de la description des cas de test dans la suite, et de regrouper plusieurs cas de test dans une caractéristique du logiciel. Cette fonction peut être directement attribuée à un ingénieur en assurance qualité pour test.","15899","6","229","Identification de cas de test similaires spécifiés en langage naturel

Dans l&#039;industrie des logiciels, qui progresse rapidement aujourd&#039;hui, nous connaissons une croissance technologique passionnante chaque année dans un large éventail de domaines tels que le développement innovant alimenté par l&#039;IA, le cloud et l&#039;informatique de pointe, l&#039;apprentissage automatique, le web progressif et léger et les applications mobiles, etc. Cependant, il n&#039;est pas tout à fait pertinent pour l&#039;industrie du test de logiciels. La plupart des entreprises continuent à utiliser le processus de test manuel obsolète malgré la disponibilité des procédures de test Automation. Il peut fonctionner pour de petites équipes qui testent le logiciel en utilisant un nombre limité de cas tests. Cependant, avec l&#039;augmentation de la taille de l&#039;équipe et du nombre de cas de test au fil du temps, le coût, l&#039;effort et le temps nécessaires pour valider et examiner manuellement les cas tests augmentent de dix fois. Elle conduit souvent à des cas tests récurrents et peu clairs dans la suite de tests, fournis par différents employés qui travaillent souvent entre les équipes. Ces cas tests sont représentés en langage naturel. En outre, les cas tests redondants peuvent avoir un impact sur le processus de test manuel en testant la même fonctionnalité plusieurs fois et peuvent réduire la possibilité d&#039;écrire plusieurs méthodes pour tester la même fonctionnalité lors de l&#039;automatisation de tests futurs. Par conséquent, dans le cadre de ce projet, nous proposons de traiter le problème des cas tests similaires en utilisant une approche d&#039;apprentissage non supervisé. En outre, après suppression de la redondance dans la suite de tests, nous avons l&#039;intention d&#039;identifier les principales caractéristiques du logiciel à tester sur la base de la description des cas de test dans la suite, et de regrouper plusieurs cas tests dans une caractéristique du logiciel. Cette fonction peut être directement attribuée à un ingénieur en assurance qualité pour test.","4","2023-06-27 15:27:00","2023-06-27 15:46:13","15899",,"1","1","1","2","2","2",NULL,,"2023-06-27 03:26:49","0"
"3911658","Purpose The aim of this study was to examine the early stages of the COVID-19 outbreak and the international communication management of Chinese diplomats as a case for extending the definition of intermestic public diplomacy. The goal was to reveal how Beijing subtly used both domestic and foreign social media to organize a network for communication about COVID-19 and purposefully soften the highly centralized and hierarchical political propaganda of the Communist Party of China (CPC). Design/methodology/approach Based on the literature on digital public diplomacy, the authors applied the existing concept of intermestic to Chinese politics in order to demonstrate the digitalization of public diplomacy, along with its forms and strategies under an authoritarian regime. A hybrid methodology combining quantitative network analysis and qualitative discourse analysis permits examination of China's intermestic online communication network dynamics, shedding light on how such an intermestic practice promoted Chinese values and power to international publics in the early stages of the COVID-19 crisis. Findings The authors’ findings extend the implications of intermestic public diplomacy from a democratic context to an authoritarian one. By analyzing the content of public diplomacy and para-diplomatic social media accounts in China and abroad at the beginning of the COVID-19 crisis, the authors outlined China's early crisis management, explaining its intermestic public diplomacy transmission modes and strategies. Moreover, the authors identified changes in the narrative strategies of Chinese diplomats and journalists during this process. Social implications The findings of this study underline that Beijing established a narrative-making virtual communication structure for disseminating favorable Chinese strategic narratives and voices through differentiated communication on domestic and foreign social media platforms. Such intermestic communication strategies were particularly evident and even further weaponized by Beijing in its large-scale Wolf Warrior diplomacy in the spring of 2020. Thus, the study’s findings help readers understand how China digitalized its public diplomacy, its digital communication patterns and strategies. Originality/value On the one hand, geopolitical uncertainty and the popularity of social media have contributed to the evolution of the intermestic model of public diplomacy. This model allows actors to coordinate homogenous and differentiated communication practices to deploy their influence. On the other hand, the authors did not examine how intermestic audiences perceive and receive public diplomacy practices. In future studies, scholars should measure the agenda-setting capacity of diplomatic actors by examining the effects of such intermestic communication efforts.","An intermestic approach to China's public diplomacy: a case study of Beijing's COVID-19 communication in the early stages","Zhao Alexandre Huang, Rui Wang","3981","Une approche intermestique de la diplomatie publique chinoise : une étude de cas de la communication COVID-19 de Pékin dans ses premières étapes

Objectif L'objectif de cette étude était d'examiner les premières phases de l'épidémie de COVID-19 et la gestion de la communication internationale par les diplomates chinois afin d'élargir la définition de la diplomatie publique intermestique. L'objectif était de révéler comment Pékin a subtilement utilisé les médias sociaux nationaux et étrangers pour organiser un réseau de communication sur le COVID-19 et atténuer à dessein la propagande politique hautement centralisée et hiérarchique du Parti communiste chinois (PCC). Conception/méthodologie/approche S'appuyant sur la littérature relative à la diplomatie publique numérique, les auteurs ont appliqué le concept existant d'intermestique à la politique chinoise afin de démontrer la numérisation de la diplomatie publique, ainsi que ses formes et stratégies sous un régime autoritaire. Une méthodologie hybride combinant l'analyse quantitative des réseaux et l'analyse qualitative du discours permet d'examiner la dynamique du réseau de communication en ligne intermestique de la Chine, mettant en lumière la manière dont une telle pratique intermestique a promu les valeurs et le pouvoir chinois auprès des publics internationaux dans les premières phases de la crise du COVID-19. Conclusions Les conclusions des auteurs étendent les implications de la diplomatie publique intermestique d'un contexte démocratique à un contexte autoritaire. En analysant le contenu des comptes de la diplomatie publique et des médias sociaux para-diplomatiques en Chine et à l'étranger au début de la crise du COVID-19, les auteurs ont décrit la gestion précoce de la crise par la Chine, en expliquant ses modes et stratégies de transmission de la diplomatie publique intermestique. En outre, les auteurs ont identifié des changements dans les stratégies narratives des diplomates et des journalistes chinois au cours de ce processus. Implications sociales Les résultats de cette étude soulignent que Pékin a mis en place une structure de communication virtuelle narrative pour diffuser des récits et des voix stratégiques chinoises favorables par le biais d'une communication différenciée sur les plateformes de médias sociaux nationales et étrangères. Ces stratégies de communication intermestique ont été particulièrement évidentes et encore plus utilisées par Pékin dans sa diplomatie à grande échelle de Wolf Warrior au printemps 2020. Ainsi, les résultats de l'étude aident les lecteurs à comprendre comment la Chine a numérisé sa diplomatie publique, ses modèles et stratégies de communication numérique. Originalité/valeur D'une part, l'incertitude géopolitique et la popularité des médias sociaux ont contribué à l'évolution du modèle intermestique de la diplomatie publique. Ce modèle permet aux acteurs de coordonner des pratiques de communication homogènes et différenciées pour déployer leur influence. En revanche, les auteurs n'ont pas examiné la manière dont les publics intermestiques perçoivent et reçoivent les pratiques de diplomatie publique. Dans de futures études, les chercheurs devraient mesurer la capacité des acteurs diplomatiques à définir l'agenda en examinant les effets de ces efforts de communication intermestique.","18797","3","233","Une approche intermestique de la diplomatie publique chinoise : une étude de cas de la communication COVID-19 de Pékin dans ses premières étapes

Objectif
L&#039;objectif de cette étude était d&#039;examiner les premières phases de l&#039;épidémie de COVID-19 et la gestion de la communication internationale par les diplomates chinois afin d&#039;élargir la définition de la diplomatie publique intermestique. L&#039;objectif était de révéler comment Pékin a subtilement utilisé les médias sociaux nationaux et étrangers pour organiser un réseau de communication sur le COVID-19 et atténuer à dessein la propagande politique hautement centralisée et hiérarchique du Parti communiste chinois (PCC).

Conception/méthodologie/approche
S&#039;appuyant sur la littérature relative à la diplomatie publique numérique, les auteurs ont appliqué le concept existant d&#039;intermestique à la politique chinoise afin de démontrer la numérisation de la diplomatie publique, ainsi que ses formes et stratégies sous un régime autoritaire. Une méthodologie hybride combinant l&#039;analyse quantitative des réseaux et l&#039;analyse qualitative du discours permet d&#039;examiner la dynamique du réseau de communication en ligne intermestique de la Chine, mettant en lumière la manière dont une telle pratique intermestique a promu les valeurs et le pouvoir chinois auprès des publics internationaux dans les premières phases de la crise du COVID-19.

Conclusions
Les conclusions des auteurs étendent les implications de la diplomatie publique intermestique d&#039;un contexte démocratique à un contexte autoritaire. En analysant le contenu des comptes de la diplomatie publique et des médias sociaux para-diplomatiques en Chine et à l&#039;étranger au début de la crise du COVID-19, les auteurs ont décrit la gestion précoce de la crise par la Chine, en expliquant ses modes et stratégies de transmission de la diplomatie publique intermestique. En outre, les auteurs ont identifié des changements dans les stratégies narratives des diplomates et des journalistes chinois au cours de ce processus.

Implications sociales
Les résultats de cette étude soulignent que Pékin a mis en place une structure de communication virtuelle narrative pour diffuser des récits et des voix stratégiques chinoises favorables par le biais d&#039;une communication différenciée sur les plateformes de médias sociaux nationales et étrangères. Ces stratégies de communication intermestique ont été particulièrement évidentes et encore plus utilisées par Pékin dans sa diplomatie à grande échelle de Wolf Warrior au printemps 2020. Ainsi, les résultats de l&#039;étude aident les lecteurs à comprendre comment la Chine a numérisé sa diplomatie publique, ses modèles et stratégies de communication numérique. 

Originalité/valeur
D&#039;une part, l&#039;incertitude géopolitique et la popularité des médias sociaux ont contribué à l&#039;évolution du modèle intermestique de la diplomatie publique. Ce modèle permet aux acteurs de coordonner des pratiques de communication homogènes et différenciées pour déployer leur influence. En revanche, les auteurs n&#039;ont pas examiné la manière dont les publics intermestiques perçoivent et reçoivent les pratiques de diplomatie publique. Dans de futures études, les chercheurs devraient mesurer la capacité des acteurs diplomatiques à définir l&#039;agenda en examinant les effets de ces efforts de communication intermestique.","4","2023-06-27 15:47:50","2023-06-27 16:02:25","18797",,"1","1","1","1","3","1",NULL,,"2023-06-27 03:47:41","0"
"3636034","How do children acquire language through unsupervised or noisy supervision? How do their brain process language? We take this perspective to machine learning and robotics, where part of the problem is understanding how language models can perform grounded language acquisition through noisy supervision and discussing how they can account for brain learning dynamics. Most prior works have tracked the co-occurrence between single words and referents to model how infants learn wordreferent mappings. This paper studies cross-situational learning (CSL) with full sentences: we want to understand brain mechanisms that enable children to learn mappings between words and their meanings from full sentences in early language learning. We investigate the CSL task on a few training examples with two sequence-based models: (i) Echo State Networks (ESN) and (ii) Long-Short Term Memory Networks (LSTM). Most importantly, we explore several word representations including One-Hot, GloVe, pretrained BERT, and fine-tuned BERT representations (last layer token representations) to perform the CSL task. We apply our approach to three diverse datasets (two grounded language datasets and a robotic dataset) and observe that (1) One-Hot, GloVe, and pretrained BERT representations are less efficient when compared to representations obtained from fine-tuned BERT. (2) ESN online with final learning (FL) yields superior performance over ESN online continual learning (CL), offline learning, and LSTMs, indicating the more biological plausibility of ESNs and the cognitive process of sentence reading. (2) LSTM with fewer hidden units showcases higher performance for small datasets, but LSTM with more hidden units is Cross-Situational Learning needed to perform reasonably well on larger corpora. (4) ESNs demonstrate better generalization than LSTM models for increasingly large vocabularies. Overall, these models are able to learn from scratch to link complex relations between words and their corresponding meaning concepts, handling polysemous and synonymous words. Moreover, we argue that such models can extend to help current human-robot interaction studies on language grounding and better understand children's developmental language acquisition. We make the code publicly available * .","Cross-Situational Learning Towards Robot Grounding","Subba Reddy Oota, Frédéric Alexandre, Xavier Hinaut","4738","Apprentissage inter-situationnel vers la mise à la terre des robots

Comment les enfants acquièrent-ils la langue grâce à une supervision non surveillée ou bruyante? Comment leur cerveau traite-t-il le langage? Nous prenons cette perspective à l’apprentissage automatique et à la robotique, où une partie du problème est de comprendre comment les modèles de langage peuvent effectuer l’acquisition du langage à la base grâce à une supervision bruyante et à discuter de la façon dont ils peuvent tenir compte de la dynamique d’apprentissage du cerveau. La plupart des travaux antérieurs ont suivi la co-occurrence entre des mots simples et des référents pour modéliser la façon dont les nourrissons apprennent les mappages de références de mots. Cet article étudie l’apprentissage intersituel (CSL) avec des phrases complètes: nous voulons comprendre les mécanismes cérébraux qui permettent aux enfants d’apprendre des cartographies entre les mots et leurs significations à partir de phrases complètes dans l’apprentissage précoce des langues. Nous étudions la tâche CSL sur quelques exemples de formation avec deux modèles basés sur des séquences: (I) Echo State Networks (ESN) et (ii) Long-Short Term Memory Networks (LSTM). Plus important encore, nous explorons plusieurs représentations de mots, y compris One-Hot, GloVe, BERT préformé, et des représentations BERT affinées (représentations de jetons de dernière couche) pour effectuer la tâche CSL. Nous appliquons notre approche à trois ensembles de données divers (deux ensembles de données de langage ancré et un jeu de données robotique) et observons que (1) les représentations One-Hot, GloVe et BERT préformées sont moins efficaces par rapport aux représentations obtenues à partir de BERT affiné. (2) L’ESN en ligne avec l’apprentissage final (FL) produit des performances supérieures à l’apprentissage continu en ligne ESN (CL), l’apprentissage hors ligne et les LSTM, ce qui indique la plausibilité plus biologique des ESN et le processus cognitif de lecture de phrases. (2) LSTM avec moins d’unités cachées présente des performances plus élevées pour les petits ensembles de données, mais LSTM avec plus d’unités cachées est Cross-Situational Learning nécessaire pour fonctionner raisonnablement bien sur les grands corpus. (4) Les ESN présentent une meilleure généralisation que les modèles LSTM pour les vocabulaires de plus en plus grands. Dans l’ensemble, ces modèles sont capables d’apprendre à partir de zéro pour lier les relations complexes entre les mots et leurs concepts de signification correspondants, en manipulant des mots polysémiques et synonymes. De plus, nous soutenons que de tels modèles peuvent s’étendre pour aider les études actuelles d’interaction homme-robot sur la mise à la terre du langage et mieux comprendre l’acquisition du langage développemental des enfants. Nous mettons le code à la disposition du public *.","11525","4","243","Apprentissage cross-situationnel vers l&#039;ancrage des robots

Comment les enfants acquièrent-ils la langue grâce à une supervision non surveillée ou bruyante ? Comment leur cerveau traite-t-il la langue ? Nous portons cette perspective à l’apprentissage automatique et à la robotique, où une partie du problème est de comprendre comment les modèles de langage peuvent effectuer une acquisition ancrée de la langue grâce à une supervision bruyante et à discuter de la façon dont ils peuvent tenir compte de la dynamique d’apprentissage du cerveau. La plupart des travaux antérieurs ont suivi la co-occurrence entre des mots simples et des référents pour modéliser la façon dont les nourrissons apprennent les mappages de références de mots. Cet article étudie l’apprentissage cross-situationnel (CSL) avec des phrases complètes : nous voulons comprendre les mécanismes cérébraux qui permettent aux enfants d’apprendre des cartographies entre les mots et leurs significations à partir de phrases complètes dans l’apprentissage précoce des langues. Nous étudions la tâche CSL sur quelques exemples de formation avec deux modèles basés sur des séquences : (I) Echo State Networks (ESN) et (ii) Long-Short Term Memory Networks (LSTM). Plus important encore, nous explorons plusieurs représentations de mots, y compris One-Hot, GloVe, BERT préformé, et des représentations BERT affinées (représentations de jetons de dernière couche) pour effectuer la tâche CSL. Nous appliquons notre approche à trois ensembles de données divers (deux ensembles de données de langage ancré et un jeu de données robotique) et observons que (1) les représentations One-Hot, GloVe et BERT pré-entraînées sont moins efficaces par rapport aux représentations obtenues à partir de BERT affiné. (2) L’ESN en ligne avec l’apprentissage final (FL) produit des performances supérieures à l’apprentissage continu en ligne ESN (CL), l’apprentissage hors ligne et les LSTM, ce qui indique la plausibilité plus biologique des ESN et le processus cognitif de lecture de phrases. (2) LSTM avec moins d’unités cachées présente des performances plus élevées pour les petits ensembles de données, mais LSTM avec plus d’unités cachées requiert un apprentissage cross-situationnel pour fonctionner raisonnablement bien sur les grands corpus. (4) Les ESN présentent une meilleure généralisation que les modèles LSTM pour les vocabulaires de plus en plus grands. Dans l’ensemble, ces modèles sont capables d’apprendre à partir de zéro pour lier les relations complexes entre les mots et leurs concepts de signification correspondants, en manipulant des mots polysémiques et synonymes. De plus, nous soutenons que de tels modèles peuvent s’étendre pour aider les études actuelles d’interaction homme-robot sur l&#039;ancrage du langage et mieux comprendre l’acquisition du langage développemental des enfants. Nous mettons le code à la disposition du public*.","4","2023-06-27 17:20:47","2023-06-27 17:22:32","11525",,"1","1","3","3","2","2",NULL,"grounding : ancrage
cross-situational learning : apprentissage cross-situationnel","2023-06-27 05:20:45","0"
"3877255","This paper presents the development of an ontology for opinion mining in French corpora. Since ontology construction from scratch is expensive and time consuming, the model is built by adapting an existing ontology modeling appraisal categories in English. The construction method consists of two main steps: first, the ontology of appraisals is translated in French by using a conceptto-concept approach; then different adaptation strategies are implemented to improve the result of this translation. Adaptation strategies are based on text mining, weakly supervised methods and the use of WordNet to refine the model. The goal of the adaptation phase is to cope with limitations of concept-to-concept translation, to integrate concepts and relations from external corpora and resources, and to build a model that captures various features of opinions. The ontology was designed and build to incorporate linguistic and extra linguistic information on the description of opinions in French. The model was validated by using both expert insights to validate the relevance of concepts and relations and formal criteria to describe the ontology qualities for practical use.","Ontology Adaptation for Opinion Mining in French Corpora","Valentina Dragos, Adrien Legros","4076","Adaptation de l’ontologie pour l’exploitation minière d’opinions en Corpora français

Cet article présente le développement d’une ontologie pour l’extraction d’opinions dans les corpus français. Étant donné que la construction ontologique à partir de zéro coûte cher et prend du temps, le modèle est construit en adaptant les catégories d’évaluation de modélisation ontologique existantes en anglais. La méthode de construction se compose de deux étapes principales: premièrement, l’ontologie des évaluations est traduite en français par une approche conceptuelle; ensuite, différentes stratégies d’adaptation sont mises en œuvre pour améliorer le résultat de cette traduction. Les stratégies d’adaptation sont basées sur l’exploration de texte, des méthodes faiblement supervisées et l’utilisation de WordNet pour affiner le modèle. L’objectif de la phase d’adaptation est de faire face aux limites de la traduction concept-to-concept, d’intégrer les concepts et les relations à partir de corpus et de ressources externes, et de construire un modèle qui capture diverses caractéristiques des opinions. L’ontologie a été conçue et construite pour intégrer des informations linguistiques et linguistiques supplémentaires sur la description des opinions en français. Le modèle a été validé en utilisant à la fois des connaissances d’experts pour valider la pertinence des concepts et des relations et des critères formels pour décrire les qualités ontologiques à des fins pratiques.","13034","4","244","Adaptation de l’ontologie pour l’extraction d’opinions dans des corpus français

Cet article présente le développement d’une ontologie pour l’extraction d’opinions dans les corpus français. Étant donné que la construction ontologique à partir de zéro coûte cher et prend du temps, le modèle est construit en adaptant les catégories d’évaluation de modélisation ontologique existant en anglais. La méthode de construction se compose de deux étapes principales : premièrement, l’ontologie des évaluations est traduite en français par une approche conceptuelle ; ensuite, différentes stratégies d’adaptation sont mises en œuvre pour améliorer le résultat de cette traduction. Les stratégies d’adaptation sont basées sur l’exploration de texte, des méthodes faiblement supervisées et l’utilisation de WordNet pour affiner le modèle. L’objectif de la phase d’adaptation est de faire face aux limites de la traduction &quot;concept-to-concept&quot;, d’intégrer les concepts et les relations à partir de corpus et de ressources externes, et de construire un modèle qui capture diverses caractéristiques des opinions. L’ontologie a été conçue et construite pour intégrer des informations linguistiques et linguistiques supplémentaires sur la description des opinions en français. Le modèle a été validé en utilisant à la fois des connaissances d’experts pour valider la pertinence des concepts et des relations et des critères formels pour décrire les qualités ontologiques à des fins pratiques.","4","2023-06-27 17:30:50","2023-06-27 17:38:05","13034",,"2","1","3","2","2","1",NULL,,"2023-06-27 05:30:44","0"
"3616243","This article presents an ontological and terminological resource guided process for targeted extraction of scientific experimental data. Our method relies on the scientific publication representation (SciPuRe) describing the extracted data through ontological, lexical and structural (using segments in the scientific documents) features. Relevance scores based on these features are computed to rank the results and filter out the numerous false positives. Linear and sequential combinations of these scores are presented and evaluated. Experiments were carried out on a corpus of 50 English language scientific papers in the food packaging field. They revealed that article segment are an effective criterion for filtering out a majority of the quantitative entity false positives using lexical scores. Moreover the best symbolic entity extraction results were obtained with a sequential combinations of semantic and lexical scores. These results enable the ranking of entities by relevance and the filtering of false positive results.","Towards combined semantic and lexical scores based on a new representation of textual data to extract experimental data from scientific publications","Martin Lentschat, Patrice Buche, Juliette Dibie-Barthelemy, Mathieu Roche","4794","Vers des scores sémantiques et lexicaux combinés basés sur une nouvelle représentation des données textuelles pour extraire des données expérimentales de publications scientifiques

Cet article présente un processus guidé par des ressources ontologiques et terminologiques pour l'extraction ciblée de données expérimentales scientifiques. Notre méthode s'appuie sur la représentation des publications scientifiques (SciPuRe) qui décrit les données extraites à l'aide de caractéristiques ontologiques, lexicales et structurelles (en utilisant des segments dans les documents scientifiques). Des scores de pertinence basés sur ces caractéristiques sont calculés pour classer les résultats et filtrer les nombreux faux positifs. Des combinaisons linéaires et séquentielles de ces scores sont présentées et évaluées. Des expériences ont été menées sur un corpus de 50 documents scientifiques en langue anglaise dans le domaine de l'emballage alimentaire. Elles ont révélé que le segment d'article est un critère efficace pour filtrer une majorité de faux positifs d'entités quantitatives à l'aide de scores lexicaux. En outre, les meilleurs résultats d'extraction d'entités symboliques ont été obtenus avec des combinaisons séquentielles de scores sémantiques et lexicaux. Ces résultats permettent de classer les entités en fonction de leur pertinence et de filtrer les résultats faussement positifs.","19264","3","245","Vers des scores sémantiques et lexicaux combinés basés sur une nouvelle représentation des données textuelles pour extraire des données expérimentales de publications scientifiques

Cet article présente un processus guidé par des ressources ontologiques et terminologiques pour l&#039;extraction ciblée de données expérimentales scientifiques. Notre méthode s&#039;appuie sur la représentation des publications scientifiques (SciPuRe) qui décrit les données extraites à l&#039;aide de caractéristiques ontologiques, lexicales et structurelles (en utilisant des segments dans les documents scientifiques). Des scores de pertinence basés sur ces caractéristiques sont calculés pour classer les résultats et filtrer les nombreux faux positifs. Des combinaisons linéaires et séquentielles de ces scores sont présentées et évaluées. Des expériences ont été menées sur un corpus de 50 documents scientifiques en langue anglaise dans le domaine de l&#039;emballage alimentaire. Elles ont révélé que le segment d&#039;article est un critère efficace pour filtrer une majorité de faux positifs d&#039;entités quantitatives à l&#039;aide de scores lexicaux. En outre, les meilleurs résultats d&#039;extraction d&#039;entités symboliques ont été obtenus avec des combinaisons séquentielles de scores sémantiques et lexicaux. Ces résultats permettent de classer les entités en fonction de leur pertinence et de filtrer les résultats faussement positifs.","4","2023-06-27 17:46:04","2023-06-27 17:52:29","19264",,"1","1","1","1","1","1",NULL,,"2023-06-27 05:42:39","0"
"3866253","Tagging in the context of online resources is a fundamental addition to search systems. Tags assist with the indexing, management, and retrieval of online products and services to answer complex user queries. Traditional methods of matching user queries with tags either rely on cosine similarity, or employ semantic similarity models that fail to recognize conceptual connections between tags, e.g. ambiance and music. In this work, we focus on subjective tags which characterize subjective aspects of a product or service. We propose conceptual similarity to leverage conceptual awareness when assessing similarity between tags. We also provide a simple cost-effective pipeline to automatically generate data in order to train the conceptual similarity model. We show that our pipeline generates high-quality datasets, and evaluate the similarity model both systematically and on a downstream application. Experiments show that conceptual similarity outperforms existing work when using subjective tags.","Conceptual Similarity for Subjective Tags","Yacine Gaci, Boualem Benatallah, Fabio Casati, Khalid Benabdeslem","467","Similarité conceptuelle pour les balises subjectives

Le marquage dans le contexte des ressources en ligne est un ajout fondamental aux systèmes de recherche. Les balises facilitent l'indexation, la gestion et l'extraction de produits et services en ligne pour répondre aux requêtes complexes des utilisateurs. Les méthodes traditionnelles de mise en correspondance des requêtes des utilisateurs avec les étiquettes reposent sur la similarité cosinus, ou utilisent des modèles de similarité sémantique qui ne reconnaissent pas les connexions conceptuelles entre les étiquettes, par exemple l'ambiance et la musique. Dans ce travail, nous nous concentrons sur les étiquettes subjectives qui caractérisent les aspects subjectifs d'un produit ou service. Nous proposons une similarité conceptuelle pour tirer parti de la conscience conceptuelle lors de l'évaluation de la similarité entre les étiquettes. Nous fournissons également un pipeline simple et rentable pour générer automatiquement des données afin de former le modèle de similarité conceptuelle. Nous montrons que notre pipeline génère des ensembles de données de haute qualité, et évaluons le modèle de similarité à la fois systématiquement et sur une application en aval. Les expériences montrent que la similarité conceptuelle surpasse le travail existant en utilisant des étiquettes subjectives.","4214","6","246","Similarité conceptuelle pour les tags subjectifs

Les tags dans le contexte des ressources en ligne sont un ajout fondamental aux systèmes de recherche. Ils facilitent l&#039;indexation, la gestion et l&#039;extraction de produits et services en ligne pour répondre aux requêtes complexes des utilisateurs. Les méthodes traditionnelles de mise en correspondance des requêtes des utilisateurs avec les tags reposent sur la similarité cosinus, ou utilisent des modèles de similarité sémantique qui ne reconnaissent pas les connexions conceptuelles entre les tags, par exemple l&#039;ambiance et la musique. Dans ce travail, nous nous concentrons sur les tags subjectifs qui caractérisent les aspects subjectifs d&#039;un produit ou service. Nous proposons une similarité conceptuelle pour tirer parti de la conscience conceptuelle lors de l&#039;évaluation de la similarité entre les tags. Nous fournissons également une pipeline simple et rentable pour générer automatiquement des données afin de former le modèle de similarité conceptuelle. Nous montrons que notre pipeline génère des ensembles de données de haute qualité, et évaluons le modèle de similarité à la fois systématiquement et sur une application en aval. Les expériences montrent que la similarité conceptuelle surpasse le travail existant en utilisant des tags subjectifs.","4","2023-06-27 17:52:38","2023-06-27 17:58:15","4214",,"2","1","3","1","1","3",NULL,,"2023-06-27 05:52:34","0"
"3855457","During an interaction, interactants exchange speaking turns. Exchanges can be done smoothly or through interruptions. Listeners can display backchannels, send signals to grab the speaking turn, wait for the speaker to yield the turn, or even interrupt and grab the speaking turn. Interruptions are very frequent in natural interactions. To create believable and engaging interaction between human interactants and embodied conversational agent ECA, it is important to endow virtual agent with the capability to manage interruptions, that is to have the ability to interrupt, but also to react to an interruption. As a first step, we focus on the later one where the agent is able to perceive and interpret the user's multimodal behaviors as either an attempt or not to take the turn. To this aim, we annotate, analyse and characterize interruptions in humanhuman conversations. In this paper, we describe our annotation schema that embeds different types of interruptions. We then provide an analysis of multimodal features, focusing of prosodic features (F0 and loudness) and body (head and hand) activity, to characterize interruptions.","Multimodal Analysis of Interruptions","Liu Yang, Catherine Achard, Catherine Pelachaud","4134","Analyse multimodale des interruptions

Au cours d'une interaction, les interlocuteurs s'échangent des tours de parole. Les échanges peuvent se faire en douceur ou par le biais d'interruptions. Les auditeurs peuvent afficher des canaux de retour, envoyer des signaux pour prendre le tour de parole, attendre que le locuteur cède son tour, ou même interrompre et prendre le tour de parole. Les interruptions sont très fréquentes dans les interactions naturelles. Pour créer une interaction crédible et attrayante entre les interlocuteurs humains et l'agent conversationnel incarné ECA, il est important de doter l'agent virtuel de la capacité de gérer les interruptions, c'est-à-dire d'avoir la capacité d'interrompre, mais aussi de réagir à une interruption. Dans un premier temps, nous nous concentrons sur cette dernière, où l'agent est capable de percevoir et d'interpréter les comportements multimodaux de l'utilisateur comme une tentative ou un refus de prendre le tour. À cette fin, nous annotons, analysons et caractérisons les interruptions dans les conversations entre humains. Dans cet article, nous décrivons notre schéma d'annotation qui intègre différents types d'interruptions. Nous fournissons ensuite une analyse des caractéristiques multimodales, en nous concentrant sur les caractéristiques prosodiques (F0 et intensité sonore) et l'activité corporelle (tête et mains), afin de caractériser les interruptions.","17145","3","250","Analyse multimodale des interruptions

Lors d&#039;une interaction, les interlocuteurs s&#039;échangent la parole. Les échanges peuvent se faire en douceur ou par le biais d&#039;interruptions. Les auditeurs peuvent afficher des canaux de retour, envoyer des signaux pour prendre la parole, attendre que le locuteur cède son tour, ou même interrompre et prendre la parole. Les interruptions sont très fréquentes dans les interactions naturelles. Pour créer une interaction crédible et attrayante entre les interlocuteurs humains et l&#039;agent conversationnel animé (ACA), il est important de doter l&#039;agent virtuel de la capacité de gérer les interruptions, c&#039;est-à-dire d&#039;avoir la capacité d&#039;interrompre, mais aussi de réagir à une interruption. Dans un premier temps, nous nous concentrons sur cette dernière, où l&#039;agent est capable de percevoir et d&#039;interpréter les comportements multimodaux de l&#039;utilisateur comme une tentative ou un refus de prendre la parole. À cette fin, nous annotons, analysons et caractérisons les interruptions dans les conversations entre humains. Dans cet article, nous décrivons notre schéma d&#039;annotation qui intègre différents types d&#039;interruptions. Nous fournissons ensuite une analyse des caractéristiques multimodales, en nous concentrant sur les caractéristiques prosodiques (F0 et intensité sonore) et l&#039;activité corporelle (tête et mains), afin de caractériser les interruptions.","4","2023-06-28 16:12:44","2023-06-28 16:44:04","17145",,"1","1","2","2","1","1",NULL,"problème de collocation (&quot;prendre le tour&quot; de parole et pas &quot;prendre la parole&quot;) comptabilisé comme un problème de style","2023-06-28 04:12:39","0"
"3714863","In the context of Human-Robot Interaction (HRI), emotional understanding is becoming more popular because it turns robots more humanized and user-friendly. Giving a robot the ability to recognize emotions has several difficulties due to the limits of the robots’ hardware and the real-world environments in which it works. In this sense, an out-of-robot approach and a multimodal approach can be the solution. This paper presents the implementation of a previous proposed multi-modal emotional system in the context of social robotics; that works on a server and bases its prediction in four modalities as inputs (face, posture, body, and context features) captured through the robot’s sensors; the predicted emotion triggers some robot behavior changes. Working on a server allows overcoming the robot’s hardware limitations but gaining some delay in the communication. Working with several modalities allows facing complex real-world scenarios strongly and adaptively. This research is focused on analyzing, explaining, and arguing the usability and viability of an out-of-robot and multimodal approach for emotional robots. Functionality tests were applied with the expected results, demonstrating that the entire proposed system takes around two seconds; delay justified on the deep learning models used, which are improvable. Regarding the HRI evaluations, a brief discussion about the remaining assessments is presented, explaining how difficult it can be a well-done evaluation of this work. The demonstration of the system functionality can be seen at https://youtu.be/MYYfazSa2N0.","Multimodal Emotional Understanding in Robotics","Juanpablo Heredia, Yudith Cardinale, Irvin Dongo, Ana Aguilera, Jose Diaz-Amado","4526","Compréhension émotionnelle multimodale en robotique

Dans le contexte de l’interaction Human-Robot (HRI), la compréhension émotionnelle devient de plus en plus populaire parce qu’elle rend les robots plus humanisés et plus conviviaux. Donner à un robot la capacité de reconnaître les émotions a plusieurs difficultés en raison des limites du matériel des robots et des environnements réels dans lesquels il travaille. En ce sens, une approche out-of-robot et une approche multimodale peuvent être la solution. Le présent document présente la mise en œuvre d’un système émotionnel multimodal proposé précédemment dans le contexte de la robotique sociale; qui fonctionne sur un serveur et base sa prédiction en quatre modalités sous forme d’entrées (visage, posture, corps et caractéristiques contextuelles) capturées par les capteurs du robot; L’émotion prédite déclenche certains changements de comportement des robots. Travailler sur un serveur permet de surmonter les limitations matérielles du robot mais de gagner un certain retard dans la communication. Travailler avec plusieurs modalités permet de faire face à des scénarios complexes du monde réel avec force et adaptation. Cette recherche est axée sur l’analyse, l’explication et l’argumentation de l’utilisabilité et de la viabilité d’une approche extra-robot et multimodale pour les robots émotionnels. Des tests de fonctionnalité ont été effectués avec les résultats escomptés, démontrant que l’ensemble du système proposé prend environ deux secondes; retard justifié sur les modèles d’apprentissage profond utilisés, qui sont improvisables. En ce qui concerne les évaluations de l’IRH, une brève discussion sur les évaluations restantes est présentée, expliquant à quel point il peut être difficile d’évaluer correctement ce travail. La démonstration de la fonctionnalité du système peut être consultée à l’adresse https://youtu.be/MYYfazSa2N0.","11313","4","258","Compréhension émotionnelle multimodale en robotique

Dans le contexte de l’interaction humain-robot (IHR), la compréhension émotionnelle devient de plus en plus populaire parce qu’elle rend les robots plus humanisés et plus simples d&#039;utilisation. Donner à un robot la capacité de reconnaître les émotions a plusieurs difficultés en raison des limites du matériel des robots et des environnements réels dans lesquels il travaille. En ce sens, une approche hors robot et une approche multimodale peuvent être la solution. Le présent document présente la mise en œuvre d’un système émotionnel multimodal proposé précédemment dans le contexte de la robotique sociale ; qui fonctionne sur un serveur et base sa prédiction en quatre modalités sous forme d’entrées (visage, posture, corps et caractéristiques contextuelles) capturées par les capteurs du robot. L’émotion prédite déclenche certains changements de comportement des robots. Travailler sur un serveur permet de surmonter les limitations matérielles du robot mais de gagner un certain retard dans la communication. Travailler avec plusieurs modalités permet de faire face à des scénarios complexes du monde réel avec force et adaptation. Cette recherche est axée sur l’analyse, l’explication et l’argumentation de l’utilisabilité et de la viabilité d’une approche extra-robot et multimodale pour les robots émotionnels. Des tests de fonctionnalité ont été effectués avec les résultats attendus, démontrant que l’ensemble du système proposé prend environ deux secondes ; retard justifié sur les modèles d’apprentissage profond utilisés, qui sont improvisables. En ce qui concerne les évaluations de l’IHR, une brève discussion sur les évaluations restantes est présentée, expliquant à quel point il peut être difficile d’évaluer correctement ce travail. La démonstration de la fonctionnalité du système peut être consultée à l’adresse https://youtu.be/MYYfazSa2N0.","4","2023-06-28 18:07:28","2023-06-28 18:08:15","11313",,"1","1","2","2","2","1",NULL,,"2023-06-28 06:07:26","0"
"3704256","Coarse-Grained Reconfigurable Architectures (CGRAs) emerged about 30 years ago. The very first CGRAs were programmed manually. Fortunately, some compilation approaches appeared rapidly to automate the mapping process. Numerous surveys on these architectures exist. Other surveys also gather the tools and methods, but none of them focuses on the mapping process only. This paper focuses solely on automated methods and techniques for mapping applications on CGRA and covers the last two decades of research. This paper aims at providing the terminology, the problem formulation, and a classification of existing methods. The paper ends with research challenges and trends for the future.","Twenty Years of Automated Methods for Mapping Applications on CGRA","Kevin Martin","4549","Vingt ans de méthodes automatisées pour les applications cartographiques sur CGRA

Les architectures reconfigurables à gros grains (CGRA) sont apparues il y a environ 30 ans. Les toutes premières CGRA étaient programmées manuellement. Heureusement, certaines approches de compilation sont apparues rapidement pour automatiser le processus de mise en correspondance. Il existe de nombreuses études sur ces architectures. D'autres études rassemblent également les outils et les méthodes, mais aucune d'entre elles ne se concentre uniquement sur le processus de mise en correspondance. Le présent document se concentre uniquement sur les méthodes et techniques automatisées pour les applications de cartographie sur le CGRA et couvre les deux dernières décennies de recherche. Il vise à fournir la terminologie, la formulation du problème et une classification des méthodes existantes. Il se termine par les défis de la recherche et les tendances pour l'avenir.","19019","3","259","Vingt ans de méthodes automatisées pour les applications cartographiques sur les CGRA

Les architectures reconfigurables CGRA sont apparues il y a environ 30 ans. Les toutes premières CGRA étaient programmées manuellement. Heureusement, certaines approches de compilation sont apparues rapidement pour automatiser le processus de mise en correspondance. Il existe de nombreuses études sur ces architectures. D&#039;autres études rassemblent également les outils et les méthodes, mais aucune d&#039;entre elles ne se concentre uniquement sur le processus de mise en correspondance. Le présent document se concentre uniquement sur les méthodes et techniques automatisées pour les applications de cartographie sur les CGRA et couvre les deux dernières décennies de recherche. Il vise à fournir la terminologie, la formulation du problème et une classification des méthodes existantes. Il se termine par les défis de la recherche et les tendances pour l&#039;avenir.","4","2023-06-28 18:11:15","2023-06-28 18:35:38","19019",,"1","1","3","2","1","1",NULL,"https://www.ensta-bretagne.fr/fr/equipe-architectures-materielles-et-outils-de-cao-arcad","2023-06-28 06:11:03","0"
"3839919","Dialog state tracking (DST) is a core step for task-oriented dialogue systems aiming to track the user's current goal during a dialogue. Recently a special focus has been put on applying existing DST models to new domains, in other words performing zero-shot crossdomain transfer. While recent state-of-theart models leverage large pre-trained language models, no work has been made on understanding and improving the results of first-developed zero-shot models like SUMBT. In this paper, we thus propose to improve SUMBT zero-shot results on MultiWOZ by using attention modulation during inference. This method improves SUMBT zero-shot results significantly on two domains and does not worsen the initial performance with the significant advantage of needing no additional training.","Attention Modulation for Zero-Shot Cross-Domain Dialogue State Tracking","Mathilde Veron, Guillaume Bernard, Olivier Galibert, Sophie Rosset","499","Modulation de l’attention pour le suivi de l’état du dialogue Cross-Domain à zéro coup

Le suivi des états de dialogue (DST) est une étape essentielle pour les systèmes de dialogue axés sur les tâches visant à suivre l’objectif actuel de l’utilisateur au cours d’un dialogue. Récemment, une attention particulière a été accordée à l’application des modèles DST existants à de nouveaux domaines, c’est-à-dire à l’exécution d’un transfert interdomaine zéro-shot. Alors que les modèles récents de pointe tirent parti de grands modèles de langage pré-formés, aucun travail n’a été fait sur la compréhension et l’amélioration des résultats des premiers modèles zéro-shot comme SUMBT. Dans cet article, nous proposons donc d’améliorer les résultats SUMBT zero-shot sur MultiWOZ en utilisant la modulation de l’attention lors de l’inférence. Cette méthode améliore considérablement les résultats de SUMBT zero-shot sur deux domaines et n’aggrave pas les performances initiales avec l’avantage significatif de ne pas avoir besoin d’une formation supplémentaire.","770","4","265","Modulation de l’attention pour le suivi de l’état du dialogue inter-domaine &quot;zero-shot&quot;

Le suivi de l&#039;état du dialogue (DST) est une étape essentielle pour les systèmes de dialogue axés sur les tâches visant à suivre l’objectif actuel de l’utilisateur au cours d’un dialogue. Récemment, une attention particulière a été accordée à l’application des modèles de DST existants à de nouveaux domaines, c’est-à-dire à l’exécution d’un transfert inter-domaine zero-shot. Alors que les derniers modèles de pointe tirent parti de grands modèles de langage pré-entraînés, aucun travail n’a été fait sur la compréhension et l’amélioration des résultats des premiers modèles zero-shot comme SUMBT. Dans cet article, nous proposons donc d’améliorer les résultats SUMBT zero-shot sur MultiWOZ en utilisant la modulation de l’attention lors de l’inférence. Cette méthode améliore considérablement les résultats de SUMBT zero-shot sur deux domaines et n’aggrave pas les performances initiales avec l’avantage significatif de ne pas avoir besoin d’une formation supplémentaire.","4","2023-06-29 15:11:22","2023-06-29 15:39:34","770",,"2","1","3","2","2","3",NULL,"dialog state tracking (DST) : le corpus TALN donne, pour la requête &quot;DST&quot; une occurrence &quot;suivi de l&#039;état du dialogue (DST)&quot;. La requête &quot;suivi de l&#039; état du dialogue&quot; donne 16 occurrences. Fait intéressant, la TA propose la bonne traduction dans le titre mais pas dans la première phrase du résumé.","2023-06-29 03:11:09","0"
"3807744","Faced with the ever-increasing number of scientific publications, researchers struggle to keep up, find and make sense of articles relevant to their own research. Scientific open archives play a central role in helping deal with this deluge, yet keyword-based search services often fail to grasp the richness of the semantic associations between articles. In this paper, we present the methods, tools and services implemented in the ISSA project to tackle these issues. The project aims to (1) provide a generic, reusable and extensible pipeline for the analysis and processing of articles of an open scientific archive, (2) translate the result into a semantic index stored and represented as an RDF knowledge graph; (3) develop innovative search and visualization services that leverage this index to allow researchers, decision makers or scientific information professionals to explore thematic association rules, networks of co-publications, articles with co-occurring topics, etc. To demonstrate the effectiveness of the solution, we also report on its deployment and user-driven customization for the needs of an institutional open archive of 110,000+ resources. Fully in line with the open science and FAIR dynamics, the presented work is available under an open license with all the accompanying documents necessary to facilitate its reuse. The knowledge graph produced on our use-case is compliant with common linked open data best practices.","ISSA: Generic Pipeline, Knowledge Model and Visualization tools to Help Scientists Search and Make Sense of a Scientific Archive","Anne Toulet, Franck Michel, Anna Bobasheva, Aline Menin, Sébastien Dupré, Marie-Claude Deboin, Marco Winckler, Andon Tchechmedjiev","4272","ISSA : Pipeline générique, modèle de connaissance et outils de visualisation pour aider les scientifiques à rechercher et à donner du sens à une archive scientifique

Face à l'augmentation constante du nombre de publications scientifiques, les chercheurs ont du mal à suivre, à trouver et à donner un sens aux articles pertinents pour leurs propres recherches. Les archives ouvertes scientifiques jouent un rôle central dans la gestion de ce déluge, mais les services de recherche basés sur des mots-clés ne parviennent souvent pas à saisir la richesse des associations sémantiques entre les articles. Dans cet article, nous présentons les méthodes, les outils et les services mis en œuvre dans le cadre du projet ISSA pour résoudre ces problèmes. Le projet vise à (1) fournir un pipeline générique, réutilisable et extensible pour l'analyse et le traitement des articles d'une archive scientifique ouverte, (2) traduire le résultat en un index sémantique stocké et représenté sous la forme d'un graphe de connaissances RDF ; (3) développer des services de recherche et de visualisation innovants qui exploitent cet index pour permettre aux chercheurs, aux décideurs ou aux professionnels de l'information scientifique d'explorer les règles d'association thématique, les réseaux de co-publications, les articles avec des sujets cooccurrents, etc. Pour démontrer l'efficacité de la solution, nous présentons également son déploiement et sa personnalisation par l'utilisateur pour les besoins d'une archive ouverte institutionnelle de plus de 110 000 ressources. Conformément à la dynamique de la science ouverte et de FAIR, le travail présenté est disponible sous une licence ouverte avec tous les documents d'accompagnement nécessaires pour faciliter sa réutilisation. Le graphe de connaissances produit sur notre cas d'utilisation est conforme aux meilleures pratiques en matière de données ouvertes liées.","17283","3","267","ISSA : Pipeline générique, modèle de connaissance et outils de visualisation pour aider les scientifiques à rechercher et à donner du sens à une archive scientifique

Face à l&#039;augmentation constante du nombre de publications scientifiques, les chercheurs ont du mal à suivre, à trouver et à donner un sens aux articles pertinents pour leurs propres recherches. Les archives ouvertes scientifiques jouent un rôle central dans la gestion de ce déluge, mais les services de recherche basés sur des mots-clés ne parviennent souvent pas à saisir la richesse des associations sémantiques entre les articles. Dans cet article, nous présentons les méthodes, les outils et les services mis en œuvre dans le cadre du projet ISSA pour résoudre ces problèmes. Le projet vise à (1) fournir un pipeline générique, réutilisable et extensible pour l&#039;analyse et le traitement des articles d&#039;une archive scientifique ouverte, (2) traduire le résultat en un index sémantique stocké et représenté sous la forme d&#039;un graphe RDF ; (3) développer des services de recherche et de visualisation innovants qui exploitent cet index pour permettre aux chercheurs, aux décideurs ou aux professionnels de l&#039;information scientifique d&#039;explorer les règles d&#039;association thématique, les réseaux de co-publications, les articles avec des sujets cooccurrents, etc. Pour démontrer l&#039;efficacité de la solution, nous présentons également son déploiement et sa personnalisation par l&#039;utilisateur pour les besoins d&#039;une archive ouverte institutionnelle de plus de 110 000 ressources. Conformément à la dynamique de la science ouverte et de FAIR, le travail présenté est disponible sous une licence ouverte avec tous les documents d&#039;accompagnement nécessaires pour faciliter sa réutilisation. Le graphe de connaissances produit sur notre cas d&#039;utilisation est conforme aux meilleures pratiques en matière de données ouvertes liées.","4","2023-06-29 15:39:55","2023-06-29 15:55:43","17283",,"1","1","2","1","1","1",NULL,,"2023-06-29 03:39:48","0"
"3660218","This paper introduces a novel method for merging opendomain terminological knowledge. It takes advantage of the Region Connection Calculus (RCC5), a formalism used to represent regions in a topological space and to reason about their set-theoretic relationships. To this end, we first propose a faithful translation of terminological knowledge provided by several and potentially conflicting sources into region spaces. The merging is then performed on these spaces, and the result is translated back into the underlying language of the input sources. Our approach allows us to benefit from the expressivity and the flexibility of RCC5 while dealing with conflicting knowledge in a principled way.","Region-Based Merging of Open-Domain Terminological Knowledge (extended version including the proofs of propositions)","Zied Bouraoui, Sébastien Konieczny, Thanh Ma, Nicolas Schwind, Ivan Varzinczak","4681","Fusion régionale des connaissances terminologiques à domaine ouvert (version étendue incluant les preuves des propositions)

Cet article présente un nouveau procédé de fusion de connaissances terminologiques opendomaines. Il tire parti du calcul de connexion de région (RCC5), un formalisme utilisé pour représenter les régions dans un espace topologique et pour raisonner sur leurs relations théoriques. A cette fin, nous proposons d'abord une traduction fidèle des connaissances terminologiques fournies par plusieurs sources potentiellement contradictoires dans les espaces régionaux. La fusion est ensuite effectuée sur ces espaces, et le résultat est retraduit dans le langage sous-jacent des sources d'entrée. Notre approche nous permet de bénéficier de l'expressivité et de la flexibilité de RCC5 tout en traitant les connaissances contradictoires d'une manière basée sur des principes.","16597","6","269","Fusion régionale des connaissances terminologiques à domaine ouvert (version étendue incluant les preuves des propositions)

Cet article présente un nouveau procédé de fusion de connaissances terminologiques à domaine ouvert. Il tire parti du calcul des connexions entre régions (RCC5), un formalisme utilisé pour représenter les régions dans un espace topologique et pour raisonner sur leurs relations théoriques. À cette fin, nous proposons d&#039;abord une traduction fidèle des connaissances terminologiques fournies par plusieurs sources potentiellement contradictoires dans les espaces régionaux. La fusion est ensuite effectuée sur ces espaces, et le résultat est retraduit dans le langage sous-jacent des sources d&#039;entrée. Notre approche nous permet de bénéficier de l&#039;expressivité et de la flexibilité du RCC5 tout en traitant les connaissances contradictoires d&#039;une manière basée sur des principes.","4","2023-06-29 15:55:55","2023-06-29 16:04:04","16597",,"1","1","2","1","2","1",NULL,,"2023-06-29 03:55:48","0"
"4009515","The BigScience Workshop was a value-driven initiative that spanned one and half years of interdisciplinary research and culminated in the creation of ROOTS, a 1.6TB multilingual dataset that was used to train BLOOM, one of the largest multilingual language models to date. In addition to the technical outcomes and artifacts, the workshop fostered multidisciplinary collaborations around large models, datasets, and their analysis. This in turn led to a wide range of research publications spanning topics from ethics to law, data governance, modeling choices and distributed training. This paper focuses on the collaborative research aspects of BigScience and takes a step back to look at the challenges of large-scale participatory research, with respect to participant diversity and the tasks required to successfully carry out such a project. Our main goal is to share the lessons we learned from this experience, what we could have done better and what we did well. We show how the impact of such a social approach to scientific research goes well beyond the technical artifacts that were the basis of its inception.","BigScience: A Case Study in the Social Construction of a Multilingual Large Language Model","Christopher Akiki, Giada Pistilli, Margot Mieskes, Matthias Gallé, Thomas Wolf, Suzana Ilić, Yacine Jernite","3755","BigScience : Étude de cas sur la construction sociale d'un modèle multilingue de grandes langues

L'atelier BigScience était une initiative axée sur la valeur qui a duré un an et demi de recherche interdisciplinaire et a abouti à la création de ROOTS, un ensemble de données multilingue de 1,6 To qui a été utilisé pour former BLOOM, l'un des plus grands modèles linguistiques multilingues à ce jour. En plus des résultats techniques et des artefacts, l'atelier a favorisé des collaborations multidisciplinaires autour de grands modèles, ensembles de données et leur analyse. Cela a donné lieu à un large éventail de publications de recherche couvrant des sujets allant de l'éthique au droit, la gouvernance des données, les choix de modélisation et la formation distribuée. Ce document se concentre sur les aspects de recherche collaborative de BigScience et prend un peu de recul pour examiner les défis de la recherche participative à grande échelle, en ce qui concerne la diversité des participants et les tâches requises pour mener à bien un tel projet. Notre objectif principal est de partager les leçons que nous avons apprises de cette expérience, ce que nous aurions pu faire mieux et ce que nous avons bien fait. Nous montrons comment l'impact d'une telle approche sociale de la recherche scientifique va bien au-delà des artefacts techniques qui étaient à la base de sa création.","15671","6","272","BigScience : Étude de cas sur la construction sociale d&#039;un grand modèle de langue multilingue

Le workshop BigScience était une initiative axée sur la valeur qui a duré un an et demi de recherche interdisciplinaire et a abouti à la création de ROOTS, un ensemble de données multilingue de 1,6 To qui a été utilisé pour former BLOOM, l&#039;un des plus grands modèles de langue multilingues à ce jour. En plus des résultats techniques et des artéfacts, l&#039;atelier a favorisé des collaborations multidisciplinaires autour de grands modèles, ensembles de données et leur analyse. Cela a donné lieu à un large éventail de publications de recherche couvrant des sujets allant de l&#039;éthique au droit, la gouvernance des données, les choix de modélisation et la formation distribuée. Ce document se concentre sur les aspects de recherche collaborative de BigScience et prend un peu de recul pour examiner les défis de la recherche participative à grande échelle, en ce qui concerne la diversité des participants et les tâches requises pour mener à bien un tel projet. Notre objectif principal est de partager les leçons que nous avons tirées de cette expérience, ce que nous aurions pu faire mieux et ce que nous avons bien fait. Nous montrons comment l&#039;impact d&#039;une telle approche sociale de la recherche scientifique va bien au-delà des artéfacts techniques qui étaient à la base de sa création.","4","2023-06-29 16:04:24","2023-06-29 16:16:14","15671",,"2","1","3","2","1","1",NULL,,"2023-06-29 04:04:15","0"
"3757538","This preface for the proceedings of the first international workshop on Modular Knowledge (MK2022) presents the content of the workshop and reports on the results of the interactive sessions that took place during the event. The dramatic increase in the amount of open and linked data and the increasing semantification of such data make clear that knowledge is not monolithic, static or uniform. This requires a renewed push for dealing with heterogeneous and distributed knowledge as a constellation of modules. Each module stores a portion of knowledge about one particular subdomain, described in a specific schema, and valid under a set of circumstances. In such a scenario we need well-founded conceptual approaches and practical techniques for modular knowledge management, for example, to recognize relevant partitions of a monolithic knowledge source, but also to define a modularized vision of the domain qualifying the knowledge with a given situation or agent, integrating heterogeneous modules of knowledge, including knowledge represented in sub-symbolic models. The discussion of such modularity notions and techniques, their development and exploitation are the focus of the proposed Modular Knowledge workshop. The Modular Knowledge workshop combines the efforts of previous experiences (like WoMO, ARCOE-Logic and WOMoCoE workshops) into an interdisciplinary venue for discussing and developing solutions for modularity of knowledge. The workshop series aims at covering the use of various approaches (ranging from rich semantic representations, like Knowledge Graphs and formal ontology, to simpler schemas, like RDF and database schemas) for representing knowledge, its context, its evolution, and for making it accessible to automatic reasoning and knowledge management tasks. The Modular Knowledge workshop covers logic-based languages as well as subsymbolic and numerical representations.","Modular Knowledge 2022 Preface","Loris Bozzato, Valentina Carriero, Torsten Hahmann, Antoine Zimmermann","4422","Préface Connaissance Modulaire 2022

Cette préface pour les actes du premier atelier international sur le savoir modulaire (MK2022) présente le contenu de l'atelier et rend compte des résultats des sessions interactives qui ont eu lieu pendant l'événement. L'augmentation spectaculaire de la quantité de données ouvertes et liées et la sémantisation croissante de ces données montrent clairement que la connaissance n'est pas monolithique, statique ou uniforme. Cela nécessite un nouvel élan pour traiter les connaissances hétérogènes et distribuées comme une constellation de modules. Chaque module stocke une partie des connaissances concernant un sous-domaine particulier, décrit dans un schéma spécifique, et valide dans un ensemble de circonstances. Dans un tel scénario, nous avons besoin d'approches conceptuelles bien fondées et de techniques pratiques pour la gestion modulaire des connaissances, par exemple, pour reconnaître les partitions pertinentes d'une source de connaissances monolithique, mais aussi pour définir une vision modulaire du domaine qualifiant les connaissances avec une situation ou un agent donné, intégrant des modules hétérogènes de connaissances, y compris des connaissances représentées dans des modèles sous-symboliques. La discussion de ces notions et techniques de modularité, leur développement et leur exploitation sont au centre de l'atelier proposé sur les connaissances modulaires. L'atelier Connaissances Modulaires combine les efforts des expériences précédentes (comme les ateliers WoMO, ARCOE-Logic et WOMoCoE) en un lieu interdisciplinaire pour discuter et développer des solutions pour la modularité des connaissances. La série d'ateliers vise à couvrir l'utilisation de diverses approches (allant de riches représentations sémantiques, comme les graphes de connaissance et l'ontologie formelle, à des schémas plus simples, comme RDF et des schémas de base de données) pour représenter la connaissance, son contexte, son évolution, et pour la rendre accessible aux tâches de raisonnement automatique et de gestion de la connaissance. L'atelier Connaissance Modulaire couvre les langages logiques ainsi que les représentations sous-symboliques et numériques.","16338","6","274","Préface Connaissance modulaire 2022

Cette préface pour les actes du premier workshop international sur le savoir modulaire (MK2022) présente le contenu du workshop et rend compte des résultats des sessions interactives qui ont eu lieu pendant l&#039;événement. L&#039;augmentation spectaculaire de la quantité de données ouvertes et liées et la sémantisation croissante de ces données montrent clairement que la connaissance n&#039;est pas monolithique, statique ou uniforme. Cela nécessite un nouvel élan pour traiter les connaissances hétérogènes et distribuées comme une constellation de modules. Chaque module stocke une partie des connaissances concernant un sous-domaine particulier, décrit dans un schéma spécifique, et valide dans un ensemble de circonstances. Dans un tel scénario, nous avons besoin d&#039;approches conceptuelles bien fondées et de techniques pratiques pour la gestion modulaire des connaissances, par exemple, pour reconnaître les partitions pertinentes d&#039;une source de connaissances monolithique, mais aussi pour définir une vision modulaire du domaine qualifiant les connaissances avec une situation ou un agent donné, intégrant des modules hétérogènes de connaissances, y compris des connaissances représentées dans des modèles sous-symboliques. La discussion de ces notions et techniques de modularité, leur développement et leur exploitation sont au centre du workshop proposé sur les connaissances modulaires. L&#039;atelier Connaissances modulaires combine les efforts des expériences précédentes (comme les ateliers WoMO, ARCOE-Logic et WOMoCoE) en un lieu interdisciplinaire pour discuter et développer des solutions pour la modularité des connaissances. La série d&#039;ateliers vise à couvrir l&#039;utilisation de diverses approches (allant de riches représentations sémantiques, comme les graphes de connaissance et l&#039;ontologie formelle, à des schémas plus simples, comme RDF et des schémas de base de données) pour représenter la connaissance, son contexte, son évolution, et pour la rendre accessible aux tâches de raisonnement automatique et de gestion de la connaissance. L&#039;atelier Connaissance modulaire couvre les langues logiques ainsi que les représentations sous-symboliques et numériques.","4","2023-06-29 16:19:55","2023-06-29 16:26:50","16338",,"1","1","2","2","2","1",NULL,,"2023-06-29 04:19:18","0"
"3858484","Evaluating automatic speech recognition (ASR) systems is a classical but difficult and still open problem, which often boils down to focusing only on the word error rate (WER). However, this metric suffers from many limitations and does not allow an in-depth analysis of automatic transcription errors. In this paper, we propose to study and understand the impact of rescoring using language models in ASR systems by means of several metrics often used in other natural language processing (NLP) tasks in addition to the WER. In particular, we introduce two measures related to morpho-syntactic and semantic aspects of transcribed words: 1) the POSER (Part-of-speech Error Rate), which should highlight the grammatical aspects, and 2) the Em-bER (Embedding Error Rate), a measurement that modifies the WER by providing a weighting according to the semantic distance of the wrongly transcribed words. These metrics illustrate the linguistic contributions of the language models that are applied during a posterior rescoring step on transcription hypotheses.","Qualitative Evaluation of Language Model Rescoring in Automatic Speech Recognition","Thibault Roux, Mickael Rouvier, Jane Wottawa, Richard Dufour","4125","Évaluation qualitative du recalage des modèles de langue dans la reconnaissance automatique de la parole

L'évaluation des systèmes de reconnaissance automatique de la parole (ASR) est un problème classique, difficile et encore ouvert, qui se résume souvent à se concentrer uniquement sur le taux d'erreurs de mots (WER). Cependant, cette métrique souffre de nombreuses limitations et ne permet pas une analyse approfondie des erreurs de transcription automatique. Dans cet article, nous proposons d'étudier et de comprendre l'impact du rescoring à l'aide de modèles de langage dans les systèmes ASR à l'aide de plusieurs mesures souvent utilisées dans d'autres tâches de traitement du langage naturel (NLP) en plus du WER. En particulier, nous introduisons deux mesures liées aux aspects morpho-syntaxiques et sémantiques des mots transcrits : 1) le POSER (Part-of-speech Error Rate), qui devrait mettre en évidence les aspects grammaticaux, et 2) l'Em-bER (Embedding Error Rate), une mesure qui modifie le WER en fournissant une pondération en fonction de la distance sémantique des mots mal transcrits. Ces mesures illustrent les contributions linguistiques des modèles de langage qui sont appliqués lors d'une étape de recalage a posteriori des hypothèses de transcription.","17136","3","275","Évaluation qualitative du rescoring des modèles de langue dans la reconnaissance automatique de la parole

L&#039;évaluation des systèmes de reconnaissance automatique de la parole (ASR) est un problème classique, difficile et encore ouvert, qui se résume souvent à se concentrer sur le taux d&#039;erreur sur les mots (WER). Cependant, cette métrique souffre de nombreuses limitations et ne permet pas une analyse approfondie des erreurs de transcription automatique. Dans cet article, nous proposons d&#039;étudier et de comprendre l&#039;impact du rescoring à l&#039;aide de modèles de langage dans les systèmes d&#039;ASR à l&#039;aide de plusieurs métriques souvent utilisées dans d&#039;autres tâches de traitement automatique des langues (TAL) en plus du WER. En particulier, nous introduisons deux mesures liées aux aspects morpho-syntaxiques et sémantiques des mots transcrits : 1) le POSER (Part-of-speech Error Rate), qui devrait mettre en évidence les aspects grammaticaux, et 2) l&#039;Em-bER (Embedding Error Rate), une mesure qui modifie le WER en fournissant une pondération en fonction de la distance sémantique des mots mal transcrits. Ces mesures illustrent les contributions linguistiques des modèles de langage qui sont appliqués lors d&#039;une étape de rescoring a posteriori des hypothèses de transcription.","4","2023-06-29 16:28:14","2023-06-29 16:38:57","17136",,"1","1","2","2","1","1",NULL,,"2023-06-29 04:28:05","0"
"3832870","The main objective of this work is to study the expressivity transfer in a speaker's voice for which no expressive speech data is available in non-autoregressive end-to-end TTS systems. We investigated the expressivity transfer capability of probability density estimation based on deep generative models, namely Generative Flow (Glow) and diffusion probabilistic models (DPM). The usage of deep generative models provides better log likelihood estimates and tractability of the system, subsequently providing high-quality speech synthesis with faster inference speed. Furthermore, we propose the usage of various expressivity encoders, which assist in expressivity transfer in the text-to-speech (TTS) system. More precisely, we used self-attention statistical pooling and multi-scale expressivity encoder architectures for creating a meaningful representation of expressivity. In addition to traditional subjective metrics used for speech synthesis evaluation, we incorporated cosine-similarity to measure the strength of attributes associated with speaker and expressivity. The performance of a non-autoregressive TTS system with a multi-scale expressivity encoder showed better expressivity transfer on Glow and DPM-based decoders. Thus, illustrating the ability of multi-scale architecture to apprehend the underlying attributes of expressivity from multiple acoustic features.","Analysis of expressivity transfer in non-autoregressive end-to-end multispeaker TTS systems","Ajinkya Kulkarni, Vincent Colotte, Denis Jouvet","4204","Analyse du transfert d'expressivité dans les systèmes TTS non autorégressifs de bout en bout à plusieurs locuteurs

L'objectif principal de ce travail est d'étudier le transfert d'expressivité dans la voix d'un locuteur pour lequel aucune donnée vocale expressive n'est disponible dans les systèmes TTS de bout en bout non autorégressifs. Nous avons étudié la capacité de transfert d'expressivité de l'estimation de la densité de probabilité basée sur des modèles génératifs profonds, à savoir Generative Flow (Glow) et les modèles probabilistes de diffusion (DPM). L'utilisation de modèles génératifs profonds permet d'obtenir de meilleures estimations de la vraisemblance logarithmique et d'améliorer la traçabilité du système, ce qui permet d'obtenir une synthèse vocale de haute qualité avec une vitesse d'inférence plus rapide. En outre, nous proposons l'utilisation de divers codeurs d'expressivité, qui aident au transfert de l'expressivité dans le système de synthèse vocale. Plus précisément, nous avons utilisé la mise en commun statistique de l'auto-attention et des architectures de codeurs d'expressivité multi-échelles pour créer une représentation significative de l'expressivité. En plus des mesures subjectives traditionnelles utilisées pour l'évaluation de la synthèse vocale, nous avons incorporé la cosinusimilarité pour mesurer la force des attributs associés au locuteur et à l'expressivité. Les performances d'un système TTS non autorégressif doté d'un codeur d'expressivité multi-échelle ont montré un meilleur transfert d'expressivité sur les décodeurs basés sur Glow et DPM. Cela illustre la capacité de l'architecture multi-échelle à appréhender les attributs sous-jacents de l'expressivité à partir de plusieurs caractéristiques acoustiques.","17215","3","277","Analyse du transfert d&#039;expressivité dans les systèmes TTS à plusieurs locuteurs non autorégressifs de bout en bout

L&#039;objectif principal de ce travail est d&#039;étudier le transfert d&#039;expressivité dans la voix d&#039;un locuteur pour lequel aucune donnée vocale expressive n&#039;est disponible dans les systèmes TTS de bout en bout non autorégressifs. Nous avons étudié la capacité de transfert d&#039;expressivité de l&#039;estimation de la densité de probabilité basée sur des modèles génératifs profonds, à savoir Generative Flow (Glow) et les modèles probabilistes de diffusion (DPM). L&#039;utilisation de modèles génératifs profonds permet d&#039;obtenir de meilleures estimations de la vraisemblance logarithmique et d&#039;améliorer la traçabilité du système, ce qui permet d&#039;obtenir une synthèse vocale de haute qualité avec une vitesse d&#039;inférence plus rapide. En outre, nous proposons l&#039;utilisation de divers codeurs d&#039;expressivité, qui aident au transfert de l&#039;expressivité dans le système de synthèse vocale. Plus précisément, nous avons utilisé la mise en commun statistique de l&#039;auto-attention et des architectures de codeurs d&#039;expressivité multi-échelles pour créer une représentation significative de l&#039;expressivité. En plus des mesures subjectives traditionnelles utilisées pour l&#039;évaluation de la synthèse vocale, nous avons incorporé la similarité cosinus pour mesurer la force des attributs associés au locuteur et à l&#039;expressivité. Les performances d&#039;un système TTS non autorégressif doté d&#039;un codeur d&#039;expressivité multi-échelle ont montré un meilleur transfert d&#039;expressivité sur les décodeurs basés sur Glow et DPM. Cela illustre la capacité de l&#039;architecture multi-échelle à appréhender les attributs sous-jacents de l&#039;expressivité à partir de plusieurs caractéristiques acoustiques.","4","2023-06-29 16:41:57","2023-06-29 17:05:46","17215",,"2","1","3","2","1","2",NULL,,"2023-06-29 04:39:41","0"
"3572743","This edited book takes an interdisciplinary approach to shed light on the complex dynamics involved in the incidence of online hate speech against migrants in user-generated contexts. The authors draw on case studies from Finland, France, Germany, Italy, Poland and the UK, bringing together qualitative and quantitative analyses on user-generated online comments. The authors argue that online hate speech against migrants must be understood as a symptom of a representation crisis on migration, which can only be fully perceived through the study of the complex linguistic, interactional and connective processes within which it emerges. They focus on representations and shared meanings, community building and otherness, and delve into the role of network ecosystems in the process of the construction of public problems. This book will be of interest to undergraduate and post-graduate students as well as academics working on hate speech and migration studies in a variety of fields, and can also contribute to improving research protocols for automated analyses and detections of online hate speech.","Cyberhate in the Context of Migrations",,"4873","Le cyberhat dans le contexte des migrations

Ce livre édité adopte une approche interdisciplinaire pour mettre en lumière les dynamiques complexes impliquées dans l'incidence des discours de haine en ligne contre les migrants dans des contextes générés par les utilisateurs. Les auteurs s'appuient sur des études de cas menées en Finlande, en France, en Allemagne, en Italie, en Pologne et au Royaume-Uni, et rassemblent des analyses qualitatives et quantitatives sur les commentaires en ligne générés par les utilisateurs. Les auteurs soutiennent que le discours de haine en ligne contre les migrants doit être compris comme un symptôme d'une crise de représentation sur la migration, qui ne peut être pleinement perçue que par l'étude des processus linguistiques, interactionnels et connectifs complexes au sein desquels il émerge. Ils se concentrent sur les représentations et les significations partagées, la construction de communautés et l'altérité, et approfondissent le rôle des écosystèmes de réseaux dans le processus de construction des problèmes publics. Cet ouvrage intéressera les étudiants de premier et deuxième cycles ainsi que les universitaires travaillant sur les discours de haine et les études sur les migrations dans divers domaines. Il peut également contribuer à l'amélioration des protocoles de recherche pour les analyses et les détections automatisées des discours de haine en ligne.","17525","3","279","La haine en ligne dans le contexte des migrations

Ce livre adopte une approche interdisciplinaire pour mettre en lumière les dynamiques complexes impliquées dans l&#039;incidence des discours haineux en ligne envers les migrants dans des contextes générés par les utilisateurs. Les auteurs s&#039;appuient sur des études de cas menées en Finlande, en France, en Allemagne, en Italie, en Pologne et au Royaume-Uni, et rassemblent des analyses qualitatives et quantitatives sur les commentaires en ligne générés par les utilisateurs. Les auteurs soutiennent que les discours haineux en ligne envers les migrants doit être compris comme un symptôme d&#039;une crise de représentation sur les migrations, qui ne peut être pleinement perçue que par l&#039;étude des processus linguistiques, interactionnels et connectifs complexes au sein desquels il émerge. Ils se concentrent sur les représentations et les significations partagées, la construction de communautés et l&#039;altérité, et approfondissent le rôle des écosystèmes de réseaux dans le processus de construction des problèmes publics. Cet ouvrage intéressera les étudiants de premier et deuxième cycles ainsi que les universitaires travaillant sur les discours haineux et les études sur les migrations dans divers domaines. Il peut également contribuer à l&#039;amélioration des protocoles de recherche pour les analyses et les détections automatisées des discours haineux en ligne.","4","2023-06-29 17:08:18","2023-06-29 17:14:59","17525",,"1","1","3","2","1","1",NULL,,"2023-06-29 05:08:15","0"
"3772359","The spoofing-aware speaker verification (SASV) challenge was designed to promote the study of jointly-optimised solutions to accomplish the traditionally separately-optimised tasks of spoofing detection and speaker verification. Jointly-optimised systems have the potential to operate in synergy as a better performing solution to the single task of reliable speaker verification. However, none of the 23 submissions to SASV 2022 are jointly optimised. We have hence sought to determine why separately-optimised sub-systems perform best or why joint optimisation was not successful. Experiments reported in this paper show that joint optimisation is successful in improving robustness to spoofing but that it degrades speaker verification performance. The findings suggest that spoofing detection and speaker verification sub-systems should be optimised jointly in a manner which reflects the differences in how information provided by each sub-system is complementary to that provided by the other. Progress will also likely depend upon the collection of data from a larger number of speakers.","On the potential of jointly-optimised solutions to spoofing attack detection and automatic speaker verification","Wanying Ge, Hemlata Tak, Massimiliano Todisco, Nicholas Evans","589","Sur le potentiel des solutions optimisées conjointement pour la détection des attaques par usurpation d'identité et la vérification automatique du locuteur

Le défi ""spoofing-aware speaker verification"" (SASV) a été conçu pour promouvoir l'étude de solutions optimisées conjointement pour accomplir les tâches traditionnellement optimisées séparément de la détection de l'usurpation et de la vérification du locuteur. Les systèmes optimisés conjointement ont le potentiel de fonctionner en synergie en tant que solution plus performante à la tâche unique de vérification fiable du locuteur. Cependant, aucun des 23 systèmes soumis à SASV 2022 n'est optimisé conjointement. Nous avons donc cherché à déterminer pourquoi les sous-systèmes optimisés séparément sont plus performants ou pourquoi l'optimisation conjointe n'a pas été couronnée de succès. Les expériences présentées dans cet article montrent que l'optimisation conjointe permet d'améliorer la résistance à l'usurpation, mais qu'elle dégrade les performances de la vérification du locuteur. Les résultats suggèrent que les sous-systèmes de détection de l'usurpation et de vérification du locuteur devraient être optimisés conjointement de manière à refléter les différences dans la façon dont les informations fournies par chaque sous-système sont complémentaires de celles fournies par l'autre. Les progrès dépendront probablement aussi de la collecte de données provenant d'un plus grand nombre de locuteurs.","10841","3","285","Sur le potentiel des solutions optimisées conjointement pour la détection des attaques par usurpation d&#039;identité et la vérification automatique du locuteur

Le défi &quot;spoofing-aware speaker verification&quot; (SASV) a été conçu pour promouvoir l&#039;étude de solutions optimisées conjointement pour accomplir les tâches traditionnellement optimisées séparément de la détection de l&#039;usurpation et de la vérification du locuteur. Les systèmes optimisés conjointement ont le potentiel de fonctionner en synergie en tant que solution plus performante à la tâche unique de vérification fiable du locuteur. Cependant, aucun des 23 systèmes soumis à SASV 2022 n&#039;est optimisé conjointement. Nous avons donc cherché à déterminer pourquoi les sous-systèmes optimisés séparément sont plus performants ou pourquoi l&#039;optimisation conjointe n&#039;a pas été couronnée de succès. Les expériences présentées dans cet article montrent que l&#039;optimisation conjointe permet d&#039;améliorer la résistance à l&#039;usurpation, mais qu&#039;elle dégrade les performances de la vérification du locuteur. Les résultats suggèrent que les sous-systèmes de détection de l&#039;usurpation et de vérification du locuteur devraient être optimisés conjointement de manière à refléter les différences dans la façon dont les informations fournies par chaque sous-système sont complémentaires de celles fournies par l&#039;autre. Les progrès dépendront probablement aussi de la collecte de données provenant d&#039;un plus grand nombre de locuteurs.","4","2023-06-30 07:29:49","2023-06-30 07:44:50","10841",,"1","1","1","1","1","1",NULL,,"2023-06-30 07:29:45","0"
"3803880","BERT models used in specialized domains all seem to be the result of a simple strategy: initializing with the original BERT and then resuming pre-training on a specialized corpus. This method yields rather good performance (e.g. BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), BlueBERT (Peng et al., 2019)). However, it seems reasonable to think that training directly on a specialized corpus, using a specialized vocabulary, could result in more tailored embeddings and thus help performance. To test this hypothesis, we train BERT models from scratch using many configurations involving general and medical corpora. Based on evaluations using four different tasks, we find that the initial corpus only has a weak influence on the performance of BERT models when these are further pre-trained on a medical corpus.","Re-train or train from scratch? Comparing pre-training strategies of BERT in the medical domain","Hicham El Boukkouri, Olivier Ferret, Thomas Lavergne, Pierre Zweigenbaum","552","Re-trainer ou s’entraîner à partir de zéro? Comparaison des stratégies de préformation du BERT dans le domaine médical

Les modèles Bert utilisés dans les domaines spécialisés semblent tous être le résultat d’une stratégie simple: initialiser avec le BERT original, puis reprendre la préformation sur un corpus spécialisé. Cette méthode donne de bons résultats (par exemple BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), Bluebert (Peng et al., 2019)). Cependant, il semble raisonnable de penser que la formation directement sur un corpus spécialisé, à l’aide d’un vocabulaire spécialisé, pourrait entraîner des intégrations plus personnalisées et ainsi aider à la performance. Pour tester cette hypothèse, nous formons des modèles BERT à partir de zéro en utilisant de nombreuses configurations impliquant des corpus généraux et médicaux. Sur la base d’évaluations utilisant quatre tâches différentes, nous constatons que le corpus initial n’a qu’une faible influence sur la performance des modèles BERT lorsque ceux-ci sont encore pré-formés sur un corpus médical.","823","4","286","Ré-entraîner ou entraîner à partir de rien ? Comparaison des stratégies de pré-entraînement du BERT dans le domaine médical

Les modèles BERT utilisés dans les domaines spécialisés semblent tous être le résultat d’une stratégie simple : commencer avec le modèle BERT original, puis reprendre le pré-entraînement sur un corpus spécialisé. Cette méthode donne de bons résultats (par exemple BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), BlueBERT (Peng et al., 2019)). Cependant, il semble raisonnable de penser que l&#039;entraînement réalisé directement sur un corpus spécialisé, à l’aide d’un vocabulaire spécialisé, pourrait entraîner des intégrations plus personnalisées et ainsi augmenter la performance. Pour tester cette hypothèse, nous formons des modèles BERT à partir de zéro en utilisant de nombreuses configurations impliquant des corpus généraux et médicaux. Sur la base d’évaluations utilisant quatre tâches différentes, nous constatons que le corpus initial n’a qu’une faible influence sur la performance des modèles BERT lorsque ceux-ci sont ensuite pré-entraînés sur un corpus médical.","4","2023-06-30 07:45:02","2023-06-30 07:55:17","823",,"2","4","3","4","2","3",NULL,,"2023-06-30 07:44:56","0"
"3602177","Background: In life sciences, there has been a long-standing effort of standardization and integration of reference datasets and databases. Despite these efforts, many studies data are provided using specific and non-standard formats. This hampers the capacity to reuse the studies data in other pipelines, the capacity to reuse the pipelines results in other studies, and the capacity to enrich the data with additional information. The Regulatory Circuits project is one of the largest efforts for integrating human cell genomics data to predict tissue-specific transcription factor-genes interaction networks. In spite of its success, it exhibits the usual shortcomings limiting its update, its reuse (as a whole or partially), and its extension with new data samples. To address these limitations, the resource has previously been integrated in an RDF triplestore so that TF-gene interaction networks could be generated with two SPARQL queries. However, this triplestore did not store the computed networks and did not integrate metadata about tissues and samples, therefore limiting the reuse of this dataset. In particular, it does not enable to reuse only a portion of Regulatory Circuits if a study focuses on a subset of the tissues, nor to combine the samples described in the datasets with samples from other studies. Overall, these limitations advocate for the design of a complete, flexible and reusable representation of the Regulatory Circuits dataset based on Semantic Web technologies.Results: We provide a modular RDF representation of the Regulatory Circuits, called Linked Extended Regulatory Circuits (LERC). It consists in (i) descriptions of biological and experimental context mapped to the references databases, (ii) annotations about TF-gene interactions at the sample level for 808 samples, (iii) annotations about TF-gene interactions at the tissue level for 394 tissues, (iv) metadata connecting the knowledge graphs cited above. LERC is based on a modular organisation into 1,205 RDF named graphs for representing the biological data, the sample-specific and the tissue-specific networks, and the corresponding metadata. In total it contains 3,910,794,050 triples and is available as a SPARQL endpoint.Conclusion: The flexible and modular architecture of LERC supports biologically-relevant SPARQL queries. It allows an easy and fast querying of the resources related to the initial Regulatory Circuits datasets and facilitates its reuse in other studies.Associated website: https://regulatorycircuits-lod.genouest.org","Improving reusability along the data life cycle: a Regulatory Circuits Case Study","Marine Louarn, Fabrice Chatonnet, Xavier Garnier, Thierry Fest, Anne Siegel, Catherine Faron, Olivier Dameron","4817","Améliorer la réutilisabilité tout au long du cycle de vie des données : une étude de cas sur les circuits de régulation

Contexte : Dans le domaine des sciences de la vie, la normalisation et l'intégration des ensembles de données et des bases de données de référence font l'objet d'un effort de longue haleine. Malgré ces efforts, de nombreuses données d'études sont fournies dans des formats spécifiques et non standard. Cela entrave la capacité de réutiliser les données des études dans d'autres pipelines, la capacité de réutiliser les résultats des pipelines dans d'autres études, et la capacité d'enrichir les données avec des informations supplémentaires. Le projet Regulatory Circuits est l'un des plus grands efforts d'intégration des données génomiques des cellules humaines pour prédire les réseaux d'interaction entre les gènes et les facteurs de transcription spécifiques aux tissus. Malgré son succès, il présente les lacunes habituelles qui limitent sa mise à jour, sa réutilisation (totale ou partielle) et son extension avec de nouveaux échantillons de données. Pour remédier à ces limitations, la ressource a été précédemment intégrée dans un triplestore RDF afin que les réseaux d'interaction TF-gènes puissent être générés à l'aide de deux requêtes SPARQL. Cependant, ce triplestore ne stockait pas les réseaux calculés et n'intégrait pas les métadonnées sur les tissus et les échantillons, ce qui limite la réutilisation de ce jeu de données. En particulier, il ne permet pas de réutiliser une partie seulement des circuits de régulation si une étude se concentre sur un sous-ensemble de tissus, ni de combiner les échantillons décrits dans les ensembles de données avec des échantillons provenant d'autres études. Dans l'ensemble, ces limitations plaident en faveur de la conception d'une représentation complète, flexible et réutilisable de l'ensemble de données sur les circuits de régulation, basée sur les technologies du Web sémantique : Nous fournissons une représentation RDF modulaire des circuits de régulation, appelée Linked Extended Regulatory Circuits (LERC). Elle consiste en (i) des descriptions du contexte biologique et expérimental mises en correspondance avec les bases de données de référence, (ii) des annotations sur les interactions TF-gène au niveau de l'échantillon pour 808 échantillons, (iii) des annotations sur les interactions TF-gène au niveau du tissu pour 394 tissus, (iv) des métadonnées reliant les graphes de connaissances cités ci-dessus. LERC est basé sur une organisation modulaire en 1 205 graphes nommés RDF pour représenter les données biologiques, les réseaux spécifiques aux échantillons et aux tissus, et les métadonnées correspondantes. Au total, il contient 3 910 794 050 triplets et est disponible en tant que point de terminaison SPARQL : L'architecture flexible et modulaire de LERC prend en charge les requêtes SPARQL pertinentes sur le plan biologique. Elle permet d'interroger facilement et rapidement les ressources liées aux ensembles de données initiaux sur les circuits de régulation et facilite leur réutilisation dans d'autres études.Site web associé : https://regulatorycircuits-lod.genouest.org","17469","3","287","Améliorer la réutilisabilité tout au long du cycle de vie des données : étude de cas sur les circuits de régulation

Contexte : Dans le domaine des sciences de la vie, la normalisation et l&#039;intégration des ensembles de données et des bases de données de référence font l&#039;objet d&#039;un effort de longue haleine. Malgré ces efforts, de nombreuses données d&#039;études sont fournies dans des formats spécifiques et non standard. Cela entrave la capacité de réutiliser les données des études dans d&#039;autres pipelines, la capacité de réutiliser les résultats des pipelines dans d&#039;autres études, et la capacité d&#039;enrichir les données avec des informations supplémentaires. Le projet Regulatory Circuits est l&#039;un des plus grands efforts d&#039;intégration des données génomiques des cellules humaines pour prédire les réseaux d&#039;interaction entre les gènes et les facteurs de transcription spécifiques aux tissus. Malgré son succès, il présente les lacunes habituelles qui limitent sa mise à jour, sa réutilisation (totale ou partielle) et son extension avec de nouveaux échantillons de données. Pour remédier à ces limitations, la ressource a été précédemment intégrée dans un triplestore RDF afin que les réseaux d&#039;interaction entre gènes et facteurs de transcription puissent être générés à l&#039;aide de deux requêtes SPARQL. Cependant, ce triplestore ne stockait pas les réseaux calculés et n&#039;intégrait pas les métadonnées sur les tissus et les échantillons, ce qui limite la réutilisation de ce jeu de données. En particulier, il ne permet pas de réutiliser une partie seulement des circuits de régulation si une étude se concentre sur un sous-ensemble de tissus, ni de combiner les échantillons décrits dans les ensembles de données avec des échantillons provenant d&#039;autres études. Dans l&#039;ensemble, ces limitations plaident en faveur de la conception d&#039;une représentation complète, flexible et réutilisable de l&#039;ensemble de données sur les circuits de régulation, basée sur les technologies du Web sémantique : nous fournissons une représentation RDF modulaire des circuits de régulation, appelée Linked Extended Regulatory Circuits (LERC). Elle consiste en (i) des descriptions du contexte biologique et expérimental mises en correspondance avec les bases de données de référence, (ii) des annotations sur les interactions entre gènes et facteurs de transcription au niveau de l&#039;échantillon pour 808 échantillons, (iii) des annotations sur les interactions entre gènes et facteurs de transcription au niveau du tissu pour 394 tissus, (iv) des métadonnées reliant les graphes de connaissances cités ci-dessus. LERC est basé sur une organisation modulaire en 1 205 graphes nommés RDF pour représenter les données biologiques, les réseaux spécifiques aux échantillons et aux tissus, et les métadonnées correspondantes. Au total, il contient 3 910 794 050 triplets et est disponible en tant que point de terminaison SPARQL : L&#039;architecture flexible et modulaire de LERC prend en charge les requêtes SPARQL pertinentes sur le plan biologique. Elle permet d&#039;interroger facilement et rapidement les ressources liées aux ensembles de données initiaux sur les circuits de régulation et facilite leur réutilisation dans d&#039;autres études. Site web associé : https://regulatorycircuits-lod.genouest.org","4","2023-06-30 07:56:29","2023-06-30 08:48:40","17469",,"1","1","3","2","1","1",NULL,"&quot;TF-gene interaction&quot; : interaction entre gènes et facteurs de transcription (TF = transcription factor. &quot;Les facteurs de transcription sont des protéines qui se lient à des séquences d&#039;ADN spécifiques afin de réguler l&#039;expression d&#039;un gène donné.&quot; https://www.anticorps-enligne.fr/areas/epigenetics/transcription-factors/)
Il semble qu&#039;il n&#039;y a pas d&#039;abréviation en français.","2023-06-30 07:56:20","0"
"3694666","The Open Access Citation Advantage (OACA) has been a major topic of discussion in the literature over the past twenty years. In this paper, we propose a method to constitute a control group to isolate the OACA effect. Thus, we compared citation impact (MNCS) of 2,458,378 publications in fully OA journals to that (weighted MNCS) of a control group of non-OA publications (#10,310,842). Similarly, we did the same exercise for OA publications in hybrid journals (#1,024,430) and their control group (#11,533,001), over the period 2010-2020. The results showed that there is no OACA for publications in fully OA journals, and that there is rather a disadvantage. Conversely, the OACA seems to be a reality in hybrid journals, suggesting that a better accessibility tends to improve the impact of publications. The lack of OACA for publications in fully OA journals is to be expected, as a great proportion of OA journals are newly created and less attractive to high-impact senior researchers.Another striking result of this paper is the fall of the OACA from 2016. The citation advantage fell from 70% to 9% between 2016 and 2020 (for publications in hybrid journals). We wonder if this fall is linked to the increase in the notoriety of pirate sites (eg Sci-Hub) from 2016. In other words, the democratization of pirate sites instantly cancels the positive effect of OA publication insofar as the question of access to scientific content no longer arises.","Does Open Access Really Increase Impact? A Large-Scale Randomized Analysis","Abdelghani Maddi, David Sapinho","4590","L'Open Access Augmente-T-Il Réellement Son Impact ? Analyse Randomisée À Grande Échelle

L'avantage de la citation libre accès (OACA) a été un sujet de discussion majeur dans la littérature au cours des vingt dernières années. Dans cet article, nous proposons une méthode pour constituer un groupe témoin pour isoler l'effet OACA. Ainsi, nous avons comparé l'impact des citations (MNCS) de 2 458 378 publications dans des revues entièrement consacrées à l'arthrose à celui (MNCS pondérés) d'un groupe témoin de publications non liées à l'arthrose (10 310 842). De même, nous avons effectué le même exercice pour les publications sur l'arthrose dans des revues hybrides (#1 024 430) et leur groupe témoin (#1 533 001), au cours de la période 2010-2020. Les résultats ont montré qu'il n'y a pas d'OACA pour les publications dans les revues entièrement OA, et qu'il y a plutôt un désavantage. À l'inverse, l'OACA semble être une réalité dans les revues hybrides, suggérant qu'une meilleure accessibilité tend à améliorer l'impact des publications. Il faut s'attendre à ce qu'il n'y ait pas suffisamment de publications du BCAA dans les revues entièrement consacrées à l'arthrose, car une grande proportion de ces revues sont nouvellement créées et moins attrayantes pour les chercheurs chevronnés à impact élevé. L'avantage de citation est passé de 70 % à 9 % entre 2016 et 2020 (pour les publications dans des revues hybrides). Nous nous demandons si cette chute est liée à l'augmentation de la notoriété des sites pirates (par exemple Sci-Hub) à partir de 2016. En d'autres termes, la démocratisation des sites pirates annule instantanément l'effet positif de la publication OA dans la mesure où la question de l'accès au contenu scientifique ne se pose plus.","16506","6","288","L&#039;Open Access augmente-t-il réellement l&#039;impact ? Analyse randomisée à grande échelle

L&#039;avantage de la citation open access (OACA, Open Access Citation Advantage) a été un sujet de discussion majeur dans la littérature au cours des vingt dernières années. Dans cet article, nous proposons une méthode pour constituer un groupe témoin pour isoler l&#039;effet OACA. Ainsi, nous avons comparé l&#039;impact des citations (MNCS) de 2 458 378 publications dans des revues entièrement consacrées à l&#039;arthrose à celui d&#039;un groupe témoin de 10 310 842 publications non liées à l&#039;arthrose (MNCS pondérés). Nous avons effectué le même exercice pour les publications sur l&#039;arthrose dans des revues hybrides (1 024 430) et leur groupe témoin (1 533 001), sur la période 2010-2020. Les résultats ont montré qu&#039;il n&#039;y a pas d&#039;OACA pour les publications dans les revues entièrement OA, et qu&#039;il y a plutôt un désavantage. À l&#039;inverse, l&#039;OACA semble être une réalité dans les revues hybrides, suggérant qu&#039;une meilleure accessibilité tend à améliorer l&#039;impact des publications. Il faut s&#039;attendre à ce qu&#039;il n&#039;y ait pas suffisamment de publications du BCAA dans les revues entièrement consacrées à l&#039;arthrose, car une grande proportion de ces revues sont nouvellement créées et moins attrayantes pour les chercheurs chevronnés à impact élevé. L&#039;avantage de citation est passé de 70 % à 9 % entre 2016 et 2020 (pour les publications dans des revues hybrides). Nous nous demandons si cette chute est liée à l&#039;augmentation de la notoriété des sites pirates (par exemple Sci-Hub) à partir de 2016. En d&#039;autres termes, la démocratisation des sites pirates annule instantanément l&#039;effet positif de la publication OA dans la mesure où la question de l&#039;accès au contenu scientifique ne se pose plus.","4","2023-06-30 08:52:19","2023-06-30 09:04:23","16506",,"1","2","1","3","3","2",NULL,,"2023-06-30 08:48:57","0"
"3996465","We propose a semantically-aware speech driven model to generate expressive and natural upper-facial and head motion for Embodied Conversational Agents (ECA). In this work, we aim to produce natural and continuous head motion and upper-facial gestures synchronized with speech. We propose a model that generates these gestures based on multimodal input features: the first modality is text, and the second one is speech prosody. Our model makes use of Transformers and Convolutions to map the multimodal features that correspond to an utterance to continuous eyebrows and head gestures. We conduct subjective and objective evaluations to validate our approach and compare it with state of the art.","Transformer Network for Semantically-Aware and Speech-Driven Upper-Face Generation","Mireille Fares, Catherine Pelachaud, Nicolas Obin","3775","Réseau de transformateurs pour la génération de visages sémantiquement conscients et pilotés par la parole

Nous proposons un modèle sémantiquement conscient de la parole pour générer des mouvements expressifs et naturels de la tête et du haut du visage pour les agents conversationnels incarnés (ECA). Dans ce travail, nous visons à produire des mouvements naturels et continus de la tête et des gestes de la partie supérieure du visage synchronisés avec la parole. Nous proposons un modèle qui génère ces gestes en se basant sur des caractéristiques d'entrée multimodales : la première modalité est le texte, et la seconde est la prosodie de la parole. Notre modèle utilise des transformateurs et des convolutions pour faire correspondre les caractéristiques multimodales correspondant à un énoncé à des gestes continus des sourcils et de la tête. Nous menons des évaluations subjectives et objectives pour valider notre approche et la comparer à l'état de l'art.","18591","3","289","Réseau de transformateurs pour la génération de hauts de visages sémantiquement conscients et pilotés par la parole

Nous proposons un modèle sémantiquement conscient de la parole pour générer des mouvements expressifs et naturels de la tête et du haut du visage pour les agents conversationnels animés (ACA). Dans ce travail, nous visons à produire des mouvements naturels et continus de la tête et des gestes de la partie supérieure du visage synchronisés avec la parole. Nous proposons un modèle qui génère ces gestes en se basant sur des caractéristiques d&#039;entrée multimodales : la première modalité est le texte, et la seconde est la prosodie de la parole. Notre modèle utilise des transformateurs et des convolutions pour faire correspondre les caractéristiques multimodales correspondant à un énoncé à des gestes continus des sourcils et de la tête. Nous menons des évaluations subjectives et objectives pour valider notre approche et la comparer à l&#039;état de l&#039;art.","4","2023-06-30 09:04:42","2023-06-30 09:46:19","18591",,"1","1","3","1","1","1",NULL,,"2023-06-30 09:04:32","0"
"3943681","Feature structures have been several times considered to enrich categorial grammars in order to build fine-grained grammars. Most attempts to unify both frameworks either model categorial types as feature structures or add feature structures on top of categorial types. We pursue a different approach: using feature structure as categorial atomic types. In this article, we present a procedure to create, from a simplified HPSG grammar, an equivalent abstract categorial grammar (ACG). We represent a feature structure by the enumeration of its totally well-typed upper bounds, so that unification can be simulated as intersection. We implement this idea as a meta-ACG preprocessor.","Simulating Feature Structures with Simple Types","Valentin Richard","398","Simuler des structures d'entités avec des types simples

Les structures de caractéristiques ont été envisagées à plusieurs reprises pour enrichir les grammaires catégorielles afin de construire des grammaires à grain fin. La plupart des tentatives d'unification des deux cadres modélisent les types catégoriels comme des structures de caractéristiques ou ajoutent des structures de caractéristiques au-dessus des types catégoriels. Nous poursuivons une approche différente : nous utilisons les structures de caractéristiques comme des types catégoriels atomiques. Dans cet article, nous présentons une procédure permettant de créer, à partir d'une grammaire HPSG simplifiée, une grammaire catégorielle abstraite (ACG) équivalente. Nous représentons une structure de caractéristiques par l'énumération de ses limites supérieures totalement bien typées, de sorte que l'unification peut être simulée comme une intersection. Nous implémentons cette idée sous la forme d'un préprocesseur méta-ACG.","10152","3","294","Simuler des structures d&#039;entités avec des types simples

Les structures de caractéristiques ont été envisagées à plusieurs reprises pour enrichir les grammaires catégorielles afin de construire des grammaires précises. La plupart des tentatives d&#039;unification des deux cadres modélisent les types catégoriels comme des structures de caractéristiques ou ajoutent des structures de caractéristiques au-dessus des types catégoriels. Nous adoptons une approche différente : nous utilisons les structures de caractéristiques comme des types catégoriels atomiques. Dans cet article, nous présentons une procédure permettant de créer, à partir d&#039;une grammaire HPSG simplifiée, une grammaire catégorielle abstraite (ACG) équivalente. Nous représentons une structure de caractéristiques par l&#039;énumération de ses limites supérieures totalement bien typées, de sorte que l&#039;unification peut être simulée comme une intersection. Nous implémentons cette idée sous la forme d&#039;un préprocesseur méta-ACG.","4","2023-06-30 09:48:36","2023-06-30 10:34:54","10152",,"1","1","1","2","1","1",NULL,,"2023-06-30 09:48:31","0"
"3866083","Knowledge Graphs (KG) offer easy-to-process information. An important issue to build a KG from texts is the Relation Extraction (RE) task that identifies and labels relationships between entity mentions. In this paper, to address the RE problem, we propose to combine a deep learning approach for relation detection, and a symbolic method for relation classification. It allows to have at the same time the performance of deep learning methods and the interpretability of symbolic methods. This method has been evaluated and compared with state-ofthe-art methods on TACRED, a relation extraction benchmark, and has shown interesting quantitative and qualitative results.","A Two-Step Approach for Explainable Relation Extraction","Hugo Ayats, Peggy Cellier, Sébastien Ferré","4101","Une approche en deux étapes pour l’extraction de relations explicables

Les Knowledge Graphs (KG) offrent des informations faciles à traiter. Un problème important pour construire un KG à partir de textes est la tâche Relation Extraction (RE) qui identifie et étiquette les relations entre les mentions d’entité. Dans cet article, pour aborder le problème des RE, nous proposons de combiner une approche d’apprentissage profond pour la détection des relations, et une méthode symbolique pour la classification des relations. Il permet d’avoir en même temps la performance des méthodes d’apprentissage profond et l’interprétabilité des méthodes symboliques. Cette méthode a été évaluée et comparée à des méthodes de pointe sur Tacred, un référentiel d’extraction de relations, et a montré des résultats quantitatifs et qualitatifs intéressants.","13059","4","314","Une approche en deux étapes pour l’extraction de relations explicables

Les graphes de connaissances (KG) offrent des informations faciles à traiter. Un problème important pour construire un KG à partir de textes est la tâche Relation Extraction (RE), qui identifie et étiquette les relations entre les mentions d’entité. Dans cet article, pour aborder le problème des RE, nous proposons de combiner une approche d’apprentissage profond pour la détection des relations, et une méthode symbolique pour la classification des relations. Il permet d’avoir en même temps la performance des méthodes d’apprentissage profond et l’interprétabilité des méthodes symboliques. Cette méthode a été évaluée et comparée à des méthodes de pointe sur Tacred, un référentiel d’extraction de relations, et a montré des résultats quantitatifs et qualitatifs intéressants.","4","2023-07-03 14:19:20","2023-07-03 14:35:43","13059",,"1","2","3","1","1","1",NULL,,"2023-07-03 02:17:41","0"
"3757579","In this work, our aim is to provide a structured answer in natural language to a complex information need. Particularly, we envision using generative models from the perspective of data-to-text generation. We propose the use of a content selection and planning pipeline which aims at structuring the answer by generating intermediate plans. The experimental evaluation is performed using the TREC Complex Answer Retrieval (CAR) dataset. We evaluate both the generated answer and its corresponding structure and show the effectiveness of planning-based models in comparison to a text-to-text model. This work has been published at ECIR 2022.","Does Structure Matter? Leveraging Data-to-Text Generation for Answering Complex Information Needs -Abstract","Hanane Djeddal, Thomas Gerald, Laure Soulier, Karen Pinel-Sauvagnat, Lynda Tamine","4421","La structure est-elle importante? Tirer parti de la génération de données à texte pour répondre aux besoins complexes d’information -Abstract

Dans ce travail, notre objectif est de fournir une réponse structurée en langage naturel à un besoin d’information complexe. En particulier, nous envisageons d’utiliser des modèles génératifs du point de vue de la génération de données au texte. Nous proposons l’utilisation d’un pipeline de sélection et de planification de contenu qui vise à structurer la réponse en générant des plans intermédiaires. L’évaluation expérimentale est réalisée à l’aide de l’ensemble de données TREC Complex Answer Retrieval (CAR). Nous évaluons à la fois la réponse générée et sa structure correspondante et montrons l’efficacité des modèles basés sur la planification par rapport à un modèle texte-texte. Ces travaux ont été publiés à l’ECIR 2022.","13379","4","320","La structure est-elle importante? Tirer parti de la génération &quot;data-to-text&quot; pour répondre aux besoins complexes d’information - Résumé

Dans ce travail, notre objectif est de fournir une réponse structurée en langage naturel à un besoin d’information complexe. En particulier, nous envisageons d’utiliser des modèles génératifs du point de vue de la génération &quot;data-to-text&quot;. Nous proposons l’utilisation d’un pipeline de sélection et de planification de contenu qui vise à structurer la réponse en générant des plans intermédiaires. L’évaluation expérimentale est réalisée à l’aide de l’ensemble de données TREC Complex Answer Retrieval (CAR). Nous évaluons à la fois la réponse générée et sa structure correspondante, et montrons l’efficacité des modèles basés sur la planification par rapport à un modèle &quot;text-to-text&quot;. Ces travaux ont été publiés à l’ECIR 2022.","4","2023-07-03 14:36:35","2023-07-03 14:52:26","13379",,"1","2","1","2","1","1",NULL,,"2023-07-03 02:36:26","0"
"3712978","Even though hate speech (HS) online has been an important object of research in the last decade, most HS-related corpora oversimplify the phenomenon of hate by attempting to label user comments as hate or neutral. This ignores the complex and subjective nature of HS, which limits the real-life applicability of classifiers trained on these corpora. In this study, we present the M-Phasis corpus, a corpus of ∼ 9k German and French user comments collected from migration-related news articles. It goes beyond the hate-neutral dichotomy and is instead annotated with 23 features, which in combination become descriptors of various types of speech, ranging from critical comments to implicit and explicit expressions of hate. The annotations are performed by 4 native speakers per language and achieve high (0.77 ≤ κ ≤ 1) inter-annotator agreements. Besides describing the corpus creation and presenting insights from a content, error and domain analysis, we explore its data characteristics by training several classification baselines.","Placing M-Phasis on the Plurality of Hate: A Feature-Based Corpus of Hate Online","Dana Ruiter, Liane Reiners, Ashwin Geet d'Sa, Thomas Kleinbauer, Dominique Fohr, Irina Illina, Dietrich Klakow, Christian Schemer, Angeliki Monnier","683","Placer M-Phasis sur la pluralité de la haine: Un Corpus basé sur les fonctionnalités de haine en ligne

Même si le discours de haine (HS) en ligne a été un objet important de recherche au cours de la dernière décennie, la plupart des corpus liés au SH simplifient le phénomène de la haine en essayant d’étiqueter les commentaires des utilisateurs comme haineux ou neutres. Cela ignore la nature complexe et subjective du SH, qui limite l’applicabilité réelle des classificateurs formés sur ces corpus. Dans cette étude, nous présentons le corpus M-Phasis, un corpus de commentaires d’utilisateurs allemands et français de 9k recueillis à partir d’articles d’actualité liés à la migration. Il va au-delà de la dichotomie neutre de la haine et est plutôt annoté avec 23 caractéristiques, qui, en combinaison, deviennent des descripteurs de divers types de discours, allant des commentaires critiques aux expressions implicites et explicites de la haine. Les annotations sont effectuées par 4 locuteurs natifs par langue et obtiennent des accords inter-annotateurs élevés (0,77 ≤ Δ ≤ 1). En plus de décrire la création de corpus et de présenter des idées à partir d’une analyse de contenu, d’erreur et de domaine, nous explorons ses caractéristiques de données en formant plusieurs bases de classification.","954","4","321","Analyser pluralité de la haine grâce à M-Phasis : un corpus de haine en ligne basé sur les caractéristiques

Même si les discours haineux (DH) en ligne ont été un objet important de recherche au cours de la dernière décennie, la plupart des corpus liés aux DH simplifient le phénomène de la haine en essayant d’étiqueter les commentaires des utilisateurs comme haineux ou neutres. Cela ignore la nature complexe et subjective des DH, qui limite l’applicabilité réelle des classificateurs formés sur ces corpus. Dans cette étude, nous présentons le corpus M-Phasis, un corpus de 9000 commentaires d’utilisateurs allemands et français recueillis à partir d’articles d’actualité liés à la migration. Il va au-delà de la dichotomie neutre de la haine et est plutôt annoté avec 23 caractéristiques, qui, en combinaison, deviennent des descripteurs de divers types de discours, allant des commentaires critiques aux expressions implicites et explicites de la haine. Les annotations sont effectuées par 4 locuteurs natifs pour chaque langue et obtiennent des accords inter-annotateurs élevés (0,77 ≤ Δ ≤ 1). En plus de décrire la création de corpus et de présenter des idées à partir d’une analyse de contenu, d’erreur et de domaine, nous explorons ses caractéristiques de données en formant plusieurs bases de classification.","4","2023-07-03 14:53:25","2023-07-03 15:44:19","954",,"2","2","3","2","1","1",NULL,,"2023-07-03 02:53:13","0"
"3783904","The way in which authors express themselves is unique but changes over their lifetime. However, quantitative studies of this idiolectal evolution are rare. Using the Corpus for Idiolectal Research (CIDRE) that contains the dated works of 11 prolific 19th century French fiction writers, we propose new methods to identify, quantify and describe the grammatical-stylistic changes that take place using lexico-morphosyntactic patterns, also called motifs. To examine the strength of the chronological signal of change, we developed a method to calculate if a distance matrix of literary works contains a stronger chronological signal than expected by chance. Ten out of 11 corpora showed a higher than chance chronological signal, leading us to conclude that the evolution of the idiolect is in a mathematical sense monotonic, supporting the rectilinearity hypothesis previously put forward in the stylometric literature. The rectilinear property of the evolution of the idiolect found for most authors in CIDRE subsequently enabled us to propose a machine learning task: predicting the year in which a work was written. For the majority of the authors in our corpus, the accuracy and the amount of variance that is explained by the model were high and we discuss why the technique might fail for others. After applying a feature selection algorithm, we examined the most important features, i.e. the motifs that have the greatest influence on idiolectal evolution. We find that some of those features are stylistic and have been previously identified in qualitative literature studies. We report some remarkable stylistic constructions revealed by our algorithm to illustrate which kind of stylistic patterns can be extracted using our method.","The Evolution of the Idiolect over the Lifetime: A Quantitative and Qualitative Study of French 19th Century Literature","Olga Seminck, Philippe Gambette, Dominique Legallois, Thierry Poibeau","4343","L’évolution de l’idiolecte au cours de la vie: Une étude quantitative et qualitative de la littérature française du XIXe siècle

La façon dont les auteurs s’expriment est unique mais change au cours de leur vie. Cependant, les études quantitatives de cette évolution idiolectique sont rares. En utilisant le Corpus for idiolectal Research (CIDRE) qui contient les œuvres datées de 11 écrivains de fiction français prolifiques du 19ème siècle, nous proposons de nouvelles méthodes pour identifier, quantifier et décrire les changements grammaticaux-stylistiques qui ont lieu à l’aide de motifs lexico-morphosyntactiques, également appelés motifs. Pour examiner la force du signal chronologique de changement, nous avons développé une méthode pour calculer si une matrice de distance d’œuvres littéraires contient un signal chronologique plus fort que prévu par hasard. Dix corpus sur 11 ont montré un signal chronologique plus élevé que le hasard, ce qui nous amène à conclure que l’évolution de l’idiolecte est dans un sens mathématique monotonique, soutenant l’hypothèse de rectilinéarité précédemment mise en avant dans la littérature stylométrique. La propriété rectiligne de l’évolution de l’idiolecte trouvée pour la plupart des auteurs dans CIDRE nous a ensuite permis de proposer une tâche d’apprentissage automatique: prédire l’année au cours de laquelle une œuvre a été écrite. Pour la majorité des auteurs de notre corpus, la précision et la quantité de variance expliquées par le modèle étaient élevées et nous discutons pourquoi la technique pourrait échouer pour les autres. Après avoir appliqué un algorithme de sélection de caractéristiques, nous avons examiné les caractéristiques les plus importantes, c’est-à-dire les motifs qui ont la plus grande influence sur l’évolution idiolectique. Nous constatons que certaines de ces caractéristiques sont stylistiques et ont déjà été identifiées dans des études de littérature qualitative. Nous rapportons quelques constructions stylistiques remarquables révélées par notre algorithme pour illustrer quel type de motifs stylistiques peut être extrait à l’aide de notre méthode.","13301","4","322","L’évolution de l’idiolecte au cours de la vie : une étude quantitative et qualitative de la littérature française du XIXe siècle

La façon dont les auteurs s’expriment est unique mais change au cours de leur vie. Cependant, les études quantitatives de cette évolution idiolectique sont rares. En utilisant le Corpus for Idiolectal Research (CIDRE) qui contient les œuvres datées de 11 écrivains de fiction français prolifiques du XIXème siècle, nous proposons de nouvelles méthodes pour identifier, quantifier et décrire les changements grammaticaux et stylistiques qui ont lieu à l’aide de motifs lexico-morphosyntaxiques, également appelés simplement &quot;motifs&quot;. Pour examiner la force du signal chronologique de changement, nous avons développé une méthode pour calculer si une matrice de distance d’œuvres littéraires contient un signal chronologique plus fort que ce que le hasard pourrait générer. Dix corpus sur onze ont montré un signal chronologique plus élevé que le hasard, ce qui nous amène à conclure que l’évolution de l’idiolecte est dans un sens mathématique monotonique, soutenant l’hypothèse de rectilinéarité précédemment mise en avant dans la littérature stylométrique. La propriété rectiligne de l’évolution de l’idiolecte trouvée pour la plupart des auteurs dans CIDRE nous a ensuite permis de proposer une tâche d’apprentissage automatique: prédire l’année au cours de laquelle une œuvre a été écrite. Pour la majorité des auteurs de notre corpus, la précision et la quantité de variance expliquées par le modèle étaient élevées et nous discutons pourquoi la technique pourrait échouer pour les autres. Après avoir appliqué un algorithme de sélection de caractéristiques, nous avons examiné les caractéristiques les plus importantes, c’est-à-dire les motifs qui ont la plus grande influence sur l’évolution idiolectique. Nous constatons que certaines de ces caractéristiques sont stylistiques et ont déjà été identifiées dans des études de littérature qualitative. Nous rapportons quelques constructions stylistiques remarquables révélées par notre algorithme pour illustrer quels types de motifs stylistiques peuvent être extraits à l’aide de notre méthode.","4","2023-07-03 15:44:38","2023-07-03 16:34:58","13301",,"1","2","3","3","2","1",NULL,,"2023-07-03 03:44:25","0"
"3714051","We study several classes of symbolic weighted formalisms: automata (swA), transducers (swT) and visibly pushdown extensions (swVPA, swVPT). They combine the respective extensions of their symbolic and weighted counterparts, allowing a quantitative evaluation of words over a large or infinite input alphabet. We present properties of closure by composition, the computation of transducer-defined distances between nested words and languages, as well as a PTIME 1-best search algorithm for swVPA. These results are applied to solve in PTIME a variant of parsing over infinite alphabets. We illustrate this approach with a motivating use case in automated music transcription.","Symbolic Weighted Language Models, Quantitative Parsing and Automated Music Transcription","Florent Jacquemard, Lydia Rodriguez de La Nava","4530","Modèles de langage pondérés symboliques, analyse quantitative et transcription de musique automatisée

Nous étudions plusieurs classes de formalismes symboliques pondérés: automates (SWA), transducteurs (SWT) et extensions visibles de poussée (swVPA, swVPT). Ils combinent les extensions respectives de leurs homologues symboliques et pondérés, permettant une évaluation quantitative des mots sur un alphabet d’entrée grand ou infini. Nous présentons les propriétés de fermeture par composition, le calcul des distances définies par le transducteur entre les mots imbriqués et les langues, ainsi qu’un algorithme de recherche PTIME 1-meilleur pour swVPA. Ces résultats sont appliqués pour résoudre dans PTIME une variante de l’analyse sur des alphabets infinis. Nous illustrons cette approche avec un cas d’utilisation motivant dans la transcription musicale automatisée.","11317","4","323","Modèles de langage pondérés symboliques, analyse quantitative et transcription de musique automatisée

Nous étudions plusieurs classes de formalismes symboliques pondérés : les automates (swA), les transducteurs (swT) et extensions visibles de poussée (swVPA, swVPT). Ils combinent les extensions respectives de leurs homologues symboliques et pondérés, permettant une évaluation quantitative des mots sur un alphabet d’entrée grand ou infini. Nous présentons les propriétés de fermeture par composition, le calcul des distances définies par le transducteur entre les mots imbriqués et les langues, ainsi que l&#039;un des meilleurs algorithmes de recherche pour les swVPA, PTIME. Ces résultats sont appliqués pour résoudre dans PTIME une variante de l’analyse sur des alphabets infinis. Nous illustrons cette approche avec un cas d’utilisation motivant dans la transcription musicale automatisée.","4","2023-07-03 16:35:12","2023-07-03 17:00:19","11317",,"3","2","3","3","2","2",NULL,,"2023-07-03 04:35:05","0"
"3741656","In this paper, we present our contribution to the FinTOC-2022 Shared Task ""Financial Document Structure Extraction"". We participated in the three tracks dedicated to English, French and Spanish document processing. Our main contribution consists in considering financial prospectus as a bundle of documents, i.e., a set of merged documents, each with their own layout and structure. Therefore, Document Layout and Structure Analysis (DLSA) first starts with the boundary detection of each document using general layout features. Then, the process applies inside each single document, taking advantage of the local properties. DLSA is achieved considering simultaneously text content, vectorial shapes and images embedded in the native PDF document. For the Title Detection task in English and French, we observed a significant improvement of the F-measures for Title Detection compared with those obtained during our previous participation.","GREYC@FinTOC-2022: Handling Document Layout and Structure in Native PDF Bundle of Documents","Emmanuel Giguet, Nadine Lucas","644","GREYC@FinTOC-2022: Gestion de la mise en page et de la structure des documents en format PDF autochtone

Dans cet article, nous présentons notre contribution à la tâche partagée FinTOC-2022 «Extraction de la structure des documents financiers». Nous avons participé aux trois pistes dédiées au traitement des documents en anglais, français et espagnol. Notre contribution principale consiste à considérer le prospectus financier comme un ensemble de documents, c’est-à-dire un ensemble de documents fusionnés, chacun ayant sa propre présentation et sa structure. Par conséquent, l’analyse de la mise en page et de la structure des documents (DLSA) commence d’abord par la détection des limites de chaque document à l’aide de caractéristiques générales de mise en page. Ensuite, le processus s’applique à l’intérieur de chaque document, en profitant des propriétés locales. DLSA est réalisé en tenant compte simultanément du contenu texte, des formes vectorielles et des images intégrées dans le document PDF natif. Pour la tâche de détection de titres en anglais et en français, nous avons observé une amélioration significative des mesures F pour la détection des titres par rapport à celles obtenues lors de notre participation précédente.","915","4","350","GREYC@FinTOC-2022 : Gestion de la mise en page et de la structure des documents dans un ensemble de documents PDF natifs

Dans cet article, nous présentons notre contribution à la tâche partagée FinTOC-2022 &quot;Financial Document Structure Extraction&quot; (« Extraction de la structure des documents financiers »). Nous avons participé aux trois pistes dédiées au traitement des documents en anglais, en français et en espagnol. Notre principale contribution consiste à considérer les prospectus financiers comme un ensemble de documents, c&#039;est-à-dire un ensemble de documents fusionnés, chacun ayant sa propre mise en page et sa propre structure. Par conséquent, l&#039;analyse de la mise en page et de la structure des documents (DLSA) commence par la détection des limites de chaque document à l&#039;aide de caractéristiques de mise en page générales. Ensuite, le processus s&#039;applique à l&#039;intérieur de chaque document, en tirant parti des propriétés locales. La DLSA prend en compte simultanément le contenu textuel, les formes vectorielles et les images intégrées dans le document PDF natif. Pour la détection de titres en anglais et en français, nous avons observé une amélioration significative des mesures F pour la détection de titres par rapport à celles obtenues lors de notre précédente participation.","4","2023-07-04 12:55:36","2023-07-04 13:09:48","915",,"1","1","2","2","1","1",NULL,,"2023-07-04 12:55:30","0"
"3754055","Verbal and nonverbal communication skills are essential for human-robot interaction, in particular when the agents are involved in a shared task. We address the specific situation where the robot is the only agent knowing about both the plan and the goal of the task, and has to instruct the human partners. The case study is a brick assembly. We here describe a multilayered verbal depictor whose semantic, syntactic, and lexical settings have been collected and evaluated via crowdsourcing. One crowdsourced experiment involves a robot-instructed pick-and-place task. We show that implicitly referring to achieved subgoals (stairs, pillars, etc) increases the performance of human partners.","Automatic Verbal Depiction of a Brick Assembly for a Robot Instructing Humans","Rami Younes, Gérard Bailly, Damien Pellier, Frédéric Elisei","621","Représentation verbale automatique d'un assemblage de briques pour un robot instruisant des humains

Les compétences en communication verbale et non verbale sont essentielles pour l'interaction homme-robot, en particulier lorsque les agents sont impliqués dans une tâche partagée. Nous abordons la situation spécifique où le robot est le seul agent connaissant à la fois le plan et l'objectif de la tâche, et doit instruire les partenaires humains. L'étude de cas est un assemblage de briques. Nous décrivons ici un dépeintre verbal multicouche dont les paramètres sémantiques, syntaxiques et lexicaux ont été collectés et évalués par crowdsourcing. Une expérience de crowdsourcing consiste en une tâche de ramassage et de mise en place commandée par un robot. Nous montrons qu'en se référant implicitement aux sous-objectifs atteints (escaliers, piliers, etc.), on augmente la performance des partenaires humains.","4368","6","351","Représentation verbale automatique d&#039;un assemblage de briques pour un robot instruisant des humains

Les compétences en communication verbale et non verbale sont essentielles pour l&#039;interaction homme-robot, en particulier lorsque les agents sont impliqués dans une tâche partagée. Nous abordons la situation spécifique où le robot est le seul agent connaissant à la fois le plan et l&#039;objectif de la tâche, et doit instruire les partenaires humains. L&#039;étude de cas est un assemblage de briques. Nous décrivons ici une représentation verbale multicouche dont les paramètres sémantiques, syntaxiques et lexicaux ont été collectés et évalués par crowdsourcing. Une expérience de crowdsourcing consiste en une tâche de ramassage et de mise en place commandée par un robot. Nous montrons qu&#039;en se référant implicitement aux sous-objectifs atteints (escaliers, piliers, etc.), on augmente la performance des partenaires humains.","4","2023-07-04 13:15:31","2023-07-04 13:54:31","4368",,"1","1","3","2","1","1",NULL,,"2023-07-04 01:15:25","0"
"3555462","Anonymisation and pseudonymisation are two similar concepts used in privacy preservation for speech data. With no established definitions for these tasks, nor standard approaches to assessment, this paper provides definitions and presents two complementary assessment frameworks. The first is based on voice similarity matrices which provide both an immediate visualisation of privacy protection performance at the speaker level and two objective measures in the form of de-identification and voice distinctiveness preservation. The approach readily highlights imbalances in system performance at the speaker level. The second, referred to as the zero evidence biometric recognition assessment (ZEBRA) framework, is based on information theory and measures the amount of private information disclosed in speech data. The paper presents also an extension to the original ZEBRA framework. It aims to reflect the robustness of the privacy safeguard when a privacy adversary adapts to the protected speech. We demonstrate the application of both frameworks to assess pseudonymisation performance on the two VoicePrivacy 2020 challenge baseline solutions plus a third one. The two frameworks were designed independently of each other. The ZEBRA framework is fully consistent with the Bayesian decision theory and the other framework focuses instead on speaker-wise visualisations of a system performance. Thus, while metrics derived from them bear similarities, they expose differences in safeguard behavior. The assessment of pseudonymisation remains challenging and merits greater attention in the future.","Towards a unified assessment framework of speech pseudonymisation","Paul-Gauthier Noé, Andreas Nautsch, Nicholas Evans, Jose Patino, Jean-François Bonastre, Natalia Tomashenko, Driss Matrouf","4919","Vers un cadre d'évaluation unifié de la pseudonymisation vocale

L'anonymisation et la pseudonymisation sont deux concepts similaires utilisés dans la préservation de la confidentialité des données vocales. En l'absence de définitions établies pour ces tâches, ni d'approches normalisées en matière d'évaluation, le présent document fournit des définitions et présente deux cadres d'évaluation complémentaires. La première est basée sur des matrices de similarité vocale qui fournissent à la fois une visualisation immédiate de la performance de protection de la vie privée au niveau du locuteur et deux mesures objectives sous la forme de la désidentification et de la préservation du caractère distinctif de la voix. Cette approche met en évidence les déséquilibres de performance du système au niveau des haut-parleurs. Le second, appelé cadre d'évaluation de la reconnaissance biométrique sans preuve (ZEBRA), est basé sur la théorie de l'information et mesure la quantité d'informations privées divulguées dans les données vocales. Le document présente également une extension du cadre ZEBRA original. Il vise à refléter la robustesse de la protection de la vie privée lorsqu'un adversaire de la vie privée s'adapte à la parole protégée. Nous démontrons l'application des deux cadres pour évaluer la performance de pseudonymisation sur les deux solutions de base de défi VoicePrivacy 2020 plus une troisième. Les deux cadres ont été conçus indépendamment l'un de l'autre. Le cadre ZEBRA est entièrement cohérent avec la théorie bayésienne de décision et l'autre cadre se concentre plutôt sur les visualisations par haut-parleur de la performance d'un système. Ainsi, bien que les mesures dérivées de ces mesures présentent des similitudes, elles exposent des différences dans le comportement de sauvegarde. L'évaluation de la pseudonymisation demeure difficile et mérite une plus grande attention à l'avenir.","16835","6","353","Vers un cadre d&#039;évaluation unifié de la pseudonymisation vocale

L&#039;anonymisation et la pseudonymisation sont deux concepts similaires utilisés dans la préservation de la confidentialité des données vocales. En l&#039;absence de définitions établies pour ces tâches, ni d&#039;approches normalisées en matière d&#039;évaluation, le présent document fournit des définitions et présente deux cadres d&#039;évaluation complémentaires. La première est basée sur des matrices de similarité vocale qui fournissent à la fois une visualisation immédiate de la performance de protection de la vie privée au niveau du locuteur et deux mesures objectives sous la forme de la désidentification et de la préservation du caractère distinctif de la voix. Cette approche met en évidence les déséquilibres de performance du système au niveau des locuteurs. Le second, appelé cadre d&#039;évaluation de la reconnaissance biométrique sans preuve (ZEBRA, zero evidence biometric recognition assessment), est basé sur la théorie de l&#039;information et mesure la quantité d&#039;informations privées divulguées dans les données vocales. Le document présente également une extension du cadre ZEBRA original. Il vise à refléter la robustesse de la protection de la vie privée lorsqu&#039;un adversaire de la vie privée s&#039;adapte à la parole protégée. Nous démontrons l&#039;application des deux cadres pour évaluer la performance de pseudonymisation sur les deux solutions de base de défi VoicePrivacy 2020 plus une troisième. Les deux cadres ont été conçus indépendamment l&#039;un de l&#039;autre. Le cadre ZEBRA est entièrement cohérent avec la théorie bayésienne de décision et l&#039;autre cadre se concentre plutôt sur les visualisations par haut-parleur de la performance d&#039;un système. Ainsi, bien que les mesures dérivées de ces mesures présentent des similitudes, elles exposent des différences dans le comportement de sauvegarde. L&#039;évaluation de la pseudonymisation demeure difficile et mérite une plus grande attention à l&#039;avenir.","4","2023-07-04 14:00:31","2023-07-04 14:46:53","16835",,"1","2","3","2","1","1",NULL,,"2023-07-04 02:00:26","0"
"3714189","Calor-Dial is an enriched version of the Calor corpus, collected from French encyclopedic data in order to study Information Extraction on domain specific data. The corpus was initially annotated in semantic Frames (Calor-Frame) and enriched with a first set of questions for Machine Reading Question Answering (Calor-Quest). The new Calor-Dial version presented here addresses the scope of conversational Question Answering. The main originality is that different types of questions are annotated, including more challenging configurations than in classical QA corpora. This paper describes the corpus and proposes some baseline results obtained with models trained on the FQuAD corpus.","Calor-Dial : a corpus for Conversational Question Answering on French encyclopedic documents","Frédéric Béchet, Ludivine Robert, Lina Rojas-Barahona, Géraldine Damnati","4528","Calor-Dial : un corpus pour la réponse aux questions de conversation sur les documents encyclopédiques français

Calor-Dial est une version enrichie du corpus Calor, collecté à partir de données encyclopédiques françaises afin d'étudier l'extraction d'informations sur des données spécifiques à un domaine. Le corpus a d'abord été annoté en cadres sémantiques (Calor-Frame) et enrichi d'un premier ensemble de questions pour la réponse aux questions de lecture automatique (Calor-Quest). La nouvelle version de Calor-Dial présentée ici aborde la portée de la réponse aux questions conversationnelle. La principale originalité est que différents types de questions sont annotés, y compris des configurations plus difficiles que dans les corpus QA classiques. Cet article décrit le corpus et propose quelques résultats de base obtenus avec des modèles formés sur le corpus FQuAD.","16444","6","360","Calor-Dial : un corpus pour la réponse aux questions en conversation sur des documents encyclopédiques français

Calor-Dial est une version enrichie du corpus Calor, collecté à partir de données encyclopédiques françaises afin d&#039;étudier l&#039;extraction d&#039;informations sur des données spécifiques à un domaine. Le corpus a d&#039;abord été annoté en cadres sémantiques (Calor-Frame) et enrichi d&#039;un premier ensemble de questions pour la réponse aux questions de lecture automatique (Calor-Quest). La nouvelle version de Calor-Dial présentée ici aborde la portée de la réponse aux questions en conversation. La principale originalité est que différents types de questions sont annotés, y compris des configurations plus difficiles que dans les corpus QA classiques. Cet article décrit le corpus et propose quelques résultats de base obtenus avec des modèles formés sur le corpus FQuAD.","4","2023-07-04 15:29:01","2023-07-04 15:30:57","16444",,"2","1","2","2","2","1",NULL,,"2023-07-04 03:28:56","0"
"3807745","Technological advancement provides an increasing number and variety of solutions to interact with digital content. However, the complexity of the devices we use to interact with such content grows according to the users' needs as well as the complexity of the target interactions. This also includes all those tools designed to mediate touch interactions with virtual and/or remote environments, i.e., haptic interfaces and rendering techniques. We propose three hours of tutorials to discuss the technology, challenges, and perspective of haptic systems and rendering techniques for immersive human-computer interaction.","Tutorial in “Frontiers in Haptic Technology and Interaction Design: the Challenges, the Technology, the Perspectives”","Francesco Chinello, Claudio Pacchierotti, Cheng Fang, Hasti Seifi","4271","Tutoriel dans ""Frontiers in Haptic Technology and Interaction Design : the Challenges, the Technology, the Perspectives"" (Frontières de la technologie haptique et du design d'interaction : les défis, la technologie, les perspectives)

Les progrès technologiques offrent un nombre et une variété croissants de solutions pour interagir avec le contenu numérique. Cependant, la complexité des dispositifs que nous utilisons pour interagir avec ce contenu augmente en fonction des besoins des utilisateurs et de la complexité des interactions visées. Cela inclut également tous les outils conçus pour faciliter les interactions tactiles avec les environnements virtuels et/ou distants, c'est-à-dire les interfaces haptiques et les techniques de rendu. Nous proposons trois heures de travaux dirigés pour discuter de la technologie, des défis et des perspectives des systèmes haptiques et des techniques de rendu pour l'interaction immersive homme-machine.","17282","3","365","Tutoriel dans &quot;Frontiers in Haptic Technology and Interaction Design : the Challenges, the Technology, the Perspectives&quot; (Frontières de la technologie haptique et du design d&#039;interaction : défis, technologie, perspectives)

Les progrès technologiques offrent un nombre et une variété croissants de solutions pour interagir avec le contenu numérique. Cependant, la complexité des dispositifs que nous utilisons pour interagir avec ce contenu augmente en fonction des besoins des utilisateurs et de la complexité des interactions visées. Cela inclut également tous les outils conçus pour faciliter les interactions tactiles avec les environnements virtuels et/ou distants, c&#039;est-à-dire les interfaces haptiques et les techniques de rendu. Nous proposons trois heures de travaux dirigés pour discuter de la technologie, des défis et des perspectives des systèmes haptiques et des techniques de rendu pour l&#039;interaction immersive homme-machine.","4","2023-07-04 15:46:49","2023-07-04 16:12:45","17282",,"1","1","1","2","1","1",NULL,,"2023-07-04 03:31:08","0"
"3345478","Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement: to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text. We present an analysis of possible implementations of dual decoding, and experiment with four applications. Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations.","One Source, Two Targets: Challenges and Rewards of Dual Decoding","Jitao Xu, François Yvon","1016","Une Source, Deux Cibles : Défis et avantages du décodage double

On entend généralement par traduction automatique la génération d'un texte cible à partir d'un document source d'entrée. Dans le présent document, nous considérons une exigence plus forte : pour générer conjointement deux textes de sorte que chaque côté sortie dépend effectivement de l'autre. Comme nous le verrons, un tel dispositif a plusieurs applications pratiques, de la traduction automatique multi-cible à la génération de variations contrôlées du texte cible. Nous présentons une analyse des implémentations possibles du décodage double, et expérimentons quatre applications. Regarder le problème sous plusieurs angles nous permet de mieux mettre en évidence les défis du décodage double et d'analyser en profondeur les avantages de générer des traductions appariées plutôt qu'indépendantes.","5261","6","368","Une Source, deux Cibles : Défis et bénéfices du double décodage

On entend généralement par traduction automatique la génération d&#039;un texte cible à partir d&#039;un document source en entrée. Dans le présent document, nous considérons une exigence plus forte : générer conjointement deux textes de sorte que chaque texte cible dépende effectivement de l&#039;autre. Comme nous le verrons, un tel dispositif a plusieurs applications pratiques, de la traduction automatique multi-cible à la génération de variations contrôlées du texte cible. Nous présentons une analyse des implantations possibles du double décodage, et expérimentons quatre applications. Regarder le problème sous plusieurs angles nous permet de mieux mettre en évidence les défis du double décodage et d&#039;analyser en profondeur les avantages de générer des traductions appariées plutôt qu&#039;indépendantes.","4","2023-07-04 16:20:20","2023-07-04 16:28:59","5261",,"2","2","3","2","3","1",NULL,,"2023-07-04 04:19:58","0"
"1522314","We introduce a constituency parser based on a bi-LSTM encoder adapted from re- cent work (Cross and Huang, 2016b; Kiperwasser and Goldberg, 2016), which can incorporate a lower level character bi- LSTM (Ballesteros et al., 2015; Plank et al., 2016). We model two important in- terfaces of constituency parsing with aux- iliary tasks supervised at the word level: (i) part-of-speech (POS) and morpholog- ical tagging, (ii) functional label predic- tion. On the SPMRL dataset, our parser obtains above state-of-the-art results on constituency parsing without requiring ei- ther predicted POS or morphological tags, and outputs labelled dependency trees.","Multilingual Lexicalized Constituency Parsing with Word-Level Auxiliary Tasks","Maximin Coavoux, Benoît Crabbé","1798","Lexicalized Constituency Parsing multilingue avec des tâches auxiliaires de niveau Word

Nous introduisons un analyseur de circonscription basé sur un encodeur bi-LSTM adapté du travail de re-cent (Cross et Huang, 2016b; Kiperwasser et Goldberg, 2016), qui peuvent intégrer un caractère de niveau inférieur bi- LSTM (Ballesteros et al., 2015; Plank et al., 2016). Nous modélisons deux aspects importants de l’analyse des circonscriptions avec des tâches aux iliaires supervisées au niveau des mots: (I) la partie de la parole (POS) et l’étiquetage morphologique, (ii) la prédic-tion fonctionnelle de l’étiquette. Sur l’ensemble de données SPMRL, notre analyseur obtient ci-dessus des résultats de pointe sur l’analyse des circonscriptions sans nécessiter une prévision de POS ou d’étiquettes morphologiques, et des sorties marquées d’arbres de dépendance.","1572","4","369","Analyse syntaxique de constituants lexicaux multilingues avec tâches auxiliaires au niveau des mots 

Nous présentons un analyseur de constituants basé sur un encodeur bi-LSTM adapté du travail récent (Cross et Huang, 2016b ; Kiperwasser et Goldberg, 2016), qui peut intégrer un caractère bi-LSTM de niveau inférieur (Ballesteros et al., 2015 ; Plank et al., 2016). Nous modélisons deux aspects importants de l’analyse des circonscriptions avec des tâches auxiliaires supervisées au niveau des mots : (I) la partie du discours et l’étiquetage morphologique, (ii) la prédiction des étiquettes fonctionelles. Sur l’ensemble de données SPMRL, notre analyseur obtient des résultats supérieurs à l&#039;état de l&#039;art sur l’analyse des constituants sans nécessiter de prédiction des parties du discours ou des étiquettes morphologiques, ni des sorties marquées d’arbres de dépendance.","4","2023-07-04 16:29:32","2023-07-04 17:09:20","1572",,"3","2","4","4","4","3",NULL,,"2023-07-04 04:29:29","0"
"2395709","Keyphrase generation is the task of predicting a set of lexical units that conveys the main content of a source text. Existing datasets for keyphrase generation are only readily available for the scholarly domain and include non-expert annotations. In this paper we present KPTimes, a large-scale dataset of news texts paired with editor-curated keyphrases. Exploring the dataset, we show how editors tag documents , and how their annotations differ from those found in existing datasets. We also train and evaluate state-of-the-art neural keyphrase generation models on KPTimes to gain insights on how well they perform on the news domain. The dataset is available online at https:// github.com/ygorg/KPTimes.","KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents","Ygor Gallina, Florian Boudin, Béatrice Daille","3729","KPTimes : Un ensemble de données à grande échelle pour la génération de phrases clés sur les documents d'actualité

La génération de phrases clés est la tâche de prédire un ensemble d'unités lexicales qui transmettent le contenu principal d'un texte source. Les ensembles de données existants pour la génération de phrases clés ne sont disponibles que pour le domaine universitaire et incluent des annotations non spécialisées. Dans cet article, nous présentons KPTimes, un ensemble de données à grande échelle de textes d'actualité jumelés avec des phrases clés organisées par l'éditeur. En explorant le dataset, nous montrons comment les éditeurs balisent les documents, et comment leurs annotations diffèrent de celles trouvées dans les datasets existants. Nous formons et évaluons également des modèles de génération de syntagmes neuronaux de pointe sur KPTimes pour obtenir des informations sur leur performance dans le domaine de l'actualité. L'ensemble de données est disponible en ligne à l'adresse https:// github.com/ygorg/KPTimes.","6483","6","374","KPTimes : Un ensemble de données à grande échelle pour la génération de phrases clés sur les documents d&#039;actualité

La génération de phrases clés consiste à prédire un ensemble d&#039;unités lexicales qui transmettent le contenu principal d&#039;un texte source. Les ensembles de données existants pour la génération de phrases clés ne sont disponibles que pour le domaine universitaire et incluent des annotations non spécialisées. Dans cet article, nous présentons KPTimes, un ensemble de données à grande échelle de textes d&#039;actualité jumelés avec des phrases clés organisées par l&#039;éditeur. En explorant l&#039;ensemble de données, nous montrons comment les éditeurs étiquettent les documents, et comment leurs annotations diffèrent de celles trouvées dans les ensembles de données existants. Nous entraînons et évaluons également des modèles de génération de syntagmes neuronaux de pointe sur KPTimes pour obtenir des informations sur leur performance dans le domaine de l&#039;actualité. L&#039;ensemble de données est disponible en ligne à l&#039;adresse suivante : https://github.com/ygorg/KPTimes.","4","2023-07-05 15:00:54","2023-07-05 15:08:43","6483",,"2","2","3","3","2","3",NULL,,"2023-07-05 03:00:52","0"
"1960670","abstract","Towards contextual adaptation for any-text translation","Li Gong, Aurélien Max, François Yvon","2005","Vers l'adaptation contextuelle pour la traduction tout-texte

abstrait","7515","3","375","Vers l&#039;adaptation contextuelle pour la traduction de n&#039;importe quel texte

résumé","4","2023-07-05 15:10:29","2023-07-05 15:14:18","7515",,"3","4","2","3","2","4",NULL,,"2023-07-05 03:10:26","0"
"3727214","Pre-trained language models have established the state-of-the-art on various natural language processing tasks, including dialogue summarization, which allows the reader to quickly access key information from long conversations in meetings, interviews or phone calls. However, such dialogues are still difficult to handle with current models because the spontaneity of the language involves expressions that are rarely present in the corpora used for pre-training the language models. Moreover, the vast majority of the work accomplished in this field has been focused on English. In this work, we present a study on the summarization of spontaneous oral dialogues in French using several language specific pre-trained models: BARThez, and BelGPT-2, as well as multilingual pre-trained models: mBART, mBARThez, and mT5. Experiments were performed on the DECODA (Call Center) dialogue corpus whose task is to generate abstractive synopses from call center conversations between a caller and one or several agents depending on the situation. Results show that the BARThez models offer the best performance far above the previous state-of-the-art on DECODA. We further discuss the limits of such pre-trained models and the challenges that must be addressed for summarizing spontaneous dialogues.","Effectiveness of French Language Models on Abstractive Dialogue Summarization Task","Yongxin Zhou, François Portet, Fabien Ringeval","664","Efficacité des modèles de langue française sur la tâche de synthèse du dialogue abstrait

Les modèles linguistiques pré-formés ont établi l’état de l’art sur diverses tâches de traitement du langage naturel, y compris la synthèse du dialogue, qui permet au lecteur d’accéder rapidement aux informations clés provenant de longues conversations lors de réunions, d’entretiens ou d’appels téléphoniques. Cependant, de tels dialogues sont encore difficiles à gérer avec les modèles actuels car la spontanéité du langage implique des expressions rarement présentes dans les corpus utilisés pour la préformation des modèles linguistiques. En outre, la grande majorité du travail accompli dans ce domaine a été axée sur l’anglais. Dans ce travail, nous présentons une étude sur la synthèse des dialogues oraux spontanés en français en utilisant plusieurs modèles pré-formés spécifiques à la langue: Barthez, et BelGPT-2, ainsi que des modèles pré-formés multilingues: MBart, mBARThez et mT5. Des expériences ont été réalisées sur le corpus de dialogue DECODA (Call Center) dont la tâche est de générer des synopses abstraites à partir des conversations du centre d’appels entre un appelant et un ou plusieurs agents en fonction de la situation. Les résultats montrent que les modèles Barthez offrent les meilleures performances bien au-dessus de l’état de la technologie précédent sur DECODA. Nous discutons en outre des limites de ces modèles pré-formés et des défis à relever pour résumer les dialogues spontanés.","935","4","376","Efficacité des modèles de langue française sur le dialogue abstrait, tâche de synthèse

Les modèles linguistiques pré-entraînés ont établi l’état de l’art sur diverses tâches de traitement automatique des langues, y compris la synthèse du dialogue, qui permet au lecteur d’accéder rapidement aux informations clés provenant de longues conversations lors de réunions, d’entretiens ou d’appels téléphoniques. Cependant, de tels dialogues sont encore difficiles à gérer avec les modèles actuels car la spontanéité de la langue implique des expressions rarement présentes dans les corpus utilisés pour le pré-entraînement des modèles linguistiques. En outre, la grande majorité du travail accompli dans ce domaine a été axée sur l’anglais. Dans ce travail, nous présentons une étude sur la synthèse des dialogues oraux spontanés en français en utilisant plusieurs modèles pré-entraînés spécifiques à la langue : Barthez, et BelGPT-2, ainsi que des modèles pré-entraînés multilingues : MBart, mBARThez et mT5. Des expériences ont été réalisées sur le corpus de dialogue DECODA (Call Center) dont la tâche est de générer des synopsis abstraits à partir des conversations du centre d’appels entre un appelant et un ou plusieurs agents, en fonction de la situation. Les résultats montrent que les modèles Barthez offrent les meilleures performances bien au-dessus de l’état de l&#039;art précédent sur DECODA. Nous discutons en outre des limites de ces modèles pré-entraînés et des défis à relever pour résumer les dialogues spontanés.","4","2023-07-05 15:15:20","2023-07-05 15:48:35","935",,"3","1","4","2","2","2",NULL,,"2023-07-05 03:14:56","0"
"1960962","The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. For lack of sufficient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture. In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance.","Continuous space translation models with neural networks","Le Hai Son, Alexandre Allauzen, François Yvon","2010","Modèles de traduction spatiale continue avec réseaux neuronaux

L’utilisation d’estimations conventionnelles de probabilité maximale entrave la performance des modèles de traduction basés sur des phrases existantes. Faute de données suffisantes sur la formation, la plupart des modèles ne tiennent compte que d’une petite quantité de contexte. Comme remède partiel, nous explorons ici plusieurs modèles de traduction de l’espace continu, où les probabilités de traduction sont estimées à l’aide d’une représentation continue d’unités de traduction au lieu de représentations discrètes standard. Afin de gérer un grand ensemble d’unités de traduction, ces représentations et les estimations associées sont calculées conjointement à l’aide d’un réseau neuronal multicouche avec une architecture SOUL. Dans des expériences à petite échelle et à grande échelle de l’anglais vers le français, nous montrons que les modèles qui en résultent peuvent être efficacement formés et utilisés en plus d’un système de traduction n-gram, ce qui permet d’améliorer considérablement les performances.","2283","4","380","Modèles de traduction à réseaux neuronaux dans l&#039;espace continu

L’utilisation d’estimations conventionnelles de probabilité maximale entrave la performance des modèles de traduction basés sur des phrases existantes. Faute de données suffisantes sur la formation, la plupart des modèles ne tiennent compte que d’une petite quantité de contexte. Comme remède partiel, nous explorons ici plusieurs modèles de traduction dans l’espace continu, où les probabilités de traduction sont estimées à l’aide d’une représentation continue d’unités de traduction au lieu de représentations discrètes standard. Afin de gérer un grand ensemble d’unités de traduction, ces représentations et les estimations associées sont calculées conjointement à l’aide d’un réseau neuronal multicouche avec une architecture SOUL. Dans des expériences à petite échelle et à grande échelle de l’anglais vers le français, nous montrons que les modèles qui en résultent peuvent être efficacement formés et utilisés en plus d’un système de traduction à n-gramme, ce qui permet d’améliorer considérablement les performances.","4","2023-07-05 15:49:15","2023-07-05 16:11:18","2283",,"2","2","4","3","2","2",NULL,,"2023-07-05 03:49:04","0"
"1425728","This paper gives an overview of the MultiTal project, which aims to create a research infrastructure that ensures long-term distribution of NLP tools descriptions. The goal is to make NLP tools more accessible and usable to end-users of different disciplines. The infrastructure is built on a meta-data scheme modelling and standardising multilingual NLP tools documentation. The model is conceptualised using an OWL ontology. The formal representation of the ontology allows us to automatically generate organised and structured documentation in different languages for each represented tool.","The MultiTal NLP tool infrastructure","Driss Sadoun, Satenik Mkhitaryan, Damien Nouvel, Mathieu Valette","1699","Infrastructure de l'outil MultiTal NLP

Cet article donne un aperçu du projet MultiTal, qui vise à créer une infrastructure de recherche qui assure la distribution à long terme des descriptions des outils NLP. L'objectif est de rendre les outils de PNL plus accessibles et utilisables pour les utilisateurs finaux de différentes disciplines. L'infrastructure repose sur un schéma de métadonnées qui modélise et normalise la documentation des outils multilingues NLP. Le modèle est conceptualisé en utilisant une ontologie OWL. La représentation formelle de l'ontologie nous permet de générer automatiquement une documentation organisée et structurée dans différentes langues pour chaque outil représenté.","4949","6","382","Infrastructure de l&#039;outil de TAL MultiTal

Cet article donne un aperçu du projet MultiTal, qui vise à créer une infrastructure de recherche qui assure la distribution à long terme des descriptions des outils de TAL. L&#039;objectif est de rendre les outils de TAL plus accessibles et utilisables pour les utilisateurs finaux de différentes disciplines. L&#039;infrastructure repose sur un schéma de métadonnées qui modélise et normalise la documentation des outils de TAL multilingues. Le modèle est conceptualisé en utilisant une ontologie OWL. La représentation formelle de l&#039;ontologie nous permet de générer automatiquement une documentation organisée et structurée dans différentes langues pour chaque outil représenté.","4","2023-07-05 16:12:12","2023-07-05 16:24:46","4949",,"3","1","4","2","1","4",NULL,,"2023-07-05 04:11:55","0"
"3036776","Out-of-vocabulary (OOV) words can pose serious challenges for machine translation (MT) tasks, and in particular, for low-resource language (LRL) pairs, i.e., language pairs for which few or no parallel corpora exist. Our work adapts variants of seq2seq models to perform transduction of such words from Hindi to Bhojpuri (an LRL instance), learning from a set of cognate pairs built from a bilingual dictionary of Hindi - Bhojpuri words. We demonstrate that our models can be effectively used for language pairs that have limited parallel corpora; our models work at the character level to grasp phonetic and orthographic similarities across multiple types of word adaptations, whether synchronic or diachronic, loan words or cognates. We describe the training aspects of several character level NMT systems that we adapted to this task and characterize their typical errors. Our method improves BLEU score by 6.3 on the Hindi-to-Bhojpuri translation task. Further, we show that such transductions can generalize well to other languages by applying it successfully to Hindi - Bangla cognate pairs. Our work can be seen as an important step in the process of: (i) resolving the OOV words problem arising in MT tasks; (ii) creating effective parallel corpora for resource constrained languages; and (iii) leveraging the enhanced semantic knowledge captured by word-level embeddings to perform character-level tasks.","Learning cross-lingual phonological and orthagraphic adaptations: a case study in improving neural machine translation between low-resource languages","Saurav Jha, Akhilesh Sudhakar, Anil Kumar Singh","1250","Apprentissage des adaptations phonologiques et orthophotographiques interlinguistiques : étude de cas sur l'amélioration de la traduction automatique neuronale entre des langages à faibles ressources

Les mots hors vocabulaire (OOV) peuvent poser de sérieux défis pour les tâches de traduction automatique (MT), et en particulier, pour les paires de langues à faibles ressources (LRL), c'est-à-dire, les paires de langues pour lesquelles il n'existe que peu ou pas de corpus parallèle. Notre travail adapte des variantes de modèles seq2seq pour effectuer la transduction de tels mots de Hindi à Bhojpuri (une instance LRL), en apprenant d'un ensemble de paires apparentées construites à partir d'un dictionnaire bilingue de mots Hindi - Bhojpuri. Nous démontrons que nos modèles peuvent être utilisés efficacement pour des paires de langues qui ont des corpus parallèles limités ; nos modèles fonctionnent au niveau des caractères pour saisir les similitudes phonétiques et orthographiques entre plusieurs types d'adaptations de mots, qu'ils soient synchrones ou diachroniques, mots empruntés ou apparentés. Nous décrivons les aspects de formation de plusieurs systèmes NMT de niveau caractère que nous avons adaptés à cette tâche et caractérisons leurs erreurs typiques. Notre méthode améliore le score BLEU de 6,3 sur la tâche de traduction Hindi-Bhojpuri. De plus, nous montrons que de telles transductions peuvent bien se généraliser à d'autres langues en l'appliquant avec succès à des paires apparentées hindi - bengali. Notre travail peut être considéré comme une étape importante dans le processus de : (i) résoudre le problème de mots OOV survenant dans les tâches MT ; (ii) créer des corpus parallèles efficaces pour les langues à ressources limitées ; et (iii) en tirant parti des connaissances sémantiques améliorées capturées par les intégrations de niveau mot pour effectuer des tâches de niveau caractère.","5495","6","386","Apprentissage des adaptations phonologiques et orthographiques interlingues : étude de cas sur l&#039;amélioration de la traduction automatique neuronale entre des langues peu dotées

Les mots hors vocabulaire (HV) peuvent poser de sérieux défis pour les tâches de traduction automatique (TA), et en particulier, pour les paires de langues peu dotées (LPD), c&#039;est-à-dire, les paires de langues pour lesquelles il n&#039;existe que peu ou pas de corpus parallèle. Notre travail adapte des variantes de modèles seq2seq pour effectuer la transduction de tels mots de l&#039;hindi au bhojpuri (une instance LPD), en apprenant d&#039;un ensemble de paires apparentées construites à partir d&#039;un dictionnaire bilingue hindi - bhojpuri. Nous démontrons que nos modèles peuvent être utilisés efficacement pour des paires de langues qui ont des corpus parallèles limités. Nos modèles fonctionnent au niveau des caractères pour saisir les similitudes phonétiques et orthographiques entre plusieurs types d&#039;adaptations de mots, qu&#039;ils soient synchrones ou diachroniques, mots empruntés ou apparentés. Nous décrivons les aspects de formation de plusieurs systèmes de traduction automatique neuronale (TAN) au niveau du caractère que nous avons adaptés à cette tâche et caractérisons leurs erreurs typiques. Notre méthode améliore le score BLEU de 6,3 sur la tâche de traduction hindi - bhojpuri. De plus, nous montrons que de telles transductions peuvent bien se généraliser à d&#039;autres langues en les appliquant avec succès à des paires apparentées hindi - bengali. Notre travail peut être considéré comme une étape importante pour : (i) résoudre le problème de mots HV survenant dans les tâches de TA ; (ii) créer des corpus parallèles efficaces pour les langues peu dotées ; et (iii) tirer parti des connaissances sémantiques améliorées capturées par les intégrations au niveau des mots pour effectuer des tâches au niveau des caractères.","4","2023-07-06 15:07:22","2023-07-06 16:05:00","5495",,"2","4","4","2","3","3",NULL,,"2023-07-06 03:07:20","0"
"2962195","Simultaneous machine translation consists in starting output generation before the entire input sequence is available. Wait-k decoders offer a simple but efficient approach for this problem. They first read k source tokens, after which they alternate between producing a target token and reading another source token. We investigate the behavior of wait-k decoding in low resource settings for spoken corpora using IWSLT datasets. We improve training of these models using unidirectional encoders, and training across multiple values of k. Experiments with Transformer and 2D-convolutional architectures show that our wait-k models generalize well across a wide range of latency levels. We also show that the 2D-convolution architecture is competitive with Transformers for simultaneous translation of spoken language.","Efficient Wait-k Models for Simultaneous Machine Translation","Maha Elbayad, Laurent Besacier, Jakob Verbeek","6258","Modèles Wait-k efficaces pour la traduction automatique simultanée

La traduction automatique simultanée consiste à commencer la génération de la sortie avant que la séquence d'entrée ne soit entièrement disponible. Les décodeurs Wait-k offrent une approche simple et efficace de ce problème. Ils lisent d'abord k tokens source, après quoi ils alternent entre la production d'un token cible et la lecture d'un autre token source. Nous étudions le comportement du décodage wait-k dans des environnements à faibles ressources pour des corpus parlés en utilisant des ensembles de données IWSLT. Nous améliorons l'apprentissage de ces modèles en utilisant des encodeurs unidirectionnels, et l'apprentissage à travers de multiples valeurs de k. Les expériences avec les architectures Transformer et 2D-convolutionnelle montrent que nos modèles wait-k se généralisent bien à travers une large gamme de niveaux de latence. Nous montrons également que l'architecture 2D-convolutionnelle est compétitive avec les transformateurs pour la traduction simultanée de la langue parlée.","19570","3","391","Modèles wait-k efficaces pour la traduction automatique simultanée

La traduction automatique simultanée consiste à commencer la génération de la sortie avant que la séquence d&#039;entrée ne soit entièrement disponible. Les décodeurs wait-k offrent une approche simple et efficace de ce problème. Ils lisent d&#039;abord k tokens source, après quoi ils alternent entre la production d&#039;un token cible et la lecture d&#039;un autre token source. Nous étudions le comportement du décodage wait-k pour des langues peu dotées dans des corpus parlés en utilisant des ensembles de données IWSLT. Nous améliorons l&#039;apprentissage de ces modèles en utilisant des encodeurs unidirectionnels, et l&#039;apprentissage à travers de multiples valeurs de k. Les expériences avec les architectures Transformer et 2D-convolutionnelle montrent que nos modèles wait-k se généralisent bien à travers une large gamme de niveaux de latence. Nous montrons également que l&#039;architecture 2D-convolutionnelle est compétitive avec les transformateurs pour la traduction simultanée de la langue parlée.","4","2023-07-06 16:09:23","2023-07-06 16:15:57","19570",,"1","1","3","1","2","2",NULL,,"2023-07-06 04:08:09","0"
"3299010","Recent studies on the analysis of the multilingual representations focus on identifying whether there is an emergence of languageindependent representations, or whether a multilingual model partitions its weights among different languages. While most of such work has been conducted in a ""black-box"" manner, this paper aims to analyze individual components of a multilingual neural translation (NMT) model. In particular, we look at the encoder self-attention and encoder-decoder attention heads (in a many-to-one NMT model) that are more specific to the translation of a certain language pair than others by (1) employing metrics that quantify some aspects of the attention weights such as ""variance"" or ""confidence"", and (2) systematically ranking the importance of attention heads with respect to translation quality. Experimental results show that surprisingly, the set of most important attention heads are very similar across the language pairs and that it is possible to remove nearly one-third of the less important heads without hurting the translation quality greatly.","Do Multilingual Neural Machine Translation Models Contain Language Pair Specific Attention Heads?","Zae Kim, Laurent Besacier, Vassilina Nikoulina, Didier Schwab","1058","Les modèles de traduction automatique neuronale multilingue contiennent-ils des têtes d'attention spécifiques aux paires de langues ?

Des études récentes sur l'analyse des représentations multilingues visent à déterminer s'il y a émergence de représentations indépendantes de la langue, ou si un modèle multilingue répartit ses poids entre différentes langues. Bien que la plupart de ces travaux aient été menés dans une « boîte noire », cet article vise à analyser les composantes individuelles d'un modèle de traduction neuronale multilingue (NMT). En particulier, nous examinons les têtes d'attention d'autoattention d'encodeur et d'encodeur-décodeur (dans un modèle NMT plusieurs-à-un) qui sont plus spécifiques à la traduction d'une certaine paire de langues que d'autres en (1) employant des métriques qui quantifient certains aspects des poids d'attention tels que la ""variance"" ou la ""confiance"", et (2) classant systématiquement l'importance des têtes d'attention par rapport à la qualité de la traduction. Les résultats expérimentaux montrent que, de façon surprenante, l'ensemble des têtes d'attention les plus importantes sont très similaires dans les paires de langues et qu'il est possible de supprimer près d'un tiers des têtes les moins importantes sans nuire grandement à la qualité de la traduction.","5303","6","396","Les modèles de traduction automatique neuronale multilingue contiennent-ils des têtes d&#039;attention spécifiques aux paires de langues ?

Des études récentes sur l&#039;analyse des représentations multilingues visent à déterminer s&#039;il y a émergence de représentations indépendantes de la langue, ou si un modèle multilingue répartit ses poids entre différentes langues. Bien que la plupart de ces travaux aient été menés par la méthode de la « boîte noire », cet article vise à analyser les composantes individuelles d&#039;un modèle de traduction automatique neuronale multilingue (TANM). En particulier, nous examinons les têtes d&#039;attention d&#039;autoattention d&#039;encodeur-décodeur (dans un modèle de TANM &quot;many-to-one&quot;) qui sont plus spécifiques à la traduction d&#039;une certaine paire de langues que d&#039;autres (1) en employant des métriques qui quantifient certains aspects des poids d&#039;attention tels que la &quot;variance&quot; ou la &quot;confiance&quot;, et (2) en classant systématiquement l&#039;importance des têtes d&#039;attention par rapport à la qualité de la traduction. Les résultats expérimentaux montrent que, de façon surprenante, l&#039;ensemble des têtes d&#039;attention les plus importantes sont très similaires dans les paires de langues et qu&#039;il est possible de supprimer près d&#039;un tiers des têtes les moins importantes sans nuire grandement à la qualité de la traduction.","4","2023-07-06 17:40:04","2023-07-06 17:40:50","5303",,"2","2","2","2","3","3",NULL,,"2023-07-06 05:40:02","0"
"3834732","We present a new approach to perform zeroshot cross-modal transfer between speech and text for translation tasks. Multilingual speech and text are encoded in a joint fixed-size representation space. Then, we compare different approaches to decode these multimodal and multilingual fixed-size representations, enabling zero-shot translation between languages and modalities. All our models are trained without the need of cross-modal labeled translation data. Despite a fixed-size representation, we achieve very competitive results on several text and speech translation tasks. In particular, we outperform the state of the art for zero-shot speech translation on Must-C. We also introduce the first results for zero-shot direct speechto-speech and text-to-speech translation.","T-Modules: Translation Modules for Zero-Shot Cross-Modal Machine Translation","Paul-Ambroise Duquenne, Hongyu Gong, Benoît Sagot, Holger Schwenk","501","Modules T : Modules de traduction pour la traduction automatique intermodale à tir nul

Nous présentons une nouvelle approche pour effectuer un transfert intermodal zéro-shot entre la parole et le texte pour les tâches de traduction. La parole et le texte multilingues sont codés dans un espace de représentation commun de taille fixe. Ensuite, nous comparons différentes approches pour décoder ces représentations multimodales et multilingues de taille fixe, permettant une traduction de zéro plan entre les langues et les modalités. Tous nos modèles sont formés sans avoir besoin de données de traduction étiquetées intermodales. Malgré une représentation de taille fixe, nous obtenons des résultats très compétitifs sur plusieurs tâches de traduction de texte et de parole. En particulier, nous surpassons l'état de la technique pour la traduction vocale à tir nul sur Must-C. Nous présentons également les premiers résultats pour la traduction vocale directe à tir nul et la traduction de texte à parole.","4248","6","397","Modules T : Modules de traduction pour la traduction automatique intermodale &quot;zero-shot&quot;

Nous présentons une nouvelle approche pour effectuer un transfert intermodal zero-shot entre la parole et le texte pour les tâches de traduction. La parole et le texte multilingues sont codés dans un espace de représentation commun de taille fixe. Ensuite, nous comparons différentes approches pour décoder ces représentations multimodales et multilingues de taille fixe, permettant une traduction zero-shot entre les langues et les modalités. Tous nos modèles sont formés sans avoir besoin de données de traduction étiquetées intermodales. Malgré une représentation de taille fixe, nous obtenons des résultats très compétitifs sur plusieurs tâches de traduction de texte et de parole. En particulier, nous surpassons l&#039;état de l&#039;art pour la traduction vocale zero-shot sur Must-C. Nous présentons également les premiers résultats pour la traduction vocale directe zero-shot et la traduction &quot;text-to-speech&quot;.","4","2023-07-06 17:41:29","2023-07-06 17:56:39","4248",,"3","2","4","2","2","4",NULL,,"2023-07-06 05:41:18","0"
"1865091","The European ""Tenders Electronic Daily"" (TED) is a large source of semi-structured and multilingual data that is very valuable to the Natural Language Processing community. This data sets can effectively be used to address complex machine translation, multilingual terminology extraction, text-mining, or to benchmark information retrieval systems. Despite of the services offered by the user-friendliness of the web site that is made available to the public to access the publishing of the EU call for tenders, collecting and managing such kind of data is a great burden and consumes a lot of time and computing resources. This could explain why such a resource is not very (if any) exploited today by computer scientists or engineers in NLP. The aim of this paper is to describe two documented and easy-to-use multilingual corpora (one of them is a parallel corpus), extracted from the TED web source that we will release for the benefit of the NLP community.","Two Multilingual Corpora Extracted from the Tenders Electronic Daily for Machine Learning and Machine Translation Applications","Oussama Ahmia, Nicolas Béchet, Pierre-François Marteau","3472","Deux corpus multilingues extraits du Tenders Electronic Daily pour des applications d'apprentissage et de traduction automatiques

Le ""Tenders Electronic Daily"" (TED) européen est une source importante de données semi-structurées et multilingues très précieuses pour la communauté du traitement du langage naturel. Ces ensembles de données peuvent être utilisés efficacement pour la traduction automatique complexe, l'extraction de terminologie multilingue, l'exploration de texte ou l'évaluation des systèmes de recherche d'informations. Malgré les services offerts par la convivialité du site web mis à la disposition du public pour accéder à la publication des appels d'offres de l'UE, la collecte et la gestion de ce type de données représentent une charge importante et consomment beaucoup de temps et de ressources informatiques. Cela pourrait expliquer pourquoi une telle ressource n'est pas (ou très peu) exploitée aujourd'hui par les informaticiens ou les ingénieurs en TAL. L'objectif de cet article est de décrire deux corpus multilingues documentés et faciles à utiliser (l'un d'entre eux est un corpus parallèle), extraits de la source web TED, que nous mettrons à la disposition de la communauté du TAL.","8486","3","398","Deux corpus multilingues extraits du Tenders Electronic Daily pour des applications d&#039;apprentissage et de traduction automatiques

Le &quot;Tenders Electronic Daily&quot; (TED) européen est une source importante de données semi-structurées et multilingues très précieuses pour la communauté du traitement automatique des langues. Ces ensembles de données peuvent être utilisés efficacement pour la traduction automatique complexe, l&#039;extraction de terminologie multilingue, la fouille de textes ou l&#039;évaluation des systèmes de recherche d&#039;informations. Malgré les services offerts généreusement par le site web et mis à la disposition du public pour accéder à la publication des appels d&#039;offres de l&#039;UE, la collecte et la gestion de ce type de données représentent une charge importante et consomment beaucoup de temps et de ressources informatiques. Cela pourrait expliquer pourquoi une telle ressource n&#039;est pas (ou très peu) exploitée aujourd&#039;hui par les informaticiens ou les ingénieurs en TAL. L&#039;objectif de cet article est de décrire deux corpus multilingues documentés et faciles à utiliser (l&#039;un d&#039;entre eux est un corpus parallèle), extraits de la source web TED, que nous mettrons à la disposition de la communauté du TAL.","4","2023-07-06 17:57:23","2023-07-06 18:01:38","8486",,"2","1","2","1","1","1",NULL,,"2023-07-06 05:57:09","0"
"3424174","This paper aims at identifying the information flow in state-of-the-art machine translation systems, taking as example the transfer of gender when translating from French into English. Using a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system. Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer.","Screening Gender Transfer in Neural Machine Translation","Guillaume Wisniewski, Lichao Zhu, Nicolas Ballier, François Yvon","945","Screening du transfert de genre dans la traduction automatique neuronale

Cet article vise à identifier le flux d'informations dans les systèmes de traduction automatique de pointe, en prenant comme exemple le transfert du genre lors de la traduction du français vers l'anglais. En utilisant un ensemble contrôlé d'exemples, nous expérimentons plusieurs façons d'étudier comment l'information sur le genre circule dans une architecture codeur-décodeur en considérant à la fois des techniques de sondage et des interventions sur les représentations internes utilisées dans le système de traduction automatique. Nos résultats montrent que l'information sur le genre peut être trouvée dans toutes les représentations de jetons construites par le codeur et le décodeur et nous amènent à conclure qu'il existe de multiples voies pour le transfert du genre.","11197","3","400","Criblage du transfert de genre dans la traduction automatique neuronale

Cet article vise à identifier le flux d&#039;informations dans les systèmes de traduction automatique à l&#039;état de l&#039;art, en prenant comme exemple le transfert du genre lors de la traduction du français vers l&#039;anglais. En utilisant un ensemble contrôlé d&#039;exemples, nous expérimentons plusieurs façons d&#039;étudier comment l&#039;information sur le genre circule dans une architecture encodeur-décodeur en considérant à la fois des techniques de sondage et des interventions sur les représentations internes utilisées dans le système de traduction automatique. Nos résultats montrent que l&#039;information sur le genre peut être trouvée dans tous les tokens de représentations construits par l&#039;encodeur et le décodeur, et nous amènent à conclure qu&#039;il existe de multiples voies pour le transfert du genre.","4","2023-07-06 18:03:35","2023-07-06 18:07:09","11197",,"1","1","3","1","1","1",NULL,,"2023-07-06 06:03:33","0"
"1821049","From many years, machine translation and computational linguistic research community has given immense attention towards the development of machine translation techniques. In order to fulfill the goal of machine translation “translation without losing meaning”, a lot of translation methods have been proposed. All of these translation methods differ in their theories and implementation strategies. Although some basic rules of translation are same but many of them vary with the selection of language pair. While concerning with the scientific text, every science domain has thousands of terminologies. Translation of these terminologies according to the domain boosts the performance of translation. Translation of scientific text is ignored in the literature, as it needs more effort and expertise of both domain and language are required. In this research, we have proposed an effective scientific text translator for English to Urdu to cope with the challenge of scientific text translation. This method tags and translate the terms according to the domain. We have introduced a term tagger for tagging terms. The system can work for any domain but for experimental purpose we have selected the domain of computer science. System is evaluated on self-generated corpus of computer science. It is also compared with the existing translators to demonstrate the dominance of proposed translator as compared to the competitor. The comparative results of proposed approach and existing are shown in the form of tables.","Corpus Based Machine Translation for Scientific Text","Irsha Tehseen, Ghulam Tahir, Khadija Shakeel, Mubbashir Ali","3427","Traduction automatique de textes scientifiques à partir de corpus

Depuis de nombreuses années, la communauté des chercheurs en traduction automatique et en linguistique computationnelle accorde une grande attention au développement des techniques de traduction automatique. Afin d'atteindre l'objectif de la traduction automatique, à savoir ""traduire sans perdre le sens"", de nombreuses méthodes de traduction ont été proposées. Toutes ces méthodes de traduction diffèrent par leurs théories et leurs stratégies de mise en œuvre. Bien que certaines règles de base de la traduction soient identiques, beaucoup d'entre elles varient en fonction de la sélection de la paire de langues. En ce qui concerne les textes scientifiques, chaque domaine scientifique comporte des milliers de terminologies. La traduction de ces terminologies en fonction du domaine augmente les performances de la traduction. La traduction de textes scientifiques est ignorée dans la littérature, car elle nécessite plus d'efforts et une expertise à la fois du domaine et de la langue. Dans cette recherche, nous avons proposé un traducteur de texte scientifique efficace pour l'anglais vers l'urdu afin de relever le défi de la traduction de texte scientifique. Cette méthode permet d'étiqueter et de traduire les termes en fonction du domaine. Nous avons introduit un marqueur de termes pour étiqueter les termes. Le système peut fonctionner dans n'importe quel domaine, mais à des fins expérimentales, nous avons choisi le domaine de l'informatique. Le système est évalué sur un corpus d'informatique auto-généré. Il est également comparé aux traducteurs existants afin de démontrer la prédominance du traducteur proposé par rapport à ses concurrents. Les résultats comparatifs de l'approche proposée et des traducteurs existants sont présentés sous forme de tableaux.","8441","3","404","Traduction automatique de textes scientifiques à partir de corpus

Depuis de nombreuses années, la communauté des chercheurs en traduction automatique et en linguistique computationnelle accorde une grande attention au développement des techniques de traduction automatique. Afin d&#039;atteindre l&#039;objectif de la traduction automatique, à savoir &quot;traduire sans perte de sens&quot;, de nombreuses méthodes de traduction ont été proposées. Toutes ces méthodes de traduction diffèrent par leurs théories et leurs stratégies de mise en œuvre. Bien que certaines règles de base de la traduction soient identiques, beaucoup d&#039;entre elles varient en fonction de la sélection du couple de langues. En ce qui concerne les textes scientifiques, chaque domaine scientifique possède une terminologie spécifique avec des milliers de termes. La traduction de ces termes en fonction du domaine augmente les performances de la traduction. La traduction de textes scientifiques est ignorée dans la littérature, car elle nécessite plus d&#039;efforts et une expertise à la fois du domaine et de la langue. Dans cette recherche, nous avons proposé un traducteur de texte scientifique efficace de l&#039;anglais vers l&#039;urdu afin de relever le défi de la traduction de texte scientifique. Cette méthode permet d&#039;étiqueter et de traduire les termes en fonction du domaine. Nous avons introduit un marqueur pour étiqueter les termes. Le système peut fonctionner dans n&#039;importe quel domaine, mais à des fins expérimentales, nous avons choisi le domaine de l&#039;informatique. Le système est évalué sur un corpus d&#039;informatique auto-généré. Il est également comparé aux traducteurs existants afin de démontrer la prédominance du traducteur proposé par rapport à ses concurrents. Les résultats comparatifs de l&#039;approche proposée et des traducteurs existants sont présentés sous forme de tableaux.","4","2023-07-07 14:19:58","2023-07-07 14:56:38","8441",,"1","2","2","2","1","1",NULL,,"2023-07-07 02:19:55","0"
"3014110","Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT). We evaluate the effect of using similar translations as priming cues on the NMT network. We propose a method to inject priming cues into the NMT networkand compare our framework to other mechanisms that perform micro-adaptation during inference. Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy.Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources.","Priming Neural Machine Translation","Minh Quang Pham, Jitao Xu, Josep-Maria Crego, Jean Senellart, François Yvon","1270","Amorçage de la traduction automatique de neurones

L’amorçage est un phénomène psychologique bien connu et étudié basé sur la présentation préalable d’un stimulus (cue) pour influencer le traitement d’une réponse. Dans cet article, nous proposons un cadre pour imiter le processus d’amorçage dans le contexte de la traduction automatique neuronale (NMT). Nous évaluons l’effet de l’utilisation de traductions similaires comme indices d’amorçage sur le réseau NMT. Nous proposons une méthode pour injecter des signaux d’amorçage dans le réseau NMT et comparer notre framework à d’autres mécanismes qui effectuent la micro-adaptation pendant l’inférence. Dans l’ensemble, les expériences menées dans un contexte multi-domaines confirment que l’ajout d’indicateurs d’amorçage dans le décodeur NMT peut contribuer grandement à améliorer la précision de la traduction.","3531","4","410","Amorçage de la traduction automatique neuronale

L’amorçage est un phénomène psychologique bien connu et étudié basé sur la présentation préalable d’un stimulus (signal) pour influencer le traitement d’une réponse. Dans cet article, nous proposons un cadre pour imiter le processus d’amorçage dans le contexte de la traduction automatique neuronale (TAN). Nous évaluons l’effet de l’utilisation de traductions similaires comme indices d’amorçage sur le réseau de TAN. Nous proposons une méthode pour injecter des signaux d’amorçage dans le réseau TAN et comparer notre approche à d’autres mécanismes qui effectuent la micro-adaptation pendant l’inférence. Dans l’ensemble, les expériences menées dans un contexte multi-domaines confirment que l’ajout d’indicateurs d’amorçage dans le décodeur TAN peut contribuer grandement à améliorer la précision de la traduction. D&#039;autre part, nous montrons que notre approche convient pour collecter des informations valables pour un réseau de TAN à partir de ressources monolingues.","4","2023-07-07 14:57:36","2023-07-07 15:32:36","3531",,"4","1","4","2","2","3",NULL,"Dernière phrase complètement omise","2023-07-07 02:57:31","0"
"1588171","In this paper, we address the problem of generating English tag questions (TQs) (e.g. it is, isn't it?) in Machine Translation (MT). We propose a post-edition solution, formulating the problem as a multi-class classification task. We present (i) the automatic annotation of English TQs in a parallel corpus of subtitles and (ii) an approach using a series of classifiers to predict TQ forms, which we use to post-edit state-of-the-art MT outputs. Our method provides significant improvements in English TQ translation when translating from Czech, French and German, in turn improving the fluidity, naturalness, grammatical correctness and pragmatic coherence of MT output.","Machine Translation, it’s a question of style, innit? The case of English tag questions","Rachel Bawden","2998","Traduction automatique, c'est une question de style, n'est-ce pas ? Le cas des questions de tag en anglais

Dans cet article, nous abordons le problème de la génération de questions à étiquette (TQ) en anglais (par exemple, it is, isn't it ?) dans le cadre de la traduction automatique (TA). Nous proposons une solution de post-édition, en formulant le problème comme une tâche de classification multi-classes. Nous présentons (i) l'annotation automatique des QT anglais dans un corpus parallèle de sous-titres et (ii) une approche utilisant une série de classificateurs pour prédire les formes de QT, que nous utilisons pour post-éditer les résultats de traduction automatique les plus récents. Notre méthode apporte des améliorations significatives à la traduction des QT en anglais à partir du tchèque, du français et de l'allemand, améliorant ainsi la fluidité, le naturel, l'exactitude grammaticale et la cohérence pragmatique des résultats de la traduction assistée par ordinateur.","8012","3","413","La traduction automatique, c&#039;est une question de style, n&#039;est-ce pas ? Le cas des &quot;tag questions&quot; en anglais

Dans cet article, nous abordons le problème de la génération de &quot;tag questions&quot; (TQ) en anglais (par exemple, &quot;it is, isn&#039;t it?&quot;) dans le cadre de la traduction automatique (TA). Nous proposons une solution de post-édition, en formulant le problème comme une tâche de classification multi-classes. Nous présentons (i) l&#039;annotation automatique des TQ anglais dans un corpus parallèle de sous-titres et (ii) une approche utilisant une série de classificateurs pour prédire les formes de TQ, que nous utilisons pour post-éditer les résultats de traduction automatique les plus récents. Notre méthode apporte des améliorations significatives à la traduction des TQ en anglais à partir du tchèque, du français et de l&#039;allemand, améliorant ainsi la fluidité, le naturel, l&#039;exactitude grammaticale et la cohérence pragmatique des résultats de la TA.","4","2023-07-07 15:33:10","2023-07-07 15:58:02","8012",,"3","2","4","2","1","4",NULL,,"2023-07-07 03:33:04","0"
"3852190","This paper tackles the problem of the selected Mongolic languages: Khamnigan Mongol, Oirat Mongol and Dagur spoken in Russia, Mongolia and China. The aim of this presentation is to answer the following questions: What is the current sociolinguistic condition of these languages and which NLP tools have been applied to which Mongolic languages so far? The presented paper will demonstrate that the development of the NLP tools for the family of Mongolic languages started only recently and refers mostly to the information retrieval (IR) tasks and topics which are related to it, such as applying stemming and stoplist in IR, keyword retrieval system for locating words in historical Mongolian document images, n-gram-based retrieval units and speech recognition tools devoted to the Mongolian phenomenon of vowel harmony.","The application of natural language processing (NLP) tools in relation to selected Mongolic languages: review of the current literature, available NLP tools and outlooks for the future","Joanna Dolińska-Streltsov","4147","L'application des outils de traitement du langage naturel (PNL) en relation avec les langues mongoles sélectionnées : examen de la documentation actuelle, des outils de PNL disponibles et des perspectives d'avenir

Ce document aborde le problème des langues mongoles sélectionnées : Khamnigan Mongol, Oirat Mongol et Dagur parlés en Russie, Mongolie et Chine. Le but de cette présentation est de répondre aux questions suivantes : Quelle est la situation sociolinguistique actuelle de ces langues et quels outils de PNL ont été appliqués à quelles langues mongoles jusqu'à présent ? Le document présenté montrera que le développement des outils NLP pour la famille des langues mongoles a commencé seulement récemment et se réfère principalement aux tâches de récupération d'informations (IR) et les sujets qui sont liés à elle, tels que l'application de bourrage et stoplist dans IR, système de récupération de mots clés pour localiser des mots dans des images historiques de documents mongoles, n-gram base de récupération unités et des outils de reconnaissance vocale consacrés au phénomène mongol de voyelle harmonie.","16063","6","415","L&#039;application des outils de traitement automatique des langues (TAL) en relation avec les langues mongoles sélectionnées : examen de la documentation actuelle, des outils de TAL disponibles et perspectives d&#039;avenir

Ce document aborde le problème des langues mongoles sélectionnées : le khamnigan, l&#039;oïrate et le daur, parlés en Russie, en Mongolie et en Chine. Le but de cette présentation est de répondre aux questions suivantes : quelle est la situation sociolinguistique actuelle de ces langues et quels outils de TAL ont été appliqués à quelles langues mongoles jusqu&#039;à présent ? Le document présenté montre que le développement des outils de TAL pour la famille des langues mongoles n&#039;a commencé que récemment et se réfère principalement aux tâches de recherche d&#039;information (RI) et les sujets qui y sont liés, tels que l&#039;application de bourrage et la stoplist en RI, système de récupération de mots clés pour localiser des mots dans des images historiques de documents mongoles, unités de recherche basées sur des n-grammes, et des outils de reconnaissance vocale consacrés au phénomène mongol d&#039;harmonie vocalique.","4","2023-07-07 16:04:03","2023-07-07 17:13:10","16063",,"3","4","4","3","2","2",NULL,,"2023-07-07 04:03:46","0"
"3670165","This chapter investigates the linguistic interpretation of complexity metrics in L2 proficiency assessment. By analysing 84 formulas of metrics linked to lexical diversity, readability and syntactic complexity, we identify a taxonomy of their underlying linguistic scopes. These metrics are classified according to text, sentence, clause, phrase and word scopes with attributes and methods. Homogeneity of scopes was evaluated by applying a mixed clustering PCA approach to metrics computed for 329 L2 texts. Discriminative power was evaluated with a random forest approach on the same dataset including the CEFR levels. Results show that metrics are diversely clustered but they also suggest in-cluster homogeneity. The CEFR classification shows mixed results suggesting that diversity, repetition and size in word and text scopes are significant.","Investigating the scopes of textual metrics for learner level discrimination and learner analytics","Thomas Gaillat","4655","Enquête sur la portée des mesures textuelles pour la discrimination au niveau de l’apprenant et l’analyse de l’apprenant

Ce chapitre examine l’interprétation linguistique des métriques de complexité dans l’évaluation des compétences en L2. En analysant 84 formules de métriques liées à la diversité lexicale, à la lisibilité et à la complexité syntaxique, nous identifions une taxonomie de leurs étendues linguistiques sous-jacentes. Ces métriques sont classées en fonction de la portée du texte, de la phrase, de la clause, de la phrase et des mots avec des attributs et des méthodes. L’homogénéité des champs d’application a été évaluée en appliquant une approche mixte de l’APC aux métriques calculées pour 329 textes L2. Le pouvoir discriminatif a été évalué avec une approche forestière aléatoire sur le même ensemble de données, y compris les niveaux du CECR. Les résultats montrent que les métriques sont diversement regroupées, mais elles suggèrent également une homogénéité en grappe. La classification du CECR montre des résultats mitigés suggérant que la diversité, la répétition et la taille dans les champs de mots et de texte sont significatives.","11442","4","417","Enquête sur la portée des mesures textuelles pour la discrimination au niveau de l’apprenant et l’analyse de l’apprenant

Ce chapitre examine l’interprétation linguistique des métriques de complexité dans l’évaluation des compétences en L2. En analysant 84 formules de métriques liées à la diversité lexicale, à la lisibilité et à la complexité syntaxique, nous identifions une taxonomie de leurs étendues linguistiques sous-jacentes. Ces métriques sont classées en fonction de la portée du texte, de la phrase, du syntagme, de l&#039;expression et des mots avec des attributs et des méthodes. L’homogénéité des champs d’application a été évaluée en appliquant une approche mixte de l’APC aux métriques calculées pour 329 textes L2. Le pouvoir discriminatif a été évalué avec une approche &quot;random forest&quot; sur le même ensemble de données, y compris les niveaux du CECR. Les résultats montrent que les métriques sont diversement regroupées, mais elles suggèrent également une homogénéité en grappe. La classification du CECR montre des résultats mitigés suggérant que la diversité, la répétition et la taille dans les champs de mots et de texte sont significatives.","4","2023-07-07 17:19:41","2023-07-07 17:31:45","11442",,"3","2","4","2","2","3",NULL,,"2023-07-07 05:19:18","0"
"3932196","Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models’ pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pre-trained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16x its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6x its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource.","Multitask Prompted Training Enables Zero-Shot Task Generalization","Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng-Xin Yong, Harshit Pandey, Michael Mckenna, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Fries, Ryan Teehan, Tali Bers, Stella Biderman, Leo Gao, Thomas Wolf, Alexander Rush","3932","Une formation accélérée multitâches permet une généralisation des tâches zéro-shot

Il a récemment été démontré que les grands modèles linguistiques atteignent une généralisation raisonnable et nulle sur un ensemble diversifié de tâches (Brown et al., 2020). On a émis l’hypothèse qu’il s’agit d’une conséquence de l’apprentissage multitâche implicite dans la préformation des modèles linguistiques (Radford et al., 2019). La généralisation zéro-shot peut-elle plutôt être directement induite par l’apprentissage multitâche explicite? Pour tester cette question à grande échelle, nous développons un système permettant de cartographier facilement toutes les tâches en langage naturel dans une forme à incitation humaine. Nous convertissons un grand ensemble d’ensembles de données supervisés, chacun avec plusieurs invites avec des formulations diverses. Ces ensembles de données invités permettent de comparer la capacité d’un modèle à effectuer des tâches complètement bloquées. Nous affinons un modèle de décodeur-décodeur pré-formé (Raffel et al., 2020; Lester et al., 2021) sur ce mélange multitâches couvrant une grande variété de tâches. Le modèle atteint de fortes performances zéro-shot sur plusieurs ensembles de données standard, souvent surperformant les modèles jusqu’à 16x sa taille. En outre, notre approche atteint de solides performances sur un sous-ensemble de tâches du benchmark BIG-bench, surperformant les modèles jusqu’à 6 fois sa taille. Tous les modèles formés sont disponibles à l’adresse https://github.com/bigscience-workshop/t-zero, et toutes les instructions sont disponibles sur https://github.com/bigscience-workshop/promptsource.","12890","4","420","Une formation accélérée multitâches permet une généralisation des tâches zéro-shot

Il a récemment été démontré que les grands modèles linguistiques atteignent une généralisation zéro-shot raisonnable sur un ensemble diversifié de tâches (Brown et al., 2020). On a émis l’hypothèse qu’il s’agit d’une conséquence de l’apprentissage multitâche implicite dans la préformation des modèles linguistiques (Radford et al., 2019). La généralisation zéro-shot peut-elle plutôt être directement induite par l’apprentissage multitâche explicite ? Pour tester cette question à grande échelle, nous développons un système permettant de cartographier facilement toutes les tâches en langage naturel dans une forme à incitation humaine. Nous convertissons un grand nombre d’ensembles de données supervisés, chacun avec plusieurs invites et des formulations diverses. Ces ensembles de données avec invite permettent de comparer la capacité d’un modèle à effectuer des tâches complètement bloquées. Nous affinons un modèle de décodeur-décodeur pré-formé (Raffel et al., 2020; Lester et al., 2021) sur ce mélange multitâches couvrant une grande variété de tâches. Le modèle atteint de fortes performances zéro-shot sur plusieurs ensembles de données standard, souvent dépassant les modèles jusqu’à 16 fois sa taille. En outre, notre approche atteint de solides performances sur un sous-ensemble de tâches du benchmark BIG-bench, dépassant les modèles jusqu’à 6 fois sa taille. Tous les modèles formés sont disponibles à l’adresse https://github.com/bigscience-workshop/t-zero, et toutes les instructions sont disponibles sur https://github.com/bigscience-workshop/promptsource.","4","2023-07-10 11:16:00","2023-07-10 12:11:48","12890",,"2","2","3","3","2","3",NULL,,"2023-07-10 11:15:57","0"
"3873355","Binary classifiers are traditionally studied by propositional logic (PL). PL can only represent them as white boxes, under the assumption that the underlying Boolean function is fully known. Binary classifiers used in practical applications and trained by machine learning are however opaque. They are usually described as black boxes. In this paper, we provide a product modal logic called PLC (Product modal Logic for binary input Classifier) in which the notion of ""black box"" is interpreted as the uncertainty over a set of classifiers. We give results about axiomatics and complexity of satisfiability checking for our logic. Moreover, we present a dynamic extension in which the process of acquiring new information about the actual classifier can be represented.","A Logic of ""Black Box"" Classifier Systems","Xinghan Liu, Emiliano Lorini","4084","Une logique des systèmes de classification «Black Box»

Les classificateurs binaires sont traditionnellement étudiés par la logique propositionnelle (PL). PL ne peut les représenter que comme des boîtes blanches, sous l’hypothèse que la fonction booléenne sous-jacente est pleinement connue. Les classificateurs binaires utilisés dans les applications pratiques et formés par l’apprentissage automatique sont cependant opaques. Ils sont généralement décrits comme des boîtes noires. Dans cet article, nous fournissons une logique modale de produit appelée PLC (Product modal Logic for binaire input Classifier) dans laquelle la notion de «boîte noire» est interprétée comme l’incertitude sur un ensemble de classificateurs. Nous donnons des résultats sur l’axiomatique et la complexité de la vérification de la satisfaction pour notre logique. De plus, nous présentons une extension dynamique dans laquelle le processus d’acquisition de nouvelles informations sur le classificateur réel peut être représenté.","13042","4","421","Logique des systèmes de classification « Black Box »

Les classificateurs binaires sont traditionnellement étudiés par la logique propositionnelle (LP). La LP ne peut les représenter que comme des boîtes blanches, en partant du principe que la fonction booléenne sous-jacente est pleinement connue. Les classificateurs binaires utilisés dans les applications pratiques et formés par l’apprentissage automatique sont cependant opaques. Ils sont généralement décrits comme des « black boxes » (boîtes noires). Dans cet article, nous fournissons une logique modale de produit appelée PLC (Product modal Logic for binary input Classifier) dans laquelle la notion de « Black Box » est interprétée comme l’incertitude sur un ensemble de classificateurs. Nous donnons des résultats sur l’axiomatique et la complexité de la vérification de la satisfaction pour notre logique. De plus, nous présentons une extension dynamique dans laquelle le processus d’acquisition de nouvelles informations sur le classificateur réel peut être représenté.","4","2023-07-10 12:14:15","2023-07-10 12:52:28","13042",,"2","1","3","2","3","3",NULL,,"2023-07-10 12:11:54","0"
"3937320","The present study aims at predicting the speech fluency of children using automatic acoustic measures derived from forward-backward divergence segmentation (FBDS). Thirteen Korean children were recorded while reading out loud a set of sentences. Three native-Korean speakers evaluated the fluency of each sentence on a five-point scale. A FBDS algorithm was used to segment speech recordings into sub-phonemic units and silent segments. In addition to the low-level acoustic features directly derived from FBDS segments, higher-level acoustic features were computed by clustering FBDS segments into pseudo-syllables and silent breaks. Both low-and higher-level features were used to predict average ratings of speech fluency, using a leave-one-speaker-out cross-validation scheme and three regression models: a multiple linear regression, a support vector regression, and a random-forest regressor. Highly accurate predictions were achieved, with average root-mean-square errors (RMSEs) as low as 0.3. Prediction accuracy did not significantly change as a function of regression model. Using higher-level features yielded lower RMSEs than using raw FBDS features. The results of a multiple linear regression using higher-level features (R 2 = 0.94) suggest that speech/silence ratio and pseudo-syllable rate are the two most important predictors of speech fluency.","Predicting speech fluency in children using automatic acoustic features","Lionel Fontan, Shinyoung Kim, Verdiana De Fino, Sylvain Detey","3908","Prédiction de la fluidité de la parole chez les enfants à l'aide de caractéristiques acoustiques automatiques

La présente étude vise à prédire la fluidité de la parole des enfants à l'aide de mesures acoustiques automatiques dérivées de la segmentation de la divergence avant-arrière (FBDS). Treize enfants coréens ont été enregistrés alors qu'ils lisaient à haute voix une série de phrases. Trois locuteurs natifs coréens ont évalué la fluidité de chaque phrase sur une échelle de cinq points. Un algorithme FBDS a été utilisé pour segmenter les enregistrements vocaux en unités sous-phonémiques et en segments silencieux. Outre les caractéristiques acoustiques de bas niveau directement dérivées des segments FBDS, des caractéristiques acoustiques de plus haut niveau ont été calculées en regroupant les segments FBDS en pseudo-syllabes et en pauses silencieuses. Les caractéristiques de bas et de haut niveau ont été utilisées pour prédire les évaluations moyennes de la fluidité de la parole, à l'aide d'un système de validation croisée (leave-one-speaker-out) et de trois modèles de régression : une régression linéaire multiple, une régression à vecteur de support et un régresseur à forêt aléatoire. Des prédictions très précises ont été obtenues, avec des erreurs quadratiques moyennes (RMSE) aussi faibles que 0,3. La précision des prédictions n'a pas changé de manière significative en fonction du modèle de régression. L'utilisation de caractéristiques de niveau supérieur a permis d'obtenir des RMSE plus faibles que l'utilisation des caractéristiques brutes du FBDS. Les résultats d'une régression linéaire multiple utilisant des caractéristiques de niveau supérieur (R 2 = 0,94) suggèrent que le rapport parole/silence et le taux de pseudo-syllabes sont les deux prédicteurs les plus importants de la fluidité de la parole.","18724","3","424","Prédiction de la fluidité de la parole chez les enfants à l&#039;aide de caractéristiques acoustiques automatiques

La présente étude vise à prédire la fluidité de la parole des enfants à l&#039;aide de mesures acoustiques automatiques dérivées de la segmentation de la divergence avant-arrière (FBDS, forward-backward divergence segmentation). Treize enfants coréens ont été enregistrés alors qu&#039;ils lisaient à haute voix une série de phrases. Trois locuteurs natifs coréens ont évalué la fluidité de chaque phrase sur une échelle de cinq points. Un algorithme FBDS a été utilisé pour segmenter les enregistrements vocaux en unités sous-phonémiques et en segments silencieux. Outre les caractéristiques acoustiques de bas niveau directement dérivées des segments FBDS, des caractéristiques acoustiques de plus haut niveau ont été calculées en regroupant les segments FBDS en pseudo-syllabes et en pauses silencieuses. Les caractéristiques de bas et de haut niveau ont été utilisées pour prédire les évaluations moyennes de la fluidité de la parole, à l&#039;aide d&#039;un système de validation croisée (leave-one-speaker-out) et de trois modèles de régression : une régression linéaire multiple, une régression à vecteur de support et un régresseur à forêt aléatoire. Des prédictions très précises ont été obtenues, avec des racines de l&#039;erreur quadratique moyenne (REQM) très faibles, à 0,3. La précision des prédictions n&#039;a pas changé de manière significative en fonction du modèle de régression. L&#039;utilisation de caractéristiques de niveau supérieur a permis d&#039;obtenir des REQM plus faibles que l&#039;utilisation des caractéristiques brutes du FBDS. Les résultats d&#039;une régression linéaire multiple utilisant des caractéristiques de niveau supérieur (R 2 = 0,94) suggèrent que le rapport parole/silence et le taux de pseudo-syllabes sont les deux prédicteurs les plus importants de la fluidité de la parole.","4","2023-07-10 14:13:17","2023-07-10 14:14:05","18724",,"3","2","4","4","3","3",NULL,,"2023-07-10 02:13:11","0"
"3653914","Substantial progress in spoofing and deepfake detection has been made in recent years. Nonetheless, the community has yet to make notable inroads in providing an explanation for how a classifier produces its output. The dominance of black box spoofing detection solutions is at further odds with the drive toward trustworthy, explainable artificial intelligence. This paper describes our use of SHapley Additive exPlanations (SHAP) to gain new insights in spoofing detection. We demonstrate use of the tool in revealing unexpected classifier behaviour, the artefacts that contribute most to classifier outputs and differences in the behaviour of competing spoofing detection models. The tool is both efficient and flexible, being readily applicable to a host of different architecture models in addition to related, different applications. All results reported in the paper are reproducible using open-source software.","Explaining Deep Learning Models for Spoofing and Deepfake Detection with Shapley Additive Explanations","Wanying Ge, Jose Patino, Massimiliano Todisco, Nicholas Evans","4701","Expliquer les modèles d'apprentissage profond pour l'usurpation et la détection de faux avec Shapley Additive Explications

Des progrès substantiels en matière d'usurpation d'identité et de détection de faux ont été réalisés ces dernières années. Néanmoins, la communauté n'a pas encore fait de percée notable en fournissant une explication de la façon dont un classifieur produit sa production. La prédominance des solutions de détection d'usurpation de boîte noire va à l'encontre de la tendance vers une intelligence artificielle fiable et explicable. Cet article décrit notre utilisation de SHapley Additive exPlanations (SHAP) pour obtenir de nouvelles informations sur la détection d'usurpation. Nous montrons l'utilisation de l'outil pour révéler le comportement inattendu du classificateur, les artefacts qui contribuent le plus aux sorties du classificateur et les différences dans le comportement des modèles concurrents de détection d'usurpation. L'outil est à la fois efficace et flexible, et peut être facilement appliqué à un grand nombre de modèles d'architecture différents, en plus d'applications différentes associées. Tous les résultats rapportés dans l'article sont reproductibles en utilisant un logiciel open source.","16617","6","431","Expliquer les modèles d&#039;apprentissage profond pour la détection d&#039;usurpations et d&#039;infox vidéos avec Shapley Additive Explications

Des progrès substantiels en matière de détection d&#039;usurpation d&#039;identité et d&#039;infox vidéos ont été réalisés ces dernières années. Néanmoins, la communauté n&#039;a pas encore fait de percée notable en fournissant une explication de la façon dont un classifieur produit sa production. La prédominance des solutions de détection d&#039;usurpation avec la méthode &quot;black box&quot; va à l&#039;encontre de la tendance à tenter de créer une intelligence artificielle fiable et explicable. Cet article décrit notre utilisation de SHapley Additive exPlanations (SHAP) pour obtenir de nouvelles informations sur la détection d&#039;usurpation. Nous montrons l&#039;utilisation de l&#039;outil pour révéler le comportement inattendu du classificateur, les artéfacts qui contribuent le plus aux sorties du classificateur et les différences dans le comportement des modèles concurrents de détection d&#039;usurpation. L&#039;outil est à la fois efficace et flexible, et peut être facilement appliqué à un grand nombre de modèles d&#039;architecture différents, en plus d&#039;applications différentes associées. Tous les résultats rapportés dans l&#039;article sont reproductibles en utilisant un logiciel open source.","4","2023-07-10 18:22:21","2023-07-10 18:50:25","16617",,"2","1","4","3","2","3",NULL,,"2023-07-10 06:21:45","0"
"3680561","This article studies the application of the #BenderRule in Natural Language Processing (NLP) articles according to two dimensions. Firstly, in a contrastive manner, by considering two major international conferences, LREC and ACL, and secondly, in a diachronic manner, by inspecting nearly 14,000 articles over a period of time ranging from 2000 to 2020 for LREC and from 1979 to 2020 for ACL. For this purpose, we created a corpus from LREC and ACL articles from the above-mentioned periods, from which we manually annotated nearly 1,000. We then developed two classifiers to automatically annotate the rest of the corpus. We show that LREC articles tend to respect the #BenderRule (80 to 90% of them respect it), whereas only around half of ACL articles do. Interestingly, over the considered periods, the results appear to be stable for the two conferences, even though a rebound in ACL 2020 could be a sign of the influence of the blog post about the #BenderRule.","Do we Name the Languages we Study? The #BenderRule in LREC and ACL articles","Fanny Ducel, Karën Fort, Gaël Lejeune, Yves Lepage","731","Nommons-nous les langues que nous étudions? Le #BenderRule dans les articles LREC et ACL

Cet article étudie l’application des articles #BenderRule dans Natural Language Processing (NLP) selon deux dimensions. D’une part, d’une manière contrastée, en examinant deux grandes conférences internationales, LREC et ACL, et deuxièmement, de manière diachronique, en inspectant près de 14 000 articles sur une période allant de 2000 à 2020 pour LREC et de 1979 à 2020 pour l’ACL. À cette fin, nous avons créé un corpus d’articles LREC et ACL des périodes susmentionnées, dont nous annotons manuellement près de 1 000. Nous avons ensuite développé deux classificateurs pour annoter automatiquement le reste du corpus. Nous montrons que les articles LREC ont tendance à respecter le #BenderRule (80 à 90 % d’entre eux le respectent), alors que seulement environ la moitié des articles ACL le font. Fait intéressant, au cours des périodes considérées, les résultats semblent stables pour les deux conférences, même si un rebond dans ACL 2020 pourrait être un signe de l’influence du billet de blog sur le #BenderRule.","1002","4","432","Nommons-nous les langues que nous étudions ? La #BenderRule dans les articles LREC et ACL

Cet article étudie l’application des articles #BenderRule en traitement automatique des langues (TAL) selon deux dimensions. D’une part, d’une manière contrastée, en examinant deux grandes conférences internationales, LREC et ACL, et deuxièmement, de manière diachronique, en inspectant près de 14 000 articles sur une période allant de 2000 à 2020 pour LREC et de 1979 à 2020 pour l’ACL. À cette fin, nous avons créé un corpus d’articles LREC et ACL des périodes susmentionnées, et nous en annotons manuellement près de 1 000. Nous avons ensuite développé deux classificateurs pour annoter automatiquement le reste du corpus. Nous montrons que les articles LREC ont tendance à respecter la #BenderRule (80 à 90 % d’entre eux le respectent), alors que seulement environ la moitié des articles ACL le font. Fait intéressant, au cours des périodes considérées, les résultats semblent stables pour les deux conférences, même si un rebond dans ACL 2020 pourrait être un signe de l’influence du billet de blog sur la #BenderRule.","4","2023-07-10 18:51:19","2023-07-10 19:34:45","1002",,"1","1","2","3","3","1",NULL,,"2023-07-10 06:50:47","0"
"3855336","This paper introduces a novel method for merging open-domain terminological knowledge. It takes advantage of the Region Connection Calculus (RCC5), a formalism used to represent regions in a topological space and to reason about their set-theoretic relationships. To this end, we first propose a faithful translation of terminological knowledge provided by several and potentially conflicting sources into region spaces. The merging is then performed on these spaces, and the result is translated back into the underlying language of the input sources. Our approach allows us to benefit from the expressivity and the flexibility of RCC5 while dealing with conflicting knowledge in a principled way.","Region-Based Merging of Open-Domain Terminological Knowledge","Zied Bouraoui, Sébastien Konieczny, Thanh Ma, Nicolas Schwind, Ivan Varzinczak","4135","Fusion régionale des connaissances terminologiques à domaine ouvert

Cet article introduit une nouvelle méthode pour fusionner les connaissances terminologiques de domaine ouvert. Il tire parti du calcul de la connexion régionale (RCC5), un formalisme utilisé pour représenter les régions dans un espace topologique et pour raisonner sur leurs relations théorétiques. À cette fin, nous proposons d’abord une traduction fidèle des connaissances terminologiques fournies par plusieurs sources potentiellement conflictuelles dans les espaces régionaux. La fusion est ensuite effectuée sur ces espaces, et le résultat est traduit dans la langue sous-jacente des sources d’entrée. Notre approche nous permet de bénéficier de l’expressivité et de la flexibilité de RCC5 tout en traitant les connaissances contradictoires d’une manière fondée sur des principes.","13093","4","434","Fusion régionale des connaissances terminologiques à domaine ouvert

Cet article introduit une nouvelle méthode pour fusionner les connaissances terminologiques à domaine ouvert. Il tire parti du calcul des connexions entre régions (RCC5), un formalisme utilisé pour représenter les régions dans un espace topologique et pour raisonner sur leurs relations théorétiques. À cette fin, nous proposons d’abord une traduction fidèle des connaissances terminologiques fournies par plusieurs sources potentiellement conflictuelles dans les espaces régionaux. La fusion est ensuite effectuée sur ces espaces, et le résultat est traduit dans la langue sous-jacente des sources d’entrée. Notre approche nous permet de bénéficier de l’expressivité et de la flexibilité du RCC5 tout en traitant les connaissances contradictoires d’une manière fondée sur des principes.","4","2023-07-10 19:36:12","2023-07-10 20:09:30","13093",,"1","1","3","2","1","3",NULL,,"2023-07-10 07:36:07","0"
"3606998","In this paper, a representation based on digital assets and semantic annotations is established for Traditional Craft instances, in a way that captures their socio-historic context and preserves both their tangible and intangible Cultural Heritage dimensions. These meaningful and documented experiential presentations are delivered to the target audience through narratives that address a range of uses, including personalized storytelling, interactive Augmented Reality (AR), augmented physical artifacts, Mixed Reality (MR) exhibitions, and the Web. The provided engaging cultural experiences have the potential to have an impact on interest growth and tourism, which can support Traditional Craft communities and institutions. A secondary impact is the attraction of new apprentices through training and demonstrations that guarantee long-term preservation. The proposed approach is demonstrated in the context of textile manufacturing as practiced by the community of the Haus der Seidenkultur, a former silk factory that was turned into a museum where the traditional craft of Jacquard weaving is still practiced.","Multimodal Narratives for the Presentation of Silk Heritage in the Museum","Hansgeorg Hauser, Cynthia Beisswenger, Nikolaos Partarakis, Xenophon Zabulis, Ilia Adami, Emmanouil Zidianakis, Andreas Patakos, Nikolaos Patsiouras, Effie Karuzaki, Michalis Foukarakis, Aggeliki Tsoli, Ammar Qammaz, Antonis Argyros, Nedjma Cadi, Evangelia Baka, Nadia Magnenat Thalmann, Brenda Olivias, Dimitrios Makrygiannis, Alina Glushkova, Sotiris Manitsaris, Vito Nitti, Lucia Panesse","4811","Récits multimodals pour la présentation du patrimoine de la soie au Musée

Dans cet article, une représentation basée sur les actifs numériques et les annotations sémantiques est établie pour les instances de l’artisanat traditionnel, d’une manière qui capture leur contexte socio-historique et préserve à la fois leurs dimensions tangibles et immatérielles du patrimoine culturel. Ces présentations expérientielles significatives et documentées sont présentées au public cible par le biais de récits qui abordent une gamme d’utilisations, y compris la narration personnalisée, la réalité augmentée interactive, les artefacts physiques augmentés, les expositions de réalité mixte (MR) et le Web. Les expériences culturelles engageantes fournies ont le potentiel d’avoir un impact sur la croissance de l’intérêt et le tourisme, ce qui peut soutenir les communautés et les institutions de l’artisanat traditionnel. Un impact secondaire est l’attrait des nouveaux apprentis par des formations et des démonstrations qui garantissent une préservation à long terme. L’approche proposée est démontrée dans le contexte de la fabrication textile telle que pratiquée par la communauté du Haus der Seidenkultur, une ancienne usine de soie transformée en musée où l’artisanat traditionnel du tissage Jacquard est encore pratiqué.","11598","4","435","Récits multimodaux pour la présentation du patrimoine de la soie au musée

Dans cet article, une représentation basée sur les actifs numériques et les annotations sémantiques est établie pour les instances de l’artisanat traditionnel, d’une manière qui capture leur contexte socio-historique et préserve à la fois les dimensions tangibles et immatérielles de leur patrimoine culturel. Ces représentations expérientielles significatives et documentées sont présentées au public cible par le biais de récits qui abordent une gamme d’utilisations, y compris la narration personnalisée, la réalité augmentée interactive, les artéfacts physiques augmentés, les expositions de réalité mixte (RM) et le Web. Les expériences culturelles engageantes fournies ont le potentiel d’avoir un impact sur la croissance de l’intérêt et le tourisme, ce qui peut soutenir les communautés et les institutions de l’artisanat traditionnel. Un impact secondaire est l’attrait des nouveaux apprentis par des formations et des démonstrations qui garantissent une préservation à long terme. L’approche proposée est démontrée dans le contexte de la fabrication textile telle que pratiquée par la communauté du Haus der Seidenkultur, une ancienne usine de soie transformée en musée où l’artisanat traditionnel du tissage Jacquard est encore pratiqué.","4","2023-07-11 12:55:17","2023-07-11 13:33:45","11598",,"1","1","2","2","2","1",NULL,,"2023-07-11 12:55:01","0"
"3860827","Our discourses are full of potential lexical ambiguities, due in part to the pervasive use of words having multiple senses. Sometimes, one word may even be used in more than one sense throughout a text. But, to what extent is this true for different kinds of texts? Does the use of polysemous words change when a discourse involves two people, or when speakers have time to plan what to say? We investigate these questions by comparing the polysemy level of texts of different nature, with a focus on spontaneous spoken dialogs; unlike previous work which examines solely scripted, written, monolog-like data. We compare multiple metrics that presuppose different conceptualizations of text polysemy, i.e., they consider the observed or the potential number of senses of words, or their sense distribution in a discourse. We show that the polysemy level of texts varies greatly depending on the kind of text considered, with dialog and spoken discourses having generally a higher polysemy level than written monologs. Additionally, our results emphasize the need for relaxing the popular ""one sense per discourse"" hypothesis.","Polysemy in Spoken Conversations and Written Texts","Aina Soler, Matthieu Labeau, Chloé Clavel","481","Polysémie dans les conversations parlées et les textes écrits

Nos discours sont pleins d’ambiguïtés lexicales potentielles, en partie dues à l’utilisation généralisée de mots ayant des sens multiples. Parfois, un mot peut même être utilisé dans plus d’un sens dans un texte. Mais, dans quelle mesure est-ce vrai pour différents types de textes? L’utilisation de mots polysémiques change-t-elle lorsqu’un discours implique deux personnes, ou lorsque les orateurs ont le temps de planifier quoi dire? Nous étudions ces questions en comparant le niveau polysémique de textes de nature différente, en mettant l’accent sur les dialogues spontanés parlés; contrairement aux travaux précédents qui examinent uniquement des données scriptées, écrites, de type monolog. Nous comparons plusieurs métriques qui présupposent différentes conceptualisations du texte polysémie, c’est-à-dire qu’elles considèrent le nombre observé ou potentiel de sens des mots, ou leur distribution des sens dans un discours. Nous montrons que le niveau polysémique des textes varie considérablement en fonction du type de texte considéré, avec des dialogues et des discours parlés ayant généralement un niveau polysémique plus élevé que les monologues écrits. De plus, nos résultats soulignent la nécessité de relâcher l’hypothèse populaire «un sens par discours».","3235","4","441","Polysémie dans les conversations parlées et les textes écrits

Nos discours sont pleins d’ambiguïtés lexicales potentielles, en partie dues à l’utilisation généralisée de mots ayant des sens multiples. Parfois, un mot peut même être utilisé dans plusieurs sens au sein d&#039;un même texte. Mais, dans quelle mesure est-ce vrai pour différents types de textes ? L’utilisation de mots polysémiques change-t-elle lorsqu’un discours implique deux personnes, ou lorsque les locuteurs ont le temps de planifier quoi dire ? Nous étudions ces questions en comparant le niveau polysémique de textes de nature différente, en mettant l’accent sur les dialogues spontanés parlés ; contrairement aux travaux précédents qui examinent uniquement des données scriptées, écrites, de type monologue. Nous comparons plusieurs métriques qui présupposent différentes conceptualisations de polysémie textuelle, c’est-à-dire qu’elles considèrent le nombre observé ou potentiel de sens des mots, ou leur distribution des sens dans un discours. Nous montrons que le niveau polysémique des textes varie considérablement en fonction du type de texte considéré, avec des dialogues et des discours parlés ayant généralement un niveau polysémique plus élevé que les monologues écrits. De plus, nos résultats soulignent la nécessité de relâcher l’hypothèse populaire « un sens par discours ».","4","2023-07-11 15:17:01","2023-07-11 15:17:22","3235",,"3","2","2","4","3","3",NULL,,"2023-07-11 03:16:59","0"
"3757587","In information retrieval (IR) systems, trends and users' interests may change over time, altering either the distribution of requests or contents to be recommended. Since neural ranking approaches heavily depend on the training data, it is crucial to understand the transfer capacity of recent IR approaches to address new domains in the long term. In this paper, we first propose a dataset based upon the MSMarco corpus aiming at modeling a long stream of topics as well as IR property-driven controlled settings. We then in-depth analyze the ability of recent neural IR models while continually learning those streams. Our empirical study highlights in which particular cases catastrophic forgetting occurs (e.g., level of similarity between tasks, peculiarities on text length, and ways of learning models) to provide directions in terms of model design. The integral version of the paper has been published at ECIR 2022.","Continual Learning of Long Topic Sequences in Neural Information Retrieval -abstract","Laure Soulier, Thomas Gerald","4420","Apprentissage continu de longues séquences thématiques dans la recherche d'informations neuronales - abstract

Dans les systèmes de recherche d'informations, les tendances et les intérêts des utilisateurs peuvent changer au fil du temps, modifiant la distribution des demandes ou du contenu à recommander. Étant donné que les approches de classement neuronal dépendent fortement des données de formation, il est crucial de comprendre la capacité de transfert des approches IR récentes pour aborder de nouveaux domaines à long terme. Dans cet article, nous proposons d'abord un ensemble de données basé sur le corpus MSMarco visant à modéliser un long flux de sujets ainsi que des paramètres contrôlés par propriété IR. Nous analysons ensuite en profondeur la capacité des modèles récents d'IR neuronal tout en apprenant continuellement ces flux. Notre étude empirique met en évidence dans quels cas particuliers se produit l'oubli catastrophique (par exemple, niveau de similitude entre les tâches, particularités sur la longueur du texte, et façons d'apprendre les modèles) pour fournir des orientations en termes de conception de modèle. La version intégrale du document a été publiée à l’ECIR 2022.","16336","6","442","Apprentissage continu de longues séquences thématiques dans la recherche d&#039;information neuronale - Résumé

Dans les systèmes de recherche d&#039;information (RI), les tendances et les intérêts des utilisateurs peuvent changer au fil du temps, modifiant la distribution des demandes ou du contenu à recommander. Étant donné que les approches de classement neuronal dépendent fortement des données de formation, il est crucial de comprendre la capacité de transfert des approches RI récentes pour aborder de nouveaux domaines à long terme. Dans cet article, nous proposons d&#039;abord un ensemble de données basé sur le corpus MSMarco visant à modéliser un long flux de sujets ainsi que des paramètres contrôlés par propriété RI. Nous analysons ensuite en profondeur la capacité des modèles récents de RI neuronale tout en apprenant continuellement ces flux. Notre étude empirique met en évidence dans quels cas particuliers se produit l&#039;oubli catastrophique (par exemple, niveau de similitude entre les tâches, particularités sur la longueur du texte, et façons d&#039;apprendre les modèles) pour fournir des orientations en termes de conception de modèle. La version intégrale du document a été publiée à l’ECIR 2022.","4","2023-07-11 15:17:44","2023-07-11 15:25:37","16336",,"2","2","4","3","3","2",NULL,,"2023-07-11 03:17:31","0"
"3767632","A new method to extract knowledge structured as n-Ary relations from scientific articles is presented. We designed and assessed different approaches to reconstruct instances of n-Ary relations extracted from scientific articles in experimental domains, driven by an Ontological and Terminological Resource (OTR) and based on multi-feature representation of relations and their arguments. The proposed method starts with the identification of partial n-Ary relations in tables of scientific articles and then seeks to reconstruct them with argument instances in the article texts. Based on the so-called Scientific Publication Representation (SciPuRe) of textual arguments and Scientific Table Representation (STaRe) of n-Ary relations representation of an n-Ary relation called STARE (Scientific Table Representation, originating from partial n-Ary relations extracted from document tables), here we propose and evaluate different approaches for the selection of textual argument instances that could complement partial n-Ary relations: structural, frequentist and word embedding models. The application domain concerns food packaging, especially composition and permeability data. Experiments were conducted on a corpus of 332 relation instances composed of 1547 arguments. Corpora of full and partial relations recognized in document tables and argument instances extracted from texts are available online. Different methods and strategies were measured with an f-score ranging from.34 to.74. These results show that n-Ary relations reconstruction approach depends on the number of selected candidate argument instances.","A new method to extract n-Ary relation instances from scientific documents","Martin Lentschat, Patrice Buche, Juliette Dibie-Barthelemy, Mathieu Roche","4387","Une nouvelle méthode pour extraire des instances de relations n-Ary à partir de documents scientifiques

Une nouvelle méthode d'extraction de connaissances structurées sous forme de relations n-Ary à partir d'articles scientifiques est présentée. Nous avons conçu et évalué différentes approches pour reconstruire des instances de relations n-Ary extraites d'articles scientifiques dans des domaines expérimentaux, à partir d'une ressource ontologique et terminologique (OTR) et sur la base d'une représentation multifonctionnelle des relations et de leurs arguments. La méthode proposée commence par l'identification de relations n-Ary partielles dans des tableaux d'articles scientifiques et cherche ensuite à les reconstruire avec des instances d'arguments dans les textes des articles. Sur la base de la représentation des arguments textuels dite Scientific Publication Representation (SciPuRe) et de la représentation des relations n-Ary dite Scientific Table Representation (STaRe) d'une relation n-Ary appelée STARE (Scientific Table Representation, originating from partial n-Ary relations extracted from document tables), nous proposons et évaluons ici différentes approches pour la sélection des instances d'arguments textuels qui pourraient compléter les relations n-Ary partielles : modèles structurels, fréquentistes et d'intégration de mots (word embedding models). Le domaine d'application concerne les emballages alimentaires, en particulier les données de composition et de perméabilité. Des expériences ont été menées sur un corpus de 332 instances de relations composées de 1547 arguments. Des corpus de relations complètes et partielles reconnues dans des tables de documents et des instances d'arguments extraites de textes sont disponibles en ligne. Différentes méthodes et stratégies ont été évaluées avec un score f allant de 0,34 à 0,74. Ces résultats montrent que l'approche de reconstruction des relations n-Ary dépend du nombre d'instances d'arguments candidats sélectionnés.","17398","3","444","Une nouvelle méthode pour extraire des instances de relations n-aires à partir de documents scientifiques

Une nouvelle méthode d&#039;extraction de connaissances structurées sous forme de relations n-aires à partir d&#039;articles scientifiques est présentée. Nous avons conçu et évalué différentes approches pour reconstruire des instances de relations n-aires extraites d&#039;articles scientifiques dans des domaines expérimentaux, à partir d&#039;une ressource ontologique et terminologique (ROT) et sur la base d&#039;une représentation multifonctionnelle des relations et de leurs arguments. La méthode proposée commence par l&#039;identification de relations n-aires partielles dans des tableaux d&#039;articles scientifiques et cherche ensuite à les reconstruire avec des instances d&#039;arguments dans les textes des articles. Sur la base de la représentation des arguments textuels dite Scientific Publication Representation (SciPuRe) et de la représentation des relations n-aires dite Scientific Table Representation (STaRe) d&#039;une relation n-aire appelée STARE (Scientific Table Representation, qui provient de relations n-ares partielles extraites de tableaux), nous proposons et évaluons ici différentes approches pour la sélection des instances d&#039;arguments textuels qui pourraient compléter les relations n-aires partielles : modèles structurels, fréquentiels et modèles de plongement lexical. Le domaine d&#039;application concerne les emballages alimentaires, en particulier les données de composition et de perméabilité. Des expériences ont été menées sur un corpus de 332 instances de relations composées de 1547 arguments. Des corpus de relations complètes et partielles reconnues dans des tables de documents et des instances d&#039;arguments extraites de textes sont disponibles en ligne. Différentes méthodes et stratégies ont été évaluées avec un score f allant de 0,34 à 0,74. Ces résultats montrent que l&#039;approche de reconstruction des relations n-aires dépend du nombre d&#039;instances d&#039;arguments candidats sélectionnés.","4","2023-07-11 16:32:07","2023-07-11 16:32:38","17398",,"4","2","4","4","3","3",NULL,"parties entre parenthèses non-traduites","2023-07-11 04:32:06","0"
"3762942","We present the AGODA (Analyse sémantique et Graphes relationnels pour l'Ouverture des Débats à l'Assemblée nationale) project, which aims to create a platform for consulting and exploring digitised French parliamentary debates (1881-1940) available in the digital library of the National Library of France. This project brings together historians and NLP specialists: parliamentary debates are indeed an essential source for French history of the contemporary period, but also for linguistics. This project therefore aims to produce a corpus of texts that can be easily exploited with computational methods, and that respect the TEI standard. Ancient parliamentary debates are also an excellent case study for the development and application of tools for publishing and exploring large historical corpora. In this paper, we present the steps necessary to produce such a corpus. We detail the processing and publication chain of these documents, in particular by mentioning the problems linked to the extraction of texts from digitised images. We also introduce the first analyses that we have carried out on this corpus with ""bag-of-words"" techniques not too sensitive to OCR quality (namely topic modelling and word embedding).","Between History and Natural Language Processing: Study, Enrichment and Online Publication of French Parliamentary Debates of the Early Third Republic (1881-1899)","Marie Puren, Aurélien Pellet, Nicolas Bourgeois, Pierre Vernus, Fanny Lebreton","611","Entre histoire et traitement du langage naturel: Étude, enrichissement et publication en ligne des débats parlementaires français du début de la Troisième République (1881-1899)

Nous présentons le projet AGODA (Analyse sémantique et Graphes relationnels pour l’Ouverture des Débats à l’Assemblée nationale), qui vise à créer une plateforme de consultation et d’exploration des débats parlementaires français numérisés (1881-1940) disponible dans la bibliothèque numérique de la Bibliothèque nationale de France. Ce projet rassemble des historiens et des spécialistes de la PNL: les débats parlementaires sont en effet une source essentielle pour l’histoire française de l’époque contemporaine, mais aussi pour la linguistique. Ce projet vise donc à produire un corpus de textes qui peuvent être facilement exploités avec des méthodes de calcul, et qui respectent la norme TEI. Les débats parlementaires anciens constituent également une excellente étude de cas pour le développement et l’application d’outils d’édition et d’exploration de grands corpus historiques. Dans cet article, nous présentons les étapes nécessaires pour produire un tel corpus. Nous détaillons la chaîne de traitement et de publication de ces documents, notamment en mentionnant les problèmes liés à l’extraction de textes à partir d’images numérisées. Nous introduisons également les premières analyses que nous avons effectuées sur ce corpus avec des techniques de «bag-of-words» pas trop sensibles à la qualité de l’OCR (à savoir la modélisation des sujets et l’intégration de mots).","882","4","445","Entre histoire et traitement automatique des langues : Étude, enrichissement et publication en ligne des débats parlementaires français du début de la Troisième République (1881-1899)

Nous présentons le projet AGODA (Analyse sémantique et Graphes relationnels pour l’Ouverture des Débats à l’Assemblée nationale), qui vise à créer une plateforme de consultation et d’exploration des débats parlementaires français numérisés (1881-1940) disponible dans la bibliothèque numérique de la Bibliothèque Nationale de France. Ce projet rassemble des historiens et des spécialistes du TAL : les débats parlementaires sont en effet une source essentielle pour l’histoire française de l’époque contemporaine, mais aussi pour la linguistique. Ce projet vise donc à produire un corpus de textes qui peuvent être facilement exploités avec des méthodes de calcul, et qui respectent la norme TEI. Les débats parlementaires anciens constituent également une excellente étude de cas pour le développement et l’application d’outils d’édition et d’exploration de grands corpus historiques. Dans cet article, nous présentons les étapes nécessaires pour produire un tel corpus. Nous détaillons la chaîne de traitement et de publication de ces documents, notamment en mentionnant les problèmes liés à l’extraction de textes à partir d’images numérisées. Nous introduisons également les premières analyses que nous avons effectuées sur ce corpus avec des techniques de « sac-de-mots » pas trop sensibles à la qualité de l’OCR (à savoir la modélisation des sujets et le plongement lexical).","4","2023-07-12 10:50:50","2023-07-12 11:01:01","882",,"1","1","3","1","3","1",NULL,,"2023-07-12 10:49:02","0"
"3902196","We tackle the problem of group fairness in classification, where the objective is to learn models that do not unjustly discriminate against subgroups of the population. Most existing approaches are limited to simple binary tasks or involve difficult to implement training mechanisms. This reduces their practical applicability. In this paper, we propose FairGrad, a method to enforce fairness based on a reweighting scheme that iteratively learns group specific weights based on whether they are advantaged or not. FairGrad is easy to implement and can accommodate various standard fairness definitions. Furthermore, we show that it is comparable to standard baselines over various datasets including ones used in natural language processing and computer vision.","FairGrad: Fairness Aware Gradient Descent","Gaurav Maheshwari, Michaël Perrot","4005","FairGrad : Descente De Gradient Sensible A L'Equite

Nous abordons le problème de l'équité de groupe dans la classification, où l'objectif est d'apprendre des modèles qui ne discriminent pas injustement contre des sous-groupes de la population. La plupart des approches existantes se limitent à de simples tâches binaires ou impliquent des mécanismes de formation difficiles à mettre en oeuvre. Cela réduit leur applicabilité pratique. Dans cet article, nous proposons FairGrad, une méthode pour appliquer l'équité basée sur un schéma de repondération qui apprend itérativement des poids spécifiques de groupe en fonction de s'ils sont avantagés ou non. FairGrad est facile à mettre en oeuvre et peut s'adapter à diverses définitions d'équité standard. En outre, nous montrons qu'il est comparable aux lignes de base standard sur divers ensembles de données, y compris ceux utilisés dans le traitement du langage naturel et la vision par ordinateur.","15921","6","448","FairGrad : Algorithme du gradient sensible à l&#039;équité

Nous abordons le problème de l&#039;équité de groupe dans la classification, où l&#039;objectif est d&#039;apprendre des modèles qui ne discriminent pas injustement des sous-groupes de la population. La plupart des approches existantes se limitent à de simples tâches binaires ou impliquent des mécanismes de formation difficiles à mettre en œuvre. Cela réduit leur applicabilité pratique. Dans cet article, nous proposons FairGrad, une méthode pour appliquer l&#039;équité basée sur un schéma de repondération qui apprend itérativement des poids spécifiques de groupe en fonction de s&#039;ils sont avantagés ou non. FairGrad est facile à mettre en œuvre et peut s&#039;adapter à diverses définitions standards d&#039;équité. En outre, nous montrons qu&#039;il est comparable aux lignes de base standards sur divers ensembles de données, y compris ceux utilisés dans le traitement automatique des langues et la vision par ordinateur.","4","2023-07-12 11:25:54","2023-07-12 12:02:13","15921",,"2","2","4","2","3","2",NULL,,"2023-07-12 11:25:51","0"
"3925147","We describe three models submitted for the CODI-CRAC 2022 shared task. To perform identity anaphora resolution, we test several combinations of the incremental clustering approach based on the Workspace Coreference System (WCS) with other coreference models. The best result is achieved by adding the ""cluster merging"" version of the coref-hoi model, which brings up to 10.33% improvement 1 over vanilla WCS clustering. Discourse deixis resolution is implemented as multi-task learning: we combine the learning objective of corefhoi with anaphor type classification. We adapt the higher-order resolution model introduced in Joshi et al. (2019) for bridging resolution given gold mentions and anaphors.","Anaphora Resolution in Dialogue: System Description (CODI-CRAC 2022 Shared Task)","Tatiana Anikina, Natalia Skachkova, Joseph Renner, Priyansh Trivedi","417","Résolution Anaphora dans le dialogue : Description du système (tâche partagée CODI-CRAC 2022)

Nous décrivons trois modèles soumis pour la tâche partagée CODI-CRAC 2022. Pour effectuer une résolution d'anaphore d'identité, nous testons plusieurs combinaisons de l'approche de clustering incrémentiel basée sur le système de référence d'espace de travail (WCS) avec d'autres modèles de référence. Le meilleur résultat est obtenu en ajoutant la version ""fusion de clusters"" du modèle coref-hoi, qui apporte jusqu'à 10,33% d'amélioration 1 par rapport au clustering WCS vanille. La résolution de la démixtion du discours est mise en oeuvre comme apprentissage multitâche : nous combinons l'objectif d'apprentissage de corefhoi avec une classification de type anaphore. Nous adaptons le modèle de résolution d'ordre supérieur introduit dans Joshi et al. (2019) pour ponter la résolution en fonction des mentions d'or et des anaphores.","4164","6","450","Résolution des anaphores dans le dialogue : Description du système (tâche partagée CODI-CRAC 2022)

Nous décrivons trois modèles soumis pour la tâche partagée CODI-CRAC 2022. Pour résoudre l&#039;identité dans le cas d&#039;une anaphore, nous testons plusieurs combinaisons de l&#039;approche de clustering incrémentiel basée sur le Workspace Coreference System (WCS) avec d&#039;autres modèles de référence. Le meilleur résultat est obtenu en ajoutant la version &quot;fusion de clusters&quot; du modèle coref-hoi, qui apporte jusqu&#039;à 10,33% d&#039;amélioration par rapport au clustering WCS vanille. La résolution de la deixis du discours est mise en œuvre comme apprentissage multitâche : nous combinons l&#039;objectif d&#039;apprentissage de coref-hoi avec une classification de type anaphore. Nous adaptons le modèle de résolution d&#039;ordre supérieur introduit dans Joshi et al. (2019) pour ponter la résolution en fonction des mentions d&#039;or et des anaphores.","4","2023-07-12 12:26:19","2023-07-12 12:40:05","4164",,"3","2","4","3","4","4",NULL,,"2023-07-12 12:26:15","0"
"3707232","Discovered by (Austin,1962) and extensively promoted by (Searle, 1975), speech acts (SA) have been the object of extensive discussion in the philosophical and the linguistic literature, as well as in computational linguistics where the detection of SA have shown to be an important step in many down stream NLP applications. In this paper, we attempt to measure for the first time the role of SA on urgency detection in tweets, focusing on natural disasters. Indeed, SA are particularly relevant to identify intentions, desires, plans and preferences towards action, providing therefore actionable information that will help to set priorities for the human teams and decide appropriate rescue actions. To this end, we come up here with four main contributions: (1) A two-layer annotation scheme of SA both at the tweet and subtweet levels, (2) A new French dataset of 6,669 tweets annotated for both urgency and SA, (3) An in-depth analysis of the annotation campaign, highlighting the correlation between SA and urgency categories, and (4) A set of deep learning experiments to detect SA in a crisis corpus. Our results show that SA are correlated with urgency which is a first important step towards SA-aware NLP-based crisis management on social media.","Give me your Intentions, I'll Predict Our Actions: A Two-level Classification of Speech Acts for Crisis Management in Social Media","Enzo Laurenti, Nils Bourgon, Farah Benamara, Alda Mari, Véronique Moriceau, Camille Courgeon","688","Donnez-moi vos intentions, je vais prédire nos actions: Classification à deux niveaux des actes de parole pour la gestion des crises dans les médias sociaux

Découverts par (Austin, 1962) et largement promus par (Searle, 1975), les actes de parole (SA) ont fait l’objet de discussions approfondies dans la littérature philosophique et linguistique, ainsi que dans la linguistique informatique où la détection de SA s’est révélée être une étape importante dans de nombreuses applications de PNL en aval. Dans cet article, nous essayons de mesurer pour la première fois le rôle de l’AS sur la détection des urgences dans les tweets, en mettant l’accent sur les catastrophes naturelles. En effet, l’AS est particulièrement pertinente pour identifier les intentions, les désirs, les plans et les préférences en matière d’action, fournissant ainsi des informations exploitables qui aideront à fixer des priorités pour les équipes humaines et à décider des actions de sauvetage appropriées. À cette fin, nous présentons ici quatre contributions principales: (1) Un schéma d’annotation à deux niveaux de SA tant au niveau du tweet que du sous-tweet, (2) Un nouveau jeu de données français de 6 669 tweets annotés à la fois pour l’urgence et pour SA, (3) Une analyse approfondie de la campagne d’annotation, mettant en évidence la corrélation entre SA et catégories d’urgence, et (4) Un ensemble d’expériences d’apprentissage profond pour détecter la SA dans un corpus de crise. Nos résultats montrent que l’AS est corrélée avec l’urgence, ce qui est une première étape importante vers une gestion des crises basée sur la PNL basée sur les SA sur les médias sociaux.","959","4","452","Dites-moi vos intentions, je vais prédire nos actions : Classification à deux niveaux des actes de langage pour la gestion des crises sur les réseaux sociaux

Découverts par Austin (1962) et largement promus par Searle (1975), les actes de langage (AL) ont fait l’objet de discussions approfondies dans la littérature philosophique et linguistique, ainsi que dans la linguistique informatique où la détection des AL s’est révélée être une étape importante dans de nombreuses applications postérieures en TAL. Dans cet article, nous essayons de mesurer pour la première fois le rôle des AL sur la détection des urgences dans les tweets, en mettant l’accent sur les catastrophes naturelles. En effet, les AL sont particulièrement pertinents pour identifier les intentions, les désirs, les plans et les préférences en matière d’action, fournissant ainsi des informations exploitables qui aideront à fixer des priorités pour les équipes humaines et à décider des actions de sauvetage appropriées. À cette fin, nous présentons ici quatre contributions principales : (1) un schéma d’annotation à deux niveaux d&#039;AL tant au niveau du tweet que du commentaire, (2) un nouveau jeu de données français de 6 669 tweets annotés à la fois pour l’urgence et pour les AL, (3) une analyse approfondie de la campagne d’annotation, mettant en évidence la corrélation entre AL et catégories d’urgence, et (4) un ensemble d’expériences d’apprentissage profond pour détecter les AL dans un corpus de crise. Nos résultats montrent que les AL sont corrélés avec l’urgence, ce qui est une première étape importante vers une gestion des crises grâce au TAL, à l&#039;aide des AL sur les réseaux sociaux.","4","2023-07-12 12:41:09","2023-07-12 13:19:51","959",,"3","2","4","4","4","3",NULL,,"2023-07-12 12:40:45","0"
"3840530","The birth and development of digital libraries—broadly understood as curated collections of electronic documents accessible online on dedicated platforms with tools for search and consultation—have radically transformed research activities. Consequently, traditional practices observed in physical places of knowledge such as libraries and archives—searching catalogues, browsing through shelving, taking notes—have been supplemented with a series of digital practices developed by researchers to browse websites and databases— searching by keywords, filtering results, navigating through links. The ambition of this project is to reclaim a spatial understanding of digital libraries through the investigation of navigation as a fundamental information practice for researchers. Taking Gallica as a case study, this research situated at the crossroads of ethnography, data science, and digital humanities strives to shed light on how researchers “orient themselves” within a digital corpus. Bridging the gap between the ethnography of the digital—the qualitative study of scholars’ practices through observations and interviews—and digital ethnography—the quantitative analysis of navigation traces—, it developed mixed methods combining interviews with topological analysis of navigation paths extracted from server logs. This exploratory study led to promising findings on researcher’s navigation within the Dewey classification, on the role of “pivotal literature”, and on a first sketch of a variety of “regimes of navigation” clustered based on their topological features. More importantly, this research illustrates a perfect case of heuristic dovetailing between a quantitative approach enhanced by digital tools, and a more traditional qualitative approach. Along the course of this ongoing project, emphasis has been put on a reflexive critique of the nature of the data processed, the implications of every step of the pipeline, as well as their link with actual practices observed and objectified by researchers themselves.","How to Orient Oneself in Thinking in the Digital Era: A mixed-methods ethnography of researchers' navigational practices on Gallica","Simon Dumas Primbault","4176","Comment s’orienter dans la pensée à l’ère numérique: Une ethnographie mixte des pratiques de navigation des chercheurs sur Gallica

La naissance et le développement de bibliothèques numériques — largement comprises comme des collections organisées de documents électroniques accessibles en ligne sur des plateformes dédiées avec des outils de recherche et de consultation — ont radicalement transformé les activités de recherche. Par conséquent, les pratiques traditionnelles observées dans les lieux de connaissances physiques tels que les bibliothèques et les archives — la recherche de catalogues, la navigation dans les étagères, la prise de notes — ont été complétées par une série de pratiques numériques développées par les chercheurs pour parcourir les sites Web et les bases de données — la recherche par mots-clés, le filtrage des résultats, la navigation à travers les liens. L’ambition de ce projet est de récupérer une compréhension spatiale des bibliothèques numériques à travers l’investigation de la navigation en tant que pratique d’information fondamentale pour les chercheurs. Prenant Gallica comme une étude de cas, cette recherche située au carrefour de l’ethnographie, de la science des données et des humanités numériques s’efforce de faire la lumière sur la façon dont les chercheurs «s’orientent» au sein d’un corpus numérique. Comblant le fossé entre l’ethnographie du numérique — l’étude qualitative des pratiques des chercheurs par le biais d’observations et d’entretiens — et l’ethnographie numérique — l’analyse quantitative des traces de navigation — elle a développé des méthodes mixtes combinant des entretiens et des analyses topologiques de chemins de navigation extraits des journaux des serveurs. Cette étude exploratoire a conduit à des résultats prometteurs sur la navigation des chercheurs dans la classification de Dewey, sur le rôle de la «littérature pivotique», et sur une première esquisse d’une variété de «régimes de navigation» regroupés en fonction de leurs caractéristiques topologiques. Plus important encore, cette recherche illustre un parfait exemple de concordance heuristique entre une approche quantitative renforcée par des outils numériques, et une approche qualitative plus traditionnelle. Au cours de ce projet en cours, l’accent a été mis sur une critique réflexive de la nature des données traitées, des implications de chaque étape du pipeline, ainsi que de leur lien avec les pratiques réelles observées et objectivées par les chercheurs eux-mêmes.","13134","4","454","Comment s’orienter dans la pensée à l’ère du numérique : Une ethnographie mixte des pratiques de navigation des chercheurs sur Gallica

La naissance et le développement de bibliothèques numériques — largement comprises comme des collections organisées de documents électroniques accessibles en ligne sur des plateformes dédiées avec des outils de recherche et de consultation — ont radicalement transformé les activités de recherche. Par conséquent, les pratiques traditionnelles observées dans les lieux de connaissances physiques tels que les bibliothèques et les archives — la recherche de catalogues, la navigation dans les étagères, la prise de notes — ont été complétées par une série de pratiques numériques développées par les chercheurs pour parcourir les sites Web et les bases de données — la recherche par mots-clés, le filtrage des résultats, la navigation à travers les liens. L’ambition de ce projet est d&#039;acquérir une compréhension spatiale des bibliothèques numériques à travers l’investigation de la navigation en tant que pratique d’information fondamentale pour les chercheurs. Prenant Gallica comme étude de cas, cette recherche située au carrefour de l’ethnographie, de la science des données et des humanités numériques s’efforce de faire la lumière sur la façon dont les chercheurs « s’orientent » au sein d’un corpus numérique. Comblant le fossé entre l’ethnographie du numérique — l’étude qualitative des pratiques des chercheurs par le biais d’observations et d’entretiens — et l’ethnographie numérique — l’analyse quantitative des traces de navigation — elle a développé des méthodes mixtes combinant des entretiens et des analyses topologiques de chemins de navigation extraits des journaux des serveurs. Cette étude exploratoire a conduit à des résultats prometteurs sur la navigation des chercheurs dans la classification de Dewey, sur le rôle de la « littérature pivotique », et sur une première esquisse d’une variété de « régimes de navigation » regroupés en fonction de leurs caractéristiques topologiques. Plus important encore, cette recherche illustre un parfait exemple de concordance heuristique entre une approche quantitative renforcée par des outils numériques, et une approche qualitative plus traditionnelle. Au cours de ce projet en cours, l’accent a été mis sur une critique réflexive de la nature des données traitées, des implications de chaque étape du pipeline, ainsi que de leur lien avec les pratiques réelles observées et objectivées par les chercheurs eux-mêmes.","4","2023-07-12 13:21:40","2023-07-12 13:49:44","13134",,"1","1","3","1","2","1",NULL,,"2023-07-12 01:21:14","0"
"3811323","Evidence-based medicine aims at making decisions about the care of individual patients based on the explicit use of the best available evidence in the patient clinical history and the medical literature results. Argumentation represents a natural way of addressing this task by (i) identifying evidence and claims in text, and (ii) reasoning upon the extracted arguments and their relations to make a decision. ACTA 2.0 is an automated tool which relies on Argument Mining methods to analyse the abstracts of clinical trials to extract argument components and relations to support evidence-based clinical decision making. ACTA 2.0 allows also for the identification of PICO (Patient, Intervention, Comparison, Outcome) elements, and the analysis of the effects of an intervention on the outcomes of the study. A REST API is also provided to exploit the tool's functionalities.","ACTA 2.0: A Modular Architecture for Multi-Layer Argumentative Analysis of Clinical Trials","Benjamin Molinet, Santiago Marro, Elena Cabrio, Serena Villata, Tobias Mayer","4265","ACTA 2.0 : Une architecture modulaire pour l'analyse argumentative multicouche des essais cliniques

La médecine fondée sur les preuves vise à prendre des décisions concernant les soins prodigués à des patients individuels sur la base de l'utilisation explicite des meilleures preuves disponibles dans l'histoire clinique du patient et dans les résultats de la littérature médicale. L'argumentation représente un moyen naturel d'aborder cette tâche en (i) identifiant les preuves et les affirmations dans le texte, et (ii) en raisonnant sur les arguments extraits et leurs relations pour prendre une décision. ACTA 2.0 est un outil automatisé qui s'appuie sur des méthodes d'exploration d'arguments pour analyser les résumés d'essais cliniques afin d'extraire les composants et les relations des arguments pour soutenir la prise de décision clinique fondée sur des preuves. ACTA 2.0 permet également d'identifier les éléments PICO (Patient, Intervention, Comparaison, Résultat) et d'analyser les effets d'une intervention sur les résultats de l'étude. Une API REST est également fournie pour exploiter les fonctionnalités de l'outil.","17276","3","463","ACTA 2.0 : Une architecture modulaire pour l&#039;analyse argumentative multicouche des essais cliniques

La médecine fondée sur les preuves vise à prendre des décisions concernant les soins prodigués à des patients individuels sur la base de l&#039;utilisation explicite des meilleures preuves disponibles dans l&#039;histoire clinique du patient et dans les résultats de la littérature médicale. L&#039;argumentation représente un moyen naturel d&#039;aborder cette tâche en (i) identifiant les preuves et les affirmations dans le texte, et (ii) en raisonnant sur les arguments extraits et leurs relations pour prendre une décision. ACTA 2.0 est un outil automatisé qui s&#039;appuie sur des méthodes de fouille d&#039;arguments pour analyser les résumés d&#039;essais cliniques afin d&#039;extraire les composants et les relations des arguments pour soutenir la prise de décision clinique fondée sur des preuves. ACTA 2.0 permet également d&#039;identifier les éléments PICR (Patient, Intervention, Comparaison, Résultat) et d&#039;analyser les effets d&#039;une intervention sur les résultats de l&#039;étude. Une API REST est également fournie pour exploiter les fonctionnalités de l&#039;outil.","4","2023-07-12 15:32:07","2023-07-12 15:50:19","17276",,"1","1","3","2","1","1",NULL,,"2023-07-12 03:31:52","0"
"3787715","The VoicePrivacy 2020 Challenge focuses on developing anonymization solutions for speech technology. This report complements the summary results and analyses presented by Tomashenko et al. (2021). After quickly recalling the challenge design and the submitted anonymization systems, we provide more detailed results and analyses. First, we present objective evaluation results for the primary challenge metrics and for alternative metrics and attack models, and we compare them with each other. Second, we present subjective evaluation results for speaker verifiability, speech naturalness, and speech intelligibility. Finally, we compare these objective and subjective evaluation results with each other.","Supplementary material to the paper The VoicePrivacy 2020 Challenge: Results and findings","Natalia Tomashenko, Xin Wang, Emmanuel Vincent, Jose Patino, Brij Mohan Lal Srivastava, Paul-Gauthier Noé, Andreas Nautsch, Nicholas Evans, Junichi Yamagishi, Benjamin O'Brien, Anaïs Chanclu, Jean-François Bonastre, Massimiliano Todisco, Mohamed Maouche","4334","Documents supplémentaires au document Le défi VoicePrivacy 2020 : Résultats et conclusions

Le défi VoicePrivacy 2020 se concentre sur le développement de solutions d'anonymisation pour la technologie vocale. Le présent rapport complète les résultats sommaires et les analyses présentés par Tomashenko et al. (2021). Après avoir rappelé rapidement la conception de défi et les systèmes d'anonymisation soumis, nous fournissons des résultats et des analyses plus détaillées. Premièrement, nous présentons des résultats d'évaluation objectifs pour les paramètres de défi primaire et pour les paramètres alternatifs et les modèles d'attaque, et nous les comparons les uns aux autres. Deuxièmement, nous présentons des résultats d'évaluation subjective pour la vérifiabilité du locuteur, le naturel de la parole et l'intelligibilité de la parole. Enfin, nous comparons ces résultats d'évaluation objectifs et subjectifs entre eux.","16250","6","464","Documents supplémentaires au document Le défi VoicePrivacy 2020 : Résultats et conclusions

Le défi VoicePrivacy 2020 se concentre sur le développement de solutions d&#039;anonymisation pour la technologie vocale. Le présent rapport complète les résultats sommaires et les analyses présentés par Tomashenko et al. (2021). Après avoir rappelé rapidement la conception du défi et les systèmes d&#039;anonymisation soumis, nous fournissons des résultats et des analyses plus détaillées. Premièrement, nous présentons des résultats d&#039;évaluation objectifs pour les paramètres de défi primaire et pour les paramètres alternatifs et les modèles d&#039;attaque, et nous les comparons les uns aux autres. Deuxièmement, nous présentons des résultats d&#039;évaluation subjective pour la vérifiabilité du locuteur, le naturel de la parole et l&#039;intelligibilité de la parole. Enfin, nous comparons ces résultats d&#039;évaluation objectifs et subjectifs entre eux.","4","2023-07-12 15:50:48","2023-07-12 16:20:36","16250",,"1","1","3","2","2","1",NULL,,"2023-07-12 03:50:36","0"
"3677438","Non-negative Matrix Factorization (NMF) and its variants have been successfully used for clustering text documents. However, NMF approaches like other models do not explicitly account for the contextual dependencies between words. To remedy this limitation, we draw inspiration from neural word embedding and posit that words that frequently co-occur within the same context (e.g., sentence or document) are likely related to each other in some semantic aspect. We then propose to jointly factorize the document-word and word-word co-occurrence matrices. The decomposition of the latter matrix encourages frequently co-occurring words to have similar latent representations and thereby reflecting the relationships among them. Empirical results, on several real-world datasets, provide strong support for the benefits of our approach. Our main finding is that we can drastically improve the clustering performance of NMF by leveraging the contextual relationships among words explicitly.","Improving NMF clustering by leveraging contextual relationships among words","Mickael Febrissy, Aghiles Salah, Melissa Ailem, Mohamed Nadif","4641","Améliorer le regroupement des NMF en tirant parti des relations contextuelles entre les mots

La factorisation de la matrice non négative (NMF) et ses variantes ont été utilisées avec succès pour regrouper des documents texte. Cependant, les approches NMF comme d’autres modèles ne tiennent pas explicitement compte des dépendances contextuelles entre les mots. Pour remédier à cette limitation, nous nous inspirons de l’incorporation de mots neuronaux et postulons que les mots qui coexistent fréquemment dans le même contexte (p. ex., phrase ou document) sont probablement liés les uns aux autres dans un aspect sémantique. Nous proposons ensuite de factoriser conjointement les matrices de cooccurrence document-mot et mot-mot. La décomposition de cette dernière matrice encourage souvent les mots qui coexistent à avoir des représentations latentes similaires et à refléter ainsi les relations entre eux. Les résultats empiriques, sur plusieurs ensembles de données du monde réel, apportent un soutien solide aux avantages de notre approche. Notre principale conclusion est que nous pouvons améliorer considérablement la performance de clustering de NMF en tirant explicitement parti des relations contextuelles entre les mots.","11428","4","465","Améliorer le regroupement de NMF en tirant parti des relations contextuelles entre les mots

La factorisation en matrices non-négatives (NMF) et ses variantes ont été utilisées avec succès pour regrouper des documents textuels. Cependant, les approches NMF comme d’autres modèles ne tiennent pas explicitement compte des dépendances contextuelles entre les mots. Pour remédier à cette limitation, nous nous inspirons de l’incorporation de mots neuronaux et postulons que les mots qui coexistent fréquemment dans le même contexte (p. ex., phrase ou document) sont probablement liés les uns aux autres dans un aspect sémantique. Nous proposons ensuite de factoriser conjointement les matrices de cooccurrence document-mot et mot-mot. La décomposition de cette dernière matrice encourage souvent les mots qui coexistent à avoir des représentations latentes similaires et à refléter ainsi les relations entre eux. Les résultats empiriques, sur plusieurs ensembles de données du monde réel, apportent un soutien solide aux avantages de notre approche. Notre principale conclusion est que nous pouvons améliorer considérablement la performance de clustering de la NMF en tirant explicitement parti des relations contextuelles entre les mots.","4","2023-07-12 16:23:25","2023-07-12 17:01:06","11428",,"2","1","3","2","1","2",NULL,,"2023-07-12 04:23:16","0"
"3714951","This paper presents a corpus of AZee discourse expressions, i.e. expressions which formally describe Sign Language utterances of any length using the AZee approach and language. The construction of this corpus had two main goals: a first reference corpus for AZee, and a test of its coverage on a significant sample of real-life utterances. We worked on productions from an existing corpus, namely the 40 brèves, containing an hour of French Sign Language. We wrote the corresponding AZee discourse expressions for the entire video content, i.e. expressions capturing the forms produced by the signers and their associated meaning by combining known production rules, a basic building block for these expressions. These are made available as a version 2 extension of the 40 brèves. We explain the way in which these expressions can be built, present the resulting corpus and set of production rules used, and perform first measurements on it. We also propose an evaluation of our corpus: for one hour of discourse, AZee allows to describe 94% of it, while ongoing studies are increasing this coverage. This corpus offers a lot of future prospects, for instance concerning synthesis with virtual signers, machine translation or formal grammars for Sign Language.","A First Corpus of AZee Discourse Expressions","Camille Challant, Michael Filhol","681","Un premier corpus d'expressions de discours AZee

Cet article présente un corpus d'expressions de discours AZee, c'est-à-dire des expressions qui décrivent formellement des énoncés de langage gestuel de n'importe quelle longueur en utilisant l'approche et le langage AZee. La construction de ce corpus avait deux objectifs principaux : un premier corpus de référence pour AZee, et un test de sa couverture sur un échantillon significatif d'énoncés de la vie réelle. Nous avons travaillé sur des productions issues d'un corpus existant, à savoir les 40 brèves, contenant une heure de Langue des Signes Française. Nous avons écrit les expressions de discours AZee correspondantes pour l'ensemble du contenu vidéo, c'est-à-dire des expressions capturant les formes produites par les signataires et leur signification associée en combinant des règles de production connues, un bloc de base pour ces expressions. Ceux-ci sont disponibles en version 2 extension des 40 brèves. Nous expliquons la façon dont ces expressions peuvent être construites, présentons le corpus résultant et l'ensemble des règles de production utilisées, et effectuons les premières mesures. Nous proposons également une évaluation de notre corpus : pendant une heure de discours, AZee permet d'en décrire 94%, alors que des études en cours accroissent cette couverture. Ce corpus offre de nombreuses perspectives d'avenir, par exemple en matière de synthèse avec des signataires virtuels, de traduction automatique ou de grammaires formelles pour la langue des signes.","4428","6","507","Un premier corpus d&#039;expressions discursives AZee

Cet article présente un corpus d&#039;expressions discursives  AZee, c&#039;est-à-dire des expressions qui décrivent formellement des énoncés en langue des signes de n&#039;importe quelle longueur en utilisant l&#039;approche et le langage AZee. La construction de ce corpus avait deux objectifs principaux : constituer un premier corpus de référence pour AZee, et tester sa couverture sur un échantillon significatif d&#039;énoncés de la vie réelle. Nous avons travaillé sur des productions issues d&#039;un corpus existant, à savoir celui des 40 brèves, qui contient une heure de langue des signes française. Nous avons écrit les expressions discursives AZee correspondantes pour l&#039;ensemble du contenu vidéo, c&#039;est-à-dire des expressions qui reconnaissent les formes produites par les signeurs et leur signification associée en combinant des règles de production connues, des blocs de construction pour ces expressions. Celles-ci sont disponibles comme extension version 2 des 40 brèves. Nous expliquons la façon dont ces expressions peuvent être construites, présentons le corpus résultant et l&#039;ensemble des règles de production utilisés; nous effectuons en outre les premières mesures. Nous proposons également une évaluation de notre corpus :  AZee permet de décrire 94% d&#039;une heure de discours. Cependant, des études en cours accroissent cette couverture. Ce corpus offre de nombreuses perspectives d&#039;avenir, par exemple en matière de synthèse avec des signeurs virtuels, de traduction automatique ou de grammaires formelles pour la langue des signes.","10","2023-09-19 14:55:50","2023-09-19 15:13:09","4428",,"4","1","4","4","1","3",NULL,,"2023-09-19 02:55:44","0"
"3720096","This article presents a new French Sign Language (LSF) corpus called Rosetta-LSF. It was created to support future studies on the automatic translation of written French into LSF, rendered through the animation of a virtual signer. An overview of the field highlights the importance of a quality representation of LSF. In order to obtain quality animations understandable by signers, it must surpass the simple ""gloss transcription"" of the LSF lexical units to use in the discourse. To achieve this, we designed a corpus composed of four types of aligned data, and evaluated its usability. These are: news headlines in French, translations of these headlines into LSF in the form of videos showing animations of a virtual signer, gloss annotations of the ""traditional"" type-although including additional information on the context in which each gestural unit is performed as well as their potential for adaptation to another context-and AZee representations of the videos, i.e. formal expressions capturing the necessary and sufficient linguistic information. This article describes this data, exhibiting an example from the corpus. It is available online for public research.","Rosetta-LSF: an Aligned Corpus of French Sign Language and French for Text-to-Sign Translation","Élise Bertin-Lemée, Annelies Braffort, Camille Challant, Claire Danet, Boris Dauriac, Michael Filhol, Emmanuella Martinod, Jérémie Segouat","676","Rosetta-LSF : un corpus aligné de langue des signes française et de français pour la traduction texte-signe

Cet article présente un nouveau corpus en langue des signes française (LSF) appelé Rosetta-LSF. Il a été créé pour soutenir de futures études sur la traduction automatique du français écrit en LSF, rendue par l'animation d'un signeur virtuel. Un aperçu du domaine met en évidence l'importance d'une représentation de qualité de la LSF. Afin d'obtenir des animations de qualité compréhensibles par les signeur, il faut dépasser la simple ""transcription glossaire"" des unités lexicales en LSF à utiliser dans le discours. Pour ce faire, nous avons conçu un corpus composé de quatre types de données alignées, et évalué son utilisabilité. Il s'agit de titres d'actualité en français, de traductions de ces titres en LSF sous forme de vidéos montrant des animations d'un signeur virtuel, d'annotations de gloses de type ""traditionnel"" - bien qu'incluant des informations supplémentaires sur le contexte dans lequel chaque unité gestuelle est exécutée ainsi que leur potentiel d'adaptation à un autre contexte - et de représentations AZee des vidéos, c'est-à-dire d'expressions formelles capturant les informations linguistiques nécessaires et suffisantes. Cet article décrit ces données et présente un exemple du corpus. Il est disponible en ligne pour la recherche publique.","10928","3","510","Rosetta-LSF : un corpus aligné langue des signes française et français pour la traduction texte-signe

Cet article présente un nouveau corpus en langue des signes française (LSF) appelé Rosetta-LSF. Il a été créé pour soutenir de futures études sur la traduction automatique du français écrit vers la LSF, rendue par l&#039;animation d&#039;un signeur virtuel. La vue d&#039;ensemble du domaine met en évidence l&#039;importance d&#039;une représentation de qualité de la LSF. Afin d&#039;obtenir des animations de qualité compréhensibles par les signeur, il faut dépasser la simple &quot;transcription glosée&quot; des unités lexicales en LSF à utiliser dans le discours. Pour ce faire, nous avons conçu un corpus composé de quatre types de données alignées, et évalué son utilisabilité. Il s&#039;agit de titres d&#039;actualité en français, de traductions de ceux-ci en LSF sous forme de vidéos montrant les animations d&#039;un signeur virtuel, d&#039;annotations de gloses de type &quot;traditionnel&quot; (celles-ci incluent cependant aussi des informations supplémentaires sur le contexte dans lequel chaque unité gestuelle est exécutée ainsi que leur potentiel d&#039;adaptation à un autre contexte) et de représentations AZee des vidéos, c&#039;est-à-dire d&#039;expressions formelles capturant les informations linguistiques nécessaires et suffisantes. Cet article décrit ces données et présente un exemple du corpus. Il est disponible en ligne pour la recherche publique.","10","2023-09-19 15:14:16","2023-09-19 15:28:58","10928",,"4","2","4","3","1","1",NULL,"Peu de problèmes graves, mais un problème terminologique grave: &quot;transcription glossaire&quot; au lieu de &quot;transcription glosée&quot; (je crois). Le style rend le texte parfois un peu difficile à suivre. ","2023-09-19 03:14:15","0"
"3686763","Building effective Neural Machine Translation models often implies accommodating diverse sets of heterogeneous data so as to optimize performance for the domain(s) of interest. Such multi-source / multi-domain adaptation problems are typically approached through instance selection or reweighting strategies, based on a static assessment of the relevance of training instances with respect to the task at hand. In this paper, we study dynamic data selection strategies that are able to automatically re-evaluate the usefulness of data samples in the course of training. Based on the results of multiple experiments, we show that our method offer a generic framework to automatically handle several real-world situations, from multi-source or unsupervised domain adaptation to multidomain learning.","Multi-Domain Adaptation in Neural Machine Translation with Dynamic Sampling Strategies","Minh-Quang Pham, Josep Crego, François Yvon","725","Adaptation multi-domaine en traduction automatique neuronale avec stratégies d'échantillonnage dynamique

La construction de modèles efficaces de traduction automatique neuronale implique souvent de prendre en compte divers ensembles de données hétérogènes afin d'optimiser les performances pour le ou les domaines d'intérêt. De tels problèmes d'adaptation multi-sources / multi-domaines sont généralement abordés par le biais de stratégies de sélection ou de repondération d'instances, basées sur une évaluation statique de la pertinence des instances d'apprentissage par rapport à la tâche en cours. Dans cet article, nous étudions les stratégies de sélection dynamique des données qui sont capables de réévaluer automatiquement l'utilité des échantillons de données au cours de la formation. Sur la base des résultats de multiples expériences, nous montrons que notre méthode offre un cadre générique pour gérer automatiquement plusieurs situations réelles, de l'adaptation de domaine multi-source ou non supervisée à l'apprentissage multidomaine.","4472","6","536","Adaptation multi-domaine en traduction automatique neuronale avec stratégies d&#039;échantillonnage dynamique

La construction de modèles efficaces de traduction automatique neuronale implique souvent de prendre en compte divers ensembles de données hétérogènes afin d&#039;optimiser les performances pour le ou les différents domaines spécialisés. De tels problèmes d&#039;adaptation multi-sources / multi-domaines sont généralement abordés grâce à des stratégies de sélection ou de repondération d&#039;instances, qui se fondent sur une évaluation statique de la pertinence des instances d&#039;apprentissage par rapport à la tâche en cours. Dans cet article, nous étudions les stratégies de sélection dynamique des données qui sont capables de réévaluer automatiquement l&#039;utilité des échantillons de données pendant l&#039;entraînement. En nous fondant sur les résultats de multiples expériences, nous montrons que notre méthode offre un cadre générique pour gérer automatiquement plusieurs situations réelles, de l&#039;adaptation de domaine multi-source ou non supervisée à l&#039;apprentissage multidomaine.","10","2023-09-22 15:49:30","2023-09-22 15:56:37","4472",,"4","1","4","2","1","1",NULL,,"2023-09-22 03:49:28","0"
