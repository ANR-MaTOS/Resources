This paper presents the Papillon project dedicated to the building of a linguistically rich multilingual lexical database. This project is based on collaborative construction principle, which allows each one, professional or amateur, institution or individual, to contribute, with its own means, to this building task. For such a collaboratif work to be effective, it is necessary to provide a important set of multilingual lexical information, that will be the base of the contributors' work. After a presentation of the linguistic, lexical and software architectures of the Papillon project, we detail the method used to create the initial lexical information.
We propose in this paper a formalization of the BDéf lexicographic definitions proposed by (Altman et Polguère, 2003), in order to carry calculus on them. The information richness of such definitions suggest many calculus that are useful for lexicographic practice as well as for a more general reflection on modelling lexical semantics. Such calculus cannot be realized without a thorough formalization of such definitions that we propose to represent as typed feature structures (Carpenter, 1992). The formalization proposed allow to automatically check for the lexicon consistency, which suggest a new methodology for lexical description based on successive enrichment of the lexical database and the meta data that describe it.
In this paper, we introduce a new parser, called SXLFG, based on the Lexical Functional Grammars formalism (LFG). We describe the underlying context-free parser and how functional structures are efficiently computed on top of the CFG shared forest thanks to computation sharing, lazy evaluation, and compact data representation. We then present various error recovery and tolerance techniques we implemented in order to build a robust parser. Finally, we offer concrete results when SXLFG is used with an existing grammar for French. We show that our parser, while being a deep and non-probabilistic parser, is both efficient and robust, allowing to parse huge corpora, although the grammar used for the evaluation is very ambiguous.
Research in natural language processing (NLP) has both scientific and technological dimensions. In both cases, it is necessary to evaluate the implemented systems in order to assess the success of a study. This article, grounded in the ISO framework for software evaluation, introduces a typology of NLP systems based on the role of language as input or output data, in order to analyze the central role of evaluation metrics at several stages of the NLP research process. The article focuses on the evaluation metrics that compare the response of a system to a set of correct responses. The analysis of several evaluation examples, in particular the case of machine translation systems, shows the importance of a coherent choice of metrics and of the joint use of several metrics. The influence of the context of use on the set of metrics and the case of interactive systems are discussed as a conclusion.
Some tags used in XML documents create arbitrary breaks in the natural flow of the text. This flexibility may raise some difficulties for some techniques of document engineering. This article presents this issue and proposes answers, theoretically first, with the introduction of a new concept of reading context, and in practice afterwards, with an automatic classification of tags and the presentation of a generic tool for XML content handling.
This introductory paper adresses the problem of terminology networking. The aim is to build terminologies which are not mere lists of terms but in which the terms are linked to each other. The paper gives an overview of the various forms of terminological networks.
We describe a corpus analysis method for generating an ontology in the field of e-recruitment Using statistics and n-gram analyses, we create a structured set of terms and concepts in English and French for 440 occupations in 27 fields of activity. Each occupation is linked with the necessary practice skills for around 6000 different skills. A manual evaluation of the results was performed by a domain expert and has shown excellent results.
Three automatic alignment systems are compared in their adequacy to account for the vowel schwa as compared to a manual transcription obtained from two judges. Error rates and types are analysed, as well as the linguistic factors involved. The type of surrounding consonants and the duration of schwa influence the decisions of the three systems. Moreover, the systems behave differently, depending on some of their architectural characteristics and on the task (detection vs. boundary determination). These results underline the necessity to have a fair understanding of the characteristics and bias of the alignment system to be used in a phonetic analysis.
This article describes a method for classifying dialogue utterances and detecting the interlocutor's agreement or disagreement. This labelling can help improve dialogue management by providing additional information on the utterance's content without deep parsing. The proposed technique improves upon state of the art approaches by using a Support Vector Machine cascade. A combination of three binary support vector machines in a cascade is employed to filter out utterances that are easy to classify, thus reducing the noise in the learning of labels for more ambiguous utterances. The approach achieves higher accuracy (by 2.47%) than the state of the art while using a simpler approach which relies only on shallow local features of the utterances.
Then we deal with the formal representation of calendar expressions, and we provide a functional approach, based on a categorical representation of ordinals. Finally, an ongoing application providing a help for the reading of long biography is presented.
Linguistic knowledge that is obvious to a human user must often be made explicit for automatic systems. Morphosemantic relations in the lexicon are a case in point. While English has a finite list of productive morphology rules for deriving nouns and verbs from one another, the semantic relations among derived words are not transparent to an automatic system. We discuss the addition of morphosemantic links in WordNet.
We focus on dialogue management for collaborative activities, involving multiple concurrent tasks. Conversational context for multiple concurrent activities is represented using a "Dialogue Move Tree" and an "Activity Tree" which support multiple interleaved threads of dialogue about different activities and their execution status. We also describe the incremental message selection, aggregation, and generation method employed in this context. The generation component must also be able to handle contexts where there are multiple topics being co-ordinated by the conversation. We show how to use dynamic context representations for flexible interpretation and natural generation of dialogue contributions in such applications. We demonstrate that these techniques are viable in a dialogue system for multi-modal conversations with semi-autonomous mobile robots.
In this paper, we first propose a terminology to be used in the French presentation of evaluation results of automatic summaries. In the second part of the paper, we describe experimental parameters that should be included in the presentation of evaluation results, based on the analysis of twenty-two experiments in summarization evaluation. These experimental parameters are pieces of information about the evaluation design. As well, we point out the most popular trends and methodological problems associated with these parameters and give recommendations to guide the researcher who plans on evaluating automatic summaries.
Today, the most common architecture for speech understanding consists of a combination of statistical recognition and robust semantic analysis. For many applications, however, grammar-based recognisers can offer concrete advantages. In this paper, we present a methodology and an Open Source platform (Regulus), which together permit rapid derivation of linguistically motivated recognisers, in either language, from a bilingual grammar of Catalan, French and Spanish.
The constitution of corpora is one of the first priorities faced by less-resourced languages. The emergence of Internet-based resources of increasing size and covering more and more languages may suggest that this issue has been resolved, but this is not the case. In parallel to a manual evaluation, we considered the possibility of using one or more language identification modules to filter the content of these resources, which turns out to be possible but at the cost of low recall. For this task, we tested and re-trained various systems in order to adapt them to Corsican. This work makes it possible to provide a model allowing the identification of 17 European languages as well as Corsican.
Based on an earlier evaluation of a number of well-established algorithms for the generation of referring expressions, this paper explores several problems that arise in designing evaluation for this task, and identifies general considerations that need to be met in evaluating Natural Language Generation subtasks.
This paper deals with spontaneous speech, considering first its specificities, and then its transcription both diachronically and synchronically. The paper continues by listing the main problems spontaneous speech causes to automatic speech recognition systems, which were identified through several experiments.
This paper focuses on the ASR performance prediction task. Two prediction approaches are compared: a state-of-the-art performance prediction based on engineered features and a new strategy based on learnt features using convolutional neural networks. We also try to better understand which information is captured by the deep model and its relation with different conditioning factors. To take advantage of this analysis, we then try to leverage these 3 types of information at training time through multi-task learning, which is slightly more efficient on ASR performance prediction task.
This work is part of a large research project entitled "Oreillodule" aimed at developing tools for automatic speech recognition, translation, and synthesis for Arabic language. Our attention has mainly been focused on an attempt to present the semantic analyzer developed for the automatic comprehension of the standard spontaneous arabic speech. The findings on the effectiveness of the semantic decoder are quite satisfactory.
This article introduces S XPipe 2, a modular and customizable chain aimed to apply to raw corpora a cascade of surface processing steps. Necessary preliminary step before parsing, they can be also used to prepare other tasks. Developed for French and for other languages, S XPipe 2 includes, among others, various named entities recognition modules in raw text, a sentence segmenter and tokenizer, a spelling corrector and compound words recognizer, and an original context-free patterns recognizer, used by several specialized grammars (numbers, impersonal constructions...). We describe the theoretical foundations of these modules, their implementation on French and a quantitative evaluation for some of them
The Ritel project aims at integrating spoken language dialog and open-domain information retrieval to allow a human to ask general questions and refine her search interactively. In this paper, we describe the current platform and more specifically the analysis, information extraction and natural language generation modules. We present some preliminary results of the different modules.
Predicting liaison in French is a non-trivial problem to model. We compare a memory-based machine-learning algorithm with a rule-based baseline. The memory-based learner is trained to predict whether liaison occurs between two words on the basis of lexical, orthographic, morphosyntactic, and sociolinguistic features. Best performance is obtained using only a selection of lexical and syntactic features (a window of the five last letters of a word and the five first letters of the following word, whether the liaison is obligatory or optional, Part-of-Speech tags, the number of syllables in a word and the Levenshtein distance to the 20 nearest phonological neighbors. Counter to our expectations, including sociolinguistic features even lowered the precision and recall of our predictions. Selecting only lexical and syntactic features yields a best overall performance at a precision of.80, with recall at.85. The F-scores, the harmonic mean of precision and recall, of the memory-based algorithm are higher than that of a baseline based on the rules of Grevisse and Goosse (2011), IGTree (a decision-tree learner) and the Naive Bayes classifier. Ripper, a more sophisticated rule induction algorithm, was able to produce similar results to our memory-based algorithm, but when it comes to optional liaison contexts, Ripper misses more instances in which real speakers would produce a liaison. It appears that predicting liaison benefits from being able to generalize from specific examples in context.
We present CROC (Coreference Resolution for Oral Corpus), the first machine learning system for coreference resolution in French. One specific aspect of the system is that it has been trained on data that are exclusively oral, namely ANCOR (ANaphora and Coreference in ORal corpus), the first corpus in oral French with anaphorical relations annotations. In its current state, the CROC system requires pre-annotated mentions. We detail the features that we chose to be used by the learning algorithms, and we present a set of experiments with these features. The scores we obtain are close to those of state-of-the-art systems for written English. Then we give future works on the design of an end-to-end system for oral and written French.
Inter-coders agreement measures are used to assess the reliability of annotated corpora in NLP. Now, the interpretation of these agreement measures in terms of reliability level relies on pure subjective opinions that are not supported by any experimental validation. In this paper, we present several experiments on real or simulated data that aim at providing a clear interpretation of agreement measures in terms of the level of reproductibility of the reference annotation with any other set of coders.
In contrast with other NLP subtasks, there has been only few and limited evaluation campaigns for terminology acquisition. It is nevertheless important to assess the progress made, the quality and limitations of terminological tools. This paper argues that it is possible to define evaluation protocols for tasks as complex as computational terminology. Our approach relies on the decomposition into elementary terminological subtasks and on metric definition. We take into account the specificity of computational terminology, the complexity of its outputs, the role of application and user and the absence of well-established gold standard.
The idea we defend in this paper is the possibility to obtain significant semantic concepts using clustering methods. We start by defining some semantic measures to quantify the semantic relations between words. Then, we use some clustering methods to build up concepts in an automatic way. We test two well known methods: the K-means algorithm and the Kohonen maps. Then, we propose the use of a Bayesian network conceived for clustering and called AutoClass. To group the words of the vocabulary in various classes, we test three vector representations of words. The first is a simple contextual representation. The second associates to each word a vector which represents its similarity with each word of the vocabulary. The third representation is a combination of the first and the second one.
This paper presents a model for generating French sign language gestures, which is based on both a semi-formal modelling approach, and on a specification formalism yielding to the translation of an utterance into a continuous data flow for the control of a virtual character. This approach benefits from knowledge of structural linguistics proper to sign language, and results of motion capture analysis.
This paper presents an exploration of a lexical graph, within the framework of lexical semantics. The aim of the exploration is reveal the structure of the lexicon modelled by the graph so an automatic system can reach the information it contains. These tools were developed using a graph of adjectival synonymy and its scale-free small-world structure. We now want now to use them to explore any lexical graph, and more generally any graph related to human activities.
This paper presents the CasEN transducer cascade to recognize French Named Entities and two other cascades using a CasEN annotated text. The first cascade adds speaker information into the Eslo socio-linguitic survey and the second one links named entities in Le Monde newspaper corpus. This cascade is implemented with the CasSys software of the Unitex plateform and will be put at user free disposal.
This paper investigates the relation between TT-MCTAG, and RCG. RCG is known to describe exactly the class PTIME. We show that TT-MCTAG with an additional limitation can be transformed into equivalent "simple" RCG. This result is interesting for theoretical reasons (since it shows that TT-MCTAG in this limited form is mildly context-sensitive) and also for practical reasons (the transformation has been implemented in a parser for TT-MCTAG).
This paper develops some aspects of a theory of demonstrative reference and dependence (coreference) within a framework that advocates a non directly-referential point of view on indexicality. An account of demonstratives as quantifiers subject to several restrictions is developed. These restrictions mostly set up the proper contextual perspective for the interpretation of the utterance. Evidence from Spanish is presented to support this proposal. The analysis is implemented in Discourse Representation Theory. Finally, the behavior of demonstratives in discourse is considered, as they are used in different types of dependent and non-dependent interpretations.
This article presents an attempt to apply efficient parsing methods based on recursive neural networks to languages for which very few resources are available. We propose an original approach based on multilingual word embeddings acquired from different languages so as to determine the best language combination for learning. The approach yields competitive results in contexts considered as linguistically difficult.
Search engines on the web and most existing question-answering systems provide the user with a set of hyperlinks and/or web page extracts containing answer(s) to a question. These answers are often incoherent to a certain degree (partly overlapping, contradictory, etc.). It is then quite difficult for a user to know which answer is the correct one. We present an approach which aims at providing synthetic numerical answers in a question-answering system. These answers are generated in natural language and, in a cooperative perspective, the aim is to explain and justify to the user the variation of numerical values when several values, apparently incoherent, are extracted from the web as possible answers to a question.
This paper describes the formalism of Probabilistic Generative Dependency Grammars (GDPG) as well as the results of parsing expriments with grammars that were automatically extracted from a french treebank. GDPG are different from contemporary probabilistic grammars with respect to both their algebraic and probabilistic models. The algebraic model is a generative system for dependency grammars and the probabilistic model is a Markov process which models the probability of a dependency between a governor and one of its dependent based on the other dependents of the gorvernor.
Various families of classifiers have been trained in a supervised way for 14 types of detections made by a commercial grade French grammar checker. Eight of the 14 classifiers we devised are now part of the latest edition of the grammar checker, embedded in a popular writing assistant. This project was conducted over a six-month period and is an interesting illustration of how a machine learning component can be successfully embedded in a robust, popular natural language application for commercial use.
Due to the idiosyncrasy of collations, their efficient and linguistically motived representation is a problem in both monolingual and multilingual computational lexical. Drawning on recent studies in Explanatory Combinatorial Lexicology (ECL), we observe that there is a correlation shows a correspondence across languages. We propose a structure of a multilingual collational lexicon that makes use of this observation.
In this article, we propose a method for improving distributional thesauri based on a bootstrapping mechanism: a set of positive and negative examples of semantically similar words are selected in an unsupervised way and used for training a supervised classifier. This classifier is then applied for reranking the semantic neighbors of the thesaurus used for example selection. We show how the relations between the mono-terms of similar nominal compounds can be used for performing this selection and how to associate this criterion, either by early fusion or late fusion, with an already tested criterion based on the symmetry of semantic relations. We evaluate the interest of the proposed procedure for a large set of English nouns with various frequencies. This article is an extended version of (Ferret, 2013 ; Ferret, 2015a).
This paper reports on the development of the PROIEL parallel corpus of New Testament texts, which contains the Greek original of the New Testament and its earliest Indo-European translations, into Latin, Gothic, Old Church Slavic and Classical Armenian. A web application has been constructed specifically for the purpose of annotating the texts at multiple levels: morphology, syntax, alignment at sentence, dependency graph and token level, information structure and semantics. We describe this web application and our annotation schemes. Although designed for investigating pragmatic resources, the corpus with its rich annotation is an important resource in contrastive and historical Indo-European syntax and pragmatics, easily expandable to include other old Indo-European languages.
While detecting simple language errors (e.g. misspellings, number agreement, etc.) is nowadays standard functionality in all but the simplest text-editors, other more complicated language errors might go unnoticed. A difficult case are errors that come in the disguise of a valid word that fits syntactically into the sentence. We use the Wikipedia revision history to extract a dataset with such errors in their context. We evaluate statistical and knowledge-based measures of contextual fitness on this dataset and show that the new dataset provides a more realistic picture of task performance. We make the full experimental framework publicly available which will allow other scientists to reproduce our experiments as well as to conduct follow-up experiments.
Clinical concept normalization involves linking entity mentions in clinical narratives to their corresponding concepts in standardized medical terminologies. It can be used to determine the specific meaning of a mention, facilitating effective use and exchange of clinical information, and to support semantic cross-compatibility of texts. Additionally, we train a multi-class baseline based on BERT. Our multi-pass sieve approach achieves an accuracy of 82.0% on the MCN corpus, the highest for any rule-based method.
Nonetheless, the peculiarities of this construction raise questions that challenge our understanding of the syntax of this specific construction and, more broadly, of how the core and the periphery of Universal Grammar (UG) are organized (Culicover and Jackendoff, 1999). To date, no study has investigated the comparative correlative construction in any sign language. The purpose of the present paper is to fill this gap and provide a survey of the main characteristics of comparative correlatives in Italian Sign Language (LIS).
This new model relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and for various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. Regarding the contribution of NLP to readability, our findings suggest that NLP-based models are not significantly better than the classic ones, although combining both type of information leads to significant improvment.
We present a general framework for integrating annotations from different tools and tag sets. When annotating corpora at multiple linguistic levels, annotators may use different expert tools for different phenomena or types of annotation. These tools employ different data models and accompanying approaches to visualization, and they produce different output formats. For the purposes of uniformly processing these outputs, we developed a pivot format called PAULA, along with converters to and from tool formats. Different annotations are not only integrated at the level of data format, but are also joined on the level of conceptual representation. For this purpose, we introduce OLiA, an ontology of linguistic annotations that mediates between alternative tag sets that cover the same class of linguistic phenomena. For cross-tag set querying and statistical evaluation, ANNIS uses the ontology of linguistic annotations. Finally, ANNIS is also tied to a machine learning component for semiautomatic annotation.
In this article, I describe my research on wide-coverage semantics for French-language texts and on its application to produce detailed semantic descriptions of itineraries. Using a categorial grammar semi-automatically extracted from the French Treebank and a manually constructed semantic lexicon, the resulting parser computes discourse representation structures representing the meaning of arbitrary text. The main goal of this paper is to apply and specialize this general framework of wide-coverage semantics to the spatial and temporal organisation of the Itipy corpus --- a set of 19th century texts discussing voyages through the Pyrenees mountains. The implemented system gives satisfying results and opens the door to the integration with specialized extensions, such as a separate module computating the discourse relations between the textual units.
The demand for more sophisticated natural human-computer and human-robot interactions is rapidly increasing, as users become more accustomed to conversation-like interactions with their devices. In this paper, we introduce VoxWorld, a multimodal simulation platform for modeling human-computer interactions.
Identification of cognates and recurrent sound correspondences is a component of two principal tasks of historical linguistics: demonstrating the relatedness of languages, and reconstructing the histories of language families. We propose methods for detecting and quantifying three characteristics of cognates: recurrent sound correspondences,phonetic similarity, and semantic affinity. The ultimate goal is to identify cognates and correspondences directly from lists of words representing pairs of languages that are known to be related. The proposed solutions are language independent, and are evaluated against authentic linguistic data. The results of evaluation experiments involving the Indo-European, Algonquian, and Totonac language families indicate that our methods are more accurate than comparable programs, and achieve high precision and recall on various test sets. The results also suggest that combining various types of evidence substantially increases cognate identification accuracy.
This work focuses on the concept of lexical context that is central to the historical approach of bilingual lexicon extraction from specialized comparable corpora. First, we revisit the two main strategies dedicated to lexical context characterization, that rely on the use of window-based and syntactic-based representations. We show that the combination of these two representations has a particular interest in the task of building bilingual lexicons. Second, we address the problem of the reliability of context words observations in specialized comparable corpora. To answer this difficulty, we propose to implement strategies to re-estimate words cooccurrence observations by smoothing or prediction techniques. Our results show a significant improvement in the quality of extracted lexicons.
This introduction to the special issue of the TAL journal on distributional semantics provides an overview of the current topics of this field and gives a brief summary of the contributions.
Going beyond traditional vocabulary lists based on a lexical frequency strategy, we propose a computationnal model that enables to introduce the learner into the rules of the graphic system as a whole, its phonological cues and their reliability. At the crossroad between different disciplines, our NLP approach integrates the research results from Language teaching, Psycholinguistics and Neuro-imaging
Interest in automatic closed captioning systems has risen on account of legal obligations concerning accessibility and the sheer amount of audiovisual content being produced by multiple sources. Such systems usually proceed by coupling Automatic Speech Recognition (ASR) and Machine Translation (MT) from transcript to captions. The "translation" task consist of a simplification and segmentation of the text, which must observe norms with respect to display, while handling ASR errors. In the case of TV shows, both the initial audio stream and the target captions vary significantly in form and content according to the program. Taking inspiration in MT literature, this paper implements and compare televisual genre adaptation methods for closed captioning.
Identifying proper names in text is necessary in several text processing applications, such as information retrieval, natural-language understanding or machine translation. We review recent systems developed for proper name recognition on English or French written language: their goals, their methods and their results. We show that for this particular task, a background has been built and that the methods employed have allowed the implementation of systems with high performance.
Though numerous numerical studies have investigated language change, grammaticalization and diachronic phenomena of language renewal have been left aside, or so it seems. We argue that previous models, dedicated to other purposes, make representational choices that cannot easily account for this type of phenomenon. In this paper we propose a new framework, aiming to depict linguistic renewal through numerical simulations. We illustrate it with a specific implementation which brings to light the phenomenon of semantic bleaching.
We present a system for finding, from Twitter data, events that raised the interest of users within a given time period and the important dates for each event. An event is represented by many terms whose frequency increases suddenly at one or more moments during the analysed period. In order to determine the terms (especially the hashtags) dealing with a topic, we propose methods to cluster similar terms: phonetic methods adapted to the writing mode used by users and some statistical methods. In order to select the set of events, we used three main criteria: frequency, variation and Tf*Idf.
Within the framework of Morpho Challenge 2009, we developed an unsupervised morphological analysis system based on formal analogies between words. After the evaluation campaign, we took time to correct some problems in our system and to analyze its main characteristics. This paper focusses on the knowledge we acquired during this post-mortem exercise. We hope this work will contribute to show that formal analogy is not only a viable solution, but is also a competitive one.
However, compiling historical corpora is different from compiling contemporary corpora. Corpus designers have to deal with several characteristics inherent in historical texts, such as: absence of a spelling standard, pervasive use of abbreviations plus their spelling variations, lack of space between words, irregular use of hyphenation, non-standard typographical symbols.
This paper focuses on methods applied for sign language video processing. The method uses a model of skin color and three particle filters to track the two hands and the face, with re-sampling and annealed update steps to increase their robustness to occultation and high acceleration variations of body parts that occure frequently in sign language (SL). Evaluations of the trackers show the improvement bringed by these enhancements. The second part presents SL models to process those results. A model of signing space allows us to represent the spatial structure of a signed sentence. We explain several ways which could be used to perform an automatic sign segmentation. To conclude, we expose a range of application perspectives enabled by such methods.
This paper presents the modelling of Proper Name domain defined by the Prolex project. This modelling is based on two main concepts: the Conceptual Proper Name and the Prolexeme. The Conceptual Proper Name do not represents the referent, but a point of view on this referent. It has a specific concept in each language, the Prolexeme, that is a structured family of lexemes. Around them, we have defined other concepts and relations (synonymy, meronymy, accessibility, eponymy...). Each Conceptual Proper Name is an hyponym of a type and an existence within an ontology.
We propose D-STAG, a new formalism for the automatic analysis of the discourse structure of texts. The analyses computed by D-STAG are hierarchical discourse structures annotated with discourse relations, that are compatible with discourse structures computed in SDRT. The discourse analysis extends the sententiall analysis, without modifying it, which makes conceivable the realization of a discourse analyzer.
Our paper describes the creation and evaluation of a Distributional Semantics model of ancient Greek. We developed a vector space model where every word is represented by a vector which encodes information about its linguistic context(s). We validate different vector space models by testing their output against benchmarks obtained from scholarship from the ancient world, modern lexicography, and an NLP resource. Finally, to show how the model can be applied to a research task, we provide the example of a small-scale study of semantic variation in epic formulae, recurring units with limited linguistic flexibility.
Both in the field of psychology and in natural language processing, norms related to semantic properties, such as concreteness, polarity or emotionality, are important resources. The manual construction of these norms, by asking participants to rate the words, is expensive, hence the need to develop automatic methods of construction or extension. Several methods have been proposed, but they focus on only one dimension: polarity. We propose to determine whether one of these methods can be extended to six other norms, for French and Spanish. The experiments confirm the effectiveness of the technique, not only to extend a norm, but also to highlight words for which the values that were assigned by the raters seem unreliable.
We introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems. We applied this technique to parsing results produced on several million words by two distinct parsing systems, which share the syntactic lexicon and the pre-parsing processing chain. We are thus able to identify incorrectness and incompleteness sources in the resources. In particular, by comparing both systems' results, we are able to isolate problems coming from shared resources from those coming from grammars.
Dialogue is full of intuitively complete utterances that are not sentential in their outward form, most prototypically the "short answers" used to respond to queries. As is well known, processing such non-sentential utterances (NSUs) is a difficult problem on both theoretical and computational grounds. In this paper we present a corpus-based study of NSUs. We propose a comprehensive, theoretically grounded classification of NSUs in dialogue based on a sub-portion of the British National Corpus (BNC). The study suggests that the interpretation of NSUs is amenable to resolution using a relatively intricate grammar combined with an utterance dynamics approach. That is, a strategy that keeps track of a highly structured dialogue record of entities that get introduced into context as a result of utterances. Complex, domain-based reasoning is not, on the whole, very much in evidence.
Structured prediction lies at the heart of modern Natural language Processing (NLP). We also present an overview of existing applications of these techniques to NLP problems and discuss their potential benefits.
Morphological analysis can provide intelligent assistance to beginning students of French. It can be used to provide information about the grammatical meaning of morphology, to enable access to online dictionaries, and to allow users to compare other occurrences of unfamiliar words. This paper reports on a fully implemented system whose prototype has proven successful in an initial user study.
We present a medical question answering approach, called MEANS. This approach relies on natural language processing techniques to extract medical entities and relations from the user questions and medical corpora. It also uses semantic Web languages to represent and query the information searched by the users. This feature allows to share the information extracted from textual corpora using standard languages and to consider incremental knowledge acquisition in mid-and-long terms. MEANS constructs an initial SPARQL query and several relaxed queries as semantic interpretations of a user question. The evaluation of MEANS on a real dataset shows promising results for both precision and MRR and showed the significant benefits of the query relaxation technique.
The continuous improvement of the quality of machine translation and of speech recognition systems opens new perspectives for the development of spoken translation applications. In this study, based on our own experience with the development of Spoken Translation Systems (STS) for conferences, we analyze and quantify the main difficulties raised by STSs, and discuss possible strategies to migitate these issues.
The growth of the Arabic textual content on social media platforms has been caused by the continuous crises in the Arab World evoking the need to analyze the opinions of the public against the ongoing events. Arabic Sentiment Analysis (ASA) is, therefore, becoming the focus of many recent NLP studies. With several Arabic NLP resources being publicly available along with the emergence of deep learning techniques, researchers could handle the complex nature of Arabic language more efficiently. In the last decade, various ASA systems have been built. Yet, their achievements have not been investigated or compared against each other. This survey covers the ASA research carried out during the past five years. We compare and evaluate the performances and give insight into the ability of the created Arabic resources to support the future ASA research.
Some specialized domains, such as medicine, make extensive use of proper names in their terminology. The present work studies criteria which can help to classify words as proper names in biomedical terminologies. Useful characteristics include word capitalization, the presence of words in several translations and patterns in terms. Since no individual criterion achieves both high precision and high recall, we propose a combination of five criteria that allowed us to reach 86% precision and recall on one terminology and 98% on another one. Strategic implications are discussed.
This approach provides fine grained annotation of NE and is also a first step of named entity disambiguation.
The dictionary lookup of unknown words is particularly difficult in Japanese due to the requirement of knowing the correct word reading. We propose a system which supplements partial knowledge of word readings by allowing learners of Japanese to look up words according to their expected, but not necessarily correct, reading. This is an improvement from previous systems which provide no handling of incorrect readings. In preprocessing, we calculate the possible readings each kanji character can take and different types of phonological alternations and reading errors that can occur, and associate a probability with each. Using these probabilities and corpus-based frequencies we calculate a plausibility measure for each generated reading given a dictionary entry, based on the naive Bayes model. In response to a user-entered reading, the system displays a list of candidate dictionary entries for the user to choose from. The system is implemented in a web-based environment and available for general use. In the evaluation on Japanese Proficiency Test data and naturally occurring misreading data, the system significantly reduced the number of unsuccessful dictionary queries when queried with incorrect readings.
In this paper we present SIMDIAL, a paradigm to assess Spoken Language Dialogue Systems (SLDSs) using deterministic user simulation. The paradigm allows automatic or semi-automatic evaluation on different tasks and uses natural langage to interact with evaluated SLDSs.
We present a methodology for constructing automatically verifiable and human-readable interlinguas, which simultaneously combine expressivity and simplicity. By defining the interlingua using the same formalism as the one used to define the analysis and generation grammars, it is feasible, in a natural way, both (1) to generate a human-readable gloss for an interlingua representation, (2) to determine whether said representation is well-formed. As well as serving as a pivot language for carrying out translation, we show how the interlingua can be used for two other important purposes: improving the quality of recognition, by filtering out hypotheses which are not semantically well-formed, and simplifying the development cycle, which thanks to the gloss form can be transformed into a disjoint set of monolingual tasks.
The understanding of language mechanisms needs to take into account very precisely the interaction between all the different domains or modalities, which implies the constitution and the development of resources. We describe here the CID (Corpus of Interactional Data), an audio-video corpus in French recorded and processed at the Laboratoire Parole et Langage (LPL). The corpus has been annotated in a multimodal perspective including phonetics, prosody, morphology, syntax, discourse and gesture studies. The first results of our studies on the CID lead to confirm the relevance of an analysis which takes into account as many linguistic fields as possible to draw up a more precise knowledge of discourse phenomena.
We present the model of language implemented in the words prediction engine of the "Plateforme de Communication Aternative" (PCA), an assistive communication software designed for impaired persons. The model relies on a large coverage lexicon for french langage which provides for each entry its word frequency and its set of associated morphosyntactic categories. The engine includes a user model (personal lexicon for unknown words typed by the user, computation of personal word frequencies, storage of the generated sentences) and a morphosyntactic prediction system which weight the word frequencies of predicted words according to the syntactical context of the sentence. The evaluation of the model of language is quite encouraging: a keystrokes saving rate of 55 % for 9 propositions. The major contribution comes from the raw prediction obtained by using the words frequencies of the general lexicon.
Detecting aspectual properties of clauses in the form of semantic clause types has been shown to depend on a combination of syntactic-semantic and contextual features. We explore this task in a deep-learning framework, where tuned word representations capture linguistic features. We introduce an attention mechanism that pinpoints relevant context information. Our model implicitly captures task-relevant features and avoids the need to reproduce explicit linguistic features for other languages. We present experiments for English and German that achieve competitive performance, and analyze the outputs of our systems from a linguistic point of view. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another.
We present a framework and its implementation relying on Natural Language Processing methods, which aims at the identification of exercise item candidates from corpora. The hybrid system combining heuristics and machine learning methods includes a number of relevant selection criteria. We focus on two fundamental aspects: linguistic complexity and the dependence of the extracted sentences on their original context. In addition to a detailed description of the system, we present the results of an empirical evaluation conducted with language teachers and learners which indicate the usefulness of the system for educational purposes.
We report results of an effort to enable computers to segment US adjudicatory decisions into sentences. We created a data set of 80 court decisions from four different domains. We show that legal decisions are more challenging for existing sentence boundary detection systems than for non-legal texts. Existing sentence boundary detection systems are based on a number of assumptions that do not hold for legal texts, hence their performance is impaired. We show that a general statistical sequence labeling model is capable of learning the definition more efficiently. We have trained a number of conditional random fields models that outperform the traditional sentence boundary detection systems when applied to adjudicatory decisions.
The article introduces a transition-based constituent parsing method relying on a deep-learning weighting scheme. This weighting method is compared to a structured perceptron, which is a more traditional method. First of all, we introduce a syntactic parser weighted by a local greedy neural network based on symbol embeddings. Then, we extend this model to a global beam-search based model. Our experiments highlight the surprisingly good properties of the greedy local neural parser.
We show how terminological resources can be prototyped rapidly. The context of the experiment is the www.droit.org website, which publishes the 'Laws and Decrees' edition (95,000 documents) of the bulletin issued by the French Republic giving details of laws and official announcements. SYNTEX, a corpus parser, processes a reference corpus made up of 12 crucial codes within French Law. Syntex extracts a network of 130,000 nouns and phrases structured through syntactic dependency. UPERY, the distributional analysis module, adds neighbouring links between nouns and phrases appearing in the same syntactic contexts (simple or complex contexts). The output ontology is integrated into the website interface and an evaluation protocol is set up to validate the first version of the ontology.
We give particular attention to modelling question-answering sequences, including clarification requests, and argue that metavariables, arising from unification in the proof search, play a decisive role in providing a natural formalisation. We show that our framework is not only well suited from a theoretical perspective, but it is also suitable for implementation which we exemplify with a small scale implementation.
The TETRIS project at the Institut für Angewandte Informationsforschung is a MULTILINT follow-up project in collaboration with BMW and a number of smaller German companies. TETRIS provides linguistically intelligent components for specification, production and maintenance of multilingual documents supporting authors for technical documentation and manuals as well as terminologists. The paper discusses the technical realization and implementation in detail.
In this work, we focus on the problems of the automatic treatment of oral spoken in the Tunisian media. This oral is marked by the use of code-switching between the Modern Standard Arabic (MSA) and the Tunisian dialect (TD). Our goal is to build useful resources to learn language models that can be used in automatic speech recognition applications. As it is a variant of MSA, we describe in this paper an adjustment process of the MSA resources to the TD. A first evaluation in terms of lexical coverage and perplexity is presented.
This article proposes an approach to recommendation oriented analysis queries. In particular, we focus on queries where the user is looking for similarities between books, authors or collections. Our approach is, firstly, to identify these requests through an automatic supervised classification method. Secondly, a dependency analyzer is used to identify the syntactic and semantic links present in this type of requests. Once these dependencies extracted several expansion strategies of these requests are put in place to exploit them as inputs an index and then, by a model DFR: InL2. Our experiments show that these achievements are a further step towards the understanding of user requirements expressed in the lengthy and detailed queries.
In this paper, we present a summarization system that is specifically designed to process blog posts, where factual information is mixed with opinions on the discussed facts. Our approach combines redundancy analysis with new information tracking and is enriched by a module that computes the polarity of textual fragments in order to summarize blog posts more efficiently. The system is evaluated against English data, especially through the participation in TAC (Text Analysis Conference), an international evaluation framework for automatic summarization, in which our system obtained interesting results.
A significant trend of research reduces dialogue modelling to the modelling of the intentional structure of the dialogue participants. This paper aims at offering a survey of this approach, reviewing its evolution, and introducing the different models that have emerged from it. Its philosophical and linguistic roots are also emphasised. We then critically discuss this approach, and conclude by proposing some possible improvements to the intentional models.
We investigated the efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing. We first tested the beam thresholding and iterative parsing. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The quick check, iterative parsing and hybrid parsing greatly contributed to total parsing performance. Finally, we tested robustness and scalability of HPSG parsing on the MEDLINE corpus consisting of around 1.4 billion words. The entire corpus was parsed in 9 days with 340 CPUs.
We present an original and ﬂexible implementation of the edit-distance: the filtered composition, a special kind of composition of two finite-state machines through a filter that models all valid edit-operations. The filter is either a weighted transducer or a cascade of weighted transducers. It is built from weighted rewrite rules that take advantage of a new concept defined in our finite-state framework: the rules' marker, a symbol that does not belong to the alphabet in use, but is inserted into a rewrite rule in order to mark a phenomenon and to track its evolution. Markers disambiguate and make it easy to express conditions and constraints. The method is illustrated on the task of correcting out-of-vocabulary words.
On the other hand, the paper proves that these taggers can be used as true heuristic instruments and can help to improve significantly the description of the corpus.
This work was carried out on a corpus of newswire texts in English provided by the Agence France Presse (AFP). For the time being, we have focused only on extracting the dates and not the events to which they are related.
An understanding of spatial information in natural language is necessary for many computational linguistics and artificial intelligence applications. In this paper, we discuss what problems researchers face with respect to this topic, focusing on the need for a well-developed annotation scheme. The desiderata for such a specification language are defined along with what representational mechanisms are required for the specification to be successful. We then review several spatial information annotation schemes, focusing on the latest version of the ISO-Space specification. Finally, we discuss where ISO-Space still falls short and propose some ways that the specification could be enriched.
Cuxac's Model predicts that each sign of the LSF lexicon is a combination of morphemic rather than phonemic values. This paper exposes the theoretical principles and conditions of falsifiability we propose to constitute an inventory of morphemic values, their assignation to the parametric components and, in fine, to establish which role is played by the different kinds of constraints exerted on lexical signs. These results corroborate the hypothesis of morphemic compositionality but they call into question the nature and status of current parameters and those of their mutual interdependencies.
The notion of connection is defined as an equivalence class of combinations, which allows us to introduce a notion of syntactic structure that cut itself off from the notion of syntactic unit. Consequences of that on the conception of the syntactic structure are investigated.
As text/discourse linguistics moves toward corpus approaches, also in connection with the development of large text bases and of computational instruments, we explore potential new forms of convergence.
In this paper, we present a machine learning system for identifying non-referential it. Types of non-referential it are examined to determine relevant linguistic patterns.
The analysis of Ovid's Metamorphoses tenth book (myth of Orpheus and Eurydice) leads us to examine the treatment of texts that are closely related in terms of content, but not on the level of structure, vocabulary and state of language. Our objective is to prove that, after some enrichements to the corpus and by adjusting the tools to cooperate, it is possible to create a modular and readable visualization adapted to all readers. By suggesting evolutionary paths for both softwares, we try to engage a close dialogue between computer scientists who develop the tools and literary researchers who exploit them.
An easy way out commercial translation systems usually offer their users is the possibility to add unknown words and their translations into a dedicated lexicon. Recently, (Stroppa et Yvon, 2005) shown how analogical learning alone deals nicely with morphology in different languages. In this study we show that analogical learning offers as well an elegant and efficient solution to the problem of identifying potential translations of unknown words.
Current electronic dictionaries allow us to have access to the entries and their information efficiently by means of sophisticated search functions. Here lies their main added value. Nevertheless their interest fades once the user reaches the entry, for he finds the paper-based version presentation and its limits again. In this paper, by considering the DAFLES, a new learner's French dictionary, we show how the entry may be reorganized in a more dynamic way, more adapted to the user's needs. This is made possible by a flexible and coherent structuring of the lexical data.
Stylometric authorship attribution is a fundamental problem. The basic idea behind the research is that one can determine the authorship of a document on the basis of cognitive and linguistic quirks that uniquely identify a person. In many cases, however, noise in the original documents can make this analysis more difficult and less reliable. We investigate the errors introduced by a typical optical character recognition (OCR) process. Using simulated (random) errors in a standard benchmark corpus, we test to see how sensitive the authorship attribution process is to character mis-recognition. Our results indicate that, while accuracy decreases measurably with noise, the decrease is not substantial.
This paper considers natural language dialogue from the analysis point of view, by combining a formal semantics approach to a rhetorical analysis of discourse. Dynamic formal semantics allows us to study utterances in context. We can thus deal with problems also found in monologues such as anaphoric references. Moreover it helps explain how common references are agreed on during a conversation. Semantic representation of dialogue turns can then feed pragmatic inference processes that lead to the building of complex rhetorical structures for the whole dialogue. In the dialogue's case, this structure helps refining interpretation mechanisms of e.g. conventional phenomena. We have focused on a task-oriented kind of dialogue (namely, route explanations) so that the semantic domain can be properly constrained and we could focus on the processes of information exchange.
Finally, we present an implementation of this system called WISIGOTH.
In this paper, we describe a Frame Semantics based computational approach for Arabic language processing. We explore the representational of Frame Semantics approach to Arabic text semantics, the adaptability of Berkeley FrameNet database and the transferability of FrameNet tools for Arabic, a language that differ typologically from English. We describe our attempts to build an equivalent Arabic FrameNet and the use of such a semantic resource for Arabic text semantic analysis, representation and annotation. Here we present a frame based contrastive study of motion events expression in bilingual text (English-Arabic) using our FrameNet based tool for semantic annotation. Our study results confirm the cross-linguistic nature of Frame Semantics approach and the suitability of the theory for Arabic processing. The current work is based on an analysis of a representative corpus of motion events expressions.
Typed lexical relations between terms are indispensable for the tasks realized in NLP, but collecting lexical information is a difficult process. The approach we present here consists in having people take part in a collective project by offering them a playful application accessible on the web. From an already existing base of terms, the players themselves thus build the lexical network, by supplying associations which are validated only by an agreeing pair of users. Furthermore, these typed relations are weighted according to the number of pairs of users who provide them. We then approach the question of word usage determination for a term, by searching relations between this term and its neighbours in the network, before introducing the notion of similarity between these different word usages. We are thus able to build the tree of word usages for a term. Finally, we briefly present the realization and the first obtained results.
This paper presents some advantages of the conceptual vector model for lexical semantics. Besides being a robust representation, this model allows the emergence of relations between terms, as relative synonymy and antonymy. We describe the underlying model, as well as the basic functions which allow to define the notion of thematic proximity. The extension of the indexation mechanism of a term extracted from a document of some speciality domain is mainly based on the notion of vector folding and unfolding between vector spaces. Any term defined thanks to other terms which may not belong to the specialty terminology, will impose the union of the vector space generative families, and lead to vector unfoldings toward a larger base. We show how the lexical distribution, the antonymy, and the synonymy, play as structure spotlight and transversally brigde terms across extended vector spaces. Some experiments focus us on the interest of this kind of approach for terminology.
Automatic speech processing methods and tools can contribute to shedding light on many issues relating to phonemic variability in speech. The processing of huge amounts of speech thus allows to extract main tendencies, for which detailed interpretations then require both linguistic and methodological insights. The experimental study focuses on the variability of French oral vowels in the PFC and ESTER corpora, which are widely used both by linguists and researchers in automatic speech processing. Duration and formant measures allow to illustrate global variations depending on different parameters, which include speech style, syllable position and the speakers' regional origins. The last part addresses the phonetic realization of close-mid front vowels, using automatic classification in a Bayesian framework.
We present a work in the framewok of machine translation from French to French Sign Language (FSL) with synthesis of gestures through a virtual agent. We first give some descriptive and theoretical elements about the FSL. Then we propose a formalization of the standard part of the FSL lexicon as well as propositions concerning some morpho-syntactic phenomena. Then we present the machine translation system based on interlingua representation system "TiLT" developed at France Telecom R&D and the adaptation of its generation module to FSL. We end with the avatar technology also developed at France Telecom R&D and its FSL treatment.
We present an overview of the Index Thomisticus Treebank project (IT-TB). The IT-TB consists of around 60,000 tokens from the Index Thomisticus by Roberto Busa SJ, an 11-million-token Latin corpus of the texts by Thomas Aquinas. We briefly describe the annotation guidelines, shared with the Latin Dependency Treebank (LDT). The application of data-driven dependency parsers on IT-TB and LDT data is reported on. We present training and parsing results on several datasets and provide evaluation of learning algorithms and techniques. Furthermore, we introduce the IT-TB valency lexicon extracted from the treebank. We report on quantitative data of the lexicon and provide some statistical measures on subcategorisation structures.
This article presents the building and putting online OF the oral corpus ESLO. Our purpose is to show that it is important not only to collect and make available language data and metadata but also to make explicit the whole chain of treatments. In the first part, we will present the project and the corpus, then we will specify the legal and methodological problems which determined all corpus treatments, in particular the anonymisation procedures which are required to freely make available this kind of resource. In the second part, we present different annotations made on the raw data with some examples of their use. We will explain the followed methodology which is always guided by the nature of the data and by the final objective: build a large sociolinguistic variationist oral corpus of French. Finally, we will discuss the issues of putting the corpus online.
In this article, we present and evaluate an approach to the combination of a grammar-driven and a data-driven parser which exploits machine learning for the acquisition of syntactic analyses guided by both parsers. We show how conversion of LFG output to dependency representation allows for a technique of parser stacking, whereby the output of the grammar-driven parser supplies features for a data-driven dependency parser. We evaluate on English and German and show significant improvements in overall parse results stemming from the proposed dependency structure as well as other linguistic features derived from the grammars. Finally, we perform an application-oriented evaluation and explore the use of the stacked parsers as the basis for the projection of dependency annotation to a new language.
This paper addresses semantic ellipsis control in language generation for human-computer dialogue. For this, the rhetorical structure of the dialogue is used, as well as a repository of facts that are accepted by the recipient of the utterance. Starting from these principles, a set of algorithms are proposed. Then, this method is extended to multi-party dialogue and illustrated through several examples.
In this paper, we have considered the problem of partitioning a text's referring expressions into mutually-exclusive coreference chains. In the present work, we have emphasized the automatic identification of chains headed by a proper noun, using a simple algorithm (i.e. which does not require a complete syntactic parse or complete POS tagging) partially adapted to the type of text and domain by giving a special role to a small set of domain nouns. To design this algorithm, we have compared three types of texts from vastly differing domains and we have used the XML markup language to represent the relevant metadata. We have also had to consider a few subproblems, such as the automatic identification of noun phrases (i.e. finding the left and right boundaries) and the characterisation of the "important" coreference chains.
Various paradigms of text analysis systems followed one another over the last twenty years. In the eighties, text understanding systems aimed at a deep and exhaustive analysis of texts, while Information Extraction systems of the nineties are based on local analyses. This paper contrasts these different approaches. It analyses the return to global approaches (by symbolic or statistical machine learning-based systems especially) in systems based on a rather local analysis. The genericity and the adaptability of the systems are discussed and illustrated through application examples.
Unsupervised and semi-supervised learning of morphology provide practical solutions for processing morphologically rich languages with less human labor than the traditional rule-based analyzers. Direct evaluation of the learning methods using linguistic reference analyses is important for their development, as evaluation through the final applications is often time consuming. However, even linguistic evaluation is not straightforward for full morphological analysis, because the morpheme labels generated by the learning method can be arbitrary. We review the previous evaluation methods for the learning tasks and propose new variations. In order to compare the methods, we perform an extensive meta-evaluation using the large collection of results from the Morpho Challenge competitions.
We present a probabilistic method aimed at extracting opinion-related strings from corpora labeled according to customer mind. These strings first allow us to improve text categorization systems according to opinions (positive, negative or neutral). Second, we use them to display easily what are the frequent comments made by customers about products or services. We test the method on two critical corpora written by internet users about video games and movies (respectively in French language and in English language) and on a customer satisfaction phone survey. For each of them, we present some examples of extracted word chains and the observed improvement obtained for opinion-oriented text categorization task.
In this article, we present an information filtering method that selects from a set of documents their most significant excerpts in relation to an user profile. This method relies on both structured profiles and a topical analysis of documents. The topical analysis is also used for expanding a profile in relation to a particular document by selecting the terms of the document that are closely linked to those of the profile. This expansion is a way for selecting in a more reliable way excerpts that are linked to profiles but also for selecting excerpts that may bring new and interesting information about their topics. This method was implemented by the REDUIT system, which was successfully evaluated for document filtering and passage extraction.
These discriminative models integrate features dedicated to compounds, some of theme being computed from external lexical resources. We show that the pregrouping strategy largely overtakes current state-of-the-art results, while the reranking strategy is somewhat deceiving. All the experiments conducted in this paper open new interesting research directions.
We present in this paper a method to automatically acquire a syntactic lexicon of subcategorization frames for French verbs directly from large corpora. The method is evaluated against existing lexical resources: we show that our system is capable of producing new frames that were not previously registered. Lastly, we show that it is possible to induce lexico-semantic classes «à la Levin» (1993) from these data.
In event-based Information Extraction systems, a major task is the filling from a text of a template gathering information related to a particular event. Such template filling may be a hard task when the information is scattered throughout the text and mixed with similar pieces of information relative to different events. We propose in this paper a two-step approach for template filling: first, an event-based segmentation is performed to select the parts of the text related to the target event; then, a graph-based method is applied to choose the most relevant entities in these parts for characterizing the event. An evaluation of this model based on an annotated corpus for earthquake events shows a 77% F1-measure for the template-filling task.
Recurrent Neural Networks have proved effective on several NLP tasks. Despite such great success, their ability to model sequence labeling is still limited. This lead research toward solutions where RNNs are combined with models successfully employed in this context, like CRFs. In this work we propose a simpler solution: an evolution of the Jordan RNN, where labels are reinjected as input into the network and converted into embeddings, the same way as words. We compare this variant to all the other RNN models, Elman, Jordan, LSTM and GRU, on two tasks of Spoken Language Understanding. Results show that the new variant, which is more complex than Elman and Jordan RNNs, but far less than LSTM and GRU, is more effective than other RNNs and outperforms sophisticated CRF models
Bilingual dictionaries hold great potential as a source of lexical resources for training and testing automated systems for optical character recognition, machine translation, and cross-language information retrieval. In this paper, we describe a system for extracting term lexicons from printed bilingual dictionaries. Our work was divided into three phases - dictionary segmentation, entry tagging, and generation. In segmentation, pages are divided into logical entries based on structural features learned from selected examples. The extracted entries are associated with functional labels and passed to a tagging module which associates linguistic labels with each word or phrase in the entry. The output of the system is a structure that represents the entries from the dictionary. We have used this approach to parse a variety of dictionaries with both Latin and non-Latin alphabets, and demonstrate the results of term lexicon generation for retrieval from a collection of French news stories using English queries.
QRISTAL is a question answering system which makes intensive use of natural language processing techniques, for indexing documents as well as for extracting answers. This system recently ranked first in the EQueR evaluation exercise (Evalda, Technolangue2) and also first in the CLEF 2005 and 2006 campaigns, for monolingual french track and for multilingual track. After a functional description of the system and a complete analysis of the process for one question, its results in these evaluations are detailed. These results and some additional tests allow to evaluate the contribution of each NLP component. The feedback of the first QRISTAL users encourage further thoughts about the ergonomics and the constraints of question answering systems, faced with the web search engines.
The understanding and summarization of narratives remains a challenge, in part because of the inability to adequately capture the "aboutness" of information content. This can result in inappropriate content selection as well as summary incoherence. The paper sketches a general framework for narrative summarization that relies in part on exploitation of temporal information. The framework distinguishes three levels of narrative: scene, story, and plot. The paper concludes with a discussion of the many challenges in the area.
Artificial intelligence (AI) has evolved in recent years along with societal concerns. Various committees were introduced in order to brainstorm on the consequences of these developments. These authorities are also concerned by Natural Language Processing (NLP), not only as a subfield of AI but also as a specific field with which it interacts. In this article we review the links between AI and NLP but also where they differ. We focus on ethical clues for both of them. Finally we argue for not using ethics as a unique solution, but rather as the way to abstract over our researches, and we go back on how to interpret machine learning methods in the context of NLP.
Research on bilingual lexicon extraction from comparable corpora leads to promising results using large corpora (hundreds of billions of words) using the direct alignment method. However, when using smaller corpora (hundreds of thousands of words), results obtained are slightly lower. We propose to introduce some anchor points on which we can rely for the alignment process using the direct approach on small corpora and show how they influence the translation candidates.
This paper presents the KeyGlass system: a text entry system with dynamic addition of characters based on those previously entered. The prediction system that we use to optimizeour system is based on the joint use of a lexicographic tree and a system using bigrams. We present in this article the different steps that led us to this prediction system. Finally we study, through two experiments (one theoretical and the other one with users), the usefulness and effectiveness of our system during a task of text copy. The results show a significant reduction in the distance covered by the pointer on the soft keyboard. However, users are slower to enter text.
This paper presents an investigation on using four different types of contextual information for improving the accuracy of automatic correction of such errors. The task is framed as contextually-informed re-ranking of correction candidates. Immediate local context, as captured by word n-grams, is utilized with statistics from a web-scale language model. The second approach measures how well a candidate correction fits in the semantic fabric of the local lexical neighborhood, using a very large Distributional Semantic Model. The fourth approach looks at context beyond the text itself. If the approximate topic can be known in advance, spelling correction can be biased towards the topic. The effectiveness of proposed methods is demonstrated with an annotated corpus of 3000 student essays from international high-stakes English language assessments. The paper also provides a description of an implemented system that achieves high accuracy on this task.
From corpus extracts, we show that modeling the interpretation of referring expressions (eventually with designation gestures) only from linguistic constraints is not sufficient for the dialogue with visual aids. A fine exploitation of these constraints must go through a representation of the contexts implying the visual perception, task and user memory working principles. We focus here on the mechanisms of the construction and the exploitation of these heterogeneous contexts in an unified framework. We therefore propose a model for reference interpretation which is generic enough to be applicable to different types of interactions and different types of tasks.
Like other specialized domains, the medical area conveys very specific terms (eg, blepharospasm, alexitymia, appendicectomy), which are difficult to understand by people without medical training. We propose an automatic method for the acquisition of paraphrases, which we expect to be easier to understand than the original terms. The method is based on the morphological analysis of terms, syntactic analysis of texts, and text mining of non specialized texts. An analysis of the results and their evaluation indicate that such paraphrases can indeed be found and show easier understanding level. According to the setting of the method, precision of the extractions ranges between 90 and 7.4%. Such resources are useful for several NLP applications (eg, information retrieval, text simplification, QA systems).
Citizen science, in particular voluntary crowdsourcing, is still little experimented solution to produce language resources for less-resourced languages with enough connected speakers. We present here experiments we led on part-of-speech annotation for non standardized languages, namely Alsatian and Guadeloupean Creole. We detail the methodology we used and show that it is adaptable to other languages, then we present the results we obtained. An analysis of the limits of this platform led us to develop a new one, that allows the creation of raw corpora and part-of-speech annotations, and the construction of a multivariant lexicon. The created platforms, language resources and tagging models are all freely available.
Metaphors are often perceived as being essential to the diachronic study of ideas formulated in scientific texts pertaining to social sciences. However, very few research endeavours have tried to apply any existing NLP metaphor detection method to such texts. The present article describes an attempt to identify conceptual metaphors in geography texts written in English and in French using an LDA-based method (Heintz et al., 2013).
This article presents the work of Gabriel G. Bès, which extends from the 60s to the 2000s, from structuralism to industrial language analysis. The work of Gabriel G. Bès is marked throughout by the epistemological question and the will to put into practice linguistics as an empirical science. The central problems are the explicitation of the observations of reality, the formalisation of hypotheses, and the rigorous testing of the adequation of hypotheses with respect to observations.
This article focuses on the field of Semantic Information Retrieval (IR), which is at the intersection of several disciplines: Information Retrieval, Knowledge Engineering and Natural Language Processing. We detail the role of the documents' annotation phase using a resource and the processing of these annotations in the indexing space. We detail the three main types of information retrieval models and how the new indexing space is exploited.
Parallel corpora are the bread and butter of a number of machine translation technologies. This task often involves a rather cumbersome manual inspection and it is rather difficult to set up a strategy that fits all the needs. A classifier trained to recognize parallel text coupled to an information retrieval engine controlling the search space of candidate pairs are the main components of our approach. We tested PARADOCS on a number of tasks involving numerous pairs of languages and report good results.
Robust statistical NLP tools, such as pos-taggers and syntax parsers, often use "knowledge-poor" features, which can easily be applied to any language, but which do not take into account language specifics. In this study, we attempt to improve analysis for two French phenomena by injecting richer knowledge: pos-tagging of the function word "que" and syntax parsing of coordinated structures. We compare several techniques: automatic transformation of the corpus to other annotation standards prior to training, addition of rich targeted features during training, and addition of symbolic rules during analysis. We attain 55% error reduction for the pos-tagging of "que", and 37% for coordinated structure parsing.
This paper presents an unsupervised approach for natural language generation within the framework of question-answering systems. This approach shows promising results for English and for French.
We argue that the traditional machine translation (MT) lexicon architecture, while arguably adequate for unrelated languages, makes it impossible to capture important generalisations about related languages. These generalisations, if captured, can help to produce more robust multilingual natural language processing (NLP) systems for such languages. In this paper, we describe the hierarchical inheritance architecture for multiingual lexicons for related languages originally proposed in Cahill and Gazdar (1995) and subsequently used as the basis for developing a trilingual lexical of Dutch, English and German.
Current advances in NLP show the need for assembling into complex processing streams, sets of treatments involving a variety of lingustic objects and analysis methods. To achieve this goal, the Linguastream platform offers both a software architecture and a large set of tools: generic analysis models (grammar-based analysers, transducers, projection of lexicons...), bridges toward external modules (taggers, syntactic analysers...), lexicometric procedures, devices for visualising text annotations, etc. The design of LinguaStream, and its use in different projects, rely on a set of methodological principles discussed in this paper. Then, three applications exploiting the platform and this methodology are shortly presented: a lingusitic analyser of discourse frames, a geographical search engine, and a thematic segmenter using text mining techniques.
In this article, we present the notions of local and global algorithms, for the word sense disambiguation of texts. A local algorithm allows to calculate the semantic similarity between two lexical objects. Global algorithms propagate local measures at the upper level. We use this notion to compare an ant colony algorithm to other methods from the state of the art: a genetic algorithm and simulated annealing. Through their evaluation on a reference corpus, we show that the run-time efficiency of the ant colony algorithm makes the automated estimation of parameters possible and in turn the improvement of the quality results as well. Last, we study several late classifier fusion strategies over the results to improve the performance.
This study belongs to the field of automatic scoring of texts written in a foreign language. It aims to evaluate in depth the utility to take into account automatic measures of the phraseological competence. The phraseological features were obtained by assigning to each bigram in a text several association scores computed on the basis of a large native corpus. The analyzes performed on three datasets indicate that the phraseological features are useful for estimating text quality and that they are complementary to simpler lexical measures. External validation, carried out by learning and testing the predictive model on very different sets of texts, shows that these indices have a high degree of generalizability.
A common problem in Question Answering - and Information Retrieval in general - is information overload, i.e. an excessive amount of data from which to search for relevant information. This results in the risk of high recall but low precision of the information returned to the user. In turn, this affects the relevance of answers with respect to the users' needs, as queries can be ambiguous and even answers extracted from documents with relevant content may be ill-received by users if they are too difficult (or simple) for them. We address the issue by integrating a User Modelling component to personalize the results of a Web-based opendomain Question Answering system based on the user's reading level and interests.
One distinctive aspect of our contribution lies in its connection with the field of corpus phonology, especially oral L2 learners' corpora. In conclusion, we call for more interactions between speech engineers and L2 education specialists to promote the use of such systems in L2 curricula.
This research focuses on the identification and characterisation of accents in French. For both foreign and regional accents, we started with perceptual identification experiments, we measured phonetic features which may characterise these accents using automatic phoneme alignment, and we ranked the most discriminating features by using classification techniques. The following features are perceivable and happen to be quite robust for automatic identification: devoicing of voiced stop consonants, shift of /e/ toward [i], /b/~/v/ and /s/~/z/ confusions, "rolled r", schwa fronting or raising for German, English, Arabic, Spanish, Italian and Portuguese accents in French; /O/ fronting in the North of France, schwa pronunciation and denasalised nasal vowels in the South, for regional French accents.
From our participation at EQueR, we also studies if our system can predict its ability to answer to questions. With a machine learning algorithm using questions features, we show that we can predict with 70 % of certainty if the system will be able to answer.
This paper presents (i) a classification of Bulgarian proper nouns, (ii) a methodology for automatic morphological analysis and generation of proper nouns, (iii) some approaches to automatically build a dictionary of proper nouns. Bulgarian proper nouns are divided into classes. Every class comprises rules for generation of the paradigm. The pattern is a lexical representation, which matches all forms of the paradigm. The morphological analysis is based on the pattern matching process between the proper noun and the pattern. The pattern and the class incorporate information about the whole paradigm of a particular proper noun. An electronic dictionary of proper nouns has been created. It consists of pairs <pattern, class number>.
After a brief review of history and terminology, we will address the topic of a gold standard for natural language processing, of annotation quality, of the amount of data, of the difference between technology evaluation and usage evaluation, of dialog systems, and of standards, before concluding with a short discussion of the articles in this special issue and some prospective remarks.
We present a model of coordination within Interaction Grammars. The notion of polarity lets us define the interface of a syntactic structure as its capacity to combine with other syntactic structures in terms of requirements and resources. The coordination of two conjuncts is then viewed as the superposition of their syntactic structure interfaces. Those structures are represented by means of tree descriptions to integrate under specification so they combine in a very flexible way.
The resource presented in this paper joints together a semantically and syntactically annotated corpus of deverbal nouns based on the French Treebank, and an electronic lexicon, providing descriptions of morphological, syntactic and semantic properties of the deverbal nouns found in our corpus.
We will show that depending on the type of proper noun considered, whether mono- or polylexical, there are regularities allowing us to determine the adequate translation procedure. These instructions are intended to be used for processing a corpus, aiming at creating an electronic German-French dictionary of proper nouns.
This corpus also allows us to investigate the information flow in a encoder-decoder architecture and to identify how gender information can be transfered between languages. Considering both probing as well as interventions on the internal representations of the MT system, we show that gender information is encoded in all token representations built by the encoder and the decoder and that there are multiple paths to transfer gender.
Using distributional analysis methods to compute semantic proximity links between words has become commonplace in NLP. This paper focuses on the issues of evaluating a distributional resource. We consider that setting up an evaluation procedure is a first step towards the characterization of the resource, and a way to improve its overall quality. We then propose a new protocol for in-text annotation of distributional neighbors, which is used to build a reliable reference data set. The data generated are analyzed and used to guide the automatic categorization of distributional links.
Discriminative probabilistic models are able to cope with enriched linguistic representations, typically in the form of extremely large feature vectors. Working in high dimensional spaces is however problematic, and these problems are compounded in the case of structured output models, such as conditional random fields (CRF). In this context, feature selection techniques help building more compact and efficient models. In this work, we propose a novel estimation algorithm for CRF with L1 penalization, which yields sparse representations, thus implicitly selecting relevant features. We also report experiments conducted on two standard language engineering tasks (chunking and named Entity recognition), for which we analyze the generalization performance and the patterns of selected features. We finally suggest various implementation speed-ups that should allow to efficiently tackle even larger feature vectors.
Over the last few years there has been substantial research on text summarization, but comparatively little research has been carried out on adaptable components that allow rapid development and evaluation of summarization solutions. This paper presents a set of adaptable summarization components together with well-established evaluation tools, all within the GATE paradigm. The toolkit includes resources for the computation of summarization features which are combined in order to provide functionalities for single-document, multi-document, query-based, and multi/cross-lingual summarization. The summarization tools have been successfully used in a number of applications including a fully-ﬂedged information access system
We present a review of the French Treebank (FTB) (1996-2016), a lexical and syntactic resource with rich annotation and manual validation, which is usable by linguists and for NLP and has about 300 users in the world. We summarize the building principles and the main annotation choices, and describe the final version, the different formats and a first evaluation. We also present some derived resources and some query examples.
In the biomedical field, using of specialized terms is key to access information. However, in most Indo-European languages, these terms are complex morphological structures. The presented work aims at identifying the various meaningful components of these terms and use them to improve biomedical Information Retrieval (IR). We present different approaches combining automatic alignments with a pivot language, Japanese, and analogical learning that allows an accurate morphological analysis of terms. These morphological analysis are used to improve the indexing of medical documents. The experiments reported in this paper show the validity of this approach with a 10% MAP improvement over a standard IR system.
This article is devoted to a grammar of the Akkadian verb using finite state technology. It is based on new techniques for which relationships between several representations of a form (four in the Akkadian grammar) are expressed using a tree structure.
The aim of this paper is to present a description of antonym constructions, which is integrated in a formal grammar of spontanous french in order to allow its use for NLP. The characteristic of these constructions is to be simultaneously constrained at many different levels (lexicon, syntax, semantics and pragmatics). We first present a description of the phenomenon, then we illustrate it by the example of an analysis, and finally we show its formalization and its integration in the grammar we develop.
We show that every set of productions of every natural language can be described as a three-dimensional space: the syntagmatic axis is the axis of combinations of signs within the text, the paradigmatic axis is the axis of commutations in each position in the text, and the semiotic axis is the axis internal to the signs, that links the signified to the signifier, the meaning to the text. Every formal modeling of a natural language has to cover this 3-dimensional space. Moreover, in order to capture the regularities, a model seeks after covering the largest possible parts of this space with each rule, which is difficult due to the mutual dependency of the three dimensions. In order to better understand the issues, we will compare two strategies illustrated by a well-known model, LTAG, and another similar one, MTUG, which is inspired by the former but favors a cutting along the syntagmatic and semiotic axes.
In this paper, we present relevant aspects in the acquisition of multilingual computational semantic lexicons, involving the building of core lexicons and large-scale lexicons. We discuss the importance of methodological decisions (e.g. corpus-driven versus mental-driven acquisition) and of training the acquirers in order to build a high quality core lexicon. Then, we report on various ways to scale up the acquisition of core lexicons using tools such as lexical rules and Off the Shelf resources, and show the advantage of working within the semantic-based paradigm. Finally, we emphasize multilingual aspects.
We are interested in the recognition of named entities for the speech modality. Some difficulties may arise for this task due to speech processing. In this work, we propose to study the tight pairing between the speech recognition task and the named entity recognition task. For that purpose, we take away the basic functionnalities of a speech recognition system to turn it into a named entity recognition system. Therefore, by mobilising the inherent knowledge of the speech processing to the named entity recognition task, we ensure a better synergy between the two tasks.
We show that it is possible, without loss of information, to represent Prolexbase according to a semasiological view compliant with LMF.
Our study applies statistical methods to French and Italian corpora to examine the phenomenon of multi-word term reduction in specialty languages. There are two kinds of reduction: anaphoric and lexical. We show that anaphoric reduction depends on the discourse type (vulgarization, pedagogical, specialized) but is independent of both domain and language; that lexical reduction depends on domain and is more frequent in technical, rapidly evolving domains; and that anaphoric reductions tend to follow full terms rather than precede them. We define the notion of the anaphoric tree of the term and study its properties. Concerning lexical reduction, we attempt to prove statistically that there is a notion of term lifecycle, where the full form is progressively replaced by a lexical reduction.
A huge collection of documents cannot be manually processed with a reasonable cost: only automatic systems are a relevant solution. In this paper, we consider the extraction of speaker identity (firstname and lastname) from audio records of broadcast news. Using a rich transcription system, we present a method which allows to extract speaker identities from automatic transcripts and to assign them to speaker turns. Experiments are carried out on French broadcast news records from the ESTER 1 phase II evaluation campaign.
The aim of ths paper is to compare the discourse structures proposed in RST, SDRT and in dependency DAGs which extend the semantic level of MTT for discourses. Hence the term of «strong generative capacity» taken from formal grammars.
In this paper, we report a set of results obtained by tuning a base of lexical relation patterns for CAMÉLÉON, a tool that supports ontology engineering from texts. When evaluating these patterns on eight different corpora, their efficiency varied strongly depending on the test corpus.
This paper presents a survey of the state of the art in NLP techniques dedicated to alternative and augmentative communication for disabled people. As a conclusion, we show that the question of text prediction concerns on the whole any text entry application.
To produce coherent linear documents, natural language generation systems have traditionally exploited the structuring role of textual discourse markers such as relational and referential phrases. These coherence markers of the traditional notion of text, however, do not work in non-linear documents: a new set of graphical devices is needed together with formation rules to govern their usage, supported by sound theoretical frameworks. In this paper, we present our theoretical and empirical work in progress, which explores new possibilities for expressing coherence in the generation of hypertext documents.
This article describes a modular and multilingual NLP platform, which is enriched by a system of multicriteria decision-aid. Further we describe the linguistic data used by this platform as well as the applications based on its technology.
This paper presents the work that we have carried out in investigating the purpose of discourse structure for why-question answering (why-QA). We developed a system for answering why-questions that employs the discourse relations in a pre-annotated document collection (the RST Treebank). With this method, we obtain a recall of 53.3% with a mean reciprocal rank (MRR) of 0.662. We argue that the maximum recall that can be obtained from the use of RST relations as proposed in the present paper is 58.0%. If we discard the questions that require world knowledge, maximum recall is 73.9%. We conclude that discourse structure can play an important role in complex question answering, but that more forms of linguistic processing are needed for increasing recall.
This article presents TERMINAE last developments which is now a platform suited to build terminological resources from texts, including ontologies. TERMINAE may be used to build terminological forms or a conceptual network which structures concepts issued from terms or to build a formal ontology upon which inferences may be made. The methodology and the tool are detailed on an example about glass manufacture, from a text describing the process. We point out the way to explore text with the help of NLP tools available on the platform.
We examine the various ways of representing entities of these kinds in DRT (on the basis of their linguistic properties), and the consequences of these representations on the representation of the anaphoric link itself. After an inventory of the anaphoric material involved, we survey the various possible anaphoric links, either direct or mixed
It is widely recognized that terminology acquisition is about to reach the stage of a mature technology. Robust tools have been developed to support these corpus-based acquisition processes. However, practitioners in this field cannot yet benefit from reference architectures that may greatly help to build large-scale applications. We propose a service oriented architecture that ease the development of applications through reuse of generic services and adaptation of domain-specific services.
This paper deals with the description and the automatic tracking of text segments containing obsolescence in encyclopedia texts. We assume that despite the non-linguistic nature of this phenomenon, discursive cues are relevant to track those segments. For that purpose, we have worked on a corpus which has been manually annotated by experts and on which we have projected automatically tracked cues. We use machine learning techniques to evaluate the predictive power of our cues. We show, using supervised classification, that our hypotheses enable us to build an automatic procedure to assist human experts.
The purpose of automatic simplification is to create version of texts which is easier to understand for a given targeted population. Usually, lexicon and rules required for the simplification are acquired from parallel corpora. Since such corpora are not available for French, we propose methods for their creation from comparable corpora. Our method relies on filtering step, which purpose is to keep the best sentence candidates for alignment, and alignment step considered as categorization problem. The aim is to decide whether a pair of sentences is alignable or not. We exploit different types of features (mainly issued from lexicon and corpora) and get up to 0.97 F-measure with balanced data.
The goal of the project presented here is to implement a prototype of an advanced electronic version of the Basque monolingual dictionary Euskal Hiztegia (EH), hereafter eEH. We will focus on two aspects: i) the methodology followed to correct and update the contents of the dictionary after the automatic conversion from the MRD (Machine Readable Dictionary) to the TEI version (Text Encoding Initiative) and ii) the application developed in order to facilitate the navigation and search through the information contained in it. Our motivation is twofold: 1) to get a well-structured electronic representation of a significant resource for the Basque language, and 2) to offer common users a set of more useful utilities than those offered by paper dictionaries or classic dictionary browser applications. We include a brief description of the EH dictionary itself, outlining its content and structure.
This article takes place in the context of unsupervised information extraction in open domain and focuses on the extraction and the clustering at a large scale of relations between named entities without defining their type a priori. The extraction step combines the use of basic but efficient criteria and a filtering procedure based on machine learning. The clustering step organizes extracted relations into clusters to characterize their type according to a multi-level strategy that takes into account both large volumes of relations and sophisticated clustering criteria. Experiments show that our approach is able to extract relations with a good precision and to organize them according to their semantic and topical similarity.
The decision to make an electronic version came as the dictionary was almost completed. This paper presents the main aspects of this project, from the making of a high level XML encoding, to its availability on the Web. It also presents the different ways to use the TLF, as well as several related databases, created during the project and very useful to improve its efficiency.
One of the recurring questions in natural language processing is the models's ability to account for the reality of language ability. Chomsky's Generative Theory and Minimalism are interested in understanding human language as a cognitive process, which is especially highlighted in the latest proposals by the principle of derivation by phases. A first formalization of Minimalism was introduced in (Stabler, 1997) to study the computational properties. The extension proposed here attempts to account for the idea of phase in a logical framework that allows to easily define a semantic calculus from parsing. This approach raises the problem of using the commutativity and non-commutativity in the Minimalist Categorial Grammars, (Amblard, 2011).
What can be done to increase industrial applications using natural language generation (NLG)? In other words, why is it interesting from a practical point of view to use these techniques for real applications? We try to answer this last question in two steps: firstly, we study the advantages of automatic generation in general, and secondly, we focus on natural language generation, and we find that the specificity of this NLG resides in a combination of the generated texts' quality and variability, the maintainability and the adaptability of the system. Finally, we propose a program of work for the first question: "What can be done to increase industrial applications using NLG?".
Question answering systems aim at retrieving precise information. In a classical question answering system, the step which matches the question to the possible answers is the answer extraction, which begins by selecting sentences that may contain the answer. Selecting and ranking sentences is crucial, since it determines the weights of the short answers, and thus their ranks. This article considers the problem of sentence selection, by evaluating selection strategies.
In this paper we propose to view Lexicalized TAG as the compilation of a more abstract and modular layer of linguistic description: the metagrammar (MG). MG provides a hierarchical representation of syntactic knowledge and principles that capture the well-formedness of lexicalized structures, expressed using syntactic functions. A tool compiles an instance of MG into an LTAG, automatically carrying out the relevant combinations of linguistic phenomena. We then describe an MG for French (MGf) and an MG for Italian (MGi). MGf was built starting with an existing LTAG, augmented as a result. MGi was built by systematic contrast with MGf. The automatic compilation gives two parallel LTAG, compatible for NLP applications.
The vector space models used for the distributional analysis suffer from data sparseness in the context matrix and from the great number of dimensions of the matrix. These limitations make its use on domain-specific corpora difficult and terms are usually not considered while they are essential. In this paper, we propose an adaptation of the distributional analysis in order to apply it efficiently on domain-specific corpora. The proposed approach performs an abstraction of the distributional contexts in order to reduce data sparseness and thus improve the quality of the distributional classes, and also to include terms in these classes. We evaluated our approach on two medical corpora.
French Sign Language (LSF) automatic processing research has to deal with uncomplete corpora of a language which is not yet fully described and has no written form. So, the researchers must carry out reductions at every representational level, constrain the conditions of acquisition of LSF discourses and limit the field of their applications. It is important that they should explain and justify those reductions and know what such reductions imply. In this paper, we will present some of the characteristics of LSF organisation and evolution as well as the different approaches that allow to reduce the complexity of the treatments while preserving most of LSF peculiarity.
This article studies the enunciative and semantic relations in reported speech, based on the analysis of the linguistic markers in the written media's quotation practices. The categorisations and linguistic resources are to be used in several text annotations' tasks, in the Semantic Web field, among others.
Scientific and technical texts are full of assertions (i.e., hypothesis, condition, possibility) that shade the discourse and give more or less certainty to information. We focus on the assertions that influence the relation {patient, medical problem} in patients' records. We aim at automatically identifying certainty and the degrees of certainty in medical texts, as well as their polarity (positive/negative). We propose to exploit morphological, contextual and structural markers for identification and disambiguation of assertions. Our system has been evaluated within the I2B2 international challenge. The evaluation shows that serveral categories of assertion are correctly identified while others remain difficult to detect. We explore also the contribution of each marker and analyse the evolution of the results, as well as performances in the detection of different categories of assertions.
The Antelope linguistic platform, inspired by Meaning-Text Theory, targets the syntactic and semantic analysis of texts, and can handle large corpora. Antelope integrates several pre-existing (parsing) components as well as broad-coverage linguistic data originating from various sources. Efforts towards integration of all components nonetheless make for a homogeneous platform. Our direct contribution deals with components for semantic analysis, and the formalization of a unified text analysis model. This paper introduces the platform and compares it with state-of-the-art projects. It offers to the NLP community a feedback from a software company, by underlining the architectural measures that should be taken to ensure that such complex software remains maintainable.
Most of the sentiment analysis tools process only Modern Standard Arabic (MSA). Indeed, few dialects are considered by the actual tools, in particular Algerian dialect where we do not identify any free tool carrying texts of this dialect. In this article we present a tool for sentiment analysis of messages written in Algerian dialect. This tool is based on an approach which uses both lexicons and specific treatment of agglutination. This approach was experimented using two sentiment lexicons and a test corpus containing 749 messages. The obtained results were encouraging and showing continuous improvement after each step of the considered approach.
Currently, the automatic processing of the legal language is a major issue because of the growing footprint of the law on the web and its complexity in contemporary globalised societies. In addition, through the prism of a specialised language, here legal language, we can measure the progress of natural language processing (NLP). This issue of TAL aims to draw attention to the issues and challenges of legal NLP, to present recent research in this field, and, more broadly, to show how different methods of analysis are organised for this specialised language.
We show how parsli, our model of inflectional morphology, manages to represent many non-canonical phenomena and to formalise them in way allowing for their subsequent implementation. We illustrate it with data about a variety of languages.
We propose an algebraic definition of this notion, which applies to a large range of structured objects: words, attribute-value structures, labeled trees. We test this inference model by applying it to the task of learning morphological analyses of unknown forms. We can assess the validity of the approach thanks to experimental results that are given for several lexicons.
Proper names are usually keys to understand the information contained in a document. Our work focuses on increasing the vocabulary size of a speech transcription system by automatically retrieving proper names from contemporary diachronic text corpus. We assume that some proper names appear in documents relating to the same time period and in similar lexical contexts. We proposed methods that dynamically augment the automatic speech recognition system vocabulary using lexical and temporal features. Three proposed selection methods are based on co-occurrences statistics inside windows of fixed size, on mutual information and on vector space model. Different metrics for proper name selection in order to limit the vocabulary augmentation are studied. Recognition results show a significant reduction of the proper name error rate using augmented vocabulary with retrieved proper names.
This article present our method for dealing with automatic summarization techniques in the legal domain, which helps a legal expert determine the key ideas of a judgement in order to find relevant documents. Our approach is based on the exploration of the document's architecture and its thematic structure in order to build automatically a table style summary, which improves coherency and readability in the summary. The summary is built in four phases: thematic segmentation which identifies the structure of the document, filtering of less important units such as citations of law articles, selection of relevant textual units and production of the summary within the size limit of the abstract. We present the components of a system, called LetSum, built according to this approach, its implementation and some preliminary results of the evaluation.
We present the approach we adopted to carry out a system oriented evaluation in the context of the development of a word alignment system, ALIBI. We look at three evaluation procedures corresponding to fundamental aspects in the development of natural language processing systems: an evaluation resulting from the annotation performed on output alignments to accurately examine the behaviour of each module under study, an evaluation with multicorpus references allowing to observe the behaviour of a system depending on the type of input corpus and an evaluation with a standard reference allowing to observe its behaviour in comparison with other systems of the same type. We describe each procedure and discuss the measures particular to each of them as well as their contribution to system oriented evaluation.
Back-of-the-book indexes are traditional devices that help readers to get access to document content. Such indexes present the book topics in a different form and order than in the document itself. In this paper, we show that back-of-the-book indexes, which give a synthetic view over the document content, are quite similar to indicative summaries. The new methods that are developed for the fine-grained indexing of documents can be compared with summarization technics. Besides their similarities, both types of tools belong to different traditions and the underlying methods are quite different. The confrontation helps to understand the specificity of each approach.
In a Human-Machine Interaction context, automatic in-voice affective state detection systems have to be robust to variabilities and computationally efficient. This paper presents the performance that can be reached using para-verbal (non-verbal) cues. We propose a methodology to select robust parameters families, based on the study of three sets of descriptors, and tested on three different corpora of spontaneous data collected in Human-Machine Interaction contexts. The key finding of our study puts forward perceptive parameters linked to spectral energy, particularly energy on Bark bands, which yield the same performance on a four-emotion detection task as the reference set of descriptors used in the Interspeech 2009 challenge.
This paper describes a modeling in a logical framework of the semantic content of a class of action nouns in French that are ambiguous between a processive interpretation and a resultative one in which these nouns refer to new objects in the world that are created by the process - artefacts. These extensions are added to a previous modelisation based on the notion of dotted types and their modelings in the framework of typed λ-calculus.
This research aims at validating a methodology for the study of segmentation markers in large corpora. Two indices signalling a thematic break in a text are proposed. The first is based on the presence of a paragraph mark and employs the odds ratio to identify the best markers. The second takes into account lexical cohesion between sentences via an index resulting from latent semantic analysis. These two indices were applied mainly to the study of temporal adverbial expressions in literary texts. The analyses carried out confirm a series of linguistic hypotheses about the segmentation function of temporal adverbials.
Annotated corpora are increasingly important for linguistic scholarship, science and technology. This special issue briefly surveys the development of the field and points to challenges within the current framework of annotation using analytical categories as well as challenges to the framework itself. It presents three articles, one concerning the evaluation of the quality of annotation, and two concerning French treebanks, one dealing with the oldest project for French, the French Treebank, the second concerning the conversion of French corpora into the cross-lingual framework of Universal Dependencies, thus offering an illustration of the history of treebank development worldwide.
This article presents SLAM, an Automatic Solver for Lexical Metaphors like "déshabiller* une pomme" (to undress* an apple). SLAM calculates a conventional solution for these productions. To carry on it, SLAM has to intersect the paradigmatic axis of the metaphorical verb "déshabiller*", where "peler" ("to peel") comes closer, with a syntagmatic axis that comes from a corpus where "peler une pomme" (to peel an apple) is semantically and syntactically regular. We test this model on DicoSyn, which is a "small world" network of synonyms, to compute the paradigmatic axis and on Frantext.20, a French corpus, to compute the syntagmatic axis. Further, we evaluate the model with a sample of an experimental corpus of the database of Flexsem.
We present an efficient technique for calculating the probability of occurrence of a discontinued sequence of words, i.e., the probability that those words occur, and that they occur in a given order, regardless of which and how many other words may occur between them. Our method relies on the formalization into a particular Markov chain model, whose specificities are combined with techniques of probability and linear algebra to offer competitive computational complexity. This work is further extended to permit the efficient calculation of the expected document frequency of a sequence. We finally present an application, a fast, automatic, and direct method to evaluate the interestingness of word sequences, by comparing their expected and observed frequencies.
VerbNet is a lexical resource for English verbs that has proven useful for NLP thanks to its high lexical and syntactic coverage and its systematic coding of thematic roles. We present how we have developed Verbenet from VerbNet while using as far as possible the available lexical resources for French, and how the various French alternations are coded, focusing on differences with English (existence of pronominal forms, for example). This paper should allow an NLP researcher to use Verbenet in a simple and efficient way for a task such as semantic role labeling.
Because ethics is an important subject, but is underrepresented in the natural language processing (NLP) domain as compared to others, we examine the causes of this relative lack of interest. To do so, we put NLP back in its historical and social contexts. Although they cannot cover all the subjects, the articles in this special issue of the TAL journal represent an interesting first step to bringing the scientific community at large to reflect upon these issues, to capitalize on it and, finally, to take action. The main motivation for this special issue is to develop the theoretical and practical tools that will allow us, as a community, not to sweep ethical issues under the carpet.
NLP is mainly focused on words and sentences even if most people agree that a better understanding of text structure could help extracting knowledge. In this article we show that, in certain domains, textual approaches are relevant for NLP, taking as an example a specific task related to the medical domain: the automatic modelling of health practice guidelines. The system has been validated on three complementary aspects: usefulness, performances and relevance of the method.
Keyphrases are single or multi-word expressions that represent the main content of a document. In this article we present TopicRank, an unsupervised graph-based method for keyphrase extraction. This method clusters the keyphrase candidates into topics, ranks these topics and extracts the most representative candidate for each of the best topics. Our experiments show a significant improvement over the state-of-the-art graph-based methods for keyphrase extraction.
Several research efforts tackled medical entity recognition from texts. However, to our knowledge, there are no comparative studies for the following approaches: (i) the extraction of noun phrases in an independent step before the final categorization step and (ii) identifying simultaneously entity boundaries and categories. We compare their performance and evaluate their scalability on two standard medical corpora. The results confirm that machine learning methods are more robust than rule-based ones provided that a sufficient number of examples is available. They also point out the lack of scalability of such methods on corpora of different genres. Hybrid methods combining statistical and semantic techniques allow improving the performance obtained by machine learning
Lexicon-Grammar tables, whose development was initiated by (Gross, 1975), are a very rich syntactic lexicon for the French language. This paper presents the work done to formalize the existing classification of verbs using logical formulas, in order to maintain the lexicon. We describe the different types of definitional features and their coding in the table of classes, which is composed by all the features of all classes of verbs. The formal definition of all the defining features for each class has to consider the classification with a decision tree to help finding the class associated with a new entry, which ensures the future extension of the lexicon.
This article details the results of analyses we conducted on the discourse of schizophrenic patients, at the oral production (disfluences) and lexical (part-of-speech and lemmas) levels. The corpus contains more than 375,000 words, its analysis therefore required that we use Natural Language Processing (NLP) and lexicometric tools. In particular, we processed disfluencies and parts-of-speech separately, which allowed us to demonstrate that if schizophrenic patients do produce more disfluencies than control, their lexical richness is not significatively different.
The annotation of temporal relations remains a challenge, being a very difficult task for humans, not to mention machines, to reliably and consistently annotate temporal relations in natural language texts. This paper advocates a change in the definition of the problem itself, by proposing a staged divide-and-conquer approach guided by syntax, that offers a more principled way of selecting temporal entities involved in a temporal relation. The decomposition of the problem into smaller syntactically motivated tasks, and the identification of accurate and linguistically grounded solutions to solve them, promote a sound understanding of the phenomena involved in establishing temporal relations. We illustrate the potential of linguistically informed solutions in the area of temporal relation identification by proposing and evaluating an initial set of syntactically motivated tasks.
This work uses parallel monolingual corpora for a detailed study of the task of subsentential paraphrase acquisition. We argue that the scarcity of this type of resource is compensated by the fact that it is the most suited type for studies on paraphrasing. We propose a large exploration of this task with experiments on two languages with five different acquisition techniques, selected for their complementarity, and their combinations. We report a significant gain over all techniques by validating candidate paraphrases using a maximum entropy classifier. An important result of our study is the identification of difficult-to-acquire paraphrase pairs, which are classified and quantified in a bilingual typology.
We carry out an experiment aimed at using subcategorization information into a syntactic parser for PP attachment disambiguation. The subcategorization lexicon consists of probabilities between a word (verb, noun, adjective) and a preposition. The lexicon is acquired automatically from a 200 million word corpus, that is partially tagged and parsed. In order to assess the lexicon, we use four different corpora in terms of genre and domain.
In this paper we describe our work on a treebank for Serbian, which aims to provide this language with tools and resources needed for parsing and, more globally, to encourage research on this language both in NLP (natural language processing) and in theoretical linguistics. Beyond the results of this resource-building project, we also provide a description of a treebank-building method that optimizes the limited resources available for an under-resourced language, both from the technical point of view (tools and corpora) and from that of human resources (annotation process). We show how best to take advantage of what is available in order to facilitate the manual work and accelerate the corpus enrichment process, all the while maintaining a high-quality annotation. Being based on language-independent principles, this method should help forward the creation of treebanks for other under-resourced languages.
The training and the evaluation of anaphora solvers require the development of annotated corpora. In this paper, we propose a methodology for annotating definite descriptions that supports the consistent annotation of a large corpus (roughly 5 000 definite descriptions). We then present the results of the annotation work and discuss the implications for the automatic resolution of definite descriptions.
Examples of Automated Evaluation platforms deployed as Web server are currently very rare and often underestimated. Time and, effort savings, faster system improvement, common paradigm of evaluation for a community, the benefits offered by such services are plentiful. In this paper, we present a platform for evaluating automatically parsers and we comment on its deployment during an evaluation campaign. First, we draw up a state-of-the-art for platforms used in evaluation of NLP systems, then we present the tools available for Web server deployment. Next, we describe our platform and its deployment in the PASSAGE project as a Web server. Finally we show the interest of generalizing such service to other NLP domains.
In this paper, we propose IrcamCorpusTools, an open and easily extensible platform for analysis, query and visualization of speech corpora. It is already used for unit selection speech synthesis, for prosody and expressivity studies, and to exploit various corpora of spoken French or other languages.
This paper describes the synthesis of two types of sentences containing extractions in Modern French---sentences with a relative or an indirect interrogative. At the same time, our research sheds some light on the formalization of the linguistic correspondence rules that ensure the transition from the semantic to the deep-syntactic level and on the over-all organization of the set of theses rules (= the semantic module of a Meaning-Text model).
Beyond general search engines, tools able to mine specialised document collections are required to answer precise queries. A semantic analysis adapted to the documents and the domain must be previously performed. This is the role of the Ogmios platform. The performance for the annotation of web documents is compatible with the speed of the document crawling. We also explain how the platform has been integrated in a specialised search engine.
This paper presents an automatic annotation of spatial expressions enabling the improvement of the indexing process of geographic information from textual documents. The proposed method combines the use of specific intrasentential relationships and external resources: lexicons of specialized verbs, general thesauri, geographical ontology, and gazetteers. It relies on expanded spatial named entities recognition to deduce a symbolic representation expressed in terms of semantic features, on the one hand, and, to give them a numerical representation, on the other hand. Our method was integrated and tested in a fully automated process of spatial annotation and indexing dedicated to textual documents. Experiments were performed on a corpus of travelogues.
This article describes MorphoClust and MorphoNet, two methods for the unsupervised acquisition of morphological families. MorphoClust builds families by iterative conflations, similarly to hierchical clustering methods. The MorphoNet method relies on community detection in lexical networks. The nodes of these networks stand for words while edges represent morphological transformation rules which are automatically acquired based on graphical similarities between words.
This paper details a typology of risk factors that should concern digital technologies and more specifically NLP. It aims at providing an evaluation grid for an ethical assessment of researches and applications.
The availability of robust and deep syntactic parsing can improve the performance of all modules of a Question Answering system. The system can make use of information found in the fully parsed version of the document collections. We demonstrate that this improves the performance of various components of the system, such as answer extraction and selection, lexical acquisition, off-line relation extraction, and passage retrieval.
Microblogging is a recent trend in today's Internet. Users express their opinions using microblogging platforms such as Twitter. In our research, we use Twitter as a multilingual data source to collect a corpus of sentiment labeled texts. Using the lexicon extracted from our corpus, we build a sentiment classifier which we apply to three kinds of tasks: classification of sentiments in short texts, disambiguation of sentiment ambiguous adjectives, and construction of affective lexicons in different languages. We call them âmicro sentiment analysisâ tasks as they operate on small texts or spans of texts.
Like many NLP tasks, the question of Named Entity Recognition can be adressed either using a symbolic or a data-centered approach. In this paper, we present a hybrid approach which consists in the adaptation of data mining techniques. Our system, mXS, relies on a sequential hierarchical text mining techniques. It implements a data-centered approach to extract symbolic patterns. Besides, mXS relies on an original strategy of recognition which consists in detecting separately the begining and the ending of entities. This strategy is robust on noisy data, especially when speech disfluences or recognition errors occur. Our system has participated to the ETAPE French-speaking evaluation campaign over conversational speech. This paper describes mXS and reports results obtained on reference data (ESTER 2 and ETAPE).
Current work in terminology mining from corpora tends to favour corpus size over corpus quality. Concerning multilingual lexical acquisition from comparable corpora, the best results are obtained on large corpora, i.e. several million words. But for many domains and language pairs, such huge corpora are simply not available. Our main hypothesis is that corpus quality is not only as important as corpus size, it is also what guarantees the quality of the acquired terminological resources. More precisely, we show how important it is to take discourse type into account when building the corpus.
We present a classification-based word prediction model based on IGTREE, a decision-tree induction algorithm with favorable scaling abilities. Through a first series of experiments we demonstrate that the system exhibits log-linear increases in prediction accuracy and decreases in discrete perplexity, a new evaluation metric, with increasing numbers of training examples. The induced trees grow linearly with the amount of training examples. Trained on 30 million words of newswire text, prediction accuracies reach 42.2% on the same type of text. In a second series of experiments we show that this generic approach to word prediction can be specialized to confusible prediction, yielding high accuracies on nine example confusible sets in all genres of text. The confusible-specific approach outperforms the generic word-prediction approach, but with more data the difference decreases.
In this paper, we review some of the problems related to the processing of proper names in the context of speech processing applications. Two specific applications are focused on, automatic speech recognition (ASR) and text-to-speech synthesis (TTS). For both applications, we discuss the extend to which the various linguisitic modules involved (language modelling for ASR, grapheme-to-phoneme conversion for TTS) can benefit from the analysis of large corpora, and from the development of large lexica of proper names. Practical methods for the development of such resources are presented and discussed.
In this paper, we describe an annotation scheme for the attribution of abstract objects (propositions, facts, and eventualities) associated with discourse relations and their arguments annotated in the Penn Discourse TreeBank.
Most automatic summary systems generate as a result an entity that is independent from the source text and that is assumed to have a meaning on its own.
This article deals with formal representation of sign language lexicons. Present models are based on systematic parameter specification, which we think lacks flexibility. We suggest an adaptable model based on a sequential and geometric approach. Then we discuss a graphical representation for it that allows for a clearer view of the descriptions' structures and dependencies. We give hints on the benefits we see in the model onto two different domains: sign language synthesis and data base driven studies of sign languages
The paper investigates the morphological impact of quantitative properties of lexical and sublexical structures in the decomposition of morphologically complex words by means of an activation-based simulation. A complex nucleus of blind-to-semantics relationships turns out to allow the emergence of proto-morphological representations and to provide a cognitively efficient route to complex word decomposition. A computational account of how this bundle of information is handled with in the lexical processing of complex pseudo-words is provided. The model is tested against an edit-distance algorithm and a non-word similarity rating task performed by a group of native speakers.
The NLP community has developed many corpora with rich annotations but these resources are not easily accessible to researchers with little computer expertise. If the NLP community is eager to make available annotated corpora to a wider audience of non-specialists, it is imperative to design and develop user-friendly interfaces, which is not a trivial problem. In this article, in the framework of the Scientext project, we examine several criteria and methods in order to develop such an interface and we highlight the drawbacks of existing systems. We then present the ScienQuest system, dedicated to several kinds of linguistic queries: textual parts, part of speech, syntactic functions.
Unlike most modern languages, Middle French is a language whose spelling is not yet stabilized. There is a great deal of variation in the spelling of a word and accordingly the traditional methods for lemmatization cannot be used. LGeRM (lemmes, graphies et règles morphologiques) proposes a solution based on a databank containing known lemmatized spellings and a set of graphical and morphological rules specific to the medieval language. LGeRM can provide help in consulting a dictionary, browsing or lemmatizing medieval texts, and it can be useful in the electronic edition of manuscripts and the automatic construction of glossaries. This multipurpose tool is accessible on the Internet at www.atilf.fr/dmf.
The present article shows that the normalization of documents in limited domains, such as drug leaflets, can be improved over what can be done with current technologies and without putting more constraints on the technical writer's work. We propose a novel approach to document normalization that uses well-formed content representations expressed at the level of communicative goals and associated normalized texts. This approach combines automatic analysis and interactive negociation with an expert of the domain. Traditional document authoring techniques based on text input are reviewed and critiziced, and the emerging approach to symbolic document authoring, to which our approach belongs, is introduced.
This paper is about opinion classification of posts from a social networks by supervised machine learning, in order to use them in a recommender system. We compare different pre-processings, representations and machine learning tools on real data about movies having specificities (very short texts in English, containing a lot of sms-like codes, abbreviations, misspelling...). We study in detail the results of different classifiers and the contribution of the pre-processings on this kind of data. Finally, we evaluate the best classifier with a recommender system based on collaborative filtering.
This report describes a study of the resolution of zero anaphors and overt pronouns in the Penn Korean Treebank. An automatic anaphor resolution algorithm was developed using the Centering model in Optimality Theory framework, with the addition of constraints that utilize zero anaphors and global topic marking. The best pronoun resolution performance that the model achieved is 74%. Our experiments show that both global topic alignment and pronoun topicality are important factors for pronoun resolution in this corpus
CorpusReader is a framework for creating and querying multi-layer corpora, which contains several levels of analysis (morphology, syntax, semantics, etc.) and which are aimed at observing correlations between these levels. Building, representing and querying multi-layer corpora is complex. CorpusReader's specificity essentially lies in merging the outputs of existing corpus analysis tools, avoiding the problem of integrating them at the software level.
Topic segmentation was addressed by a large amount of work from which it is not easy to draw conclusions, especially about the need for knowledge. In this article, we propose in the same framework two methods for improving the results of a topic segmenter based on lexical reiteration. These topics are then used to facilitate the detection of topical similarity between discourse units. The second approach achieves the same goal by relying on an external resource, that is a network of lexical co-occurrences built from a large corpus. These two approaches are also combined. An evaluation of these approaches and their combination is performed in a reference framework and shows the interest of this combination.
'Corpus de la parole' is a collaborative project between the Ministry of Culture of France and the CNRS, which aims to build a collection of resources on French and other languages of France. A Web site provides an editorialised access to this collection. This article presents the main points of the organization of this program: the data collection, the access, dissemination and sustainability aspects of the digital data.
The present paper reports on the development and evaluation of a historical corpus designed to support detailed empirical studies on the interaction of information structure and syntax in Old High German (OHG). The creation and exploration of this corpus are part of a more general investigation concerning the role of information-structural factors in the explanation of word order variation and change in the Germanic languages.
Books of Hours are the number one best seller of the Middle Ages, with more than 10 000 copies preserved. They are a crucial witness to the medieval mindset, but their textual contents have been very scarcely studied. They are very long and offer a complex hierarchical entangled structure, with several characteristics specific to medieval daily Prières office. This paper presents the methods and processing applied to books of hours: handwritten text recognition and text segmentation adapted to medieval manuscripts. We propose a weak supervised approach, based on the overarching structure of the manuscripts, that provides the first state-of-the-art results on transcript texts and despite remaining errors for this new challenging task.
This paper describes the ANNODIS ressource, a corpus of written French enriched with several markups, including a manual annotation of discourse structures. The resource is original in that it offers a diversified corpus representing several text types, and two annotations based on different approaches to discourse organisation. As well as a description of the ressource -- annotated objects, composition of the corpus -- the paper presents the theoretical underpinnings of the annotation models and the methodological choices underlying corpus preparation and annotation. It also sketches the potential contribution of such a resource for linguistics and NLP, and describes initial results of its exploitation.
We introduce a generic framework in Statistical Machine Translation (SMT) in which lexical hypotheses, in the form of a target language model local to the input sentence, are used to guide the search for the best translation, thus performing a lexical microadaptation. An instantiation of this framework is presented and evaluated on three language pairs, where these auxiliary hypotheses are derived through triangulation via an auxiliairy language. Our first experiments consider nine auxiliary languages, allowing us to measure their individual contribution. We then combine all their hypotheses through a decoding by consensus. Our experiments show that SMT systems can be improved by automatically produced auxiliary hypotheses.
This paper describes the problem of monolingual alignment with search for displaced segments (hereafter called move search). It occurs in genetic criticism, a subfield of literary studies. Existing alignment applications fare poorly on this NP-hard problem. We propose to borrow a family of algorithms from bioinformatics and text algorithmics called fragment chaining alignment. An adaptation of this type of algorithms to NLP is described. An experimental evaluation presents the good results we obtained compared to other approaches.
This paper presents the Prolex project on automatic proper name processing. More specifically, we deal with the construct of a relational dictionary, the parsing of proper name relations in a text and the study of the morphological derivation between a toponym and an inhabitant name.
This work addresses the issue of accessing the content of digital texts, in so as it is anchored in time. It is thus both about implementing interaction systems and modeling the semantics of one the most salient textual units that contributes to anchor in time the situations described in texts: temporal locating adverbials. The linguistic analysis aims to show that language engineering can benefit from considering in a new way the “temporal expressions” in texts, by seeking to uncover the different semantic operations at work in temporal locating adverbials. Pinpointing the semantic values of these operations, we show that it becomes possible to develop new Information Retrieval systems, able of processing queries involving both a calendar criterion and a set of keywords, such as "universities in the early twelfth century", for instance.
External methods for evaluating MT systems define various measures based on MT results and their usage. While operational systems are mostly evaluated since long by task-based methods, evaluation campaigns of the last years use (parsimoniously) quite expensive subjective methods based on unreliable human judgments, and (for the most part) methods based on reference translations, that are impossible to use during the real usage of a system, less correlated with human judgments when quality increases, and totally unrealistic in that they force to measure progress on fixed corpora, endlessly retranslated, and not on new texts to be translated for real needs. There are also numerous biases introduced by the desire to diminish costs, in particular the usage of parallel corpora in the direction opposed to that of their production, and of monolingual rather than bilingual judges. We prove the above by an analysis of the history of MT evaluation, of the «mainstream» evaluation methods, and of certain recent evaluation campaigns. We propose to abandon the reference-based methods in external evaluations, and to replace them by strictly task-based methods, while reserving them for internal evaluations.
This paper shows the benefit of using data mining methods for Biological Natural Language Processing. A method for discovering linguistic patterns based on a recursive sequential pattern mining is proposed. It does not require a sentence parsing nor other resource except a training data set. For the named entities recognition problem, we propose a method based on a new kind of patterns taking account the sequence and its context.
Named Entity metonymy resolution is a challenging natural langage processing task, which has been recently subject to a growing interest. In this paper, we describe the method we have developed in order to solve Named entity metonymy in the framework of the SemEval 2007 competition. In order to perform Named Entity metonymy resolution on location names and company names, as required for this task, we developed a hybrid system based on the use of a robust parser that extracts deep syntactic relations combined with a non supervised distributional approach, also relying on the relations extracted by the parser. We describe this methodology as well as the results obtained at SemEval 2007.
This paper presents SIBYSEM, a word prediction engine, which was developed for the Sibylle AAC system. SIBYSEM incorporates an adaptive model which 1) can be trained on the messages typed by the user and 2) achieves a semantic adaptation (based on Latent Semantic Analysis) which considers the current topic of communication. Several experiments are described, showing the benefits of this adaptive behaviour. We also summarize in the paper our experience of seven years of daily use of the SIBYLLE system with patients from the Kerpape rehabilitation center.