<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-s181183. segId begin by 1, tuid = segId</note>
        <docid>s181183</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>En effet, pour être performants, ces modèles ont besoin de corpus annotés de taille importante.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Moreover, NLP researchers have focused much of their effort on training NLP models on the news domain, due to the availability of training data.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Par conséquent, uniquement les langues bien dotées peuvent bénéficier directement de l'avancée apportée par les RNs, comme par exemple les formes formelles des langues.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>However, many research works have highlighted that models trained on news fail to work efficiently on out-of-domain data, due to their lack of robustness against domain shifts.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Dans le cadre de cette thèse, nous proposons des méthodes d'apprentissage par transfert neuronal pour la construction d'outils de TAL pour les langues et domaines peu dotés en exploitant leurs similarités avec des langues et des domaines bien dotés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This thesis presents a study of transfer learning approaches, through which we propose different methods to take benefit from the pre-learned knowledge from high-resourced domains to enhance the performance of neural NLP models in low-resourced settings.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Précisément, nous expérimentons nos approches pour le transfert à partir du domaine source des textes formels vers le domaine cible des textes informels (langue utilisée dans les réseaux sociaux).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Indeed, despite the importance of its valuable content for a variety of applications (e.g. public security, health monitoring, or trends highlight), this domain is still lacking in terms of annotated data.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Tout au long de cette thèse nous présentons différentes contributions.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We~present different contributions.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Tout d'abord, nous proposons deux approches pour le transfert des connaissances encodées dans les représentations neuronales d'un modèle source, pré-entraîné sur les données annotées du domaine source, vers un modèle cible, adapté par la suite sur quelques exemples annotés du domaine cible.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>First, we propose two methods to transfer the knowledge encoded in the neural representations of a source model -- pretrained on large labelled datasets from the source domain -- to the target model, further adapted by a fine-tuning on few annotated examples from the target domain.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Ensuite, nous effectuons une série d'analyses pour repérer les limites des méthodes proposées.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Second, we perform a series of analysis to spot the limits of the above-mentioned proposed methods.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Nous constatons que, même si l'approche d'apprentissage par transfert proposée améliore les résultats du domaine cible, un transfert négatif « dissimulé » peut atténuer le gain final apporté par l'apprentissage par transfert.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We find that even though transfer learning enhances the performance on social media domain, a hidden negative transfer might mitigate the final gain brought by transfer learning.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>De plus, une analyse interprétative du modèle pré-entraîné montre que les neurones pré-entraînés peuvent être biaisés par ce qu'ils ont appris du domaine source, et donc peuvent avoir des difficultés à apprendre des «~patterns~» spécifiques au domaine cible.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Besides, an interpretive analysis of the pretrained model shows that pretrained neurons may be biased by what they have learnt from the source domain, thus struggle with learning uncommon target-specific patterns.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Suite à cette analyse, nous proposons un nouveau schéma d'adaptation qui augmente le modèle cible avec des neurones normalisés, pondérés et initialisés aléatoirement permettant une meilleure adaptation au domaine cible tout en conservant les connaissances apprises du domaine source.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Third, stemming from our analysis, we propose a new adaptation scheme which augments the target model with normalised, weighted and randomly initialised neurons that beget a better adaptation while maintaining the valuable source knowledge.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Enfin, nous proposons une approche d'apprentissage par transfert qui permet de tirer profit des similarités entre différentes tâches, en plus des connaissances pré-apprises du domaine source.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Finally, we propose a model that, in addition to the pre-learned knowledge from the high-resource source-domain, takes advantage of various supervised NLP tasks.</seg>
            </tuv>
        </tu>
    </body>
</tmx>