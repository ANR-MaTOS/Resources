<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2017AZUR4115. segId begin by 1, tuid = segId</note>
        <docid>2017AZUR4115</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Un défi pour les systèmes de recherche basée sur le contenu réside dans la nécessité d'avoir une base annotée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>One daunting challenge of Content Based Image Retrieval systems is the requirement of annotated databases.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Cette thèse propose un système d'annotation d'images interactif par le regard afin d'alléger la tâche d'annotation.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To limit the burden of annotation, this thesis proposes a system of image annotation based on gaze data.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Le but est de classer un petit ensemble d'images en fonction d'une catégorie cible (classification binaire) pour classer un grand ensemble d'images.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The purpose is to classify a small set of images according to a target category (binary classification) in order to classify a set of unseen images.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Parmi les caractéristiques du regard pointées comme informatives sur l'intention des utilisateurs, nous avons élaboré un estimateur d'intention par le regard, calculable en temps réel, indépendant de l'utilisateur et de la catégorie cible.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Among the gaze features known to be informative about the intentions of the participants, we have determined a Gaze-Based Intention Estimator (GBIE), computable in real-time; independent from both the participant and the target category.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Cette annotation implicite est meilleure qu'une annotation aléatoire mais reste incertaine.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This implicit annotation is better than random annotation but is inherently uncertain.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Dans une deuxième partie, les images ainsi annotées sont utilisées pour classifier un plus grand ensemble d'images avec un algorithme prenant en compte l'incertitude des labels : P-SVM combinant classification et régression.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In a second part, the images annotated by the GBIE from the participants' gaze data are used to classify a bigger set of images with an algorithm that handles label uncertainty: P-SM combining classification and regression SVM.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Nous avons déterminé parmi différentes stratégies un critère de pertinence pour discriminer les labels les plus fiables, utilisés pour la classification, des labels les plus incertains, utilisés pour la régression.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We have determined among different strategies a criterion of relevance in order to discriminate the most reliable labels, involved in the classification part, from the most uncertain labels, involved in the regression part.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>La précision du P-SVM est évaluée dans différents contextes et peut atteindre les performances d'un algorithme de classification standard entraîné avec les labels certains.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The average accuracy of P-SVM is evaluated in different contexts and can compete with the performances of standard classification algorithm trained with true-class labels.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Ces évaluations ont tout d'abord été menées sur un benchmark standard pour se comparer à l'état de l'art, et dans un second temps, sur une base d'images de nourriture.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These evaluations were first conducted on a standard benchmark for comparing with state-of-the-art results and later conducted on food image dataset.</seg>
            </tuv>
        </tu>
    </body>
</tmx>