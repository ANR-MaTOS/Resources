<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2020GRALM012. segId begin by 1, tuid = segId</note>
        <docid>2020GRALM012</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>L'apprentissage profond a permis des avancées significatives dans le domaine de la traduction automatique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In recent years, deep learning has enabled impressive achievements in Machine Translation.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>La traduction automatique neuronale (NMT) s'appuie sur l'entrainement de réseaux de neurones avec un grand nombre de paramètres sur une grand quantité de données parallèles pour apprendre à traduire d'une langue à une autre.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Neural Machine Translation (NMT) relies on training deep neural networks with large number of parameters on vast amounts of parallel data to learn how to translate from one language to another.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Un facteur primordial dans le succès des systèmes NMT est la capacité de concevoir des architectures puissantes et efficaces.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>One crucial factor to the success of NMT is the design of new powerful and efficient architectures.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>À cette fin, nous introduisons Pervasive Attention, un modèle basé sur des convolutions bidimensionnelles qui encodent conjointement les séquences source et cible avec des interactions qui sont omniprésentes dans le réseau neuronal.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For this purpose, we introduce Pervasive Attention, a model based on two-dimensional convolutions that jointly encode the source and target sequences with interactions that are pervasive throughout the network.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Pour améliorer l'efficacité des systèmes NMT, nous étudions la traduction automatique simultanée où la source est lue de manière incrémentielle et le décodeur est alimenté en contextes partiels afin que le modèle puisse alterner entre lecture et écriture.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To improve the efficiency of NMT systems, we explore online machine translation where the source is read incrementally and the decoder is fed partial contexts so that the model can alternate between reading and writing.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Nous abordons également l'efficacité computationnelle des modèles NMT et affirmons qu'ajouter plus de couches à un réseau de neurones n'est pas requis pour tous les cas.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We also address the resource-efficiency of encoder-decoder models and posit that going deeper in a neural network is not required for all instances.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Nous concevons des décodeurs Transformer qui peuvent émettre des prédictions à tout moment dotés de mécanismes d'arrêt adaptatifs pour allouer des ressources en fonction de la complexité de l'instance.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We design depth-adaptive Transformer decoders that allow for anytime prediction and sample-adaptive halting mechanisms to favor low cost predictions for low complexity instances and save deeper predictions for complex scenarios.</seg>
            </tuv>
        </tu>
    </body>
</tmx>