<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2020UPASG010. segId begin by 1, tuid = segId</note>
        <docid>2020UPASG010</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>De nos jours, l'IA repose en grande partie sur l'utilisation de données de grande taille et sur des méthodes d'apprentissage machine améliorées qui consistent à développer des algorithmes de classification et d'inférence en tirant parti de grands ensembles de données de grande taille.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>AI nowadays relies largely on using large data and enhanced machine learning methods which consist in developing classification and inference algorithms leveraging large datasets of large sizes.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Ces grandes dimensions induisent de nombreux phénomènes contre-intuitifs, conduisant généralement à une mauvaise compréhension du comportement de nombreux algorithmes d'apprentissage machine souvent conçus avec des intuitions de petites dimensions de données.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These large dimensions induce many counter-intuitive phenomena, leading generally to a misunderstanding of the behavior of many machine learning algorithms often designed with small data dimension intuitions.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>En tirant parti du cadre multidimensionnel (plutôt que d'en souffrir), la théorie des matrices aléatoires (RMT) est capable de prédire les performances de nombreux algorithmes non linéaires aussi complexes que certains réseaux de neurones aléatoires, ainsi que de nombreuses méthodes du noyau telles que les SVM, la classification semi-supervisée, l'analyse en composantes principales ou le regroupement spectral.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>By taking advantage of (rather than suffering from) the multidimensional setting, random matrix theory (RMT) is able to predict the performance of many non-linear algorithms as complex as some random neural networks as well as many kernel methods such as Support Vector Machines, semi-supervised classification, principal component analysis or spectral clustering.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Pour caractériser théoriquement les performances de ces algorithmes, le modèle de données sous-jacent est souvent un modèle de mélange gaussien (MMG) qui semble être une hypothèse forte étant donné la structure complexe des données réelles (par exemple, des images).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To characterize the performance of these algorithms theoretically, the underlying data model is often a Gaussian mixture model (GMM) which seems to be a strong assumption given the complex structure of real data (e.g., images).</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>En outre, la performance des algorithmes d'apprentissage automatique dépend du choix de la représentation des données (ou des caractéristiques) sur lesquelles ils sont appliqués.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Furthermore, the performance of machine learning algorithms depends on the choice of data representation (or features) on which they are applied.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Encore une fois, considérer les représentations de données comme des vecteurs gaussiens semble être une hypothèse assez restrictive.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Once again, considering data representations as Gaussian vectors seems to be quite a restrictive assumption.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>S'appuyant sur la théorie des matrices aléatoires, cette thèse vise à aller au-delà de la simple hypothèse du MMG, en étudiant les outils classiques d'apprentissage machine sous l'hypothèse de vecteurs aléatoires concentrés qui généralisent les vecteurs Gaussiens.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Relying on random matrix theory, this thesis aims at going beyond the simple GMM hypothesis, by studying classical machine learning tools under the hypothesis of Lipschitz-ally transformed Gaussian vectors also called concentrated random vectors, and which are more generic than Gaussian vectors.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Cette hypothèse est particulièrement motivée par l'observation que l'on peut utiliser des modèles génératifs (par exemple, les GAN) pour concevoir des structures de données complexes et réalistes telles que des images, grâce à des transformations Lipschitzienne de vecteurs gaussiens.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This hypothesis is particularly motivated by the observation that one can use generative models (e.g., GANs) to design complex and realistic data structures such as images, through Lipschitz-ally transformed Gaussian vectors.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Cela suggère notamment que l'hypothèse de concentration sur les données mentionnée ci-dessus est un modèle approprié pour les données réelles et qui est tout aussi mathématiquement accessible que les MMG.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This notably suggests that making the aforementioned concentration assumption on data is a suitable model for real data and which is just as mathematically accessible as GMM models.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Par conséquent, nous démontrons à travers cette thèse, en nous appuyant sur les GANs, l'intérêt de considérer le cadre des vecteurs concentrés comme un modèle pour les données réelles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Therefore, we demonstrate through this thesis, leveraging on GANs, the interest of considering the framework of concentrated vectors as a model for real data.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>En particulier, nous étudions le comportement des matrices de Gram aléatoires qui apparaissent au cœur de divers modèles linéaires, des matrices à noyau qui apparaissent dans les méthodes à noyau et également des méthodes de classification qui reposent sur une solution implicite (par exemple, la couche de Softmax dans les réseaux de neurones), avec des données aléatoires supposées concentrées.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In particular, we study the behavior of random Gram matrices which appear at the core of various linear models, kernel matrices which appear in kernel methods and also classification methods which rely on an implicit solution (e.g., Softmax layer in neural networks), with concentrated random inputs.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>L'analyse de ces méthodes pour des données concentrées donne le résultat surprenant qu'elles ont asymptotiquement le même comportement que pour les données de MMG.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Analyzing these methods for concentrated data yields to the surprising result that they have asymptotically the same behavior as for GMM data (with the same first and second order statistics).</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Ce résultat suggère fortement l'aspect d'universalité des grands classificateurs d'apprentissage machine par rapport à la distribution sous-jacente des données.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This result strongly suggest the universality aspect of large machine learning classifiers w.r.t.</seg>
            </tuv>
        </tu>
    </body>
</tmx>