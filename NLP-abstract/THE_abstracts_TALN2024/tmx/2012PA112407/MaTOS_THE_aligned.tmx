<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2012PA112407. segId begin by 1, tuid = segId</note>
        <docid>2012PA112407</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Leur rôle est fondamentale dans de nombreux cadres d'application comme la reconnaissance automatique de la parole, la traduction automatique, l'extraction et la recherche d'information.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>They play an important role in many successful applications of Natural Language Processing, such as Automatic Speech Recognition, Machine Translation and Information Extraction.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Ce type de modèle prédit un mot uniquement en fonction des n-1 mots précédents.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this way, language models predict a word based on its n-1 previous words.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Pourtant, cette approche est loin d'être satisfaisante puisque chaque mot est traité comme un symbole discret qui n'a pas de relation avec les autres.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In spite of their prevalence, conventional n-gram based language models still suffer from several limitations that could be intuitively overcome by consulting human expert knowledge.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Ainsi les spécificités du langage ne sont pas prises en compte explicitement et les propriétés morphologiques, sémantiques et syntaxiques des mots sont ignorées.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>One critical limitation is that, ignoring all linguistic properties, they treat each word as one discrete symbol with no relation with the others.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Sa construction repose sur le dénombrement de successions de mots, effectué sur des données d'entrainement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This kind of model is constructed based on the count of n-grams in training data.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Ce sont donc uniquement les textes d'apprentissage qui conditionnent la pertinence de la modélisation n-gramme, par leur quantité (plusieurs milliards de mots sont utilisés) et leur représentativité du contenu en terme de thématique, époque ou de genre.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Therefore, the pertinence of these models is conditioned only on the characteristics of the training text (its quantity, its representation of the content in terms of theme, date).</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Cette représentation continue confère aux modèles neuronaux une meilleure capacité de généralisation et leur utilisation a donné lieu à des améliorations significative en reconnaissance automatique de la parole et en traduction automatique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These representations and the associated objective function (the likelihood of the training data) are jointly learned using a multi-layer neural network architecture.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Ainsi par le passé, les modèles neuronaux ont été utilisés soit pour des tâches avec peu de données d'apprentissage, soit avec un vocabulaire de mots à prédire limités en taille.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This approach has shown significant and consistent improvements when applied to automatic speech recognition and statistical machine translation tasks.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>La première contribution de cette thèse est donc de proposer une solution qui s’appuie sur la structuration de la couche de sortie sous forme d’un arbre de classification pour résoudre ce problème de complexité.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>A major difficulty with the continuous space neural network based approach remains the computational burden, which does not scale well to the massive corpora that are nowadays available.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Le modèle se nomme Structure OUtput Layer (SOUL) et allie une architecture neuronale avec les modèles de classes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For this reason, the first contribution of this dissertation is the definition of a neural architecture based on a tree representation of the output vocabulary, namely Structured OUtput Layer (SOUL), which makes them well suited for large scale frameworks.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>La deuxième contribution de cette thèse est d'analyser les représentations continues induites et de comparer ces modèles avec d'autres architectures comme les modèles récurrents.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The second contribution is to provide several insightful analyses on their performances, their pros and cons, their induced word space representation.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Enfin, la troisième contribution est d'explorer la capacité de la structure SOUL à modéliser le processus de traduction.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Finally, the third contribution is the successful adoption of the continuous space neural network into a machine translation framework.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Les résultats obtenus montrent que les modèles continus comme SOUL ouvrent des perspectives importantes de recherche en traduction automatique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>New translation models are proposed and reported to achieve significant improvements over state-of-the-art baseline systems.</seg>
            </tuv>
        </tu>
    </body>
</tmx>