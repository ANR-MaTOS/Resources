<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2020LYSES008. segId begin by 1, tuid = segId</note>
        <docid>2020LYSES008</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>De nombreuses applications en traitement du langage naturel (TALN) reposent sur les représentations de mots, ou “word embeddings”.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Many natural language processing applications rely on word representations (also called word embeddings) to achieve state-of-the-art results.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Ces représentations doivent capturer à la fois de l’information syntaxique et sémantique pour donner des bonnes performances dans les tâches en aval qui les utilisent.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These numerical representations of the language should encode both syntactic and semantic information to perform well in downstream tasks.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Cependant, les méthodes courantes pour les apprendre utilisent des textes génériques comme Wikipédia qui ne contiennent pas d’information sémantique précise.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>However, common models (word2vec, GloVe) use generic corpus like Wikipedia to learn them and they therefore lack specific semantic information.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>De plus, un espace mémoire important est requis pour pouvoir les sauvegarder car le nombre de représentations de mots à apprendre peut être de l’ordre du million.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Moreover it requires a large memory space to store them because the number of representations to save can be in the order of a million.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>J’ai développé dict2vec, un modèle qui utilise l’information des dictionnaires linguistiques lors de l’apprentissage des word embeddings.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>I developed dict2vec, a model that uses additional information from online lexical dictionaries when learning word representations.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Les word embeddings appris par dict2vec obtiennent des scores supérieurs d’environ 15% par rapport à ceux appris avec d’autres méthodes sur des tâches de similarités sémantiques de mots.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The dict2vec word embeddings perform ∼15% better against the embeddings learned by other models on word semantic similarity tasks.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>La seconde partie de mes travaux consiste à réduire la taille mémoire des word embeddings.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The second part of my work is to reduce the memory size of the embeddings.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>J’ai développé une architecture basée sur un auto-encodeur pour transformer des word embeddings à valeurs réelles en vecteurs binaires, réduisant leur taille mémoire de 97% avec seulement une baisse de précision d’environ 2% dans des tâches de TALN en aval.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>I developed an architecture based on an autoencoder to transform commonly used real-valued embeddings into binary embeddings, reducing their size in memory by 97% with only a loss of ∼2% in accuracy in downstream NLP tasks.</seg>
            </tuv>
        </tu>
    </body>
</tmx>