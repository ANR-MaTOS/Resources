<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2019AVIG0233. segId begin by 1, tuid = segId</note>
        <docid>2019AVIG0233</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Au cours des dernières années, l’apprentissage profond est devenu l’approche privilégiée pour le développement d’une intelligence artificielle moderne (IA).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the recent years, deep learning has become the leading approach to modern artificial intelligence (AI).</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>L’augmentation importante de la puissance de calcul, ainsi que la quantité sans cesse croissante de données disponibles ont fait des réseaux de neurones profonds la solution la plus performante pour la resolution de problèmes complexes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The important improvement in terms of processing time required for learning AI based models alongside with the growing amount of available data made of deep neural networks (DNN) the strongest solution to solve complex real-world problems.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Pour résoudre ce problème, les réseaux de neurones basés sur les algèbres des nombres complexes et hypercomplexes ont été développés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The natural multidimensionality of the data is elegantly embedded within complex and hypercomplex neurons composing the model.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>En particulier, les réseaux de neurones de quaternions (QNN) ont été proposés pour traiter les données tridimensionnelles et quadridimensionnelles, sur la base des quaternions représentant des rotations dans notre espace tridimensionnel.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In particular, quaternion neural networks (QNN) have been proposed to deal with up to four dimensional features, based on the quaternion representation of rotations and orientations.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Malheureusement, et contrairement aux réseaux de neurones à valeurs complexes qui sont de nos jours acceptés comme une alternative aux réseaux de neurones réels, les QNNs souffrent de nombreuses lacunes qui sont en partie comblées par les différents travaux détaillés par ce manuscrit.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Unfortunately, and conversely to complex-valued neural networks that are nowadays known as a strong alternative to real-valued neural networks, QNNs suffer from numerous limitations that are carrefuly addressed in the different parts detailled in this thesis.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Ainsi, la thèse se compose de trois parties qui introduisent progressivement les concepts manquants, afin de faire des QNNs une alternative aux réseaux neuronaux à valeurs réelles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The thesis consists in three parts that gradually introduce the missing concepts of QNNs, to make them a strong alternative to real-valued NNs.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>La premiere partie présente et répertorie les précédentes découvertes relatives aux quaternions et aux réseaux de neurones de quaternions, afin de définir une base pour la construction des QNNs modernes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The first part introduces and list previous findings on quaternion numbers and quaternion neural networks to define the context and strong basics for building elaborated QNNs.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>La deuxième partie introduit des réseaux neuronaux de quaternions état de l’art, afin de permettre une comparaison dans des contextes identiques avec les architectures modernes traditionnelles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The second part introduces state-of-the-art quaternion neural networks for a fair comparison with real-valued neural architectures.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Plus précisément, les QNNs étaient majoritairement limités par leurs architectures trop simples, souvent composées d’une seule couche cachée comportant peu de neurones.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>More precisely, QNNs were limited by their simple architectures that were mostly composed of a single and shallow hidden layer.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Premièrement, les paradigmes fondamentaux, tels que les autoencodeurs et les réseaux de neurones profonds sont présentés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this part, we propose to bridge the gap between quaternion and real-valued models by presenting different quaternion architectures.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Ensuite, les très répandus et étudiés réseaux de neurones convolutionnels et récurrents sont étendus à l’espace des quaternions.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>First, basic paradigms such as autoencoders and deep fully-connected neural networks are introduced.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Dans un scénario traditionnel impliquant des QNNs, les caractéristiques d’entrée sont manuellement segmentées en quatre composants, afin de correspondre à la representation induite par les quaternions.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In a conventional QNN scenario, input features are manually segmented into three or four components, enabling further quaternion processing.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Malheureusement, il est difficile d’assurer qu’une telle segmentation est optimale pour résoudre le problème considéré.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Unfortunately, there is no evidence that such manual segmentation is the representation that suits the most to solve the considered task.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>De plus, une segmentation manuelle réduit fondamentalement l’application des QNNs à des tâches naturellement définies dans un espace à au plus quatre dimensions.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Morevover, a manual segmentation drastically reduces the field of application of QNNs to four dimensional use-cases.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>De ce fait, la troisième partie de cette thèse introduit un modèle supervisé et un modèle non supervisé permettant l’extraction de caractéristiques d’entrée désentrelacées et significatives dans l’espace des quaternions, à partir de n’importe quel type de signal réel uni-dimentionnel, permettant l’utilisation des QNNs indépendamment de la dimensionnalité des vecteurs d’entrée et de la tâche considérée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Therefore the third part introduces a supervised and an unsupervised model to extract meaningful and disantengled quaternion input features, from any real-valued input signal, enabling the use of QNNs regardless of the dimensionality of the considered task.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>Les expériences menées sur la reconnaissance de la parole et la classification de documents parlés montrent que les approches proposées sont plus performantes que les représentations traditionnelles de quaternions.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Conducted experiments on speech recognition and document classification show that the proposed approaches outperform traditional quaternion features.</seg>
            </tuv>
        </tu>
    </body>
</tmx>