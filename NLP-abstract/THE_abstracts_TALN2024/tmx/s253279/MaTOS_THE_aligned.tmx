<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-s253279. segId begin by 1, tuid = segId</note>
        <docid>s253279</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Les réseaux de neurones sont devenus un algorithme phare de l'intelligence artificielle, avec des succès majeurs dans la résolution de tâches complexes telles que la reconnaissance d'images et le jeu.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Neural networks have become a flagship algorithm of Artificial intelligence (AI), with major successes in solving complex tasks such as image recognition and game playing.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Les ordinateurs et les cartes graphiques consomment malheureusement une quantité très élevée d'énergie lorsqu'ils exécutent des réseaux de neurones.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Digital computers unfortunately consume an inordinate amount of energy when they run neural networks.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Par exemple, l'entraînement d'un modèle de traitement du langage naturel de pointe sur un superordinateur moderne consomme 1000 kW.h [1], soit l'énergie consommée par un cerveau humain pour l'ensemble de ses tâches sur une durée de six ans.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For example, training a state-of-the art natural language processing model on a modern supercomputer consumes 1000 kW.h [1], which is the energy consumed by a human brain for the entirety of its tasks over a duration of six years.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Dans le cerveau, les neurones - qui peuvent être considérés comme effectuant le calcul - ont un accès direct à la mémoire, portée par les synapses.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the brain, neurons -- which can roughly be seen as carrying out the computation -- have a direct access to memory, supported by synapses.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>L'électronique actuelle, sur laquelle reposent les GPU et les CPU utilisés en IA, sépare intrinsèquement la mémoire et le calcul dans des unités physiques distinctes, entre lesquelles les données doivent être transportées.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Current electronics, on which rely the GPUs and CPUs used in AI, intrinsically separates memory and computing into distinct physical units, between which data must be carried back and forth.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Ce "goulot d'étranglement de von Neuman" est un problème pour les algorithmes d'intelligence artificielle qui nécessitent la lecture de quantités considérables de données à chaque étape, l'exécution d'opérations avancées sur ces données, puis la réécriture des résultats en mémoire [2]-[4].</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This "von Neuman bottleneck" is an issue for artificial intelligence algorithms which require reading considerable amounts of data at each step, performing advanced operations on this data, and then writing the results back to memory [2],[4].</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Ce processus ralentit le calcul et augmente considérablement la consommation d'énergie pour l'apprentissage et l'inférence.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This process slows down computing and considerably increases the energy consumption for learning and inference.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Le paradigme général du calcul neuromorphique consiste donc à s'inspirer de la topologie du cerveau pour construire des circuits composés de neurones physiques interconnectés par des synapses physiques qui mettent en œuvre la mémoire in-situ, de manière non volatile, ce qui réduit considérablement la nécessité de déplacer les données dans le circuit et permet d'énormes gains en termes de vitesse et d'efficacité énergétique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The general paradigm in neuromorphic computing is therefore to take inspiration from the topology of the brain to build circuits composed of physical neurons interconnected by physical synapses that implement memory in-situ, in a non-volatile way, thus drastically cutting the need to move data around the circuit and allowing huge gains in speed and energy efficiency.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>L'un des défis majeurs du calcul neuromorphique est de parvenir à former les réseaux sur puce de manière efficace.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>One of the major challenges in neuromorphic computing is to achieve on chip training of the networks in an efficient way.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>La mise en œuvre d'un algorithme d'entraînement traditionnel tel que la rétropropagation du gradient nécessite des circuits de grande surface et gourmands en énergie pour stocker les activations, calculer les gradients, stocker les gradients, puis modifier séquentiellement chaque synapse physique du réseau.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Implementing state-of-the art training algorithm such as backpropagation requires bulky and energy-hungry circuits to store activations, compute gradients, store gradients, then sequentially modify each physical synapse in the network.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>C'est un obstacle immense au développement d'une IA intégrée capable d'apprendre.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This is an immense hurdle towards the development of an embedded AI that can learn.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>C'est pourquoi les puces d'IA développées aujourd'hui dans les universités et l'industrie visent principalement l'inférence avec un apprentissage hors puce [5]-[7], ou mettent en œuvre des algorithmes d'entraînement dont les performances sont loin des méthodes de descente de gradient les plus modernes [8].</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For this reason, the AI chips developed today in academia and industry mostly target inference with off-chip learning [5],[7], or implement training algorithms with performance far from state-of-the-art gradient descent methods [8].</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Dans ce contexte, les travaux de pionniers de l'IA, tels que Geoffrey Hinton [9] et Yoshua Bengio [10]-[12], visant à comprendre comment la descente de gradient peut être effectuée dans le cerveau, donnent des indications inspirantes pour atteindre le même résultat dans les systèmes neuromorphiques.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this context, works by AI pioneers such as Geoffrey Hinton [9] and Yoshua Bengio [10],[12] to understand how gradient descent could be performed in the brain gives inspirational guidelines to achieve the same in neuromorphic systems.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Des algorithmes tels que la propagation de l'équilibre, proposée pour la première fois par Scellier et Bengio [12], et la propagation de l'éligibilité (E-Prop) de Wolfgang Maas [13] montrent que, dans les réseaux de neurones de pointe, les gradients peuvent être transportés à travers le réseau directement par l'activité des neurones.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Algorithms such as Equilibrium propagation, first proposed by Scellier and Bengio [12], and E-Prop by Wolfgang Maas [13] show that in spiking neural networks gradients can be carried through the network directly by neuron activity.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>Le C2N et le CNRS/Thales, en collaboration avec l'équipe de Yoshua Bengio au Mila, ont montré que les gradients calculés par la propagation de l'équilibre sont égaux à ceux dérivés par la rétropropagation du gradient dans le temps, et ont souligné le potentiel de ce résultat pour la formation de systèmes neuromorphiques avec des performances de pointe dans une publication dans NeurIPS l'année dernière (acceptée comme présentation orale) [14].</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>C2N and CNRS/Thales in collaboration with the team of Yoshua Bengio at Mila have shown that the gradients computed by Equilibrium Propagation are equal to those derived by backpropagation through time, and highlighted the potential of this result for training neuromorphic systems with state-of-the-art performance in a publication in NeurIPS last year (accepted as oral presentation) [14].</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>L'objectif ultime de la thèse est de fabriquer des circuits neuromorphiques qui apprennent localement et de manière autonome grâce à des algorithmes tels que la propagation de l'équilibre, qui codent des gradients dans l'activité impulsionnelle des neurones, et peuvent être directement appliqués aux synapses qu'ils connectent.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The ultimate objective of the thesis is to fabricate neuromorphic spiking circuits that learn locally and autonomously through algorithms such as Equilibrium Propagation that encode gradients in the spiking activity of neurons and can be directly applied to the synapses they connect.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>À cette fin, le/la doctorant.e devra d'abord développer, coder et tester par des simulations de nouveaux algorithmes pour entraîner des réseaux de neurones à impulsions récurrents, inspirés de la propagation de l'équilibre mais adaptés au matériel neuromorphique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For this purpose, the doctoral student will first develop, code and test through simulations novel algorithms to train recurrent spiking neural networks, inspired by Equilibrium propagation but adapted to neuromorphic hardware.</seg>
            </tuv>
        </tu>
        <tu tuid="18">
            <tuv xml:lang="FR">
                <seg>Il/elle réalisera ensuite ces réseaux avec des nanocomposants, par des expériences à l'échelle du laboratoire, puis dans des systèmes plus importants.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Then he or she will realize these networks in hardware, through lab-scale experiments and then in larger systems.</seg>
            </tuv>
        </tu>
        <tu tuid="19">
            <tuv xml:lang="FR">
                <seg>Les éléments de base qui seront utilisés pour développer ce matériel sont des composants électroniques à l'échelle nanométrique appelés dispositifs à commutation résistive ou memristors (abréviation de 'memory resistors') [15].</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The building blocks that will be used for developing this hardware are nanoscale electronic components called resistive switching devices or memristors (short for memory-resistors) [15].</seg>
            </tuv>
        </tu>
        <tu tuid="20">
            <tuv xml:lang="FR">
                <seg>Ils sont constitués d'un oxyde isolant pris en sandwich entre deux électrodes métalliques (Fig. 1(a-b).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>They are made of an insulating oxide material sandwiched between two metallic electrodes (Fig. 1(a-b)).</seg>
            </tuv>
        </tu>
        <tu tuid="21">
            <tuv xml:lang="FR">
                <seg>Ces dispositifs à l'échelle nanométrique sont très prometteurs pour l'informatique neuromorphique car ils peuvent imiter à la fois les synapses et les neurones avec une très faible énergie, et ont de très petites dimensions.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These nanoscale devices are very promising for neuromorphic computing because they can imitate both synapses and neurons with very low energy in a very small area.</seg>
            </tuv>
        </tu>
        <tu tuid="22">
            <tuv xml:lang="FR">
                <seg>Ils présentent des comportements différents selon les matériaux utilisés pour l'oxyde et les électrodes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>They exhibit different behaviours depending on the materials that are used for the oxide and the electrodes.</seg>
            </tuv>
        </tu>
        <tu tuid="23">
            <tuv xml:lang="FR">
                <seg>Ils peuvent être passifs et présenter une commutation de résistance non-volatile (par exemple, avec des hétérostructures Pt/TaOx/Pt, Fig. 1(d)), ce qui est utile pour les caractéristiques synaptiques (mémoire et plasticité à plusieurs niveaux).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>They can be passive and exhibit non-volatile resistance switching (e.g., with Pt/TaOx/Pt heterostructures), which is useful for synaptic features (multilevel memory and plasticity, (Fig. 1(d)).</seg>
            </tuv>
        </tu>
        <tu tuid="24">
            <tuv xml:lang="FR">
                <seg>Ils peuvent également être actifs et présenter une commutation volatile par résistance différentielle négative (par exemple, avec des hétérostructures Pt/NbOx/Pt), ce qui peut générer des caractéristiques neuronales oscillantes (comme des impulsions et des trains d'impulsions, Fig. 1(c)).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>They can also be active and display volatile switching through negative differential resistance (e.g., with Pt/NbOx/Pt heterostructures), which can generate oscillating neuron features (like spiking and bursting, Fig. 1(c)).</seg>
            </tuv>
        </tu>
        <tu tuid="25">
            <tuv xml:lang="FR">
                <seg>Le C2N et le CNRS/Thales ont une expertise de longue date dans la fabrication et l'étude de ces nanodispositifs [3], [16]-[20].</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>C2N and CNRS/Thales have a long standing expertise in the fabrication and study of these nanodevices [3], [16],[20].</seg>
            </tuv>
        </tu>
        <tu tuid="26">
            <tuv xml:lang="FR">
                <seg>Pour l'instant, les études ont surtout porté sur les circuits qui utilisent ces dispositifs soit comme synapses, soit comme neurones, mais pas les deux à la fois.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For now, studies have mostly focused on circuits that use these devices either as synapses, or as neurons, but not the two at the same time.</seg>
            </tuv>
        </tu>
        <tu tuid="27">
            <tuv xml:lang="FR">
                <seg>Il a été démontré que les synapses mémorielles peuvent apprendre grâce à l'apprentissage bio-inspiré appelé Spike Timing Dependent Plasticity [3], [21].</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>It has been shown that memristive synapses can learn through the bio-inspired learning rule called Spike Timing Dependent Plasticity [3], [21].</seg>
            </tuv>
        </tu>
        <tu tuid="28">
            <tuv xml:lang="FR">
                <seg>Il a aussi été prouvé que les neurones mémoriels peuvent imiter de nombreuses caractéristiques des neurones comme leur réponse impulsionnelle et les trains d'impulsions [15].</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>It has also been demonstrated that memristive neurons can imitate many features of neurons including spiking and bursting [15].</seg>
            </tuv>
        </tu>
        <tu tuid="29">
            <tuv xml:lang="FR">
                <seg>Le but de cette thèse est d'assembler ces composants individuels pour former des réseaux de neurones à impulsions capables d'apprendre de manière autonome grâce à des algorithmes de pointe reproduisant la retro-propagation du gradient *in materio*.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The goal of the thesis is to assemble these individual components in spiking neural networks that learn autonomously through state-of-the-art algorithms emulating backpropagation in materio.</seg>
            </tuv>
        </tu>
    </body>
</tmx>