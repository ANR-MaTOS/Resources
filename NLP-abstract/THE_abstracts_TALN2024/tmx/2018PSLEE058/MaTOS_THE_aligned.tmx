<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2018PSLEE058. segId begin by 1, tuid = segId</note>
        <docid>2018PSLEE058</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>La compréhension automatique de vidéos devrait impacter notre vie de tous les jours dans de nombreux domaines comme la conduite autonome, les robots domestiques, la recherche et le filtrage de contenu, les jeux vidéo, la défense ou la sécurité.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Automatic video understanding is expected to impact our lives through many applications such as autonomous driving, domestic robots, content search and filtering, gaming, defense or security.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Le nombre de vidéos croît plus vite chaque année, notamment sur les plateformes telles que YouTube, Twitter ou Facebook.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Video content is growing faster each year, for example on platforms such as YouTube, Twitter or Facebook.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>L'analyse automatique de ces données est indispensable pour permettre à de nouvelles applications de voir le jour.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Automatic analysis of this data is required to enable future applications.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>L'analyse vidéo, en particulier en environnement non contrôlé, se heurte à plusieurs problèmes comme la variabilité intra-classe (les échantillons d'un même concept paraissent très différents) ou la confusion inter-classe (les exemples provenant de deux activités distinctes se ressemblent).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Video analysis, especially in uncontrolled environments, presents several difficulties such as intraclass variability (samples from the same concept appear very differently) or inter-class confusion (examples from two different activities look similar).</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Bien que ces difficultés puissent être traitées via des algorithmes d'apprentissage supervisé, les méthodes pleinement supervisées sont souvent synonymes d'un coût d'annotation élevé.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>While these problems can be addressed with the supervised learning algorithms, fully-supervised methods are often associated with high annotation cost.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Dépendant à la fois de la tâche à effectuer et du niveau de supervision requis, la quantité d'annotations nécessaire peut être prohibitive.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Depending on both the task and the level of required supervision, the annotation can be prohibitive.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Dans le cas de la localisation d'actions, une approche pleinement supervisée nécessite les boîtes englobantes de l'acteur à chaque image où l'action est effectuée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>For example, in action localization, a fully-supervised approach demands person bounding boxes to be annotated at every frames where an activity is performed.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Le coût associé à l'obtention d'un telle annotation empêche le passage à l'échelle et limite le nombre d'échantillons d'entraînement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The cost of getting such annotation prohibits scalability and limits the number of training samples.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Cette thèse adresse les problèmes évoqués ci-dessus dans le contexte de deux tâches, la classification et la localisation d'actions humaines.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This thesis addresses above problems in the context of two tasks, namely human action classification and localization.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>La classification consiste à reconnaître l'activité effectuée dans une courte vidéo limitée à la durée de l'action.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The former aims at recognizing the type of activity performed in a short video clip trimmed to the temporal extent of the action.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>La localisation a pour but de détecter en temps et dans l'espace des activités effectuées dans de plus longues vidéos.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The latter additionally extracts the space-time locations of potentially multiple activities in much longer videos.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Notre approche pour la classification d'actions tire parti de l'information contenue dans la posture humaine et l'intègre avec des descripteurs d'apparence et de mouvement afin d'améliorer les performances.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our approach to action classification leverages information from human pose and integrates it with appearance and motion descriptors for improved performance.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Notre approche pour la localisation d'actions modélise l'évolution temporelle des actions à l'aide d'un réseau récurrent entraîné à partir de suivis de personnes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our approach to action localization models the temporal evolution of actions in the video with a recurrent network trained on the level of person tracks.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Enfin, la troisième méthode étudiée dans cette thèse a pour but de contourner le coût prohibitif des annotations de vidéos et utilise le regroupement discriminatoire pour analyser et combiner différents types de supervision.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Finally, the third method in this thesis aims to avoid a prohibitive cost of video annotation and adopts discriminative clustering to analyze and combine different levels of supervision.</seg>
            </tuv>
        </tu>
    </body>
</tmx>