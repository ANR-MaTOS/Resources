<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2018SACLS313. segId begin by 1, tuid = segId</note>
        <docid>2018SACLS313</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Le travail présenté dans cette thèse explore les méthodes pratiques utilisées pour faciliter l'entraînement et améliorer les performances des modèles de langues munis de très grands vocabulaires.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This work investigates practical methods to ease training and improve performances of neural language models with large vocabularies.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>La principale limite à l'utilisation des modèles de langue neuronaux est leur coût computationnel : il dépend de la taille du vocabulaire avec laquelle il grandit linéairement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The main limitation of neural language models is their expensive computational cost: it depends on the size of the vocabulary, with which it grows linearly.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>La façon la plus aisée de réduire le temps de calcul de ces modèles reste de limiter la taille du vocabulaire, ce qui est loin d'être satisfaisant pour de nombreuses tâches.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Despite several training tricks, the most straightforward way to limit computation time is to limit the vocabulary size, which is not a satisfactory solution for numerous tasks.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>La plupart des méthodes existantes pour l'entraînement de ces modèles à grand vocabulaire évitent le calcul de la fonction de partition, qui est utilisée pour forcer la distribution de sortie du modèle à être normalisée en une distribution de probabilités.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Most of the existing methods used to train large-vocabulary language models revolve around avoiding the computation of the partition function, ensuring that output scores are normalized into a probability distribution.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Ici, nous nous concentrons sur les méthodes à base d'échantillonnage, dont le sampling par importance et l'estimation contrastive bruitée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Here, we focus on sampling-based approaches, including importance sampling and noise contrastive estimation.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Ces méthodes permettent de calculer facilement une approximation de cette fonction de partition.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These methods allow an approximate computation of the partition function.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>L'examen des mécanismes de l'estimation contrastive bruitée nous permet de proposer des solutions qui vont considérablement faciliter l'entraînement, ce que nous montrons expérimentalement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>After examining the mechanism of self-normalization in noise-contrastive estimation, we first propose to improve its efficiency with solutions that are adapted to the inner workings of the method and experimentally show that they considerably ease training.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Enfin, nous exploitons les informations données par les unités sous-mots pour enrichir les représentations en sortie du modèle.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Finally, we aim at improving performances on full vocabulary language models, by augmenting output words representation with subwords.</seg>
            </tuv>
        </tu>
    </body>
</tmx>