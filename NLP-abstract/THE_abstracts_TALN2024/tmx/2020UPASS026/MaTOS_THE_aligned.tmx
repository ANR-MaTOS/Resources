<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2020UPASS026. segId begin by 1, tuid = segId</note>
        <docid>2020UPASS026</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>La structure d'un réseau de neurones détermine dans une large mesure son coût d'entraînement et d'utilisation, ainsi que sa capacité à apprendre.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The structure of a neural network determines to a large extent its cost of training and use, as well as its ability to learn.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Ces deux aspects sont habituellement en compétition : plus un réseau de neurones est grand, mieux il remplira la tâche qui lui a été assignée, mais plus son entraînement nécessitera des ressources en mémoire et en temps de calcul.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These two aspects are usually in competition: the larger a neural network is, the better it will perform the task assigned to it, but the more it will require memory and computing time resources for training.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Dans ce contexte, des réseaux de neurones aux structures variées doivent être entraînés, ce qui nécessite un nouveau jeu d'hyperparamètres d'entraînement à chaque nouvelle structure testée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Within this context, neural networks with various structures are trained, which requires a new set of training hyperparameters for each new structure tested.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>L'objectif de la thèse est de traiter différents aspects de ce problème.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The aim of the thesis is to address different aspects of this problem.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>La première contribution est une méthode d'entraînement de réseau qui fonctionne dans un vaste périmètre de structures de réseaux et de tâches à accomplir, sans nécessité de régler le taux d'apprentissage.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The first contribution is a training method that operates within a large perimeter of network structures and tasks, without needing to adjust the learning rate.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>La deuxième contribution est une technique d'entraînement et d'élagage de réseau, conçue pour être insensible à la largeur initiale de celui-ci.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The second contribution is a network training and pruning technique, designed to be insensitive to the initial width of the network.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>La dernière contribution est principalement un théorème qui permet de traduire une pénalité d'entraînement empirique en a priori bayésien, théoriquement bien fondé.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The last contribution is mainly a theorem that makes possible to translate an empirical training penalty into a Bayesian prior, theoretically well founded.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Ce travail résulte d'une recherche des propriétés que doivent théoriquement vérifier les algorithmes d'entraînement et d'élagage pour être valables sur un vaste ensemble de réseaux de neurones et d'objectifs.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This work results from a search for properties that theoretically must be verified by training and pruning algorithms to be valid over a wide range of neural networks and objectives.</seg>
            </tuv>
        </tu>
    </body>
</tmx>