<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2016LORR0235. segId begin by 1, tuid = segId</note>
        <docid>2016LORR0235</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>De nos jours, avec l'abondance croissante de données de très grande taille, les problèmes de classification de grande dimension ont été mis en évidence comme un challenge dans la communauté d'apprentissage automatique et ont beaucoup attiré l'attention des chercheurs dans le domaine.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>These days with the increasing abundance of data with high dimensionality, high dimensional classification problems have been highlighted as a challenge in machine learning community and have attracted a great deal of attention from researchers in the field.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Au cours des dernières années, les techniques d'apprentissage avec la parcimonie et l'optimisation stochastique se sont prouvées être efficaces pour ce type de problèmes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In recent years, sparse and stochastic learning techniques have been proven to be useful for this kind of problem.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Dans cette thèse, nous nous concentrons sur le développement des méthodes d'optimisation pour résoudre certaines classes de problèmes concernant ces deux sujets.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this thesis, we focus on developing optimization approaches for solving some classes of optimization problems in these two topics.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Nos méthodes sont basées sur la programmation DC (Difference of Convex functions) et DCA (DC Algorithm) étant reconnues comme des outils puissants d'optimisation non convexe.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our methods are based on DC (Difference of Convex functions) programming and DCA (DC Algorithms) which are wellknown as one of the most powerful tools in optimization.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>La thèse est composée de trois parties.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The thesis is composed of three parts.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>La première partie aborde le problème de la sélection des variables.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The first part tackles the issue of variable selection.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>La deuxième partie étudie le problème de la sélection de groupes de variables.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The second part studies the problem of group variable selection.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>La dernière partie de la thèse liée à l'apprentissage stochastique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The final part of the thesis concerns the stochastic learning.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Dans la première partie, nous commençons par la sélection des variables dans le problème discriminant de Fisher (Chapitre 2) et le problème de scoring optimal (Chapitre 3), qui sont les deux approches différentes pour la classification supervisée dans l'espace de grande dimension, dans lequel le nombre de variables est beaucoup plus grand que le nombre d'observations.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the first part, we start with the variable selection in the Fisher's discriminant problem (Chapter 2) and the optimal scoring problem (Chapter 3), which are two different approaches for the supervised classification in the high dimensional setting, in which the number of features is much larger than the number of observations.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Poursuivant cette étude, nous étudions la structure du problème d'estimation de matrice de covariance parcimonieuse et fournissons les quatre algorithmes appropriés basés sur la programmation DC et DCA (Chapitre 4).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Continuing this study, we study the structure of the sparse covariance matrix estimation problem and propose four appropriate DCA based algorithms (Chapter 4).</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Deux applications en finance et en classification sont étudiées pour illustrer l'efficacité de nos méthodes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Two applications in finance and classification are conducted to illustrate the efficiency of our methods.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>La deuxième partie étudie la L_p,0régularisation pour la sélection de groupes de variables (Chapitre 5).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The second part studies the L_p,0regularization for the group variable selection (Chapter 5).</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>En utilisant une approximation DC de la L_p,0norme, nous prouvons que le problème approché, avec des paramètres appropriés, est équivalent au problème original.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Using a DC approximation of the L_p,0norm, we indicate that the approximate problem is equivalent to the original problem with suitable parameters.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Considérant deux reformulations équivalentes du problème approché, nous développons différents algorithmes basés sur la programmation DC et DCA pour les résoudre.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Considering two equivalent reformulations of the approximate problem we develop DCA based algorithms to solve them.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>Comme applications, nous mettons en pratique nos méthodes pour la sélection de groupes de variables dans les problèmes de scoring optimal et d'estimation de multiples matrices de covariance.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Regarding applications, we implement the proposed algorithms for group feature selection in optimal scoring problem and estimation problem of multiple covariance matrices.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>Dans la troisième partie de la thèse, nous introduisons un DCA stochastique pour des problèmes d'estimation des paramètres à grande échelle (Chapitre 6) dans lesquelles la fonction objectif est la somme d'une grande famille des fonctions non convexes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the third part of the thesis, we introduce a stochastic DCA for large scale parameter estimation problems (Chapter 6) in which the objective function is a large sum of nonconvex components.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>Comme une étude de cas, nous proposons un schéma DCA stochastique spécial pour le modèle loglinéaire incorporant des variables latentes</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>As an application, we propose a special stochastic DCA for the loglinear model incorporating latent variables</seg>
            </tuv>
        </tu>
    </body>
</tmx>