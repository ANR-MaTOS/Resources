<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2018GREAM031. segId begin by 1, tuid = segId</note>
        <docid>2018GREAM031</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>L’apprentissage faiblement supervisé cherche à réduire au minimum l’effort humain requis pour entrainer les modèles de l’état de l’art.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Weakly-supervised learning studies the problem of minimizing the amount of human effort required for training state-of-the-art models.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Toutefois, dans la pratique, les méthodes faiblement supervisées sont nettement moins efficaces que celles qui sont totalement supervisées.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>However, in practice weakly-supervised methods perform significantly worse than their fully-supervised counterparts.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Plus particulièrement, dans l’apprentissage profond, où les approches de vision par ordinateur sont les plus performantes, elles restent entièrement supervisées, ce qui limite leurs utilisations dans les applications du monde réel.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This is also the case in deep learning, where the top-performing computer vision approaches remain fully-supervised, which limits their usage in real world applications.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Cette thèse tente tout d’abord de combler le fossé entre les méthodes faiblement supervisées et entièrement supervisées en utilisant l’information de mouvement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This thesis attempts to bridge the gap between weakly-supervised and fully-supervised methods by utilizing motion information.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Puis étudie le problème de la segmentation des objets en mouvement, en proposant l’une des premières méthodes basées sur l’apprentissage pour cette tâche.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>It also studies the problem of moving object segmentation itself, proposing one of the first learning-based methods for this task.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>Le défi est de capturer de manières précises les bordures des objets et d’éviter les optimums locaux (ex : segmenter les parties les plus discriminantes).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This is especially challenging due to the need to precisely capture object boundaries and avoid local optima, as for example segmenting the most discriminative parts.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Contrairement à la plupart des approches de l’état de l’art, qui reposent sur des images statiques, nous utilisons les données vidéo avec le mouvement de l’objet comme informations importantes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In contrast to most of the state-of-the-art approaches, which rely on static images, we leverage video data with object motion as a strong cue.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Notre méthode utilise une approche de segmentation vidéo de l’état de l’art pour segmenter les objets en mouvement dans les vidéos.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In particular, our method uses a state-of-the-art video segmentation approach to segment moving objects in videos.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Les masques d’objets approximatifs produits par cette méthode sont ensuite fusionnés avec le modèle de segmentation sémantique appris dans un EM-like framework, afin d’inférer pour les trames vidéo, des labels sémantiques au niveau des pixels.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The approximate object masks produced by this method are then fused with the semantic segmentation model learned in an EM-like framework to infer pixel-level semantic labels for video frames.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Ainsi, au fur et à mesure que l’apprentissage progresse, la qualité des labels s’améliore automatiquement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Thus, as learning progresses, the quality of the labels improves automatically.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Nous intégrons ensuite cette architecture à notre approche basée sur l’apprentissage pour la segmentation de la vidéo afin d’obtenir un framework d’apprentissage complet pour l’apprentissage faiblement supervisé à partir de vidéos.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We then integrate this architecture with our learning-based approach for video segmentation to obtain a fully trainable framework for weakly-supervised learning from videos.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Dans la deuxième partie de la thèse, nous étudions la segmentation vidéo non supervisée, plus précisément comment segmenter tous les objets dans une vidéo qui se déplace indépendamment de la caméra.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the second part of the thesis we study unsupervised video segmentation, the task of segmenting all the objects in a video that move independently from the camera.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>De nombreux défis tels qu’un grand mouvement de la caméra, des inexactitudes dans l’estimation du flux optique et la discontinuité du mouvement, complexifient la tâche de segmentation.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This task presents challenges such as strong camera motion, inaccuracies in optical flow estimation and motion discontinuity.</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Nous abordons le problème du mouvement de caméra en proposant une méthode basée sur l’apprentissage pour la segmentation du mouvement : un réseau de neurones convolutif qui prend le flux optique comme entrée et qui est entraîné pour segmenter les objets qui se déplacent indépendamment de la caméra.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We address the camera motion problem by proposing a learning-based method for motion segmentation: a convolutional neural network that takes optical flow as input and is trained to segment objects that move independently from the camera.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>Il est ensuite étendu avec un flux d’apparence et un module de mémoire visuelle pour améliorer la continuité temporelle.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>It is then extended with an appearance stream and a visual memory module to improve temporal continuity.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>Le flux d’apparence tire profit de l’information sémantique qui est complémentaire de l’information de mouvement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The appearance stream capitalizes on the semantic information which is complementary to the motion information.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>Le module de mémoire visuelle est un paramètre clé de notre approche : il combine les sorties des flux de mouvement et d’apparence et agréger une représentation spatio-temporelle des objets en mouvement.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The visual memory module is the key component of our approach: it combines the outputs of the motion and appearance streams and aggregates a spatio-temporal representation of the moving objects.</seg>
            </tuv>
        </tu>
        <tu tuid="18">
            <tuv xml:lang="FR">
                <seg>La segmentation finale est ensuite produite à partir de cette représentation agrégée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The final segmentation is then produced based on this aggregated representation.</seg>
            </tuv>
        </tu>
        <tu tuid="19">
            <tuv xml:lang="FR">
                <seg>L’approche résultante obtient des performances de l’état de l’art sur plusieurs jeux de données de référence, surpassant la méthode d’apprentissage en profondeur et heuristique simultanée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The resulting approach obtains state-of-the-art performance on several benchmark datasets, outperforming the concurrent deep learning and heuristic-based methods.</seg>
            </tuv>
        </tu>
    </body>
</tmx>