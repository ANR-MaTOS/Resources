<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2020LORR0078. segId begin by 1, tuid = segId</note>
        <docid>2020LORR0078</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Les assistants vocaux font partie de notre vie quotidienne.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Voice based personal assistants are part of our daily lives.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Leurs performances sont mises à l'épreuve en présence de distorsions du signal, telles que le bruit, la réverbération et les locuteurs simultanés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Their performance suffers in the presence of signal distortions, such as noise, reverberation, and competing speakers.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Cette thèse aborde le problème de l'extraction du signal d'intérêt dans de telles conditions acoustiques difficiles en localisant d'abord le locuteur cible puis en utilisant la position spatiale pour extraire le signal de parole correspondant.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This thesis addresses the problem of extracting the signal of interest in such challenging conditions by first localizing the target speaker and using the location to extract the target speech.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Dans un premier temps, nous considérons la situation courante où le locuteur cible prononce un mot ou une phrase connue, comme le mot de réveil d'un système de commande vocale mains-libres.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In a first stage, a common situation is considered when the target speaker utters a known word or sentence such as the wake-up word of a distant-microphone voice command system.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Nous proposons une méthode afin d'exploiter cette information textuelle pour améliorer la localisation du locuteur en présence de locuteurs simultanés.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>A method that exploits this text information in order to improve the speaker localization performance in the presence of competing speakers is proposed.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>La solution proposée utilise un système de reconnaissance vocale pour aligner le mot de réveil au signal vocal corrompu.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The proposed solution uses a speech recognition system to align the wake-up word to the corrupted speech signal.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Un spectre de référence représentant les phones alignés est utilisé pour calculer un identifiant qui est ensuite utilisé par un réseau de neurones profond pour localiser le locuteur cible.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>A model spectrum representing the aligned phones is used to compute an identifier which is then used by a deep neural network to localize the target speaker.</seg>
            </tuv>
        </tu>
        <tu tuid="8">
            <tuv xml:lang="FR">
                <seg>Les résultats sur des données simulées montrent que la méthode proposée réduit le taux d'erreur de localisation par rapport à la méthode classique GCC-PHAT.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Results on simulated data show that the proposed method reduces the localization error rate compared to the classical GCC-PHAT method.</seg>
            </tuv>
        </tu>
        <tu tuid="9">
            <tuv xml:lang="FR">
                <seg>Des améliorations similaires sont constatées sur des données réelles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Similar improvements are observed on real data.</seg>
            </tuv>
        </tu>
        <tu tuid="10">
            <tuv xml:lang="FR">
                <seg>Étant donnée la position spatiale estimée du locuteur cible, la séparation de la parole est effectuée en trois étapes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Given the estimated location of the target speaker, speech separation is performed in three stages.</seg>
            </tuv>
        </tu>
        <tu tuid="11">
            <tuv xml:lang="FR">
                <seg>Dans la première étape, une simple formation de voie delay-and-sum (DS) est utilisée pour rehausser le signal provenant de cette direction, qui est utilisé dans la deuxième étape par un réseau de neurones pour estimer un masque temps-fréquence.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In the first stage, a simple delay-and-sum (DS) beamformer is used to enhance the signal impinging from that location which is then used in the second stage to estimate a time-frequency mask corresponding to the localized speaker using a neural network.</seg>
            </tuv>
        </tu>
        <tu tuid="12">
            <tuv xml:lang="FR">
                <seg>Ce masque est utilisé pour calculer les statistiques du second ordre et pour effectuer une formation de voie adaptative dans la troisième étape.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>This mask is used to compute the second-order statistics and to derive an adaptive beamformer in the third stage.</seg>
            </tuv>
        </tu>
        <tu tuid="13">
            <tuv xml:lang="FR">
                <seg>Un ensemble de données réverbéré, bruité avec plusieurs canaux et plusieurs locuteurs---inspiré du célèbre corpus WSJ0-2mix---a été généré et la performance de la méthode proposée a été étudiée en terme du taux d'erreur sur les mots (WER).</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>A multichannel, multispeaker, reverberated, noisy dataset---inspired from the famous WSJ0-2mix dataset---was generated and the performance of the proposed pipeline was investigated in terms of the word error rate (WER).</seg>
            </tuv>
        </tu>
        <tu tuid="14">
            <tuv xml:lang="FR">
                <seg>Pour rendre le système plus robuste aux erreurs de localisation, une approche par déflation guidée par la localisation (SLOGD) qui estime les sources de manière itérative est proposée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To make the system robust to localization errors, a Speaker LOcalization Guided Deflation (SLOGD) based approach which estimates the sources iteratively is proposed.</seg>
            </tuv>
        </tu>
        <tu tuid="15">
            <tuv xml:lang="FR">
                <seg>À chaque itération, la position spatiale d'un locuteur est estimée puis utilisée pour estimer un masque correspondant à ce même locuteur.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>At each iteration the location of one speaker is estimated and used to estimate a mask corresponding to that speaker.</seg>
            </tuv>
        </tu>
        <tu tuid="16">
            <tuv xml:lang="FR">
                <seg>La source estimée est retirée du mélange avant d'estimer la position et le masque de la source suivante.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The estimated source is removed from the mixture before estimating the location and mask of the next source.</seg>
            </tuv>
        </tu>
        <tu tuid="17">
            <tuv xml:lang="FR">
                <seg>La méthode proposée surpasse Conv-TasNet.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The proposed method is shown to outperform Conv-TasNet.</seg>
            </tuv>
        </tu>
        <tu tuid="18">
            <tuv xml:lang="FR">
                <seg>Enfin, le problème d'expliquer la robustesse des réseaux de neurones utilisés pour calculer les masques temps-fréquence à des conditions de bruit différentes.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Finally, we consider the problem of explaining the robustness of neural networks used to compute time-frequency masks to mismatched noise conditions.</seg>
            </tuv>
        </tu>
        <tu tuid="19">
            <tuv xml:lang="FR">
                <seg>Nous utilisons la méthode dite SHAP pour quantifier la contribution de chaque point temps-fréquence du signal d'entrée au masque temps-fréquence estimé.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We employ the so-called SHAP method to quantify the contribution of every time-frequency bin in the input signal to the estimated time-frequency mask.</seg>
            </tuv>
        </tu>
        <tu tuid="20">
            <tuv xml:lang="FR">
                <seg>Nous définissons une métrique qui résume les valeurs SHAP et montrons qu'elle est corrélée au WER obtenu sur la parole séparée.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>We define a metric that summarizes the SHAP values and show that it correlates with the WER achieved on separated speech.</seg>
            </tuv>
        </tu>
        <tu tuid="21">
            <tuv xml:lang="FR">
                <seg>À notre connaissance, il s'agit de la première étude sur l'explicabilité des réseaux de neurones dans le contexte de la séparation de la parole.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>To the best of our knowledge, this is the first known study on neural network explainability in the context of speech separation.</seg>
            </tuv>
        </tu>
    </body>
</tmx>