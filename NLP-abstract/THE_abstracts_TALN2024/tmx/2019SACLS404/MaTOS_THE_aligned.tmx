<?xml version='1.0' encoding='utf-8'?>
<tmx version="1.4b">
    <header creationtool="xml.etree.ElementTree" creationtoolversion="1.3.0" datatype="PlainText" segtype="sentence" adminlang="en-us" srclang="FR" o-tmf="XML" creationdate="2023-04-28" creationid="MaTOS">
        <note>This is the sentence alignement file for THE-theses.fr-2019SACLS404. segId begin by 1, tuid = segId</note>
        <docid>2019SACLS404</docid>
        <elem type="sourceLanguage">FR</elem>
        <elem type="targetLanguage">EN</elem>
    </header>
    <body>
        <tu tuid="1">
            <tuv xml:lang="FR">
                <seg>Les méthodes d’analyse rythmique existantes se concentrent généralement sur l'un de ces aspects à la fois et n’exploitent pas la richesse de la structure musicale, ce qui compromet la cohérence musicale des estimations automatiques.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Existing methods for rhythmic analysis typically focus on one of those levels, failing to exploit music’s rich structure and compromising the musical consistency of automatic estimations.</seg>
            </tuv>
        </tu>
        <tu tuid="2">
            <tuv xml:lang="FR">
                <seg>Dans ce travail, nous proposons de nouvelles approches tirant parti des informations multi-échelles pour l'analyse automatique du rythme.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>In this work, we propose novel approaches for leveraging multi-scale information for computational rhythm analysis.</seg>
            </tuv>
        </tu>
        <tu tuid="3">
            <tuv xml:lang="FR">
                <seg>Nos modèles prennent en compte des interdépendances intrinsèques aux signaux audio de musique, en permettant ainsi l’interaction entre différentes échelles de temps et en assurant la cohérence musicale entre elles.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our models account for interrelated dependencies that musical audio naturally conveys, allowing the interplay between different time scales and accounting for music coherence across them.</seg>
            </tuv>
        </tu>
        <tu tuid="4">
            <tuv xml:lang="FR">
                <seg>Ce système est conçu pour tirer parti des informations de structure musicale (c'est-à-dire des répétitions de sections musicales) dans un cadre unifié.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Our methods are systematically evaluated on a diverse group of datasets, ranging from Western music to more culturally specific genres, and compared to state-of-the-art systems and simpler variations.</seg>
            </tuv>
        </tu>
        <tu tuid="5">
            <tuv xml:lang="FR">
                <seg>Nous proposons également un modèle linguistique pour la détection conjointe des temps et du micro-timing dans la musique afro-latino-américaine.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The overall results show that our models for downbeat tracking perform on par with the state of the art, while being more musically consistent.</seg>
            </tuv>
        </tu>
        <tu tuid="6">
            <tuv xml:lang="FR">
                <seg>De plus, notre modèle d’estimation conjointe des temps et du microtiming représente une avancée vers des systèmes plus interprétables.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>Moreover, our model for the joint estimation of beats and microtiming takes further steps towards more interpretable systems.</seg>
            </tuv>
        </tu>
        <tu tuid="7">
            <tuv xml:lang="FR">
                <seg>Les méthodes présentées ici offrent des alternatives nouvelles et plus holistiques pour l'analyse numérique du rythme, ouvrant des perspectives vers une analyse automatique plus complète de la musique.</seg>
            </tuv>
            <tuv xml:lang="EN">
                <seg>The methods presented here offer novel and more holistic alternatives for computational rhythm analysis, towards a more comprehensive automatic analysis of music.</seg>
            </tuv>
        </tu>
    </body>
</tmx>