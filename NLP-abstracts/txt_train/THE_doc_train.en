With the unprecedented growth of user-generated content produced on microblogging platforms, finding interesting content for a given user has become a major issue. However due to the intrinsic properties of microblogging systems, such as the volumetry, the short lifetime of posts and the sparsity of interactions between users and content, recommender systems cannot rely on traditional methods, such as collaborative filtering matrix factorization. After a thorough study of a large Twitter dataset, we present a propagation model which relies on homophily to propose post recommendations. Our approach relies on the construction of a similarity graph based on retweet behaviors on top of the Twitter graph. We then conduct experiments on our real dataset to demonstrate the quality and scalability of our method. We find that, counter-intuitively, in most cases recommender systems tend to open users perspectives. However, for some specific users, the bubble effect is noticeable and we propose a model relying on communities to provide a list of recommendations closer to the user's usage of the platform.
Pattern recognition is a fundamental task for living beings and is perform very efficiently by the brain. Artificial deep neural networks are making quick progress in reproducing these performance and have many applications such as image recognition or natural language processing. However, they require extensive training on large datasets and heavy computations. A promising alternative are spiking neural networks, which closely mimic what happens in the brain, with spiking neurons and spike-timing dependent plasticity (STDP). They are able to perform unsupervised learning and have been used for visual or auditory pattern recognition. However, for now applications using STDP networks lag far behind classical deep learning. Developing new applications for this kind of networks is all the more at stake that they could be implemented in low power neuromorphic hardware that currently undergoes important developments, in particular with analog miniaturized memristive devices able to mimic synaptic plasticity. In this work, we chose to develop an STDP neural network to perform a specific task: spike-sorting, which is a crucial problem in neuroscience. Brain implants based on microelectrode arrays are able to record the activity of individual neurons, appearing in the recorded signal as peak potential variations called action potentials. Several neurons can be recorded by the same electrode. The goal of spike-sorting is to extract and separate the activity of different neural cells from a common extracellular recording taking advantage of the fact that the shape of an action potential on an electrode depends on the neuron it stems from. Most classical spike-sorting approaches use three separated steps: detecting all action potentials in the signal, extract features characterizing their shapes, and separating these features into clusters that should correspond to different neural cells. Though online methods exists, most widespread spike-sorting methods are offline or require an offline preprocessing step, which is not compatible with online application such as Brain-computer interfaces (BCI). Using an STDP network brings a new approach to meet these requirements. We designed a network that take the electrode signal as an input, and output spikes that correspond to the spiking activity of the recorded neural cells. It is organized into several layers, designed to achieve different processing steps, connected in feedforward way. The first layer, composed of neurons acting as sensory neurons, convert the input signal into spike train. The following layers are able to learn patterns from the previous layer thanks to STDP rules. Each layer implement different mechanisms that improve their performance, such as resource-dependent STDP, intrinsic plasticity, plasticity triggered by inhibition, or neuron models having rebound spiking properties. An attention mechanism has been implemented to make the network sensitive only to part of the signal containing action potentials. This network was first designed to process data from a single electrode, and then adapted to process data from multiple electrodes. It has been tested on simulated data, which allowed to compare the network output to the known ground truth, and also on real extracellular recordings associated with intracellular recordings that give an incomplete ground truth. Different versions of the network were evaluated and compared to other spike-sorting algorithms, and found to give very satisfying results. Following these software simulations, we initiated an FPGA implementation of the method, which constitutes a first step toward embedded neuromorphic implementation.
This study is intented to provide a first framework for the definition of an electronic dictionary of the Malagasy language to be used in the context of Natural Language Processing components. Our work is based, on the one hand, on a descriptive study of the Malagasy language, and, on the other hand, on the proposition of a computer representation of the corresponding phenomena. Following a preliminary chapter dedicated to a general presentation of the geographical and sociological context, the first part of the thesis presents a thorough analysis of Malagasian terms from the point of view of their morphology and syntactic behaviour. In particular, we describe the various voices and modalities of Malagasian utterances: tense, aspect and mood. The second part of the thesis comprises the formel representation that we associate to the linguistic data that have been previously described, so that a possible implementation can be derived in the form of a parser / generator of Malagasian predicative terms (AGTM). Our system can also provide all the possible forms that can be derived from a root, given a set of restrictions provided as input. The thesis comprises several appendixes which present the whole software implemented in Prolog, as well as sample results.
Emphasizing on the key role of polysemy in forming the lexicon is the main goal to be achieved in this dissertation paper. The paper suggests a qualitative evaluation of polysemy in comparing it with other relations that form the lexicon. The research confirms that the polysemic links must not be modeled independently from derivative links or conversational links. This evaluation leads us to reveal that the boundary between polysemy and conversation is porous. The properties of analogy has been used to compare the relations, which is well adapted in characterizing the links between the relations. They are the links that connect lexis which form the objects of a comparison. A polysemic link is a link by which two lexis are connected to each other in a polysemic relation. This link can be compared to a link that connects two other lexis in a conversional relation. In this paper, Wolof, an Atlantic language in West-Africa, is studied. This language provides a fertile breeding ground for our explorations.
Modeling natural language is among fundamental challenges of artificial intelligence and the design of interactive machines, with applications spanning across various domains, such as dialogue systems, text generation and machine translation. We propose a discriminatively trained log-linear model to learn the distribution of words following a given context. The outcome is an efficient model that suitably captures long dependencies in language without a significant increase in time or space requirements. In a log-linear model, both training and testing become increasingly expensive with growing number of classes. The number of classes in a language model is the size of the vocabulary which is typically very large. A common trick is to cluster classes and apply the model in two-steps; the first step picks the most probable cluster and the second picks the most probable word from the chosen cluster. This idea can be generalized to a hierarchy of larger depth with multiple levels of clustering. However, the performance of the resulting hierarchical classifier depends on the suitability of the clustering to the problem. We study different strategies to build the hierarchy of categories from their observations.
This thesis develops a theoretical formalism that takes into account semantical discourse dynamics. It focuses on the extension of Montague semantic with the notion of continuation and an exception handling and raising mechanism. The formalism allows to handle dynamic phenomena such as cross-sentential anaphora, presuppositions triggered by referring expressions and presupposition projection.
Starting from the postulate that this French political party is now anchored in the system and, paradoxically, claims to be an 'anti-system'party, we study the speech construction of this opposition. To carry out this research, we have used the Hyperbase software, text data analysis platform, to create a vast corpus over three million occurrences structured in five databases. We have thus applied our tools and our analysis to more than 300 Lepenian speeches and three French presidential campaigns. They progress from an infra-textual analysis, centred on the study of the lexical and syntactic specificities of the FNF discourse, to a textual analysis devoted to the inter- and supra-phrastic cohesion of the Lepenian textuality, in order to arrive at the discursive organization and the relations that the FNF discourse locally and globally poses to other discourses.
The aim of this thesis is to carry out lexical analysis of written texts in Lithuanian by automatic means, according to a heuristics from form to content based on symbolic methods. This study attempts to make an expanded use of marks given by linguistic forms, drawing on graphemic and morphological aspects. This formal starting point in conjunction with automation of linguistic tasks required a revision of the traditional grammatical point of view, concerning mainly parts of speech, lexical structure and suffixation. This linguistic model, which needs further expansion, served as a basis for ALeksas, an analyzer of lexical forms. This software implements a hybrid structure expanding a system of finite state automata. The prototype computes the analysis of word forms, giving grammatical interpretations according to a set of formal criteria, instead of making use of a lexical database. The results of the analysis of a corpus complied from various texts allowed us to delineate more precisely the advantages and shortcomings of Aleksas, as compared with other similar tools, and to also suggest possible enhancements.
Historically, Probabilistic Graphical Models (PGMs) are a solution for learning from uncertain and flat data, also called propositional data or attributevalue representations. In the early 2000s, great interest was addressed to the processing of relational data which includes a large number of objects participating in different relations. Probabilistic Relational Models (PRMs) present an extension of PGMs to the relational context. With the rise of the internet, numerous technological innovations and web applications are driving the dramatic increase of various and complex data. Consequently, Big Data has emerged. However, all PRMs structure learning use wellstructured data that are stored in relational databases. Graph databases are unstructured and schema-free data stores. Edges between nodes can have various signatures. Since, relationships that do not correspond to an ER model could be depicted in the database instance. These relationships are considered as exceptions. In this thesis, we are interested by this type of data stores. Also, we study two kinds of PRMs namely, Direct Acyclic Probabilistic Entity Relationship (DAPER) and Markov Logic Networks (MLNs). We propose two significant contributions. First, an approach to learn DAPERs from partially structured graph databases. A second approach consists to benefit from first-order logic to learn DAPERs using MLN framework to take into account the exceptions that are dropped during DAPER learning. We are conducting experimental studies to compare our proposed methods with existing approaches.
This dissertation is composed of three distinct studies that empirically examine the role of the information disclosed on goodwill impairment for key firm stakeholders (i.e., financial analysts, peer firms, and external auditors). In the first study, I examine the effect of disclosure transparency on disagreement among analysts, and disagreement between analysts and managers, in the context of goodwill impairment. The second study examines whether the reporting of significant goodwill impairment by a firm (impairment firm) affects the corporate investment behavior of other firms in the same industry (peer firms). The third study investigates the impact of the expanded audit report disclosure on firms' financial disclosure decisions. Specifically, I examine whether firms adjust the levels of disclosure on goodwill impairment when auditors flag goodwill impairment as a risk of material misstatements in the expanded audit report. This study contributes to the debate about the usefulness of the expanded audit report by identifying the mechanism through which expanded audit report impacts financial reporting and corporate decisions.
Dysarthria is a speech disorder resulting from neurological impairments of the speech motor control. It can be caused by different pathologies (Parkinson's disease, A myotrophic Lateral Sclerosis - ALS, etc.) and affects different levels of speech production (respiratory,laryngeal and supra-laryngeal). The majority of research work dedicated to the study of dysarthric speech relies on perceptual analyses. The most known study, by F. L. Darley in 1969, led to the organization and the classification of dysarthria within 6 classes (completed with 2 additional classes in 2005). Nowadays, perceptual evaluation is still the most used method in clinical practice for the diagnosis and the therapeutic monitoring of patients. However, this method is known to be subjective, non reproductive and time-consuming. These limitations make it inadequate for the evaluation of large corpora (in case of phonetic studies) or for the follow-up of the progression of the condition of dysarthric patients. The work presented in this document falls within this framework and studies the contributions that these tools can have in the evaluation of dysarthric, and more generally pathological speech. In this work, an automatic approach for the detection of abnormal phones in dysarthric speech is proposed and its behavior is analyzed on different speech corpora containing different pathologies, dysarthric classes, dysarthria severity levels and speech styles (read and spontaneous speech). Unlike the majority of the automatic methods proposed in the literature that provide a global evaluation of the speech on general items such as dysarthria severity, intelligibility, etc., our proposed method focuses on the phone level aiming to achieve a better characterization of dysarthria effects and to provide a precise and useful feedback to the potential users (clinicians, phoneticians,patients). This method consists on two essential phases: (1) an automatic phone alignment of the speech (2) an automatic classification of the resulting phones in two classes:normal and abnormal phones. When compared to an annotation of phone anomalies provided by a human expert considered to be the "gold standard", the approach showed encouraging results and proved to be able to detect anomalies on the phone level. The approach was also able to capture the evolution of the severity of the dysarthria suggesting a potential relevance and use in the longitudinal follow-up of dysarthric patients or for the automatic prediction of their intelligibility or the severity of their dysarthria. Also, the automatic phone alignment precision was found to be dependent on the severity,the pathology, the class of the dysarthria and the phonetic category of each phone. Furthermore, the speech style was found to have an interesting effect on the behaviors of both automatic phone alignment and anomaly detection. Finally, the results of an evaluation campaign conducted by a jury of experts on the annotations provided by the proposed approach are presented and discussed in order to draw a panel of the strengths and limitations of the system.
Initially created for a reimbursement purpose, non-clinical claim databases are exhaustive Electronic Health Records (EHRs) which are particularly valuable for evidence-based studies. New process models and an adapted process discovery algorithm are introduced, with the objective of accurately model characteristic transitions and time hidden in non-clinical claims data. The second contribution is a preprocessing solution to handle one complexity of such data, which is the representation of medical events by multiple codes belonging to different standard coding systems, organized in hierarchical structures. The proposed method uses auto-encoders and clustering in an adequate latent space to automatically produce relevant and explainable labels. From these contributions, an optimization-based predictive method is introduced, which uses a process model to perform binary classification from event logs and highlight distinctive patterns as a global explanation. A second predictive method is also proposed, which uses images to represent patient pathways and a modified Variational Auto-Encoders (VAE) to predict. This method globally explains predictions by showing an image of identified predictive factors which can be both frequent and infrequent.
With the rise of Big Data, the processing of Volume, Velocity (growth and evolution) and data Variety concentrates the efforts of communities to exploit these new resources. These new resources have become so important that they are considered the new "black gold". In recent years, volume and velocity have been aspects of the data that are controlled, unlike variety, which remains a major challenge. This thesis presents two contributions in the field of heterogeneous data matching, with a focus on the spatial dimension. The first contribution is based on a two-step process for matching heterogeneous textual data: georepresentation and geomatching. In the first phase, we propose to represent the spatial dimension of each document in a corpus through a dedicated structure, the Spatial Textual Representation (STR). This graph representation is composed of the spatial entities identified in the document, as well as the spatial relationships they maintain. To identify the spatial entities of a document and their spatial relationships, we propose a dedicated resource, called Geodict. The second phase, geomatching, computes the similarity between the generated representations (STR). To assess the relevance of a match, we propose a set of 6 criteria based on a definition of the spatial similarity between two documents. The second contribution is based on the thematic dimension of textual data and its participation in the spatial matching process. We propose to identify the themes that appear in the same contextual window as certain spatial entities. The objective is to induce some of the implicit spatial similarities between the documents. To do this, we propose to extend the structure of STR using two concepts: the thematic entity and the thematic relationship. The thematic entity represents a concept specific to a particular field (agronomic, medical) and represented according to different spellings present in a terminology resource, in this case a vocabulary. A thematic relationship links a spatial entity to a thematic entity if they appear in the same window. The selected vocabularies and the new form of STR integrating the thematic dimension are evaluated according to their coverage on the studied corpora, as well as their contributions to the heterogeneous textual matching process on the spatial dimension.
This thesis explores the use of structured losses in two different domains. In the first contribution, we focus on multi-agent reinforcement learning (MARL), in environments that can be separated into several loosely coupled tasks. We set out to find policies that can generalize well to more agents and tasks than seen during training, effectively scaling up the size of problems that can be tackled. Our solution assigns agents to tasks by approximately solving a centralized optimization problem whose objective function is parameterized by a neural network. We study how the expressivity of the optimization problem and that of the neural network influence the generalization capabilities of the model, and show that with the right choices, the policy can generalize to more than 5 times more agents than seen during training. In the second contribution we formulate object detection as a set prediction problem, and design a model that can effectively tackle this formulation. Our solution leverages a deep convolutional network, as is customary in computer vision, and a transformer encoder-decoder network, an architecture that has enabled significant progress in natural language processing. Crucially, our solution incorporates minimal inductive bias, thereby alleviating the need for hand-designed detection-specific components such as anchors or non-maximal suppression. With a comparable parameter budget, our model matches the performance of well-established and highly-optimized baselines such as Retinanet and Faster R-CNN on the challenging COCO detection dataset. Finally, we show that the method can be naturally extended to perform panoptic segmentation, where it outperforms competing approaches, thus showing the versatility of the model.
The diarization task tries to determine the number of speakers as well as their interventions in an audio file. It is an interesting task for any enterprise willing to index its audiovisual contents. Especially, the French National Audiovisual Institute (INA) desires to apply this task on its archives so as to improve its accessibility and its annotation. However, the uses of the institute need a minimal quality which, most of the time, is not reached by the state-of-the-art automatic diarization systems yet. In order to reach the wanted effectiveness, a human can correct the output of a diarization system. Nevertheless, a human intervention is generally time-consuming and expensive. In order to reduce these costs, a possible solution is to use a computer-assisted system: a human gives some information to a system in order that it can improve its predictions so as to decrease its intervention cost. The present manuscript revolves around the computer-assisted diarization. It proposes a metric so as to assess the human intervention cost to correct a diarization, a framework to evaluate the human corrections of a speaker diarization, an automaton simulating the human corrections to do for a diarization and some computer-assisted diarization systems decreasing the total human intervention cost. More precisely, the proposed computer-assisted diarization systems reassess either only the speaker clustering or the segmentation and the speaker clustering.
They have given rise to numerous studies in Natural Language Processing. Indeed, their study and precise identification are essential, both from a theoretical and applicative perspective. However, most of the researches about the subject relate to everyday uses of language: "small talk" dialogs, requests for schedule, speeches, etc. But what about spontaneous speech production made in a restrained framework? To our knowledge, no study has ever been carried out in this context. However, we know that using a "language specialty" in the framework of a given task leads to specific behaviours. Our thesis work is devoted to the linguistic and computational study of disfluencies within such a framework. These dialogs concern air traffic control, which entails both pragmatic and linguistic constraints. We carry out an exhaustive study of disfluencies phenomena in this context. At first we conduct a subtle analysis of these phenomena. Then we model them to a level of abstraction, which allows us to obtain the patterns corresponding to the different configurations observed. Finally we propose a methodology for automatic processing. It consists of several algorithms to identify the different phenomena, even in the absence of explicit markers. It is integrated into a system of automatic processing of speech. Eventually, the methodology is validated on a corpus of 400 sentences.
Driven with the objective of rendering robots as socio-communicative, there has been a heightened interest towards researching techniques to endow robots with social skills and "commonsense" to render them acceptable. This social intelligence or ``commonsense'' of the robot is what eventually determines its social acceptability in the long run. Commonsense, however, is not that common. Robots can, thus, only learn to be acceptable with experience. However, teaching a humanoid the subtleties of a social interaction is not evident. Even a standard dialogue exchange integrates the widest possible panel of signs which intervene in the communication and are difficult to codify (synchronization between the expression of the body, the face, the tone of the voice, etc.). In such a scenario, learning the behavioral model of the robot is a promising approach. This learning can be performed with the help of AI techniques. This study tries to solve the problem of learning robot behavioral models in the Automated Planning and Scheduling (APS) paradigm of AI. In the domain of Automated Planning and Scheduling (APS), intelligent agents by virtue require an action model (blueprints of actions whose interleaved executions effectuates transitions of the system state) in order to plan and solve real world problems. During the course of this thesis, we introduce two new learning systems which facilitate the learning of action models, and extend the scope of these new systems to learn robot behavioral models. These techniques can be classified into the categories of non-optimal and optimal. Non-optimal techniques are more classical in the domain, have been worked upon for years, and are symbolic in nature. However, they have their share of quirks, resulting in a less-than-desired learning rate. The optimal techniques are pivoted on the recent advances in deep learning, in particular the Long Short Term Memory (LSTM) family of recurrent neural networks. These techniques are more cutting edge by virtue, and produce higher learning rates as well. This study brings into the limelight these two aforementioned techniques which are tested on AI benchmarks to evaluate their prowess. They are then applied to HRI traces to estimate the quality of the learnt robot behavioral model. This is in the interest of a long term objective to introduce behavioral autonomy in robots, such that they can communicate autonomously with humans without the need of ``wizard'' intervention.
A corpus as vast and rich as that of the tale in the oral tradition of the province of Valladolid justifies the creation of a lexicostatistic tool. Computer analysis makes it possible to visualize precise and exhaustive data and to bring to the fore the various lexical units used in tales taken from the works of Joaquin DIAZ and from those of Aurelio M. ESPINOSA (hijo). This study first dwells on the particular regional context and defines the terminological apparatus. It then consists in the creation of a lexical data base -made possible by the processing of the whole corpus-derived from a large terminological body organized into several lexicosemantic categories; in the counting and representing by graphs of the data obtained; and finally in the interpretation of the results, registered and measured according to a sociolinguistic and sociocultural approach. In the end, this study allows the classification of tales by a new method of textual processing.
In formal semantics, researchers assign meanings to sentences of a natural language. This work is guided by the principle of compositionality: the meaning of an expression is a function of the meanings of its parts. These functions are often formalized using the [lambda]-calculus. However, there are areas of language which challenge the notion of compositionality, e.g. anaphoric pronouns or presupposition triggers. These force researchers to either abandon compositionality or adjust the structure of meanings. In the first case, meanings are derived by processes that no longer correspond to pure mathematical functions but rather to context-sensitive procedures, much like the functions of a programming language that manipulate their context with side effects. In the second case, when the structure of meanings is adjusted, the new meanings tend to be instances of the same mathematical structure, the monad. Monads themselves being widely used in functional programming to encode side effects, the common theme that emerges in both approaches is the introduction of side effects. Furthermore, different problems in semantics lead to different theories which are challenging to unite. Our thesis claims that by looking at these theories as theories of side effects, we can reuse results from programming language research to combine them. In the first part of the thesis, we prove some of the fundamental properties of this calculus: subject reduction, confluence and termination. Then in the second part, we demonstrate how to use the calculus to implement treatments of several linguistic phenomena: deixis, quantification, conventional implicature, anaphora and presupposition. In the end, we build a grammar that features all of these phenomena and their interactions.
The advent of online platforms such as weblogs and social networking sites provided Internet users with an unprecedented means to express their opinions on a wide range of topics, including policy and commercial products. This large volume of opinionated data can be explored and exploited through text mining techniques known as opinion mining or sentiment analysis. Contrarily to traditional opinion mining work which mostly focuses on positive and negative opinions (or an intermediate in-between), we study a more challenging type of opinions: viewpoints. Viewpoint mining reaches beyond polarity-based opinions (positive/negative) and enables the analysis of more subtle opinions such as political opinions. In our first contribution, we explored the idea of separating opinion words (specific to both viewpoints and topics) from topical, neutral words based on parts of speech, inspired by similar practices in the litterature of non viewpoint-related opinion mining. Our second contribution tackles viewpoints expressed by social network users. We aimed to study to what extent social interactions between users – in addition to text content – can be beneficial to identify users'viewpoints. Our different contributions were evaluated and benchmarked against state-of-the-art baselines on real-world datasets
Action recognition in videos is one of the key problems in visual data interpretation. Despite intensive research, differencing and recognizing similar actions remains a challenge. This thesis deals with fine-grained classification of sport gestures from videos, with an application to table tennis. In this manuscript, we propose a method based on deep learning for automatically segmenting and classifying table tennis strokes in videos. For developing such a system with fine-grained classification, a very specific dataset is needed to supervise the learning process. To that aim, we built the “TTStroke-21” dataset, which is composed of 20 stroke classes plus a rejection class. These recorded sessions were annotated by professional players or teachers using a crowdsourced annotation platform. The annotations consist in a description of the handedness of the player and information for each stroke performed (starting and ending frames, class of the stroke).Fine-grained action recognition has some notable differences with coarse-grained action recognition. In general, datasets used for coarse-grained action recognition, the background context often provides discriminative information that methods can use to classify the action, rather than focusing on the action itself. In this thesis, we introduce a Twin Spatio-Temporal Convolutional Neural Network. This deep learning network takes as inputs an RGB image sequence and its computed Optical Flow. The RGB image sequence allows our model to capture appearance features while the optical flow captures motion features. Those two streams are processed in parallel using 3D convolutions, and fused at the last stage of the network. Our method gets an average classification performance of 87.3% with a best run of 93.2% accuracy on the test set. When applied on joint detection and classification task, the proposed method reaches an accuracy of 82.6%.A systematic study of the influence of each stream and fusion types on classification accuracy has been performed, giving clues on how to obtain the best performances. A comparison of different optical flow methods and the role of their normalization on the classification score is also done. The extracted features are also analyzed by back-tracing strong features from the last convolutional layer to understand the decision path of the trained model. Finally, we introduce an attention mechanism to help the model focusing on particular characteristic features and also to speed up the training process. For comparison purposes, we provide performances of other methods on TTStroke-21 and test our model on other datasets. We notice that models performing well on coarse-grained action datasets do not always perform well on our fine-grained action dataset. The research presented in this manuscript was validated with publications in one international journal, five international conference papers, two international workshop papers and a reconductible task in MediaEval workshop in which participants can apply their action recognition methods to TTStroke-21. Two additional international workshop papers are in process along with one book chapter.
With our thesis, we intend to lay out the linguistic and didactic foundations necessary for the future elaboration of a program or a method in Slavic intercomprehension by taking the example of the Western and the South-Western Slavic languages and in providing a linguistic analysis of three languages: Czech, Slovene and Croatian. In our work, we seek mainly to provide two elements: - A series of linguistic hypotheses aimed at determining the points to be taught in an intercomprehension method concerning Czech, Slovene and Croatian; - A presentation of programs and support in intercomprehension didactics realized and tested as part of our curriculum. In our work, we find that the didactics of Slavic intercomprehension differs in many ways from classical learning. In the case of intercomprehension, many points that are normally heavy and complex to master may be only passed through quickly. Thanks to our linguistical and didactical analyzes, we have been able to provide a reflection on one of the forms that Slavic intercomprehension formation can take in the future. We particularly recommend the use of online resources, for example via the website www.rozrazum.eu, developed as a part of this thesis to test activities following the methodology made for Eurom 5 (Bonvino et al., 2001). This website can initially be used as a test and development platform for didactical approaches, while being functional, and therefore available to a public of learners.
Epidemic intelligence aims to detect, investigate and monitor potential health threats while relying on formal (e.g. official health authorities) and informal (e.g. media) information sources. Monitoring of unofficial sources, or so-called event-based surveillance (EBS), requires the development of systems designed to retrieve and process unstructured textual data published online. The first objective of this thesis is to propose and compare approaches to enhance the identification and extraction of relevant epidemiological information from the content of online news. This manuscript proposes new textual representation approaches by selecting, expanding, and combining relevant epidemiological features. We show that adapting and extending text mining and classification methods improves the added value of online news sources for event-based surveillance. We stress the role of domain expert knowledge regarding the relevance and the interpretability of methods proposed in this thesis. While our researches are conducted in the context of animal disease surveillance, we discuss the generic aspects of our approaches regarding unknown threats and One Health surveillance.
Machine Translation is one of the most difficult tasks in natural language and speech processing. The linguistic peculiarities of some languages makes the machine translation task more difficult. In this thesis, we present a detailed study of machine translation systems from arabic to french and to english. Our principle researches carry on building parallel corpora, arabic preprocessing and adapting translation and language models. We propose a method for automatic extraction of parallel news corpora from a comparable corpora. Two approaches for translation model adaptation are explored using whether parallel corpora extracted automatically or parallel corpora constructed automatically. We demonstrate that adapting data used to build machine translation system improves translation. Arabic texts have to be preprocessed before machine translation and this because of the agglutinative character of arabic language. A prepocessing tool for arabic, SAPA (Segmentor and Part-of-speech tagger for Arabic), much faster than the state of the art tools and totally independant of any other external resource was developed. This tool predicts simultaneously morphosyntactic tags and proclitics (conjunctions, prepositions, etc.) for every word, then splits off words into lemma and proclitics. We describe also in this thesis, our named entity recognition tool for arabic, NERAr, and we focus on the impact of integrating named entity recognition in the preprocessing task. We used bilingual dictionaries to propose translations of the detected named entities. We present then many approaches to adapt thematically translation and language models using a corpora consists of a set of multicategoric sentences. These experiments open important research perspectives such as combining many systems when translating. It would be interesting also to focus on a temporal adaptation of translation and language models. Finally, improved machine translation systems from arabic to french and english are integrated in a multimedia platform analysis and shows improvements compared to basic machine translation systems.
The understanding of Vietnamese consumer behaviors toward brands is crucial for not only local but also foreign marketers to be prepared for the competition in the Vietnamese market. The investigation is based on the theories of brand personality, antecedents (self-congruence and partner quality), consequences (WTP, consideration set size, and WOM), hot and cold BRQ, and brand purchase intention. The aim behind of this research can be summed up in the following objective: firstly, to determine the effects of brand personality on antecedents of two components of BRQ in the context of Vietnam; secondly, to investigate the impacts of antecedents and consequences of two components of BRQ on brand purchase intention in the context of Vietnam. A structure model was developed illustrating the relationships (assumed) between brand personality on antecedents and consequences of brand relationship quality (BRQ). This resulted in the developed of twenty hypotheses. To address the research aims, data were collected which focused on six product classes and 634 questionnaires were collected in final. First, the results of our findings reveal that brand personality has a positive influence on two variables self-congruence and partner quality, but it is clearly seen that there is a different level of influence and importance. Secondly, given that self-congruence is a more significant effect on hot than cold BRQ, on the other hand, partner quality is a more significant effect on cold than hot BRQ. However, based on the path coefficient of self-congruence and partner quality, the results reveal that self-congruence has a positive significant effect on both hot and cold BRQ compared to partner quality. We found that consideration set size and WOM have no relationship with brand purchase intention, while WTP has a positive significant effect on brand purchase intention. The key contributions of this research provide a better understanding consumer behavior in the Vietnamese market. The findings of our study show that hot BRQ has been shown to have a stronger and significant influence on consumer's WTP. Cold BRQ, however, was found to strongly impact the consumer's WOM. Therefore, hot BRQ, which is the emotional relationship quality, mainly increases the loyalty behavior of customers; in contrast, cold BRQ helps to attract new customers by positively word-of-mouth communication of customers. Both the retention of current customers and the attraction of news customers are crucial drivers for the sustainable future of a brand or a product. Managers need, therefore, try to positively impact both hot and cold BRQ of their customers. Furthermore, based on the research results, they should focus on a willingness to pay price premium in order to increase their brand purchasing intention.
The genome of bacteria is classically separated into essential, stable and slow evolving replicons (chromosomes) and accessory, mobile and rapidly evolving replicons (plasmids). This paradigm is being questioned since the discovery of extra-chromosomal essential replicons (ECERs), be they called ”megaplasmids”, ”secondary chromosomes” or ”chromids”, which possess both chromosomal and plasmidic features. However, their true nature and the mechanisms permitting their integration within the sable genome are yet to be formally determined. The relationships between replicons, with reference to their genetic information inheritance systems (GIIS), were explored under the assumption that the inheritance of ECERs is integrated to the cell cycle and highly constrained in contrast to that of standard plasmids.
The recent advances of Information and Communication Technology (ICT) have resulted in the development of several industries. Adopting semantic technologies has proven several benefits for enabling a better representation of the data and empowering reasoning capabilities over it, especially within an Information Retrieval (IR) application. This has, however, few applications in the industries as there are still unresolved issues, such as the shift from heterogeneous interdependent documents to semantic data models and the representation of the search results while considering relevant contextual information. In this thesis, we address two main challenges. The second one focuses on providing users with innovative search results, from the heterogeneous document corpus, helping the users in interpreting the information that is relevant to their inquiries and tracking cross document dependencies. To cope with these challenges, we first propose a semantic representation of a heterogeneous document corpus that generates a semantic graph covering both the structural and the domain-specific dimensions of the corpus. Then, we introduce a novel data structure for query answers, extracted from this graph, which embeds core information together with structural-based and domain-specific context. In order to provide such query answers, we propose an innovative query processing pipeline, which involves query interpretation, search, ranking, and presentation modules, with a focus on the search and ranking modules. However, in this thesis, it has been experimented in the Architecture, Engineering and Construction (AEC) industry using real-world construction projects.
The presented thesis deals with the adaptation of the conversion of a written text into speech using a parametric approach to the Arabic language. Different methods have been developed in order to set up synthesis systems. These methods are based on a description of the speech signal by a set of parameters. Besides, each sound is represented by a set of contextual features containing all the information affecting the pronunciation of this sound. Part of these features depend on the language and its peculiarities, so in order to adapt the parametric synthesis approach to Arabic, a study of its phonological peculiarities wasneeded. Two phenomena were identified: the gemination and the vowels quantity (short/ long). Two features associated to these phenomena have been added to the contextual features set. Four combinations of modeling are possible: alternating the differentiation or fusion of simple and geminated consonants on the one hand and short and long vowels on the other hand. A set of perceptual and objective tests was conducted to evaluate the effect of the fourunit modelling approaches on the quality of the generated speech. The evaluations were made in the case of parametric synthesis by HMM then in the case of parametric synthesisby DNN. The subjective results showed that when the HMM approach is used, the four approaches produce signals with a similar quality, this result that was confirmed by the objective measures calculated to evaluate the prediction of the durations of the speech units. However, the results of objective evaluations in the case of the DNN approach have shown that the differentiation of simple consonants (respectively short vowels) geminated consonants (respectively long vowels) leads to a slightly better prediction of the durations than the other modelling approaches. The last part of this thesis was devoted to the comparison of the synthesis approach by the HMMs to that by the DNNs. All the tests conducted have shown that the use of DNNs has improved the perceived quality of the generated signals.
This "syntactic memory" is built from experience and particularly from the observation of sequences of objects whose organization obeys syntactic rules. It must have the capability to aid recognizing as well as generating valid sequences in the future, i.e., sequences respecting the learnt rules. This production of valid sequences can be done either in an explicit way, that is, by evoking the underlying rules, or implicitly, when the learning phase has made it possible to capture the principle of organization of the sequences without explicit recourse to the rules. Although the latter is faster, more robust and less expensive in terms of cognitive load as compared to explicit reasoning, the implicit process has the disadvantage of not giving access to the rules and thus becoming less flexible and less explicable. At first, the expert makes a choice to explicitly follow the rules of the trade. But then, by dint of repetition, the choice is made automatically, without explicit evocation of the underlying rules. This change in encoding rules in an individual in general and particularly in a business expert can be problematic when it is necessary to explain or transmit his or her knowledge. Indeed, if the business concepts can be formalized, it is usually in any other way for the expertise which is more difficult to extract and transmit. In our work, we endeavor to observe sequences of electrical components and in particular the problem of extracting rules hidden in these sequences, which are an important aspect of the extraction of business expertise from technical drawings. We place ourselves in the connectionist domain, and we have particularly considered neuronal models capable of processing sequences. We have evaluated these two models on different artificial grammars (Reber's grammar and its variations) in terms of learning, their generalization abilities and their management of sequential dependencies. Finally, we have also shown that it is possible to extract the encoded rules (from the sequences) in the recurrent network with LSTM units, in the form of an automaton.
These systems are trained on large amount of data, and big data is now considered the « New Oil of the 21st century ». AI systems can recognize patterns and regularities in data that humans can't, due to their memory limits. We therefore propose to study the links between training data and performances of automatic speech recognition technology, with a focus on the gender distribution. We choose to study speech as it now the new interface for human-machine interaction. We aim at creating a continuum instead of class in order to describe corpus no longer in terms of gender, but in terms of vocal diversity. This representation will allow us to highlight the vocal variability as well as questioning the pertinence of gender as a categorical distinction, alongside a sociophonetic study of speaker roles and interactions.
Even if the concept of decentralization is embedded to some extent at the very core of the Internet, today's “network of networks” integrates this principle only partially. In this quest, a number of developers look back to the evergreen qualities of a relatively old technology, peer-to-peer (P2P), that leverages the socio-technical resources of the network's "dwarfs"-its periphery or "edge"-in a way that is, in fact, closer to the pre-commercial Internet. This dissertation explores the distributed and decentralized approach to the technical architecture of Internet-based services. It illustrates the co-shaping of a decentralized network architecture and of several different dynamics: the articulation between actors and contents, the allocation of responsibilities and the capacity to exert control, the organization of the market, the forms of existence and role of entities such as the nodes of a network, its users, its central or coordinating units. This work analyses the conditions under which a network that structures itself according to a non-hierarchical or hybrid model, and delegates the responsibility of its functioning to its edge, can develop and thrive in today's Internet. The dissertation follows the developers of three Internet services-a search engine, a storage service and a video streaming application-built on primarily decentralized network models; it also follows the collectives of pioneer users developing with these services, and selectively, the political venues where the Internet's medium-and long-term organization and governance are discussed.
This thesis contributes to the study of reliability and safety of computer and software systems which are modeled as discrete event systems. The major contributions include the theory of Control Systems (C Systems) and the model monitoring approach. In the first part of the thesis, we study the theory of control systems which combines and significantly extends regulated rewriting in formal languages theory and supervisory control. The control system is a generic framework, and contains two components: the controlled component and the controlling component that restricts the behavior of the controlled component. The two components are expressed using the same formalism, e.g., automata or grammars. We consider various classes of control systems based on different formalisms, for example, automaton control systems, grammar control systems, and their infinite versions and concurrent variants. After that, an application of the theory is presented. The Büchi automata based control system is used to model and check correctness properties on execution traces specified by nevertrace claims. In the second part of the thesis, we investigate the model monitoring approach whose theoretical foundation is the theory of control systems. The key principle of the approach is “property specifications as controllers”. In other words, the functional requirements and property specification of a system are separately modeled and implemented, and the latter one controls the behavior of the former one. The model monitoring approach contains two alternative techniques, namely model monitoring and model generating. The approach can be applied in several ways to improve reliability and safety of various classes of systems. We present some typical applications to show its strong power. First, the approach provides better support for the change and evolution of property specifications. Second, it provides the theoretical foundation of safety-related systems in the standard IEC 61508 for ensuring the functional validity. Third, it is used to formalize and check guidelines and consistency rules of UML.These results lay out the foundations for further study of more advanced control mechanisms, and provide a new way for ensuring reliability and safety
Interactions on the Internet require trust between each involved party. Internet entities assume, at the same time, several roles, each having their own interests and motivations; leading to conflicts that must be addressed to enable security and trust. In this thesis, we use, and focus on, Keystroke Dynamics (the way a user type on its keyboard) in an attempt to solve some of these conflicts. Keystroke Dynamics is a a costless and transparent biometric modality as it does not require neither additional sensors nor additional actions from the user. Unfortunately, Keystroke Dynamics also enables users profiling (s.a. identification, gender, age), against their knowledge and consent. In order to protect users privacy, we propose to anonymize Keystroke Dynamics. Still, such information can be legitimately needed by services in order to straighten user authentication. We then propose a Personal Identity Code Respecting Privacy, enabling biometric users authentication without threatening users privacy. We also propose a Social Proof of Identity enabling to verify claimed identities while respecting user privacy, as well as ensuring users past behaviors through a system of accountability.
Our research is centered on the analysis of the role of morphological derivation in the elaboration of the lexical aspectual meaning. The prefix RE- is particularly interesting because of the complexity of its processual structure. RE- implies a relation between a presupposed process and a posited process, and this relation is established by a third - intermediate - process, which can signify continuity, resumption or interruption. The activation of this relation depends in particular on the meaning of the lexical base of the derived term and on the aspectual meaning of this base.
One aspect of a successful human-machine interface (e.g. human-robot interaction, chatbots, speech, handwriting…,etc) is the ability to have a personalized interaction. This affects the overall human experience, and allow for a more fluent interaction. At the moment, there is a lot of work that uses machine learning in order to model such interactions. However, these models do not address the issue of personalized behavior: they try to average over the different examples from different people in the training set. Identifying the human styles (persona) opens the possibility of biasing the models output to take into account the human preference. In this thesis, we focused on the problem of styles in the context of handwriting. The objective of my thesis is to study these problems of styles, in the domain of handwriting. Available to us is IRONOFF dataset, an online handwriting datasets, with 410 writers, with ~25K examples of uppercase, lowercase letters and digits drawings. For transfer learning, we used an extra dataset, QuickDraw!, a sketch drawing dataset containing ~50 million drawing over 345 categories. Major contributions of my thesis are: 1) Propose a work pipeline to study the problem of styles in handwriting. This involves proposing methodology, benchmarks and evaluation metrics. We choose temporal generative models paradigm in deep learning in order to generate drawings, and evaluate their proximity/relevance to the intended/ground truth drawings. We proposed two metrics, to evaluate the curvature and the length of the generated drawings. In order to ground those metics, we proposed multiple benchmarks - which we know their relative power in advance -, and then verified that the metrics actually respect the relative power relationship. 2) Propose a framework to study and extract styles, and verify its advantage against the previously proposed benchmarks. We settled on the idea of using a deep conditioned-autoencoder in order to summarize and extract the style information, without the need to focus on the task identity (since it is given as a condition). We validate this framework to the previously proposed benchmark using our evaluation metrics. We also to visualize on the extracted styles, leading to some exciting outcomes! 3) Using the proposed framework, propose a way to transfer the information about styles between different tasks, and a protocol in order to evaluate the quality of transfer. We leveraged the deep conditioned-autoencoder used earlier, by extract the encoder part in it - which we believe had the relevant information about the styles - and use it to in new models trained on new tasks. We extensively test this paradigm over a different range of tasks, on both IRONOFF and QuickDraw! datasets. We show that we can successfully transfer style information between different tasks.
Study 1 determined text input speed in persons with cervical spinal cord injury and the influence of personal characteristics and type of computer device on text input speed. Study 2 evaluated the effect of a dynamic virtual keyboard coupled with word prediction software on text input speed in persons with functional tetraplegia. Study 3 analysed the word prediction software settings commonly prescribed by health-related professionals for people with cervical spinal cord injury. Studies 4 and 5 evaluated the influence of the number of words displayed in the prediction list and the frequency of use setting on text input speed. Finally, study 6 evaluated the influence of a training program on the use of word prediction software for persons with cervical spinal cord injury on text input speed. The influence of the different word prediction software settings (number of words displayed in the prediction list and the frequency of use) on text input speed, the number of errors or comfort of use, differed depending on the level of injury. We also found differences between the perception of the importance of some settings by health professionals and data in the literature regarding the optimization of settings. Finally, training persons with cervical spinal cord injury in the use of word prediction software increased text input speed. The results of this work highlighted that word prediction software settings influence text input speed in persons with cervical spinal cord injury, however not all professionals are aware of this. Persons with cervical spinal cord injury training programs in the use of word prediction software need to be developed and validated.
Our logometric practice is a statistical application of François Rastier's semantic theories on a corpus of Pierre Mendès France's discurses. Our statistical way determine particularity by generality: words by paragraph, paragraphs by text, texts by corpus. We want to describe how situation – historical or social practice – influe on textual structure. To study the distribution high frequency words in the corpus, our works use two innovent tools: the cotextual environnement and the asymmetric cooccurrence. The former produces "smallworlds": description of lexical semantic networks in texts ; the latter describes rythms of lexical variations. In our corpus, these rythms are associated to distinct argumentative values: persuasive-explanatory vs informative.
Let us define a specific preference as a preference that is not shared by any group of user. A user with several specific preferences will likely be poorly served by a classic CF approach. This is the problem of Grey Sheep Users (GSU). In this thesis, I focus on three separate questions. 1) What is a specific preference? I give an answer by proposing associated hypotheses that I validate experimentally. 2) How to identify GSU in preference data? This identification is important to anticipate the low quality recommendations that will be provided to these users. I propose numerical indicators to identify GSU in a social recommendation dataset. These indicators outperform those of the state of the art and allow to isolate users whose quality of recommendations is very low. 3) How can I model GSU to improve the quality of the recommendations they receive? I propose new recommendation approaches to allow GSU to benefit from the opinions of other users.
This thesis lies in the difficult context of linguistics and computer science. More precisely, we aim to demonstrate the value of the simultaneous consideration of the document structure and linguistic knowledge for the classification of documents according to their style. For this, we defined new descriptors, which, combined with linguistic descriptors exploiting hierarchy of text, are relevant to characterize the types of documents. Then, we proposed a classification method based on non-presence of patterns in the documents. One of originalities of our work is to combine linguistic and machine learning methods with techniques search for local patterns. This hierarchy represents the logical structure of the document based on the principle that different windows of observation correspond to different types of information. These are interconnected through the concept of inheritance of context in order to preserve the global coherence of the document. On the other hand, assumptions related to the task of categorization have emerged, such as exploitation of the total or partial absence of patterns under certain constraints, which can be used to build new analogies for the categorization of documents. Then, by analyzing by evidence pattrens with low or zero frequencies, a new approach of categorization by exclusion-inclusion was proposed by introducing a new concept such as exclusive patterns
For the past thirty years, scientific images have been an object of growing interest in the fields of human and social sciences, art history, history of science and more generally epistemology. This favored the birth and rise, in the Anglo-Saxon world, of visual studies and more particularly of a branch which specifically concerns epistemocritics: the visual studies of science. Indeed, in their production and dissemination of knowledge, sciences have an intimate link with visual representations diagrams, pictures, geometric figures, equations, etc. This articulation between "seeing" and "knowing" has opened up new fields for research in epistemocriticism, which now establishes fruitful relationships with visual studies. The latter insisted at the same time on the crucial role of the image in the process of knowledge acquisition (modeling, force of paradigmatic proposals, visual support for didactic vocation, document-witness of scientific practices, etc.). At the heart of our project, there is a particular type of image: diagrams which are heuristic tools allowing groping knowledge to rise and take a more clearly-defined shape. As hybrid forms, made of image and writing, the diagrams ask in their own way the question of the relationships between knowledge and visualization in the scientific process, a question which took on particular acuity with the crisis of language and representation, sparked by the scientific revolution at the turn of the 19th and 20th centuries. Traditionally, we oppose the concept, associated with discursive argumentation, to the diagram which corresponds to intuition and imagination. The beginning of the 20th century is also marked by formal innovations in literature, in particular by the use of visualization and spatialization procedures which aimed at renewing the modes of representation. If we consider that these innovations are the product of diagrammatic imagination, then the latter can constitute a bridge between the scientific approach and the literary one. The diagram is a visual representation of concepts or ideas and of the links between them. It can also show the constitutive relationships of an object or the relationships between heterogeneous objects. As diagram theorists have shown, the diagram is not an illustration: it is not a spatialized form of an idea that preexists it, but it brings the idea to life through visual representation. The analogical relation with the object (iconic relation) on which the diagram is based on concerns the relations between the parts with each other and with the whole. Furthermore, diagrams are characterized by their hybrid aspect which mixes text and image. By taking as its object diagrammatic thinking, our project will analyse all configurations where text and image combine in graphic figures which maintain an iconic relationship with their object. More broadly, we will make the hypothesis that diagrammatic imagination brings into play a series of tensions which partly cover the tensions between literature and science: tension between image and concept, between imagination and reason, between the intelligible and the sensitive, between the textual and the visual, between the visible and the readable, etc. Our texts will therefore draw a synchronic corpus within the late XIXth-XXth century period which corresponds to the moment when the sciences, and mathematics in particular, were confronted with a crisis of representation which once again questioned the relationships between language and the world. We will wonder how literary texts record and reflect this crisis, how they stage it with their own means, and how mathematical thinking can become the very origin of the creative process.
In the context of climate change, as droughts intensify and as more areas are subject to high water stress, this dissertation focuses on how to manage the imbalance between water resource availability and growing demand in two metropolises of the arid West of the United States. Using an urban political ecology framework, the goal is to observe and analyze the power struggles between stakeholders involved in water resource management in a context where the system of large hydraulic infrastructures underpinning urban growth is increasingly called into question. This mixed-methods survey brings together critical discourse analysis to deconstruct the dominant arguments and position-takings on water conservation, semi-structured interviews with water sector actors (institutions and environmental activists) and participant observation to question the tensions between discourses and changes in urban practices at the local level for adapting the urban metabolism to a world of less water. This thesis shows, on the one hand, that adaptation strategies are implemented by dominant actors within the framework of socio-ecological fixes in order to maintain the growth trajectory of particularly attractive cities.
In this thesis, we introduce conversational implicatures intuitively using Jerry Hobbs's broad concept of granularity. Then, we study conversational implicatures from an interdisciplinary perspective starting from its Gricean origins and moving into: sociology through politeness theory, inference through abduction and dialogue systems through speech act theory. Finally, we develop the two lines of attack used in this thesis to study conversational implicatures: empirical analysis of a corpus of situated task-oriented conversation, and analysis by synthesis in the setup of a text-adventure game.
The increased availability of large amounts of data, from images in social networks, speech waveforms from mobile devices, and large text corpuses, to genomic and medical data, has led to a surge of machine learning techniques. Such methods exploit statistical patterns in these large datasets for making accurate predictions on new data. In recent years, deep learning systems have emerged as a remarkably successful class of machine learning algorithms, which rely on gradient-based methods for training multi-layer models that process data in a hierarchical manner. These methods have been particularly successful in tasks where the data consists of natural signals such as images or audio; this includes visual recognition, object detection or segmentation, and speech recognition. For such tasks, deep learning methods often yield the best known empirical performance; yet, the high dimensionality of the data and large number of parameters of these models make them challenging to understand theoretically. Their success is often attributed in part to their ability to exploit useful structure in natural signals, such as local stationarity or invariance, for instance through choices of network architectures with convolution and pooling operations. However, such properties are still poorly understood from a theoretical standpoint, leading to a growing gap between the theory and practice of machine learning. This thesis is aimed towards bridging this gap, by studying spaces of functions which arise from given network architectures, with a focus on the convolutional case. Our study relies on kernel methods, by considering reproducing kernel Hilbert spaces (RKHSs) associated to certain kernels that are constructed hierarchically based on a given architecture. This allows us to precisely study smoothness, invariance, stability to deformations, and approximation properties of functions in the RKHS. These representation properties are also linked with optimization questions when training deep networks with gradient methods in some over-parameterized regimes where such kernels arise. They also suggest new practical regularization strategies for obtaining better generalization performance on small datasets, and state-of-the-art performance for adversarial robustness on image tasks.
A Software Product Line (SPL) manages commonalities and variability of a related software products family. This approach is characterized by a systematic reuse that reduces development cost and time to market and increases software quality. However, building an SPL requires an initial expensive investment. However, the efficiency of this practice degrades proportionally to the growth of the family of products in concern, that becomes difficult to manage. In this dissertation, we propose a hybrid approach that utilizes both SPL and C&amp;O to develop and evolve a family of software products. The developer can then reduce these possibilities by expressing her preferences (e.g. products, artifacts) and using the proposed cost estimations on the operations. We realized our approach by developing SUCCEED, a framework for SUpporting Clone-and-own with Cost-EstimatEd Derivation. We validate our works on a case study of families of web portals.
The area of multimodal interaction has expanded rapidly since the seminal "Put that there" demonstrator of R. Bolt that combines speech and gesture. In parallel with the development of the Graphical User Interface technology, natural language processing and gesture recognition have made significant progress. In addition, recent interaction paradigms such as tangible and embodied user interfaces as well as augmented reality open a vast world of possibilities for interaction modalities. In this thesis we address this problem of design and development for input multimodal interfaces (from the user to the system). We describe a conceptual model of multimodality as a unified framework for modalities and combinations of modalities. Based on this conceptual model, we define a generic component-based approach called ICARE which allows the easy and rapid design, development and maintenance of multimodal interfaces. We have developed the ICARE tool to prove the usefulness of our component-based approach. The ICARE tool is a graphical platform that enables the designer/developer to graphically manipulate and assemble ICARE software components in order to specify the multimodal interaction.
Routing delivery vehicles in dynamic and uncertain environments like dense city centers is a challenging task, which requires robustness and flexibility. Their application to more complex problems has been facilitated by recent progresses in Deep Neural Networks, which can learn to represent a large class of functions in high dimensional spaces to approximate solutions with high performances. To address DS-VRPs, we then introduce a new sequential multi-agent model we call sMMDP. This fully observable model is designed to capture the fact that consequences of decisions can be predicted in isolation.
However, the implementation of this technology in real applications is hampered by the great degradation of performances in presence of acoustic nuisances. A lot of effort has been invested by the research community in the design of nuisance compensation techniques in the past years. These algorithms operate at different levels: signal, acoustic parameters, models or scores. In order to implement this methodology, pairs of clean and corrupted data are artificially generated then used to develop nuisance compensation algorithms. This method avoids making complex derivations and approximations. The second class of techniques does not use any distortion model in the i-vectors domain. Experiments are carried-out on noisy data and short utterances ; artificially corrupted NIST SRE 2008 data and natural SITW (short / noisy segments).
Even with the recent switch to 280 characters, Twitter messages considered in their singularity, without any additional exogenous information, can confront their readers with difficulties of interpretation. The integration of contextualization on these messages is therefore a promising avenue of research to facilitate access to their information content. In the last decade, most works have focused on building summaries from complementary sources of information such as Wikipedia. In this thesis, we choose a different complementary path that relies on the analysis of conversations on Twitter in order to extract useful information for the contextualization of a tweet. These information were integrated in a prototype which, for a given tweet, offers a visualization of a subgraph of the conversation graph associated with the tweet. This subgraph, automatically extracted from the analysis of structural indicators distributions, allows to highlight particular individuals who play a major role in the conversation and tweets that have contributed to the dynamics of exchanges. This prototype was tested on a panel of users to validate its efficiency and open up prospects for improvement.
The motivating theme behind corrupt feedback is that the feedback the learner receives is a corrupted form of the corresponding reward of the selected arm. We consider two goals for the MAB problem with corrupt feedback: best arm identification and exploration-exploitation.
This thesis aims to detect figurative language devices in social networks. We focus in particular on irony and sarcasm in Twitter and propose an approach based on supervised learning to predict if a tweet is ironic or not. The obtained results for this extremely complex task are very encouraging and will allow a significant improvement of polarity detection in sentiments analysis.
Academic publishing in specialized journals and conference proceedings is the main way to communicate progress in science. The underlying editorial and program committees represent the cornerstone of the evaluation process. With the development of journals and the increasing number of scientific conferences held annually, searching for experts who would serve in these committees is a time-consuming and yet critical activity. This PhD thesis focuses on the task of suggesting program committee (PC) members for scientific conferences. It is organized into three parts. First, we propose a modelling of the multifaceted scientific expertise of researchers based on a weighted heterogeneous graph. Second, we define scientometric indicators to quantify the criteria involved in the composition of CPs. Third, we design a CP member suggestion approach for a given conference, combining the results of the aforementioned scientometric indicators. Our approach is experimented in the context of leading conferences of our research community: SIGIR, considering its editions from 1971 to 2015, and topically close conferences.
In human-agent interaction the engagement of the user is an essential aspect to complete the goal of the interaction. In this thesis we study how the user's engagement could be favoured by the agent's behaviour. Based on the outcomes of the latter study we propose an engagement-driven Topic Manager (computational model) that personalises the topics of an interaction in human-agent information-giving chat. The Topic Selection component of the Topic Manager decides what the agent should talk about and when. For this it takes into account the agent's dynamically updated perception of the user as well as the agent's own mental state. The Topic Transition component of the Topic Manager, based upon an empirical study, computes how the agent should introduce the topics in the ongoing interaction without loosing the coherence of the interaction. We implemented and evaluated the Topic Manager in a conversational virtual agent that plays the role of a visitor in amuseum.
Since the first sequencing of the human genome in the early 2000s, large endeavours have set out to map the genetic variability among individuals, or DNA alterations in cancer cells. They have laid foundations for the emergence of precision medicine, which aims at integrating the genetic specificities of an individual with its conventional medical record to adapt treatment, or prevention strategies. Translating DNA variations and alterations into phenotypic predictions is however a difficult problem. DNA sequencers and microarrays measure more variables than there are samples, which poses statistical issues. The data is also subject to technical biases and noise inherent in these technologies. Finally, the vast and intricate networks of interactions among proteins obscure the impact of DNA variations on the cell behaviour, prompting the need for predictive models that are able to capture a certain degree of complexity. This thesis presents novel methodological contributions to address these challenges. First, we define a novel representation for tumour mutation profiles that exploits prior knowledge on protein-protein interaction networks. For certain cancers, this representation allows improving survival predictions from mutation data as well as stratifying patients into meaningful subgroups. Second, we present a new learning framework to jointly handle data normalisation with the estimation of a linear model. Our experiments show that it improves prediction performances compared to handling these tasks sequentially. Finally, we propose a new algorithm to scale up sparse linear models estimation with two-way interactions. The obtained speed-up makes this estimation possible and efficient for datasets with hundreds of thousands of main effects, thereby extending the scope of such models to the data from genome-wide association studies.
The thesis deams with "pronominal" verbs in french, english and german. In part i an evaluation of previous studies in done from classical studies related to terminology, grammar and the typology of pronominal verbs, to works in the field of natural language processing and computer-aided translation. Tests and transformations are applied to these contexts to identity syntactic similarities. In part 3 the resulting typology is described, providing a basis for the formalization phase prior to an implementation on sygmart, a tree-transfer system. Finally a synchronic investigation leads to the conclusion that dual interpretation (reflexive and reciprocal) of some pronominal contexts can not be disambiguated unless a comprehensive knowledge basis can be integrated to the chain
This thesis aims to analyse the effects that a plurilingual course on line may have on the acquisition of language skills, especially the acquisition of a target language (French) by school students in Barcelona. In order to test the hypothesis that such a course may enhance students'skills both in the languages of the training course and in French, the chosen methodology implied the drafting of an (almost) experimental protocol designed for the students of a school in Barcelona. This protocol involved having a group of students participate in one of the training sessions set up on the Galanet platform, integrating it to their school teachings and including the drawing-up of different tests carried out before and after the course. Moreover, each student had to describe their language profile at the beginning of the training. Each test contains the same type of exercises, focusing on the same skills, and for each situation (either initial or final), there is one test on documents written in French and another on documents (written) in other romance languages (Italian, Portuguese, Romanian and Occitan - from the Vivarois region-). The skills'targets are as follows: textual coherence/cohesion, global and accurate comprehension, the identification of the degree of perception of interculturality, translation/reformulation, segmentation and the identification/ implementation of a speech model. All the data collected during the tests have been subject to both quantitative and qualitative analyses, and complemented by statistic processing, especially when correlations had to be made. At the same time the semi-automatic processing of the corpus of messages that was carried out using the Calico platform helped determine the degree of intercomprehensive interactivity (taking into account the code-switching in a bi/plurilingual context) of the messages posted on the forum during the plurilingual training course. This twofold analysis, based on concrete experiential data made it possible to conduct a case study and to determine, through the longitudinal study of their different results, if undergoing a training course in intercomprehension in a satisfactory way in terms of participation has a positive influence on the acquisition of the skills assessed by the tests, on which particular skill and how.
Students'questions are useful for their learning and for teachers'pedagogical adaptation. We address this issue mainly in the context of a hybrid training program in which students ask questions online each week, using a flipped classroom approach, to help teachers prepare their on-site Q&amp;A session. Our objective is to support the teacher to determine the types of questions asked by different groups of learners. To conduct this work, we developed a question coding scheme guided by student's intention and teacher's pedagogical reaction. Several automatic classification tools have been designed, evaluated and combined to categorize the questions. We have shown how a clustering-based model built on data from previous sessions can be used to predict students'online profiles using exclusively the nature of the questions they ask. These results allowed us to propose three alternative questions' organizations to teachers (based on questions' categories and learners' profiles), opening up perspectives for different pedagogical approaches during Q&amp;A sessions. We have tested and demonstrated the possibility of adapting our coding scheme and associated tools to the very different context of a MOOC, which suggests a form of genericity in our approach.
Autonomous vehicles represent highly complex systems, where multiple types of failures could occur leading to a wrong execution on the road. Therefore, each component should be thoroughly tested to anticipate potential failures and mitigate them. Simulation testing methods are used to complement real test driving in the validation process. The main contributions of this PhD thesis are threefold: detecting a maximum number of failures of the command law, detecting scenarios as close as possible to the border separating zones of failed and safe scenarios, and building explainable border models to identify the border as accurately as possible. The algorithms of the first two objectives use machine learning (Random Forest) and optimization (CMA-ES) techniques to abide with the industrial requirement of reducing the computing power needed, and three approaches are considered to build border models while comparing their performances and explainabilities: Neural Networks, Mixed-Integer Linear Programming (MILP), and Genetic Programming (GP) applied to symbolic regression.
The work developed in this PhD thesis is focused on video sequence analysis. The latter consists of object detection, categorization and tracking. The development of reliable solutions for the analysis of video sequences opens new horizons for several applications such as intelligent transport systems, video surveillance and robotics. In this thesis, we put forward several contributions to deal with the problems of detecting and tracking multi-objects on video sequences. The proposed frameworks are based on deep learning networks and transfer learning approaches. In a first contribution, we tackle the problem of multi-object detection by putting forward a new transfer learning framework based on the formalism and the theory of a Sequential Monte Carlo (SMC) filter to automatically specialize a Deep Convolutional Neural Network (DCNN) detector towards a target scene. In a second contribution, we propose an original multi-object tracking framework based on spatio-temporal strategies (interlacing/inverse interlacing) and an interlaced deep detector, which improves the performances of tracking-by-detection algorithms and helps to track objects in complex videos (occlusion, intersection,strong motion). In a third contribution, we provide an embedded system for traffic surveillance, which integrates an extension of the SMC framework so as to improve the detection accuracy in both day and night conditions and to specialize any DCNN detector forboth mobile and stationary cameras. Throughout this report, we provide both quantitative and qualitative results. On several aspects related to video sequence analysis, this work outperformsthe state-of-the-art detection and tracking frameworks. In addition, we havesuccessfully implemented our frameworks in an embedded hardware platform forroad traffic safety and monitoring.
Although a considerable body of research work has addressed the problem of ontology matching, few studies have tackled the large ontologies used in the biomedical domain. This approach integrates a novel partitioning algorithm as well as a set of matching learning techniques. The partitioning method is based on hierarchical clustering and does not generate isolated partitions. In addition, focusing on context-aware local training sets based on local feature selection and resampling techniques significantly enhances the obtained results.
Given the ambiguities created by the anaphora at the natural languages, many researchers in the field of natural language processing (NLP) have implemented various approaches to solving the problem. We propose to adapt these approaches to the automatic pronominal anaphora resolution in RESUMAN. In the first part presents the cognitive models, linguistic and textual that reflect the operation and interpretation of pronouns. In the second part, the procedures for interpretation of pronominal anaphora are on display: the minimum distance of procedure, the procedure parallel functions, the procedure of the subject or the theme proceedings and procedures morphological analysis, semantics and pragmatics. In the third part, a new version of algorithm, based on a statistical approach, is presented. The RESUMAN software improves the performance of a TAL in case of dense texts anaphora. The performance of this software are evaluated and limits are discussed.
Comparative genomics is as a fundamental discipline to unravel evolutionary biology. To overcome a mere descriptive knowledge of it the first challenge is to develop a higher-level description of the content of a genome. Therefore we used the modular representation of genomes to explore quantitative laws that regulate how genomes are built from elementary functional and evolutionary ingredients. The first part sets off from the observation that the number of domains sharing the same function increases as a power law of the genome size. Since functional categories are aggregates of domain families, we asked how the abundance of domains performing a specific function emerges from evolutionary moves at the family level. We found that domain families are also characterized by family-dependent scaling laws. The second chapter provides a theoretical framework for the emergence of shared components from dependency in empirical component systems with non-binary abundances. The ensemble of resulting realizations reproduces both the distribution of shared components and the law for the growth of the number of distinct families with genome size. The last chapter extends the component systems approach to microbial ecosystems. Using our findings about families scaling laws, we analyzed how the abundance of domain families in a metagenome is affected by the constraint of power-law scaling of family abundance in individual genomes. The result is the definition of an observable, whose functional form contains quantitative information on the original composition of the metagenome.
In the last years, the amount of available data on the social Web has exploded. For the average user, it became hard to find quality content without being overwhelmed with publications. For service providers, the scalability of such services became a challenging task. The aim of this thesis is to achieve a better user experience by offering the filtering and recommendation features. Filtering consists to provide for a given user, the ability of receiving only a subset of the publications from the direct network. Where recommendation allows content discovery by suggesting relevant content producers on given topics. We developed MicroFilter, a scalable filtering system able to handle Web-like data flows and RecLand, a recommender system that takes advantage of the network topology as well as the content in order to provide relevant recommendations.
This thesis focuses on acoustic and prosodic (fundamental frequency (F0), duration, intensity) analyses of French from large-scale audio corpora portraying different speaking styles: prepared and spontaneous speech. We are interested in particularities of segmental phonetics and prosody that may characterize pronunciation. Automatic classification (AC) was conducted to discriminate homophones by only acoustic and prosodic properties depending on their part-of-speech function or their position within prosodic words. Results from AC of two homophone pairs, et/est (and/is) and à/a (ton/has), revealed that the et/est pair was more discriminable. A selection of prosodic and inter-phoneme attributes, that is 15 attributes, performed as good results as with 62 attributes. Then corresponding perceptual tests have been conducted to verify if humans also use acoustico-prosodic parameters for the discrimination. Results suggested that acoustic and prosodic information might help in operating the correct choice in similar ambiguous syntactic structures. From the hypothesis that pronunciation variants were due to varying prosodic constraints, we examined overall prosodic properties of French on a lexical and phrase level. The comparison between lexical and grammatical words revealed F0 rise and lengthening at the end of final syllable on lexical words, while these phenomena were not observed for grammatical words. Analyses also revealed that the mean profile of a n length noun phrase could be different from that of a n length noun with a low F0 at the beginning of a noun phrase. The prosodic profiles can be helpful to locate word boundaries. Findings in this thesis will lead to localize focus and named-entity using discriminative classifiers, and to improve word boundary locations by an ASR post-processing step.
Recently, malware, short for malicious software has greatly evolved and became a major threat to the home users, enterprises, and even to the governments. To mitigate this problem, malware researchers have proposed various data mining and machine learning approaches for detecting and classifying malware samples according to the their static or dynamic feature set. Although the proposed methods are effective over small sample set, the scalability of these methods for large data-set are in question. Moreover, it is well-known fact that the majority of the malware is the variant of the previously known samples. Consequently, the volume of new variant created far outpaces the current capacity of malware analysis. Thus developing malware classification to cope with increasing number of malware is essential for security community. The key challenge in identifying the family of malware is to achieve a balance between increasing number of samples and classification accuracy. To achieve our goal, firstly we developed a portable, scalable and transparent malware analysis system called VirMon for dynamic analysis of malware targeting Windows OS. Secondly we set up a cluster of five machines for our online learning framework module (i.e. Jubatus), which allows to handle large scale of data. This configuration allows each analysis machine to perform its tasks and delivers the obtained results to the cluster manager. Essentially, the proposed framework consists of three major stages. The first stage consists in extracting the behavior of the sample file under scrutiny and observing its interactions with the OS resources. At this stage, the sample file is run in a sandboxed environment. Our framework supports two sandbox environments: VirMon and Cuckoo. During the second stage, we apply feature extraction to the analysis report. The label of each sample is determined by using Virustotal, an online multiple anti-virus scanner framework consisting of 46 engines. Then at the final stage, the malware dataset is partitioned into training and testing sets.
The thesis objective is to design and build a high quality Hidden Markov Model (HMM-)based Text-To-Speech (TTS) system for Vietnamese – a tonal language. The system is called VTED (Vietnamese TExt-tospeech Development system). In view of the great importance of lexical tones, a “tonophone” – an allophone in tonal context – was proposed as a new speech unit in our TTS system. A new training corpus, VDTS (Vietnamese Di-Tonophone Speech corpus), was designed for 100% coverage of di-phones in tonal contexts (i.e. di-tonophones) using the greedy algorithm from a huge raw text. A total of about 4,000 sentences of VDTS were recorded and pre-processed as a training corpus of VTED. In the HMM-based speech synthesis, although pause duration can be modeled as a phoneme, the appearanceof pauses cannot be predicted by HMMs. Lower phrasing levels above words may not be completely modeled with basic features. This research aimed at automatic prosodic phrasing for Vietnamese TTS using durational clues alone as it appeared too difficult to disentangle intonation from lexical tones. Syntactic blocks, i.e. syntactic phrases with a bounded number of syllables (n), were proposed for predicting final lengthening (n = 6) and pause appearance (n = 10). Improvements for final lengthening were done by some strategies of grouping single syntactic blocks. The quality of the predictive J48-decision-tree model for pause appearance using syntactic blocks combining with syntactic link and POS (Part-Of-Speech) features reached F-score of 81.4% Precision=87.6%, Recall=75.9%), much better than that of the model with only POS (F-score=43.6%)or syntactic link (F-score=52.6%) alone. The architecture of the system was proposed on the basis of the core architecture of HTS with an extension of a Natural Language Processing part for Vietnamese. Pause appearance was predicted by the proposed model. Contextual feature set included phone identity features, locational features, tone-related features, and prosodic features (i.e. POS, final lengthening, break levels). Mary TTS was chosen as a platform for implementing VTED. In the MOS (Mean Opinion Score) test, the first VTED, trained with the old corpus and basic features, was rather good, 0.81 (on a 5 point MOS scale) higher than the previous system – HoaSung (using the non-uniform unit selection with the same training corpus); but still 1.2-1.5 point lower than the natural speech. The quality of the final VTED, trained with the new corpus and prosodic phrasing model, progressed by about 1.04 compared to the first VTED, and its gap with the natural speech was much lessened. In the tone intelligibility test, the final VTED received a high correct rate of 95.4%, only 2.6% lower than the natural speech, and 18% higher than the initial one. The error rate of the first VTED in the intelligibility test with the Latin square design was about 6-12% higher than the natural speech depending on syllable, tone or phone levels. The final one diverged about only 0.4-1.4% from the natural speech.
The interest in hyperspectral image data has been constantly increasing during the last years. Indeed, hyperspectral images provide more detailed information about the spectral properties of a scene and allow a more precise discrimination of objects than traditional color images or even multispectral images. In this thesis, we are mainly interested in the reduction and partitioning of hyperspectral images of high spatial dimension. The proposed approach consists essentially of two steps: features extraction and classification of pixels of an image. A new approach for features extraction based on spatial and spectral tri-occurrences matrices defined on cubic neighborhoods is proposed. A comparative study shows the discrimination power of these new features over conventional ones as well as spectral signatures. Concerning the classification step, we are mainly interested in this thesis to the unsupervised and non-parametric classification approach because it has several advantages: no a priori knowledge, image partitioning for any application domain, and adaptability to the image information content. A comparative study of the most well-known semi-supervised (knowledge of number of classes) and unsupervised non-parametric methods (K-means, FCM, ISODATA, AP) showed the superiority of affinity propagation (AP). Secondly, the partitioning of large size hyperspectral images is hampered by its quadratic computational complexity. To overcome these two drawbacks, we propose an approach which consists of reducing the number of pixels to be classified before the application of AP by automatically grouping data points with high similarity. We also introduce a step to optimize the preference parameter value by maximizing a criterion related to the interclass variance, in order to correctly estimate the number of classes. The proposed approach was successfully applied on synthetic images, mono-component and multi-component and showed a consistent discrimination of obtained classes. It was also successfully applied and compared on hyperspectral images of high spatial dimension (1000 × 1000 pixels × 62 bands) in the context of a real application for the detection of invasive and non-invasive vegetation species.
In French, accentuation is said to be post-lexical, marking the phrase rather than the word. That is, the primary final accent (FA) is considered to be perceptively weakened when co-occurring with a major prosodic boundary, while the Initial Accent (IA), regarded as a secondary and optional accent, is thought to hold merely a rhythmic function in balancing longer constituents. The existence of a third level, the Intermediate Phrase (ip), while advanced by some authors, remains controversial. The aim of our study is to investigate the organization of prosodic phrasing in French. We propose a perception study on a corpus in which syntactically ambiguous structures were manipulated, and asked 80 participants to perform 3 distinct perception tasks: a prominence, boundary and grouping task. The perceived prosodic events were then related to their acoustic realization. Taken together, our results indicate that listeners are able to distinguish finer-grained grouping levels than those predicted in traditional French descriptions. Moreover, lexical words are systematically realized by an accentual bipolarization (IA+FA), with each accent carrying the same metrical weight. The function of IA is shown to be more one of structuration than rhythmic balancing, with IA even marking structure more readily than FA. Finally, our results indicate that FA is not perceptively weakened when co-occurring with major prosodic boundaries, but instead remains a metrical mark at the level of the lexical word, in a manner independent from the level of constituency.
In order to optimize the existing language, we set out to assess the appropriate levels of simplification that would achieve more accurate and faster comprehension with minimum pilot training. We first delved into the controlled language domain to form an overview of the existing controlled languages, their context, and rules. From this research we attempted to find solutions for optimization, but at the same time we strove to offer an original contribution to the field through this work.
Conversational virtual agents with social behavior are often based on at least two different disciplines: computer science and psychology. In most cases, psychological findings are converted into computational mechanisms in order to make agents look and behave in a believable manner. More precisely, we are interested in task-oriented conversational agents, which are used as a custumer-relationship channel to respond to users request. We propose an affective model of emotional responses' generation and control during a task-oriented interaction. Our proposed model is based, on one hand, on the theory of Action Tendencies (AT) in psychology to generate emotional responses during the interaction. This model has been implemented in an agent architecture endowed with a natural language processing engine developed by the company DAVI. In order to confirm the relevance of our approach, we realized several experimental studies. In the second, we studied the impact of different emotional regulation strategies on the agent perception by the user. Finally, the third study focuses on the evaluation of emotional agents in real-time interactions. Our results show that the regulation process contributes in increasing the credibility and perceived competence of agents as well as in improving the interaction. Our results highlight the need to take into consideration of the two complementary emotional mechanisms: the generation and regulation of emotional responses. They open perspectives on different ways of managing emotions and their impact on the perception of the agent.
In this thesis I investigate both in a theoretical and historical perspective the occurrence of macroeconomic crisis. Building on narrative economics, text mining techniques and complex system theories I provide evidence of the mechanisms driving systemic crisis and show that rather than the nature of the shocks it is the architecture of the economic system that creates the vulnerability of economies and the ingredients for large scale crisis. This chapter use a combination of Natural Language Processing(NPL) techniques and narrative approach to create a structured data, reflecting the macroeconomic outlook of IMF members since the early 1950s. Using a combination of supervised and unsupervised learning techniques we create a large training sample of around 30,000 observations from IMF reports. This training sample characterized each documents in around 20economically relevant crisis categories, from Sovereign default, currency crisis or banking crisis to epidemic outbreaks or violent conflicts. The database and quantitative tools will be available in a R package (shortly available in github). Building on narrative economics, complex system theory and ecological models of resilience I show that macroeconomic systems are systems in perpetual disequilibrium with recurrent crisis outbreaks (the triggers) that heterogeneously affects the core of the system, (the spreaders) and eventually the critical nodes (the hotspots) in a dynamic similar to wildfire forest. In a third chapter I take a narrower perspective by considering the episodes looking at the experiences when the IMF intervened with a lending arrangements. In this Chapter entitle "The Instruments of the Lender of Last Resort: curing the fundamentals or calming the panic?" I provide an historical overview of 70 years of intervention of the IMF to provide assistance to distress sovereign. In a fourth chapter I provide evidence of the Taylor rule of the IMF when designing the lending arrangements to provide assistance to distress countries. This work provide new lights on the occurrence of systemic crisis and on the dynamic of the global perturbations that some unexpected events can generate. This work try to bring at the center of the analysis the narrative nature of economic crisis and the importance of the architecture of the system of crisis and intend to drag the attention of the importance of the finding the right balance between economic development, complexity of economic systems and severity of crisis. In a context of a very fragile and vulnerable international economic system combined with the rising emergence of non economic- and ecologically critical crisis, particular attention should be given to the development of new standards of crisis manage-ment targeted at reducing the intensity of the crisis at the same time of mitigating the vulnerability due to the interconnection of economic sectors, increasing the self-resilience of the different economic nodes, and isolating the more critical elements of the system to the avoid systemic wide spread economic collapses
Stories are a communication tool that allow people to make sense of the world around them. It represents a platform to understand and share their culture, knowledge and identity. Stories carry a series of real or imaginary events, causing a feeling, a reaction or even trigger an action. For this reason, it has become a subject of interest for different fields beyond Literature (Education, Marketing, Psychology, etc.) that seek to achieve a particular goal through it (Persuade, Reflect, Learn, etc.). However, stories remain underdeveloped in Computer Science. There are works that focus on its analysis and automatic production. However, those algorithms and implementations remain constrained to imitate the creative process behind literary texts from textual sources. Thus, there are no approaches that produce automatically stories whose 1) the source consists of raw material that passed in real life and 2) and the content projects a perspective that seeks to convey a particular message. Working with raw data becomes relevant today as it increase exponentially each day through the use of connected devices. Given the context of Big Data, we present an approach to automatically generate stories from ambient data. The objective of this work is to bring out the lived experience of a person from the data produced during a human activity. Any areas that use such raw data could benefit from this work, for example, Education or Health. It is an interdisciplinary effort that includes Automatic Language Processing, Narratology, Cognitive Science and Human-Computer Interaction. This approach is based on corpora and models and includes the formalization of what we call the activity récit as well as an adapted generation approach. It consists of 4 stages: the formalization of the activity récit, corpus constitution, construction of models of activity and the récit, and the generation of text. Each one has been designed to overcome constraints related to the scientific questions asked in view of the nature of the objective: manipulation of uncertain and incomplete data, valid abstraction according to the activity, construction of models from which it is possible the Transposition of the reality collected though the data to a subjective perspective and rendered in natural language. We used the activity narrative as a case study, as practitioners use connected devices, so they need to share their experience. The results obtained are encouraging and give leads that open up many prospects for research.
In this thesis we present a general framework, based on the object-oriented paradigm, for modeling and designing a model of speech data representation, and we propose a particular use of it for cineradiographic data, including sagittal views of the vocal tract, frontal pictures of the lips, and acoustic signals. Indeed, the notion of a derived data model has been useful for users, to manage raw data and the results of data analysis in the same way. Our investigation consisted in developing a data model to represent basic speech data entities used most often in the speech research community. This model has been useful in managing all speech databases available at icp. Data abstraction techniques such as inheritance were found to be essential for describing either primary data (x ray realizations and video realizations) or derived data and their components. The tokens were pronounced by a female french speaker at the institute of phonetics in strasbourg. We focused our study on anticipatory coarticulation, our aim is to contribute to understanding the programming unit in speech production. We observed situations where vcv sequences were produced in accordance with the khozhevnikov &amp; chistovich model; other situations confirm the look-ahead model hypothesis, or the m. E. M (abry &amp; lallouache) model hypothesis, or the ohman's theory. Of these four hypotheses, only the m. E. M model appears to be consistent these data.
Assuming that workplace significantly affects information seeking and information management patterns,this study explores accessibility and management of information sources among a group of research engineers. The study explores how these engineers, who belong to the RandD entity of a major energy group,require, search and manage information sources in given professional contexts. The study provides an analytic cartography of the various components of the organizational and informational environments whereby the activities and tasks of the above mentioned actors take place. A wide range of practices has been identified via interviews but also through the activities and work rhythms observed.
Knowledge Base Population (KBP) is an important and challenging task specially when it has to be done automatically. A Knowledge Base (KB) contains different entities, relationships among them and various properties of the entities. Generally, relations are extracted based on the lexical and syntactical information at the sentence level. However, global information about known entities has not been explored yet for RE task. We propose to extract a graph of entities from the overall corpus and to compute features on this graph that are able to capture some evidence of holding relationships between a pair of entities. In order to evaluate the relevance of the proposed features, we tested them on a task of relation validation which examines the correctness of relations that are extracted by different RE systems. Experimental results show that the proposed features lead to outperforming the state-of-the-art system.
This thesis focuses on building semantic analysis models and tools for exploiting corpora of interviews carried out by a company specialized innovation projects support. The interviews are analyzed to determine whether an innovation project meets various criteria of consumer acceptance. Beyond the general difficulties of ambiguity in semantic analysis of opinions, the automatized analysis will have to be able to process contents that belong to a multitude of languages for specific purposes. The models and tools will have to include general NLP processes (syntaxic and semantic analysis) and ontologies allowing the integration of various terminologies.
High-Performance Computing (HPC) platforms are growing in size and complexity. In an adversarial manner, the power demand of such platforms has rapidly grown as well, and current top supercomputers require power at the scale of an entire power plant. In an effort to make a more responsible usage of such power, researchers are devoting a great amount of effort to devise algorithms and techniques to improve different aspects of performance such as scheduling and resource management. But HPC platform maintainers are still reluctant to deploy state of the art scheduling methods and most of them revert to simple heuristics such as EASY Backfilling, which is based in a naive First-Come-First-Served (FCFS) ordering. Newer methods are often complex and obscure, and the simplicity and transparency of EASY Backfilling are too important to sacrifice. At a first moment we explored Machine Learning (ML) techniques to learn on-line parallel job scheduling heuristics. Using simulations and a workload generation model, we could determine the characteristics of HPC applications (jobs) that lead to a reduction in the mean slowdown of jobs in an execution queue. Modeling these characteristics using a nonlinear function and applying this function to select the next job to execute in a queue improved the mean task slowdown in synthetic workloads. When applied to real workload traces from highly different machines, these functions still resulted in performance improvements, attesting the generalization capability of the obtained heuristics. At a second moment, using simulations and workload traces from several real HPC platforms, we performed a thorough analysis of the cumulative results of four simple scheduling heuristics (including EASY Backfilling). We also evaluated effects such as the relationship between job size and slowdown, the distribution of slowdown values, and the number of backfilled jobs, for each HPC platform and scheduling policy. We show experimental evidence that one can only gain by replacing EASY Backfilling with the Smallest estimated Area First (SAF) policy with backfilling, as it offers improvements in performance by up to 80% in the slowdown metric while maintaining the simplicity and the transparency of EASY. SAF reduces the number of jobs with large slowdowns and the inclusion of a simple thresholding mechanism guarantees that no starvation occurs. Overall we achieved the following remarks: (i) simple and efficient scheduling heuristics in the form of a nonlinear function of the jobs characteristics can be learned automatically, though whether the reasoning behind their scheduling decisions is clear or not can be up to argument. (ii) The area (processing time estimate multiplied by the number of processors) of the jobs seems to be a quite important property for good parallel job scheduling heuristics, since many of the heuristics (notably SAF) that achieved good performances have the job's area as input. (iii) The backfilling mechanism seems to always help in increasing performance, though it does not outperform a better sorting of the jobs waiting queue, such as the sorting performed by SAF.
This thesis addresses the extraction of relational information from scientific documents in Life Sciences, i.e. transforming unstructured text into machine-readable structured information. The extraction of specialized semantic relationships between entities detected in text makes explicit and formalizes the underlying structures. Current state-of-the art methods rely on supervised machine learning. Supervised learning, and even more so recent deep learning methods, require many training examples that are costly to produce, all the more in specific domains such as Life Sciences. We hypothesize that combining information and knowledge available in specific domains with the latest deep learning word embedding models can offset the absence or limited amount of annotated training data. For this purpose, the thesis will design a rich representation of texts that draws both from linguistic information obtained from syntactic parsing and domain knowledge obtained from knowledge graphs such as ontologies. Integrating ontologies in the information extraction process will additionally facilitate information integration with other data, such as experimental or analytical data.
Information and communication technologies largely affect many branches of the law. The contract law is not an exception and many contracts are now concluded online, regardless of the device used. The use of this means of communication is not without influence on the perfection of the contract, in particular on the means of expression of the intention of the parties in the digital environment. Indeed, the latter offers vast prospects in terms of instantaneousness, immateriality and automation of the expression of its contractual agreement, raising questions about the validity of contracts formed by electronic means. Observing the practices that have taken place on the Internet, it is now possible to note the influence of digital means of communication on the expression of contractual agreement, that is to say on the individual intentions of the parties, as well as the meeting of these. Individual intentions have thus been subjected to a process consisting of a series of obligatory steps, supposed to limit the cases in which the conclusion of the agreement would be an error. However, this process opens the way to the automation of the expression of individual intentions and their meeting, announcing then the age of contracts concluded or even executed nearly instantaneously thanks to recent development in artificial intelligence applied to the legal field.
Localization is the process of determining the position of an entity in a local or global coordinate system. The applications of localization are widely spread across different contexts. For instance, in events, tracking the participants can save lives during crises. In health-care, elderly people can be tracked to respond to their needs in critical situations like falling. In warehouses, robots transferring products from one place to another require accurate knowledge of products'positions as well as other robots. In industrial context of the factory of the future, localization is invaluable to achieve automated processes that are flexible enough to be reconfigured for various purposes. Localization is considered a topic of high interest both in the academia and industry especially with the advent of 5G. The requirements of 5G pave the way for revolutionizing localization capabilities; Enhanced Mobile Broadband (eMBB) that is expected to reach 10 Gbits/s, Ultra-Reliable Low-Latency Communication (URLLC) which is less than 1 ms and massive Machine-Type Communication (mMTC) allowing to connect around 1 million devices per km. In this work, we focus on two main types of localization; range-based localization and fingerprinting based localization. In range-based localization, a network of devices with a maximum communication range estimate inter-distance values to their first-hop neighbors. These distances along with knowledge of positions of few anchor nodes are used to localize other nodes in the network using a triangulation based solution. The proposed method is capable of localizing ≈ 90% of nodes in a network with an average degree of ≈ 10. In the second contribution, wireless channel responses, aka. Channel State Information (CSI) is used to estimate the position of a transmitter communicating with a MIMO antenna. In this work, we apply classical learning techniques (K-nearest neighbors) and deep learning techniques (Multi-Layer Perceptron Neural Network and Convolutional Neural Networks) to localize a transmitter in indoor and outdoor contexts. Our work achieved the first place in the indoor positioning competition prepared by IEEE's Communication Theory Workshop among 8 teams from highly reputable universities worldwide by achieving a Mean Square Error of 2.3 cm.
This thesis addresses the task of establishing adense correspondence between an image and a 3D object template. We aim to bring vision systemscloser to a surface-based 3D understanding of objects by extracting information that is complementary to existing landmark- or partbased representations. We use convolutional neural networks (CNNs) to densely associate pixels with intrinsic coordinates of 3D object templates. Through the established correspondences we effortlessly solve a multitude of visual tasks, such as appearance transfer, landmark localization and semantic segmentation by transferring solutions from the template to an image. We show that geometric correspondence between an imageand a 3D model can be effectively inferred forboth the human face and the human body.
This work represents one attempt to develop a computer aided diagnosis system for epilepsy lesion detection based on neuroimaging data, in particular T1-weighted and FLAIR MR sequences. Given the complexity of the task and the lack of a representative voxel-level labeled data set, the adopted approach, first introduced in Azami et al., 2016, consists in casting the lesion detection task as a per-voxel outlier detection problem. Manual features, designed to mimic the characteristics of certain epilepsy lesions, such as focal cortical dysplasia (FCD), on neuroimaging data, are tailored to individual pathologies and cannot discriminate a large range of epilepsy lesions. Such features reflect the known characteristics of lesion appearance; however, they might not be the most optimal ones for the task at hand. Our first contribution consists in proposing various unsupervised neural architectures as potential feature extracting mechanisms and, eventually, introducing a novel configuration of siamese networks, to be plugged into the outlier detection context. The proposed system, evaluated on a set of T1-weighted MRIs of epilepsy patients, showed a promising performance but a room for improvement as well. To this end, we considered extending the CAD system so as to accommodate multimodality data which offers complementary information on the problem at hand. Our second contribution, therefore, consists in proposing strategies to combine representations of different imaging modalities into a single framework for anomaly detection. The extended system showed a significant improvement on the task of epilepsy lesion detection on T1-weighted and FLAIR MR images. Our last contribution focuses on the integration of PET data into the system. Given the small number of available PET images, we make an attempt to synthesize PET data from the corresponding MRI acquisitions. Eventually we show an improved performance of the system when trained on the mixture of synthesized and real images.
Analogical reasoning is recognized as a core component of human intelligence. It has been extensively studied from philosophical and psychological viewpoints, but recent works also address the modeling of analogical reasoning for computational purposes, particularly focused on analogical proportions. We are interested here in the use of analogical proportions for making predictions, in a machine learning context. In recent works, analogy-based classifiers have achieved noteworthy performances, in particular by performing well on some artificial problems where other traditional methods tend to fail. Starting from this empirical observation, the goal of this thesis is twofold. The first topic of research is to assess the relevance of analogical learners on real-world, practical application problems. The second topic is to exhibit meaningful theoretical properties of analogical classifiers, which were yet only empirically studied. The field of application that was chosen for assessing the suitability of analogical classifiers in real-world setting is the topic of recommender systems. A common reproach addressed towards recommender systems is that they often lack of novelty and diversity in their recommendations. As a way of establishing links between seemingly unrelated objects, analogy was thought as a way to overcome this issue. Experiments here show that while offering sometimes similar accuracy performances to those of basic classical approaches, analogical classifiers still suffer from their algorithmic complexity. On the theoretical side, a key contribution of this thesis is to provide a functional definition of analogical classifiers, that unifies the various pre-existing approaches. So far, only algorithmic definitions were known, making it difficult to lead a thorough theoretical study. We were also able to identify a criterion that ensures a safe application of our analogical inference principle, which allows us to characterize analogical reasoning as some sort of linear process.
Our objective was to develop numerical quantitative biomarkers to characterize the geometrical organization of the four FN variants (that differ by the inclusion/exclusion of EDA/EDB) from 2D confocal microscopy images, and to compare sane and cancerous tissues. First, we showed through two classification pipelines, based on curvelet features and deep learning framework, that the FN variants can be distinguished with a similar performance to that of a human annotator. We constructed a graph-based representation of the fibers, which were detected using Gabor filters. Graphspecific attributes were employed to classify the variants, proving that the graph representation embeds relevant information from the confocal images. Furthermore, we identified various techniques capable to differentiate the graphs, allowing us to compare the FN variants quantitatively and qualitatively. Performance analysis using toy graphs showed that the methods, which are based on graph matching and optimal transport, can meaningfully compare graphs. Using the graph-matching framework, we proposed different methodologies for defining the prototype graph, representative of a certain FN class. Additionally, the graph matching served as a tool to compute parameter deformation maps between the variants. These deformation maps were analyzed in a statistical framework showing whether or not the variation of the parameters can be explained by the variance within the same class.
In our work, we investigate both aspects in proposing machine learning-based algorithms adapted to the different information sources that can be collected. In terms of outdoor mobility, we use the collected GPS coordinate data to discover the daily mobility patterns of the users. This clustering method is based on estimating probability densities of the trajectories, which alleviate the problems caused by the data noise. By contrast, we utilize the collected WiFi fingerprint data to study indoor human mobility. Moreover, as for accurate indoor location recognition, we presume that there exists a latent distribution governing the input and output at the same time. Based on this assumption, we develop a variational auto-encoder (VAE)-based semi-supervised learning model. In the unsupervised learning procedure, we employ a VAE model to learn a latent distribution of the input, the WiFi fingerprint data. In the supervised learning procedure, we use a neural network to compute the target, the user coordinates. Furthermore, based on the same assumption used in the VAE-based semi-supervised learning model, we leverage the information bottleneck theory to devise a variational information bottleneck (VIB)-based model. This is an end-to-end deep learning model which is easier to train and has better performance.
The aim of contextual MT is to overcome this limitation by providing ways of integrating extra-sentential context into the translation process. Context, concerning the other sentences in the text (linguistic context) and the scenario in which the text is produced (extra-linguistic context), is important for a variety of cases, such as discourse-level and other referential phenomena. Successfully taking context into account in translation is challenging. Evaluating such strategies on their capacity to exploit context is also a challenge, standard evaluation metrics being inadequate and even misleading when it comes to assessing such improvement in contextual MT. In this thesis, we propose a range of strategies to integrate both extra-linguistic and linguistic context into the translation process. We accompany our experiments with specifically designed evaluation methods, including new test sets and corpora. Our contextual strategies include pre-processing strategies designed to disambiguate the data on which MT models are trained, post-processing strategies to integrate context by post-editing MT outputs and strategies in which context is exploited during translation proper. We cover a range of different context-dependent phenomena, including anaphoric pronoun translation, lexical disambiguation, lexical cohesion and adaptation to properties of the scenario such as speaker gender and age. Our experiments for both phrase-based statistical MT and neural MT are applied in particular to the translation of English to French and focus specifically on the translation of informal written dialogues.
In this work a framework to interface efficient probabilistic modeling for both the SLU and the DM modules is described and investigated. Tractability is ensured by the use of an intermediate summary space. Also to reduce the development cost of SDS an approach based on clustering is proposed to automatically derive the master-summary mapping function. A implementation is presented in the Media corpus domain (touristic information and hotel booking) and tested with a simulated user.
The relatively recent presence of transnational migrants calls for a renewed look at the dynamics of identification between "indigenous" and "non-indigeneous" and, more particularly, its implications for education. By giving disposable cameras to secondary school students in the town of Bozen-Bolzano, a city that seems to embody a certain hybridism, with the mission of photographing "the languages of the neighborhood" to create an exhibition, the goal is to investigate the ways young people interpret the plurality that surrounds them.
Providing the computational infrastucture needed to solve complex problems arising in modern society is a strategic challenge. Organisations usually address this problem by building extreme-scale parallel and distributed platforms. High Performance Computing (HPC) vendors race for more computing power and storage capacity, leading to sophisticated specific Petascale platforms, soon to be Exascale platforms. These systems are centrally managed using dedicated software solutions called Resource and Job Management Systems(RJMS). A crucial problem addressed by this software layer is the job scheduling problem, where the RJMS chooses when and on which resources computational tasks will be executed. This manuscript provides ways to address this scheduling problem. No two platforms are identical. Indeed, the infrastructure, user behavior and organization's goals all change from one system to the other. We therefore argue that scheduling policies should be adaptative to the system's behavior. In this manuscript, we provide multiple ways to achieve this adaptativity. Through an experimental approach, we study various trade-offs between the complexity of the approach, the potential gain, and the risks taken.
Our problematic concerns the links between logic and topoï, semantics and topoï and finally between logic and semantics. The work therefore consists in highlighting the durability of the notion of topos, its evolution and its integration in logic, in semantics and even in computer science. From then on, topoi, logic and semantics are the three facets of this problematic. It also shows that this notion is present in all linguistic approaches and that semantics is the unifying axis of all linguistic research. It is not our intention to give a new definition, but we have tried to summarize the main characteristics of the topoi and to draw up a table of past and current research developed on this notion. The approach we have adopted is constructive and diachronic. She highlighted the evolution of topoi, their forms, their senses and their uses. The notion of topoi continues to exist from antiquity to the present day, since it has semantic foundations, which are inherent in all natural languages. The first presents the terminological conception of the concept of topoi and its evolutionary aspect. It highlights the different jobs, the different definitions and the evolutionary aspect of the topoï. In this first part, we tried to sketch out the history of the notion of topoi, the use that has been made by logicians, rhetoricians, linguists, pragmatists and computer scientists. The second is devoted to the presentation of the semantic foundations of logic and topoi and to the complementary relationship of these three notions, namely logic, semantics and topoi. We have specified that the theory of topoi is a theory of meaning. And the third part deals with the lexical foundations of the topoi and the modern exploitation of this notion, namely the automatic processing, the theory of the frames and the ontology, while presenting the different semantic argumentative theories. We presented a small example, a sample extraction of information using the NooJ platform.
Natural language understanding often relies on common-sense reasoning, for which knowledge about semantic relations, especially between verbal predicates, may be required. This thesis addresses the challenge of using a distibutional method to automatically extract the necessary semantic information for common-sense inference. Typical associations between pairs of predicates and a targeted set of semantic relations (causal, temporal, similarity, opposition, part/whole) are extracted from large corpora, by exploiting the presence of discourse connectives which typically signal these semantic relations. In order to appraise these associations, we provide several significance measures inspired from the literature as well as a novel measure specifically designed to evaluate the strength of the link between the two predicates and the relation. The relevance of these measures is evaluated by computing their correlations with human judgments, based on a sample of verb pairs annotated in context. We assess the potential of these representations for several applications. Regarding discourse analysis, the tasks of predicting attachment of discourse units, as well as predicting the specific discourse relation linking them, are investigated. Using only features from our resource, we obtain significant improvements for both tasks in comparison to several baselines, including ones using other representations of the pairs of predicates. We also propose to define optimal sets of connectives better suited for large corpus applications by performing a dimension reduction in the space of the connectives, instead of using manually composed groups of connectives corresponding to predefined relations. These diverse applications aim to demonstrate the promising contributions provided by our approach, namely allowing the unsupervised extraction of typed semantic relations.
This research deals with the study of a subset of expressive conversational formulas (FEC) (you speak, what good is it, it surprises me) apprehended from the angle of their analysis and description syntactic, semantic, pragmatic and discursive. This study examines complex questions such as the terminology chosen, semantic values, syntactic behavior, the role of context or pragmatic workings. We also propose to reflect on the status of FECs in lexicography, which allows them to better understand their description in relation to what general and specialized dictionaries offer. We conducted this study in four stages. First, on a theoretical level, we have presented a synthesis of existing work by showing the difficulties of a terminological and definitional nature linked to this linguistic phenomenon for which a delimitation is required. For our part, we have chosen the term expressive formula and we have justified our choice through the selection of criteria allowing to define this notion. Subsequently, from a methodological point of view, the study takes the approach of corpus linguistics in a qualitative and quantitative approach. It is based on corpora considered in several settings: written (literature - tweets) and and oral (Orféo). After presenting the theoretical and methodological framework, we proceeded to the concrete study through the discursive analysis of FECs in two different sub-corpora. The primary objective of this study is to arrive at well-defined criteria limiting this subclass of pragmatic phraseologisms compared to the other subtypes proposed by researchers in this linguistic field. Finally, we proposed the lexicographic treatment of certain formulas selected within the framework of the Polonium project. The challenge here is to arrive at a functional lexicographic model for the lexicographic processing of FEC.We believe that we have succeeded in achieving the challenges that we set ourselves at the start. Through a discursive analysis of the FECs in the text, we wanted to refine the defining features and opt for an exhaustive definition of this subset. By examining their status in the field of lexicography, we have retained a detailed and promising descriptive method of FECs.
The population in cities is slated to double by mid-century according to estimates prepared by the World Health Organization. This rapid increase in population will impact transportation and economic growth, and will increase responsibilities of local managing authorities and different stakeholders. It is a need of the hour to convert cities into smart cities in order to provide new service to the public, by using available resources in an optimum manner. From crowd-sourced data and open governmental data to other online sources, a variety of data sources can provide users with smart tools to efficiently manage their daily activities. Moreover, with the advancement in Internet and mobile technologies, social networking platforms such as Facebook and Twitter have become popular modes of communication. They allow users to share a spectrum of information, including spatio-temporal data, both publicly and within their community of interest in real-time. Scrutinizing knowledge from different types of available, rich, geo-tagged, and crowd-sourced data and incorporating it on a map has become more feasible. In this thesis, we first propose a constraint-aware route recommendation system in lack of physical infrastructure environment that leverages geo-tagged data in social media and user-generated content to identify upcoming traffic constraints and, thus, recommend an optimized path. We have designed and developed a system using a spatial grid index to inform users about upcoming constraints and calculate a new, optimized path in minimal response time. Later, the concept of “smart maps” will be introduced by collecting, managing, and integrating heterogeneous data sources in order to infer relevant knowledge-based layers. Unlike conventional maps, smart maps extract information about live events (e.g., concert, competition, incidents, etc.), online offers, and statistical analysis (e.g., dangerous areas) by encapsulating incoming semi- and un-structured data into structured generic packets. This methodology sets the ground for providing different intelligent services and applications. Moreover, developing smart maps requires an efficient and scalable processing and the visualization of knowledge-based layers at multiple map scales, thus allowing a smooth and clutter-free browsing experience. Finally, we introduce Hadath, a scalable and efficient system that extracts social events from a variety of unstructured data streams. The system comprises a data wrapping component which digests different types of data sources, and prepossesses data to generate structured data packets out of unstructured streams. As a result, live events can be displayed at different spatio-temporal resolutions, thus allowing a smooth and unique browsing experience. Finally, to validate our proposed system, we conducted experiments on real-world data streams. The final output of our system named Hadath creates a unique and dynamic map browsing experience
This dissertation addresses text-independent Automatic Speaker Verification (ASV) using features issued from Maximum Likelihood Linear Regression (MLLR) adaptation of Markov models with Gaussian mixture observation densities. MLLR transform coefficients obtained by adaptation of a speaker-independent model to speech data capture relevant cues characterizing a speaker. We focus on the MLLR-SVM paradigm classifying these features using Support Vector Machines (SVM). We propose a purely acoustic approach which avoids the need for transcripts and structural language constraints of previous systems by using Constrained MLLR (CMLLR) transforms together with Speaker Adaptive Training (SAT) of a Universal Background Model (UBM). We focus on multi-class (C)MLLR-SVM systems using LVCSR acoustic models. We perform a comprehensive experimental study of adaptation schemes exploring multiple axes such as front-end type, transform type, number of transforms, model type or training method.
Why and how do large companies deal with customer complaints? What effects does this treatment have on the internal regulation of firms? What can the customer expect? This thesis proposes to deal with this set of questions by an ethnographic survey conducted in two large French companies. Based on the analytical tools developed by Albert O. Hirschman, it provides a historical and sociological description of the complaint-handling practices. Thus, it wishes to contribute to the question of the influence of the client of a commodity on the companies that produce and sell it.
A case law is a corpus of judicial decisions representing the way in which laws are interpreted to resolve a dispute. It is essential for lawyers who analyze it to understand and anticipate the decision-making of judges. Its exhaustive analysis is difficult manually because of its immense volume and the unstructured nature of the documents. The estimation of the judicial risk by individuals is thus impossible because they are also confronted with the complexity of the judicial system and language. The automation of decision analysis enable an exhaustive extraction of relevant knowledge for structuring case law for descriptive and predictive analyses. In order to make the comprehension of a case law exhaustive and more accessible, this thesis deals with the automation of some important tasks for the expert analysis of court decisions. First, we study the application of probabilistic sequence labeling models for the detection of the sections that structure court decisions, legal entities, and legal rules citations. Then, the identification of the demands of the parties is studied. The proposed approach for the recognition of the requested and granted quanta exploits the proximity between sums of money and automatically learned key-phrases. We also show that the meaning of the judges'result is identifiable either from predefined keywords or by a classification of decisions. Finally, for a given category of demands, the situations or factual circumstances in which those demands are made, are discovered by clustering the decisions. For this purpose, a method of learning a similarity distance is proposed and compared with established distances. This thesis discusses the experimental results obtained on manually annotated real data. Finally, the thesis proposes a demonstration of applications to the descriptive analysis of a large corpus of French court decisions.
Current mobile devices, and mobile phones in particular, are equipped with different wireless technologies that increase and diversify their communication capabilities. The combined and effective use of these technologies offers various opportunities in terms of services and applications. However, it requires detailed analysis in terms of security and choice of the communication mean to use according to context-dependent criteria: energy costs, financial costs, preferences of the involved entities, privacy issues, etc. Our contribution to this project is the creation of collaborative applications adequately using the available wireless technologies on the considered equipments. In other words, we try to use the most appropriate communication mean (according to the criteria listed above) that two or more mobile devices can use to perform exchanges (without considering their respective positions). Then, the transparency of targets localization becomes a rule. We can synthesize the central question that we have chosen to study in the following manner: how to allow a set of mobile terminals (mobile phones in particular) to securely communicate using the most appropriate technology depending on the context? Our goal is to answer this question by defining a multilevel platform taking into account the different technologies available on the considered equipments. It is necessaty to identify the elements to consider in the design of the platform, to model them, to develop reference applications and to validate the relevance of the proposed solutions with qualitative and quantitative evaluations.
This thesis proposes a new approach to scientific writings which takes discourse markers as starting point. It is part of the framework of French for Academic Purposes. In this work, we are particularly interested in multi-word discourse markers and we integrate them into a broader concept of phraseology. The particularity of this work lies in linking linguistic descriptions of discourse markers and didactic transposition of these tokens with a corpus, which is still little discussed in the didactic francophone field. We aim to meet two main objectives of linguistic and didactic nature. The linguistic objectives are to set up a model for analyzing multi-word discourse markers that combines both syntactic and semantic properties and is totally reconfigurable to other discourse markers. Linguistic analyses will then be used for the teaching/learning of these units. For didactic purposes, this research aims to develop a methodology for teaching/learning discourse markers from the observation of the corpus. Methodological considerations proposed in the framework of the thesis provide attractive ways for teaching/learning these language elements and for making access to the academic writings easier to non-native students.
People are at the center of many computer vision tasks, such as surveillance systems or self-driving cars. They are also at the center of most visual contents, potentially providing very large datasets for training models and algorithms. While stereoscopic data has been studied for long, it is only recently that feature-length stereoscopic ("3D") movies became widely available. In this thesis, we study how we can exploit the additional information provided by 3D movies for person analysis. We first explore how to extract a notion of depth from stereo movies in the form of disparity maps. Leveraging the relative ease of the person detection task in 3D movies, we develop a method to automatically harvest examples of persons in 3D movies and train a person detector for standard color movies. We formulate the segmentation problem as a multi-label Conditional Random Field problem, and our method integrates an occlusion model to produce a layered, multi-instance segmentation. After showing the effectiveness of this approach as well as its limitations, we propose a second model which only relies on tracks of person detections and not on pose estimates. We formulate our problem as a convex optimization one, with the minimization of a quadratic cost under linear equality or inequality constraints. These constraints weakly encode the localization information provided by person detections. This method does not explicitly require pose estimates or disparity maps but can integrate these additional cues. Our method can also be used for segmenting instances of other object classes from videos. We evaluate all these aspects and demonstrate the superior performance of this new method.
Equivalence relations between queries allow to group the participants into communities. Those communities are then used as an abstraction to split the general organization problem into several easier and smaller subproblems. In order to stay language-independent, the organization is based on a simple and modular API, that rely on a query answering using views mechanism, well known in databases. Choice between the different rewritten queries is done using an adjustable cost model. Relations between communities are thus materialized by a spreading mechanism, a participant from one community joining the other(s) to contribute. This allows to avoid the capacities problem on the organization's abstract level, while efficiently taking care of it on the concrete one. Inside the communities, all the participants receive the common results they need using a spanning tree. The QTor approach, incrementally built, allows an efficient reduce of the processing and diffusion costs (processing cost being optimal in some cases, e.g. containment) with a reasonable latency, for a limited organization cost. Experiments have shown that the organization is flexible, regarding both the expressed queries and the participants'capacities. A demonstrator was built, allowing to both perform (automatic or interactive) simulations, and deploy the system over a real network, with a single.
Unsupervised information extraction in open domain gains more and more importance recently by loosening the constraints on the strict definition of the extracted information and allowing to design more open information extraction systems. In this new domain of unsupervised information extraction, this thesis focuses on the tasks of extraction and clustering of relations between entities at a large scale. The objective of relation extraction is to discover unknown relations from texts. A relation prototype is first defined, with which candidates of relation instances are initially extracted with a minimal criterion. To guarantee the validity of the extracted relation instances, a two-step filtering procedures is applied: the first step with filtering heuristics to remove efficiently large amount of false relations and the second step with statistical models to refine the relation candidate selection. A multi-level clustering procedure is design, which allows to take into account the massive data and diverse linguistic phenomena at the same time. First, the basic clustering groups similar relation instances by their linguistic expressions using only simple similarity measures on a bag-of-word representation for relation instances to form high-homogeneous basic clusters. Second, the semantic clustering aims at grouping basic clusters whose relation instances share the same semantic meaning, dealing with more particularly phenomena such as synonymy or more complex paraphrase. Different similarities measures, either based on resources such as WordNet or distributional thesaurus, at the level of words, relation instances and basic clusters are analyzed. Moreover, a topic-based relation clustering is proposed to consider thematic information in relation clustering so that more precise semantic clusters can be formed. Finally, the thesis also tackles the problem of clustering evaluation in the context of unsupervised information extraction, using both internal and external measures. For the evaluations with external measures, an interactive and efficient way of building reference of relation clusters proposed. The application of this method on a newspaper corpus results in a large reference, based on which different clustering methods are evaluated.
Linguistic analysis is a fundamental and essential step for natural language processing. It often includes part-of-speech tagging and named entity identification in order to realize higher level applications, such as information retrieval, automatic translation, question answers, etc. Since the word is the elementary unit for automated language processing, it is indispensable to segment sentences into words for Chinese language processing. In most existing system described in the literature, segmentation, part-of-speech tagging and named entity recognition are often presented as three sequential, independent steps. With these combinations of steps, segmentation can be improved by complementary information supplied by part-of-speech tagging and named entity recognition, and global analysis of Chinese improved. Consequently, this approach is not suitable for creating multilingual automatic analysis systems. This dissertation studies the integration Chinese automatic analysis into an existing multilingual analysis system LIMA. Firstly, the treatment for Chinese should be compatible and follow the same flow as other languages. And secondly, in order to keep the system coherent, it is preferable to employ common modules for all the languages treated by the system, including a new language like Chinese. To respect these constraints, we chose to realize the phases of segmentation, part-of-speech tagging and named entity recognition separately. Our modular treatment includes a specific module for Chinese analysis that should be reusable for other languages with similar linguistic features. After error analysis of this purely modular approach, we were able to improve our segmentation with enriched information supplied by part-ofspeech tagging, named entity recognition and some linguistic knowledge. In our final results, three specific treatments have been added into the LIMA system: a pretreatment based on a co-occurrence model applied before segmentation, a term tokenization relative to numbers written in Chinese characters, and a complementary treatment after segmentation that identifies certain named entities before subsequent part-of-speech tagging.
The system of determination of the French language differs from the system of determination in the Polish language. The suggested descriptions are formalized in databases which are designed for automatic processing of natural languages. In the thesis, the functioning and properties of determiners in the Polish language are systematically described. Additionally their syntactic-semantic properties which results in the division into predicative determination and argumentative determination are considered. The work contributes to the development of LDI electronic dictionaries.
To support health professionals in their clinical processes, several monitoring and medical care systems have been built and deployed in the hospital setting. These systems are mainly used to collect medical data on patients,analyze and present the outcomes in different ways. They represent support and assistance to health professionals in their decision making regarding the evolution in the health status of the patients followed. The use of such systems always requires an adaptation to both the medical field and the mode of intervention. It is necessary, in a hospital setting, to adapt and evolve these systems in a simple manner, limiting any corrective or evolutionary maintenance. Moreover, these systems should be able to consider dynamically the domain knowledge from medical experts. This approach allows especially the organization of the medical data collection by taking into account the patient¿s context, the ontology-based knowledge representation of the domain and permits the exploitation of the medical guidelines and the clinical experience. In continuity of our research team¿s previous work, we chose to expand with our approach, the E-care platform which is dedicated to monitoring and early detection of any abnormality of the health status of patients with chronic diseases. We were able to adapt easily the E-care platform for the various experiments that have been conducted,including EPHAD of the Mutualité Française in Anjou-Mayenne, Hautepierre hospital and Lausanne hospital (CHUV). The outcomes of these experiments have shown the effectiveness of the proposed approach. Where, the adaptation of the platform regarding to the domain and mode of intervention of each of these experiments is limited to the simple configuration. Furthermore, the proposed approach has attracted the interest of the medical staff regarding the organization of the medical data collection, and the exploitation of the medical knowledge which brings assistance to the health professionals for better decision making.
At first, we detail a method based on a particle filter to estimate at any time, the position of the signer's head, hands, elbows and shoulders in a monoview video. This method has been adapted to the French Sign Language in order to make it more robust to occlusion, inversion of the signer's hands or disappearance of hands from the video frame. Then, we propose a classification of the motion patterns that are frequently involved in the sign of production, thanks to the analysis of motion capture data. The parametric models associated to each sign pattern are used in the frame of automatic signe retrieval in a video from a filmed sign example.
Dependency parsing is an essential component of several NLP applications owing its ability to capture complex relational information in a sentence. These systems require a significant amount of annotated data and are thus targeted toward specific languages for which this type of data are available. Unfortunately, producing sufficient annotated data for low-resource languages is time- and resource-consuming. To address the aforementioned issue, the present study investigates three bootstrapping methods, namely, (1) multi-lingual transfer learning, (2) deep contextualized embedding, and (3) Co-training. Multi-lingual transfer learning is a typical supervised learning approach that can transfer dependency knowledge using multi-lingual training data based on multi-lingual lexical representations. Deep contextualized embedding maximizes the use of lexical features during supervised learning based on enhanced sub-word representations and language model (LM). Our approaches have the advantage of requiring only a small bilingual dictionary or easily obtainable unlabeled resources (e.g., Wikipedia) to improve parsing accuracy in low-resource conditions. We evaluated our parser on 57 official CoNLL shared task languages as well as on Komi, which is a language we developed as a training and evaluation corpora for low-resource scenarios. The evaluation results demonstrated outstanding performances of our approaches in both low- and high-resource dependency parsing in the 2017 and 2018 CoNLL shared tasks. A survey of both model transfer learning and semi-supervised methods for low-resource dependency parsing was conducted, where the effect of each method under different conditions was extensively investigated.
Where they have to adapt to volatile customers who want to find cheaper products and services and that are more corresponding to their needs. The SME is then confronted with problems of responsiveness and flexibility in responding to these customers. As an effect, it seeks to reduce the costs and time to market and to provide high quality and innovative goods and services. The SME's information system is an asset on which it can rely to implement this strategy and so to maximize its responsiveness and flexibility but also to reach the sought profitability and quality. These are key qualities that guarantee autonomy and recognition, qualities that are highly needed by any SME. Part of this information system is computerized. It stores and processes the information needed by the different decision-making, business and support processes that serve the enterprise's strategy. It is crucial to understand the features, interfaces and data that make up this computerized system and develop them according to the needs of SME. The SME is therefore tempted to embark, alone or accompanied, in so-called computerization projects i.e. projects for the development or improvement of its computerized system. We are interested in projects aimed at developing management applications of SMEs. The SME – then assuming the role of project owner – along with the development team – supporting the role of project management – have to share a common vision of the computerization needs. They are then called upon to carry out jointly requirements engineering (RE) activities. RE guides the SMEs to be able to describe and formalize its needs. It then allows the development team to specify more formally these needs as requirements which then define the required development work. RE is often carried out with the assistance of project owner support. This crucial step remains difficult for SMEs. It is most often performed by the development team itself to address the lack of resources, time and skills of SMEs. However, the involvement of the SME's members is vital to the success of any computerization project, especially if it permanently affects the functioning of the enterprise. This work, developed through a collaborative with the company RESULIS, consisted in developing a requirements engineering method which offers SMEs concepts, simple languages, modeling and verification means that are easily and intuitively manipulated and provide sufficient and relevant formalization of the SME's requirements. This method is based on principles derived from both enterprise modeling and systems engineering fields for requirements elicitation. Semi-formal verification and validation means are applied to guarantee some expected qualities of the resulting requirements. The method is also integrated in the model driven development cycle to enable a posteriori the production of prototypes and make interoperable the languages and tools used by both the SME and the development team.
Citizen science, in particular voluntary crowdsourcing, represents a little experimented solution to produce language resources for some languages which are still little resourced despite the presence of sufficient speakers online. We present in this work the experiments we have led to enable the crowdsourcing of linguistic resources for the development of automatic part-of-speech annotation tools. We have applied the methodology to three non-standardised languages, namely Alsatian, Guadeloupean Creole and Mauritian Creole. For different historical reasons, multiple (ortho)-graphic practices coexist for these three languages. The difficulties encountered by the presence of this variation phenomenon led us to propose various crowdsourcing tasks that allow the collection of raw corpora, part-of-speech annotations, and graphic variants. The intrinsic and extrinsic analysis of these resources, used for the development of automatic annotation tools, show the interest of using crowdsourcing in a non-standardized linguistic framework: the participants are not seen in this context a uniform set of contributors whose cumulative efforts allow the completion of a particular task, but rather as a set of holders of complementary knowledge. The resources they collectively produce make possible the development of tools that embrace the variation. The platforms developed, the language resources, as well as the models of trained taggers are freely available.
Since its emergence in the early 1990s, the notion of ontology has been quickly distributed in many areas of research. Given the promise of this concept, many studies focus on the use of ontologies in many areas like information retrieval, electronic commerce, semantic Web, data integration, etc.. The effectiveness of all this work is based on the assumption of the existence of a domain ontology that is already built an that can be used. However, the design of such ontology is particularly difficult if you want it to be built in a consensual way. If there are tools for editing ontologies that are supposed to be already designed, and if there are also several platforms for natural language processing able to automatically analyze corpus of texts and annotate them syntactically and statistically, it is difficult to find a globally accepted procedure useful to develop a domain ontology in a progressive, explicit and traceable manner using a set of information resources within this area. The goal of ANR DaFOE4App (Differential and Formal Ontology Editor for Application) project, within which our work belongs to, was to promote the emergence of such a set of tools. Unlike other tools for ontologies development, the platform DaFOE presented in this thesis does not propose a methodology based on a fixed number of steps with a fixed representation of theses steps. Indeed, in this thesis we generalize the process of ontologies development for any number of steps. The interest of such a generalization is, for example, to offer the possibility to refine the development process by inserting or modifying steps. We may also wish to remove some steps in order to simplify the development process. The aim of this generalization is for instance, for the overall process of ontologies development, to minimize the impact of adding, deleting, or modifying a step while maintaining the overall consistency of the development process. To achieve this, our approach is to use Model Driven Engineering to characterize each step through a model and then reduce the problem of switching from one step to another to a problem of models transformation. Established mappings between models are then used to semi-automate the process of ontologies development. The originality of the MQL language lies in its ability, through queries syntactically compact, to explore the graph of mappings using the transitivity property of mappings when retrieving informations.
Language Recognition is the problem of discovering the language of a spoken definition utterance. This thesis achieves this goal by using short term acoustic information within a GMM-UBM approach. The main problem of many pattern recognition applications is the variability of problem in the observed data. In the context of Language Recognition (LR), this troublesome variability is due to the speaker characteristics, speech evolution, acquisition and transmission channels. In the context of Speaker Recognition, the variability problem is solved by solution the Joint Factor Analysis (JFA) technique. Here, we introduce this paradigm to Language Recognition. The second, more technical assumption consists in the unwanted variability part to be thought to live in a low-dimensional, globally defined subspace. In this work, we analyze how JFA behaves in the context of a GMM-UBM LR framework. We also introduce and analyze its combination with Support Vector Machines(SVMs).The first JFA publications put all unwanted information (hence the variability) improvement into one and the same component, which is thought to follow a Gaussian distribution. This handles diverse kinds of variability in a unique manner. But in practice,we observe that this hypothesis is not always verified. We have for example the case, where the data can be divided into two clearly separate subsets, namely data from telephony and from broadcast sources. In this case, our detailed investigations show that there is some benefit of handling the two kinds of data with two separate systems and then to elect the output score of the system, which corresponds to the source of the testing utterance. For selecting the score of one or the other system, we need a channel source related analyses detector. We propose here different novel designs for such automatic detectors. In this framework, we show that JFA's variability factors (of the subspace) can be used with success for detecting the source. This opens the interesting perspective of partitioning the data into automatically determined channel source categories,avoiding the need of source-labeled training data, which is not always available. The JFA approach results in up to 72% relative cost reduction, compared to the overall results GMM-UBM baseline system. Using source specific systems followed by a score selector, we achieve 81% relative improvement.
The web explosion has led Information Retrieval (IR) to be extended and web search engines emergence. The conventional IR methods, usually intended for simple textual searches, faced new documents types and rich and scalable contents. The users, facing these evolutions, ask more for IR systems search results quality. In this context, the personalization main objective is improving results returned to the end user based sing on its perception and its interests and preferences. This thesis context is concerned with these different aspects. Its main objective is to propose new and effective solutions to the personalization problem. To achieve this goal, a spatial and semantic web personalization system integrating implicit user modeling is proposed. This system has two components: 1/ user modeling; /2 implicit users'collaboration through the construction of a users'models network. A system prototype was developed for the evaluation purpose that contains: a) user model quality evaluation; b) information retrieval quality evaluation; c) information retrieval quality evaluation with the spatial user model data; d) information retrieval quality evaluation with the whole user model data and the users'models network. Experiments showed amelioration in the personalized search results compared to a baseline web search.
Semantic knowledge is mandatory for Natural Language Processing. Unfortunately, classifications that have universal goals are an utopia. There exists systems that extracts semantic knowledge from specialized texts but it is well known that it is not possible to do such an extraction from texts said to be of "general" language. The goal of this doctoral dissertation is to show that this idea is false. We show that a thematic analysis of non-specialized texts (newspapers, newswires or HTML pages gathered from the Web) usually allows to reduce the problem to a classical one where the analysis of a technical corpus is done, but where the human interventions are limited. With our approach, the theme of text segments is detected by the statistical analysis of word distributions, designed notions of similarity and aggregation. They allow to aggregate the words of similar segments to build thematic domains where higher weighted words describe the theme. We then group the words that appear as the same argument of the same verb in the various text segments belonging to a theme. We have implemented our model in a system called SVETLAN'which has been tested on several French and English million words corpus. The empirical analysis of the results shows that, as anticipated, words are usually in a strong mutual semantic relation in the classes that are obtained, in the context determined by the theme.
This project aims modelling the acoustic-prosodic variations of short informational units in spontaneous speech, so to allow their classification in large semantic categories linked to their pragmatic use. The approach is based on the Language into Act theory (henceforth L-AcT: Cresti 2000 ; Moneglia and Raso, 2014 ; Cavalcante 2016, 2020), in which the informational functions (inclusive illocutionary ones) of spontaneous speech interactions are essentially organized by prosody. Speech is segmented into intonation units encapsulating sets of words into the same prosodic envelope. This creates a contrast between the words of different prosodic envelopes: segmentation thus leads to a functional organization of speech (Barth-Weingarten, 2016; Barbosa and Raso, 2018; Izre'el et al., forthcoming-a; Izre'el et al., forthcoming-b). The L-AcT proposes that there is an isomorphic relationship between intonation unit and informational unit: each intonation unit composing the utterance acquires informational value, with the exception of scanning units, produced voluntarily (to underline a point) or involuntarily (performance problems). Syntactic compositionality is a property of informational units, which establish functional relationships guided by intonation contours, without syntactic compositionality playing a necessary role in this (Cresti 2014). In the discourse flow, we distinguish between prosodic borders with terminal or non-terminal value, which can be automatically detected with good levels of recall and precision, based on their prosodic correlates (Teixeira, 2018; Teixeira, Barbosa and Raso, 2018; Raso, Teixeira and Barbosa, forthcoming). The terminal boundaries mark the end of a sequence composed of intonative non-terminal units, marked by the non-terminal boundaries and delineating the informational units. This thesis deals with the analysis of short informational units, i.e. those performed on a single phonological word and encapsulated in an intonative unit (preceded or followed by at least one non-terminal prosodic boundary). Short units can theoretically have as a function all informational values, covering all discourse markers as well as most textual units: this will allow the observation of a wide range of informational functions. By applying the classification process to these short units alone, prosodic variations due to other linguistic levels (hierarchical, etc.) are avoided. Moreover, words and expressions appearing on short units are often frequent in oral corpora, which allows analyses to be based on larger amounts of data. Data will be extracted from C-ORAL corpora in Brazilian Portuguese, Italian and American English (Cresti and Moneglia, 2005; Raso and Mello, 2012; Cavalcante and Ramos, 2016), with a possible application to French (the corpus exists but needs to be resegmented and informationally annotated). These corpora are prosodically and informationally segmented, in accordance with the premises of the L-AcT. L-AcT methods have already been applied for the analysis of different informational units, notably in this theoretical framework (Raso and Vieira, 2016; Moneglia and Cresti, 2018; Gobbo, 2019; Cavalcante, 2020), and have demonstrated the feasibility of this task. The project aims at characterizing the different prosodic structures observed on the target units, in the framework of an acoustical-prosodic analysis, which will aim at a clustering of prosodic shapes and an estimation of the best means of prosodic representation (the phonetic and phonological levels proposing variable description formats). On the basis of these descriptions of prosodic units, a learning process will have to model the links between prosodic forms and linguistic functions of the targeted units. These two processes will aim at testing the role played by prosody in the attribution of informational meaning for spontaneous speech, and also at evaluating the cross-linguistic similarity of signifier shapes, and if their use, distribution and frequency can vary from one language to another.
In the first part of this thesis, we present a new pronunciation variant generation method which works by adapting standard i.e., dictionary-based, pronunciations to a spontaneous style. Its strength and originality lie in exploiting a wide range of linguistic, articulatory and acoustic features and to use a probabilistic machine learning framework, namely conditional random fields (CRFs) and language models. Extensive experiments on the Buckeye corpus demonstrate the effectiveness of this approach through objective and subjective evaluations. Listening tests on synthetic speech show that adapted pronunciations are judged as more spontaneous than standard ones, as well as those realized by real speakers. Speech disfluencies are one of the most pervasive phenomena in spontaneous speech, therefore being able to automatically generate them is crucial to have more expressive synthetic speech. The proposed approach provides the advantage of generating several types of disfluencies: pauses, repetitions and revisions. To achieve this task, we formalize the problem as a theoretical process, where transformation functions are iteratively composed. We present a first implementation of the proposed process using CRFs and language models, before conducting objective and perceptual evaluations. These experiments lead to the conclusion that our proposition is effective to generate disfluencies, and highlights perspectives for future improvements.
Museums are considering the personalization trend to accommodate the diversity of visitors and their visiting practices. In order to support the spread of personalized visits, we question the contribution of tangible interactions, not only for visitors but also for museum professionals. How to help cultural mediators design personalized visits that reflect the diversity of visitor profiles? How to help visitors choose and follow the visit that suits their wishes and needs? We applied a user-centered design process with partner museums to design, implement and evaluate tangible tools to answer these questions. The user needs analysis (cultural mediators and visitors) allowed us to define six main characteristics to consider for the personalization of visits and to identify the concept of multi-criteria personalization. For the cultural mediators, we propose an interface concept combining the choice of visitor characteristics to constitute a profile and the dynamic monitoring of the personalized visits creation progress for each combination. We instantiated this concept using two interaction modalities, tangible and tactile, which we compared through an experimental study with museum mediators. For the visitors, we iteratively designed prototypes to help them choose personalized visits and conducted a pilot study in a partner museum. The prototypes designed during this thesis implement the token+constraint interaction paradigm. We propose a systematic literature review referencing the token+constraint interaction paradigm, as well as a heuristic grid of 24 properties divided into five categories that resume, synthesize and illustrate the concepts of the seminal article.
The adaptation may take different forms (evolution, alignment, merging, etc.), and represents several scientific challenges. One of the most important is to preserve the consistency of the ontology during the changes. To address this issue, we are interested in this thesis to study the ontology changes and we propose a formal framework that can evolve and merge ontologies without affecting their consistency. First we propose TGGOnto (Typed Graph Grammars for Ontologies), a new formalism for the representation of ontologies and their changes using typed graph grammars (TGG). Second, we propose EvOGG (Evolving Ontologies with Graph Grammars), an ontology evolution approach that is based on the TGGOnto formalism that avoids inconsistencies using an a priori approach. We focus on OWL ontologies and we address both: (1) ontology enrichment by studying their structural level and (2) ontology population by studying the changes affecting individuals and their assertions. The proposed approach consists of three steps: (1) the similarity search between concepts based on syntactic, structural and semantic techniques; (2) the ontologies merging by the algebraic approach SPO; (3) the global ontology adaptation with graph rewriting rules. To validate our proposals, we have developed several open source tools based on AGG (Attributed Graph Grammar) tool. These tools were applied to a set of ontologies, mainly on those developed in the frame of the CCAlps (Creatives Companies in Alpine Space) European project, which funded this thesis work.
This thesis is part of the RAPSODIE project which aims at proposing a speech recognition device specialized on the needs of deaf and hearing impaired people. Two aspects are studied: optimizing the lexical models and extracting para-lexical information. This detection aims to inform the deaf and hearing impaired people when a question is addressed to them
Analysing and formalising the emotional aspect of the Human-Machine Interaction is the key to a successful relation. Beyond and isolated paralinguistic detection (emotion, disfluences…), our aim consists in providing the system with a dynamic emotional and interactional profile of the user, which can evolve throughout the interaction. This profile allows for an adaptation of the machine's response strategy, and can deal with long term relationships. A multi-level processing of the emotional and interactional cues extracted from speech (LIMSI emotion detection tools) leads to the constitution of the profile. Low level cues (F0, energy, etc.), are then interpreted in terms of expressed emotion, strength, or talkativeness of the speaker. These mid-level cues are processed in the system so as to determine, over the interaction sessions, the emotional and interactional profile of the user. The profile is made up of six dimensions: optimism, extroversion, emotional stability, self-confidence, affinity and dominance (based on the OCEAN personality model and the interpersonal circumplex theories). The social behaviour of the system is adapted according to the profile, and the current task state and robot behaviour. Fuzzy logic rules drive the constitution of the profile and the automatic selection of the robotic behaviour. These determinist rules are implemented on a decision engine designed by a partner in the project ROMEO. We implemented the system on the humanoid robot NAO. Using these systems allowed us to collect emotional data in robotic interaction contexts, by controlling several emotion elicitation parameters. This thesis presents the results of these data collections, and offers an evaluation protocol for Human-Robot Interaction through systems with various degrees of autonomy.
Thanks to recent advances in artificial intelligence and natural language processing, the goal of the multidisciplinary project IA4Allergie is to convert vast amounts of textual data stored in healthcare data warehouses to extract and infer new information in order to adapt healthcare for patients suffering from allergic respiratory diseases.
This thesis studies the role of intrinsic motivation in the emergence and development of communicative systems in populations of artificial agents. To be more specific, our goal is to explore how populations of agents can use a particular motivation system called autotelic principle to regulate their language development and the resulting dynamics at the population level. To achieve this, we first propose a concrete implementation of the autotelic principle. The relation between the two elements is not steady but regularly becomes destabilised when new skills are learned, which allows the system to attempt challenges of increasing complexity. Then, we test the usefulness of the autotelic principle in a series of language evolution experiments. In the first set of experiments, a population of artificial agents should develop a language to refer to objects with discrete values. These experiments focus on how unambiguous communicative systems can emerge when the autotelic principle is employed to scaffold language development into stages of increasing difficulty. In the second set of experiments, agents should agree on a language to communicate with about colour samples. In this part, we explore how the motivation system can regulate the linguistic complexity of interactions for a continuous domain and examine the value of the autotelic principle as a mechanism to control several language strategies simultaneously. To summarise, we have shown through our work that the autotelic principle can be used as a general mechanism to regulate complexity in language emergence in an autonomous way for discrete and continuous domains.
Gastronomy and onomastics have never previously been the subject of a joint study from a linguistic point of view. The objective of this thesis is to provide a starting point for research on the structure, nature and place of proper names in French gastronomy. After considering the various problems related to the definition of such concepts as gastronomy and proper name, we achieved a synthesis of the main theoretical elements that form the basis for research on the names of dishes from a linguistic, artistic, historic or legislative perspective. First, the evolution of the proper name in gastronomy over the past 70 years is studied from a normative perspective through the lexical comparison of the first and the latest editions of the Larousse Gastronomique (1938 and 2007) and the establishment of a categorization of proper names. Second, proper names in a corpus reflecting the use was examined using Parisian restaurant menus and flyers advertising for food delivery. The comparison of the results for the two types of corpus will shed light on the differences, both quantitative and classificatory, in the use of proper names in a normative or in a creative context.
We present an automatic semantic annotation system for Korean on the EXCOM (EXploration COntextual for Multilingual) platform. The purpose of natural language processing is enabling computers to understand human language, so that they can perform more sophisticated tasks. EXCOM identifies semantic information in Korean text using our new method, the Contextual Exploration Method. Our system properly annotates approximately 90% of standard Korean sentences, and this annotation rate holds across text domains.
This thesis focuses on the modelisation of syntax and syntax-semantics interface of sentences, and investigate how the control of the surgeneration caused by the treatment of linguistics movements with higher order types can take place at the level of derivation structures. For this purpose, we look at the possibility to extend the type system of Abstract Categorial Grammars with the constructions of disjoint sum, cartesian product and dependent product, which enable syntactic categories to be labeled by feature structures. At first, we demonstrate that the calculus associated with this extension enjoy the properties of confluence and normalization, by which beta-equivalence can be computed in the grammatical formalism. We also reduce the same problem for beta-eta-equivalence to a few hypothesis. Then, we show how this feature structures can be used to control linguistics movements, through the examples of case constraints, extraction islands for overt and covert movements and multiples interrogative extractions, and we discuss the relevancy of operating these controls on the derivation structures
This thesis focuses on the identification of multi-word expressions, addressed through a transition-based system. A multi-word expression (MWE) is a linguistic construct composed of several elements whose combination shows irregularity at one or more linguistic levels. Identifying MWEs in context amounts to annotating the occurrences of MWEs in texts, i.e. to detecting sets of tokens forming such occurrences. Transition-based analysis is a famous NLP technique to build a structured output from a sequence of elements, applying a sequence of actions (called «transitions») chosen from a predefined set, to incrementally build the output structure. In this thesis, we propose a transition system dedicated to MWE identification within sentences represented as token sequences, and we study various architectures for the classifier which selects the transitions to apply to build the sentence analysis. The first variant of our system uses a linear support vector machine (SVM) classifier. The following variants use neural models: a simple multilayer perceptron (MLP), followed by variants integrating one or more recurrent layers. The preferred scenario is an identification of MWEs without the use of syntactic information, even though we know the two related tasks. We further study a multitasking approach, which jointly performs and take mutual advantage of morphosyntactic tagging, transition-based MWE identification and dependency parsing. The thesis comprises an important experimental part. Firstly, we studied which resampling techniques allow good learning stability despite random initializations. Secondly, we proposed a method for tuning the hyperparameters of our models by trend analysis within a random search for a hyperparameter combination. Our variants produce very good results, including state-of-the-art scores for many languages in the PARSEME 1.0 and 1.1 datasets. One of the variants ranked first for most languages in the PARSEME 1.0 shared task. By the way, our models have poor performance on MWEs that are were not seen at learning time.
The Autonomous Vehicle is meant to drive itself, without any driver intervention, whatever the driving situation. This vehicle includes a new function, called AD, for Autonomous Driving, function. This function can be in different states (Available, Active for example) according to environmental conditions evolution. This states change is managed by a supervision function, named AD Supervision. The main goal of my works consists in guaranteeing that AD function remains always in a safe state. In other words, the AD Supervision must always respect all the functional and safety requirements that specify its behavior. These two fields contribute to the design of the same function but distinguish at several aspects: objectives, constraints, planning, tools… In our case study, these differences are illustrated by considered requirements: the functional requirements are allocated to global AD function, while the safety requirements specify the behavior of local redundant sub-functions ensuring a continuous service in case of failure. The consistency of the two perspectives as early as possible in the design phase and in an industrial context, is the central problematic addressed. The safety issues due to Autonomous Vehicle make this topic essential for the automotive manufacturers. To meet these concerns, we proposed a tooled and collaborative approach for safe design of AD Supervision. This approach is integrated in the normative processes (standards ISO 26262 and ISO 15288) as well as in the internal design processes at Renault. It is based on formal verification by model checking, parallel composition of finite sate automata and technical expertise. This approach advocates the utilization of a same formalism (state automata) by the two professions to perform activities sharing a common goal: behavior requirements verification in preliminary design phase. A method to translate requirements into formal properties and to build state models has been deployed. The result is a progressive consolidation of treated requirements, initially expressed in free natural language. The potential ambiguities, inconsistencies and incompleteness are exhibited and treated.
Business process management approaches are now an integral part of the life and of the performance quest in organizations. Nevertheless, this field remains historically compartmentalized, particularly concerning the role of the information system in the company. The results of this work were applied to a real industrial case, which demonstrated the relevance and necessity of a convergence between an information system and a company's business process approach.
Contemporary lexicography provide ressources offering many opportunities for natural language processing tasks. It begins with a selective overview of formalisation and computerisation for study of lexicon, wich defines the principle of exploration : the nodes are similar to objects, which have some attributes and edges represent relations. Then two sets of exploratory experiments are conducted. The first one shows that the resource formalisation makes it possible to detect automatically analogies consistent with intuition, that several kind of analogical explorations are possible and that the approach allows to check the consistency of the resource and to bring out lexical rules. The second one is focused around the concept of lexical derivation configurations. It shows how grouping of analogous subgraphs reveals recurrent connections. The progress status of the resource doesn't enable us to obtain successfully completed rules and models, but results are nontheless encouraging. Such knowledge can be used to identify linguitic phenomena and to design instruments to support lexicographic activity.
This thesis explains and presents our approach of rule-based system of arabic named entity recognition and classification. This work involves two disciplines: linguistics and computer science. Computer tools and linguistic rules are merged to give birth to a new discipline: Natural Languge Processsing, which operates in different levels (morphosyntactic, syntactic, semantic, syntactico-semantic…). This work of thesis is incorporated within the general domain of natural language processing, but it particularly falls within the scope of the continuity of the accomplished work in terms of morphosyntactic analysis and the realisation of lexical data bases of SAMIA and then DIINAR as well as the accompanying scientific recearch. To understand what it is about, it was important to start with named entity definition. To carry out this task, we distinguished between two main named entity types: pur proper name and descriptive named entities. We have also established a referential classification on the basis of different classes and sub-classes which constitue the reference for our semantic annotations. Nevertheless, we are confronted with two major difficulties: lexical ambiguity and the frontiers of complex named entities. Our system adoptes a syntactico-semantic rule-based approach. After Level 0 of morpho-syntactic analysis, the system is made up of five levels of syntactic and syntactico-semantic patterns based on tne necessary linguisic information (i.e. morphosyntactic, syntactic, semantic and syntactico-semantic information). This work has obtained very good results in termes of precision, recall and F-measure. The output of our system has an interesting contribution in different applications of the natural language processing especially in both tasks of information retrieval and information extraction. In fact, we have concretely exploited our system output in both applications (information retrieval and information extraction). In addition to this unique experience, we envisage in the future work to extend our system into the sentence extraction and classification, in which classified entities, mainly named entities and verbs, play respectively the role of arguments and predicates. The second objective consists in the enrichment of different types of lexical resources such as ontologies.
The repurposing of clinical data for research has become widespread with the development of clinical data warehouses. These data warehouses are modeled to integrate and explore structured data related to thesauri. These data come mainly from machine (biology, genetics, cardiology, etc.) but also from manual data input forms. The production of care is also largely providing textual data from hospital reports (hospitalization, surgery, imaging, anatomopathologic etc.), free text areas in electronic forms. This mass of data, little used by conventional warehouses, is an indispensable source of information in the context of rare diseases. Indeed, the free text makes it possible to describe the clinical picture of a patient with more precision and expressing the absence of signs and uncertainty. Particularly for patients still undiagnosed, the doctor describes the patient's medical history outside any nosological framework. This wealth of information makes clinical text a valuable source for translational research. However, this requires appropriate algorithms and tools to enable optimized re-use by doctors and researchers. We present in this thesis the data warehouse centered on the clinical document, which we have modeled, implemented and evaluated. In three cases of use for translational research in the context of rare diseases, we attempted to address the problems inherent in textual data: (i) recruitment of patients through a search engine adapted to textual (data negation and family history detection), (ii) automated phenotyping from textual data, and (iii) diagnosis by similarity between patients based on phenotyping. These methods and algorithms were integrated into the software Dr Warehouse developed during the thesis and distributed in Open source since September 2017.
In the latest years, the Web has shifted from a read-only medium where most users could only consume information to an interactive medium allowing every user to create, share and comment information. The downside of social media as an information source is that often the texts are short, informal and lack contextual information. On the other hand, the Web also contains structured Knowledge Bases (KBs) that could be used to enrich the user-generated content. This dissertation investigates the potential of exploiting information from the Linked Open Data KBs to detect, classify and track events on social media, in particular Twitter. More specifically, we address 3 research questions: i) How to extract and classify messages related to events? ii) How to cluster events into fine-grained categories? and 3) Given an event, to what extent user-generated contents on social medias can contribute in the creation of a timeline of sub-events? We provide methods that rely on Linked Open Data KBs to enrich the context of social media content; we show that supervised models can achieve good generalisation capabilities through semantic linking, thus mitigating overfitting; we rely on graph theory to model the relationships between NEs and the other terms in tweets in order to cluster fine-grained events. Finally, we use in-domain ontologies and local gazetteers to identify relationships between actors involved in the same event, to create a timeline of sub-events. We show that enriching the NEs in the text with information provided by LOD KBs improves the performance of both supervised and unsupervised machine learning models.
Recently, Convolutional Neural Networks have become the state-of-the-art soluion(SOA) to most computer vision problems. In order to achieve high accuracy rates, CNNs require a high parameter count, as well as a high number of operations. This greatly complicates the deployment of such solutions in embedded systems, which strive to reduce memory size. Indeed, while most embedded systems are typically in the range of a few KBytes of memory, CNN models from the SOA usually account for multiple MBytes, or even GBytes in model size. In this manuscript, the main levers allowing to tailor computational complexity of a generic CNN-based object detector are identified and studied. In order to perform object detection in an efficient way, the detection process is divided into two stages. The first stage involves a region proposal network which allows to trade-off recall for the number of operations required to perform the search, as well as the number of regions passed on to the next stage. Furthermore, CNNs also exhibit properties that confirm their over-dimensionment. This over-dimensionement is one of the key success factors of CNNs in practice, since it eases the optimization process by allowing a large set of equivalent solutions. However, this also greatly increases computational complexity, and therefore complicates deploying the inference stage of these algorithms on embedded systems. In order to ease this problem, we propose a CNN compression method which is based on Principal Component Analysis (PCA). PCA allows to find, for each layer of the network independently, a new representation of the set of learned filters by expressing them in a more appropriate PCA basis. This PCA basis is hierarchical, meaning that basis terms are ordered by importance, and by removing the least important basis terms, it is possible to optimally trade-off approximation error for parameter count. Through this method, it is possible to compress, for example, a ResNet-32 network by a factor of ×2 both in the number of parameters and operations with a loss of accuracy &lt;2%. It is also shown that the proposed method is compatible with other SOA methods which exploit other CNN properties in order to reduce computational complexity, mainly pruning, winograd and quantization. Furthermore, parallelizing the PCA compressed network over 8 PEs achieves a x11.68 speed-up with respect to the original network running on a single PE.
We are interested, in this thesis, to the study of mixture models and generalized linear models, with an application to co-infection data between arboviruses and malaria parasites. After a first part dedicated to the study of co-infection using a multinomial logistic model, we propose in a second part to study the mixtures of generalized linear models. The proposed method to estimate the parameters of the mixture is a combination of a moment method and a spectral method. Finally, we propose a final section for studing extreme value mixtures under random censoring. The estimation method proposed in this section is done in two steps based on the maximization of a likelihood.
With the emergence of the Web of Data, most notably Linked Open Data (LOD), an abundance of data has become available on the web. However, LOD datasets and their inherent subgraphs vary heavily with respect to their size, topic and domain coverage, the schemas and their data dynamicity (respectively schemas and metadata) over the time. To this extent, identifying suitable datasets, which meet specific criteria, has become an increasingly important, yet challenging task to support issues such as entity retrieval or semantic search and data linking. Particularly with respect to the interlinking issue, the current topology of the LOD cloud underlines the need for practical and efficient means to recommend suitable datasets: currently, only well-known reference graphs such as DBpedia (the most obvious target), YAGO or Freebase show a high amount of in-links, while there exists a long tail of potentially suitable yet under-recognized datasets. This problem is due to the semantic web tradition in dealing with "finding candidate datasets to link to", where data publishers are used to identify target datasets for interlinking. While an understanding of the nature of the content of specific datasets is a crucial prerequisite for the mentioned issues, we adopt in this dissertation the notion of "dataset profile" - a set of features that describe a dataset and allow the comparison of different datasets with regard to their represented characteristics. Our first research direction was to implement a collaborative filtering-like dataset recommendation approach, which exploits both existing dataset topic proles, as well as traditional dataset connectivity measures, in order to link LOD datasets into a global dataset-topic-graph. This approach relies on the LOD graph in order to learn the connectivity behaviour between LOD datasets. However, experiments have shown that the current topology of the LOD cloud group is far from being complete to be considered as a ground truth and consequently as learning data. Facing the limits the current topology of LOD (as learning data), our research has led to break away from the topic proles representation of "learn to rank" approach and to adopt a new approach for candidate datasets identication where the recommendation is based on the intensional profiles overlap between different datasets. By intensional profile, we understand the formal representation of a set of schema concept labels that best describe a dataset and can be potentially enriched by retrieving the corresponding textual descriptions. This representation provides richer contextual and semantic information and allows to compute efficiently and inexpensively similarities between proles. We identify schema overlap by the help of a semantico-frequential concept similarity measure and a ranking criterion based on the tf*idf cosine similarity. The experiments, conducted over all available linked datasets on the LOD cloud, show that our method achieves an average precision of up to 53% for a recall of 100%. Furthermore, our method returns the mappings between the schema concepts across datasets, a particularly useful input for the data linking step. In order to ensure a high quality representative datasets schema profiles, we introduce Datavore| a tool oriented towards metadata designers that provides ranked lists of vocabulary terms to reuse in data modeling process, together with additional metadata and cross-terms relations. The tool relies on the Linked Open Vocabulary (LOV) ecosystem for acquiring vocabularies and metadata and is made available for the community.
Within the context of collaborative enterprise information systems, these works aim to propose an approach for assessing the interoperability and the non-interoperability. The majority of these costs are attributable to the time and resources spent to put in place interfaces for exchanging information. This mainly affects enterprise global performance by increasing the cost and the delay to obtain the expected services. We suggest to address enterprise interoperability measurement in order to allow to any enterprise to fully evaluate, a priori, its own capacity to interoperate, and therefore to anticipate possible problems before a partnership. Our works consist in defining indicators and metrics to quantify and then to qualify the interoperability between the enterprise systems and to propose some improvement strategies when the evaluated interoperability level is not sufficient
This thesis has two principal aims. In the first place, we would like to offer an overview of the current academic knowledge, both theoretical and empirical, of the processes of linguistic accommodation between interlocutors, in a general sense, and of the rhythmic characteristics of the Spanish language, in particular. In the second place, we present two empirical studies designed to analyze the influence of sentence-level rhythmic regularity and phonological phrasing on the processes of linguistic accommodation. Taken together, the data gathered in this thesis indicate that regular rhythmic sentences, arranged in accentual groups, generate a greater amount of resemblance between Spanish speakers in terms of rhythm and F0 range, with respect to irregular rhythmic sentences and sentences arranged in accentual feet. Moreover, a lower value of F0 mean and a narrower F0 range were observed during the use of both regular rhythmic sentences and sentences arranged in accentual groups compared to the opposite conditions. In addition, some known facts related to women having a higher F0 mean, a wider F0 range, and speaking slower regarding men were also found during the first experiment.
Companies, administrations, and sometimes individuals, have to face many frauds on documents they receive from outside or process internally. Invoices, expense reports, receipts...any document used as proof can be falsified in order to earn more money or not to lose it. In France, losses due to fraud are estimated at several billion euros per year. Since the flow of documents exchanged, whether digital or paper, is very important, it would be extremely costly and time-consuming to have them all checked by fraud detection experts. That's why we propose in our thesis a system for automatic detection of false documents. While most of the work in automatic document detection focuses on graphic clues, we seek to verify the textual information in the document in order to detect inconsistencies or implausibilities. To do this, we first compiled a corpus of documents that we digitized. After correcting the characters recognition outputs and falsifying part of the documents, we extracted the information and modelled them in an ontology, in order to keep the semantic links between them. The information thus extracted, and increased by its possible disambiguation, can be verified against each other within the document and through the knowledge base established. The semantic links of ontology also make it possible to search for information in other sources of knowledge, particularly on the Internet.
In this thesis, we present two computer models to structure textual information for large databases of medieval charters. The two models, one applied to the recognition of named entities, the other to the detection of parts of the diplomatics discourse, are supervised Conditional random fields (CRF) models trained on a hand-annotated corpus of medieval charters (Corpus Burgundiae Medii Aevi or CBMA). The main Named Entity Recognition model has proven to be robust in its application to widely varying corpora in size, chronology and origin. The secondary model detecting parts of the diplomatic discourse, although less efficient, remains valid as a structuring tool. At the moment both can be used for indexing and studying a wide variety of diplomatics sources, thus saving huge human efforts. We have developed different solutions to overcome the gap between model's dependence on its original training-set and its ability to be applied to other corpora. Similarly, various corrections and additions were made to the golden-corpus from several historical and linguistic analysis concerning writing phenomena in charters, which greatly helped to improve the initial performance. In a later step we applied our automatic tools in the recognition of names of people, places and parts of the diplomatics discourse on thousands of charters from the CBMA corpus in order to study different questions concerning historical science and diplomatics. These studies concern the semi-automatic dating of a non-dated cartulary; the evolution of the spatial vocabulary in the charters of the central Middle Ages and the indexing of charters from their scriptural modules, in particular formulae of the charter protocols. This studies has a twofold purpose: on the one hand have shown different strategies for abstracting and adapting to the automatic processing well-known methods of research in history; on the other hand, seek to provide us tools with an applicative framework to obtain relevant knowledge to the historical science using massive processing.
The continuous increasing needs in medicine and healthcare, accentuate the need of well-adapted medical alert systems. Such alert systems may be used by a variety of patients and medical actors. These systems should allow monitoring a wide range of medical variables. Detected alerts have two quality indices. The applicability index which indicates how well a patient is affected by the alert, and the confidence index, which expresses the reliability of the alert concerning the freshness of the data used in its detection. Quality indices associated with a detected alert are calculated using information related to an alert situation configured by the user. An alert situation is defined from multiple activation conditions. An activation condition is constructed from a linguistic value expressing the state (e.g. high temperature) or trend (e.g. systolic blood pressure rising) of an observable entity (temperature, systolic blood pressure, etc.). When the alert condition is evaluated, the system uses knowledge previously prepared by users regarding linguistic values. That is, what linguistic value best represents a quantitative value in a specific context. Since many alerts can be detected, we define a notification policy to notify only the relevant alerts in order to keep the users'interest. First alerts are filtered from the quality indices. Of the remaining alerts, the system filters by expressiveness: to keep more sustainable trends and the most expressive linguistic values. Then, in the case of consecutive alerts, the system keeps only those that fulfill the user preferences, such as those whose applicability index increases. The ultimate goal is to keep the user loyalty to the alert system by providing quality service. The user appropriates the system while he defines the alert situation. Thus, he is able to adapt the alert situation by himself to the context to obtain better alerts. The adaptation is guided by quality indices used to reduce false positives and false negatives as well as to control the over-alerting. We propose to leverage existing systems by providing dynamism and evolution features, as well as facilities for setup and real-time adaptation to the context of use in order to fully exploit the observations.
The Information Extraction from clinical notes provides relevant information to identify adverse side effects in post-marketing surveillance of medications (Pharmacovigilance), which is more difficult to discover by traditional medical studies since patients are taking several treatments at the same time. In recent years, data mining techniques have allowed to discover knowledge stored in big datasets, such as the clinical records collected by hospitals throughout patient's life. The goal of this work is identify adverse side effects caused by treatments. This problem is divided Named Entity Recognition (NER) and Relation Extraction tasks. Nowadays, supervised approaches based on Deep Learning and Machine Learning algorithms solve this problem in the state of the art. These supervised systems require rich features in order to learn efficient models during training, therefore, we focus on building comprehensive word representations (the input of the neural network), using character-based word representations and word representations. The proposed representation improves the performance of the baseline model, and the final model reached the performances of state of the art methods. Then we have extracted contextual information through Deep Learning models and other different features obtained from the relations, in order to identify the Adverse Drug Reaction relations. The proposed model improved the overall accuracy and the extraction of Adverse Drug Reaction compared to the baseline, indicating the effectiveness of combining Deep Learning models and extensive feature engineering.
This thesis focuses on text Automatic Summarization and particularly on Update Summarization. This research problem aims to produce a differential summary of a set of new documents with regard to a set of old documents assumed to be known. It thus adds two issues to the task of generic automatic summarization: the temporal dimension of the information and the history of the user. In this context, the work presented here is based on an extractive approach using integer linear programming (ILP) and is organized around two main axes: the redundancy detection between the selected information and the user history and the maximization of their saliency. For the first axis,we were particularly interested in the exploitation of inter-sentence similarities to detect the redundancies between the information of the new documents and those present in the already known ones, by defining a method of semantic clustering of sentences. Concerning our second axis, we studied the impact of taking into account the discursive structure of documents, in the context of the Rhetorical Structure Theory (RST), to favor the selection of information considered as the most important. The benefit of the methods thus defined has been demonstrated in the context of evaluations carried out on the data of TAC and DUC campaigns. Finally, the integration of these semantic and discursive criteria through a delayed fusion mechanism has proved the complementarity of these two axes and the benefit of their combination.
With the growing web, a number of applications seek to meet the needs of users or machines having diverse cultural backgrounds. From this context of cultural diversity arises conflicts linked to different world conceptions. Offering adaptated services requires the integration of a form of cultural awareness in the system. An artificial cultural awareness is composed of formal cultural representations and mediations providing the system with the means to interpret the represented cultures and to determine their differences. Those coarse-grained models, even though they are adapted, limit the possible understanding of the represented cultures. As a consequence they constitute a bottleneck for the development of culturally-aware systems. I study the construction, the formalisation and the mediation of these emic cultural representations. My main contributions are the design and validation of, in one hand, a new semi-automatic ethnographic process for building emic models through text-mining, in another hand, an emic artificial cultural awareness based on the mapping of cultural ontologies coming from those models.
Consider an Erdős-Rényi (ER) graph with edge probability q and size n containing a planted subgraph of size m and probability p. We derive a statistical test based on the eigenvalue and eigenvector properties of a suitably defined matrix to detect the planted subgraph. We analyse the effect of side-information on the detectability threshold of Belief Propagation (BP) applied to the above problem. We show that BP correctly recovers the subgraph even with noisy side-information for any positive value of an effective SNR parameter. This is in contrast to BP without side-information which requires the SNR to be above a certain threshold. Finally, we study the asymptotic behaviour of PageRank on a class of undirected random graphs called fast expanders, using Random Matrix Theoretic techniques. We show that PageRank can be approximated for large graph sizes as a convex combination of the normalized degree vector and the personalization vector of the PageRank, when the personalization vector is sufficiently delocalized. Subsequently, we characterize asymptotic PageRank on Stochastic Block Model (SBM) graphs, and show that it contains a correction term that is a function of the community structure.
Learning stochastic models generating sequences has many applications in natural language processing, speech recognitions or bioinformatics. Traditional learning algorithms such as the one of Baum-Welch are iterative, slow and may converge to local optima. A recent alternative is to use the Method of Moments (MoM) to design consistent and fast algorithms with pseudo-PAC guarantees. However, MoM-based algorithms have two main disadvantages. First, the PAC guarantees hold only if the size of the learned model corresponds to the size of the target model. Second, although these algorithms learn a function close to the target distribution, most do not ensure it will be a distribution. Thus, a model learned from a finite number of examples may return negative values or values that do not sum to one. This thesis addresses both problems. First, we extend the theoretical guarantees for compressed models, and propose a regularized spectral algorithm that adjusts the size of the model to the data. Then, an application in electronic warfare is proposed to sequence of the dwells of a super-heterodyne receiver.
The present dissertation investigates the argument structure of two groups of Italian parasyntheticverbs: denominal verbs paraphrased as "make X become N", where N is the base noun (henceforth BN); adjectival verbs paraphrased as "make X more A", where A is the base adjective. The dissertation starts with three chapters of general interest. The first one describes new experimental methods that can be employed in generative linguistics. The second and third one describe useful frameworks and the morphological process of parasynthesis. The first part of the dissertation analyses BNs. It is shown by means of several experiments that Italian native speakers accept the pseudo-resultative construction. Results of a comparative study with French are reported and show that French behaves differently to Italian in this respect.4The second part analyses stativity diagnostics and apply them in the study of DPVs. The last chapter applies stativity diagnostics in the natural language processing domain.
Hierarchical image representations have been widely used in the image classification context. Such representations are capable of modeling the content of an image through a tree structure. In this thesis, we investigate kernel-based strategies that make possible taking input data in a structured form and capturing the topological patterns inside each structure through designing structured kernels. We develop a structured kernel dedicated to unordered tree and path (sequence of nodes) structures equipped with numerical features, called Bag of Subpaths Kernel (BoSK). It is formed by summing up kernels computed on subpaths (a bag of all paths and single nodes) between two bags. We also propose a scalable version of BoSK (SBoSK for short), using Random Fourier Features technique to map the structured data in a randomized finite-dimensional Euclidean space, where inner product of the transformed feature vector approximates BoSK. This strategy allows tile/sub-image classification. Further relying on (S)BoSK, we introduce a novel multi-source classification approach that performs classification directly from a hierarchical image representation built from two images of the same scene taken at different resolutions, possibly with different modalities. Evaluations on several publicly available remote sensing datasets illustrate the superiority of (S)BoSK compared to state-of-the-art methods in terms of classification accuracy, and experiments on an urban classification task show the effectiveness of proposed multi-source classification approach.
The main objective of our thesis paper is to examine the intelligibility of erroneous prepositional uses produced by French learners of English. We also propose certain effective pedagogical approaches to teaching English prepositions/particles. The results of our corpus analysis allow us to observe the extent to which erroneous spatial prepositions may affect the intelligibility of the transferred message.
In many fields, novel technologies employed in information acquisition and measurement (e.g. phenotyping automated greenhouses) are at the basis of a phenomenal creation of data. In particular, we focus on two real use cases: plants observations in botany and phenotyping data in biology. Our contributions can be, however, generalized to Web data. In addition to their huge volume, data are also distributed. Indeed, each user stores their data in many heterogeneous sites (e.g. personal computers, servers, cloud); yet he wants to be able to share them. Thus, the global objective of this work is to define a set of techniques enabling sharing and discovery of data in heterogeneous distributed environment, through the use of search and recommendation approaches. Diversification techniques allow users to receive results with better novelty while avoiding redundant and repetitive content. By introducing a distance between each result presented to the user, diversity enables to return a broader set of relevant items. However, few works exploit profile diversity, which takes into account the users that share each item. In this work, we show that in some scenarios, considering profile diversity enables a consequent increase in results quality: surveys show that in more than 75% of the cases, users would prefer profile diversity to content diversity. Additionally, in order to address the problems related to data distribution among heterogeneous sites, two approaches are possible. First, P2P networks aim at establishing links between peers (nodes of the network): creating in this way an overlay network, where peers directly connected to a given peer p are known as his neighbors. This overlay is used to process queries submitted by each peer. However, in state of the art solutions, the redundancy of the peers in the various neighborhoods limits the capacity of the system to retrieve relevant items on the network, given the queries submitted by the users. In this work, we show that introducing diversity in the computation of the neighborhood, by increasing the coverage, enables a huge gain in terms of quality. The second category of approaches is called multi-site. Generally, in state of the art multi-sites solutions, the sites are homogeneous and consist in big data centers. A prototype regrouping all contributions have been developed, with two versions addressing each of the use cases considered in this thesis.
In this thesis, we were interested in the impact of the quantity and quality of information exchanged between individuals in a group on their collective performance in two very specific types of tasks. In a first series of experiments, subjects had to estimate quantities sequentially, and could revise their estimates after receiving the average estimate of other subjects as social information. We controlled this social information through virtual participants (which number we controlled) giving information (which value we controlled), unknowingly to the subjects. We showed that when subjects have little prior knowledge about a quantity to estimate, (the logarithms of) their estimates follow a Laplace distribution. Since the median is a good estimator of the center of a Laplace distribution, we defined collective performance as the proximity of the median (log) estimate to the true value. We found that after social influence, and when the information provided by the virtual agents is correct, the collective performance increases with the amount of information provided (fraction of virtual agents). We also analysed subjects'sensitivity to social influence, and found that it increases with the distance between personal estimate and social information. These analyses made it possible to define five behavioral traits: to keep one's opinion, to adopt that of others, to compromise, to amplify social information or to contradict it. Our results showed that the subjects who adopt the opinion of others are the ones who best improve their performance because they are able to benefit from the information provided by the virtual agents. We then used these analyses to construct and calibrate a model of collective estimation, which quantitatively reproduced the experimental results and predicted that a limited amount of incorrect information can counterbalance a cognitive bias that makes subjects underestimate quantities, and thus improve collective performance. Further experiments have validated this prediction. In a second series of experiments, groups of 22 pedestrians had to segregate into clusters of the same "color", without visual cue (the colors were unknown), after a short period of random walk. To help them accomplish their task, we used an information filtering system (analogous to a sensory device such as the retina), taking all the positions and colors of individuals in input, and returning an acoustic signal to the subjects (emitted by tags attached to their shoulders) when the majority of their k nearest neighbors was of a different color from theirs.
This PhD thesis, conducted in cooperation with ONERA, focuses on active 3D object recognition by an autonomous visual agent. Whereas in passive recognition, acquisition modalities of observations are fixed and may generate ambiguities, active recognition exploits the possibility of controling these modalities online in a sequential inference process in order to remove these ambiguities. The aim of this work is to design, in a statistical learning framework, planning strategies in the acquisition of information while achieving a realistic implementation of active recognition. The first part of the work is dedicated to learning to plan. The second part of this work focuses on maximally exploiting observations acquired during recognition. The possibility of an active multi-scale recognition is investigated to allow an interpretation as soon as the sequential acquisition process begins. Observations are also used to robustly estimate the pose of the object to ensure consistency between the planned and actual modality of the visual agent.
This thesis focuses on zero-shot visual recognition, which aims to recognize images from unseen categories, i.e. categories not seen by the model during training. After categorizing existing methods into three main families, we argue that ranking methods habitually make several detrimental implicit assumptions. We propose to adapt the usual formulation of the hinge rank loss so that such methods may take inter and intra-class relations into account. We also propose a simple process to address the gap between accuracies on seen and unseen classes, from which these methods frequently suffer in a generalized zero-shot learning setting. In our experimental evaluation, the combination of these contributions enables our proposed model to equal or surpass the performance of generative methods, while being arguably less restrictive. In a second part, we focus on the semantic representations used in a large-scale zero-shot learning setting. In this setting, semantic information customarily comes from word embeddings of the class names. We argue that usual embeddings suffer from a lack of visual content in training corpora. We thus propose new visually oriented text corpora as well as a method to adapt word embedding models to these corpora. We further propose to complete unsupervised representations with short descriptions in natural language, whose generation requires minimal effort when compared to extensive attributes.
Construction of ontologies is a tedious task which still requires a great amount of manual work. Texts, as knowledge sources, can help, but TALN tools stop at linguistic level. Manual conceptualization fill the gap between a linguistic model and a conceptual model. In this thesis we study how a symbolic clustering method, Formal Concept Analysis, can be combined with a linguistic model to help the knowledge engineer. We have experimented on three different domains represented by same-sized corpora. We propose solutions that combine FCA and terminological analysis, to let the computer suggest usefull clusters and faithful representation of texts.
Recent deep learning architectures and algorithms have shown impressive results for several Natural Language Processing (NLP) tasks such as Named entity recognition, Part-of-Speech tagging, Dependency parsing and Semantic role labelling. The actual performance of certain NLP tools for English evaluated on in-domain data is close to human level, thanks to deep learning models trained on huge annotated datasets. Contrariwise, approaching human-level accuracy on more complex domains and low-resource languages is still a hard issue. This thesis falls within the scope of incorporating expert knowledge and linguistic resources in Deep Neural Networks (DNNs) in order to improve the performance of NLP tools for specialty areas and low-resource languages. The proposed subject aims to explore and experiment new approaches for incorporating expert knowledge and linguistic resources in deep neural networks. We propose to tackle this issue along the following key aspects, as an extension of the research work already carried out at LASTI (Laboratoire Analyse Sémantique Texte et Image) laboratory: - Taking into account heterogeneous expert knowledge and linguistic resources: Ontologies, Terminology databases, Lexicons, Named entity recognition rules, Dependency parsing recognition rules, etc. - Implementing a formalism to describe expert knowledge and linguistic resources in a multi-level representation. The objective is to define a structure in which the different types of expert knowledge and linguistic resources will be represented separately but the whole representation would be described in the same format (model). - Exploring new strategies for incorporating expert knowledge and linguistic resources in deep neural networks. The underlying idea is to propose an integration mechanism which can be adapted to each expert knowledge and linguistic resource.
My thesis proposes an analysis of the Italian parasynthetic verbs in within the framework of the Construction Morphology. The widespread definition of parasynthesis in literature corresponds to 'double simultaneous affixation on a derivational base' ([pref+[X]N/A+suff]V, cf. for example IMBARCARE 'to board'). Such definition is motivated by the impossibility of attesting the intermediate derivational stage between the base and the derived verb (cf. BARCA 'boat', *IMBARCA, *BARCARE) and it derives from a morpheme-based, incremental and concatenative approach to morphology that assumes that derivational processes are oriented rules. In my thesis I propose an alternative analysis which defines parasynthetic verbs as verbs built by prefixation. I consider as parasynthetic verb each verb belonging to the schema [préf[X]N/A]V (note that the suffix is analyzed as inflectional). This definition is purely based on the membership parameter in schema [préf[X]N/A]V (note that the suffix is analyzed as inflectional). According to this approach, the fact that a word is not attested not only represents an unreliable criterion from an empirical point of view but it also seems to be negligible within a theoretical framework considering morphological processes as non-oriented. The corpus includes 1674 lexemes automatically extracted from ItWaC. The structural variables for these verbs are (i) the prefix selected (a-, in-, s-, de-, dis-), (ii) the inflectional class (-are,-ire), (iii) the category of the base (N, A). Each lexeme is defined as a construction, i.e. a form-meaning pair (the form corresponding to a possible combination of variables and the meaning to a holistic sense). The possible semantic values are: (i) the change of state, (ii) the change of locative relation and (iii) the intensive/iterative value. For (i) and (ii) I propose a unified analysis in terms of a general semantic component expressing a change (formalized by the predicate BECOME), while the class of verbs expressing the value (iii) are not included in this generalization.
Our research work presented in this thesis aims the optimization of the performance of formant tracking algorithms. We began by analyzing different existing techniques used in the automatic formant tracking. This analysis showed that the automatic formant estimation remains difficult despite the use of complex techniques. For the non-availability of database as reference in Arabic, we have developed a phonetically balanced corpus in Arabic while developing a manual phonetic and formant tracking labeling. Then we presented our two new automatic formant tracking approaches which are based on the estimation of Fourier ridges (local maxima of spectrogram) or wavelet ridges (local maxima of scalogram) using as a tracking constraint the calculation of center of gravity of a set of candidate frequencies for each formant, while the second tracking approach is based on dynamic programming combined with Kalman filtering. Finally, we made an exploratory study using manually labeled corpus as a reference to quantify our two new approaches compared to other automatic formant tracking methods. We tested the first approach based on wavelet ridges detection, using the calculation of the center of gravity on synthetic signals and then on real signals issued from our database by testing three types of complex wavelets (CMOR, SHAN and FBSP). Following these tests, it appears that formant tracking and scalogram resolution given by CMOR and FBSP wavelets are better than the SHAN wavelet. To quantitatively evaluate our two approaches, we calculated the absolute difference average and standard deviation. We made several tests with different speakers (male and female) on various long and short vowels and continuous speech signals issued from our database using it as a reference. The formant tracking results are compared to those of Fourier ridges method calculating the center of gravity, LPC analysis combined with filter banks method of Kamran. Therefore, this method provides a correct formant tracking (F1, F2 and F3) and closer to the reference. They are also very close to the Fourier ridges method using the calculation of center of gravity. The results obtained in the case of female speakers confirm the trend observed over the male speakers
This thesis deals with the study of random methods for learning large-scale data. Firstly, we propose an unsupervised approach consisting in the estimation of the principal components, when the sample size and the observation dimension tend towards infinity. This approach is based on random matrices and uses consistent estimators of eigenvalues and eigenvectors of the covariance matrix. Then, in the case of supervised learning, we propose an approach which consists in reducing the dimension by an approximation of the original data matrix and then realizing LDA in the reduced space. Dimension reduction is based on low–rank approximation matrices by the use of random matrices. A fast approximation algorithm of the SVD and a modified version as fast approximation by spectral gap are developed. Experiments are done with real images and text data. Compared to other methods, the proposed approaches provide an error rate that is often optimal, with a small computation time. Finally, our contribution in transfer learning consists in the use of the subspace alignment and the low-rank approximation of matrices by random projections.
This thesis in contrast linguistics aims to study the syntactic-semantic specificities of inchoative verbs in Arabic and compare them with those of start to (commencer (à/par/de)) and to begin (se mettre à) in French. We relied on a corpus of French and Arabic literary texts from the following authors (Najube Mahfouz, Marcel Pagnol and Albert Camus). According to our corpus, inchoative verbs can appear in the following simple constructions: (subject + inchoative verb + direct object complement, indirect object complement, adverb or without complement), as well as in compound constructions (subject + V1 accomplished inchoative verb + V2 uncompleted) in Arabic, and (S + conjugated V1 + V2 infinitive) in French. Our analysis is based on the syntactic frameworks proposed by Peeters (1993). Indeed, the application of this theory and the syntactic variation of the grammatical elements of the Arabic sentence lead us, first, to propose different syntactic frameworks of certain inchoative Arabic verbs found in our corpus by determining their syntactic and semantic specificities. Concerning the simple form of inchoative verbs in Arabic, بدأ (bada'a) is the most frequently used verb. It is compatible with certain prepositions and accepts any type of subject and complement. As a result, it is distinguished by six grammatical constructions and four syntactic frames. On the other hand, the verbs هم (hamma), شرع (šara῾a) and أخذ ('aẖaḏa), in their simple use, each of them is governed only by a preposition, therefore, their simple construction is mainly characterized by a syntactic framework. In addition, the verb راح (rāḫa) is combined with two syntactic frames. Regarding their compound form, it should be noted that all inchoative verbs in Arabic are shared by a compound construction (S + V1 completed + V2 uncompleted). Semantically, most of these verbs are characterized by an imperfective value. It should be noted that the verb بدأ (bada'a) conveys a determinative as well as a non-determinative value. On the other hand, the verb شرع (šara῾a) is defined only by a perfective value, whereas هم (hamma) by a semi-perfective value. Therefore, from the results of Peeters as well as ours, we find that, on the one hand, the verb بدأ (bada'a) and commencer (à/par/de) are almost syntactically and semantically equivalent. On the other hand, the verbs هم (hamma) and se mettre à always serve to indicate speed and suddenness, but the action of the former is always semi-perfective. We can say that there are always points of divergence and convergence between inchoative verbs in both languages (Arabic and French). Finally, based on the written productions of FLE Libyan learners, we have tried to identify pedagogical approaches in order to identify the origin of the difficulties in the use of verbs, commencer (à/par/de) et se mettre à.
Typical Internet users today have their data scattered over several devices, applications, and services. Managing and controlling one's data is increasingly difficult. In this thesis, we adopt the viewpoint that the user should be given the means to gather and integrate her data, under her full control. In that direction, we designed a system that integrates and enriches the data of a user from multiple heterogeneous sources of personal information into an RDF knowledge base. The system is open-source and implements a novel, extensible framework that facilitates the integration of new data sources and the development of new modules for deriving knowledge. We introduce a time-based clustering algorithm to extract stay points from location history data. Using data from additional mobile phone sensors, geographic information from OpenStreetMap, and public transportation schedules, we introduce a transportation mode recognition algorithm to derive the different modes and routes taken by the user when traveling. The algorithm derives the itinerary followed by the user by finding the most likely sequence in a linear-chain conditional random field whose feature functions are based on the output of a neural network. We also show how the system can integrate information from the user's email messages, calendars, address books, social network services, and location history into a coherent whole. To do so, it uses entity resolution to find the set of avatars used by each real-world contact and performs spatiotemporal alignment to connect each stay point with the event it corresponds to in the user's calendar. Finally, we show that such a system can also be used for multi-device and multi-system synchronization and allow knowledge to be pushed to the sources.
The incursion of Islam in Sub-Saharan Africa from the 19th Century was operated through trans-Saharan commerce between the peoples of North Africa and those of the Sahel. This contact, maintained by the commercial caravans of these two peoples engendered the progressive islamisation of the Hausa-speaking populations. Under the influence of Arabic, several terms were introduced into Hausa lexicon. This Islamic effect comes with a revolution in the production of Arabic-Ajami literature. On the basis of these observations, this thesis proposes to analyse the borrowed Arabic lexicon in the poetic works of the author, and their integration into the Hausa language. Our corpus is made up of poetic works that we have first of all lemmatized using statistical calculations with the help of Excel software. The principal results, obtained in the form graphs, indicate a frequency of very high usage of words borrowed from Arabic. The association of the linguistic and computer analyses enabled us to confirm, in a formal and impartial manner, that most of the frequent borrowings fall under religious domains, and as such linked to situational vocabulary.
The use of electronic medical records (EMRs) and electronic prescribing are priorities in the various European action plans on connected health. The development of the EMR is a tremendous source of data; it captures all symptomatic episodes in a patient's life and should lead to improved medical and care practices, as long as automatic treatment procedures are set up. As such, we are working on hospitalization prediction based on EMRs and after having represented them in vector form, we enrich these models in order to benefit from the knowledge resulting from referentials, whether generalist or specific in the medical field, in order to improve the predictive power of automatic classification algorithms. Determining the knowledge to be extracted with the objective of integrating it into vector representations is both a subjective task and intended for experts, we will see a semi-supervised procedure to partially automate this process. As a result of our research, we designed a product for general practitioners to prevent their patients from being hospitalized or at least improve their health. Thus, through a simulation, it will be possible for the doctor to evaluate the factors involved on the risk of hospitalization of his patient and to define the preventive actions to be planned to avoid the occurrence of this event.
Past searches provide a useful source of information for new users (new queries). Due to the lack of ad-hoc IR collections, to this date there is a weak interest of the IR community on the use of past search results. Indeed, most of the existing IR collections are composed of independent queries. These collections are not appropriate to evaluate approaches rooted in past queries because they do not gather similar queries due to the lack of relevance judgments. Therefore, there is no easy way to evaluate the convenience of these approaches. In addition, elaborating such collections is difficult due to the cost and time needed. Thus a feasible alternative is to simulate such collections. Besides, relevant documents from similar past queries could be used to answer the new query. This principle could benefit from clustering of past searches according to their similarities. Thus, in this thesis a framework to simulate ad-hoc approaches based on past search results is implemented and evaluated. Four randomized algorithms to improve precision are proposed and evaluated, finally a new measure in the clustering context is proposed.
THE TRÉSOR DE LA LANGUE FRANÇAISE ANDTHE OXFORD ENGLISH DICTIONARY: A DIACHRONICAL ANALYSIS OF LOAN-WORDSABSTRACT There is no language that does not expand thanks to loan-words: they permit the lexical stock to get richer and refreshed as are developed the relationships between cultures and countries. English and French languages, since they have been spreading over all continents, have acquired a lot of words from other horizons, that, moreover, they often shared. Actually, we can but notice that their geographic proximity and the richness of their history have aroused an important interpenetration during more than ten centuries. That is why we wanted to show, in this study, the impact of loan-words on both languages, and to analyse the way the most extensive dictionaries on either side of the Channel — the Trésor de la Langue Française and the Oxford English Dictionary — dealt with them. In the first part of this work, we study how French and English lexicons were built up over the course of time according to foreign contributions, and we define the very notion of loan-word in order to show how complex it is. Afterwards, we present the corpus on which rests this study. The second part is dedicated to an exhaustive presentation of the Trésor de la Langue Française and of the Oxford English Dictionary. After a recounting of language dictionaries and of the creation of those two dictionaries, their main features are highlighted and their constitution accurately examined, as well macrostructurally as microstructurally. We also point out the advantages of their informatisation.
In this research, we address the problem of retrieving services which fulfil users'need expressed in query in free text. Our goal is to cope the term mismatch problems which affect the effectiveness of service retrieval models applied in prior re- search on text descriptions-based service retrieval models. These problems are caused due to service descriptions are brief. We have applied a family of Information Retrieval (IR) models for the purpose of contributing to increase the effectiveness acquired with the models applied in prior research on service retrieval. Besides, we have conducted systematic experiments to compare our family of IR models with those used in the state-of-the-art in service discovery. From the outcomes of the experiments, we conclude that our model based on query expansion via a co-occurrence thesaurus outperforms the effectiveness of all the models studied in this research. Therefore, we have implemented this model in S3niffer, which is a text description-based service search engine.
Knowledge bases are deductive databases where the machinery of logic is used to represent domain-specific and general-purpose knowledge over existing data. In the existential rules framework a knowledge base is composed of two layers: the data layer which represents the factual knowledge, and the ontological layer that incorporates rules of deduction and negative constraints. The main reasoning service in such framework is answering queries over the data layer by means of the ontological layer. As in classical logic, contradictions trivialize query answering since everything follows from a contradiction (ex falso quodlibet). Recently, inconsistency-tolerant approaches have been proposed to cope with such problem in the existential rules framework. They deploy repairing strategies on the knowledge base to restore consistency and overcome the problem of trivialization. However, these approaches are sometimes unintelligible and not straightforward for the end-user as they implement complex repairing strategies. This would jeopardize the trust relation between the user and the knowledge-based system. In this thesis we answer the research question: ``How do we make query answering intelligible to the end-user in presence of inconsistency?''. The answer that the thesis is built around is ``We use explanations to facilitate the understanding of query answering''. We propose meta-level and object-level dialectical explanations that take the form of a dialogue between the user and the reasoner about the entailment of a given query. We study these explanations in the framework of logic-based argumentation and dialectics and we study their properties and their impact on users.
These disorders are common during the course of Parkinson's disease, decrease the quality of life of subjects, and increase caregiver burden. Being able to predict which individuals are at higher risk of developing these disorders and when is of high importance. The objective of this thesis is to study impulse control disorders in Parkinson's disease from the statistical and machine learning points of view, and can be divided into two parts. The first part consists in investigating the predictive performance of the altogether factors associated with these disorders in the literature. The second part consists in studying the association and the usefulness of other factors, in particular genetic data, to improve the predictive performance.
For several years, the deployment of information and communication technology into the management of chronical pathologies is taking a considerable place, more particularly in the evolution of health's practices and in the improvement of the well-being of the patient. Chronical pathologies are of long duration and they need to be under a regular monitoring of the healthcare professional, composed of multidisciplinary or different actors in charge with the patients. On the other side the patients are also charged of following a healthcare protocol at home previously defined by the health care team. Nevertheless, the different forms of representing the contests of this protocol, it is not always complete and comprehensible for the patients. Furthermore, each one of the patients is unique and a proper definition of the health care protocol must be personalised and conform to his individual treatment and even to his personal wishes or constraints. But this is not the case of information guides or medical references that are supplied in general.
The research developed in this thesis introduces a qualitative approach for representing and reasoning on moving entities in a two-dimensional geographical space. Movement patterns of moving entities are categorized based on a series of qualitative spatial models of topological relations between a directed line and a region, and orientation relations between two directed lines, respectively. Qualitative movements are derived from the spatio-temporal relations that characterize moving entities conceptualized as either points or regions in a two-dimensional space. Such a spatio-temporal framework supports the derivation of the basic movement configurations inferred from moving and static entities. The approach is complemented by a tentative qualification of the possible natural language expressions of the primitive movements identified. The notion of conceptual transition that favors the exploration of possible trajectories in the case of incomplete knowledge configurations is introduced and explored. Composition tables are also studied and provide additional reasoning capabilities. The whole approach is applied to the analysis of flight patterns and maritime trajectories.
Computational models for automatic text understanding have gained a lot of interest due to unusual performance gains over the last few years, some of them leading to super-human scores. This success reignited some grandeur claims about artificial intelligence, such as universal sentence representation. In this thesis, we question these claims through two complementary angles. Firstly, are neural networks and vector representations expressive enough to process text and perform a wide array of complex tasks? In this thesis, we will present currently used computational neural models and their training techniques. Secondly, we will discuss the question of universality in sentence representation: what actually lies behind these universality claims? We delineate a few theories of meaning, and in a subsequent part of this thesis, we argue that semantics (unsituated, literal content) as opposed to pragmatics (meaning as use) is preponderant in the current training and evaluation data of natural language understanding models. To alleviate that problem, we show that discourse marker prediction (classification of hidden discourse markers between sentences) can be seen as a pragmatics-centered training signal for text understanding. We build a new discourse marker prediction dataset that yields significantly better results than previous work. In addition, we propose a new discourse-based evaluation suite that could incentivize researchers to take into account pragmatic considerations when evaluating text understanding models.
This study focuses on the analysis of identity discourse of Darwich's collection: "Why did you leave the horse alone?" in order to know the nature of relationship that the me of Darwich maintains with the me of the Other. In other words, our collection looks like a “dialogue” where the me of Darwich and the me of the Other plunge into an argumentative discourse. To answer our question about the relationship between the two parts of dialogue, we have firstly chosen a theoretical approach: the presentation of the emblematic figure of Darwich perceived as one, having a personal, cultural, social and national identity, different from the identity of the other one. Secondly, through a practical point of view, the type of relationship between the two sides has been shown through the analysis of the corpus consisting of six groups.
The efficient communication tends to follow the principle of the least effort. According to this principle, using a given language interlocutors do not want to work any harder than necessary to reach understanding. This fact leads to the extreme compression of texts especially in electronic communication, e.g. microblogs, SMS, search queries. However, sometimes these texts are not self-contained and need to be explained since understanding them requires knowledge of terminology, named entities or related facts. The first aim of this work is to help a user to better understand a short message by extracting a context from an external source like a text collection, the Web or the Wikipedia by means of text summarization. To this end we developed an approach for automatic multi-document summarization and we applied it to short message contextualization, in particular to tweet contextualization. The proposed method is based on named entity recognition, part-of-speech weighting and sentence quality measuring. In contrast to previous research, we introduced an algorithm for smoothing from the local context. Moreover, we developed a graph-based algorithm for sentence reordering. The method was also adapted to snippet retrieval. The evaluation results indicate good performance of the approach.
Trauma, whether accidental or intentional, is a major public health problem. In France, until recently, only road safety was the subject of attention claiming national exhaustiveness of epidemiological surveillance, coordinated by the Ministry of the Interior and with no strong link with the health system. The current system for centralizing summaries of emergency department visits (OSCOUR® network) is a powerful real-time surveillance tool that, through the coding of primary and secondary diagnoses (ICD-10 codes), provides indicators on certain seasonal pathologies suchh as influenza or gastroenteritis. However, this type of information is not sufficient to establish surveillance indicators related to trauma because the mechanism that led to the trauma (motor vehicle accident, suicide, violence, fall, etc.) is not known. The addition of mechanisms related to trauma would allow for national epidemiological surveillance of trauma, evaluation of prevention strategies, etc., as well as the development of indicators for the prevention of trauma and improve predictive patient flow management. The detailed reason for coming to the emergency department is not available in a standardized database but is described in detail in free-text clinical notes that are stored in electronic medical records. The overall objective of the project is therefore to develop a tool that would allow the creation of standardized information on trauma mechanisms from these clinical texts. To this end, the latest NLP (Natural Language Processing) techniques as part of Artifiial Intelligence will be tested, compared and applied.
This research is set within the fields of third language acquisition and multilingualism with a focus on developing our understanding of how multilingual competence functions. The research attempts to determine if the languages of a multilingual play distinctive roles in the acquisition of a new language specifically regarding the lexical and syntactic components in crosslinguistic interactions, and whether there is convergence between these phenomena. The research also examines the metalinguistic and crosslinguistic activities which allow learners to manage and comprehend the target language. The data for this research was collected from eleven case studies of speakers of Spanish and English with a variety of levels of proficiency in French. Participants were required to complete three speaking tasks. Approaches sourced from research from the fields of third language acquisition and language contact were utilised and developed to capture the complexity of the interactions, the typological relationship among the languages in contact, and the varying levels of the languages under examination. Using quantitative and qualitative analysis this research demonstrates how multilingual competence operates. The data indicates that participants activate all their languages to different degrees relating to different types of crosslinguistic interactions, syntactic properties and level of language. Participants also typically resort to their metalinguistic and crosslinguistic awareness to help them comprehend the target language and manage their output by the use of cognates, crosslinguistic consultations and inferences.
This thesis is located at the crossroads of speech processing, pattern recognition and multimedia information retrieval: the indexing of emotions for searching by content. In this context, our work is directed towards speaker independent emotional recognition and indexing. Second, the best parameters were sorted and chosen by the method of Forced Sequential Forward Selection (FSFS) with the conclusion of the effectiveness of this method in combination with the proposed approach Gb Symbolic Standardization to face the problem of robustness in our speaker independent recognition system. The other part of the thesis is based on the study of classification techniques used in the emotion recognition. Some experience on the interlanguage environment are also studied. Finally, on the basis of these results, an indexing engine was built on a reel corpus.
In this work, we explore how Natural Language Generation (NLG) techniques can be used to address the task of (semi-)automatically generating language learning material and activities in Camputer-Assisted Language Learning (CALL). In particular, we show how a grammar-based Surface Realiser (SR) can be usefully exploited for the automatic creation of grammar exercises. Our surface realiser uses a wide-coverage reversible grammar namely SemTAG, which is a Feature-Based Tree Adjoining Grammar (FB-TAG) equipped with a unification-based compositional semantics. More precisely, the FB-TAG grammar integrates a flat and underspecified representation of First Order Logic (FOL) formulae. In the first part of the thesis, we study the task of surface realisation from flat semantic formulae and we propose an optimised FB-TAG-based realisation algorithm that supports the generation of longer sentences given a large scale grammar and lexicon. The approach followed to optimise TAG-based surface realisation from flat semantics draws on the fact that an FB-TAG can be translated into a Feature-Based Regular Tree Grammar (FB-RTG) describing its derivation trees. The derivation tree language of TAG constitutes a simpler language than the derived tree language, and thus, generation approaches based on derivation trees have been already proposed. Our approach departs from previous ones in that our FB-RTG encoding accounts for feature structures present in the original FB-TAG having thus important consequences regarding over-generation and preservation of the syntax-semantics interface. The concrete derivation tree generation algorithm that we propose is an Earley-style algorithm integrating a set of well-known optimisation techniques: tabulation, sharing-packing, and semantic-based indexing. In the second part of the thesis, we explore how our SemTAG-based surface realiser can be put to work for the (semi-)automatic generation of grammar exercises. Usually, teachers manually edit exercises and their solutions, and classify them according to the degree of dificulty or expected learner level. A strand of research in (Natural Language Processing (NLP) for CALL addresses the (semi-)automatic generation of exercises. Mostly, this work draws on texts extracted from the Web, use machine learning and text analysis techniques (e.g. parsing, POS tagging, etc.). These approaches expose the learner to sentences that have a potentially complex syntax and diverse vocabulary. In contrast, the approach we propose in this thesis addresses the (semi-)automatic generation of grammar exercises of the type found in grammar textbooks. In other words, it deals with the generation of exercises whose syntax and vocabulary are tailored to specific pedagogical goals and topics. Because the grammar-based generation approach associates natural language sentences with a rich linguistic description, it permits defining a syntactic and morpho-syntactic constraints specification language for the selection of stem sentences in compliance with a given pedagogical goal.
"Sentiment", "opinion" and "emotion" are words really vaguely defined; not even the dictionary seems to be of any help, being it the first to define each of the three by using the remaining two. And yet, the civilised world is heavily affected by opinions: companies need them to understand how to sell their products; people use them to buy the most fitting product and, more generally, to weigh their decisions; researchers exploit them in Artificial Intelligence studies to understand the nature of the human being. Today we can count on a humongous amount of available information, though it's hard to use it. In fact, the so-called “Big data” are not always structured – especially for certain languages. French research suffers from a lack of readily available resources for tests. In the context of Natural Language Processing, this thesis aims to explore the nature of sentiment and emotion. Some of our contributions to the NLP research community are: creation of new resources for sentiment and emotion analysis, tests and comparisons of several machine learning methods to study the problem from different points of view-classification of online reviews using sentiment polarity, classification of product characteristics using Aspect-Based Sentiment Analysis. Finally, a psycholinguistic study-supported by a machine learning and lexical approaches – on the relation between who judges, the reviewer, and the object that has been judged, the product.
If present search engines allow to find documents corresponding to a large information need, they are not adapted to a precise information need, such as "Who was the president of the United States in 1978?". Question answering system aim at fulfilling such needs. The keyword request of a classical search engine is replaced by a question, thus in natural language, and the output consists in the precise answer to the question, instead of a list of documents to read. The question answering domain benefits from works in Information Retrieval (IR), but gives the user a facilitated interaction, thanks to the manipulation and comprehension of natural language of the Natural Language Processing domain (NLP). The goal of this work is to study how to access to a precise information, and in particular what characterizes an answer to a question. The level of linguistic knowledge needed for the system to recognize the link between a question and a potential answer is examined. The issue of the link between question and answer is studied on a syntactic level, in order to determine the impact of this type of knowledge in a precise information retrieval process.
Information retrieval in semi-structured (practically written in XML) mixes aspects of traditional information retrieval and of database querying. The structure is very important, but the information need is vague. The retrieval unit can have different sizes (a paragraph, a figure, an entire article…). Furthermore, XML flexibility may create some breaks in the natural flow of the text. Problems raised at this level are many, notably for document content analysis and querying. We studied the specific solutions that could bring the natural language processing (NLP) techniques. We proposed a theoretical frame and a practical approach to allow the use of traditional textual analysis techniques in XML documents, disregarding the structure. We also conceived an interface for querying XML documents in natural language, and proposed methods using the structure in order to improve the retrieval of relevant elements.
This thesis deals with an approach, guided by an ontology, designed to annotate documents from a corpus where each document describes an entity of the same type. In our context, all documents have to be annotated with concepts that are usually too specific to be explicitly mentioned in the texts. In addition, the annotation concepts are represented initially only by their name, without any semantic information connected to them. Finally, the characteristics of the entities described in the documents are incomplete. The population step (1) adds to the ontology information from the documents in the corpus but also from the Web of Data (Linked Open Data or LOD). The LOD represents today a promising source for many applications of the Semantic Web, provided that appropriate techniques of data acquisition are developed. In the settings of SAUPODOC, the ontology population has to take into account the diversity of the data in the LOD: multiple, equivalent, multi-valued or absent properties. The correspondences to be established, between the vocabulary of the ontology to be populated and that of the LOD, are complex, thus we propose a model to facilitate their specification. Then, we show how this model is used to automatically generate SPARQL queries and facilitate the interrogation of the LOD and the population of the ontology. The latter, once populated, is then enriched (2) with the annotation concepts and definitions that are learned through examples of annotated documents. Reasoning on these definitions finally provides the desired annotations. Experiments have been conducted in two areas of application, and the results, compared with the annotations obtained with classifiers, show the interest of the approach.
This new era of small UAVs currently populating the airspace introduces many safety concerns, due to the absence of a pilot onboard and the less accurate nature of the sensors. This necessitates intelligent approaches to address the emergency situations that will inevitably arise for all classes of UAV operations as defined by EASA (European Aviation Safety Agency). Hardware limitations for these small vehicles point to the utilization of analytical redundancy, rather than to the usual practice of hardware redundancy in manned aviation. In the course of this study, machine learning practices are implemented in order to diagnose faults on a small fixed-wing UAV to avoid the burden of accurate modeling needed in model-based fault diagnosis. A supervised classification method, SVM (Support Vector Machines) is used to classify the faults. The data used to diagnose the faults are gyro and accelerometer measurements. The idea to restrict the data set to accelerometer and gyro measurements is to check the method's classification ability, with a small and inexpensive chip set and without the need to access the data from the autopilot, such as the control input information. This work addresses the faults in the control surfaces of a UAV. More specifically, the faults considered are the control surface stuck at an angle and the loss of effectiveness. First, a model of an aircraft is simulated. This model is not used for the design of Fault Detection and Diagnosis (FDD) algorithms, but is instead utilized to generate data. Simulated data are used instead of flight data in order to isolate the probable effects of the controller on the diagnosis, which may complicate a preliminary study on FDD for drones. The results show that for simulated measurements, SVM gives very accurate results on the classification of the loss of effectiveness faults on the control surfaces. These promising results call for further investigation so as to assess SVM performance on fault classification with flight data. Real flights were arranged to generate faulty flight data by manipulating the open source autopilot, Paparazzi. All data and the code are available in the code sharing and versioning system, Github. Training is held offline due to the need for labeled data and the computational burden of the tuning phase of the classifiers. Results show that from the flight data, SVM yields an F1 score of 0.98 for the classification of control surface stuck faults. For the loss of efficiency faults, some feature engineering, involving the addition of past measurements is needed in order to attain the same classification performance. A promising result is discovered when spinors are used as features instead of angular velocities. Results show that by using spinors for classification, there is a vast improvement in classification accuracy, especially when the classifiers are untuned. Using spinors and a Gaussian Kernel, an untuned classifier gives an F1 score of 0.9555, which was 0.2712 when gyro measurements were used as features. In summary, this work shows that SVM gives a satisfactory performance for the classification of faults on the control surfaces of a drone using flight data.
This work addresses the issue of establishing and maintaining the coordination in a mixed human-agent teamwork in the context of CVET. The objective of this research is to provide human-like conversational behavior of the virtual agents in order to cooperate with a user and other agents to achieve shared goals. We propose a belief-desire-intention (BDI) like Collaborative Conversational agent architecture(C2BDI) that treats both deliberative and conversational behaviors uniformly as guided by the goal-directed shared activity. We put forward an integrated model of coordination which is founded on the shared mental model based approaches to establish coordination in a human-agent teamwork. We argue that natural language interaction between team members can affect and modify the individual and shared mental models of the participants. Finally, we describe the cultivation of coordination in a mixed human-agent teamwork through natural language conversation. In order to establish the strong coupling between decision making and the collaborative conversational behavior of the agent, we propose first, the Mascaret based semantic modeling of human activities and the VE, and second, the information state based context model. This representation allows the treatment of semantic knowledge of the collaborative activity and virtual environment, and information exchanged during the dialogue conversation in a unified manner. To endow the communicative capabilities to C2BDI agent, we put forward the information state based approach for the natural language processing of the utterances. We define collaborative conversation protocols that ensure the coordination between team members. Finally, in this thesis, we propose a decision making mechanism, which is inspired by the BDI based approach and provides the interleaving between deliberation and conversational behavior of the agent. We have applied the proposed architecture to three different scenarios in the CVET. We found that the multiparty collaborative conversational behavior of C2BDI agent is more constructive and facilitates the user to effectively coordinate with other team members to perform a shared task.
Numerous domains have interests in studying the viewpoints expressed online, be it for marketing, cybersecurity, or research purposes with the rise of computational social sciences. We propose in this manuscript two contributions to the field of stance detection, focused around the difficulty of obtaining annotated data of quality on social medias. Our first contribution is a large and complex dataset of 22853 Twitter profiles active during the French presidential campaign of 2017. This is one of the rare datasets that considers a non-binary stance classification and, to our knowledge, the first one with a large number of profiles, and the first one proposing overlapping political communities. This dataset can be used as-is to study the campaign mechanisms on Twitter, or used to test stance detection models or network analysis tools. We then propose two semi-supervised generic stance detection models using a handful of seed profiles for which we know the stance to classify the rest of the profiles by exploiting various proximities. Indeed, current stance detection models are usually grounded on the specificities of some social platforms, which is unfortunate since it does not allow the integration of the multitude of available signals. By infering proximities from differents types of elements available on social medias, we can detect profiles close enough to assume they share a similar stance on a given subject. Our first model is a sequential ensemble algorithm which propagates stances thanks to a multi-layer graph representing proximities between profiles. Using datasets from two platforms, we show that, by combining several types of proximities, we can achieve excellent results. Our second model allows us to observe the evolution of profiles'stances during an event with as little as one seed profile by stance. This model confirms that a large majority of profiles do not change their stance on social medias, or do not express their change of heart.
With the growing mass of textual data on the Web, automatic summarization of topic-oriented collections of documents has become an important research field of Natural Language Processing. The experiments described in this thesis were framed within this context. We proposed several similarity measures which were evaluated and compared on different data sets: the SemEval 2014 challenge corpus for the English language and own built datasets for French. The good performance showed by our measures led us to use them in a multi-document summary task, which implements a pagerank-type algorithm. The system was evaluated on the DUC 2007 datasets for English and RPM2 corpus for French. This simple approach, based on a resource readily available in many languages, proved efficient, robust and the encouraging outcomes open up real prospects of improvement.
The importance of collaborative systems in real-world applications has grown significantly over the recent years. The majority of new applications are designed in a distributed fashion to meet collaborative work requirements. Although such applications are more and more used into many fields, the lack of an adequate access control concept is still limiting their full potential. We propose a optimistic approach to enforce access control in existing collaborative editing solutions in the sense that a user can temporarily violate the access control policy. To enforce the policy, we resort to the selective undo approach in order to eliminate the effect of illegal document updates. We investigate a theoretical study of the undo problem and propose a generic solution for selectively undoing operations. Finally, we apply our framework on a collaboration prototype and measure its performance in the distributed grid GRID?5000 to highlight the scalability of our solution
This thesis deals with the field of information retrieval and the recommendation of reading. It has for objects: — The creation of new approach of document retrieval and recommendation using techniques of combination of results, aggregation of social data and reformulation of queries; — The creation of an approach of recommendation using methods of information retrieval and graph theories. Two collections of documents were used. First one is a collection which is provided by CLEF (Social Book Search-SBS) and the second from the platforms of electronic sources in Humanities and Social Sciences OpenEdition.org (Revues.org). The modelling of the documents of every collection is based on two types of relations: — For the first collection (SBS), documents are connected with similarity calculated by Amazon which is based on several factors (purchases of the users, the comments, the votes, products bought together, etc.); — For the second collection (OpenEdition), documents are connected with relations of citations, extracted from bibliographical references. The manuscript is structured in two parts. The first part "state of the art" includes a general introduction, a state of the art of informationretrieval and recommender systems. The second part "contributions" includes a chapter on the detection of reviews of books in Revues.org; a chapter on the methods of IR used on complex queries written in natural language and last chapter which handles the proposed approach of recommendation which is based on graph.
Question Answering is a discipline which lies in between natural language processing and information retrieval domains. Emergence of deep learning approaches in several fields of research such as computer vision, natural language processing, speech recognition etc. has led to the rise of end-to-end models. In the context of GoASQ project, we investigate, compare and combine different approaches for answering questions formulated in natural language over textual data on open domain and biomedical domain data. The thesis work mainly focuses on 1) Building models for small scale and large scale datasets, and 2) Leveraging structured and semantic information into question answering models. Hybrid data in our research context is fusion of knowledge from free text, ontologies, entity information etc. applied towards free text question answering. The current state-of-the-art models for question answering use deep learning based models. In order to facilitate using them on small scale datasets on closed domain data, we propose to use domain adaptation. We model the BIOASQ biomedical question answering task dataset into two different QA task models and show how the Open Domain Question Answering task suits better than the Reading Comprehension task by comparing experimental results. We pre-train the Reading Comprehension model with different datasets to show the variability in performance when these models are adapted to biomedical domain. We find that using one particular dataset (SQUAD v2.0 dataset) for pre-training performs the best on single dataset pre-training and a combination of four Reading Comprehension datasets performed the best towards the biomedical domain adaptation. We perform some of the above experiments using large scale pre-trained language models like BERT which are fine-tuned to the question answering task. The performance varies based on the type of data used to pre-train BERT. For BERT pre-training on the language modelling task, we find the biomedical data trained BIOBERT to be the best choice for biomedical QA. We highlight the necessity for using Lexical and Expected Answer Types in open domain and biomedical domain question answering by performing several verification experiments. These types are used to highlight entities in two QA tasks which shows improvements while using entity embeddings based on the answer type annotations. We manually annotated an answer variant dataset for BIOASQ and show the importance of learning a QA model with answer variants present in the paragraphs. Our hypothesis is that the results obtained from deep learning models can further be improved using semantic features and collective features from different paragraphs for a question. We propose to use ranking models based on binary classification methods to better rank Top-1 prediction among Top-K predictions using these features, leading to an hybrid model that outperforms state-of-art-results on several datasets. We experiment with several overall Open Domain Question Answering models on QA sub-task datasets built for Reading Comprehension and Answer Sentence Selection tasks. We show the difference in performance when these are modelled as overall QA task and highlight the wide gap in building end-to-end models for overall question answering task.
This thesis presents: 1) the development of a novel approach to find direct associations between pairs of elements linked indirectly through various common features, 2) the use of this approach to directly associate biological functions to protein domains (ECDomainMiner and GODomainMiner), and to discover domain-domain interactions, and finally 3) the extension of this approach to comprehensively annotate protein structures and sequences. Using inferred function-domain associations and considering taxonomy information, thousands of annotation rules have automatically been generated. Then, these rules have been utilized to annotate millions of protein sequences in the TrEMBL database
The work includes the definition of a model for opinion predicates and their arguments (source, topic and message), the creation of a lexicon of opinion predicates which have information from the model associated, and the implementation of three systems. Indeed these works had scores that fall between 63% and 89.5%.Moreover, in addition to the systems made for the identification of opinions, our work has led to the construction of several resources for Spanish: a lexicon of opinion predicates, a 13,000 words corpus with opinions annotated and a 40,000 words corpus with opinion predicates end sources annotated.
Producing a discourse to share an emotion or express a feeling requires the interlocutor to frame situated discursive strategies as well as in a specific communicative situation as in a particular language-culture. This study on specialized language in Ecuadorian context focus on speakers and the strategies they build to express their emotions while tasting specific products. Indeed, various interrogations arise such as: "Does a specific textual genre exist to describe the procedure and the discursive prototype of a tasting praxis?"; the one regarding the link between a discursive production and the speaker identity who uses different lexicons and grammars to talk about a specific product: "Do experts and consumers follow the same discursive strategies?"; the one regarding a situated terminology: "How do social representations and discursive praxis affect the involved speaker contributions?". Indeed, these interrogations should help to characterise the expression of the sensoriality situated in a specific professional context, tasting in Spanish language within an Ecuadorian culture, while offering a scientific interpretation of terminological choices used as sensory and hedonic descriptors which include an emotive dimension. Therefore, this study is grounded in the cognitive semantics, corpus linguistics, and textometric analysis theoretical frameworks where discursive practices are representative of sociocultural praxis. The suggested methodology uses computer-based and data-mining tools proper to natural language processing applied to the three followed corpora: 1. The corpus of diachronic texts built on the compilation of the last ten years specialised media written production which allows to identify the current descriptors used to describe the perceived feelings and emotions while tasting a chocolate bar. 2. The corpus built on speakers' texts they produce when they share their own descriptors conceptualisation and meaning. This corpus will be compiled without any contact with the product, the chocolate, in order to focus on the cognitive dimension of the selected descriptor social representation. 3. And the corpus built on spoken productions where these descriptors are in discourse to express the perceived feelings and emotions while tasting a chocolate bar. The analysis of these different corpora offers an authentic research topic which is representative of speakers' perceptions when they set up their discursive strategies to share an emotion or express a feeling. This method allows a qualitative interpretation on the basis of a quantitative data analysis in order to suggest a description of different expression strategies of sensory experience. Beside these corpus compilations, filling a questionnaire before the tasting session allows the speakers' characterisation and the data collection about the social representation they could mobilise in their discursive choices. Beyond the compilation of an unseen corpus and its discursive analysis, this study is of interest to provide a study on the strategies in use to express emotions and feelings in a tasting context, hence it offers a description of a specialized discourse genre which could improve the produced discourse to address potential consumers.
The main ambitious objective of the project is also its main originality. It consists in automatically extracting the narrative structure of TV series. Current TV series are based on complex structures involving several intertwined story arcs within the same episode. The first scientific barrier is therefore related to the so-called semantic gap between the actual storyline conveyed by TV series and the type of information that can be automatically extracted and processed by computer programs. A narrative approach could be based only on searching information such as the collection temporal structure, characters (who are they?) But, this difficult problem cannot be solved using only one of its sides (who?
The thesis, conducted as part of a CIFRE grant, and extending one of the aspects of the ANR project Traouiero, first addresses the production, extension and improvement of multilingual corpora by machine translation (MT) and contributory post-editing (PE). Functional and technical improvements have been made to the SECTra and iMAG software produced in previous PhD theses (P.C. Huynh, H.T. Nguyen), and progress has ben made toward a generic definition of the structure of a multilingual, annotated and multi-media corpus that may contain usual documents as well as pseudo-documents (such as Web pages) and meta-segments. As part of an internal project on the LIG website and of a project (TABE-FC) in cooperation with Xiamen University, it has been possible to demonstrate the value of incremental learning in statistical MT, under certain conditions, through an experiment that spread over the whole thesis. The third part of the thesis is devoted to contributing and making available computer tools and resources. The main ones are related to the COST project MUMIA of the EU and result from the exploitation of the CLEF-2011 collection of 1.5 million partially multilingual patents. Large translation memories have been extracted from it (17.5 million segments), 3 MT systems have been produced (de-fr, en-fr, fr-de), and a website of support for multilingual IR on patents has been constructed. One also describes the on-going implementation of JianDan-eval, a platform for building, deploying and evaluating MT systems.
Modeling complex processes often involve a high number of variables with anintricate correlation structure. For example, many spatially-localized processes display spatial regularity, as variables corresponding to neighboring regions are more correlated than distant ones. The formalism of weighted graphs allows us to capture relationships between interacting variables in a compact manner, permitting the mathematical formulation of many spatial analysis tasks. We introduce a new preconditioning scheme for the existing generalized forward-backward proximal splitting algorithm, specifically designed for graphs with high variability in neighbourhood configurations and edge weights. We show that our proposed approaches reach or outperform state-of-the-art for geostatistical aggregation as well as image recovery problems. The second part focuses on the development of a new model, expanding continuous-time Markov chain models to general undirected weighted graphs. This allows us to take into account the interactions between neighbouring nodes in structured classification, as demonstrated for a supervised land-use classification task from cadastral data.
This thesis presents a non-standardized text analysis approach which consists a chain process modeling allowing the automatic annotation of texts: grammar annotation using a morphosyntactic tagging method and semantic annotation by putting in operates a system of named-entity recognition. In this context, we present a system analysis of the Middle French which is a language in the course of evolution including: spelling, the flexional system and the syntax are not stable. The texts in Middle French are mainly distinguished by the absence of normalized orthography and the geographical and chronological variability of medieval lexicons. The main objective is to highlight a system dedicated to the construction of linguistic resources, in particular the construction of electronic dictionaries, based on rules of morphology. Then, we will present the instructions that we have carried out to construct a morphosyntactic tagging which aims at automatically producing contextual analyzes using the disambiguation grammars. Finally, we will retrace the path that led us to set up local grammars to find the named entities. Hence, we were asked to create a MEDITEXT corpus of texts in Middle French between the end of the thirteenth and fifteenth centuries.
This thesis deals with neural network based coarticulation modeling, and aims to synchronize facial animation of a 3D talking head with speech. We propose in this work a coarticulation model, i.e. a model able to predict spatial trajectories of articulators from speech. We rely on a sequential model, the recurrent neural networks, and more specifically the Gated Recurrent Units, which are able to consider the articulation dynamic as a central component of its modeling. Unfortunately, the typical amount of data in articulatory and audiovisual databases seems to be quite low for a deep learning approach. To overcome this difficulty, we propose to integrate articulatory knowledge into the networks during its initialization. The RNNs robustness allow uw to apply our coarticulation model to predict both face and tongue movements, in french and german for the face, and in english and german for the tongue. Evaluation has been conducted through objective measures of the trajectories, and through experiments to ensure a complete reach of critical articulatory targets. We also conducted a subjective evaluation to attest the perceptual quality of the predicted articulation once applied to our facial animation system. Finally, we analyzed the model after training to explore phonetic knowledges learned.
Nowadays, machine translation has reached good results when applied to several language pairs such as English – French, English – Chinese, English – Spanish, etc. However, research on machine translation for under-resourced language pairs always faces to the lack of training data. Thus, we have addressed the problem of retrieving a large parallel bilingual text corpus to build a statistical machine translation system. The originality of our work lies in the fact that we focus on under-resourced languages for which parallel bilingual corpora do not exist in most cases. This manuscript presents our methodology for extracting a parallel corpus from a comparable corpus, a richer and more diverse data resource over the Web. We propose three methods of extraction. The first method follows the classical approach using general characteristics of documents as well as lexical information of the document to retrieve both parallel documents and parallel sentence pairs. However, this method requires additional data of the language pair. The second method is a completely unsupervised method that does not require additional data and it can be applied to any language pairs, even under resourced language pairs. The last method deals with the extension of the second method using a third language to improve the extraction process (triangulation). The proposed methods are validated by a number of experiments applied on the under resourced Vietnamese language and the English and French languages.
This thesis describes the applications of natural language processing (NLP) to industrial risk management. We focus on the domain of civil aviation, where incident reporting and accident investigations produce vast amounts of information, mostly in the form of textual accounts of abnormal events, and where efficient access to the information contained in the reports is required. We start by drawing a panorama of the different types of data produced in this particular domain. We analyse the documents themselves, how they are stored and organised as well as how they are used within the community. We show that the current storage and organisation paradigms are not well adapted to the data analysis requirements, and we identify the problematic areas, for which NLP technologies are part of the solution. Specifically addressing the needs of aviation safety professionals, two initial solutions are implemented: automatic classification for assisting in the coding of reports within existing taxonomies and a system based on textual similarity for exploring collections of reports. Based on the observation of real-world tool usage and on user feedback, we propose different methods and approaches for processing incident and accident reports and comprehensively discuss how NLP can be applied within the safety information processing framework of a high-risk sector. By deploying and evaluating certain approaches, we show how elusive aspects related to the variability and multidimensionality of language can be addressed in a practical manner and we propose bottom-up methods for managing the overabundance of textual feedback data
This thesis addresses the analysis and generation of expressive movements for virtual human characters. Based on previous results from three different research areas (perception of emotions and biological motion, automatic recognition of affect and computer character animation), a low-dimensional motion representation is proposed. This representation consists of the spatio-temporal trajectories of end-effectors (i.e., head, hands and feet) and pelvis. We have argued that this representation is both suitable and sufficient for characterizing the underlying expressive content in human motion and for controlling the generation of expressive whole-body movements. In order to prove these claims, this thesis proposes: i.) A new motion capture database inspired by physical theater theory. This database contains examples from different motion classes (i.e., periodic movements, functional behaviors, spontaneous motions, and theater-inspired motion sequences) and distinct emotional states (happiness, sadness, relaxedness, stress and neutral) performed by several actors. ii.) A user study and automatic classification framework de-signed to qualitatively and quantitatively assess the amount of emotion-related information conveyed and encoded in the proposed representation. We have observed that although slight differences in performance were found with respect to the cases in which the entire body was used, our proposed representation preserves most of the motion cues salient to the expression of affect and emotions. iii.) A simple motion synthesis system able to capable of: a) reconstructing whole-body movements from the proposed low-dimensional representation, and b) producing novel end-effector (and pelvis) expressive trajectories. A quantitative and qualitative evaluation of the generated whole body motions shows that these motions are as expressive as the movements recorded from human actors
The extraction of spatial information from textual data has become an important research topic in the field of Natural Language Processing (NLP). It meets a crucial need in the information society, in particular, to improve the efficiency of Information Retrieval (IR) systems for different applications (tourism, spatial planning, opinion analysis, etc.). Such systems require a detailed analysis of the spatial information contained in the available textual data (web pages, e-mails, tweets, SMS, etc.). However, the multitude and the variety of these data, as well as the regular emergence of new forms of writing, make difficult the automatic extraction of information from such corpora. To meet these challenges, we propose, in this thesis, new text mining approaches allowing the automatic identification of variants of spatial entities and relations from textual data of the mediated communication. These approaches are based on three main contributions that provide intelligent navigation methods. Our first contribution focuses on the problem of recognition and identification of spatial entities from short messages corpora (SMS, tweets) characterized by weakly standardized modes of writing. The second contribution is dedicated to the identification of new forms/variants of spatial relations from these specific corpora. Finally, the third contribution concerns the identification of the semantic relations associated withthe textual spatial information.
In this thesis, we study several models of collaboration between Software Engineering and Semantic Web. The main objective of our work is to provide the developer with the tools to design, in the declarative manner, a business "executable" layer of an application in order to simulate its operation and thus show the compliance of the application with the customer requirements defined at the beginning of the software life cycle. On the other hand, another advantage of this approach is to allow the developer to share and reuse the business layer description of a typical application in a domain using ontology. This typical application description is called "Application Template". The reuse of the business layer description of an application is an interesting aspect of software engineering. That is the key point we want to consider in this thesis. In the first part of this thesis, we deal with the modeling of the business layer. We first present an ontology-based approach to represent business process and the business rules and show how to verify the consistency of business process and the set of business rules. Then, we present an automatic check mechanism of compliance of business process with a set of business rules. The second part of this thesis is devoted to define a methodology, called personalization, of creating of an application from an "Application Template". This methodology will allow the user to use an Application Template to create his own application by avoiding deadlock and semantic errors. We introduce at the end of this part the description of an experimental platform to illustrate the feasibility of the mechanisms proposed in the thesis.
Through this study we introduce a dynamic perspective of semantic annotation. This perspective considers the passage of time and the permanent ﬂow of documents that makes the collections grow and their annotation systems to extend and evolve. We also bring a vision of the quality of annotations systems based on the notion of information access. In our vision, the quality of annotation vocabulary depends on the amount and complexity of information to be navigated by a user while searching for a certain topic. To address the problem of the dynamics in semantic annotation, this work proposes a modular architecture for dynamic semantic annotation. This architecture models the activities involved in the semantic annotation process in abstract modules dedicated to the diﬀerent tasks that users have to perform. As a case of study we took blogging annotation. We gathered a corpus containing up to 10 years of annotated blog posts with categories and tags and we analyzed the annotation habits. By testing automatic tag and category strategies, we measure the impact of the dynamics in the annotation system. We propose some strategies to control this impact, which helps to evaluate the obsolescence of examples. Finally we propose a framework relying on three quality metrics and an interactive method to recover the quality of an indexing system based on semantic annotation. The metrics are evaluated over time to observe the degradation in indexing quality. A series of studied examples are presented to observe the performance of the measures to guide the restructuring of the indexing annotation system.
While vaccines represent a great achievement for public health, the risk of adverse effects is a real threat for vaccine acceptability by both the population and healthcare professionals. France still ranks as the country having the highest vaccine defiance. This often turned into poor vaccination coverages. This origin of this mistrust in vaccines is probably related to the intense polemic around anti-hepatitis B (HB) vaccination and the risk of multiple sclerosis in the 1990's. The main aim of this thesis was to assess the putative link between vaccination and demyelinating disorders by considering two examples: anti-HB and anti-papillomavirus (HPV) vaccines. For both vaccines, methods adopted a stepwise evidence-based approach. Hypothesis generation was based on evidence regarding the biological plausibility, the published case reports, the disproportionality analyses conducted in the US Vaccine Adverse Event Reporting System (VAERS) and the analysis of signals detected by spontaneous reporting systems, if any. For the research question centered on the anti-HB vaccination, observed-to-expected analyses based on all confirmed cases reported to the French pharmacovigilance in the 1990's were also conducted. Results were non-conclusive for both vaccines. For anti-HB vaccination, several elements could give credence to an association with central demyelination: a weak and indirect biological plausibility, the analysis of the French signal detected in the 1990's which revealed a complete disjunction between the target and the joint populations, and the results of the disproportionality analyses in VAERS. Nevertheless, neither the meta-analysis nor the observed-to-expected analyses (although might be easily reversed by a moderate degree of underreporting), provided statistically significant findings. If the excess risk actually existed, it would be weak and would be a concern for adults only. The current recommendations which are minimizing the probability of the French population to be exposed at an adult age, are therefore more than justified. For the anti-HPV vaccination, after reviewing all materials available, the risk of central demyelination seems, at this date, unlikely. Nevertheless, a doubt remains regarding a possible excess risk of Guillain Barré Syndrome (GBS) in the follow of an anti-HPV immunization. To conclude, a strong association with a risk of central demyelination can be ruled out for both vaccines, making the benefit and risk balances still largely positive for both products if used in their current target populations. In that context, an independent, clear and scientifically-based communication is the key element to promote vaccination programmes and to generate the confidence and adherence of the general population. The future of vaccine pharmacovigilance could rely on the implementation of a collaborative GP-patient network-based solution using SMS and smartphones, as already experimented in Australia. While collecting potential adverse effects of vaccines, it would also be a unique opportunity to place the patients at the heart of the surveillance system, giving them a voice and potentially contributing to restore their confidence in vaccines and even, in the decision-makers in the field of public health.
From a general point of view this thesis addresses an automatic path to build a solution choosing a compatible set of building blocks to provide such a solution to solve a given problem. To create the solution it is considered the compatibility of each available building block with the problem and also the compatibility between each building block to be employed within a solution all together. In the particular perspective of this thesis the building blocks are meta-models and the given problem is a description of a problem that can be solved using software using a multi-agent system paradigm. Nevertheless if no solution is found it also indicates that the problem can not be solved through this paradigm using the available meta-models. The process addressed by the thesis consists of the following main steps: (1) Through a process of characterization the problem description is analyzed in order to locate the solution domain and therefore employ it to choose a list of most domain compatible meta-models as candidates. (3) The matching step is built over a multi-agent system where each agent represents a candidate meta-model. Within this multi-agent system each agent interact with each other in order to find a group of suitable meta-models to represent a solution. Each agent use as criteria the compatibility between their represented candidate meta-model with the other represented meta-models. This thesis focuses on providing a process and a prototype tool to solve the last step. Therefore the proposed path has been created using several concepts from meta-analysis, cooperative artificial intelligence, Bayesian cognition, uncertainty, probability and statistics.
The aim of the dissertation is to analyze Polish and French manner of motion verbs and the properties that they display as predicates. In both languages, the argument structures of such predicates determine the verb grammatical behavior. Isc, jechac, plynac, biec, leciec, frunac, pelznac are known as determinate verbs; they lexicalize as manner and path of motion. The verbs taken under analysis in chapter 3 form the indetreminate sub-group: chodzic, jezdzic, plywac, biegac, latac, fruwac, pelzac conflate motion and its manner, but unlike their determinate correpondents, their lexical meaning does not contain any information relative to path. Chapter 5 summarizes the main differences between Polish and French manner of motion verbs. In other words, they are able to describe as well telic and atelic motion events.
Automatic verification has nowadays become a central domain of investigation in computer science. Over 25 years, a rich theory has been developed leading to numerous tools, both in academics and industry, allowing the verification of Boolean properties-those that can be either true or false. Current needs evolve to a finer analysis, a more quantitative one. Extension of verification techniques to quantitative domains has begun 15 years ago with probabilistic systems. However, many other quantitative properties are of interest, such as the lifespan of an equipment, energy consumption of an application, the reliability of a program, or the number of results matching a database query. Expressing these properties requires new specification languages, as well as algorithms checking these properties over a given structure. This thesis aims at investigating several formalisms, equipped with weights, able to specify such properties: denotational ones-like regular expressions, first-order logic with transitive closure, or temporal logics-or more operational ones, like navigating automata, possibly extended with pebbles. A first objective of this thesis is to study expressiveness results comparing these formalisms. In particular, we give efficient translations from denotational formalisms to the operational one. These objects, and the associated results, are presented in a unified framework of graph structures. This permits to handle finite words and trees, nested words, pictures or Mazurkiewicz traces, as special cases. Therefore, possible applications are the verification of quantitative properties of traces of programs (possibly recursive, or concurrent), querying of XML documents (modeling databases for example), or natural language processing. Second, we tackle some of the algorithmic questions that naturally arise in this context, like evaluation, satisfiability and model checking. In particular, we study some decidability and complexity results of these problems depending on the underlying semiring and the structures under consideration (words, trees...). Finally, we consider some interesting restrictions of the previous formalisms. Some permit to extend the class of semirings on which we may specify quantitative properties. Another is dedicated to the special case of probabilistic specifications: in particular, we study syntactic fragments of our generic specification formalisms generating only probabilistic behaviors.
It is well known in the enterprises that each new project to be carried out is usually similar to a certain previous projects. Those projects can be structured according to a common reference process depending on their type. The main difficulty lies in the formalization of the expertise business process. The traditional knowledge capitalization approaches based on the experts'debriefings showed their limits: the experts often leave out details which may be of relevance because the debriefings are habitually realizedexternally to the activities. Our thesis relies on the idea that it is possible to construct the operational process, implemented during the collaborative activities in a product development study, from the traces recorded by the used IT tools. The constructed operational process allows the business actors and experts to step back on their work and formalize the new deducted experience to enhance the expertise business processes of the firm. Our work had taken place in the ERPI (Equipe de Recherche sur les Processus Innovatifs) laboratory of the “Université de Lorraine” under a partnership with TDC Software society and through a CIFRE Convention. This dissertation offers five key contributions: • A double cycle to capitalize over the instrumented activities. • A global approach for the management of expertise business processes. • An ontology “OntoProcess” to conceive the generic organizational aspects, separating distinctly the concepts related to traces from those related to the business process, and providing extensions in function of the used tools. • A multi-agents system based on the ontology “OntoProcess” to support the presented global approach of the expertise business processes management. • A trace based system that allows the construction of the operational process from the traces registered over the study
The popularization of social networks and digital documents increased quickly the information available on the Internet. However, this huge amount of data cannot be analyzed manually. We also analyzed other NLP tasks (word encoding representation,semantic similarity, sentence and multi-sentence compression) to generate more stable and informative cross-lingual summaries. Most of NLP applications (including all types of text summarization) use a kind of similarity measure to analyze and to compare the meaning of words, chunks, sentences and texts in their approaches. A way to analyze this similarity is to generate a representation for these sentences that contains the meaning of them. The meaning of sentences is defined by several elements, such as the context of words and expressions, the order of words and the previous information. Analyzing these problems,we propose a neural network model that combines recurrent and convolutional neural networks to estimate the semantic similarity of a pair of sentences (or texts) based on the local and general contexts of words. Our model predicted better similarity scores than baselines by analyzing better the local and the general meanings of words and multi-word expressions. In order to remove redundancies and non-relevant information of similar sentences,we propose a multi-sentence compression method that compresses similar sentences by fusing them in correct and short compressions that contain the main information of these similar sentences. We model clusters of similar sentences as word graphs. Our approach outperformed baselines by generating more informative and correct compressions for French, Portuguese and Spanish languages. Finally, we combine these previous methods to build a cross-language text summarization system. Our system is an {English, French, Portuguese, Spanish}-to-{English,French} cross-language text summarization framework that analyzes the information in both languages to identify the most relevant sentences. Inspired by the compressive text summarization methods in monolingual analysis, we adapt our multi-sentence compression method for this problem to just keep the main information. Analyzing {English,French, Portuguese, Spanish}-to-{English, French} cross-lingual summaries, our system significantly outperforms extractive baselines in the state of the art for all these languages.
The popularity of OSM is mainly conditioned by the integrity and the quality of UGC as well as the protection of users'privacy. Based on the definition of information quality as fitness for use, the high usability and accessibility of OSM have exposed many information quality (IQ) problems which consequently decrease the performance of OSM dependent applications. Such problems are caused by ill-intentioned individuals who misuse OSM services to spread different kinds of noisy information, including fake information, illegal commercial content, drug sales, mal-ware downloads, and phishing links. The propagation and spreading of noisy information cause enormous drawbacks related to resources consumptions, decreasing quality of service of OSM-based applications, and spending human efforts. The majority of popular social networks (e.g., Facebook, Twitter, etc) over the Web 2.0 is daily attacked by an enormous number of ill-intentioned users. However, those popular social networks are ineffective in handling the noisy information, requiring several weeks or months to detect them. Moreover, different challenges stand in front of building a complete OSM-based noisy information filtering methods that can overcome the shortcomings of OSM information filters. These challenges are summarized in: (i) big data; (ii) privacy and security; (iii) structure heterogeneity; (iv) UGC format diversity; (v) subjectivity and objectivity; (vi) and service limitations. In this thesis, we focus on increasing the quality of social UGC that are published and publicly accessible in forms of posts and profiles over OSNs through addressing in-depth the stated serious challenges. As the social spam is the most common IQ problem appearing over the OSM, we introduce a design of two generic approaches for detecting and filtering out the spam content. The first approach is for detecting the spam posts (e.g., spam tweets) in a real-time stream, while the other approach is dedicated for handling a big data collection of social profiles (e.g., Twitter accounts).
The increasing need of human assistance pushed researchers to develop automatic, smart and tireless dialogue systems that can converse with humans in natural language to be either their virtual assistant or their chat companion. The industry of dialogue systems has been very popular in the last decade and many systems from industry and academia have been developed. In this thesis, we study retrieval-based dialogue systems which aim to find the most appropriate response to the conversation among a set of predefined responses. The main challenge of these systems is to understand the conversation and identify the elements that describe the problem and the solution which are usually implicit. Most of the recent approaches are based on deep learning techniques which can automatically capture implicit information. However these approaches are either complex or domain dependent. We propose a simple, end-to-end and efficient retrieval-based dialogue system that first matches the response with the history of the conversation on the sequence-level and then we extend the system to multiple levels while keeping the architecture simple and domain independent. We perform several analyzes to determine possible improvements.
The aim of this work is to improve the clarity and precision of the technical specifications written in French by the engineers at CNES (Centre National d'Études Spatiales / National Centre for Space Studies) prior to the realization of space systems. The importance of specifications (and particularly of the requirements that are part of them) for the success of large-scale projects is indeed widely acknowledged; similarly, the main risks associated with the use of natural language (ambiguity, vagueness, incompleteness) are relatively well identified. A Controlled Natural Language (CNL) – i.e. a set of linguistic rules constraining the lexicon, the syntax and the semantics – seems to be an interesting option, provided that it remains close enough to natural language. Unfortunately, the CNLs for technical writing that we have examined are not always relevant from a linguistic point of view. Our methodology for developping a CNL for requirements writing in French at CNES relies on the hypothesis of the existence of a textual genre; besides, we make use of existing Natural Language Processing tools and methods to validate the relevance of the rules on a corpus of genuine requirements written for former projects.
To evaluate the accuracy of the analyser, we submitted Malaysian texts and one Indonesian text to the system. This analyser uses: a set of rules, a few list of exceptions, a restricted list of bases and formal identification criteria. The algorithm is non deterministic. Analysed words are treated without taking account of their contexts. The evaluation of the analyser gave around 97% of correct analysis and 2% of incorrect analysis. Very few affixed words were not analysed (rate less than 0,5%)
Text has been the dominant way of storing data in computer systems and sending information around the Web. Extracting meaningful representations out of text has been a key element for modelling language in order to tackle NLP tasks like text classification. These representations can then form groups that one can use for supervised learning problems. More specifically, one can utilize these linguistic groups for regularization purposes. The main goal of this thesis is to study the aforementioned problems; first, by examining new graph-based representations of text. Next, we studied how groups of these representations can help regularization in machine learning models for text classification. Last, we dealt with sets and measuring distances between documents, utilizing our proposed linguistic groups, as well as graph-based approaches. In the first part of the thesis, we have studied graph-based representations of text. Turning text to graphs is not trivial and has been around even before word embeddings were introduced to the NLP community. In our work, we show that graph-based representations of text can capture effectively relationships like order, semantic or syntactic structure. Moreover, they can be created fast while offering great versatility for multiple tasks. In the second part, we focused on structured regularization for text. Textual data suffer from the dimensionality problem, creating huge feature spaces. Regularization is critical for any machine learning model, as it can address overfitting. In our work we present novel approaches for text regularization, by introducing new groups of linguistic structures and designing new algorithms. In the last part of the thesis, we study new methods to measure distance in the word embedding space. First, we introduce diverse methods to boost comparison between documents that consist of word vectors. Next, representing the comparison of the documents as a weighted bipartite matching, we show how we can learn hidden representations and improve results for the text classification task. Finally, we conclude by summarizing the main points of the total contribution and discuss future directions.
How are the social and semantic structures of a scientific community driving future research dynamics? In this thesis we combine natural language processing techniques and network theory methods to analyze a very large dataset of scientific publications in the field of computational linguistics,i.e.the ACL Anthology. Ultimately, our goal is to understand the role of collaborations among researchers in building and shaping the landscape of scientific knowledge, and, symmetrically, to understand how the configuration of this landscape influences individual trajectories of researchers and their interactions. We use natural language processing tools to extract the terms corresponding to scientific concepts from the texts of the publications. Then we reconstruct a socio-semantic network connecting researchers and scientific concepts, and model the dynamics of its evolution at different scales. To achieve this, we first build a statistical model, based on multivariate logistic regression, that quantifies the role that social and semantic features play in the evolution of the socio-semantic network, namely in the emergence of new links. Then, were construct the evolution of the field through different visualizations of the knowledge produced therein, and of the flow of researchers across the different subfields of the domain. To summarize, we have shown through our work that the combination of natural language processing techniques with complex network analysis makes it possible to investigate in a novel way the evolution of scientific fields.
Prosodic highlighting refers to the distinction of a constituent through various prosodic means, especially accentuation and intonation. It is taken to fulfill several functions: marking the different types of focus, as well as emphatic functions (named here “insisting” and “expressiveness”). The main goal of this thesis is to determine whether prosodic highlighting and its functions display specific features in interpreted speech, a speaking style that can be defined as the oralization of a written text previously memorized by the speaker (typically an actor). This question is relevant for linguistics and phonetics on several counts. First, little is still known about prosodic differences between functions of prosodic highlighting. Moreover, few studies have analyzed the prosodic characteristics of interpreted speech. Finally, through their innovative protocols, the two experiments described in this thesis present a methodological contribution. A production experiment consisted in having speakers replicate spontaneous conversations in read and interpreted speech. A group of experts then annotated the occurrences of prosodic highlighting in the corpus, and assigned a function to each occurrence. A perception experiment was also led in order to compare the realization of each function independently of speaking style. Despite a relatively low agreement rate between experts (which raises several methodological and theoretical questions), our analyses reveal several important results. The frequency of occurrence of prosodic highlighting is highest in interpreted speech, followed by read speech. This confirms our prediction and suggests that interpreted speech is more suited to the study of prosodic highlighting than other speaking styles. A strong association is observed between insisting and initial secondary accent, which confirms many previous studies. However, there is almost no influence of speaking style on the realization of prosodic highlighting and its functions. We attribute this result to a lack of data and to the fact that some prosodic features were not taken into account in the analysis.
This thesis provides a corpus‐based description of Kakabe, a Mande language spoken in Guinea, with a focus on phonology. It consists of a short grammatical sketch and two parts dedicated to the analysis of the segmental and the suprasegmental phonology. I also describe various strategies of loanword adaptation used in Kakabe, such as vowel epenthesis and consonant cluster simplification. Kakabe is a terraced‐level tone language (H vs. L), featuring downdrift, downstep, H raising, floating L, and a number of tonal processes, such as OCP style H‐insertion between two L domains, tone spread and leveling of HLH contour. As a result, the distance between the underlying lexical tones and their surface realization can be rather important. Each tonal process is applied within one particular prosodic unit. Therefore, tonal processes participate in phrasing the speech into prosodic units. Kakabe uses a number of boundary tones to signal illocutionary force of the utterance. Lexical tones and boundary tones coexists with intonational operations on the F0 curve. The appendices include a Kakabe‐French dictionary, comprising 3400 entries, and an oral corpus of 12 hours of various genres, transcribed, glossed and time‐aligned with audio and video.
Within the framework of the international development of open access to scientific publications, this thesis analyses more precisely the French situation in the European context. This analysis was carried out through a process of action research within a group of actors of the French Professional Group for B to B Information and Knowledge (GFII) who are concerned by open access. At first, we seek to highlight the driving forces behind the development of open access by relying on a method of prospective developed by LIPSOR/ CNAM.The results led us to contribute to the design of an information website whose purpose is to display the national publisher policies on self-archiving practices in order to support the development of new deposit practices at the national level. The prospective analysis has indeed revealed the importance of embargo periods for the financial balances of the publishers. By adopting a more distanced point of view, we initiate a new reflection about the real impact of open access on two important driving forces which seem to play an increasing role in the knowledge economy, namely creativity and interdisciplinarity.
To learn is to extract and distill pertinent information from a set of evidence. Yet, the evidence a learner has, or could have, at hand may seem insufficient for what she is aiming to acquire. The insufficiency of the evidence is often evoked in respect to language acquisition: the word meanings and grammar that individuals know appear to require more evidence than that to which they have access in their environments. While the brunt of research has focused on identifying overlooked sources of evidence and widening the evidence set, here we switch gears and probe what exactly it is that the evidence set needs to support. We propose that the evidence a learner has may be insufficient to provide her with the knowledge competent language users think they have; however, the very same evidence may be sufficient to provide a learner with the information needed for cognitive processing of language. Very broadly, there may be a gap between what we think we acquire and what we really acquire. In the introductory section of this dissertation, we begin by presenting evidence of a gap between what we feel words mean and how meanings are processed in the mind. We frame this gap in the broader context of how minds, with finite access to evidence, make sense the external world. Then, in the first chapter, we investigate how comparably little evidence can fuel acquisition of grammar in an ecologically valid setting. Our results reveal that just a handful of words can spur a virtuous cycle of grammar and vocabulary acquisition in infants. Next, in the second chapter, we examine whether the same set of evidence gives rise to productive knowledge or generalization (i.e., the capacity to use prior knowledge to interpret novel situations) across development, from infancy into childhood and through to adulthood. Our data show that infants and adults generalize a novel grammatical context to new words, but pre-school children do not. We interpret our results within the extant literature, pointing to the live possibility that what counts as knowledge may depend on where an individual places her 'knowledge threshold'rather than an immutable ideal. Finally, in the third chapter, we probe whether evidence that reflects a knowledge state (e.g., explicitly hearing a direct translation: 'Bamoule'means 'cat') is inherently more informative, or merely more appealing, for a learner. Our results demonstrate that pre-packaged knowledge-state evidence boosts confidence, but has variable effects on performance. Across three chapters and ten experiments, we build up a set of fundamental features about what it is 'to know': (1) a little evidence can go a long way, (2) how much evidence is considered to be enough may depend on a modifiable threshold, and (3) the mind may crave certainty. We advance the conclusion that the set of evidence available to an individual can be sufficient to foster knowledge. It may just not be sufficient to foster the kind of knowledge we think we have. Therefore, to better understand the way we learn, we need to investigate what is 'to know'from the point of view of the learner.
Deep learning is a major advance in Artificial Intelligence (A.I) in recent years. It has quickly established itself as a standard in many fields by beating other machine learning records in its primary domains of application: computer vision and natural language processing. Deep learning algorithms are promising in many other domains of science and especially in precision medicine. In this thesis, we will focus on the prediction of phenotypes (diagnosis, prognosis, response to treatment) from gene expression profiles. Most of these articles appeared in the past two years and only a small part deals with phenotype prediction. Contrary to images or natural language where several hundred thousand or million examples are available, GE sets contain very few patients (&lt;5000). Due to this small number of samples, the model is overfitting. We will use the state-of-the-art methods on images such as adversarial autoencoders, ladder networks or generative adversarial networks and adapt them to the specific problem of expression data. Another solution is to integrate datasets coming from different production platforms. Until now, any satisfying solution has been found to resolve this problem because of the presence of complex links among the measures of the different platforms. Indeed, a GE measure from a platform A can be spread over several expressions and mixed with other expressions from a platform B. Our idea is to build a neural network able to learn the correspondence between two platforms. To do this, we will use domain transfer methods such as CycleGAN. Another major challenge concerns the interpretation of neural networks and their predictions. The General Data Protection Regulation (GDPR), adopted recently by the European Union, imposes that the user of machine learning algorithms must be able to explain the decision of a given model. There is a real need to make neural networks more interpretable and this is particularly true in the medical field for two reasons. First, it is important to ensure that the neural network bases its predictions on a reliable representation of the data and does not focus on irrelevant artifacts. Without explanation, doctors cannot trust the decision of a neural network regardless of its performances. The interpretation of neural networks will be carried out using perturbation and back-propagation of the output signal methods. In this thesis, we will collaborate with the Institute of Cardiometabolism and Nutrition (ICAN), which will provide us with the GE profile of patients suffering from cardiometabolic diseases. They will also give us access to the data of the European Metacardis project, which contains clinical and GE data from more than 2000 patients. Expected result: We will propose appropriate deep learning approaches for the prediction from GE data and the biological interpretation of neural networks. The networks that will be trained on large datasets will be available to the scientific community. Thus, by transfer learning, researchers will be able to use our networks on smaller datasets and improve their results. The objective is to provide an equivalent of the VGG or ResNet for GE data.
In this research, we are interested in investigating issues related to query evaluation and optimization in the framework of aggregated search. Aggregated search is a new paradigm to access massively distributed information. It aims to produce answers to queries by combining fragments of information from different sources. The queries search for objects (documents) that do not exist as such in the targeted sources, but are built from fragments extracted from the different sources. The sources might not be specified in the query expression, they are dynamically discovered at runtime. In our work, we consider data dependencies to propose a framework for optimizing query evaluation over distributed graph-oriented data sources. For this purpose, we propose an approach for the document indexing/orgranizing process of aggregated search systems. We consider information retrieval systems that are graph oriented (RDF graphs). Using graph relationships, our work is within relational aggregated search where relationships are used to aggregate fragments of information. Our goal is to optimize the access to source of information in a aggregated search system. These sources contain fragments of information that are relevant partially for the query. We aim at minimizing the number of sources to ask, also at maximizing the aggregation operations within a same source. For this, we propose to reorganize the graph database(s) in partitions, dedicated to aggregated queries. We use a semantic or strucutral clustering of RDF predicates. For structural clustering, we propose to use frequent subgraph mining algorithms, we performed for this, a comparative study of their performances. For semantic clustering, we use the descriptive metadata of RDF predicates and apply semantic textual similarity methods to calculate their relatedness.
The increase of available data in almost every domain raises the necessity of employing algorithms for automated data analysis. This necessity is highlighted in predictive maintenance, where the ultimate objective is to predict failures of hardware components by continuously observing their status, in order to plan maintenance actions well in advance. These observations are generated by monitoring systems usually in the form of time series and event logs and cover the lifespan of the corresponding components. Analyzing this history of observation in order to develop predictive models is the main challenge of data driven predictive maintenance. Towards this direction, Machine Learning has become ubiquitous since it provides the means of extracting knowledge from a variety of data sources with the minimum human intervention. The goal of this dissertation is to study and address challenging problems in aviation related to predicting failures of components on-board. The amount of data related to the operation of aircraft is enormous and therefore, scalability is a key requirement in every proposed approach. This dissertation is divided in three main parts that correspond to the different data sources that we encountered during our work. In the first part, we targeted the problem of predicting system failures, given the history of Post Flight Reports. We proposed a regression-based approach preceded by a meticulous formulation and data pre-processing/transformation. Our method approximates the risk of failure with a scalable solution, deployed in a cluster environment both in training and testing. To our knowledge, there is no available method for tackling this problem until the time this thesis was written. The second part consists analyzing logbook data, which consist of text describing aircraft issues and the corresponding maintenance actions and it is written by maintenance engineers. The logbook contains information that is not reflected in the post-flight reports and it is very essential in several applications, including failure prediction. However, since the logbook contains text written by humans, it contains a lot of noise that needs to be removed in order to extract useful information. We tackled this problem by proposing an approach based on vector representations of words (or word embeddings). Our approach exploits semantic similarities of words, learned by neural networks that generated the vector representations, in order to identify and correct spelling mistakes and abbreviations. Finally, important keywords are extracted using Part of Speech Tagging. In the third part, we tackled the problem of assessing the health of components on-board using sensor measurements. In the cases under consideration, the condition of the component is assessed by the magnitude of the sensor's fluctuation and a monotonically increasing trend. In our approach, we formulated a time series decomposition problem in order to separate the fluctuation from the trend by solving a convex program.
This thesis deals with the capture, annotation, synthesis and evaluation of arm and hand motions for the animation of avatars communicating in Sign Languages (SL). Currently, the production and dissemination of SL messages often depend on video recordings which lack depth information and for which editing and analysis are complex issues. Signing avatars constitute a powerful alternative to video. They are generally animated using either procedural or data-driven techniques. Procedural animation often results in robotic and unrealistic motions, but any sign can be precisely produced. With data-driven animation, the avatar's motions are realistic but the variety of the signs that can be synthesized is limited and/or biased by the initial database. As we considered the acceptance of the avatar to be a prime issue, we selected the data-driven approach but, to address its main limitation, we propose to use annotated motions present in an SL Motion Capture database to synthesize novel SL signs and utterances absent from this initial database. To achieve this goal, our first contribution is the design, recording and perceptual evaluation of a French Sign Language (LSF) Motion Capture database composed of signs and utterances performed by deaf LSF teachers. Our second contribution is the development of automatic annotation techniques for different tracks based on the analysis of the kinematic properties of specific joints and existing machine learning algorithms. Our last contribution is the implementation of different motion synthesis techniques based on motion retrieval per phonological component and on the modular reconstruction of new SL content with the additional use of motion generation techniques such as inverse kinematics, parameterized to comply to the properties of real motions.
Specialists in didactics aim to create an efficient method, whose teaching / learning content and tools improve phonetic skills in foreign languages. As for the educational content, research studies have proved that sounds and phonemes of a foreign language are processed according to the structure of the phonetic and phonological space of the native language. Other works point out that it is particularly relevant to compare linguistic systems in order to predict future difficulties and abilities language learners will be confronted with. As for transmission tools, studies have shown the beneficial effects of interdisciplinarity and the pertinent role music plays on cognitive and learning development. Our research objective falls within this scientific context. Our purpose has been two-fold. First, we tried to identify which parameter, related to the production of the singing voice whilst separate from the speaking voice, may facilitate the perception of non-native vowels. Secondly, we aimed at comparing the effects on the ability to produce non-native vowels of two corrective phonetic methods, one of which used the “singing voice” tool. Through the results of these studies, we tried to understand how Italian as a native language interacts with the perception and the production of French as a target language. Our studies have shown that vowel pitch and duration do not impact the discrimination of /y/ and /ø/, and that the consonant sharpness plays a role on the discrimination of /y/ in a CV type syllable. We found a positive effect of the method, which uses singing-voice as a tool, on the production of the sound spectrum of French closed vowels, but not on the evolution of the sounds and phonemes into the acoustic space. Our results support the theory that phonetic teaching and learning is relevant in language classes and suggest that singing-voice may be a useful tool to ease the perception and the production of non-native vowels.
This thesis addresses the problem of multichannel audio source separation by exploiting deep neural networks (DNNs). We build upon the classical expectation-maximization (EM) based source separation framework employing a multichannel Gaussian model, in which the sources are characterized by their power spectral densities and their source spatial covariance matrices. We explore and optimize the use of DNNs for estimating these spectral and spatial parameters. Employing the estimated source parameters, we then derive a time-varying multichannel Wiener filter for the separation of each source. We extensively study the impact of various design choices for the spectral and spatial DNNs. We consider different cost functions, time-frequency representations, architectures, and training data sizes. Those cost functions notably include a newly proposed task-oriented signal-to-distortion ratio cost function for spectral DNNs. Furthermore, we present a weighted spatial parameter estimation formula, which generalizes the corresponding exact EM formulation. On a singing-voice separation task, our systems perform remarkably close to the current state-of-the-art method and provide up to 2 dB improvement of the source-to-interference ratio. On a speech enhancement task, our systems outperforms the state-of-the-art GEV-BAN beamformer by 14%, 7%, and 1% relative word error rate improvement on 6-channel, 4-channel, and 2-channel data, respectively
The aim of this study is propose a typology of predicates of motion in Hungarian. The typology reflects a simple objective perception of motion and space. The analysis uses the theory of object classes, which we applied to Hungarian. Our classification is based on semantic properties such as directionality, mood destination, goal, place and the aspectual properties. These semantic properties are completed by morpho-syntactic properties needed for natural language processing. The contrastive component of our study has made it possible to propose a better description of the classes of predicates in Hungarian and to bring out the morpho-syntactic and combinatory differences specific to both languages in the expression of motion, such as the role of verb prefixes, locative complements, and underline the importance of noun predicate.
The mass of information in the legal field, which is constantly increasing, has generated a capital need to organize and structure the contents of the documents available, and thus transform them into an intelligent guide, capable of provide complete and immediate answers to queries in natural language. As a result, question-answering systems (QASs) respond perfectly to this need by offering the different mechanisms to provide adequate and precise answers to questions expressed in natural language. Indeed, this type of system allows the user to ask a question in natural language and to receive a precise answer to his request instead of a set of documents deemed relevant, as is the case for search engines. Our objective is to set up a question-answering system operating in the legal field in Morocco which mostly uses the French and Arabic languages, see the English language. Its purpose is to give a relevant and concise answer to questions in the legal field, stated in natural language by a user, without the latter having to go through legal documents to find an answer to his question, which can be expensive in terms of time.
In this work, we study analogy-based models for Machine Learning of Natural Language. The analogical approach offers an alternative to both deductive methods (in which specific knowledge is infered from general knowledge) and inductive methods (in which general knowledge is infered from specific knowledge). In this setting, the analysis of a new entity is performed by comparison with available data; inference is directly achieved from specific knowledge to specific knowledge. In this approach, abstraction, which is involved in both deductive and inductive models is no longer required. Moreover, this approach correctly account for the paradigmatic organization of linguistic data, which easily relates one linguistic entity with others through specific schemes; the linguistic knowledge is thus implicitly represented within the corpus. In particular, this paradigmatic organization suggests to consider analogical proportions. A learning model is presented, which relies on the exploitation of analogical proportions. We introduce the notion of analogical extension, which allows for the expression of its learning bias. We also propose a formal algebraic framework which gives a meaning to the notion of analogical proportion between structured objects.
Our perception is by nature multimodal, i.e. it appeals to many of our senses. To solve certain tasks, it is therefore relevant to use different modalities, such as sound or image. This thesis focuses on this notion in the context of deep learning. For this, it seeks to answer a particular problem: how to merge the different modalities within a deep neural network? We first propose to study a problem of concrete application: the automatic recognition of emotion in audio-visual contents. This leads us to different considerations concerning the modeling of emotions and more particularly of facial expressions. We thus propose an analysis of representations of facial expression learned by a deep neural network. In addition, we observe that each multimodal problem appears to require the use of a different merge strategy. Finally, we are interested in a multimodal view of knowledge transfer. Indeed, we detail a non-traditional method to transfer knowledge from several sources, i.e. from several pre-trained models. For that, a more general neural representation is obtained from a single model, which brings together the knowledge contained in the pre-trained models and leads to state-of-the-art performances on a variety of facial analysis tasks.
At STMicroelectronics, the Business Intelligence team is daily confronted to exploit data and information to create reports about manufacturing activities in order to supervise it. In such an industrial organization, products change regularly and data can quickly become obsolete. Consequently, over time, the number of created reports is highly growing, while knowledge about their creation is lost. This is shown in a qualitative and quantitative evaluation of the main part of the STMicroelectronics'knowledge system. As a result, problems related to knowledge obsolescence, duplication, non-centralization and proliferation continuously arise. Its objective is to effectively and continuously capitalize expert knowledge while targeting business needs and providing an evolving solution. It is based on a Business Intelligence for Business Intelligence system (BI4BI). Since knowledge is embedded not only in systems and tools, but also in human minds and practices, our proposed knowledge capitalization solution also involves people and organizations: it proposes to collect users'feedbacks and insights to integrate them in knowledge representation and in our BI4BI tool.
Automata theory bas been developed to overcome both theoretical and practical problems. Nowadays, automata are considered as basic knowledge by all computer scientists, and they are used in most softwares. In 1974, Samuel Eilenberg gave a new machine model unifying the most common automata such as transducers, pushdown automata, and even Turing machines. We propose an effective design of Eilenberg machines and study simulation techniques. Our Simulator is defined by a functional program that progressively enumerates solutions, exploring a research space according to various of strategies. We introduce the notion of finite Eilenberg machines, and formally prove the correction of the underlying simulation engine. In recent years, the concept of Brzozowski's derivatives has led to many novel and efficient algorithms that compile regular expressions into non-deterministic automata. We review these state of the art algorithms, give an efficient OCaml implémentation, and compare their efficiency in a common framework.
Natural Language Processing (NLP) is a research field which grew up dramatically the last few years. Large scale communication ways made appear huge data amounts with a lot of added value, but impossible to process by hand. Meaning extraction of these exchanges is a crucial commercial stake for a lot of commercial companies, like the GAFAMI (social networks, clients feeds on web distributed goods,), but also for public entities. Academic and private research so did naturally turn to develop tools and techniques allowing to pinpoint the meaning of a text for the lowest price and the fewest functional constraints. These technologies are especially interesting for they allow setting up a social network listening system about a target place, like a railway station, a museum or an airport. It is to say that the information running about it can be exploited to answer different needs. In example, it is possible to predict peak days and to locate where the crowd will be the densest on the site to enhance staff management or to detect targets for potential malicious actions. Flow scheme building according to users spoken language can also help optimizing the customers-oriented information display to make it more accessible. Nevertheless, meaning extraction from text data can face a linguistic barrier in particular if those are written in different language. Their acquisition and their analysis must so be operable independently from the used language so the information behind the text can be exploited. Regarding all of this, building a tool capable of listening to different social networks in order to identify elements, from various languages, we want to quantify and synthetize may help to make services and security better in a place such as an airport. Events with an emergency trait are pinpointed because they may degrade the airport activity itself. For example, a storm can prevent planes to land or lift-off, and you must foresee it in order to deal with the impacted passengers, staff, connections and so on. A traffic jam on the other hand prevent people from leaving the airport, or pilots and personnel from getting on board. This is in these problematics that this research project takes its place.
We address the problem of model-independent searches for New Physics (NP), at the Large Hadron Collider (LHC) using the ATLAS detector. Particular attention is paid to the development and testing of novel Machine Learning techniques for that purpose. The present work presents three main results. Firstly, we put in place a system for automatic generic signature monitoring within TADA, a software tool from ATLAS. We explored over 30 signatures in the data taking period of 2017 and no particular discrepancy was observed with respect to the Standard Model processes simulations. Secondly, we propose a collective anomaly detection method for model-independent searches for NP at the LHC. We propose the parametric approach that uses a semi-supervised learning algorithm. This approach uses penalized likelihood and is able to simultaneously perform appropriate variable selection and detect possible collective anomalous behavior in data with respect to a given background sample. Thirdly, we present preliminary studies on modeling background and detecting generic signals in invariant mass spectra using Gaussian processes (GPs) with no mean prior information. Two methods were tested in two datasets: a two-step procedure in a dataset taken from Standard Model simulations used for ATLAS General Search, in the channel containing two jets in the final state, and a three-step procedure from a simulated dataset for signal (Z′) and background (Standard Model) in the search for resonances in the top pair invariant mass spectrum case. Our study is a first step towards a method that takes advantage of GPs as a modeling tool that can be applied to several signatures in a more model independent setup.
Because of their multiple ecological roles, macrophytes are an important component of hydrosystems, and are thus essential to conserve. However, during the summer season, high densities of submerged species cause recurrent problems in certain rivers, especially in urban areas for users and managers, and can have negative consequences for ecosystem health. In a context of global change, the overgrowth of submerged macrophytes calls for new tools to better understand the dynamics of macrophytes meadows and to predict their dynamics according to different environmental scenarios. In this context, this thesis aims to develop a toolbox accompanying a multispecific mechanistic model for the production of submerged aquatic plants, the DEMETHER model. To do this, the model requires determining certain ecophysiological parameters and having spatialized biomass data for its calibration, as well as bathymetric and substrate data. The first phase of this work then consisted in field surveys to characterize the study site and in developing numerical or experimental tools for the acquisition of these data. The first tool developed aims to monitor submerged macrophytes by remote sensing. The method explored here confirmed the potential of high spatial resolution (50 cm) multispectral imagery of Pléiades satellites, processed by machine learning algorithms, to map the distribution of macrophyte beds and quantify their biomass in situ. This approach has also led us to propose an optimized sampling strategy for macrophytes in large rivers for future investigations. This work opens up interesting perspectives for applying the method to drone imagery, and continuing its development for automated monthly monitoring. In parallel, a tool was developed for measuring physiological parameters via oximetry and applied to the two species of interest. The data obtained provide information in particular on the photosynthetic and respiratory capacities of each species in response to limiting factors (light, temperature). The second phase of this work consisted in applying the DEMETHER model to explore different climate change scenarios. Simulations of the macrophyte dynamics in terms of biomass were carried out for current thermal conditions and for a foreseeable rise in temperatures by 2041-2070. The results showed the importance of the temperature sensitivity of certain physiological processes to explain the distribution patterns of the two species studied, highlighting the interest of mechanistic modelling to understand the structuring of macrophyte communities. The first results obtained with this toolbox confirmed its functionality. However, in order to extend its application range, each of the tools developed during the thesis will need to be further improved, in particular to refine the calibration of the DEMETHER model. Specific suggestions have been made to this aim.
This PhD thesis deals with the automatic continuous Cued Speech (CS) recognition based on the images of subjects without marking any artificial landmark. In order to realize this objective, we extract high level features of three information flows (lips, hand positions and shapes), and find an optimal approach to merging them for a robust CS recognition system. We first introduce a novel and powerful deep learning method based on the Convolutional Neural Networks (CNNs) for extracting the hand shape/lips features from raw images. Theadaptive background mixture models (ABMMs) are also applied to obtain the hand position features for the first time. All these methods make significant contributions to the feature extraction in CS. Then, due to the asynchrony problem of three feature flows (i.e., lips, hand shape and hand position) in CS,the fusion of them is a challenging issue. In order to resolve it, we propose several approaches including feature-level and model-level fusion strategies combined with the context-dependent HMM. To achieve the CS recognition, we propose three tandem CNNs-HMM architectures with different fusion types. All these architectures are evaluated on the corpus without any artifice, and the CS recognition performance confirms the efficiency of our proposed methods. The result is comparable with the state of the art using the corpus with artifices. In parallel,we investigate a specific study about the temporal organization of hand movements in CS,especially about its temporal segmentation, and the evaluations confirm the superior performance of our methods.
Nonetheless, hitherto there has been no systematic theory that tries to propose a formal account regarding the make-up and range of actual and possible conversational types. In this thesis, we take a topological approach to classifying conversations and develop a formal theory of conversational types in the framework of Type Theory with Records (TTR). Conversely, we investigate whether the variation in the distribution of NSUs can serve as a means of structuring the space of conversational types.
This study is concerned with the linguistic realisation of explanation in contemporary English. After a definitional work on the notion, two types of explanation are identified: clarifying explanation and causal explanation. The study adopts an utterer-centered approach while also considering pragmatic aspects. The linguistic analysis is based on a corpus of didactic texts and it focuses on the role played by connectives in explanatory discourse. Finally, the study also aims at formalising some discourse relations involved in explanations and designing linguistic rules in order to automatically identify the discourse relations at stake.
This dissertation is part of a larger movement, both national and international, acknowledging the growing importance and inquiring about the democratic legitimacy of judicial institutions. In looking at the judicial office and its practice, it investigates the role of public opinion, largely considered an element of democratic legitimacy. To obtain a more complete perspective on judicial institutions and public opinion, a comparative approach is adopted and the United States Supreme Court, and the European Court of Human Rights are examined. This study adopts the following reasoning. At a theoretical level, it attempts to clarify The multifaceted concept of “public opinion” and to establish the different sources of judicial legitimacy, in order to determine whether public opinion can be considered such a source. It then studies the substance of judicial decisions, which reveal judges' conception of the role of public opinion in democracy and in the judicial evolution of rights and liberties. The content-study of judicial decisions focuses on first on the relationship between public opinion and democracy in the protection of freedom of expression and second on the rote of public opinion in the evolution of the rights of homosexual persons.
This dissertation offers a study of parce que constructions in oral speech, at the interface between corpus and theoretical linguistics. The corpus under study comprises conversations recorded from four surveys from the PFC (Phonologie du Français Contemporain – Phonology of Contemporary French) project database. The first stage of the annotation process allows the syntactic characterization of said constructions. To this end, we put forward a new Explanation-type relation, which allows the annotation of parce que in relation to utterance modality. We provide elements in order to identify the terms connected by parce que which confirm the benefit of a twofold analysis of the corpus, from both a syntactic and a discursive point of view.
Despite all the design efforts, there are always uses and situations for which the user interface is not perfect. This thesis investigates self-explanatory user interfaces for improving the quality perceived by end users. The approach follows the principles of model-driven engineering. It consists in keeping the design models at runtime so that to dynamically enrich the user interface with a set of possible questions and answers. The questions are related to usage (for instance, "What's the purpose of this button? ", "Why is this action not possible"?) as well as to design rationale (for instance, "Why are the items not alphabetically ordered?"). This thesis proposes a software infrastructure UsiExplain based on the UsiXML metamodels. An evaluation conducted on a case study related to a car shopping webiste confirms that the approach is relevant especially for usage questions.
The constant increase of available documents and tools to access them has led to a change of research practices. For a few years now, more and more information retrieval platforms are made available online to the scientific community or the public. Formerly, the main issue for researchers was to identify if a particular resource existed. Today, the challenge is more about finding how to access pertinent information. We have identified two distinct levers to limit the impact of this new search paradigm. First, we believe that it is necessary to analyze how the different search platforms are used. To be able to understand and read into users behavior is a necessary step to comprehend what users understand, and to identify what they need to get an in-depth understanding of the operation of such platforms. Indeed, most systems act as black boxes which conceal the underlying transformations applied on data. Why is the search engine returning those particular results? Why is this document more pertinent than another? Such seemingly naive questions are nonetheless essential to undertake an analytical approach of the information search and retrieval task. We think that users have a right and a duty to question themselves about the relevance of such and such tool at their disposal. To help them cope with these issues, we developped a dual-use information search platform. On the one hand, it can be used to observe and understand user behavior. On the other hand, it can be used as a pedagogical medium to highlight research biases users can be exposed to. At the same time, we believe that the tools themselves must be improved. In the second part of this thesis, we study the impact that the quality of documents can have on their accessibility. Because of the increase of documents available online, human operators are less and less able to insure their quality. Thus, there is a need to set up new strategies to improve the way search platform operate and process documents. We propose a new method to automatically identify and correct errors generated by information extraction process such as OCR.
The present work introduces in phonetics, the atomic decomposition of the signal also known as the Matching Pursuit and treats a group of atoms by compression without losses and finally measures the distance of the list of atoms compressed using the Kolmogorov's algorithms. The calibration is based on an initial classical analysis of the co-articulation of sound sequences of VCV and CV, or V ∈ {[i] [u] [a]} and C ∈ {[t] [d] [s] [δ]}∪ [tʕ] [dʕ] [sʕ [δʕ]} the excerpts culled from a corpus made up of four arabic speaking areas. The locus equation of CV vs CʕV, makes it possible to differentiate the varieties of the language. In the second analysis, an algorithm of atomic adaptative decomposition or Matching Pursuit is applied to the sequences VCV and VCʕV still on the same corpus. The atomic sequences representing VCV et VCʕV are then compressed without losses and the distances between them are searched for by Kolmogorov's algorithms. The classification of phonetic recordings obtained from these arabic speaking areas is equivalent to that of the first method. The findings of the study show how the introduction of Matching Pursuit's in phonetics works, the great robustness of the use of algorithms and suggesting important possibilities of automation of processes put in place, while opening new grounds for further investigations
This work has been jointly supervised by U. Jean Monnet Saint Etienne, in the Hubert Curien Lab (Frederique Laforest, Christophe Gravier, Julien Subercaze) and U. Mohamed V Rabat, LeRMA ENSIAS (Rachida Ahjoun, Mounia Abik). Knowledge, education and learning are major concerns in today's society. The technologies for human learning aim to promote, stimulate, support and validate the learning process. Our approach explores the opportunities raised by mixing the Social Web and the Semantic Web technologies for e-learning. More precisely, we work on discovering learners profiles from their activities on the social web. The Social Web can be a source of information, as it involves users in the information world and gives them the ability to participate in the construction and dissemination of knowledge. We focused our attention on tracking the different types of contributions, activities and conversations in learners spontaneous collaborative activities on social networks. The learner profile is not only based on the knowledge extracted from his/her activities on the e-learning system, but also from his/her many activities on social networks. We propose a methodology for exploiting hashtags contained in users' writings for the automatic generation of learner's semantic profiles. Hashtags require some processing before being source of knowledge on the user interests. We have defined a method to identify semantics of hashtags and semantic relationships between the meanings of different hashtags. By the way, we have defined the concept of Folksionary, as a hashtags dictionary that for each hashtag clusters its definitions into meanings. Semantized hashtags are thus used to feed the learner's profile so as to personalize recommendations on learning material. The goal is to build a semantic representation of the activities and interests of learners on social networks in order to enrich their profiles. We also discuss our recommendation approach based on three types of filtering (personalized, social, and statistical interactions with the system). We focus on personalized recommendation of pedagogical resources to the learner according to his/her expectations and profile
The contributions of this thesis are numerical and theoretical tools for the resolution of blind inverse problems in imaging. A very popular approach consists in estimating this operator from an image containing point sources (microbeads or fluorescent proteins in microscopy, stars in astronomy). Such an observation provides a measure of the impulse response of the degradation operator at several points in the field of view. Processing this observation requires robust tools that can rapidly use the data. We propose a toolbox that estimates a degradation operator from an image containing point sources. The estimated operator has the property that at any location in the field of view, its impulse response is expressed as a linear combination of elementary estimated functions. This makes it possible to estimate spatially invariant (convolution) and variant (product-convolution expansion) operators. An important specificity of this toolbox is its high level of automation: only a small number of easily accessible parameters allows to cover a large majority of practical cases. The size of the point source (e.g. bead), the background and the noise are also taken in consideration in the estimation. This tool, coined PSF-estimator, comes in the form of a module for the Fiji software, and is based on a parallelized implementation in C++. The operators generated by an optical system are usually changing for each experiment, which ideally requires a calibration of the system before each acquisition. To overcome this, we propose to represent an optical system not by a single operator (e.g. convolution blur with a fixed kernel for different experiments), but by subspace of operators. This set allows to represent all the possible states of a microscope. We introduce a method for estimating such a subspace from a collection of low rank operators (such as those estimated by the toolbox PSF-Estimator). We show that under reasonable assumptions, this subspace is low-dimensional and consists of low rank elements. In a second step, we apply this process in microscopy on large fields of view and with spatially varying operators. This implementation is possible thanks to the use of additional methods to process real images (e.g. background, noise, discretization of the observation).The construction of an operator subspace is only one step in the resolution of blind inverse problems.
In this thesis, we present a multilevel scheme consisting of both hardware and software solutions to improve the daily operational life of firefighters. As a core part of this scheme, we design and develop a smart system of wearable IoT devices used for state assessment and localization of firefighters during interventions. To ensure a maximum lifetime for this system, we propose multiple data-driven energy management techniques for resource constraint IoT devices. The first one is an algorithm that reduces the amount of data transmitted between the sensor and the destination (Sink). This latter exploits the temporal correlation of collected sensor measurements to build a simple yet robust model that can forecast future observations. Then, we coupled this approach with a mechanism that can identify lost packets, force synchronization, and reconstruct missing data. Furthermore, knowing that the sensing activity does also require a significant amount of energy, we extended the previous algorithm and added an additional adaptive sampling layer. Finally, we also proposed a decentralized data reduction approach for cluster-based sensor networks. All the previous algorithms have been tested and validated in terms of energy efficiency using custom-built simulators and through implementation on real sensor devices. The results were promising as we were able to demonstrate that our proposals can significantly improve the lifetime of the network. The last part of this thesis focusses on building data-centric decision-making tools to improve the efficiency of interventions. Since sensor data clustering is an important pre-processing phase and a stepstone towards knowledge extraction, we review recent clustering techniques for massive data management in IoT and compared them using real data for a gas leak detection sensor network. Furthermore, with our hands on a large dataset containing information on 200,000 interventions that happened during a period of 6 years in the region of Doubs, France. We study the possibility of using Machine Learning to predict the number of future interventions and help firefighters better manage their mobile resources according to the frequency of events.
The topic of the thesis is the extraction and segmentation of clothing items from still images using techniques from computer vision, machine learning and image description, in view of suggesting non intrusively to the users similar items from a database of retail products. We firstly propose a dedicated object extractor for dress segmentation by combining local information with a prior learning. A person detector is applied to localize sites in the image that are likely to contain the object. Then, an intra-image two-stage learning process is developed to roughly separate foreground pixels from the background. Finally, the object is finely segmented by employing an active contour algorithm that takes into account the previous segmentation and injects specific knowledge about local curvature in the energy function. We then propose a new framework for extracting general deformable clothing items by using a three stage global-local fitting procedure. A set of template initiates an object extraction process by a global alignment of the model, followed by a local search minimizing a measure of the misfit with respect to the potential boundaries in the neighborhood. The results provided by each template are aggregated, with a global fitting criterion, to obtain the final segmentation. In addition, we introduce a novel dataset called RichPicture, consisting of 1000 images for clothing extraction from fashion images. The methods are validated on the public database and compares favorably to the other methods according to all the performance measures considered.
Ranking data, i.e., ordered list of items, naturally appears in a wide variety of situations, especially when the data comes from human activities (ballots in political elections, survey answers, competition results) or in modern applications of data processing (search engines, recommendation systems). The design of machine-learning algorithms, tailored for these data, is thus crucial. However, due to the absence of any vectorial structure of the space of rankings, and its explosive cardinality when the number of items increases, most of the classical methods from statistics and multivariate analysis cannot be applied in a direct manner. Hence, a vast majority of the literature rely on parametric models. In this thesis, we propose a non-parametric theory and methods for ranking data. Our analysis heavily relies on two main tricks. The first one is the extensive use of the Kendall's tau distance, which decomposes rankings into pairwise comparisons. This enables us to analyze distributions over rankings through their pairwise marginals and through a specific assumption called transitivity, which prevents cycles in the preferences from happening. The second one is the extensive use of embeddings tailored to ranking data, mapping rankings to a vector space. Three different problems, unsupervised and supervised, have been addressed in this context: ranking aggregation, dimensionality reduction and predicting rankings with features. The first part of this thesis focuses on the ranking aggregation problem, where the goal is to summarize a dataset of rankings by a consensus ranking. In this work, we have investigated the hardness of this problem in two ways. Firstly, we proposed a method to upper bound the Kendall's tau distance between any consensus candidate (typically the output of a tractable procedure) and a Kemeny consensus, on any dataset. Then, we have casted the ranking aggregation problem in a rigorous statistical framework, reformulating it in terms of ranking distributions, and assessed the generalization ability of empirical Kemeny consensus. The second part of this thesis is dedicated to machine learning problems which are shown to be closely related to ranking aggregation. The first one is dimensionality reduction for ranking data, for which we propose a mass-transportation approach to approximate any distribution on rankings by a distribution exhibiting a specific type of sparsity. The second one is the problem of predicting rankings with features, for which we investigated several methods. Our first proposal is to adapt piecewise constant methods to this problem, partitioning the feature space into regions and locally assigning as final label (a consensus ranking) to each region. Our second proposal is a structured prediction approach, relying on embedding maps for ranking data enjoying theoretical and computational advantages.
There is no single technique that will allow all relevant behaviour of the speech articulators (lips, tongue, palate...) to be spatially ant temporally acquired. This includes: 2D Ultrasound (US) data to recover the dynamic of the tongue, stereovision data to recover the 3D dynamic of the lips, electromagnetic sensors that provide 3D position of points on the face and the tongue, and 3D Magnetic Resonance Imaging (MRI) that depict the vocal tract for various sustained articulations. We investigate the problems of the temporal synchronization and the spatial registration between all these modalities, and also the extraction of the shape articulators from the data (tongue tracking in US images). Finally, the fused data are evaluated on an existing articulatory model to assess their quality for an application in speech production.
Machine learning proposes numerous algorithms to solve the different tasks that can be extracted from real world prediction problems. To solve the different concerned tasks, most Machine learning algorithms somehow rely on relationships between instances. Pairwise instances relationships can be obtained by computing a distance between the vectorial representations of the instances. Considering the available vectorial representation of the data, none of the commonly used distances is ensured to be representative of the task that aims at being solved. In this work, we investigate the gain of tuning the vectorial representation of the data to the distance to more optimally solve the task. We more particularly focus on an existing graph-based algorithm for classification task. An algorithm to learn a mapping of the data in a representation space which allows an optimal graph-based classification is first introduced. By projecting the data in a representation space in which the predefined distance is representative of the task, we aim at outperforming the initial vectorial representation of the data when solving the task. A theoretical analysis of the introduced algorithm is performed to define the conditions ensuring an optimal classification. A set of empirical experiments allows us to evaluate the gain of the introduced approach and to temper the theoretical analysis.
Digital Scholarly Editions are critically annotated patrimonial literary resources, in a digital form. Such editions roughly take the shape of a transcription of the original resources, augmented with critical information, that is, of structured data. In a collaborative setting, the structure of the data is explicitly defined in a schema, an interpretable document that governs the way editors annotate the original resources and guarantees they follow a common editorial policy. Digital editorial projects classically face two technical problems. The first has to do with the expressiveness of the annotation languages, that prevents from expressing some kinds of information. The second relies in the fact that, historically, schemas of long-running digital edition projects have to evolve during the lifespan of the project. However, amending a schema implies to update the structured data that has been produced, which is done either by hand, by means of ad-hoc scripts, or abandoned by lack of technical skills or human resources. In this work, we define the theoretical ground for an annotation system dedicated to scholarly edition. We define eAG, a stand-off annotation model based on a cyclic graph model, enabling the widest range of annotation. We define a novel schema language, SeAG, that permits to validate eAG documents on-the-fly, while they are being manufactured. We also define an inline markup syntax for eAG, reminiscent of the classic annotation languages like XML, but retaining the expressivity of eAG. Eventually, we propose a bidirectional algebra for eAG documents so that, when a SeAG S is amended, giving S', an eAG I validated by S is semi-automatically translated into an eAG I'validated by S', and so that any modification applied to I (resp. I') is semi-automatically propagated to I'(resp. I) – hence working as an assistance tool for the evolution of SeAG schemas and eAG annotations.
The design of control-command systems often suffers from problems of communication and interpretation of specifications between the various designers, frequently coming from a wide range of technical fields. In order to address the design of these systems, several methods have been proposed in the literature. Among them, the so-called mixed method (bottom-up/top-down), which sees the design realized in two steps. In the first step (bottom-up), a model of the system is defined from a set of standardized components. This model undergoes, in the second (top-down) step, several refinements and transformations to obtain more concrete models (codes, applications, etc.). To guarantee the quality of the systems designed according to this method, we propose two formal verification approaches,based on Model-Checking, in this thesis. The first approach concerns the verification of standardized components and allows the verification of a complete elementary control-command chain. The second one consists in verifying the model of architecture (PandID) used for the generation of control programs. The latter is based on the definition of an architectural style in Alloy for the ANSI/ISA-5.1 standard. To support both approaches, two formal semi-automated verification flows based on Model-Driven Engineering have been proposed. This integration of formal methods in an industrial context is facilitated by the automatic generation of formal models from design models carried out by business designers. Our two approaches have been validated on a concrete industrial case of a fluid management system embedded in a ship.
In this context, user's queries often require the composition of multiple Cloud DaaS services to be answered. Defining the semantics of DaaS services is the first step towards automating their composition. An interesting approach to define the semantics of DaaS services is by describing them as semantic views over a domain ontology. However, defining such semantic views cannot always be done with certainty, especially when the service's returned data are too complex. In this dissertation, we propose a probabilistic approach to model the semantic uncertainty of data services. In our approach, a DaaS service with an uncertain semantics is described by several possible semantic views, each one is associated with a probability. Based on our modeling, we study the problem of interpreting an existing composition involving services with uncertain semantics. We also study the problem of compositing uncertain DaaS services to answer a user query, and propose efficient methods to compute the different possible compositions and their probabilities. We conduct a series of experiments to evaluate the performance of our composition algorithms. The obtained results show the efficiency and the scalability of our proposed solutions
This PhD thesis belongs to the Natural Language Processing (NLP) field, and relates to the automated, semantic analysis of discourse structure. More precisely, we address the issue of thematic analysis, which aims at studying the structure of texts with respect to the organisation of their informational content. This task is of particular importance for Information Retrieval, which constitutes the primary application of our work. The concept of "theme" being particularly complex but scarcely studied for itself in the information retrieval literature, the first part of our dissertation is devoted to a large bibliographical study about the notions of theme, topic, subject, and aboutness, within the linguistics, information science and NLP fields. We draw from this study a definition of the theme as a discursive, semantic and structured object. We propose several models and processes, devoted firstly to the semantic analysis of geographical documents, and secondly to the automatic analysis of temporal discourse frames in the sense of Michel Charolles. We generalise this work introducing the notions of composite topic and semantic axis. The last part is devoted to the LinguaStream platform, an integrated experimentation environment that we designed to ease the elaboration of operational linguistic models, and that lead us to propose some original methodological principles.
The aim of this project is to understand how organizations are able to adapt to their environment and to make it evolve in making use of the wealth of market information. We also show the importance of the objective of the intra-organizational transfer between market analysts and product managers and of the level of control over the receiver on the coordination of the two activities of customer agility.
The proliferation of social networks and all the personal data that people share brings many opportunities for developing exciting new applications. At the same time, however, the availability of vast amounts of personal data raises privacy and security concerns. In this thesis, we develop methods to identify the social networks accounts of a given user. We first study how we can exploit the public profiles users maintain in different social networks to match their accounts. We identify four important properties – Availability, Consistency, non-Impersonability, and Discriminability (ACID) – to evaluate the quality of different profile attributes to match accounts. Exploiting public profiles has a good potential to match accounts because a large number of users have the same names and other personal infor-mation across different social networks. Yet, it remains challenging to achieve practically useful accuracy of matching due to the scale of real social networks. To demonstrate that matching accounts in real social networks is feasible and reliable enough to be used in practice, we focus on designing matching schemes that achieve low error rates even when applied in large-scale networks with hundreds of millions of users. Then, we show that we can still match accounts across social networks even if we only exploit what users post, i.e., their activity on a social networks. This demonstrates that, even if users are privacy conscious and maintain distinct profiles on different social networks, we can still potentially match their accounts. Finally, we show that, by identifying accounts that correspond to the same person inside a social network, we can detect impersonators.
The analysis of the academic literature on the influence of color leads to the proposition of a conceptual framework of the possible impacts of background color on the context of online consumer reviews (chapter 2). The empirical phase of this research is based on three experimental studies. A first study examines the influence of a negative valence background color (red) for a negative online consumer review on viewer evaluations (chapter 3). A second experimental study investigates the role of the perceived valence of color as an influencing mechanism in online consumer reviews (chapter 4). The research for greater ecological validity leads to a third experimental study. Participants are exposed to several reviews simultaneously on a model page of online consumer reviews (chapter 5). This research presents a series of contributions for use by academics as well as practitioners (conclusion).
The focus of this PhD thesis is to design an optimal Energy Management System (EMS) for a Hybrid Electric Vehicle (HEV) following traffic constraints. In the current state of the art, EMS are typically divided between real-time designs relying on local optimization methods, and global optimization that is only suitable for off-line use due to computational constraints. The starting point of the thesis is that in terms of energy consumption, the stochastic aspect of the traffic conditions can be accurately modelled thanks to (speed,acceleration) probability distributions. In order to reduce the data size of the model, we use clustering techniques based on the Wasserstein distance, the corresponding barycenters being computed by either a Sinkhorn or Stochastic Alternate Gradient method. Thanks to this stochastic traffic model, an off-line optimization can be performed to determine the optimal control (electric motor torque) that minimizes the fuel consumption of the HEV over a certain road segment.
In the past years, deep neural networks such as convolutional or recurrent ones have become highly popular for solving various prediction problems, notably in computer vision and natural language processing. Yet, regularizing these networks when the amount of labeled data is scarce is still an open problem. The main popular approaches to regularize deep neural networks are based on early stopping the optimization procedure, model averaging, and data augmentation. The goal of this PhD is to study and develop optimization algorithms, regularization strategies, and architecture designs that are adapted to deep neural networks, with a particular focus on the low-sample-size regime. Spectral norms appear indeed as natural quantities for controlling the model complexity of deep networks. Other stratgies based on regularizing directly with the RKHS norm introduced in will also be investigated. In particular, we expect these architectures to play a central role in the complexity of the network training problem. In a similar vein, recent results related to the Shapley-Folkman theorem show that they also have intrinsically low duality gap in some settings.
Many resources published on the Web of data are related to spatial references that describe their location. These spatial references are a valuable asset for interlinking and visualizing data over the Web. However, these spatial references may be presented with different levels of detail and different geometric modelling from one data source to another. These differences are a major challenge for using geometries comparison as a criterion for interlinking georeferenced resources. This challenge is even amplified more due to the open and often volunteered nature of the data that causes geometric heterogeneities between the resources of a same data source. In this PhD thesis, we propose a vocabulary for formalizing the knowledge about the characteristics of every single geometry in a dataset. We propose a semi-automatic approach for acquiring this knowledge by using geographic reference data. Then, we propose to use this knowledge in approach for adapting dynamically the setting of the comparison of each pair of geometries during an interlinking process. We propose an additional interlinking approach based on geographic reference data for detecting n:m links between data sources. Finally, we propose Web mapping applications for georeferenced resources that remain readable at different map scales
This thesis focuses on using controlled language for Thai software requirements specifications. The study describes the ambiguities and problems encountered in Thai software requirements specifications; both syntactic ambiguity and semantic ambiguity. The study also describes the nature of the Thai language. The model of controlled language for Thai software requirements specifications is composed of three main components: lexical analysis,syntactic analysis, and semantic analysis. For syntactic analysis, a controlled syntax is created using Backus-Naur Form (BNF). In the lexical analysis stage, an XML format lexical resource is built to store words according to their domain. The words received from the XML resource are conceptually correct but may be semantically irrelevant. To solve this issue, the model applies Boolean Matrices to align sentences semantically. As a result, the sentences produced from the model are guaranteed to be syntactically and semantically correct. After having created this model, a program for testing the efficiency of the model is developed. The model is evaluated using four testing methods as follows: 1. functional testing for the correctness of the sentence's syntax, 2.functional testing for the semantic correctness of the sentences produced by the model, 3. acceptance testing in terms of user satisfaction with the program, and 4. acceptance testing in terms of the validity of the outputs. The positive results signify that: 1. the sentences produced by the proposed model are syntactically correct, 2. the sentences produced by the proposed model are semantically correct, 3. the users are satisfied and accept the software created, and 4. the users approve and understand the sentences produced from this model.
In this thesis, we report on our work on developing Natural Language Processing (NLP) algorithms to aid readers and authors of scientific (biomedical) articles in detecting spin (distorted presentation of research results). Our algorithm focuses on spin in abstracts of articles reporting Randomized Controlled Trials (RCTs). We studied the phenomenon of spin from the linguistic point of view to create a description of its textual features. We annotated a set of corpora for the key tasks of our spin detection pipeline: extraction of declared (primary) and reported outcomes, assessment of semantic similarity of pairs of trial outcomes, and extraction of relations between reported outcomes and their statistical significance levels. Besides, we annotated two smaller corpora for identification of statements of similarity of treatments and of within-group comparisons. We developed and tested a number of rule-based and machine learning algorithms for the key tasks of spin detection(outcome extraction,outcome similarity assessment, and outcome-significance relation extraction). The best performance was shown by a deep learning approach that consists in fine-tuning deep pre-trained domain-specific language representations(BioBERT and SciBERT models) for our downstream tasks. This approach was implemented in our spin detection prototype system, called De-Spin, released as open source code. Our prototype includes some other important algorithms, such as text structure analysis (identification of the abstract of an article, identification of sections within the abstract), detection of statements of similarity of treatments and of within-group comparisons, extraction of data from trial registries. Identification of abstract sections is performed with a deep learning approach using the fine-tuned BioBERT model, while other tasks are performed using a rule-based approach. Our prototype system includes a simple annotation and visualization interface
With the widespread use of Artificial Intelligence (AI) systems, understanding the behavior of intelligent agents and robots is crucial to guarantee smooth human-agent collaboration since it is not straightforward for humans to understand the agent's state of mind. Recent studies in the goal-driven Explainable AI (XAI) domain have confirmed that explaining the agent's behavior to humans fosters the latter's understandability of the agent and increases its acceptability. However, providing overwhelming or unnecessary information may also confuse human users and cause misunderstandings. For these reasons, the parsimony of explanations has been outlined as one of the key features facilitating successful human-agent interaction with a parsimonious explanation defined as the simplest explanation that describes the situation adequately. While the parsimony of explanations is receiving growing attention in the literature, most of the works are carried out only conceptually. This thesis proposes, using a rigorous research methodology, a mechanism for parsimonious XAI that strikes a balance between simplicity and adequacy. To provide parsimonious explanations, HAExA relies first on generating normal and contrastive explanations and second on updating and filtering them before communicating them to the human. To evaluate the proposed architecture, we design and conduct empirical human-computer interaction studies employing agent-based simulation. The studies rely on well-established XAI metrics to estimate how understood and satisfactory the explanations provided by HAExA are. The results are properly analyzed and validated using parametric and non-parametric statistical testing.
To accomplish this task, we propose in Chapter 3, three new models for modeling topic and word-topic dependencies between consecutive documents in document streams. The second extension makes use of copulas, which constitute a generic tool to model dependencies between random variables. We rely here on Archimedean copulas, and more precisely on Franck copula, as they are symmetric and associative and are thus appropriate for exchangeable random variables. Our experiments, conducted on five standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones, as dynamic topic models, temporal LDA and the Evolving Hierarchical Processes,both in terms of perplexity and for tracking similar topics in document streams. Compared to previous proposals, our models have extra flexibility and can adapt to situations where there are no dependencies between the documents. On the other hand, the "Exchangeability" assumption in topic models like LDA oftenresults in inferring inconsistent topics for the words of text spans like noun-phrases, which are usually expected to be topically coherent. In Chapter 4, we propose copulaLDA (copLDA), that extends LDA by integrating part of the text structure to the model and relaxes the conditional independence assumption between the word-specific latent topics given the per-document topic distributions. We demonstrate empirically the effectiveness of copLDA on both intrinsic and extrinsic evaluation tasks on several publicly available corpora. To complete the previous model (copLDA), Chapter 5 presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words. The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment. In addition, this model relies on both document and segment specific topic distributions so as to capture fine-grained differences in topic assignments. We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks. Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.
In this thesis, we propose a new deep-learning-based approach for online classification on streams of high-dimensional data. In recent years, Neural Networks (NN) have become the primary building block of state-of-the-art methods in various machine learning problems. Most of these methods, however, are designed to solve the static learning problem, when all data are available at once at training time. Performing Online Deep Learning is exceptionally challenging. The main difficulty is that NN-based classifiers usually rely on the assumption that the sequence of data batches used during training is stationary, or in other words, that the distribution of data classes is the same for all batches (i.i.d. assumption). When this assumption does not hold Neural Networks tend to forget the concepts that are temporarily not available in the stream. In the literature, this phenomenon is known as catastrophic forgetting. The approaches we propose in this thesis aim to guarantee the i.i.d. nature of each batch that comes from the stream and compensates for the lack of historical data. To do this, we train generative models and pseudo-generative models capable of producing synthetic samples from classes that are absent or misrepresented in the stream and complete the stream's batches with these samples. We test our approaches in an incremental learning scenario and a specific type of continuous learning. Our approaches perform classification on dynamic data streams with the accuracy close to the results obtained in the static classification configuration where all data are available for the duration of the learning. Besides, we demonstrate the ability of our methods to adapt to invisible data classes and new instances of already known data categories, while avoiding forgetting the previously acquired knowledge.
Autonomous Vehicles (AV) are emerging systems and considered cornerstones of the future of mobility. Their design is a source of many academic and industrial research efforts. The industrialization of AV is the mean for mobility stakeholders to strengthen their future position. AVs function by interacting with their operational environment and must be fit for their Operational Context (OC). This research work aims to support the architecting activities of Autonomous Vehicles to result in architectures fit for their Operational Context. An OC ontology for AV is proposed to support scenario identification and definition in the early design phase, for a scenario-based design approach. Using this ontology, a method to design AV logical architecture based on the OC is proposed. The consideration of the OC in the architecting activities of AV is strengthened with a second method aiming at assessing the impact of OC change on the AV's architecture during the design phase. The proposed contributions are validated with industrial case studies on the design of AV architectures given the OC and its evolution
Establishing the similarity of time series is at the core of many data mining tasks such as time series classification, time series clustering, time series retrieval, among others. Metrics to establish similarities between time series are specific in the sense that they must be able to take into account the differences in the values making the series as well as distortions along the timelines. The most popular similarity metric is the Dynamic Time Warping (DTW) measure. However, it is costly to compute, and using it against numerous and/or very long time series is difficult in practice. It shows how shapelets that preserve DTW measures can be used in the specific context of large scale time series retrieval. This manuscript is making major contributions: (1) it explains how DTW-preserving shapelets can be used in the specific context of time series retrieval; (2) it proposes some shapelet selection strategies in order to cope with scale, that is, in order to deal with extremely large collection of time series; (3) it details how to handle both univariate and multivariate time series, hence covering the whole spectrum of time series retrieval problems. The core of the contribution presented in this manuscript allows to easily trade-off the complexity of the transformation against the accuracy of the retrieval. Experiments using the UCR and the UEA datasets demonstrate the vast performance improvements compared to state of the art techniques.
Establishing an Environment to Manage Linguistic Resources for a Text Analysis Platform Systems integrating natural language processing often use lexicons and grammars, sometimes indirectly corpora. Because of the quantity and the complexity of the information in these linguistic resources, they are likely to become a source of inconsistency. In this thesis we explore how to improve the management of linguistic resources for an industrial search engine in nineteen languages that performs an elaborate textual analysis. We propose a method to formalize the linguistic architecture of the linguistic processing and its resources. This formalization shows how the knowledge contained in the resources is exploited and gives us the possibility to build management tools compliant with the system's architecture. The environment implemented in this way focuses on updating and acquiring the linguistic resources, while their exploitation is defined by the industrial constraints. Keywords: linguistic architecture, linguistic resource, linguistic resource management, NLP system, NLP tool, natural language processing.
We propose an empirical contribution to recent attempts to unify cognitive science and social science. We focus on Cultural Attraction Theory (CAT), a framework that proposes a common ontology made of representations for cognitive and social science to address interdisciplinary questions. CAT hypothesizes that in spite of important transformations at the micro-level, the overall distribution of representations remains stable due to dynamical attractors. We develop two case studies to show this with short written utterances. The first examines transformations that quotations undergo as they are propagated online. By connecting data mining tools with psycholinguistics, we show that word substitutions in quotations are consistent with the hypothesis of cultural attractors and with known effects of lexical features. The second case study expands these results, and makes use of a purposefully developed web experiment to gather quality transmission chain data sets. By extending a bioinformatics alignment algorithm, we decompose transformations into simpler operations, and propose a first descriptive model which relates psycholinguistic knowledge of sentence transformation to evolutionary trends elicited in the cultural evolution literature. Finally, we show that further understanding the evolution of such representations requires an account of meaning in context, a task for which we flesh out possible empirical approaches.
Learning from data streams is emerging as an important application area. When the environment changes, it is necessary to rely on on-line learning with the capability to adapt to changing conditions a.k.a. concept drifts. Adapting to concept drifts entails forgetting some or all of the old acquired knowledge when the concept changes while accumulating knowledge regarding the supposedly stationary underlying concept. Ensemble methods have been among the most successful approaches. However, the management of the ensemble which ultimately controls how past data is forgotten has not been thoroughly investigated so far. Our work shows the importance of the forgetting strategy by comparing several approaches. The results thus obtained lead us to propose a new ensemble method with an enhanced forgetting strategy to adapt to concept drifts. Experimental comparisons show that our method compares favorably with the well-known state-of-the-art systems. In our work, we go one step further by introducing a meta-learning mechanism that is able to detect relevant states of the environment, to recognize recurring contexts and to anticipate likely concepts changes. Hence, the method we suggest, deals with both the challenge of optimizing the stability-plasticity dilemma and with the anticipation and recognition of incoming concepts. This is accomplished through an ensemble method that controls a ensemble of incremental learners.
This thesis studies the integration of phonetic landmarks into standard statistical large vocabulary continuous speech recognition (LVCSR). Landmarks are discrete time instances that indicate the presence of phonetic events in the speech signal. The goal is to develop landmark detectors that are motivated by phonetic knowledge in order to model selected phonetic classes more precisely than it is possible with standard acoustic models. The thesis presents two landmark detection approaches, which make use of segment-based information and studies two different methods to integrate landmarks into the decoding, which are landmark-based pruning and a weighted combination approach. While both approaches improve speech recognition performance compared to the baseline using weighted combination of landmarks and acoustic scores during decoding, they do not outperform standard frame-based phonetic predictions. Since these results indicate that landmark-driven LVCSR requires the integration of very heterogeneous information, the thesis presents a third integration framework that is designed to integrate an arbitrary number of heterogeneous and asynchronous landmark streams into LVCSR. The results indicate that this framework is indeed ale to improve the baseline system, as soon as landmarks provide complementary information to the regular acoustic models.
The manufacturers and the operators of the fleets of cyber-physical systems (CPSs) are subjected to huge expectations expressed in terms of the availability and reliability of the provided products and services during the exploitation of these fleets in dynamic environments. These expectations foster the fleet manufacturers, particularly in the transportation sector, to develop effective mechanisms as far as the reactive planning of the maintenance operations at the fleet level is concerned. In this research work, a multi-agent system (MAS) for the reactive maintenance planning of a fleet of CPSs is proposed. The proposed MAS is conceived by using the ANEMONA design methodology and it aims at optimizing the fleet maintenance planning decisions to meet the specified objectives. The experiments carried out in the course of this work demonstrate the ability of the proposed MAS in planning the fleet maintenance effectively (i.e. satisfying the fleet's availability and reliability requirements in a static environment) and reactively (i.e. being able to adapt/modify the fleet maintenance planning decisions following perturbations). The effectiveness of the MAS model is validated by a mathematical programming model and its reactivity is tested by using simulated perturbations. An application in rail transport industry to the fleet of trains at Bombardier Transportation France is proposed. The proposed MAS is integrated in a decision support system called "MainFleet". The development of Main-Fleet at Bombardier is ongoing.
This work concerns the joint segmentation of a set images in a Bayesian framework. The proposed model combines the hierarchical Dirichlet process (HDP) and the Potts random field. Hence, for a set of images, each is divided into homogeneous regions and similar regions between images are grouped into classes. On the one hand, thanks to the HDP, it is not necessary to define a priori the number of regions per image and the number of classes, common or not. On the other hand, the Potts field ensures a spatial consistency. The arising a priori and a posteriori distributions are complex and makes it impossible to compute analytically estimators. A Gibbs algorithm is then proposed to generate samples of the distribution a posteriori. Moreover,a generalized Swendsen-Wang algorithm is developed for a better exploration of the a posteriori distribution. Finally, a sequential Monte Carlo sampler is defined for the estimation of the hyperparameters of the model. The choice of the best partition is done by minimization of a numbering free criterion. These methods have been evaluated on toy examples and natural images. The performance are assessed by metrics well-known in statistics but unused in image segmentation.
How words are recognized? How do we process word meaning? These questions have been pursued in lexical access and word recognition studies in the last half century of research in psycho-, neuro-, and linguistics. Morphological processing is an essential level of processing for information extraction during word recognition. In one extreme, full-entry models propose whole word storage in memory and post-lexical morphological processing based on paradigms; in the other extreme, decompositional models posit pre-lexical decomposition and morphemic activation based on rules; between then, dual-mechanism models consider two routes for word recognition, a whole-word associative route and a combinatorial rule-based route. In the present thesis, it was investigated the morphological processing of French inflected verbs in visual modality in five studies. Study 1 researched the mental lexicon organization in function of surface and cumulative frequencies; Study 2 explored different stem formation processes; Study 3 investigated morphological operations in the inflectional suffixes; Study 4 tested the verbal morphological processing in L2 French speakers; and Study 5 tested verbal violations coupled with electroencephalography acquisition. The results suggest that all inflected French verbs are processed by a single-mechanism model with pre-lexical morphological decomposition for lexical activation and word recognition. It is proposed different processing for the lexical and functional morphemes. Words are decomposed in atomic morphemes, morphemic representations are activated in the mental lexicon, and word constituents are recombined for word verification
Inline quality control of the product is an important objective for industries growth. Controlling a product quality requires measurements of its quality characteristics. One hundred percent control is an important objective to overcome the limits of the control by sampling, in the case of defects related to exceptional causes. However, industrial constraints have limited the deployment of measurement of product characteristics directly within production lines. Human visual control is limited by its duration incompatible with the production cycle at high speed productions, by its cost and its variability. Computer vision systems present a cost that reserves them for productions with high added value. In addition, the automatic control of the quality of the appearance of the products remains an open research topic. Our work aims to meet these constraints, as part of the injection-molding process of thermoplastics. We propose a control system that is non-invasive for the production process. Parts are checked right out of the injection molding machine. We will study the contribution of non-conventional imaging. Thermography of a hot molded part provides information on its geometry, which is complementary to conventional imaging. Specifications include complex geometric features, as well as appearance features, which are difficult to formalize. However, the appearance characteristics are difficult to formalize. To automate aspect control, it is necessary to model the notion of quality of a part. In order to exploit the measurements made on the hot parts, our approach uses statistical learning methods. Thus, the human expert who knows the notion of quality of a piece transmits his knowledge to the system, by the annotation of a set of learning data. Our control system then learns a metric of the quality of a part, from raw data from sensors. We favor a deep convolutional network approach (Deep Learning) in order to obtain the best performances in fairness of discrimination of the compliant parts. The small amount of annotated samples available in our industrial context has led us to use domain transfer learning methods. Finally, in order to meet all the constraints and validate our propositions, we realized the vertical integration of a prototype of device of measure of the parts and the software solution of treatment by statistical learning. The device integrates thermal imaging, polarimetric imaging, lighting and the on-board processing system necessary for sending data to a remote analysis server. Two application cases make it possible to evaluate the performance and viability of the proposed solution
This thesis comprises three separate but interconnected essays that focus on the determinants and economic implications of corporate narrative disclosure. Our study provides evidence that difficult-to-read annual reports can act as a non-trivial impediment to investors' ability to process information into useful trading signals. The findings are robust to a battery of sensitivity tests, including endogeneity, use of alternative regression techniques, and use of alternative liquidity and readability proxies. Using a large panel of U.S. public firms, the second essay presents the first evidence highlighting the relation between annual report readability and cost of equity capital. We hypothesize that complex textual reporting deters investors' ability to process and interpret annual reports, leading to higher information risk, and thus higher cost of equity financing. Consistent with our prediction, we find that greater textual complexity is associated with higher cost of equity capital. The third essay investigates the effect of firms' tax avoidance practices on the textual properties of their annual filings. Using a large sample of U.S.-listed firms, we document a positive and statistically significant relation between corporate tax avoidance and annual report textual complexity. The findings are also robust to a number of checks, including, using additional control variables, employing alternative regression methodologies, and addressing endogeneity concerns.
Based on a corpus of five hundred research articles and counting more than six million words, the objective is twofold. First, it is necessary to understand the mechanisms and processes by which HSS build their terminological apparatus and then to identify the combinatorial profiles of terms as well as their contextual interactive functioning. The approach being part of corpus linguistics and natural language processing (NLP), the approach is therefore semasiological, combining socioterminology and lexicometry. The analysis of the behaviour of these items in context allows us to identify semantic shifts and their relationship. We assume that the term, although often presented as monosemic and stable in its biunivocal relationship with the concept it denominates, seems to be subject to denominational variations and is, therefore, indicative of enunciation strategies that manifest themselves in its lexicogenesis.
The availability of very large electronic text corpora makes it possible today to design methods that build word representations by an automatic, exhaustive exploration of their uses in these texts. These representations are at the heart of a number of natural language processing applications, from information extraction to machine translation through question-answering. In specialized domains however, two factors hinder the applicability of these methods. On the one hand, domain-specific corpora are necessarily smaller than corpora with no domain restriction, whereas the size of the input corpora is a key factor in the quality of the created representations. On the other hand, complex terms (made of multiple words) are of particular importance in specialized domains, whereas standard methods are designed to handle simple terms (made of one word). The ADDICTE project aims to overcome these difficulties by providing a better representation of complex terms, by supplementing the analysis of domain-specific texts with additional resources (domain terminologies and out-of-domain texts), and by designing representations that can take advantage of such additional information. In this context, the proposed thesis specifically addresses the design and test of methods that aim to take advantage of external resources to build distributional word representations.
This thesis falls within the framework of Natural Language Processing. The problems of automatic summarization of Arabic documents which was approached, in this thesis, are based on two points. The first point relates to the criteria used to determine the essential content to extract. The second point focuses on the means to express the essential content extracted in the form of a text targeting the user potential needs. In order to show the feasibility of our approach, we developed the "L.A.E" system, based on a hybrid approach which combines a symbolic analysis with a numerical processing. The evaluation results are encouraging and prove the performance of the proposed hybrid approach. These results showed, initially, the applicability of the approach in the context of mono documents without restriction as for their topics (Education, Sport, Science, Politics, Interaction, etc), their content and their volume. They also showed the importance of the machine learning in the phase of classification and selection of the sentences forming the final extract.
This thesis is part of the problematics of the extraction of meaning from texts and textual flows, produced in our case during collaborative processes. More specifically, we are interested in work-related emails and collaborative textual documents, with a first application to educational documents. The motivation for this interest is to help users gain access to useful information more quickly; we hence seek to locate them in the texts. Thus, we are interested in the tasks referred to in the emails, and to the fragments of educational documents which concern the themes of their interests. Two corpora, one of e-mails and one of educational documents, mainly in French, have been created. This was essential because there is virtually no previous work on this type of data in French. Our first theoretical contribution is a generic modeling of the structure of these data. We use it to specify the formal processing of documents, a prerequisite for semantic processing. We demonstrate the difficulty of the problem of segmentation, standardization and structuring of documents in different source formats, and present the SEGNORM tool, the first software contribution of this thesis. SEGNORM segments and normalizes documents (in plain or tagged text), recursively and in units of configurable size. In the case of emails, it segments the messages containing quotations of messages into individual messages, thereby keeping the information about the chaining between the intertwined fragments. It also analyzes the metadata of the messages to reconstruct the threads of discussions, and retrieves in the quotations the messages of which one does not have the source file. We then discuss the semantic processing of these documents. We propose an (ontological) modeling of the notion of task, then describe the annotation of a corpus of several hundred messages originating from the professional context of VISEO and GETALP. We then present the second software contribution of this thesis: the tool for locating tasks and extracting their attributes (temporal constraints, assignees, etc.). This tool, based on a combination of an expert approach and machine learning, is evaluated according to classic criteria of accuracy, recall and F-measure, as well as according to the quality of use. Finally, we present our work on the MACAU-CHAMILO platform, third software contribution, which helps learning by (1) structuring of educational documents according to two ontologies (form and content), (2) multilingual access to content initially monolingual. This is therefore again about structuring along the two axes, form and meaning. (1) The ontology of forms makes it possible to annotate the fragments of documents by concepts such as theorem, proof, example, by levels of difficulty and abstraction, and by relations such as elaboration_of, illustration_of… The domain ontology models the formal objects of informatics, and more precisely the notions of computational complexity. This makes it possible to suggest to the users fragments useful for understanding notions of informatics perceived as abstract or difficult. (2) The aspect related to multilingual access has been motivated by the observation that our universities welcome a large number of foreign students, who often have difficulty understanding our courses because of the language barrier. We proposed an approach to multilingualize educational content with the help of foreign students, by online post-editing of automatic pre-translations, and, if necessary, incremental improvement of these post-editions. (Our experiments have shown that multilingual versions of documents can be produced quickly and without cost.) This work resulted in a corpus of more than 500 standard pages (250 words/page) of post-edited educational content into Chinese.
This thesis presents a computer device aimed at helping future FFL teacher training in Colombian universities. It is grounded in text linguistics and aims to contribute to improving the linguistic level of university students currently in training. To do so, this device is based on a textual corpus specifically annotated and labeled thanks to natural language processing (NLP) tools and to manual annotations in XML format. This should allow the development of activities with a formative aim, while also taking into account the needs expressed by the target public (teachers/trainers and their students, the trainees). As explained throughout this thesis, the elaboration of such a system is based on knowledge and skills stemming from several disciplines and/or fields: language didactics, educational engineering, general linguistics, textual linguistics, corpus linguistics, NLP and CALL. The ambition is to provide trainees and trainers in higher education in Colombia with a tool designed according to their needs and their learning aims and objectives. Finally, the originality of this system consists in the choice of target users, the didactic training model implemented and the specificity of the corpus annotated for the activities. It is one of the first CALL systems based on textual linguistics specifically targeted at training future FFL teachers in a non-native language context.
The increasing mass of User-Generated Content (UGC) on the Internet means that people are now willing to comment, edit or share their opinions on different topics. This content is now the main ressource for sentiment analysis on the Internet. Due to abbreviations, noise, spelling errors and all other problems with UGC, traditional Natural Language Processing (NLP) tools, including Named Entity Recognizers and part-of-speech (POS) taggers, perform poorly when compared to their usual results on canonical text (Ritter et al., 2011). This thesis deals with Named Entity Recognition (NER) on some User-Generated Content (UGC). We have created an evaluation dataset including multi-domain and multi-sources texts. We then developed a Conditional Random Fields (CRFs) model trained on User-Generated Content (UGC). In order to improve NER results in this context, we first developed a POS tagger on UGC and used the predicted POS tags as a feature in the CRFs model. To turn UGC into canonical text, we also developed a normalization model using neural networks to propose a correct form for Non-Standard Words (NSW) in the UGC.
Automatically extract information from text documents has become a major challenge during these last years. One of the main proxy task that has been proposed has a way to evaluate a reading model is question-answering. A model is expected to produce an answer regarding a question and an associated text document. For the moment, models that have been proposed require lots of training data and tend to suffer from a lack of robustness against noise in the dataset. The objective of this thesis is to investigate how adversarial learning can improve the performance of reading models.
Different components of statistical machine translation systems are considered as optimization problems. Indeed, the learning of the translation model, the decoding and the optimization of the weights of the log-linear function are three important optimization problems. Knowing how to define the right algorithms to solve them is one of the most important tasks in order to build an efficient translation system. Several optimization algorithms are proposed to deal with decoder optimization problems. They are combined to solve, on the one hand, the decoding problem that produces a translation in the target language for each source sentence, on the other hand, to solve the problem of optimizing the weights of the combined scores in the log-linear function to fix the translation evaluation function during the decoding. The reference system in statistical translation is based on a beam-search algorithm for the decoding, and a line search algorithm for optimizing the weights associated to the scores. We propose a new statistical translation system with a decoder entirely based on genetic algorithms. Genetic algorithms are bio-inspired optimization algorithms that simulate the natural process of evolution of species. They allow to handle a set of solutions through several iterations to converge towards optimal solutions. This work allows us to study the efficiency of the genetic algorithms for machine translation. This work allowed us to propose a new machine translation system with a decoder entirely based on genetic algorithms
Information security relies on the correct interaction of several abstraction layers: hardware, operating systems, algorithms, and networks. However, protecting each component of the technological stack has a cost; for this reason, many devices are left unprotected or under-protected. This thesis addresses several of these aspects, from a security and cryptography viewpoint. To that effect we introduce new cryptographic algorithms (such as extensions of the Naccache–Stern encryption scheme), new protocols (including a distributed zero-knowledge identification protocol), improved algorithms (including a new error-correcting code, and an efficient integer multiplication algorithm), as well as several contributions relevant to information security and network intrusion. Furthermore, several of these contributions address the performance of existing and newly-introduced constructions.
It consists of identifying some textual objects such as person, location and organization names. The work of this thesis focuses on the named entity recognition task for the oral modality. In the first part, we study the characteristics of the named entity recognition downstream of the automatic speech recognition system. We present a methodology which allows named entity recognition following a hierarchical and compositional taxonomy. We measure the impact of the different phenomena specific to speech on the quality of named entity recognition. In the second part, we propose to study the tight pairing between the speech recognition task and the named entity recognition task. For that purpose, we take away the basic functionnalities of a speech recognition system to turn it into a named entity recognition system. Therefore, by mobilising the inherent knowledge of the speech processing to the named entity recognition task, we ensure a better synergy between the two tasks. We carry out different types of experiments to optimize and evaluate our approach.
In this thesis we used MRI (Magnetic Resonance Imaging) data of the vocal tract to study speech production. The first part consist of the study of the impact that the velum, the epiglottis and the head position has on the phonation of five french vowels. Acoustic simulations were used to compare the formants of the studied cases with the reference in order to measure their impact. Therefore the second part presents some algorithms that one can use in order to enhance speech production data. Several image transformations were combined in order to generate estimations of vocal tract shapes which are more informative than the original ones. At this point, we envisaged apart from enhancing speech production data, to create a generic speaker model that could provide enhanced information not for a specific subject, but globally for speech. Finally, the last part of the thesis, refers to a selection of open questions of the field that are still left unanswered, some interesting directions that one can expand this thesis and some potential approaches that could help someone move forward towards these directions.
This thesis aims to extract a sample of texts used as a training population for a markov process. We adopted a stratification sampling. We set up a software called "multivariate sampling" which extracts a sample from a stratified and ambiguous corpus minimizing "the loss of information" in a certain sense, the results obtained are very satisfying since we improved the number of parts of speech tagged correctly. This probability is not unique whatever topologies used. The li isometries show us that it is impossible to obtain a unique solution to this problem. A unique solution constraints the "true" and "false" representation to be the same. It appeared that one has to distinguish the "true" associated to a logical formula from "the certain event" known in the probability theory finally, we proposed a new markov model capable to take into account he context associated to a pos.
This thesis presents the implementation of a french grammar using a formalism called tree adjoining grammar (tag). This formalism belongs to the family of unification based formalisms which have proved fruitful for natural language processing. We have added some constraints to the original formalism, especially a "lexicalization" constraint, which captures the philosophy of M. Gross 'lexicon-grammars. We have defined the elementary structures needed for describing a large range of syntactic french phenomena such as: subcategorization, agreement, complement clauses and unbounded dependencies, support verb constructions and idioms, different kind of relative clauses and cleft extractions, passive and extraposition. We have defined the necessary features associated to these structures. We present for each phenomenon a detailed comparison with other formalisms such as gpsg, lfg and string grammars. We then present the parsing strategy suitable for "lexicalized" grammars and the current state of implementation of our french parser.
The majority of recent embedded systems are based on massively parallel MPSoC architectures, hence the necessity of developing embedded parallel applications. Embedded parallel application design becomes more challenging: It becomes a parallel programming for non-trivial heterogeneous multiprocessors with diverse communication architectures and design constraints such as hardware cost, power, and timeliness. This is especially critical for embedded systems powered by MPSoC, where ever demanding applications have to run smoothly on numerous cores, each with modest power budget. Moreover, application performance does not necessarily improve as more cores are added. Application performance can be limited due to multiple bottlenecks including contention for shared resources such as caches and memory. It becomes time consuming for a developer to pinpoint in the source code the bottlenecks decreasing the performance. To overcome these issues, in this thesis, we propose a fully three automatic methods which detect the instructions of the code which lead to a lack of performance due to contention and scalability of processors on a chip. The methods are based on data mining techniques exploiting gigabytes of low level execution traces produced by MPSoC platforms. Our profiling approaches allow to quantify and pinpoint, automatically the bottlenecks in source code in order to aid the developers to optimize its embedded parallel application. We performed several experiments on several parallel application benchmarks. Our experiments show the accuracy of the proposed techniques, by quantifying and pinpointing the hotspot in the source code.
In this dissertation, the thesis that deep neural networks are suited for analysis of visual, textual and fused visual and textual content is discussed. Recurrent neural networks for spoken language understanding (slot filling): different architectures are compared for this task with the aim of modeling both the input context and output label dependencies.2) Action prediction from single images: we propose an architecture that allow us to predict human actions from a single image. The architecture was extensively studied an evaluated in international benchmarks within the task of video hyperlinking where it defined the state of the art today.4)
Social networks and online communities have become major players in the field of tourism. The information exchanged on these communities is increasingly influencing consumer behavior. At the same time, the number of mobile phone users is also growing. Mobile offers their users different services. Social networks associated with mobile technology can thus be a source of information for the tourist and thus be considered as a key factor that influences his behavior during his trip. Our purpose would be to study the behavior of the tourist in the context of the use of a mobile application that is based on the sharing of opinions and feedback from other travelers.
The present research examined the problems in adapting Thai toponyms or place names in a corpus of four French guidebooks on Thailand. The linguistic and translation analysis showed that Thai toponyms were well integrated in French at different levels. Firstly, they were romanised by various systems including using French graphemes. In the referential semantic perspective, their fundamental value is locative, but in some contexts they could be metonymically amd metaphorically interpreted. Moreover, despite the problem of meaning and non-translation of the proper names, the semantic transfer of Thai toponyms into French was possible by using various translation procedures. The author could modify the literal translation of Thai toponyms or create a new one to better present the dominant characterisation of the place with free translation technique. These strategies revealed the pragmatic nature of the guidebooks which is to make the reader discover or know the unknown place and arouse the reader's interest.
This research aims to understand how the visual electronic word of mouth emitted by a youtubeur influences the affect and the purchase intention of the consumers. We study the impact of word of mouth characteristics and the effect of social contagion. The main results are:1. The professional status of the youtubeur, the existence of commercial links between brands and youtubers and the valence of the magazine have an impact on the feeling expressed and therefore the intention to purchase.2. There is a social contagion, both emotional and behavioral within the audience of electronic and visual word of mouth that impacts its effectiveness.3. The number of subscribers of an individual (individual characteristic of the commentator) impacts his sensitivity to social contagion.
The explosion of data on the Internet leads to challenges in working with them. Semantic Web and ontology are required to address those problems. Nowadays, ontology plays more and more an important role as a means in domain knowledge representation. It requires access to a more concrete level of description where each NPI can be evaluated by science, monitored by professionals and explained to the patient. To do this, an international and evolutionary classification based on the results of science is necessary. Constructing this ontology manually is time consuming and thus an expensive process. Particularly, the step of collecting the NPI terminology requires much more time than the rest, because of heterogeneous and big resources in the context of NPIs. An automatic or semi-automatic method is thus essential to support NPI experts in this task. Therefore, a simple and friendly visualization of the ontology for NPI experts needs to be considered. The first contribution concerns the semi-automatic process for collecting NPI terms. Two approaches, knowledge-based and corpus-based, are presented to retrieve candidate NPI terms. A new similarity measure for NPI is proposed and evaluated. The second contribution is a new method for ontology visualization based on MindMap. A web-based tool is then implemented to convert OWL ontologies to FreeMind documents which can be imported by existing Mind-Mapping applications to make visualizations.
Considering the industrial context, we focused our research on some issues, in informatics and lexicography. We also show how to extend it to meet new needs such as those of the INNOVALANGUES project. Finally, we have created a "lemmatisation middleware", LEXTOH, which allows calling several morphological analyzers or lemmatizers and then to merge and filter their results. Combined with a new dictionary creation tool, CREATDICO, LEXTOH allows to build on the fly a "mini-dictionary" corresponding to a sentence or a paragraph of a text being "post-edited" online under IMAG/SECTRA, which performs the lexical proactive support functionality foreseen in [Huynh, C.-P., 2010].
With the impressive progress that has been made in transcribing spoken language, it is becoming increasingly possible to exploit transcribed data for tasks that require comprehension of what is said in a conversation. The work in this dissertation, carried out in the context of a project devoted to the development of a meeting assistant, contributes to ongoing efforts to teach machines to understand multi-party meeting speech. We have focused on the challenge of automatically generating abstractive meeting summaries. We first present our results on Abstractive Meeting Summarization (AMS), which aims to take a meeting transcription as input and produce an abstractive summary as output. We introduce a fully unsupervised framework for this task based on multi-sentence compression and budgeted submodular maximization. We also leverage recent advances in word embeddings and graph degeneracy applied to NLP, to take exterior semantic knowledge into account and to design custom diversity and informativeness measures. Next, we discuss our work on Dialogue Act Classification (DAC), whose goal is to assign each utterance in a discourse a label that represents its communicative intention. DAC yields annotations that are useful for a wide variety of tasks, including AMS. We propose a modified neural Conditional Random Field (CRF) layer that takes into account not only the sequence of utterances in a discourse, but also speaker information and in particular, whether there has been a change of speaker from one utterance to the next. We provide a novel approach to ACD in which we first introduce a neural contextual utterance encoder featuring three types of self-attention mechanisms and then train it using the siamese and triplet energy-based meta-architectures. We further propose a general sampling scheme that enables the triplet architecture to capture subtle patterns (e.g., overlapping and nested clusters).
The object of this thesis is to study causality through its discursive expression in French texts. This linguistic study has been made in the perspective of automatic language processing. This work takes place within a project of semantic filtering of texts (named SAFIR: automatic information filtering for summarizing texts) which is dedicated to the production of syntheses and summaries. But the present work ranges over knowledge acquisition through texts. Our first objective is to index the various linguistic processes that are used by authors to convey causal relations. We use an original contextual exploration method that is not based upon a "deep representation" of the text under consideration, rather upon an automatic identification of markers that are considered as relevant. We propose a map made of 1500 markers (verbs, phrases, adverbs,...)
We adopt the formalism of the classes of objects to describe metonymy in French. The objective of this description is twofold: to provide a description of the regularities in the way this mechanism works and to constitute lexical bases dedicated to the automatic treatment of languages. It is postulated that the minimal frame of analysis of the lexical items is the simple sentence defined in terms of predicates and arguments. According to this principle, the metonymy is analyzed as a semantic mechanism indicated by a transfer of predicates between classes of words correlated to appropriate predicates. When it concerns elementary nouns, the transfer operates a semantic re-categorization and creates new uses. The categorization of the words into arguments and predicates allows us to propose a tripartite classification of metonymy. The metonymy of the meronymic type and those who are of the argumental type, put in relation elementary nouns, whereas the metonymy of the predicative type operates a structural re-categorization. In this last instance, we distinguish at first a double actualization (predicative and argumental) of polysemous lexical items, before witnessing a transfer of predicates which corresponds to the structural re-categorization. Transfer and actualization are both organizing principles which bring to light the systematic character of metonymy.
The main aim of this thesis is to investigate the automatic quality assessment of spoken language translation (SLT), called Confidence Estimation (CE) for SLT. Due to several factors, SLT output having unsatisfactory quality might cause various issues for the target users. Therefore, it is useful to know how we are confident in the tokens of the hypothesis. Our first contribution of this thesis is a toolkit LIG-WCE which is a customizable, flexible framework and portable platform for Word-level Confidence Estimation (WCE) of SLT. WCE for SLT is a relatively new task defined and formalized as a sequence labelling problem where each word in the SLT hypothesis is tagged as good or bad accordingto a large feature set. We propose several word confidence estimators (WCE) based on our automatic evaluation of transcription (ASR) quality, translation (MT) quality,or both (combined/joint ASR+MT). This research work is possible because we built a specific corpus, which contains 6.7k utterances for which a quintuplet containing: ASRoutput, verbatim transcript, text translation, speech translation and post-edition of the translation is built. The conclusion of our multiple experiments using joint ASR and MT features for WCE is that MT features remain the most influent while ASR features can bring interesting complementary information. As another contribution, we propose two methods to disentangle ASR errors and MT errors, where each word in the SLT hypothesis is tagged as good, asr_error or mt_error. We thus explore the contributions of WCE for SLT in finding out the source of SLT errors. Furthermore, we propose a simple extension of WER metric in order to penalize differently substitution errors according to their context using word embeddings. Our experiments show that the correlation of the new proposed metric with SLT performance is better than the one of WER. To conclude, we have proposed several prominent strategies for CE of SLT that could have a positive impact on several applications for SLT. Robust quality estimators for SLT can be used for re-scoring speech translation graphs or for providing feedback to the user in interactive speech translation or computer-assisted speech-to-text scenarios. Keywords: Quality estimation, Word confidence estimation (WCE), Spoken Language Translation (SLT), Joint Features, Feature Selection.
The present publications cover a relatively vast area of applied linguistics from teaching foreign languages and german as a foreign language to machine translation ; they deal with questions of syntax for computational linguistics, problems in automatic indexation etc. It does not only include a treatment of a variety of issues but also a formalization of rules. A series of articles in Brazilian Portuguese written during a teaching assignment in Brazil, deal with the question of automatic text analysis of portuguese. These articles contain various aspects: a detailed formalization of morphology and syntax, application problems in automatic indexation, information systems for parliament and machine translation. Since 1985 emphasis has been on machine translation. This includes strategic considerations for determining the kind of skills needed in the development of such a system ; also, certain specific problems have been analyzed and the systems have been tested. This, in tur, has led to conclusions concerning the software part of the system.
Events are central in many Natural Language Processing tasks, despite the lack of a unified definition for the concept. Creating these templates is an expert task and therefore costly, painstaking and hard to extend to new domains. Meanwhile, the amount of data produced by individuals and organizations has grown exponentially, opening unprecedented perspectives of applications. To this end, we propose a bottom-up approach composed of three main steps. The first step clusters several textual mentions of a same particular event (i.e tied to a time and place) to identify distinct instances. The second step groups these instances together based on more abstract features to infer event types. Finally, the third and last step extracts the most salient elements of each type to produce the synthetic, template-like structure we are looking for.
The WebLab platform is an application used to define and execute media-mining workflows. A designer can create complex media-mining workflows using components, whose operation is not always known (black-boxes services). These complex workflows can lead to a problem of data quality, however, and before this work, no tool existed to analyse and improve the quality of WebLab workflows. To deal with black-box services, we choose to tackle this quality problem with a non-intrusive approach: we enhance the definition of the WebLab workflow with provenance and quality propagation rules. Provenance rules generate fine-grained data dependency links between data and services after the execution of a WebLab workflow. Then the quality propagation rules use these links to reason on the influence that the quality of the data used by a component has on the quality of the output data…
In this thesis we propose several methods for unsupervised person identification in TV broadcast using the names written on the screen. As the use of biometric models to recognize people in large video collections is not a viable option without a priori knowledge of people present in this videos, several methods of the state-of-the-art proposes to use other sources of information to get the names of those present. These methods mainly use the names pronounced as source of names. However, we can not have a good confidence in this source due to transcription or detection names errors and also due to the difficulty of knowing to who refers a pronounced name. The names written on the screen in TV broadcast have not be used in the past due to the difficulty of extracting these names in low quality videos. However, recent years have seen improvements in the video quality and overlay text integration. We therefore re-evaluated in this thesis, the use of this source of names. We first developed LOOV (for LIG Overlaid OCR in Video), this tool extract overlaid texts written in video. With this tool we obtained a very low character error rate. This allows us to have an important confidence in this source of names. We then compared the written names and pronounced names in their ability to provide the names of person present in TV broadcast. We found that twice persons are nameable by written names than by pronounced names with an automatic extraction of them. Another important point to note is that the association between a name and a person is inherently easier for written names than for pronounced names. With this excellent source of names we were able to develop several unsupervised naming methods of people in TV broadcast. We started with late naming methods where names are propagated onto speaker clusters. These methods question differently the choices made during the diarization process. We then proposed two methods (integrated naming and early naming) that incorporate more information from written names during the diarization process. To identify people appear on screen, we adapted the early naming method for faces clusters. Finally, we have also shown that this method also works for multi-modal speakers-faces clusters. With the latter method, that named speech turn and face during a single process, we obtain comparable score to the best systems that contribute during the first evaluation REPERE
Metabolomics allows large-scale studies of the metabolic profile of an individual, which is representative of its physiological state. Metabolic markers characterising a given condition can be obtained through the comparison of those profiles. Therefore, metabolomics reveals a great potential for the diagnosis as well as the comprehension of mechanisms behind metabolic dysregulations, and to a certain extent the identification of therapeutic targets. However, in order to raise new hypotheses, those applications need to put metabolomics results in the light of global metabolism knowledge. This contextualisation of the results can rely on metabolic networks, which gather all biochemical transformations that can be performed by an organism. The major bottleneck preventing this interpretation stems from the fact that, currently, no single metabolomic approach allows monitoring all metabolites, thus leading to a partial representation of the metabolome. This thesis proposes a new approach to overcome those limitations, through the suggestion of relevant metabolites, which could fill the gaps in a metabolomics signature. This method is inspired by recommender systems used for several on-line activities, and more specifically the recommendation of users to follow on social networks. This approach has been used for the interpretation of the metabolic signature of the hepatic encephalopathy. It allows highlighting some relevant metabolites, closely related to the disease according to the literature, and led to a better comprehension of the impaired mechanisms and as a result the proposition of new hypothetical scenario.
Ln th is Natural Language Processing Ph. D. Thesis, we aim to perform semantic role labeling on French domain-specific texts. This task first disambiguates the sense of predicates in a given text and annotates its child chunks with semantic roles such as Agent, Patient or Destination. The task helps many applications in domains where annotated corpora exist, but is difficult to use otherwise. We first evaluate on the FrameNet corpus an existing method based on VerbNet, which explains why the method is domain-independant. To apply this method to French, we first translate lexical resources. We first translate the WordNet lexical database. Next, we translate the VerbNet lexicon which is organized semantically using syntactic information. We obtains its translation, VerbuNet, by reusing two French verb lexicons (the Lexique-Grammaire and Les Verbes Français) and by manually modifying and reorganizing the resulting lexicon. Finally, once those building blocks are in place, we evaluate the feasibilty of semantic role labeling of French and English in three specific domains. We study the pros and cons of using VerbNet and VerbnNet to annotate those domains before explaining our future work.
With the dramatic growth of digital information, finding precise answers to natural language questions is more and more essential for retrieving domain knowledge in real time. Many research works tackled answer retrieval for factual questions in open domain. Less works were performed for domain-specific question answering such as the medical domain. Compared to the open domain, several different conditions are met in the medical domain such as specialized vocabularies, specific types of questions, different kinds of domain entities and relations. Document characteristics are also a matter of importance, as, for example, clinical texts may tend to use a lot of technical abbreviations while forum pages may use long “approximate” terms. A key process for this task is to analyze the questions and the source documents semantically and to use standard formalisms to represent the obtained annotations. We propose a medical question-answering approach based on: (i) NLP methods combing domain knowledge, rule-based methods and statistical ones to extract relevant information from questions and documents and (ii) Semantic Web technologies to represent and interrogate the extracted information.
In the same way as TV channels, data streams are represented as a sequence of successive events that can exhibit chronological relations (e.g. a series of programs, scenes, etc.). For a targeted channel, broadcast programming follows the rules defined by the channel itself, but can also be affected by the programming of competing ones. In such conditions, event sequences of parallel streams could provide additional knowledge about the events of a particular stream. In the sphere of machine learning, various methods that are suited for processing sequential data have been proposed. Long Short-Term Memory (LSTM) Recurrent Neural Networks have proven its worth in many applications dealing with this type of data. Nevertheless, these approaches are designed to handle only a single input sequence at a time. The main contribution of this thesis is about developing approaches that jointly process sequential data derived from multiple parallel streams. The application task of our work, carried out in collaboration with the computer science laboratory of Avignon (LIA) and the EDD company, seeks to predict the genre of a telecast. This prediction can be based on the histories of previous telecast genres in the same channel but also on those belonging to other parallel channels. We propose a telecast genre taxonomy adapted to such automatic processes as well as a dataset containing the parallel history sequences of 4 French TV channels. Two original methods are proposed in this work in order to take into account parallel stream sequences. The first one, namely the Parallel LSTM (PLSTM) architecture, is an extension of the LSTM model. PLSTM simultaneously processes each sequence in a separate recurrent layer and sums the outputs of each of these layers to produce the final output. The second approach, called MSE-SVM, takes advantage of both LSTM and Support Vector Machines (SVM) methods. Firstly, latent feature vectors are independently generated for each input stream, using the output event of the main one. These new representations are then merged and fed to an SVM algorithm. The PLSTM and MSE-SVM approaches proved their ability to integrate parallel sequences by outperforming, respectively, the LSTM and SVM models that only take into account the sequences of the main stream. The two proposed approaches take profit of the information contained in long sequences. However, they have difficulties to deal with short ones. Though MSE-SVM generally outperforms the PLSTM approach, the problem experienced with short sequences is more pronounced for MSE-SVM. Finally, we propose to extend this approach by feeding additional information related to each event in the input sequences (e.g. the weekday of a telecast). This extension, named AMSE-SVM, has a remarkably better behavior with short sequences without affecting the performance when processing long ones.
In this work, we consider data from functional Magnetic Resonance Imaging (fMRI), that we study in a machine learning setting: we learn a model of brain activity that should generalize on unseen data. We first focus on unsupervised analysis of terabyte-scale fMRI data acquired on subjects at rest (resting-state fMRI). We present new methods for running sparse matrix factorization/dictionary learning on hundreds of fMRI records in reasonable time. Our leading approach relies on introducing randomness in stochastic optimization loops and provides speed-up of an order of magnitude on a variety of settings and datasets. We provide an extended empirical validation of our stochastic subsampling approach, for datasets from fMRI, hyperspectral imaging and collaborative filtering. We derive convergence properties for our algorithm, in a theoretical analysis that reaches beyond the matrix factorization problem. We then turn to work with fMRI data acquired on subject undergoing behavioral protocols (task fMRI). We investigate how to aggregate data from many source studies, acquired with many different protocols, in order to learn more accurate and interpretable decoding models, that predicts stimuli or tasks from brain maps. As a consequence, our multi-study model performs better than single-study decoding. Our approach identifies universally relevant representation of brain activity, supported by a few task-optimized networks learned during model fitting. Finally, on a related topic, we show how to use dynamic programming within end-to-end trained deep networks, with applications in natural language processing.
Controlled Natural Languages (CNL) are artificial languages that use a subset of the vocabulary, morphological forms and syntactical constructions of a natural language while eliminating its polysemy. In a way, they constitute the bridge between formal languages and natural languages. Therefore, they perform the communicative function of the textual mode while being precise and computable by the machine without any ambiguity. In particular, they can be used to facilitate the population or update of knowledge bases within the framework of a human-machine interface. Since 1971, the French Marine Hydrographic and Oceanographic Service (SHOM) issues the French Coast Pilot Books Instructions nautiques, collections of general, nautical and statutory information, intended for use by sailors. They are mandatory for fishing and commercial ships. Among these standards, one of a particular interest is the universal model of hydrographic data (S-100 standard, January, 2010).This thesis analyses the use of a CNL to represent knowledge contained in the Instructions nautiques. This CNL purpose is to act as a pivot between the writing of the text by the dedicated operator, the production of the printed or online publication, and the interaction with knowledge bases and navigational aid tools. We will focus especially on the interaction between the Instructions nautiques Controlled Natural Language and the corresponding Electronic Navigational Charts (ENC).More generally, this thesis asks the question of the evolution of a CNL and the underlying ontologies involved in the Instructions nautiques project. Instructions nautiques have the particularity of combining both strictness (numerical data, electronic charts, legislation) and a certain amount of flexibility (text writing by human operators, unpredictability of the knowledge to be included due to the evolution of sailors¿ practices and needs). We define in this thesis a dynamic CNL in the same way that dynamic ontologies are defined in particular domains. The mechanisms of the CNL presented in this thesis, although developed for the domain of the maritime navigation, have the potential to be adapted to other domains using multimodal corpuses. Finally, the benefits in the future of a controlled hybrid language are undeniable: the use of the different modalities in their full potential can be used in many different applications (for example, the exploitation of the visual modality for a 3D extension).
Graph-Based Semi-Supervised Learning (G-SSL) techniques learn from both labelled and unlabelled data to build better classifiers. Despite successes, its performance can still be improved, particularly in cases of graphs with unclear clusters or unbalanced labelled datasets. To address such limitations, the main contribution of this dissertation is a novel method for G-SSL referred to as the Lγ - PageRank method. It consists of a generalization of the PageRank algorithm based on the positive γ-th powers of the graph Laplacian matrix. The theoretical study of Lγ - PageRank shows that (i) for γ &lt; 1, it corresponds to an extension of the PageRank algorithm to Levy processes: where random walkers can now perform far-distant jumps in a single step; and (ii) for γ &gt; 1, it operates on signed graphs: where nodes belonging to one same class are more likely to share positive edges while nodes from different classes are more likely to be connected with negative edges. We show the existence of an optimal γ-th power that maximizes performance, for which a method for its automatic estimation is devised and assessed. Experiments on several datasets demonstrate that the L´evy flight random walkers can enhance the detection of classes with complex local structures and that the signed graphs can significantly improve the separability of data and also override the issue of unbalanced labelled data. In addition, we study efficient implementations of Lγ -PageRank. Extensions of Power Iteration and Gauss-Southwell, successful algorithms to efficiently compute the solution of the standard PageRank algorithm, are derived for Lγ - PageRank. Moreover, the dynamic versions of Power Iteration and Gauss-Southwell, which can update the solution of standard PageRank in sublinear complexity when the graph evolves or new data arrive, are also extended to Lγ - PageRank. Lastly, we apply Lγ - PageRank in the context of Internet routing. We address the problem of identifying the Autonomous Systems (AS) of inter-AS links from the network of IP addresses and AS public registers. Experiments on tracerout measurements collected from the Internet show that Lγ - PageRank can solve this inference task with no errors, even when the expert does not provide labelled examples of all classes.
Researchers in the field of ECAs try to create agents that can be more natural, believable and easy to use. Designing an ECA requires understanding that manner, personality, emotion, and appearance are very important issues to be considered. In this thesis, we are interested in increasing believability of ECAs by placing personality at the heart of the human-agent verbal interaction. We propose a model relating personality facets and hidden communication goals that can influence ECA behaviors.
Life sciences produce a huge amount of data (e.g., clinical trials, scientific articles) so that integrating and analyzing all the datasets related to a given research question like the correlation between phenotypes and genotypes, is a key element for knowledge discovery. Indeed, ontologies provide a common domain vocabulary for humans, and formal entity definitions for machines. A large number of biomedical ontologies and terminologies has been developed to represent and annotate various datasets. However, datasets represented with different overlapping ontologies are not interoperable. It is therefore crucial to establish correspondences between the ontologies used; an active area of research known as ontology matching. Original ontology matching methods usually exploit the lexical and structural content of the ontologies to align. These methods are less effective when the ontologies to align are lexically heterogeneous i.e., when equivalent concepts are described with different labels. To overcome this issue, the ontology matching community has turned to the use of external knowledge resources as a semantic bridge between the ontologies to align. This approach arises several new issues mainly: (1) the selection of these background resources, (2) the exploitation of the selected resources to enhance the matching results. Several works have dealt with these issues jointly or separately. In our thesis, we made a systematic review and historical evaluation comparison of state-of-the-art approaches. Ontologies, others than the ones to align, are the most used background knowledge resources. Related works often select a set of complete ontologies as background knowledge, even if, only fragments of the selected ontologies are actually effective for discovering new mappings. We propose a novel BK-based ontology matching approach that selects and builds a knowledge resource with just the right concepts chosen from a set of ontologies. We propose two methods to select the most relevant mappings from the candidate ones: (1) based on a set of rules and (2) with Supervised Machine Learning. We experiment and evaluate our approach in the biomedical domain, thanks to the profusion of knowledge resources in biomedicine (ontologies, terminologies and existing alignments).We evaluated our approach with extensive experiments on two Ontology Alignment Evaluation Initiative (OAEI) benchmarks. Our results confirm the effectiveness and efficiency of our approach and overcome or compete with state-of-the-art matchers exploiting background knowledge resources.
The renovation of housing buildings is today a major focus of the fight to reduce the energy consumption and, more generally, global warming. However, if the situation seems technically simple, there are still many obstacles (economic, social, cultural) that slow down the renovation of the existing park. These brakes require to look at the renovation in a global perspective, with the work of a facilitator, able to establish a relationship of trust with the inhabitants. This global approach is even more important when it comes to renovating condominiums. Indeed, in these buildings, decision making is complicated by the number of actors and their specific organization. To allow condominiums to vote for renovation works, a long and expensive accompaniment work is necessary. To make this work of accompaniment as effective as possible, we have designed a decision tool, usable by the facilitator, which allows him to carry out a multidisciplinary diagnosis of the co-ownership and to recommend solutions of accompaniments ajusted. In particular, we defined a list of influential criteria for the decision of a condominium to undertake renovations, and we compiled them into an evaluation tool. The co-ownership is studied according to all its characteristics (quality of use, technical possibilities, economic potential, sociological profile of the owners, quality of the neighborhood, collective dynamics) and specific prescriptions are made according to the evaluation of each criterion. Although the first results are encouraging, there are still many opportunities for improvement to massively use this tool and make the diagnosis even more precise.
Many methods exist for solving multicriteria optimization problems, and it is not easy to choose the right method well adapted to a given multicriteria problem. Even after choosing a multicriteria method, various parameters (e.g. weight, utility functions, etc.) must be carefully determined either to find the optimal solution (best compromise) or to classify all feasible solutions (the set of alternatives). To overcome this potential difficulty, elicitation methods are used in order to help the decision maker to fix safely the parameters. Additionally, we assume that we have a set of feasible solutions, and we also make the assumption that we have prior information about the preferences of the decision maker, and we focus on how to use this information, rather than how to get them. In the first contribution of this work, we take advantage of a simple and quickly computable statistical measure, namely, the Spearman ρ correlation coefficient, to develop an gready approche, and two exact approaches based on constraint programming (CP) and linear integer programming (MIP). These methods are then used to automatically elicit the appropriate parameter of the lexicographic ordering method. We also propose some elicitation models for most commonly used multicriteria methods, such as MinLeximax method used to ensure fairness and efficiency requirements, the weighted sum method, and OWA operators.
Characterizing the quality of smells is a complex process that consists in identifying a set of descriptors best summarizing the olfactory sensation. Generally, this characterization results in a limited set of descriptors provided by sensorial analysis experts. These sensorial analysis sessions are however very costly for industrials. Indeed, such oriented approaches based on vocabulary learning limit, in a restrictive manner, the possible descriptors available for any uninitiated public, and therefore require a costly vocabulary-learning phase. If we could entrust this characterization to neophytes, the number of participants of a sensorial analysis session would be significantly enlarged while reducing costs. However, in that setting, each individual description is not related to a set of non-ambiguous descriptors anymore, but to a bag of terms expressed in natural language (NL). Two issues are then related to smell characterization implementing this approach. Hence, this work focuses first on the definition and evaluation of models that can be used to summarize a set of terms into unambiguous entity identifiers selected from a given ontology. Among the several strategies explored in this contribution, we propose to compare hybrid approaches taking advantages of knowledge bases (symbolic representations) and word embeddings defined from large text corpora analysis. The results we obtain highlight the relative benefits of mixing symbolic representations with classic word embeddings for this task. We then formally define the problem of summarizing sets of concepts and we propose a model mimicking Human-like Intelligence for scoring alternative summaries with regard to a specific objective function. Interestingly, this non-oriented approach for identifying the quality of odors appears to be an actual cognitive automation of the task today performed by expert operators in sensorial analysis. It therefore opens interesting perspectives for developing scalable sensorial analyses based on large sets of evaluators when assessing, for instance, olfactory pollution around industrial sites.
In recent years, deep learning has enabled impressive achievements in Machine Translation. Neural Machine Translation (NMT) relies on training deep neural networks with large number of parameters on vast amounts of parallel data to learn how to translate from one language to another. One crucial factor to the success of NMT is the design of new powerful and efficient architectures. For this purpose, we introduce Pervasive Attention, a model based on two-dimensional convolutions that jointly encode the source and target sequences with interactions that are pervasive throughout the network. To improve the efficiency of NMT systems, we explore online machine translation where the source is read incrementally and the decoder is fed partial contexts so that the model can alternate between reading and writing. We also address the resource-efficiency of encoder-decoder models and posit that going deeper in a neural network is not required for all instances. We design depth-adaptive Transformer decoders that allow for anytime prediction and sample-adaptive halting mechanisms to favor low cost predictions for low complexity instances and save deeper predictions for complex scenarios.
The automatic processing of speech is an area that encompasses a large number of works: speaker recognition, named entities detection or transcription of the audio signal into words. All this information can be exploited by automatic indexing techniques which will allow indexing of large document collections. The work presented in this thesis are interested in the automatic indexing of speakers in french audio documents. Specifically we try to identify the various contributions of a speaker and nominate them by their first and last name. This process is known as named identification of the speaker. The particularity of this work lies in the joint use of audio and its transcript to name the speakers of a document. The first and last name of each speaker is extracted from the document itself (from its rich transcription more accurately), before being assigned to one of the speakers of the document. We begin by describing the context and previous work on the speaker named identification process before submitting Milesin, the system developed during this thesis. The contribution of this work lies firstly in the use of an automatic detector of named entities (LIA_NE) to extract the first name / last name of the transcript. Afterwards, they rely on the theory of belief functions to perform the assignment to the speakers of the document and thus take into account the various conflicts that may arise. Finally, an optimal assignment algorithm is proposed. This system gives an error rate of between 12 and 20 % on reference transcripts (done manually) based on the corpus used. We then present the advances and limitations highlighted by this work. We propose an initial study of the impact of the use of fully automatic transcriptions on Milesin.
This is why many elderly people care projects: technical, academic and commercial have seen the light of day in recent years. This thesis work wasc arried out under Cifre agreement, jointly between the company KRG Corporate and the BMBI laboratory (Biomechanics and Bioengineering) of the UTC (Université of Technologie of Compiègne). Its purpose is to oﬀer a sensor for sound recognition and everyday activities, with the aim of expanding and improving the tele-assistance system already marketed by the company. Several speech recognition or speaker recognition methods have already been proven in the field of sound recognition, including GMM (Modèle de mélange gaussien – Gaussian Mixture Model), SVM-GSL (Machine à vecteurs de support, GMM-super-vecteur à noyau linéaire – Support vector machine GMM Supervector Linear kernel) and HMM (Modèle de Markov caché – Hidden Markov Model). In the same way, we proposed to use i-vectors for sound recognition. Then we broadened our spectrum, and used Deep Learning, which currently gives very good results in classification across all domains. The methods mentioned above were also tested under noisy and then real conditions.
At the beginning of this PhD, no treebank for Serbian was available. However, manually annotated treebanks are an essential resource for developing (training and evaluating) statistical tools for syntactic analysis (parsers). Efficient parsers, in turn, facilitate the annotation of large corpora, which can be used as a basis for research in theoretical linguistics. In order to address this issue, we created a suite of NLP resources for Serbian. Firstly, we created the ParCoTrain-Synt treebank, a 101 000 token corpus, complete with morphosyntactic annotation, lemmatisation and syntactic dependency annotation. We also built the ParCoLex lexicon, containing 7 million entries for 157 000 different lemmas. Using these two resources, we trained models for parsing, morphosyntactic tagging and lemmatisation. All of the above resources are available at the following address: https: //github.com/aleksandra-miletic/serbian-nlp-resources. We also used these resources in two experiments in Serbian linguistics, demonstrating that the ParCoTrain-Synt treebank is well suited to empirical studies based on quantitative data analysis.
Significant advances have been achieved in bilingual word-level alignment from comparable corpora, yet the challenge remains for phrase-level alignment. Traditional methods to phrase alignment can only handle phrase of equal length, while word embedding based approaches learn phrase embeddings as individual vocabulary entries suffer from the data sparsity and cannot handle out of vocabulary phrases. Since bilingual alignment is a vector comparison task, phrase representation plays a key role. In this thesis, we study the approaches for unified phrase modeling and cross-lingual phrase alignment, ranging from co-occurrence models to most recent neural state-of-the-art approaches. We review supervised and unsupervised frameworks for modeling cross-lingual phrase representations. Two contributions are proposed in this work. Our proposition improves significantly bilingual alignment of different length phrases.
Based on a video recording of conversational British English and within the framework of Multimodal Discourse Analysis, this study tests whether three different syntactic types of subordinate structures are evenly integrated to their environment. Subordinate constructions have been described in syntax as dependent forms elaborating on primary elements of discourse. Beyond showing that subordinate constructions are not evenly dependent on their environment depending on how speakers use the prosodic and kinetic modalities to express greater (in)dependency, our results in production as in perception suggest on the one hand that appositive clauses show more break than the other syntactic types, and on the other hand that the creation of a break mainly relies on prosodic cues.
This thesis describes the implementation of a speech understanding system on a microprocessor. The system is designed to accept continuous speech from one speaker and to work within the context of a limited task situation and small vocabularies. The system utilizes phonetic recognition at the phonetic level and an optimal one-pass dynamic programming algorithm at the lexical and syntactic levels. The system has an interactive program for the definition of grammars for a given specific task language and a program of orthographic-phonetic translation that takes into account some phonological variations of words.
Graphs are ubiquitous in many fields of research ranging from sociology to biology. A graph is a very simple mathematical structure that consists of a set of elements, called nodes, connected to each other by edges. Graph clustering is a central problem in the analysis of graphs whose objective is to identify dense groups of nodes that are sparsely connected to the rest of the graph. These groups of nodes, called clusters, are fundamental to an in-depth understanding of graph structures. There is no universal definition of what a good cluster is, and different approaches might be best suited for different applications. Whereas most of classic methods focus on finding node partitions, i.e. on coloring graph nodes so that each node has one and only one color, more elaborate approaches are often necessary to model the complex structure of real-life graphs and to address sophisticated applications. In particular, in many cases, we must consider that a given node can belong to more than one cluster. Besides, many real-world systems exhibit multi-scale structures and one much seek for hierarchies of clusters rather than flat clusterings. Furthermore, graphs often evolve over time and are too massive to be handled in one batch so that one must be able to process stream of edges. In this work, we study alternative approaches and design novel algorithms to tackle these different problems. The novel methods that we propose to address these different problems are mostly inspired by variants of modularity, a classic measure that accesses the quality of a node partition, and by random walks, stochastic processes whose properties are closely related to the graph structure. We provide analyses that give theoretical guarantees for the different proposed techniques, and endeavour to evaluate these algorithms on real-world datasets and use cases.
This thesis is devoted to the study of the preposition iz in modern Russian, through the constructions that contain it and form a complex semantic network. While prepositions have always been a central part of linguistic research, iz has not been described in much detail in terms of its semantics and function, even though some works have been devoted to it. Chapter II presents a detailed analysis of the uses of iz within a construction, considered as a semantic-syntactic unit. This detailed analysis highlights the parameters that show similarities between the various uses of iz by establishing links between them and by organising them into a well-structured network of meanings. This network of meanings forms the semantic profile of iz itself. Chapter III provides a summary including the main ideas developed in the chapter analysing iz, and compares them through a contrastive study on the uses of the prepositions ot and s, which appear in similar contexts and, as a result, enter into competition with iz. This contrastive analysis makes it possible to identify the specificities of the uses of each of the three prepositions and to determine the way in which they separate areas of use. Elsewhere, particular consideration is given to the quantitative analysis of combinations of the three prepositions iz, ot and s with prefix verbs. The results obtained clearly confirm the hypothesis of a correlation between the prepositions and the homonymous prefixes ot and ot-, s and s-, and the synonymous prefixes iz and vy-. Lastly, the arguments developed throughout the work are used to explain erroneous use cases involving the preposition iz in works for learners of Russian (mainly French and English-speaking). Thanks to this comparative dimension between languages with a different structure, the analysis carried out highlights the fact that certain constructions with iz (in particular, 'belonging', 'identification of an entity within a group of entities') have specific parameters that are not present in “equivalent” constructions in French and English. The general conclusion of the thesis that concludes this study presents all of the results and data obtained, as does the Annex, which brings together the main quantitative data taken from research on prepositions studied mainly in the Russian National Corpus (ruscorpora.ru). The results obtained can contribute to linguistic studies of prepositions and to lexicographical entries, as well as to research related to acquisition of Russian by non-Russian-speaking adults, and didactics of Russian as a foreign language for example.
In this thesis, we study information diffusion in online social networks. Websites like Facebook or Twitter have indeed become information medias, on which users create and share a lot of data. Most existing models of the information diffusion phenomenon relies on strong hypothesis about the structure and dynamics of diffusion. In this document, we study the problem of diffusion prediction in the context where the social graph is unknown and only user actions are observed. - We propose a learning algorithm for the independant cascades model that does not take time into account. Experimental results show that this approach obtains better results than time-based learning schemes. - We then propose several representations learning methods for this task of diffusion prediction. This let us define more compact and faster models. - Finally, we apply our representation learning approach to the source detection task, where it obtains much better results than graph-based approaches.
This computer science thesis presents a computational cognitive modeling work using eye movements of people faced to different information search tasks on textual material. People should judge whether a piece of text is semantically related or not to a goal expressed by a few words. More specifically, we analyzed eye movements during two information search tasks: reading a paragraph with the task of quickly deciding i) if it is related or not to a given goal and ii) whether it is better related to a given goal than another paragraph processed previously. One model is proposed for each of these situations. Our simulations are done at the level of eye fixations and saccades. In particular, we predicted the time at which participants would decide to stop reading a paragraph because they have enough information to make their decision. The models make predictions at the level of words that are likely to be fixated before a paragraph is abandoned. We followed a statistical parametric approach in the construction of our models. The models are based on a Bayesian classifier. We proposed a two-variable linear threshold to account for the decision to stop reading a paragraph, based on the Rank of the fixation and i) the semantic similarity (Cos) between the paragraph and the goal and ii) the difference of semantic similarities (Gap) between each paragraph and the goal. For both models, the performance results showed that we are able to replicate in average people's behavior faced to the information search tasks studied along the thesis. The thesis includes two main parts: 1) designing and carrying out psychophysical experiments in order to acquire eye movement data and 2) developing and testing the computational cognitive models.
As multimedia sources have become massively available online, helping users to understand the large amount of information they generate has become a major issue. One way to approach this is by summarizing multimedia content, thus generating abridged and informative versions of the original sources. This PhD thesis addresses the subject of text and audio-based multimedia summarization in a multilingual context. Text-based multimedia summarization uses transcripts to produce summaries that may be presented either as text or in their original format. The transcription of multimedia sources can be done manually or automatically by an Automatic Speech Recognition(ASR) system. The transcripts produced using either method differ from well formed written language given their source is mostly spoken language. In addition, ASR transcripts lack syntactic information. For example, capital letters and punctuation marks are unavailable, which means sentences are nonexistent. Finally, we extend ARTEX, astate-of-the-art extractive text summarization method, to process documents in MSA by adapting preprocessing modules. The resulting summaries can be presented as plain text or in their original multimedia format by aligning the selected SUs. Concerning audio-based summarization, we introduce an extractive method which represents the informativeness of the source based on its audio features to select the segments that are most pertinent to the summary. During the training phase, our method uses available transcripts of the audio documents to create an informativeness model which maps a set of audio features with a divergence value. Subsequently, when summarizing new audio documents, transcripts are not needed anymore. Results over amulti-evaluator scheme show that our approach provides understandable and informative summaries. Evaluation measures is also a field which we deal with. We also explore the possibility of measuring the quality of an automatic transcript based on its informativeness. In addition, we study to what extent automatic summarization may compensate for the problems raised during the transcription phase. Lastly, we study how text informativeness evaluation measures may be extended to passage interestingness evaluation.
With tremendous generation of data, we have data collected from different information sources having heterogeneous properties, thus it is important to consider these representations or views of the data. This problem of machine learning is referred as multiview learning. It has many applications for e.g. in medical imaging, we can represent human brain with different set of features for example MRI, t-fMRI, EEG, etc. In this thesis, we focus on supervised multiview learning, where we see multiview learning as combination of different view-specific classifiers or views. Therefore, according to our point of view, it is interesting to tackle multiview learning issue through PAC-Bayesian framework. It is a tool derived from statistical learning theory studying models expressed as majority votes. One of the advantages of PAC-Bayesian theory is that it allows to directly capture the trade-off between accuracy and diversity between voters, which is important for multiview learning. The first contribution of this thesis is extending the classical PAC-Bayesian theory (with a single view) to multiview learning (with more than two views). To do this, we considered a two-level hierarchy of distributions over the view-specific voters and the views. Based on this strategy, we derived PAC-Bayesian generalization bounds (both probabilistic and expected risk bounds) for multiview learning. It iteratively learns the weights over the views by optimizing the multiview C-Bound which controls the trade-off between the accuracy and the diversity between the views. The second algorithm is based on late fusion approach where we combine the predictions of view-specific classifiers using the PAC-Bayesian algorithm CqBoost proposed by Roy et al. Finally, we show that minimization of classification error for multiview weighted majority vote is equivalent to the minimization of Bregman divergences. This allowed us to derive a parallel update optimization algorithm (referred as MωMvC2) to learn our multiview weighted majority vote.
Today we collect data from various sources both online and offline. Usually this data is mostly referring to acts already committed and are usually analyzed a posteriori. Given that early warning leads to early (and hopefully more effective) action, in this work we suggest to try and identify early warning signals of actions or activities that will happen in the future. A weak signal is a past, or current, evidence with ambiguous interpretations that can be correlated to other, more important present, or future, events and tendencies. Weak signals are unclear clues that can warn us for other emerging patterns. The key problem is to detect such relevant signals hidden in the mass of data that is available. Before being sure that the attention of human analysts is necessary, it is important to detect but also verify and validate these signals through a series of different methods. After being identified and validated, a weak signal becomes an early sign that should be carefully monitored. In this work, we want to establish methods of identifying weak signals inside massive datasets, like datasets originating from social media or other publicly available sources. Algorithms based on statistical or machine learning or Artificial Intelligence methods combined with formal semantic descriptions of the events we are interested in, would allow us to identify early warning signals and turn them to indications of future acts. Moreover, methods related to Named Entity Detection would allow us to bind these early warning signals to participating entities (e.g. persons, places, etc.) and use the entity descriptions to improve and extend the description of the possible events. The work will be validated by applying its results to actual real-world data provided by PJGN, which is a partner in this work. Identifying references to events and resolving the information around those events is an important problem for PJGN and the work will be directly applicable.
My thesis is part of Arabic sentiment analysis. Our study focuses on the use of a neural approach to improve polarity detection, using embeddings. These embeddings have revealed fundamental in various natural languages processing tasks (NLP). Our contribution in this thesis concerns several axis. First, we begin with a preliminary study of the various existing pre-trained word embeddings resources in arabic. These embeddings consider words as space separated units in order to capture semantic and syntactic similarities in the embedding space. We propose arabic specific embeddings that take into account agglutination and morphological richness of Arabic. These specific embeddings have been used, alone and in combined way, as input to neural networks providing an improvement in terms of classification performance. Finally, we evaluate embeddings with intrinsic and extrinsic methods specific to sentiment analysis task. For intrinsic embeddings evaluation, we propose a new protocol introducing the notion of sentiment stability in the embeddings space. We propose also a qualitaive extrinsic analysis of our embeddings by using visualisation methods.
The combination of semantic processing of knowledge and modelling steps of reasoning employed in the clinical field offers exciting and necessary opportunities to develop ontologies relevant to the practice of medicine. In this context, multiple medical databases such as MEDLINE, PubMed are valuable tools but not sufficient because they cannot acquire the usable knowledge easily in a clinical approach. Indeed, abundance of inappropriate quotations constitutes the noise and requires a tedious sort incompatible with the practice of medicine. In an iterative process, the objective is to build an approach as automated as possible, the reusable medical knowledge bases is founded on an ontology of the concerned fields. In this thesis, the author will develop a series of tools for knowledge acquisition combining the linguistic analysis operators and clinical modelling based on the implemented knowledge typology and an implementation of different forms of employed reasoning. Knowledge is not limited to the information from data, but also and especially on the cognitive operators of reasoning for making them operational in the context relevant to the practitioner. A multi-agent system enables the integration and cooperation of the various modules used in the development of a medical ontology. The data sources are from medical databases such as MEDLINE, the citations retrieved by PubMed, and the concepts and vocabulary from the Unified Medical Language System (UMLS). Regarding the scope of produced knowledge bases, the research concerns the entire clinical process: diagnosis, prognosis, treatment, and therapeutic monitoring of various diseases in a given medical field. On the whole, we worked on logical aspects related to cognitive operators of used reasoning, and we organized the cooperation and integration of exploited knowledge during the various stages of the clinical process (diagnosis, prognosis, treatment, therapeutic monitoring). This integration is based on a SMAAD: multi-agent system for decision support.
The aim of this thesis is to make an Embodied Conversational Agent (ECA) sincere in order to, on one hand, improve its believability from the human's point of view, and on the other hand make it acceptable in an affective relationship between an artificial companion and a human. The mental states carried by the MCA are formalised in logics: our will to represent mental states stemming from complex forms of reasoning (based on counterfactual reasoning or on the agent's norms and goals) that are mainly expressed via language (Oatley 1987) led us to design the BIGRE model (Beliefs, Ideals, Goals, Responsibility, Emotions). This model, based on a BDI-like logic (Belief, Desire, Intention), allows us to also represent some particular emotions that we call complex emotions, such as rejoicing, gratitude or regret. We implemented the MCL in the ECA Greta, which enabled an evaluation of this language in terms of believability and sincerity perceived by the human. The second part of this work is about the ECA's reasoning capabilities: in order to allow the agent to reason in the dialogue, that is to update its mental states and its emotions and select its communicative intention, we designed a reasoning engine. This reasoning engine is based on the BDI behaviour cycle of Perception - Decision - Action and on the operators from the BIGRE model, thus enabling the manipulation of mental states resulting from complex reasoning (including complex emotions). The MCA in our language are part of our reasoning engine, and are used to achieve the ECA's communicative intention: for example if the ECA intends to express its gratitude, it builds a plan to achieve this intention, that consists of the MCA thank or congratulate, depending on the intensity of the emotion. One type of communicative intention, triggered by discourse obligations rules, participates in the local regulation of dialogue. Further, since the ECA is emotional, its sincerity brings it to express all its emotions. The generic character of this reasoning engine allowed us to implement it in the ECA Greta (where it is linked with the MCL) as well as in the agent MARC. The multimodal expression of MCA by the agent MARC was made possible by integrating Scherer's checks in the reasoning engine that we adapted to the context of dialogue. An evaluation of the reasoning engine with the agent MARC shows that the mental states deduced by the engine are appropriate to the situation, and that their expression (the expression of the agent's sincerity) is also appropriate.
Artificial Intelligence has penetrated into every aspect of our lives in this era of Big Data. It has brought revolutionary changes upon various sectors including e-commerce and finance. In this thesis, we present four applications of AI which improve existing goods and services, enables automation and greatly increase the efficiency of many tasks in both domains. Firstly, we improve the product search service offered by most e-commerce sites by using a novel term weighting scheme to better assess term importance within a search query. Then we build a predictive model on daily sales using a time series forecasting approach and leverage the predicted results to rank product search results in order to maximize the revenue of a company. Next, we present the product categorization challenge we hold online and analyze the winning solutions, consisting of the state-of-the-art classification algorithms, on our real dataset. We perform an extensive study on every single stocks of S&amp;P 500 index using four state-of-the-art classification algorithms and report very promising results
Electrical products and networks are subject to a regulatory and normative environment divided according to geography, voltage level and sector of application. The growing importance of new technologies associated with the energy transition (renewable energy, electric vehicle, energy storage and optimization) is leading to an acceleration of the production of normative and regulatory benchmarks. In this context, the design of new products, systems and installations requires constant monitoring of the reference texts to be taken into account, the evolution of the vocabulary and concepts used and obviously, the constraints that must be respected. The objective of the proposed thesis is to extract from documentary collections the design rules and constraints to be applied by translating them into a formal actionable language (i.e. understandable and usable by experts in the field) for the design assistance. This includes the formalization of domain knowledge, the analysis and understanding of texts, the construction of a synthesis of extracted rules, as well as the detection of conflicts and duplicates
With the ever-growing mass of published text, natural language understanding stands as one of the most sought-after goal of artificial intelligence. In natural language, not every fact expressed in the text is necessarily explicit: human readers naturally infer what is missing through various intuitive linguistic skills, common sense or domain-specific knowledge, and life experiences. Natural Language Processing (NLP) systems do not have these initial capabilities. Unable to draw inferences to fill the gaps in the text, they cannot truly understand it. This dissertation focuses on this problem and presents our work on the automatic resolution of textual inferences in the context of machine reading. A textual inference is simply defined as a relation between two fragments of text: a human reading the first can reasonably infer that the second is true. A lot of different NLP tasks more or less directly evaluate systems on their ability to recognize textual inference. Among this multiplicity of evaluation frameworks, inferences themselves are not one and the same and also present a wide variety of different types. We reflect on inferences for NLP from a theoretical standpoint and present two contributions addressing these levels of diversity: an abstract contextualized inference task encompassing most NLP inference-related tasks, and a novel hierchical taxonomy of textual inferences based on their difficulty. Automatically recognizing textual inference currently almost always involves a machine learning model, trained to use various linguistic features on a labeled dataset of samples of textual inference. However, specific data on complex inference phenomena is not currently abundant enough that systems can directly learn world knowledge and commonsense reasoning. Instead, systems focus on learning how to use the syntactic structure of sentences to align the words of two semantically related sentences. To extend what systems know of the world, they include external background knowledge, often improving their results. But this addition is often made on top of other features, and rarely well integrated to sentence structure. The main contributions of our thesis address the previous concern, with the aim of solving complex natural language understanding tasks. With the hypothesis that a simpler lexicon should make easier to compare the sense of two sentences, we present a passage retrieval method using structured lexical expansion backed up by a simplifying dictionary. This simplification hypothesis is tested again in a contribution on textual entailment: syntactical paraphrases are extracted from the same dictionary and repeatedly applied on the first sentence to turn it into the second. We then present a machine learning kernel-based method recognizing sentence rewritings, with a notion of types able to encode lexical-semantic knowledge. This approach is effective on three tasks: paraphrase identification, textual entailment and question answering. Reading comprehension tests are used for evaluation: these multiple-choice questions on short text constitute the most practical way to assess textual inference within a complete context. Our system is founded on a efficient tree edit algorithm, and the features extracted from edit sequences are used to build two classifiers for the validation and invalidation of answer candidates. This approach reaches second place at the "Entrance Exams" CLEF 2015 challenge.
We propose an approach to detect topics, overlapping communities of interest, expertise, trends and activities in user-generated content sites and in particular in question-answering forums such as StackOverFlow. We first describe QASM (Question &amp; Answer Social Media), a system based on social network analysis to manage the two main resources in question-answering sites: users and contents. We also introduce the QASM vocabulary used to formalize both the level of interest and the expertise of users on topics. We then propose an efficient approach to detect communities of interest. It relies on another method to enrich questions with a more general tag when needed. We compared three detection methods on a dataset extracted from the popular Q&amp;A site StackOverflow. Our method based on topic modeling and user membership assignment is shown to be much simpler and faster while preserving the quality of the detection. We then propose an additional method to automatically generate a label for a detected topic by analyzing the meaning and links of its bag of words. We conduct a user study to compare different algorithms to choose the label. Finally we extend our probabilistic graphical model to jointly model topics, expertise, activities and trends. We performed experiments with realworld data to confirm the effectiveness of our joint model, studying the users' behaviors and topics dynamics.
We are surrounded by decisions to take, what book to read next? What film to watch this night and in the week-end? As the number of items became tremendous the use of recommendation systems became essential in daily life. At the same time social network become indispensable in people's daily lives; people from different countries and age groups use them on a daily basis. While people are spending time on social networks, they are leaving valuable information about them attracting researchers'attention. Recommendation is one domain that has been affected by the social networks widespread; the result is the social recommenders'studies. However, in the literature we've found that most of the social recommenders were evaluated over Epinions, flixter and other type of domains based recommender social networks, which are composed of (users, items, ratings and relations).
Smart speakers offer the possibility of interacting with smart home systems, and make it possible to issue a range of requests about various subjects. They represent the first ambient voice interfaces that are frequently available in home environments. Very often they are only capable of inferring voice commands of a simple syntax in short utterances in the realm of smart homes that promote home care for senior adults. They support them during everyday situations by improving their quality of life, and also providing assistance in situations of distress. As a result, these research projects frequently concentrate on human activity detection, resulting in a lack of attention for the communicative aspects in a smart home design. However the availability of these corpora are crucial for developing interactive communication systems between the smart home and its inhabitants. Such corpora at one's disposal could also contribute to the development of a generation of smart speakers capable of extracting more complex voice commands. As a consequence, part of our work consisted in developing a corpus generator, producing home automation domain specific voice commands, automatically annotated with intent and concept labels. The extraction of intents and concepts from these commands, by a Spoken Language Understanding (SLU) system is necessary to provide the decision-making module with the information, necessary for their execution. As several studies have shown, the interaction between ASR and NLU in a sequential SLU approach accumulates errors. To achieve this goal, we first develop a sequential SLU approach as our baseline approach, in which a classic ASR method generates transcriptions that are passed to the NLU module, before continuing with the development of an End-to-end SLU module. These two SLU systems were evaluated on a corpus recorded in the home automation domain. We investigate whether the prosodic information that the end-to-end SLU system has access to, contributes to SLU performance. We position the two approaches also by comparing their robustness, facing speech with more semantic and syntactic variation. The context of this thesis is the ANR VocADom project.
Because of its key societal, economic and cultural stakes, Artificial Intelligence (AI) is a hot topic. One of its main goal, is to develop systems that facilitates the daily life of humans, with applications such as household robots, industrial robots, autonomous vehicle and much more. The rise of AI is highly due to the emergence of tools based on deep neural-networks which make it possible to simultaneously learn, the representation of the data (which were traditionally hand-crafted), and the task to solve (traditionally learned with statistical models). This resulted from the conjunction of theoretical advances, the growing computational capacity as well as the availability of many annotated data. Specialization: learn representations from few specific tasks with the goal to be able to carry out very specific tasks (specialized in a certain field) with a very good level of performance; (ii) Universality: learn representations from several general tasks with the goal to perform as many tasks as possible in different contexts. Regarding the quantification of universality, we proposed to evaluate universalizing methods in a Transferlearning scheme.
When it comes to the construction of multilingual lexico-semantic resources, the first thing that comes to mind is that the resources we want to align, should share the same data model and format (representational interoperability). However, with the emergence of standards such as LMF and their implementation and widespread use for the production of resources as lexical linked data (Ontolex), representational interoperability has ceased to be a major challenge for the production of large-scale multilingual resources. However, as far as the interoperability of sense-level multi-lingual alignments is concerned, a major challenge is the choice of a suitable interlingual pivot. The use of acception-based interlingual representations, a solution proposed over 20 years ago, could be viable. However, the manual construction of such language-independent pivot representations is very difficult due to the lack of expert speaking enough languages fluently and algorithms for their automatic constructions have never since materialized, mainly because of the lack of a formal axiomatic characterization that ensures the pre-servation of their correctness properties. In this thesis, we address this issue by first formalizing acception-based interlingual pivot architectures through a set of axiomatic constraints and rules that guarantee their correctness. Then, we propose algorithms for the initial construction and the update (dynamic interoperability) of interlingual acception-based multilingual resources by exploiting the combinatorial properties of pairwise bilingual translation graphs. Secondly, we study the practical considerations of applying our construction algorithms on a tangible resource, DBNary, a resource periodically extracted from Wiktionary in many languages in lexical linked data.
This research examines the divergences and convergences across the histories of three national galleries, in England, Canada and the United States, providing evidence of a common model that emanates particularly from two types of institutions that appeared in England during the 17th century: public museums founded on scientific and populist principles, and private art galleries anchored in elitist traditions. The continued struggle to truly integrate technologies in the documentation of art betrays the difficult heritage between these two opposing models. Throughout the unique historical trajectories of these institutions, there was little proof that optical or digital technologies had had important repercussions on the methodologies or the philosophies of the documentation of works of art. On the contrary, it was observed that documentation, even in digital form, continued to rely on minimal standards of data gathering, restricted groups of persons trained to collect data, and limited access to any data captured. This research reinforces the need to redefine museum documentation in order to rethink its strategies and guiding philosophies, to enable new research into museum collections, and to enlarge the integration of digital technologies into the process.
This study is a dialectometric analysis of Berber dialects of Kabylia. This work includes a sample of 168 Kabyle dialects spread across the Kabyle territory. The analyzed corpus includes 130 entries (lexemes and phrases) collected in each of the varieties considered. We opted for the Levenshtein method to calculate the distance between the variants. We chose the algorithm of Ward's Method for grouping varieties. We tested three methods to calculate the distance between the sounds: the binary method, the Euclidean distance and the Manhattan distance. The analysis of the results allowed us to show the dialect continuum in Kabylia and classify Kabyle dialects into five mains areas.
The scientific activity of this thesis consists of: Understanding the nature of anaphora and coreference relationships in messages from mediated electronic communication, with priority given to emails. This will lead to methods that can weave semantic links between different sentences and messages. Analysing more precisely the event anaphors (Danlos, 2004) and the nature of relations between events. This represents a major scientific lock for understanding natural language with a direct impact on the development of Emvista's Prevyo solution. The scientific advancement and the appropriation of the resulting technologies will enable Emvista to position itself as an important player in the understanding of natural language
Our study concerns the language of tourism from a lexicographical perspective. This corpus is composed by about 10.000 texts in three languages (French, Italian and English), aligned using “Alinea”. Starting from terminological extraction, we analysed some collocations at the aim to create a trilingual and tri-directional glossary. We chose this subject according to the increasing importance taken from tourism economy in the world. Our study fields are thematic terminology, corpus linguistics and automatic language treatment. First of all, we introduced to corpus linguistics presenting the different categories of corpus and pointing out our attention on two main notions: representativeness and context. Therefore, we explained the link between Language for Special Purposes and tourism discourse as a Specialized Discourse. In the second chapter, we showed the trilingual thematic corpus we created during our researches. We described the main steps to create a corpus: collection of texts, cleaning and annotation. In this chapter, we gave a particular attention to the presentation of “Alinea”. Finally, the third chapter is a study of frequent collocations with the term “town” (ville).The annexes present the glossary as well as the methodological principals we followed in the redaction.
Differences between training and testing conditions may significantly degrade recognition accuracy in automatic speech recognition (ASR) systems. Adaptation is an efficient way to reduce the mismatch between models and data from a particular speaker or channel. There are two dominant types of acoustic models (AMs) used in ASR: Gaussian mixture models (GMMs) and deep neural networks (DNNs). The GMM hidden Markov model (GMM-HMM) approach has been one of the most common technique in ASR systems for many decades. Speaker adaptation is very effective for these AMs and various adaptation techniques have been developed for them. The main purpose of this thesis is to develop a method for efficient transfer of adaptation algorithms from the GMM framework to DNN models. The proposed technique provides a general framework for transferring adaptation algorithms, developed for GMMs, to DNN adaptation. It is explored for various state-of-the-art ASR systems and is shown to be effective in comparison with other speaker adaptation techniques and complementary to them.
Associating word senses with temporal orientation to grasp the temporal information in language is relatively straightforward task for humans by using world knowledge. With this in mind, a lexical temporal knowledge-base associating word senses automatically with their underlying temporal orientation would be crucial for the computational tasks aiming at interpretation of language of time in text. In this research, we introduce a temporal ontology namely TempoWordNet where all the synsets of WordNet are augmented with their intrinsic temporal dimensions: atemporal, past, present, and future. The resource is evaluated both intrinsically and extrinsically, the underlying idea being that a reliable resource must evidence high quality time-tagging as well as improved performance for some external tasks. Both the evaluations results confirm the quality and usefulness of the resource. To complement our research we also experiment how a search application can benefit from this resource. Feedback from TempoWordNet users advocate for more reliable resource. At the end, we propose a strategy that shows steady improvements over the previous versions of TempoWordNet.
After an overview of what underpins the notion of meaning, this thesis intends to identify a number of linguistic features that could serve as an abstract characterisation informing the uses of the English verb want. It is claimed that a vestige of its original use as a verb expressing lack has been kept in its modern uses as a verb expressing desire in that it marks a prime operation of localisation by which the grammatical subject connects to an object or a property p that is not immediately available to him in the speaking situation. It is shown that this relation relies on a subjectification process which culminates in a strong tendency towards the modalisation of the want sentence resulting in both epistemic and deontic effects. It is argued that this organisation can be described as a notional domain centring on the missing object p, acting as a starting point for a second localisation operation whose aim is to locate a relation to palliate the lack of p. It is thus possible to account for the difference between desire and volition as degrees in the determinations that can be constructed from the missing object. This thesis then correlates the syntactic constructions in the internal argument of want with those types of determination.
These knowledge resources, described according to different representation models and for different contexts of use, raise the problem of complexity of their interoperability, especially for actual public health problematics such as personalized medicine, translational medicine and the secondary use of medical data. Indeed, these knowledge resources may represent the same notion in different ways or represent different but complementary notions. For being able to use knowledge resources jointly, we studied three processes that can overcome semantic conflicts (difficulties encountered when relating distinct knowledge resources): the alignment, the integration and the semantic enrichment of the integration. The integration aims not only to find mappings but also to organize all knowledge resources' entities into a unique and coherent structure. Finally, the semantic enrichment of integration consists in finding all the required mapping relations between entities of distinct knowledge resources (equivalence, subsumption, transversal and, failing that, disjunction relations).In this frame, we firstly realized the alignment of laboratory tests terminologies: Then, we suppressed erroneous mappings (confounding conflicts) using the structure of LOINC.Secondly, we integrated RxNorm to SNOMED CT. We constructed formal definitions for each entity in RxNorm by using their definitional features (active ingredient, strength, dose form, etc.) according to the design patterns proposed by SNOMED CT. Our process resolved some cases of naming conflicts but was confronted to confounding and scaling conflicts, which highlights the need for improving RxNorm and SNOMED CT.Finally, we performed a semantically enriched integration of ICD-10 and ICD-O3 using SNOMED CT as support. As ICD-10 describes diagnoses and ICD-O3 describes this notion according to two different axes (i.e., histological lesions and anatomical structures), we used the SNOMED CT structure to identify transversal relations between their entities (resolution of open conflicts). During the process, the structure of the SNOMED CT was also used to suppress erroneous mappings (naming and confusion conflicts) and disambiguate multiple mappings (scale conflicts).
Recent advances in artificial intelligence have seen limited adoption in systematic reviews,and much of the systematic review process remains manual, time-consuming, and expensive. Authors conducting systematic reviews face issues throughout the systematic review process. It is difficult and time-consuming to search and retrieve,collect data, write manuscripts, and perform statistical analyses. Screening automation has been suggested as a way to reduce the workload, but uptake has been limited due to a number of issues,including licensing, steep learning curves, lack of support, and mismatches to workflow. There is a need to better a lign current methods to the need of the systematic review community. Diagnostic test accuracy studies are seldom indexed in an easily retrievable way, and suffer from variable terminology and missing or inconsistently applied database labels. Methodological search queries to identify diagnostic studies therefore tend to have low accuracy, and are discouraged for use in systematic reviews. Consequently, there is a particular need for alternative methods to reduce the workload in systematic reviews of diagnostic test accuracy. In this thesis we have explored the hypothesis that automation methods can offer an efficient way tomake the systematic review process quicker and less expensive, provided we can identify and overcomebarriers to their adoption. Automated methods have the opportunity to make the process cheaper as well as more transparent, accountable, and reproducible.
This study concerns a comparative analysis of the syntactic units (syntagms) and the fundamental constructions of French and Persian, with regards to Controlled Languages and problematic and ambiguous cases for translation. After a historical survey of these languages and a brief presentation of the writing and phonetic systems of Persian, the ward classes (parts of speech) and their traditional and modern classifications are compared. The structures of determinant, nominal, adjectival, prepositional, adverbial and verbal syntagms and the nature of their component, as well as the fundamental constructions of the basic sentence in thesetwo languages are then analysed. During the study, as a result of translation tests carried out by Persian students, some problematic cases for translation have been recognized and analysed for a potential French-Persian controlled language. In the final synthesis, the syntagmatic structures and some instructions for developing a controlled language relating French and Persian languages have been assembled.
The aviation market is facing nowadays a fast growth of innovative airborne systems. Drone cargo, drone taxi, airships, stratospheric balloons, to cite a few, could be part of the next generation of air transportation. In the same time, Small and Medium-sized Enterprises (SMEs) are becoming more and more involved in designing and developing new forms of air transportation, transitioning from the traditional role of supplier to those of system designer and integrator. This situation changes drastically the scope of SMEs' responsibility. As integrators they become responsible for certification of the components and the manufacturing process, an area in which they have little experience. Certification mandates very specific knowledge, regarding the regulations, norms and standards. Certification is a mandatory process and a critical activity for the enterprises in the aerospace industry. It constitutes a major challenge for SMEs who have to take on this certification responsibility with only limited resources. In this thesis, two major needs are identified: methodological support is not easily available for SMEs; and certification requirements are not easily comprehensive and adaptable to each situation. We examine alternate paths, reducing the complexity and bringing one step closer to solving the problem for the innovative SMEs. The objective is to provide support so that they can be more efficient to comprehend and integrate rules, legislations and guidelines to their internal processes in a simpler way. This thesis proposes then a methodological approach to support such organisation. Developed in close cooperation with a French SME in this situation, the approach is composed of a set of models (metamodel, structural, and behavioural models) covered by a certification governance mechanism.
This trend is reinforced by the Cloud based economy that allows sharing of costs and resources. However, the lack of trust in such cloud environments, that involve higher security requirements, is often seen as a braking force to the development of such services. The objective of this thesis is to study the concepts of service orchestration and trust in the context of the Cloud. It proposes an approach which supports a trust model in order to allow the orchestration of trusted business process components on the cloud. The contribution is threefold and consists in a method, a model and a framework. The method categorizes techniques to transform an existing business process into a risk-aware process model that takes into account security risks related to cloud environments. The model formalizes the relations and the responsibilities between the different actors of the cloud. This allows to identify the different information required to assess and quantify security risks in cloud environments. The framework is a comprehensive approach that decomposes a business process into fragments that can automatically be deployed on multiple clouds. The framework also integrates a selection algorithm that combines security information with other quality of service criteria to generate an optimized configuration. Finally, the work is implemented in order to validate the approach. The framework is implemented in a tool. The security assessment model is also applied over an access control model. The last part presents the results of the implementation of our work on a real world use case.
The context of this PhD thesis is the mining of biomedical texts. The main purpose is the extraction of interactions between food and drugs. Indeed, food may have interactions with drugs, and those can lead to harmful consequences on the patient's health and well-being. But, contrary to drug-drug interactions, food-drug interactions are less known and studied. Hence, it is necessary to propose text mining methods for the analysis of scientific literature and for the extraction of information related to intractions between food and drugs. Informations available in existing terminologies and linked data should be used.
Our thesis has for objective the processing of the polysemic adjective applicable to the domain of Natural Language Processing (NLP) and particularly to automatic translation (French-Korean). The various meaning of the lexicon used in the stock market generates several different semantic interpretations and therefore several translations. To resolve this problem in NLP, we propose a disambiguisation model making use of co-textual information and more particularly the noun. The latter forms a noun clause with the adjective thus allowing one to remove the semantic ambiguity of the adjective and determine its correct meaning. The co-textual hints that we have used are the following: the syntactic order of the adjective, the property of the noun and the semantic class to which the noun belongs. Our work consists in attributing to each polysemic adjective selected the co-textual information mentioned above so that the machine can find its correct meaning. The methodology can be generalized to other parts of speech, we have already applied it to the verb and its co-text, the noun
The strong emergence of smartphones on human daily life as well as the high broadband access supplied by operators have triggered pervasive demand on video streaming mobile services, requiring the exploration of novel approaches on video content delivery. To afford video streaming services at sustainable quality, the idea of adjusting the streaming to the time-varying users' contexts has been actively investigated during the recent years. Today streaming solutions mostly rely on the user's contextual information such as his link capacity or his available bandwidth to provide an acceptable final QoE. Such contextual information can be easily acquired thanks to the existence of wireless sensors and dedicated smart applications on today mobile devices. Various studies on users' mobility patterns also showed that people daily routes exhibit a high degree of spatial and temporal regularity, especially on public transportation or on road ways to/from frequently visited places. Coupled with radio maps, these mobility patterns can give high accuracy on context predictability along users' trips. In this thesis, we analyse the impact of adapting video streaming to the user's context on the final QoE. We start by proposing CAMS (Context Aware Mode Switching),a context-aware resource allocation mechanism, for real (i.e, non adaptive) video streaming delivery to reduce the number of video stalling. CAMS is designed to be applied in a particular network topology under a particular mobility of users. Then,we explore the impact of knowing the future throughput variations on video quality adaptation and delivery cost in adaptive video streaming. We propose NEWCAST (a Nticipating qoE With threshold scheme And a Scending biTrate levels) as a proactive algorithm for cost adjustment and quality adaptation under the assumption of a perfect throughput estimation. We then extend the study to the case where throughput prediction errors may exist and propose a bench of adaptive algorithms inspired from NEWCAST. To explore the feasibility of implementing these algorithms in real world streaming, we conduct some experiments with the DASH-If Reference player in an emulated environment. Finally, we explore the impact of knowing the future throughput variations when exploited with machine learning on the global QoE enhancement in adaptive video streaming. We propose a closed-loop framework based on users' feedbacks to progressively learn their QoE profiles and to fittingly optimize video deliveries. This framework is in particular suited for heterogenous populations where the QoE profiles of users are quite different and unknown in advance.
This thesis offers a conventional model of the structure of a human-machine collaborative task to design an interactive system helping a human performing complex tasks. More specifically, we focus on tasks whose resolution is highly opportunistic and can not be planned. We introduce a task representation model based on dialogue games, dialogue patterns making it possible to describe the interaction structure with sequences of conventionally acceptable dialogue acts. We organize these dialogue patterns in states, structures describing the expected behaviors from each interlocutor during the different sub-tasks of the collaborative task. These states group dialogue patterns together in a coherent set in regard to the sub-task they are associated to and enrich them with locally relevant rules. They make it possible to describe the effects of the execution of a dialogue pattern by the interlocutors in a given context of the task. The states are used by the system to link an utterance of the human to the current state of the dialogue and of the task and to take the initiative of behaviors relevant with the interaction and helpful for the task. The decisions taken by the system are based on the concept of maturity, a value associated to each state representing the system's ability to take initiatives in this state. The decision model of the system is designed to be resilient and give as much freedom as possible to the user. This model is implemented in CoCoA, a system that collaborates with a human to assist him performing a complex task. From the study of a human-human dialogue corpus on a medical collaborative information retrieval task, we use our model to describe the task. This use case is implemented with CoCoA and an evaluation is performed by simulating a human user with an information need and varying its actions according to behaviors identified in the corpus.
This thesis proposes a morphosyntactic, prosodic and conversational analysis of discourse markers in presidential debates and televised political talkshows in the United States of America. Discourse markers, in correlation with gestures and intonation, contribute significantly to the construction of the verbal exchange between the various participants in mediated political discourse. For the analysis, the approach proposed by Haselow (2017) was adopted because it considers all the other approaches mentioned above and takes into account cognitive theories. Concerning the organisation of interaction during presidential debates or talkshows, discourse markers, accompanied by gestures and intonation, play an important role in the management of turn takings and in the sequential organization of actions. On the enunciative level, in correlation with manual gestures and gaze, they participate in expressing emphasis, reformulation and opposition. They also help candidates to draw the attention of their interlocutors in order to introduce a justification, a change of point of view, a reported speech, or with the aim of interrupting them. Discourse markers in presidential debates, also allow speakers to anticipate or consider the point of view of their interlocutors (passive or active) by positioning themselves in a logic of acceptance or opposition with them.
Video interpretation and understanding is one of the long-term research goals in computer vision. However, in order to deploy visual recognition systems on large-scale in practice it becomes important to address the scalability of the techniques. The main goal is this thesis is to develop scalable methods for video content analysis (eg for ranking, or classification).
Restoring natural speech in paralyzed and aphasic people could be achieved using a brain-computer interface controlling a speech synthesizer in real-time. The aim of this thesis was thus to develop three main steps toward such proof of concept. First, a prerequisite was to develop a speech synthesizer producing intelligible speech in real-time with a reasonable number of control parameters. We thus developed a speech synthesizer that produced intelligible speech from articulatory data. This was achieved by first recording a large dataset of synchronous articulatory and acoustic data in a single speaker. Then, we used machine learning techniques, especially deep neural networks, to build a model able to convert articulatory data into speech. This synthesizer was built to run in real time. Finally, as a first step toward future brain control of this synthesizer, we tested that it could be controlled in real-time by several speakers to produce intelligible speech from articulatory movements in a closed-loop paradigm. Second, we investigated the feasibility of decoding speech and articulatory features from neural activity essentially recorded in the speech motor cortex. We built a tool that allowed to localize active cortical speech areas online during awake brain surgery at the Grenoble Hospital and tested this system in two patients with brain cancer. Results show that the motor cortex exhibits specific activity during speech production in the beta and gamma bands, which are also present during speech imagination. The recorded data could be successfully analyzed to decode speech intention, voicing activity and the trajectories of the main articulators of the vocal tract above chance. Finally, we addressed ethical issues that arise with the development and use of brain-computer interfaces.
Born-analog documents contain enormous knowledge which is valuable to our society. For the purpose of preservation and easy accessibility, several digitisation projects have converted these documents into digital texts by using optical character recognition (OCR) software. A notable limitation is the fact that OCRed books are often split into pages with paragraphs, lines, and words. Thus, it is not convenient for users to navigate or search information inside books. Another constraint is that the accuracy of modern OCR engines on historical documents substantially decreases. Erroneous OCR output considerably impacts on the performance of search engines and natural language processing systems. This thesis facilitates access to historical digitised documents by addressing such problems. In order to facilitate access to historical documents, several approaches are proposed within this thesis, aiming to reconstruct the logical book structures and to improve the quality of digitised text. Experimental results show that our approach outperforms the state-of-the-art for both evaluation metrics. The major contribution of this thesis is to provide methodologies to reduce OCR errors. Common and different features between OCR errors and human misspellings are clarified for better designing post-OCR processing. Normally, a post-processing system detects and corrects remaining errors. However, it is reasonable to treat them separately in some applications which allow to filter out, flag, or selectively reprocess such data. Results reveal that the performance of our proposals is comparable to several strong baselines on English datasets of the first two rounds of the competition on post-OCR text correction organised in the International Conference on Document Analysis and Recognition in 2017 and 2019.
Social Media has changed the way we communicate between individuals, within organizations and communities. The availability of these social data opens new opportunities to understand and influence the user behavior. Therefore, Social Media Mining is experiencing a growing interest in various scientific and economic circles. In this thesis, we are specifically interested in the users of these networks whom we try to characterize in two ways: (i) their expertise and their reputations and (ii) the sentiments they express. Conventionally, social data is often mined according to its network structure. However, the textual content of the exchanged messages may reveal additional knowledge that can not be known through the analysis of the structure. Until recently, the majority of work done for the analysis of the textual content was proposed for English. The originality of this thesis is to develop methods and resources based on the textual content of the messages for French Social Media Mining. In the first axis, we initially suggest to predict the user expertise. For this, we used forums that recruit health experts to learn classification models that serve to identify messages posted by experts in any other health forum. We demonstrate that models learned on appropriate forums can be used effectively on other forums. Then, in a second step, we focus on the user reputation in these forums. The idea is to seek expressions of trust and distrust expressed in the textual content of the exchanged messages, to search the recipients of these messages and use this information to deduce users'reputation. We propose a new reputation measure that weighs the score of each response by the reputation of its author. Automatic and manual evaluations have demonstrated the effectiveness of the proposed approach. In the second axis, we focus on the extraction of sentiments (emotions and polarity). For this, we started by building a French lexicon of sentiments and emotions that we call FEEL (French Expanded Emotions Lexicon). This lexicon is built semi-automatically by translating and expanding its English counterpart NRC EmoLex. We then compare FEEL with existing French lexicons from literature on reference benchmarks. The results show that FEEL improves the classification of French texts according to their polarities and emotions. Finally, we propose to evaluate different features, methods and resources for the classification of sentiments in French. The conducted experiments have identified useful features and methods in the classification of sentiments for different types of texts. The learned systems have been particularly efficient on reference benchmarks. Generally, this work opens promising perspectives on various analytical tasks of Social Media Mining including: (i) combining multiple sources in mining Social Media users; (ii) multi-modal Social Media Mining using not just text but also image, videos, location, etc. and (iii) multilingual sentiment analysis.
Kernel methods are known to be effective to analyse complex objects by implicitly embedding them into some feature space. This work proposes a pre-image estimation method for time series kernel analytics that consists of two steps. In the first step, a time warp function, driven by distance constraints in the feature space, is defined to embed time series in a metric space where analytics can be performed conveniently. In the second step, the time series pre-image estimation is cast as learning a linear (or a nonlinear) transformation that ensures a local isometry between the time series embedding space and the feature space. The proposed method is compared to state of the art through three major tasks that require pre-image estimation: 1) time series averaging, 2) time series reconstruction and denoising, and 3) time series representation learning.
We then try to give as much colors as possible on the credit default swap market, a relatively unknown market from the general public but for its role in the contagion of bank failures during the global financial crisis of 2007-2008, while introducing the datasets that have been used in the empirical studies. For that purpose, we discuss their consistency and propose alternative measures of similarity that can be plugged in the clustering methodologies. We study empirically their impact on the clusters. Results of the empirical studies can be explored at www.datagrapple.com.
Linguistic skills are essential for translators. However, a profound knowledge of source and target cultures is also fundamental for transmitting all the richness of a work. Researchers have been working on the importance of translating culture for thirty years, and the criteria they define are more and more precise. However, translation from Polish into French has been little studied. Therefore, we examine the expression of cultural and contextual knowledge in Polish-to-French translations. To do that, we mainly use a corpus of broadcasting texts and question the importance of both individual and national culture when the primary goal of text is not cultural transmission: how to translate such cultural elements? We focus our research on the works of the famous Polish fantasy writer Andrzej Sapkowski and their French translations. We also include thrillers and historical novels in our research. In Polish texts where the primary objective is not transmitting culture but entertaining a wide range of public,what kind of cultural and contextual knowledge is expressed, what importance does it have and how is it translated into French? We also question the impact of the absence of such cultural elements in the translations. To answer all these questions, we establish a classification of cultural references that can be considered as a possible typology for the cultureme in translation from Polish into French that may apply to other types of texts. For each category and subcategory, we analyze specific examples from our corpus of texts. This allows us to recognize most frequent practices in broadcasting literature translations, and perhaps offer new approaches.
In this computer-assisted analysis of herzog by saul bellow, three different domains are involved: literature, linguistics and computer sciences.
The notoriety of clickbait can be partially attributed to misinformation as clickbait use an attractive headline that is deceptive, misleading or sensationalized. A major type of clickbait are in the form of spam and advertisements that are used to redirect users to web sites that sells products or services (often of dubious quality). Another common type of clickbait are designed to appear as news headlines and redirect readers to their online venues intending to make revenue from page views, but these news can be deceptive, sensationalized and misleading. In my Thesis, I propose two innovative approaches to explore clickbait generated by news media in social media.
Image Retrieval is still a very active field of image processing as the number of available image datasets continuously increases. One of the principal objectives of Content-Based Image Retrieval (CBIR) is to return the most similar images to a given query with respect to their visual content. Our work fits in a very specific application context: indexing small expert image datasets, with no prior knowledge on the images. Because of the image complexity, one of our contributions is the choice of effective descriptors from literature placed in direct competition. Two strategies are used to combine features: a psycho-visual one and a statistical one. In this context, we propose an unsupervised and adaptive framework based on the well-known bags of visual words and phrases models that select relevant visual descriptors for each keypoint to construct a more discriminative image representation. Experiments show the interest of using this this type of methodologies during a time when convolutional neural networks are ubiquitous. We also propose a study about semi interactive retrieval to improve the accuracy of CBIR systems by using the knowledge of the expert users.
Advances in computer technology and recent advances in sensing and storage technology have created many high-volume, high-dimensional data sets. This increase in both the volume and the variety of data calls for advances in methodology to understand, process, summarize and extract information from such kind of data. From a more technical point of view, understanding the structure of large data sets arising from the data explosion is of fundamental importance in data mining and machine learning. Unlike supervised learning, unsupervised learning can provide generic tools for analyzing and summarizing these data sets when there is no welldefined notion of classes. In this thesis, we focus on three important techniques of unsupervised learning for data analysis, namely data dimensionality reduction, data clustering and data co-clustering. Our major contribution proposes a novel way to consider the clustering (resp. coclustering) and the reduction of the dimension simultaneously. The main idea presented is to consider an objective function that can be decomposed into two terms where one of them performs the dimensionality reduction while the other one returns the clustering (resp. We have further introduced the regularized versions of our approaches with graph Laplacian embedding in order to better preserve the local geometry of the data. Finally, we have investigated the integration of some selected instance-level constraints in the graph Laplacians of both data samples and data features. By doing that, we show how the addition of priory knowledge can assist in data co-clustering and improves the quality of the obtained co-clusters.
In this thesis I shall deal with the issues of confidence estimation for machine translation and statistical machine translation of large vocabulary spontaneous speech translation. I review the performances yielded by different techniques, present the results obtained during the WMT2012 internation evaluation campaign and give the details of an application to post edition of automatically translated documents. I then deal with the issue of speech translation. After going into the details of what makes it a very specific and particularly challenging problem, I present original methods to partially solve it, by using phonetic confusion networks, confidence estimation techniques and speech segmentation.
To meet carbon reduction goals in Europe but worldwide too, a large number of renewable distributed energy resources (DER) still need to be deployed. Aiming at mobilizing private capitals, several plans have been developed to put end-customers at the heart of the energy transition, hoping to accelerate the adoption of green energy by increasing its attractiveness and profitability. These prosumers will still be connected to the main power grid and they will have the option, as they do today, to buy and sell to/from their utility company at a fixed price (a flat rate or a Time-of-Use, for example). For these agents to fully benefit from the advantages of local energy trading, we shall assume that they own appliances (such as batteries) that, without changing their perceived energy demand, can enable them to change their net energy demand as seen from outside their homes. Modeling prosumers as rational utility maximizers, they will schedule their battery to decrease the cost associated with their net energy demand (as their perceived demand remains unchanged).In the first part of the thesis, we investigate competitive models in which prosumers sell their surplus to their neighbors via a local energy market. For the studied model we show that a stable solution (in the core of the game) exists in which all participants cooperate and we provide an efficient algorithm to find it. Furthermore, we also show that cooperation is stable for participants that already own batteries and PVs but prefer to operate them in coordination to increase their value, effectively implementing collective auto-consumption. For this later model, we propose to decouple the return over investments (ROI) obtained between the ROI produced by the investment in hardware and the ROI obtained by cooperation itself. By doing so, we can offer the former profit to external investors to raise the required capital (although nothing forbids the member of the coalition to contribute) and the latter to the actual consumers.
This thesis, carried out within the framework of a CIFRE contract with the OctopusMind company, is focused on developing a set of automated tools dedicated and optimized to assist call for tender databases processing, for the purpose of strategic intelligence monitoring. It contains more than 2 million documents translated into 24 languages published over the last 9 years. The second chapter presents a study on the questions of words, sentences and documents embedding, likely to capture semantic features at different scales. We proposed two approaches: the first one is based on a combination between a word embedding (word2vec) and latent semantic analysis (LSA). The second one is based on a novel artificial neural network architecture based on two-level convolutional attention mechanisms. These embedding methods are evaluated on classification and text clustering tasks. The end of the third chapter concerns the application aspect, in particular with the implementation of some solutions deployed within OctopusMind's software environment, including information extraction, a recommender system, as well as the combination of these different modules to solve some more complex problems
The operationalization of terminologies for information processing purposes requires a computational representation of the conceptual system. The theory of concept of the theory currently defined in ISO Terminology does not allow a computer representation [Roche TKE 2012]. Although ISO standards on Terminology does not aim to operationalize terminologies but communication between humans and used to model information and data [ISO 704: 2009]. The results of disciplines such as engineering knowledge helped to highlight the need for a theory of concept that could lead to a computer representation. In this context, ontologies, from knowledge engineering, is one of the most interesting perspectives to model the conceptual system of Terminology [Roche, 2015, Handbook of Terminology]
This thesis presents new computational tools for quantifying deformations and motion of anatomical structures from medical images as required by a large variety of clinical applications. Generic deformable registration tools are presented that enable deformation analysis useful for improving diagnosis, prognosis and therapy guidance. These tools were built by combining state-of-the-art medical image analysis methods with cutting-edge machine learning methods. First, we focus on difficult inter-subject registration problems. Second, we develop a diffeomorphic deformation model that allows for accurate multiscale registration and deformation analysis by learning a low-dimensional representation of intra-subject deformations. The unsupervised method uses a latent variable model in form of a conditional variational autoencoder (CVAE) for learning a probabilistic deformation encoding that is useful for the simulation, classification and comparison of deformations. Third, we propose a probabilistic motion model derived from image sequences of moving organs. This generative model embeds motion in a structured latent space, the motion matrix, which enables the consistent tracking of structures and various analysis tasks. For instance, it leads to the simulation and interpolation of realistic motion patterns allowing for faster data acquisition and data augmentation. Finally, we demonstrate the importance of the developed tools in a clinical application where the motion model is used for disease prognosis and therapy planning. It is shown that the survival risk for heart failure patients can be predicted from the discriminative motion matrix with a higher accuracy compared to classical image-derived risk factors.
Expert finding consists in the identification of a set of individuals who are considered tobe experts in a particular topic. This is an essential problem in the academic world. Indeed,it is constantly necessary to identify suitable researchers when setting up reading orevaluation committees for research projects, for example. Indeed, it is particularly useful to automatically identify experts on a specific field from the scientific literature. We suggest an approach for knowledge discovery and enrichment based on a semantic annotation of scientific articles, on their representation in the form of scientific collaboration networks and their exploration using a graph abstraction method. This method makes it possible to focus on dense areas of networks and to discover experts and their associated expertise using connectivity constraints. The latter make it possible to take into account a validationby peers, materialized by the density of scientific collaboration relations that individuals maintain with each other. We test our approach on a corpus of scientific publications, propose an original method for evaluating our results and compare our performance to expert research methods implemented in the LT ExpertFinder evaluation framework. We obtain better performance than the state of the art and discover that the most decisive indicators of expertise are the writing of highly cited articles but also the ability to cite the appropriate scientific literature.
The success of deep learning in recent year has depended in large part on large-scale annotated data collection which is very expensive and severely limit the amount of available data. Self-supervised learning machines have become popular in the computer vision, machine learning, and natural language processing communities in the past ten years. They leverage the power of supervised learning machinery but request much less human intervention since the supervisory signal is provided by the input data itself (e.g., predicting missing frames from a video) without additional manual annotation. A promising alternative is to consider instead a family of techniques that measure the discrepancy between predicted and actual inputs using a function parameterized by some latent variable, regularized to avoid trivial solutions. In particular the corresponding implicit model allows for multimodal predictions that reflect non-deterministic aspects of the real world. This thesis will develop novel techniques adapting this framework to computer vision, and apply them to tasks such as self-supervised learning of photoconsistency functions for multi-view stereo, and self-supervised learning of object models for visual recognition.
As image recognition systems are becoming more and more relevant, researchers in artificial intelligence now seek for the next generation vision systems that can perform high-level scene understanding. In this thesis, we are interested in Visual Question Answering (VQA), which consists in building models that answer any natural language question about any image. To tackle this problem, typical approaches involve modern Deep Learning (DL) techniques. In the first part, we focus on developping multi-modal fusion strategies to model the interactions between image and question representations. More specifically, we explore bilinear fusion models and exploit concepts from tensor analysis to provide tractable and expressive factorizations of parameters. These fusion mechanisms are studied under the widely used visual attention framework: the answer to the question is provided by focusing only on the relevant image regions. In the last part, we move away from the attention mechanism and build a more advanced scene understanding architecture where we consider objects and their spatial and semantic relations. All models are thoroughly experimentally evaluated on standard datasets and the results are competitive with the literature.
Query evaluation over probabilistic databases (probabilistic query evaluation, or PQE) is known to be intractable in many cases, even in data complexity, i.e., when the query is fixed. Although some restrictions of the queries and instances have been proposed to lower the complexity, these known tractable cases usually do not apply to combined complexity, i.e., when the query is not fixed. My thesis investigates the question of which queries and instances ensure the tractability ofPQE in combined complexity. My first contribution is to study PQE of conjunctive queries on binary signatures, which we rephrase as a probabilistic graph homomorphism problem. We restrict the query and instance graphs to be trees and show the impact on the combined complexity of diverse features such as edge labels, branching,or connectedness. While the restrictions imposed in this setting are quite severe, my second contribution shows that,if we are ready to increase the complexity in the query, then we can evaluate a much more expressive language on more general instances. Specifically, I show that PQE for a particular class of Data log queries on instances of bounded tree width can be solved with linear complexity in the instance and doubly exponential complexity in the query. To prove this result, we use techniques from tree automata and knowledge compilation. The third contribution is to show the limits of some of these techniques by proving general lower bounds on knowledge compilation and tree automata formalisms.
CSR reporting standardisation has become an increasing issue for companies when they address their stakeholders. GRI standards are, at the moment, the most widely-used standards for CSR reporting. G3C corpus has been built to be representative of the genre " GRI standardised CSR reports ".
A huge amount of data is collected during the course of clinical care in electronic health records (EHRs) softwares. EHRs can be useful data sources to support clinical research and can eliminate the need for duplicate data collection. However secondary use of healthcare data is still technically difficult because of the heterogeneity between systems in a hospital. In 2015, the Bordeaux Hospital University Center created an i2b2 data warehouse to integrate data from several disparate sources. Still, many barriers remain to the reuse of routinely recorded clinical data like the unstructured nature of data and the semantic heterogeneity of data sources.
Architecting seeks now to be distinct from its original domain, systems engineering, becoming an emergent domain. Far from being recognized as a science or a discipline, its practice is nowadays more and more widespread. However, this practice is still poorly formalized, and insufficiently being taught, lacking a well-established and accessible corpus of knowledge, techniques or approaches. This thesis contributes filling that gap by proposing a paradigm of the architectural design of artificial complex systems. The latter is built based on existing paradigms that are combined, then completed. It aims at providing architects with an effective, even performative framework. It results in an approach of the architectural design structured in four levels. A so-called archetypal level grasps the core principles of any approach of architectural design of artificial complex systems. These principles are derived from various approaches already applied, mainly in the field of system or product design, but also of architectural design of buildings. This vision of the architect's way of working directly impacts the kind of artefacts he handles. We sho how to aggregate these artefacts into models, reflecting either his perception of the present, or his development of futures while progressing through some identified processes. A so-called particular level aims at allowing the storytelling of a given design. To achieve this goal, a notation of the design process is suggested.
Many efforts have been done these last two decades to facilitate the management and representation of cultural heritage data. However, many systems used in cultural institutions are still based on flat models and are generally isolated which prevents any reuse or validation of information. This Ph.D. aims at proposing new solutions for enhancing the representation and enrichment of cultural entities using the Semantic Web technologies. This study is based on a real-world case study using the concepts from the Functional Requirements for Bibliographic Records (FRBR) which allows to generate graph-based knowledge bases. However, in this case, the aggregation of information from heterogeneous sources requires additional steps to match and merge both correspondences at schema and instance level
The collection of palm leaf manuscripts is an important part of Southeast Asian people's culture and life. Following the increasing of the digitization projects of heritage documents around the world, the collection of palm leaf manuscripts in Southeast Asia finally attracted the attention of researchers in document image analysis (DIA). The research work conducted for this dissertation focused on the heritage documents of the collection of palm leaf manuscripts from Indonesia, especially the palm leaf manuscripts from Bali. This dissertation took part in exploring DIA researches for palm leaf manuscripts collection. This collection offers new challenges for DIA researches because it uses palm leaf as writing media and also with a language and script that have never been analyzed before. These systems aim at making palm leaf manuscripts more accessible, readable and understandable to a wider audience and, to scholars and students all over the world. This research developed a DIA system for document images of palm leaf manuscripts, that includes several image processing tasks, beginning with digitization of the document, ground truth construction, binarization, text line and glyph segmentation, ending with glyph and word recognition, transliteration and document indexing and retrieval. We also developed the glyph recognition system and the automatic transliteration system for the Balinese palm leaf manuscripts. This dissertation proposed a complete scheme of spatially categorized glyph recognition for the transliteration of Balinese palm leaf manuscripts. The proposed scheme consists of six tasks: the text line and glyph segmentation, the glyph ordering process, the detection of the spatial position for glyph category, the global and categorized glyph recognition, the option selection for glyph recognition and the transliteration with phonological rules-based machine. An implementation of knowledge representation and phonological rules for the automatic transliteration of Balinese script on palm leaf manuscript is proposed.
In this thesis, I investigate how lexical resources based on the organisation of lexical knowledge in classes which share common (syntactic, semantic, etc. features support natural language processing and in particular symbolic recognition of textual entailment. First, I present a robust and wide coverage approach to lexico-structural verb paraphrase recognition based on Levin's (1993) classification of English verbs. Then, I show that by extending Levin's framework to general inference patterns, a classification of English adjectives can be obtained that compared with previous approaches, provides a more fine grained semantic characterisation of their inferential properties. Further, I develop a compositional semantic framework to assign a semantic representation to adjectives based on an ontologically promiscuous approach (Hobbs, 1985) and thereby supporting first order inference for all types of adjectives including extensional ones. Finally, I present a test suite for adjectival inference I developed as a resource for the evaluation of computational systems handling natural language inference.
With the availability of massive amounts of digital images in personal and on-line collections, effective techniques for navigating, indexing and searching images become more crucial. In this thesis, we rely on the image visual content as the main source of information to represent images. First, we enhance the BOW representation by characterizing the spatial-color constitution of an image with a mixture of n Gaussians in the feature space. This leads to propose a novel descriptor, the Edge Context, which plays a role as a complementary descriptor in addition to the SURF descriptor. Second, we introduce a new probabilistic topic model, Multilayer Semantic Significance Analysis (MSSA) model, in order to study a semantic inference of the constructed visual words. Consequently, we generate the Semantically Significant Visual Words (SSVWs). Finally, we propose a new spatial weighting scheme and a Multiclass Vote-Based Classifier (MVBC) based on the proposed SSVIG representation. The large-scale extensive experimental results show that the proposed higher-level visual representation outperforms the traditional part-based image representations in retrieval, classification and object recognition.
This thesis is in the context of the ANR project GEONTO covering the constitution, alignment, comparison and exploitation of heterogeneous geographic ontologies. The goal is to automatically extract terms from topographic travelogues to enrich a geographical ontology originally designed by IGN. The proposed method allows identification and extraction of terms contained in a text with a topographical connotation. Our method is based on a model that relies on certain grammatical relations to locate these terms. The implementation of this model requires the use of methods or techniques of NLP (Processing of Language). Our model represents the relationships between terms to extract and other elements of the texts that can be identified by using external predefined resources, such as specific lexicons: verbs of travelogue (verbs of displacement, verbs of perceptions, topographical verbs), pre-positions (prepositions of place, adverbs, adjectives), place name, generic thesauri, ontologies of domain (in our case the geographical ontology originally designed by IGN). The advantage of our approach is that the method can extract not only the terms related directly to place name but also those embedded in sentence structure in which other terms coexisted. Experiments on a corpus consisting of 12 travel stories (2419 pages, provided by the library of Pau) showed that our method is robust. As a result, it was used to extract 2173 distinct terms with 1191 valid terms, with a precision of 0.55. This demonstrates that the use of the proposed relationships is more effective than that of couples (term, place name) (which gives 733 distinct terms valid with an accuracy of 0.38). Our method can also be used for other applications such as geographic named entity recognition, spatial indexing of textual documents.
The integration of Information and Communication Technologies in Education (ICTEs) has often been described as a thorny endeavour. Digital Work Environments (DWE) can be considered as a particularly interesting ICTE tool because of their widespread and compulsory deployment in the French educational system. This study investigates how the DWE tool has integrated professional representations based on the theoretical approach of social representations. We use a lexicometric approach in order to identify the transversal themes and the idiosyncrasies of these discourses. The second part of this study is based on a survey by questionnaires on 625 secondary education teachers of Toulouse academy. Here, the hierarchical evocation method is used to examine the content of professional representations on four objects: the DWE, the teaching profession, the notions of information and of communication. The structure analyses of the responses and the interrelations study between these objects show the peculiarity of the teachers'universes compared to the ones identified in the three social discourses. The complex system formed by the four objects allows us to observe that although the representations of the teaching profession and the DWE have few common contents, they share numerous elements with the representations of communication and of information. Finally, these results enlighten us on two larger tendencies on the way secondary teachers describe the investigated objects: one part of the population seem to systematically privilege functional elements whereas the other part of the population prefers to associate evaluative elements when relating to each one of these objects.
A nautical chart is a kind of map used to describe the seafloor morphology and the shoreline of adjacent lands. One of its main purposes is to guaranty safety of maritime navigation. As a consequence, construction of a nautical chart follows very specific rules. The cartographer has to select and highlight undersea features according to their relevance to navigation. In an automated process, the system must be able to identify and classify these features from the terrain model. An undersea feature is a subjective individuation of a part of the seafloor. Landform recognition is a difficult task because its definition usually relies on a qualitative and fuzzy description. Achieving automatic recognition of landforms requires a formal definition of the landforms properties and their modelling. This terminology is here used as a starting point for the automatic classification of the features from a terrain model. In order to integrate knowledge about the submarine relief and its representation on the chart, this research aims to define ontologies of the submarine relief and nautical chart. Then, the ontologies are applied to generalisation of nautical chart. In the first part of the research, an ontology is defined to organize geographical and cartographic knowledge for undersea feature representation and nautical chart generalisation. First, a domain ontology of the submarine relief introduces the different concepts of undersea features with their geometric and topological properties. This ontology is required for the classification of features. Second, a representation ontology is presented, which describes how bathymetric entities are portrayed on the map. Third, a generalisation process ontology defines constraints and operations in nautical chart generalisation. In the second part, a generalisation process based on the ontology is designed relying on a multi-agent system. Four kinds of agents (isobath, sounding, feature and group of features) are defined to manage cartographic objects on the chart. A database model was generated from the ontology. At first, geometrical properties describing the feature shape are computed from soundings and isobaths and are used for feature classification. Then, conflicts are evaluated in a MAS and generalisation plans are provided.
The aim of this thesis is to propose a speech emotion recognition (SER) system for application in classroom. This system has been built up using novel features based on the amplitude and frequency (AM-FM) modulation model of speech signal. This model is based on the joint use of empirical mode decomposition (EMD) and the Teager-Kaiser energy operator (TKEO). In this system, the discrete (or categorical) emotion theory was chosen to represent the six basic emotions (sadness, anger, joy, disgust, fear and surprise) and neutral emotion. Automatic recognition has been optimized by finding the best combination of features, selecting the most relevant ones and comparing different classification approaches. Two reference speech emotional databases, in German and Spanish, were used to train and evaluate this system. A new database in French, more appropriate for the educational context was built, tested andvalidated.
Disease staging and monitoring of chronic lung diseases are two major challenges for patient care and evaluation of new therapies. Monitoring mainly relies on pulmonary function testing but morphological assessment is a key point for diagnosis and staging In the first part, we propose different models to score bronchial disease severity on computed tomography (CT) scan. A simple thresholding approach using adapted thresholds and a more sophisticated machine learning approach with radiomics are evaluated In the second part, we evaluate deep learning methods to segment lung fibrosis on chest CT scans in patients with systemic sclerosis. We combine elastic registration to atlases of different thoracic morphology and deep learning to produce a model performing as well as radiologists In the last part of the thesis, we demonstrate that lung deformation assessment between inspiratory and expiratory magnetic resonance images can be used to depict fibrotic lung areas, which show less deformation during respiration and that CT assessment of lung deformation on serial CT scans can be used to diagnose lung fibrosis worsening
In this work, we focus on eyebrows movements in face to face interaction as a resource usedby one speaker to inform the other that there is an understanding problem. Our aim is to describe the interactional sequence in which this movement appears, and to describe the interactional trajectories used by the speaker to solve the misunderstanding that took place. This method consists in first annotating the eyebrows movements (raising and frowning) that are produced as a response by the interlocutor in 3 different interaction corpora. Then, the annotation of the different sequences that include an understanding problem is made from pre-existing criteria from the literature (Weigand, 1999 ; Antaki, 2012).This work will allow a better understanding of the mechanisms of social interaction, taking all of the multimodal complexity that they imply into account. This study sheds light on the role played by eyebrows movements as a practice to initiate understanding-problem sequences, and also as a practice used as a disalignment and realignment cues during the aforementioned sequences.
This work is introduced in the framework of natural language processing. Two results were obtained: first, the negation causes coherence in the direction where it east defines as operation within the "notional domain" introduced by the speech. The structure of "domain" facilitates the interpretation of the negative statements because it makes it possible to express them in a positive way. Second, the negation causes inconsistency and it can be a factor of construction of a new space or world. We proposed a fonnalisation of the two types of negations in the oriented-object formalism, whose theoretical bases are the lesniewski's logical systems. The concept of domain was introduced into the two universes (intension and extension) of the model, in terms of the calculus of the names and the mereology. This formalisation can enable us to return account various inferential mechanisms of the negation.
Most of text-classification methods use the ``bag of words” paradigm to represent texts. However Bloahdom and Hortho have identified four limits to this representation: (1) some words are polysemics, (2) others can be synonyms and yet differentiated in the analysis, (3) some words are strongly semantically linked without being taken into account in the representation as such and (4) certain words lose their meaning if they are extracted from their nominal group. To overcome these problems, some methods no longer represent texts with words but with concepts extracted from a domain ontology (Bag of Concept), integrating the notion of meaning into the model. Models integrating the bag of concepts remain less used because of the unsatisfactory results, thus several methods have been proposed to enrich text features using new concepts extracted from knowledge bases. Using the naive Bayes classifier algorithm, I tested and compared my contributions on the Ohsumed corpus using the domain ontology ``Disease Ontology”. The satisfactory results led me to analyse more precisely the role of semantic relations in the enrichment step. These new works have been the subject of a second experiment in which we evaluate the contributions of the hierarchical relations of hypernymy and hyponymy.
With the evolution of technology, the use of smart Internet-of-Things (IoT) devices, sensors, and social networks result in an overwhelming volume of IoT data streams, generated daily from several applications, that can be transformed into valuable information through machine learning tasks. In practice, multiple critical issues arise in order to extract useful knowledge from these evolving data streams, mainly that the stream needs to be efficiently handled and processed. In this context, this thesis aims to improve the performance (in terms of memory and time) of existing data mining algorithms on streams. We focus on the classification task in the streaming framework. The first part of the thesis surveys the current state-of-the-art of the classification and dimensionality reduction techniques as applied to the stream setting, by providing an updated view of the most recent works in this vibrant area. In the second part, we detail our contributions to the field of classification in streams, by developing novel approaches based on summarization techniques aiming to reduce the computational resource of existing classifiers with no--or minor--loss of classification accuracy. To address high-dimensional data streams and make classifiers efficient, we incorporate an internal preprocessing step that consists in reducing the dimensionality of input data incrementally before feeding them to the learning stage. We present several approaches applied to several classifications tasks: Naive Bayes which is enhanced with sketches and hashing trick, k-NN by using compressed sensing and UMAP, and also integrate them in ensemble methods.
In the past years, combinatorial optimization techniques have been successfully applied to computationally challenging NLP tasks. We follow this line of work in the case of LTAG parsing. More precisely, in our setting, a given NLP problem is reduced to a subgraph selection problem. Then we formulate the generic graph problem as an Integer Linear Program. Integer Linear Programing has been widely studied and many optimization methods exist. We focus on Lagrangian relaxation which previously received much attention from the NLP community.
Information fusion systems are mainly composed from mathematical tools allowing to realize data representation and combination. The aim of these systems can be expressed as a decision problem on the truth or plausibility of a proposition based on several information coming from different sources. Fusion try to manage the characteristics of the sources taking into account the information imperfection (inaccurate, incomplete, ambiguous, uncertain, etc.) and the redundant aspect, the complement and the conflictual aspect of information. Fusion systems concerned by this thesis have the ability to integrate the expert knowledge in their treatments. They are called cooperative fusion systems. Since these systems are trying to associate experts, it is important to provide to the users some informations that help them to better understand the fusion process. One of the major problems associated to information fusion systems concerns the evaluation of their performance. A pertinent evaluation will allow to improve the quality of the fusion, to improve expert/system interaction and to better adjust the parameters of the system. Generally, the evaluation of such systems is made in the ouput of the processing chain by a global evaluation of the results. But it does not allow to know the precise subpart of the treatement chain that requires an adjustment of its parameters. Another difficulty releases in the fact that a complete ground truth of the result is not always available, which complicates the performance evaluation task. The application context of this work is the interpretation of 3D images (tomographic images, seismic images, synthetic images,...). In this context, a local evaluation of the information fusion systems has been implemented. The approach has shown its interest in the efficient adjustment of parameters and the cooperation with expert.
Nowadays it is very common to represent a system in terms of relationships between objects. One of the common applications of such relational data is Recommender System (RS), which usually deals with the relationships between users and items. Probabilistic Relational Models (PRMs) can be a good choice for modeling probabilistic dependencies between such objects. A growing trend in recommender systems is to add spatial dimensions to these objects, and make recommendations considering the location of users and/or items. This thesis deals with the (not much explored) intersection of three related fields – Probabilistic Relational Models (a method to learn probabilistic models from relational data), spatial data (often used in relational settings), and recommender systems (which deal with relational data). The first contribution of this thesis deals with the overlapping of PRM and recommender systems. We have proposed a PRM-based personalized recommender system that is capable of making recommendations from user queries in cold-start systems without user profiles. Our second contribution addresses the problem of integrating spatial information into a PRM.
The task of automatically extracting insights or building computational models from knowledge on complex systems greatly relies on the choice of appropriate representation. This work makes an effort towards building a framework suitable for representation of fragmented knowledge on complex systems and its semi-automated curation --- continuous collation, integration, annotation and revision. We propose a knowledge representation system based on hierarchies of graphs related with graph homomorphisms. Individual graphs situated in such hierarchies represent distinct fragments of knowledge and the homomorphisms allow relating these fragments. Their graphical structure can be used efficiently to express entities and their relations. We focus on the design of mathematical mechanisms, based on algebraic approaches to graph rewriting, for transformation of individual graphs in hierarchies that maintain consistent relations between them. Such mechanisms provide a transparent audit trail, as well as an infrastructure for maintaining multiple versions of knowledge. We describe how the developed theory can be used for building schema-aware graph databases that provide schema-data co-evolution capabilities. The proposed knowledge representation framework is used to build the KAMI (Knowledge Aggregation and Model Instantiation) framework for curation of cellular signalling knowledge. The framework allows for semi-automated aggregation of individual facts on protein-protein interactions into knowledge corpora, reuse of this knowledge for instantiation of signalling models indifferent cellular contexts and generation of executable rule-based models.
In many scientific fields, studied data have an underlying graph or manifold structure such as communication networks (whether social or technical), knowledge graphs or molecules. A graph is composed of nodes, also called vertices, connected together by edges. Recently, deep learning algorithms have become state-of-the-art models in many fields and in particular in natural language processing and image analysis. It led the way to a great line of studies to generalize deep learning models to graphs. In particular, several formulations of convolutional neural networks were proposed and research is carried to develop new layers and network architectures to graphs. These embeddings at different scales encode hierarchical representations of graphs. Based on these embedding techniques, we propose new deep learning architectures to tackle node classification or graph classification tasks.
Reusing healthcare administrative databases for public health research is relevant and opens new perspectives. This thesis deals with the joint use of healthcare administrative databases and biomedical knowledge for the study of patient care trajectories. This includes both (1) exploration and identification through queries of relevant care pathways in voluminous flows, and (2) analysis of retained trajectories. Semantic Web technologies and biomedical ontologies from the Linked Data allowed to identify care trajectories containing a drug interaction or a potential contraindication between a prescribed drug and the patient's state of health. In addition, we have developed the R queryMed package to enable public health researchers to carry out such studies by overcoming the difficulties of using Semantic Web technologies and ontologies. After identifying potentially interesting trajectories, knowledge from biomedical nomenclatures and ontologies has also enriched existing methods of analysing care trajectories to better take into account the complexity of data. This resulted notably in the integration of semantic similarities between medical concepts. Semantic Web technologies have also been used to explore obtained results.
It is a question of studying the United Nations discourse on the Iranian nuclear crisis during the ten years between 2005 and 2015. The study is conducted on a closed and predefined corpus, in order to discern the various linguistic and discursive processes that command the discourse. It is also a question of apprehending the stakes as well as the legal and political origins of this diplomatic crisis. Our major challenge is to understand the discourse in its multiple dimensions, linguistic, discursive, political and legal. These are the questions we respond in this thesis. The apprehension of linguistic and discursive impacts is realized in the light of the political and legal data that constitute an interpretive framework for the analysis. The objective is to identify the construction of the United Nations identity through notions of values, by discursive mechanisms.
The quantity of user-generated content on the Web is constantly growing at a fast pace. A great share of this content is made of opinions and reviews on products and services. This electronic word-of-mouth is also an important factor in decisions about purchasing these products or services. Users tend to trust other users, especially if they can compare themselves to those who wrote the reviews, or, in other words, they are confident to share some characteristics. For instance, families will prefer to travel in places that have been recommended by other families. We assume that reviews that contain lived experiences are more valuable, since experiences give to the reviews a more subjective cut, allowing readers to project themselves into the context of the writer. With this hypothesis in mind, in this thesis we aim to identify, extract, and represent reported lived experiences in customer reviews by hybridizing Knowledge Extraction and Natural Language Processing techniques in order to accelerate the decision process. Forthis, we define a lived user experience as an event mentioned in a review, where the authoris among the participants. This definition considers that mentioned events in the text are the most important elements in lived experiences: all lived experiences are based on events,which on turn are clearly defined in time and space. There fore, we propose an approach to extract events from user reviews, which constitute the basis of an event-based system to identify and extract lived experiences. For the event extraction approach, we transform user reviews into their semantic representations using machine reading techniques. We perform a deep semantic parsing of reviews, detecting the linguistic frames that capture complex relations expressed in there views. The event-based lived experience system is carried out in three steps. The first step operates an event-based review filtering, which identifies reviews that may contain lived experiences. The second step consists of extracting relevant events together with their participants. In order to test our hypothesis, we carried out some experiments to verify whether lived experiences can be considered as triggers for the ratings expressed by users. Therefore, we used lived experiences as features in a classification system, comparing with the ratings of the reviews in a dataset extracted and manually annotated from Tripadvisor. The results show that lived experiences are actually correlated with the ratings. In conclusion, this thesis provides some interesting contributions in the field of opinionmining. First of all, the successful application of machine reading to identify lived experiences. Second, the confirmation that lived experiences are correlated to ratings. Finally,the dataset produced to test our hypothesis constitutes also an important contribution of the thesis.
While deep neural networks are now used in almost every component of a speech recognition system, from acoustic to language modeling, the input to such systems are still fixed, handcrafted, spectral features such as mel-filterbanks. This contrasts with computer vision, in which a deep neural network is now trained on raw pixels. Mel-filterbanks contain valuable and documented prior knowledge from human auditory perception as well as signal processing, and are the input to state-of-the-art speech recognition systems that are now on par with human performance in certain conditions. However, mel-filterbanks, as any fixed representation, are inherently limited by the fact that they are not fine-tuned for the task at hand. We hypothesize that learning the low-level representation of speech with the rest of the model, rather than using fixed features, could push the state-of-the art even further. We first explore a weakly-supervised setting and show that a single neural network can learn to separate phonetic information and speaker identity from mel-filterbanks or the raw waveform, and that these representations are robust across languages. Moreover, learning from the raw waveform provides significantly better speaker embeddings than learning from mel-filterbanks. These encouraging results lead us to develop a learnable alternative to mel-filterbanks, that can be directly used in replacement of these features. In the second part of this thesis we introduce Time-Domain filterbanks, a lightweight neural network that takes the waveform as input, can be initialized as an approximation of mel-filterbanks, and then learned with the rest of the neural architecture. Across extensive and systematic experiments, we show that Time-Domain filterbanks consistently outperform melfilterbanks and can be integrated into a new state-of-the-art speech recognition system, trained directly from the raw audio signal. Fixed speech features being also used for non-linguistic classification tasks for which they are even less optimal, we perform dysarthria detection from the waveform with Time-Domain filterbanks and show that it significantly improves over mel-filterbanks or low-level descriptors. Finally, we discuss how our contributions fall within a broader shift towards fully learnable audio understanding systems.
The work, presented in this thesis, aims to propose solutions to the problems of textual data warehousing. The interest in the textual data is motivated by the fact that they cannot be integrated and warehoused by using the traditional applications and the current techniques of decision-making systems. In order to overcome this problem, we proposed a text warehouses approach which covers the main phases of a data warehousing process adapted to textual data. We focused specifically on the integration of textual data and their multidimensional modeling. For the textual data integration, we used information retrieval (IR) techniques and automatic natural language processing (NLP). Thus, we proposed an integration framework, called ETL-Text which is an ETL (Extract-Transform-Load) process suitable for textual data. The ETL-Text performs the extracting, filtering and transforming tasks of the original textual data in a form allowing them to be warehoused. Some of these tasks are performed in our RICSH approach (Contextual information retrieval by topics segmentation of documents) for pretreatment and textual data search. It extends the classical constellation model to support the representation of textual data in a multidimensional environment. TWM includes a semantic dimension defined for structuring documents and topics by organizing the semantic concepts into a hierarchy. Also, we depend on a Wikipedia, as an external semantic source, to achieve the semantic part of the model. Furthermore, we developed WikiCat, which is a tool permit to feed the TWM semantic dimension with semantics descriptors from Wikipedia. These last two contributions complement the ETL-Text framework to establish the text warehouse device. To validate the different contributions, we performed, besides the implementation works, an experimental study for each model. For the emergence of large data, we developed, as part of a case study, a parallel processing algorithms using the MapReduce paradigm tested in the Apache Hadoop environment.
The main goal of this thesis is to propose a complete framework for automatic discovery, modeling and recognition of human activities in videos. In order to model and recognize activities in long-term videos, we propose a framework that combines global and local perceptual information from the scene and accordingly constructs hierarchical activity models. In the first variation of the framework, a supervised classifier based on Fisher vector is trained and the predicted semantic labels are embedded in the constructed hierarchical models. In the second variation, to have a completely unsupervised framework, rather than embedding the semantic labels, the trained visual codebooks are stored in the models. Finally, we evaluate the proposed frameworks on two realistic Activities of Daily Living datasets recorded from patients in a hospital environment. Furthermore, to model fine motions of human body, we propose four different gesture recognition frameworks where each framework accepts one or combination of different data modalities as input. We evaluate the developed frameworks in the context of medical diagnostic test namely Praxis. We suggest a new challenge in gesture recognition, which is to obtain an objective opinion about correct and incorrect performances of very similar gestures. The experiments show effectiveness of our deep learning based approach in gesture recognition and performance assessment tasks.
This thesis aims at exploiting the potential of spatial and temporal data present at the heart of scientific papers, in order to introduce an ontology of spatial and temporal information contained in scientific papers from a Semantic Web point of view. These new metadata could serve the production of new representations in the form of graphs or text syntheses, with applications in geoparsing, chronological analysis and visualizations. This project responds to the need to exploit the information in scientific corpora at a large scale by the analysis and semantic processing of the full text of the articles.
The main topic of investigation for this CIFRE Ph.D., in partnership with the retail consultancy firm OneTeam, is Natural Language Processing (NLP). Its specificity is the conception and implementation of a ChatBot for internal use. More precisely, its use case will be to assist in the relationship between OneTeam's clients and its online customer service, to help handling technical tickets concerning client difficulties with OneTeam's retail support software. A ChatBot is a NLP interface between a human and the machine. It is supposed to understand a limited subset of natural language relevant to the application context, provide a dialogue-like interaction capability, and be able to translate the human requests into a either a semi-formal language (to be communicated to a technical manager), or a formal language (e.g. a search query), or a pragmatic context (e.g. implement a set of actions such as launch a piece of software or run a SQL query), or a combination thereof. In the context of this Ph.D. proposal, the ChatBot should understand an informal description of a client-side technical problem to do with one of the applications from OneTeam's software suites, and provide a conditional interface based on a boolean choice: either the description matches an existing issue in stored closed tickets database, or not. In the first case, the ChatBot should construct the formal pragmatic description necessary to solve the client problem at hand. In the second case, the ChatBot should translate the informal description to a semi-formal open ticket to be written in the ticket database. While the conception and deployment of ChatBots is not by itself new, the overall system required by OneTeam has some severely challenging features: 1. the informal description in input may be a transcription from a telephone message, which makes the text much more error-ridden, punctuation-challenged, and hence more difficult to understand; 2. the limited language to be understood includes several acronyms that may be mis-spelled in the informal description, as well as ungrammatical sentences; 3. in the case the ChatBot is to directly engender the solution, there is no margin of error in the understanding and translation to the correct formal pragmatics. Addressing these features will require scientific research at the interface between algorithmics and computational linguistics. A further challenge is that the text may be in French or English, with French being by far more likely. While this feature is in itself not necessarily original, there is a severely more limited amount of available NLP software resources in French as compared to English. This will undoubtedly call for some supplementary low-level NLP task conception, design and implementation.
This work focuses on the representations of adolescence in contemporary independent cinema. It is a question of examining the presentation in images and in discourse of this social phenomenon in six films coming from Europe and America and released in room between 2009 and 2014. By situating ourselves in the domains of Sciences of the Information and Communication, we articulate a semiotic and pragmatic approach to account for the plastic, aesthetic and communicational aspects of the films of the corpus. Our poetic and socio-political analysis simultaneously explores the cinematographic language, the conditions of production and the horizon of expectation to which the works refer. The first part sets out the theoretical categories and the method to follow to detect the relations between adolescence, cinema and representation. The second part captures the forms and effects of meaning of the films. The body is taken as the foundation of the representation of adolescence, a semiotic, social and political object. The third part focuses on the emergence of images of adolescent bodies and discourses likely to engender a look at cinematographic adolescence as a symbolic and social construction. The thesis concludes that these films constitute discourses on the adolescent universe, invite to deliberation, using a set of filmic forms that lead to reflections on the social, historical and cultural world that is deployed there.
Software Product Lines (SPLs) enable the derivation of a family of products based on variability management techniques. Inspired by the manufacturing industry, SPLs use feature configurations to satisfy different customer needs, along with reusable assets to allow systematic reuse. Capitalizing on existing variants by extracting the common and varying elements is referred to as extractive approaches for SPL adoption. Also, to identify the associated implementation elements of the features, their location is needed. In addition, feature constraints should be identified to guarantee that customers are not able to select invalid feature combinations. This dissertation presents Bottom-Up Technologies for Reuse (BUT4Reuse), a unified, generic and extensible framework for mining software artefact variants. Special attention is paid to model-driven development scenarios. We also focus on benchmarks and in the analysis of variants, in particular, in benchmarking feature location techniques and in identifying families of variants in the wild for experimenting with feature identification techniques. We present visualisation paradigms to support domain experts on feature naming and to support on feature constraints discovery. Finally, we investigate and discuss the mining of artefact variants for SPL analysis once the SPL is already operational. Concretely, we present an approach to find relevant variants within the SPL configuration space guided by end user assessments.
Our thesis proposes data mining within the framework of Natural Language Processing(NLP) according to the theory of Cognitive Grammar (Langacker 1987, 1991). Providing examples from French, English and Modern Greek, we investigate theappropriate units for data mining. A function is defined as an operation involving adependent morpheme, for ex. a verb or a suffix, and an autonomous morpheme, for ex.a noun ; certain functions are primary in the sense that they are psychologically more salient ; these are verbal or deverbal dependent morphemes that profile differently one same processual scene. In order to get an automatic data mining at the phrastic level, wefirst need to prepare dictionaries of inflection and of deverbal derivation. We are taking the first steps by analysing exhaustively the verbal inflection of English, French and Modern Greek and we build electronic dictionaries of all their inflected forms.
This work takes place in the research of the group sydo-lyon, which principal activity deals with information retrieval system design. A text indexing of french textual corpus is set up by the description of the noun phrases extracted from the texts and of syntactic connections between these noun phrases. The anaphora resolution is very important for automatic text analysis because some anaphora build up syntactic connections between a noun phrase antecedent and an anaphoric, and anaphora is a relation which contributes to represent text content as it points out some noun phrases (themes). The main prospect of this work is to identify and formalise principles of anaphora resolution, so that they can be used in a computational process. It has so to study relations between anaphora and coreference, to index all anaphoric forms and antecedent forms, and to propose solution for automatic recognizing anaphorics and antecedents in texts. At last, we present the realisation of a program in prolog which gives a solution for personal pronouns. In conclusion, this work now can be pursued with improving resolution of personal pronouns and elaboration of an automatic analysis of others anaphorics.
Question answering systems find and extract a precise answer to a question in natural language. Both current question-answering systems and evaluation campaigns often assume that only one single answeris expected for a question. Our corpus studies show that this is rarely the case, specially when answers are extracted from the Web instead of a frozen collection of documents. We therefore focus on questions expecting multiple correct answers from the Web by developping the question-answering system Citron. Citron is dedicated to extracting multiple answers in open domain and identifying the shifting criteria (date, location) which is often the reason of this answer multiplicity Our corpus studies show that the answers of this kind of questions are often located in structures such as tables and lists which cannot be analysed without a suitable preprocessing. Consequently we developed the Kitten software which aims at extracting text information from HTML documents and also both identifying and formatting these structures. We finally evaluate Citron through two experiments involving users. The first experiment evaluates both Citron and human beings on a multiple answer extraction task: results show that Citron was faster than humans and that the quality difference between answers extracted by Citron and humans was reasonable. The second experiment evaluates user satisfaction regarding the presentation of multiple answers: results show that user shave a preference for Citron presentation aggregating answers and adding the shifting criteria (if it exists) over the presentation used by evaluation campaigns.
MATLAB is a computing environment with an easy programming language and a vast library of functions commonly used in Computation Science and Engineering (CSE) for fast prototyping. However, some features of its environment, such as its dynamic language or interactive style of programming affect how fast the programs can execute. Current approaches to improve MATLAB programs either translate the code to faster static languages like C or Fortran, or apply code transformations to MATLAB code systematically without considering their impact on the performance. In this thesis, we fill this gap by developing techniques for the analysis and code transformation of MATLAB programs in order to improve their performance. More precisely, we analyse and model the behaviour of the black-box MATLAB environment by measuring the execution characteristics of programs on CPU. From the resulting data, we formalise a static model which predicts the type and order of instructions scheduled by the Just-In-Time (JIT)compiler. This model allows us to propose several code transformations which increase the performance of MATLAB programs by influencing how the JIT compiler generates the machine code. The obtained results demonstrate the practical benefits of the presented methodology.
With the rise of the so-called cognitive robotics, the need of advanced tools to store, manipulate, reason about the knowledge acquired by the robot has been made clear. But storing and manipulating knowledge requires first to understand what the knowledge itself means to the robot and how to represent it in a machine-processable way. In a second part, the thesis presents in depth a particular instantiation of a knowledge representation and manipulation system called ORO, that has been designed and implemented during the preparation of the thesis. We elaborate on the inner working of this system, as well as its integration into several complete robot control stacks. A particular focus is given to the modelling of agent-dependent symbolic perspectives and their relations to theories of mind. The third part of the study is focused on the presentation of one important application of knowledge representation systems in the human-robot interaction context: situated dialogue. The thesis concludes on considerations regarding the viability and importance of an explicit management of the agent's knowledge, along with a reflection on the missing bricks in our research community on the way towards "human level robots"
Our resarch lies in the wake of the efforts to build French Sign Language public announcement systems by virtual signers, using combined prerecorded chunks of utterances. Our study focuses on modelling coarticulation for it to be used in this system. We detail the various aspects of corpus creation and annotation, and annotation analysis. Quantitative and qualitative statistics allows us to propose a coarticulation model, based on tensions and relaxations of hand configurations. We propose and implement a methodology for evaluating our mode!. Finally, we propose prospects for this model in image processing and 3d character animation in French Sign Language.
As soon as the robots step out in the real and uncertain world, they have to adapt to various unanticipated situations by acquiring new skills as quickly as possible. Unfortunately, on robots, current state-of-the-art reinforcement learning (e.g., deep-reinforcement learning) algorithms require large interaction time to train a new skill. Our primary focus is to incorporate prior knowledge from a simulator with real-world experiences of a robot to achieve rapid learning and adaptation. In our first contribution, we propose a novel model-based policy search algorithm called Multi-DEX that (1) is capable of finding policies in sparse reward scenarios (2) does not impose any constraints on the type of policy or the type of reward function and (3) is as data-efficient as state-of-the-art model-based policy search algorithm in non-sparse reward scenarios. In our third contribution, we introduce a gradient-based meta-learning algorithm called FAMLE. FAMLE meta-trains the dynamical model of the robot from simulated data so that the model can be adapted to various unseen situations quickly with the real-world observations. By using FAMLE with a model-predictive control framework, we show that our approach outperforms several model-based and model-free learning algorithms, and solves the given tasks in less interaction time than the baselines.
With the advance of the Semantic Web and the Open Linked Data initiatives, a huge quantity of RDF data is available on Internet. The goal is to make this data readable for humans and machines, adopting special formats and connecting them by using International Resource Identifiers (IRIs), which are abstractions of real resources of the world. However, studies about anonymization in the context of RDF data, are really limited. These studies are initial works for protecting individuals on RDF data, since they show a practical anonymization approach for simple scenarios as the use of generalization and suppression operations based on hierarchies. However, for complex scenarios, where a diversity of data is presented, the existing anonymization approaches does not ensure an enough privacy. Thus, in this context, we propose an anonymization framework, which analyzes the neighbors according to the background knowledge, focused on the privacy of entities represented as nodes in the RDF data. Our anonymization approach is able to provide better privacy, since it takes into account the l-diversity condition as well as the neighbors (nodes and edges) of entities of interest. Also, an automatic anonymization process is provided by the use of anonymization operations associated to the datatypes.
The aims of this thesis are two-fold, and centered on synthetic metabolic circuits, which perform sensing and computation using enzymes. The first part consisted in developing reinforcement and active learning tools to improve the design of metabolic circuits and optimize biosensing and bioproduction. In order to do this, a novel algorithm (RetroPath3.0) based on similarity-guided Monte Carlo Tree Search to improve the exploration of the search space is presented. This algorithm, combined with data-derived reaction rules and varying levels of enzyme promiscuity, allows to focus exploration on the most promising compounds and pathways for bio-retrosynthesis. The second part consisted in developing analysis tools, to generate knowledge from biological data and model biosensor response. First, the effect of plasmid copy number on sensitivity of a transcription-factor based biosensor was modeled. Then, using cell-free systems allowing for broader control over the experimental factors such as DNA concentration, resource usage was modeled to ensure our current knowledge of underlying phenomenons is sufficient to account for circuit behavior, using either empirical models or mechanistic models. Coupled with metabolic circuit design, those models allowed us to develop a new biocomputation approach, called metabolic perceptrons. Overall, this thesis presents tools to design and analyse synthetic metabolic circuits, which are a novel way to perform computation in synthetic biology.
Building discourse parsers is currently a major challenge in Natural Language Processing. The identification of the relations (such as Explanation, Contrast...) linking spans of text in the document is the main difficulty. In this thesis, we use raw data to improve automatic identification of implicit relations. First, we propose to use discourse markers in order to automatically annotate new data. We report improvements on the English corpus Penn Discourse Treebank, and especially we show that this method alleviates the need for rich resources, available but for a few languages.
Neural networks have become a flagship algorithm of Artificial intelligence (AI), with major successes in solving complex tasks such as image recognition and game playing. Digital computers unfortunately consume an inordinate amount of energy when they run neural networks. For example, training a state-of-the art natural language processing model on a modern supercomputer consumes 1000 kW.h [1], which is the energy consumed by a human brain for the entirety of its tasks over a duration of six years. In the brain, neurons -- which can roughly be seen as carrying out the computation -- have a direct access to memory, supported by synapses. Current electronics, on which rely the GPUs and CPUs used in AI, intrinsically separates memory and computing into distinct physical units, between which data must be carried back and forth. This "von Neuman bottleneck" is an issue for artificial intelligence algorithms which require reading considerable amounts of data at each step, performing advanced operations on this data, and then writing the results back to memory [2],[4]. This process slows down computing and considerably increases the energy consumption for learning and inference. The general paradigm in neuromorphic computing is therefore to take inspiration from the topology of the brain to build circuits composed of physical neurons interconnected by physical synapses that implement memory in-situ, in a non-volatile way, thus drastically cutting the need to move data around the circuit and allowing huge gains in speed and energy efficiency. One of the major challenges in neuromorphic computing is to achieve on chip training of the networks in an efficient way. Implementing state-of-the art training algorithm such as backpropagation requires bulky and energy-hungry circuits to store activations, compute gradients, store gradients, then sequentially modify each physical synapse in the network. This is an immense hurdle towards the development of an embedded AI that can learn. For this reason, the AI chips developed today in academia and industry mostly target inference with off-chip learning [5],[7], or implement training algorithms with performance far from state-of-the-art gradient descent methods [8]. In this context, works by AI pioneers such as Geoffrey Hinton [9] and Yoshua Bengio [10],[12] to understand how gradient descent could be performed in the brain gives inspirational guidelines to achieve the same in neuromorphic systems. Algorithms such as Equilibrium propagation, first proposed by Scellier and Bengio [12], and E-Prop by Wolfgang Maas [13] show that in spiking neural networks gradients can be carried through the network directly by neuron activity. C2N and CNRS/Thales in collaboration with the team of Yoshua Bengio at Mila have shown that the gradients computed by Equilibrium Propagation are equal to those derived by backpropagation through time, and highlighted the potential of this result for training neuromorphic systems with state-of-the-art performance in a publication in NeurIPS last year (accepted as oral presentation) [14]. The ultimate objective of the thesis is to fabricate neuromorphic spiking circuits that learn locally and autonomously through algorithms such as Equilibrium Propagation that encode gradients in the spiking activity of neurons and can be directly applied to the synapses they connect. For this purpose, the doctoral student will first develop, code and test through simulations novel algorithms to train recurrent spiking neural networks, inspired by Equilibrium propagation but adapted to neuromorphic hardware. Then he or she will realize these networks in hardware, through lab-scale experiments and then in larger systems. The building blocks that will be used for developing this hardware are nanoscale electronic components called resistive switching devices or memristors (short for memory-resistors) [15]. They are made of an insulating oxide material sandwiched between two metallic electrodes (Fig. 1(a-b)). These nanoscale devices are very promising for neuromorphic computing because they can imitate both synapses and neurons with very low energy in a very small area. They exhibit different behaviours depending on the materials that are used for the oxide and the electrodes. They can be passive and exhibit non-volatile resistance switching (e.g., with Pt/TaOx/Pt heterostructures), which is useful for synaptic features (multilevel memory and plasticity, (Fig. 1(d)). They can also be active and display volatile switching through negative differential resistance (e.g., with Pt/NbOx/Pt heterostructures), which can generate oscillating neuron features (like spiking and bursting, Fig. 1(c)). C2N and CNRS/Thales have a long standing expertise in the fabrication and study of these nanodevices [3], [16],[20]. For now, studies have mostly focused on circuits that use these devices either as synapses, or as neurons, but not the two at the same time. It has been shown that memristive synapses can learn through the bio-inspired learning rule called Spike Timing Dependent Plasticity [3], [21]. It has also been demonstrated that memristive neurons can imitate many features of neurons including spiking and bursting [15]. The goal of the thesis is to assemble these individual components in spiking neural networks that learn autonomously through state-of-the-art algorithms emulating backpropagation in materio.
This research thesis is focusing in creativity, innovation and collaborative creative process. There is a need to build new technical and innovative systems that can emulate a valuable human behavior consisting of visualizing and giving life to their ideas. This approach involves creativity ability and involves, as a process, many concepts such as discovery, creation, sociability, refinement and communication. This research's missions to design informatics and technological tool to support creativity during the creative phases in a creativity workshop. To achieve the design of this tool, there are several objectives to achieve during the time requested for this research. The research context is a creativity workshop “48H”, mobilizing students, professors, industrials, experts working with ideas by mean of exploratory combinatory, and transformational techniques and collaborative creative methods. To build a tool for that kind of creative event, we need research how to consider the Human Knowledge Sharing Model and how a proposed System can develop a Semantic Approach to compare Idea Cards in 48H Creativity Workshop. The main purposes of this research are to understand the knowledge sharing during a creativity workshop and to study the human organization in a creativity workshop and its general process, particularly. We study the event 48 Hours of creativity (48H) as a creative and innovative process where ideas are the final product. This event is performed in Nancy, France at the University of Lorraine (ERPI Laboratory). We design an intelligent system based in multi-agents (MAS) also to develop and annotation system. We propose a semantic approach to manage ideas and the global design system. The practical implications of our model permit to use creativity, collaborative creative methods and new technologic modern methodologies, as new tools designed to build creative and innovative systems. Some pertinent fields and models take part on this research such as multi-agent system (MAS), semantic web, ontology, organizational model to highlight knowledge and knowledge reuse organizational model KROM.
With the massive increase of video content on Internet and beyond, the automatic understanding of visual content could impact many different application fields such as robotics, health care, content search or filtering. The goal of this thesis is to provide methodological contributions in Computer Vision and Machine Learning for automatic content understanding from videos. We emphasis on problems, namely fine-grained human action recognition and visual reasoning from object-level interactions. In the first part of this manuscript, we tackle the problem of fine-grained human action recognition. We introduce two different trained attention mechanisms on the visual content from articulated human pose. The first method is able to automatically draw attention to important pre-selected points of the video conditioned on learned features extracted from the articulated human pose. We show that such mechanism improves performance on the final task and provides a good way to visualize the most discriminative parts of the visual content. The second method goes beyond pose-based human action recognition. We develop a method able to automatically identify unstructured feature clouds of interest in the video using contextual information. Furthermore, we introduce a learned distributed system for aggregating the features in a recurrent manner and taking decisions in a distributed way. We demonstrate that we can achieve a better performance than obtained previously, without using articulated pose information at test time. In the second part of this thesis, we investigate video representations from an object-level perspective. Given a set of detected persons and objects in the scene, we develop a method which learns to infer the important object interactions through space and time using the video-level annotation only. That allows to identify important objects and object interactions for a given action, as well as potential dataset bias. Finally, in a third part, we go beyond the task of classification and supervised learning from visual content by tackling causality in interactions, in particular the problem of counterfactual learning. We introduce a new benchmark, namely CoPhy, where, after watching a video, the task is to predict the outcome after modifying the initial stage of the video. We develop a method based on object-level interactions able to infer object properties without supervision as well as future object locations after the intervention.
This study aims at a classification and analysis of the greek frozen (idiomatic) sentences. It is carried out within the lexicon-grammar framework and hopes to be a contribution to the greek lexicon-grammar elaboration. The adopted theoretical approach is that of the transformation grammar as defined by z. Harris and m. Gross. The syntactic structure of frozen sentences is generally that of free sentences and they undergo the same syntactic operations which are applied to free sentences. Thus, in this study, the frozen sentences are analyzed as their "free" (non-frozen) counterparts (e.g. Subject, object, etc.). The final classification includes 13 classes; these in turn, contain about 4500 entries. Besides the classification may be used for different applications such as natural language processing, translation and teaching.
Recognizing a speaker's opinions in an oral interaction is a crucial step in improving communication between a human and a virtual agent. In this thesis, we find ourselves in a problematic of automatic speech processing (APT) on opinion phenomena in natural spontaneous oral interactions. Opinion analysis is a task that is not often addressed in TAP that focused until recently on emotions using voice and non-verbal content. In addition, most existing legacy systems do not use the interactional context to analyze the speaker's opinions. In this thesis, we focus on these topics. We are in the context of automatic detection using statistical learning models. A study on modeling the dynamics of opinion by a model with latent states within a monologue, we study how to integrate the context interactional dialogical, and finally to integrate audio to text with different types of fusion. We worked on a basic Vlogs data at a global sense, and on the basis of multimodal data dyadic interactions composed of open conversations, at the turn of speech and word pair of towers. Finally, we annotated database in opinion because existing database were not satisfactory vis-à-vis the task addressed, and did not allow a clear comparison with other systems in the state art. At the dawn of significant change brought by the advent of neural methods, we study different types of representations: the ancient representations built by hand, rigid, but precise, and new representations learned statistically, and general semantics. We study different segmentations to take into account the asynchronous nature of multi-modality. Recently, we are using a latent state learning model that can adapt to a small database, for the atypical task of opinion analysis, and we show that it allows both an adaptation of the descriptors of the written domain to the oral domain, and serve as an attention layer via its clustering power. Complex multimodal fusion is not well managed by the classifier used, and audio being less impacting on opinion than text, we study different methods of parameter selection to solve these problems.
Conceptual models (CM) serve as the blueprints of information systems and their quality plays decisive role in the success of the end system. It has been witnessed that majority of the IS change-requests result due to deficient functionalities in the information systems. Therefore, a good analysis and design method should ensure that CM are correct and complete, as they are the communicating mediator between the users and the development team. Our approach targets the problems related to conceptual modeling quality by proposing a comprehensive solution. We designed multiple artifacts for different aspects of CM quality. Most of the existing literature on CM quality evaluation represents disparate and autonomous quality frameworks proposing non-converging solutions. ii. Formulation of quality patterns to encapsulate past-experiences and good practices as the selection of relevant quality criteria (including quality attributes and metrics) with respect to a particular requirement (or goal) remains trickier for a non-expert user. These quality patterns encapsulate valuable knowledge in the form of established and better solutions to resolve quality problems in CM. iii. Designing of the guided quality driven process encompassing methods and techniques to evaluate and improve the conceptual models with respect to a specific user requirement or goal. iv. Development of a software prototype “CM-Quality”.
There are several Berber languages in the south west of Algeria. Some of them are situated in the so-called Sud-Oranais and they can be categorized as endangered languages. So I have decided to describe them before they disappear. That's why, I have carried out several fieldworks. But, this linguistic documentation work and cultural heritage conservation are just one of aspects of our thesis. I have used the methods which are applied in Geographic Information Science (GIS) and in Data Science (DS) to carry out a dialectological study. A geolinguistic study has been undertaken and has enabled to visualize the expansion of the linguistic variation of certain consonants through GIS. Based on these data, I have debated the phonological reality of the simple and geminate consonants. From this research, a dialectometric study was carried out on the basis of data partitioning methods. I have used the Unsupervised Learning Methods (HAC, k-mean, MDS,...) and the Supervised Learning Methods (CART) known in DS. Then, I have undertaken a phonetic analysis, which is based on an acoustic study of alveolar rhotics: [ɾ], [r], [ɾˤ] and [rˤ]. These phonic unities are distinguished by their temporality and their articulatory realization. Thus, the spectrograms enabled to examine the distribution of these sounds and to distinguish what was related to phonetic and phonology.
The standard discourse produced by official organisations is confronted with the unofficial or informal discourse of the social web. Empowering people to express themselves results in a new balance of authority, when it comes to knowledge and changes the way people learn. Social web discourse is available to each and everyone and its size is growing fast, which opens up new fields for both humanities and social sciences to investigate. The latter, however, are not equipped to engage with such complex and little-analysed data. The aim of this dissertation is to investigate how far social web discourse can help supplement official discourse. In it we set out a method to collect and analyse data that is in line with the characteristics of a digital environment, namely data size, anonymity, transience, structure. This field of investigation encompasses several related questions that have to do with health, society, the evolution of morals, the mismatch between different kinds of discourse. Our study is also grounded in the analysis of a comparable French corpus dealing with the same topic, whose genre and discourse characteristics are equivalent to those of the Vietnamese one: this two-pronged research highlights the specific features of different socio-cultural environments.
The goal of this project is to fully exploit the audio stream to automatically enrich speech transcripts and subtitles of TV series and movies with the name and position of the characters. speaker A: 'Nice to meet you, I am Leonard, and this is Sheldon. We live across the hall.' speaker B: 'Oh. Hi. I'm Penny.' speaker A: 'Sheldon, what the hell are you doing?' speaker C: 'I am not quite sure yet. Do you know where Howard lives?' Just looking at these two short conversations, a human can easily infer that 'speaker A' is actually 'Leonard', 'speaker B' is Penny and 'speaker C' is Sheldon. The objective of this project is to combine natural language processing and speech processing to do the same automatically.
This thesis investigates how a machine can be taught a new task from unlabeled human in structions, which is without knowing beforehand how to associate the human communicative signals with their meanings. It therefore removes the need for an expert to tune the system for each specific user, which constitutes an important step towards flexible personalized teaching interfaces, a key for the future of personal robotics. Our approach assumes the robot has access to a limited set of task hypotheses, which include the task the user wants to solve. Our method consists of generating interpretation hypotheses of the teaching signals with respect to each hypothetic task. By building a set of hypothetic interpretation, i.e. a set of signal label pairs for each task, the task the user wants to solve is the one that explains better the history of interaction. We consider different scenarios, including a pick and place robotics experiment with speech as the modality of interaction, and a navigation task in a brain computer interaction scenario. In these scenarios, a teacher instructs a robot to perform a new task using initially unclassified signals, whose associated meaning can be a feedback (correct/incorrect) or a guidance (go left, right, up,...). Our results show that a) it is possible to learn the meaning of unlabeled and noisy teaching signals, as well as a new task at the same time, and b) it is possible to reuse the acquired knowledge about the teaching signals for learning new tasks faster. We further introduce a planning strategy that exploits uncertainty from the task and the signals' meanings to allow more efficient learning sessions. We present a study where several real human subjects control successfully a virtual device using their brain and without relying on a calibration phase. Our system identifies, from scratch, the target intended by the user as well as the decoder of brain signals. Based on this work, but from another perspective, we introduce a new experimental setup to study how humans behave in asymmetric collaborative tasks. In this setup, two humans have to collaborate to solve a task but the channels of communication they can use are constrained and force them to invent and agree on a shared interaction protocol in order to solve the task. These constraints allow analyzing how a communication protocol is progressively established through the interplay and history of individual actions.
The main objective of this thesis is to propose representation learning techniques with little to no need for labeled data: unsupervised, distant or even active learning are all research pathways that could be explored. Unsupervised learning is an extreme case in which unlabeled data is abundantly available, but in which no labeled data is. The term 'self-supervised'is being increasingly used to describe this kind of approaches, as their task often consists on reconstructing the original data from an altered version of it (BERT [Devlin et al., 2018], auto-encoders, etc.). We talk about distant supervision when (sometimes inaccurate) labels are in fact available, but not for the specific targeted task. For instance, for speaker verification, it is possible to use an automatic transcription system in order to obtain phonetic labels for the voice signal. This information, orthogonal to the main task, must be able to disentangle contradictory signals to improve speaker representation [Zeghidour et al., 2016]. Active learning is a semi-supervised model where an oracle (generally a human) participates in the learning process. More precisely, starting from non-annotated data, the learning algorithm needs to determine what has to be annotated by the oracle, so as to get the best performance at the least cost [Lowell et al., 2018, Drugman et al., 2019, Feyisetan et al., 2019, Settles and Craven, 2008, Duong et al., 2018, Kholghi et al., 2016,Settles, 2009].
As healthcare informations systems evolve, practitioners are required to enter more and more information digitally through a variety of specialized software. Handling these systems always involves a training and adaptation phase that requires immediate acces to a workstation, which takes time on their assignment time. Additionally, these constraints greatly modify the work habits of physicians who are used to prescribing drugs directly on a blank page or by using a prescription writing software. The objective of this thesis is to free prescribers from a certain number of constraints, and to offer them a tool allowing them to get as close as possible to the most "natural" and "cultural" way of prescribing medicine in a near-natural language. In order to address this problem, we position ourselves in a vocal interaction context on smartphone which makes it possible treat two observed problems: unnatural interactions with prescription writing softwares and the lack of immediate access to a connected computer.
Recommendation systems try to infer their users' interests in order to suggest items relevant to them. These systems thus offer a valuable service to users in that they automatically filter non-relevant information, which avoids the nowadays common issue of information overload. This is why recommendation systems are now popular, if not pervasive in some domains such as the World Wide Web. However, an individual's interests are personal and private data, such as one's political or religious orientation. Therefore, recommendation systems gather private data and their widespread use calls for privacy-preserving mechanisms. In this thesis, we study the privacy of users' interests in the family of recommendation systems called Collaborative Filtering (CF) ones. Our first contribution is Hide &amp; Share, a novel privacy-preserving similarity mechanism for the decentralized computation of K-Nearest-Neighbor (KNN) graphs. It is a lightweight mechanism designed for decentralized (a.k.a. peer-to-peer) user-based CF systems, which rely on KNN graphs to provide recommendations. Our second contribution also applies to user-based CF systems, though it is independent of their architecture. This contribution is two-fold: first we evaluate the impact of an active Sybil attack on the privacy of a target user's profile of interests, and second we propose a counter-measure. This counter-measure is 2-step, a novel similarity metric combining a good precision, in turn allowing for good recommendations,with high resilience to said Sybil attack.
The Center for Data Science of the University of Paris-Saclay deployed a platform compatible with Linked Data in 2016. Because researchers face many difficulties utilizing these technologies, an approach and then a platform we call LinkedWiki were designed and tested over the university's cloud (IAAS) to enable the creation of modular virtual search environments (VREs) compatible with Linked Data. We are thus able to offer researchers a means to discover, produce and reuse the research data available within the Linked Open Data, i.e., the global information system emerging at the scale of the internet. This experience enabled us to demonstrate that the operational use of Linked Data within a university is perfectly possible with this approach. However, some problems persist, such as (i) the respect of protocols and (ii) the lack of adapted tools to interrogate the Linked Open Data with SPARQL. We propose solutions to both these problems. In order to be able to verify the respect of a SPARQL protocol within the Linked Data of a university, we have created the SPARQL Score indicator which evaluates the compliance of the SPARQL services before their deployments in a university's information system. In addition, to help researchers interrogate the LOD, we implemented a SPARQLets-Finder, a demonstrator which shows that it is possible to facilitate the design of SPARQL queries using autocompletion tools without prior knowledge of the RDF schemas within the LOD.
Among the designations that flourish in the economic press through the attractive name of «new business models» (in English in the text), two of them are specifically based on property sharing: «functional economy» and «sharing economy». They both connect technological innovations with the evolution of social practices. These approaches intend to take advantage of a contemporary transformation of consumption patterns, characterized by a desecration of the role given to material goods. Our research focuses on the construction and meaning of several socio-economic models, in principle aiming to foster sustainable development. Although the multiplication of designations muddles the definitions of the models, each of them is connected to its own network of actors. Even if the French translations for «functional economy», «économie de fonctionnalité» and «économie de la fonctionnalité», are distinguished only by a definite article, they refer to two contradictory approaches. Similarly, while the term «sharing economy» firstly evokes the «peer-to-peer» (in English in the text) banner, it rapidly spreads to describe a form of connexionist capitalism. The deployment of the studied models makes it possible to capture certain transformations of contemporary representations. The relative success of the models depends on the correspondence of the ideals attached to each of them and the socio-economic facts experienced by actors. Evolution of work, changes in the outlines of property or distrust towards the political class are revealed by the analysis of discourses related to our topics.
This thesis has been carried out in the field of information and communication sciences and focuses on emotion in information retrieval in health fora. The success of these systems results from an informational and emotional motivation from the participants who can access testimonials, punctual information or medical information filtered through the experience of the patients who are writing. The messages often have emotional content. This thesis focuses on developments in information activities and especially on the role that emotional has in the structuring and evaluation of information. The aim of the first analysis is to highlight the organization of messages and their emotional content through a corpus analysis (from different threads of different French-language health fora). A second study focuses on the analysis of data collected during some interviews regarding the use of health fora and the way in which participants evaluate information. The results show that medical information is very present and mostly interspersed with emotion of fear. However, the joy is the emotion mostly present in all the collected corpus. Finally, if the evaluation criteria markers are emotion, it appears that medical information are also evaluation criteria and not the information evaluated.
This thesis addresses the problem of learning with non-modular losses. In a prediction problem where multiple outputs are predicted simultaneously, viewing the outcome as a joint set prediction is essential so as to better incorporate real-world circumstances. In empirical risk minimization, we aim at minimizing an empirical sum over losses incurred on the finite sample with some loss function that penalizes on the prediction given the ground truth. In this thesis, we propose tractable and efficient methods for dealing with non-modular loss functions with correctness and scalability validated by empirical results. We then introduce an alternating direction method of multipliers (ADMM) based decomposition method for loss augmented inference, that only depends on two individual solvers for the loss function term and for the inference term as two independent subproblems. Second, we propose a novel surrogate loss function for submodular losses, the Lovász hinge, which leads to O(p log p) complexity with O(p) oracle accesses to the loss function to compute a subgradient or cutting-plane. Finally, we introduce a novel convex surrogate operator for general non-modular loss functions, which provides for the first time a tractable solution for loss functions that are neither supermodular nor submodular. This surrogate is based on a canonical submodular-supermodular decomposition.
The work presented in this thesis is part of a broader issue of study and design of MOOCs (Massive Open Online Courses). It focuses more particularly on the didactic study of an algorithmic MOOC designed for undergraduate students at Hassan First University (Morocco). This work is part of a comprehensive approach and aims more specifically to understand the process of developing algorithmic content conveyed by the MOOC and how learners construct basic knowledge essential to the course. Considering MOOC as a didactic device, two approaches: didactic and epistemological of algorithmic have been articulated. The notion of didactic performance is mobilized to examine the learning strategies adopted by students. By using discussion forums and mobilizing a questionnaire and semi-structured interviews, the discourses of students were analysed in order to characterize the constructed content, the didactic performance and the difficulties encountered by students. The characterization of the design of the MOOC highlights two steps: 1) identification of the essential concepts in algorithmics:variable, basic instructions, conditions, loops and their organization into learning units 2) development of a pedagogical scenario by describing the learning tasks of the pedagogical units and their organisation, and, on the other hand, that the course is also adapted to the massification of audiences, in particular by decreasing the hourly workload and demanding few prerequisites. The results show that students constructed two types of content: conceptual knowledge (condition, loop) and procedural knowledge (analysis of a problem, decomposition of a problem, etc.). Students showed more interest in cognitive and technical didactic performances to build, step by step, content. More specifically, students constructed algorithmic content by making greater use of these learning strategies: 1) elaboration strategies (linking the content with previous knowledge) and organization strategies such as the use of flowcharts 2) technical strategies in terms of mobilizing MOOC videos. The results also show that although students have been particularly successful in analysing problems (determination of input and output objects), some difficulties remain, such as passing from analysing problems to elaborating algorithms. These results can not only provide MOOC instructional designers with the necessary elements for content development, but also improve didactic research on MOOCs by providing researchers with elements for the study of MOOCs, taking into account the specificity of their content.
The present thesis locates itself in the interdisciplinary field of computational stylistics, namely the application of statistical and computational methods to the study of literary style. Historically, most of the work done in computational stylistics has been focused on lexical aspects especially in the early decades of the discipline. However, in this thesis, our focus is put on the syntactic aspect of style which is quite much harder to capture and to analyze given its abstract nature. As main contribution, we work on an approach to the computational stylistic study of classic French literary texts based on a hermeneutic point of view, in which discovering interesting linguistic patterns is done without any prior knowledge. Following the hermeneutic line of thought, we propose a knowledge discovery process for the stylistic characterization with an emphasis on the syntactic dimension of style by extracting relevant patterns from a given text. This knowledge discovery process consists of two main steps, a sequential pattern mining step followed by the application of some interestingness measures. In particular, the extraction of all possible syntactic patterns of a given length is proposed as a particularly useful way to extract interesting features in an exploratory scenario. We propose, carry out an experimental evaluation and report results on three proposed interestingness measures, each of which is based on a different theoretical linguistic and statistical backgrounds.
This dissertation is a contribution to the description of Kmhmouʔ, a language with an oral tradition spoken in Laos. It presents the general characteristics of this language, which is among the less described languages of the region of Southeast Asia, as shown by the bibliography. Kmhmouʔ, an isolating language, does not exhibit any grammatical morphology. Words are mostly multi-categorial as regards parts of speech, and multi-functional: the verb-noun distinction is based essentially on combinatory properties of words. This grammatical description is based on spontaneous data collected during fieldwork in Kmhmouʔ villages and from the Lao national radio (Kmhmouʔ broadcast). The fieldwork was conducted in Kmhmouʔ, and the analysis and interpretation of the corpus benefited from the fact that the author is native speaker of Kmhmouʔ. This thesis, besides making data and grammar of the Eastern Kmhmouʔ dialect available for the first time, opens up some new and challenging paths for the study of the typology of isolating languages and transcategoriality, as well as for studies of the role of language contact in grammaticalization.
In recent years, security in Information Systems (IS) has become an important issue that needs to be taken into account in all stages of IS development, including the early phase of Requirement Engineering (RE). Considering security during early stages of IS development allows IS developers to envisage threats, their consequences and countermeasures before a system is in place. Security requirements are known to be “the most difficult of requirements types”, and potentially the ones causing the greatest risk if they are not correct. Moreover, requirements engineers are not primarily interested in, or knowledgeable about, security. Their tacit knowledge about security and their primitive knowledge about the domain for which they elicit security requirements make the resulting security requirements poor and too generic. This thesis explores the approach of eliciting requirements based on the reuse of explicit knowledge. First, the thesis proposes an extensive systematic mapping study of the literature on the reuse of knowledge in security requirements engineering identifying the different knowledge forms. This is followed by a review and classification of security ontologies as the main reuse form. In the second part, AMAN-DA is presented. AMAN-DA is the method developed in this thesis. It allows the elicitation of domain-specific security requirements of an information system by reusing knowledge encapsulated in domain and security ontologies. Besides that, the thesis presents the different elements of AMANDA: (I) a core security ontology, (II) a multi-level domain ontology, (III) security goals and requirements'syntactic models, (IV) a set of rules and mechanisms necessary to explore and reuse the encapsulated knowledge of the ontologies and produce security requirements specifications. The last part reports the evaluation of the method. AMAN-DA was implemented in a prototype tool. Its feasibility was evaluated and applied in case studies of three different domains (maritime, web applications, and sales). The ease of use and the usability of the method and its tool were also evaluated in a controlled experiment. The experiment revealed that the method is beneficial for the elicitation of domain specific security requirements, and that the tool is friendly and easy to use.
The development and multiplication of information systems and platforms for information access has been accentuated over the past thirty years. The large volume of information available has raised many scientific challenges in different areas such as information retrieval. To access documents grouped in a digital corpus, one must be able to express his/her information need, often in the form of a query, to associate the relevant documents and present them in the best possible way to users. Document research in a thematic digital corpus presenting a high level of technicality in the concerned discipline can be considered as a browsing process driven by some information needs. Such browses requires the use of traditional information retrieval tools to select relevant documents based on a query But they can be improved by the use of customization and adaptation mechanisms in order to refine the representation of information needs according to the specificities of a user, his current browsing or the corpus considered. Indeed, access to digital documents raises problems related to the search for information, the visualization of the results of a query and the browsing between the documents. The process of information retrieval requires to be improved and especially by the integration of the user as a main factor to take into account in the search for satisfaction of his/her information needs. We consider several approaches to help users in their search for documents. A first assistance concerns the reformulation of queries by targeting an audience of users unfamiliar with the technical terms of the field and struggling to express in the form of a query their need. The second approach that we propose is not to consider the user in isolation but to bring it closer to those who have expressed similar research to find the documents they considered relevant. Finally, we include works from the field of the recommendation to better understand the informational needs of the user and help them find what they are looking for by recommending documentary resources. In this thesis, we propose to treat this diversity of influence by a multi-agent system interacting with a shared environment representing the users browsing so that the system may be adapted to use either assistance facilities according to the user's expertise. We applied our work for document research in a digital corpus of legal documents.
Robots are more and more used in a social context. They are required not only to share physical space with humans but also to interact with them. In this context, the robot is expected to understand some verbal and non-verbal ambiguous cues, constantly used in a natural human interaction. In particular,knowing who or what people are looking at is a very valuable information to understand each individual mental state as well as the interaction dynamics. It is called Visual Focus of Attention or VFOA. In this thesis, we are interested in using the inputs from an active humanoid robot – participating in a social interaction – to estimate who is looking at whom or what. On the one hand, we want the robot to look at people, so it can extract meaningful visual information from its video camera. We propose a novel reinforcement learning method for robotic gaze control. The model is based on a recurrent neural network architecture. The robot autonomously learns as trategy for moving its head (and camera) using audio-visual inputs. It is able to focus on groups of people in a changing environment. On the other hand, information from the video camera images are used to infer the VFOAs of people along time. We estimate the 3D head poses (location and orientation) for each face, as it is highly correlated with the gaze direction. We use it in two tasks. First, we note that objects may be looked at while not being visible from the robot point of view. Under the assumption that objects of interest are being looked at, we propose to estimate their locations relying solely on the gaze direction of visible people. We formulate an ad hoc spatial representation based on probability heatmaps. We design several convolutional neural network models and train them to perform a regression from the space of head poses to the space of object locations. In this context, we introduce a Bayesian probabilistic model, inspired from psychophysics, that describes the dependency between head poses, object locations, eye-gaze directions, and VFOAs, along time. The formulation is based on a switching state-space Markov model. A specific filtering procedure is detailed to infer the VFOAs, as well as an adapted training algorithm. The proposed contributions use data-driven approaches, and are addressed within the context of machine learning. All methods have been tested on publicly available datasets. Some training procedures additionally require to simulate synthetic scenarios; the generation process is then explicitly detailed.
The acquisition of attributive adjectives is subject to two major difficulties in French. First, adjectives express a property of a larger unit: children must be able to conceive an object as a whole and as a set of properties to use a NP with an adjective. Also, although speakers are aware of this possibility, they tend to choose a fixed position in usage. These facts raise the question of whether the input allows children to construct the notion of attributive adjectives without a resort to innate linguistic knowledge. To answer this, I propose a comparative study of the productions of three children interacting with their family at two times of their development (T1: 3 ;8, T2: 4 ;6). I examine four phenomena concerning attributive adjectives (lexicon, placement, combination with other modifiers or adjectival dependents), and I compare adjectives with other nominal modifiers. All of these phenomena show the same evolution. At T1, the children use the most frequent construction in the adult data, with a high degree of lexical specificity. T2 shows the appearance of other constructions according to their order of frequency in the adult data. The construction from T1 is also used with a greater choice of lexical units, but within the same semantic classes. The children thus show sensitivity to quantitative information and a gradual abstraction of constructions by semantic analogy, which pleads for a progressive construction of the knowledge of adjectives based on the input data.
Our society has been rapidly growing its presence on the Web, as a consequence we are digitizing a large collection of our daily happenings. In this scenario, the Web receives virtual occurrences of various events corresponding to their real world occurrences from all around the world. Scale of these events can vary from locally relevant ones up to those that receive global attention. News and social media of current times provide all essential means to reach almost a global diffusion. This big data of complex societal events provide a platform to many research opportunities for analyzing and gaining insights into the state of our society. In this thesis, we investigate a variety of social event impact analytics tasks. Specifically, we address three facets in the context of events and the Web, namely, diffusion of events in foreign languages communities, automated classification of Web contents, and news virality assessment and visualization. We hypothesize that the named entities associated with an event or a Web content carry valuable semantic information, which can be exploited to build accurate prediction models. We have shown with the help of multiple studies that raising Web contents to the entity-level captures their core essence, and thus, provides a variety of benefits in achieving better performance in diverse tasks. We report novel findings over disparate tasks in an attempt to fulfill our overall goal on societal event impact analytics.
Machine learning is the study of designing algorithms that learn from training data to achieve a specific task. The resulting model is then used to predict over new (unseen) data points without any outside help. This data can be of many forms such as images (matrix of pixels), signals (sounds,...), transactions (age,amount, merchant,...), logs (time, alerts,...). Datasets may be defined to address a specific task such as object recognition, voice identification, anomaly detection,etc. In these tasks, the knowledge of the expected outputs encourages a supervised learning approach where every single observed data is assigned to a label that defines what the model predictions should be. For example, in object recognition,an image could be associated with the label "car" which suggests that the learning algorithm has to learn that a car is contained in this picture, somewhere. This is in contrast with unsupervised learning where the task at hand does not have explicit labels. For example, one popular topic in unsupervised learning is to discover underlying structures contained in visual data (images) such as geometric forms of objects, lines, depth, before learning a specific task. This kind of learning is obviously much harder as there might be potentially an infinite number of concepts to grasp in the data. In this thesis, we focus on a specific scenario of the supervised learning setting: 1) the label of interest is under represented (e.g. anomalies) and 2) the dataset increases with time as we receive data from real-life events (e.g. credit card transactions). In fact, these settings are very common in the industrial domain in which this thesis takes place.
The development of consumer robotics comes with a new kind of telecommunications systems: telepresence robots. These are mobile robots representing a person who is able to control their movements remotely. The aim is not only to allow remote communication, but to create a sense of social and physical presence, which are not sufficiently transmitted by telephone or videoconferencing. In this context, it is especially important to ensure that the users' « social touch » is well transmitted, meaning that they are able to exchange a wide range of socio-affective signals, which are the vectors of social links. In particular, this thesis deals with a key element of social touch, which is deeply impacted by telepresence: vocal earshot, by which speakers are normally able to control who can hear them, and to adapt to varying acoustic environment conditions. In a first study, we will explore the link between vocal touch and proxemics, by asking whether a blind listener's spatial perception of an interlocutor can be influenced by the expressed socio-affects. We will then show that vocal earshot can be modified by the Lombard effect in ubiquitous telepresence, because the pilot is perceiving both the local and remote environments at the same time, and therefore adapts to noise, even if it is not noticeable by the interlocutors. Lastly, we will present our participation in an Arts-Sciences performance called Aporia, during which a unique actor embodies different characters, helped by a voice transforming algorithm.
Many measurement methods have been developed to assess the user experience, but remain immature and even contradictory. This is why studies should be conducted in order to increase the validity and reliability of this new approach. The main aim of this research is to use several methods and to combine and articulate them using various techniques to improve the measurement quality. These studies was based on a broad spectrum of indicators (physiological, behavioral and self-reported) and two triangulation strategies in particular: multi-faceted and multi-measures. Finally, these methods were tested in real applied cases and under an increasing procedure complexity and statistical processing. This research has resulted in three separate studies. The first aims to evaluate the relevance of a movie recommendation algorithm against its competitor using a multifaceted evaluation strategy. A second study was designed to test the relevance of a multi-measures evaluation model by assessing the usability of university sites based on a remote user testing software (Evalyzer) and a multimodal combination of various indicators of usability. A final study was conducted to validate multi-measures immersion protocol (questionnaire, facial expression, skin conductance, heart rate, eye behavior). These three studies have highlighted the relevance of numerous measures (of usability and user experience), the added value of some of their combinations, as well as a critical return to the multi-facet validation procedure used in this thesis
Nowadays, there are many geographic databases, (GDB), covering the same reality. The geographical data are represented differently (for example a river can be represented by a line or a polygon), they are used in different applications (visualisation, analysis) and they are created using various modes of acquisition (sources, processes). All these factors create independence between GDB, which causes problems for both producers and users. Thus, a solution is to clarify the relationships between various database objects, i.e. to match homologous objects, which represent the same reality. Because of the complexity of the matching process, the existing approaches depend on the types of data (points, lines or polygons) and the level of detail of the GDB. We realised, that most of the approaches are based on the geometry and the topology of the geographical objects, and very few approaches take into account the descriptive information of geographical objects. Besides, for most approaches, the criteria are applied one after the other and knowledge is contained within the process. Following this analysis, we proposed a matching approach that is guided by knowledge and takes into account all criteria at the same time exploiting the geometry, descriptive information and relations between geographical objects. In order to formalise knowledge and model their imperfections (imprecision, uncertainty and incompleteness), we used the Belief Theory [Shafer, 1976]. After a selection of candidates, the masses of beliefs are initialised by analysing each candidate separately from the others using different knowledge expressed by various matching criteria. Then, the matching criteria and candidates are fusioned. Finally, a decision is taken. Our approach has been tested on real data having different levels of detail and representing relief (data points) and road networks (linear data)
The work of this thesis concerns the modeling of emotions for expressive audiovisual textto-speech synthesis. Today, the results of text-to-speech synthesis systems are of good quality, however audiovisual synthesis remains an open issue and expressive synthesis is even less studied. As part of this thesis, we present an emotions modeling method which is malleable and flexible, and allows us to mix emotions as we mix shades on a palette of colors. In the first part, we present and study two expressive corpora that we have built. The recording strategy and the expressive content of these corpora are analyzed to validate their use for the purpose of audiovisual speech synthesis. In the second part, we present two neural architectures for speech synthesis. We used these two architectures to model three aspects of speech: 1) the duration of sounds, 2) the acoustic modality and 3) the visual modality. First, we use a fully connected architecture. This architecture allowed us to study the behavior of neural networks when dealing with different contextual and linguistic descriptors. We were also able to analyze, with objective measures, the network's ability to model emotions. The second neural architecture proposed is a variational auto-encoder. This architecture is able to learn a latent representation of emotions without using emotion labels. After analyzing the latent space of emotions, we presented a procedure for structuring it in order to move from a discrete representation of emotions to a continuous one. We were able to validate, through perceptual experiments, the ability of our system to generate emotions, nuances of emotions and mixtures of emotions, and this for expressive audiovisual text-to-speech synthesis.
Nowadays, information related on displacement and mobility in a transport network represents certainly a significant potential. So, this work aims to modeling, to optimize and to implement an Information System of Services to Aid the Urban Mobility (ISSAUM). The ISSAUM has firstly to decompose each set of simultaneous requests into a set of sub-requests called tasks. Each task corresponds to a service which can be proposed different by several information providers with different. The dynamic and distributed aspects of the problem incite us to adopt a multi-agent approach to ensure a continual evolution and a pragmatic flexibility of the system. So, we proposed to automate the modeling of services by using ontology idea. Our ISSAUM takes into account possible disturbance through the ETMN. In order to satisfy user requests, we developed a negotiation protocol between our system agents. The proposed ontology mapping negotiation model based on the knowledge management system for supporting the semantic heterogeneity and it organized as follow: Negotiation Layer (NL), the Semantic Layer (SEL), and the Knowledge Management Systems Layer(KMSL). We detailed also the reassignment process by using Dynamic Reassigned Tasks (DRT) algorithm supporting by ontology mapping approach. Finally, the experimental results presented in this thesis, justify the using of the ontology solution in our system and its role in the negotiation process
In this thesis we shed the light on the danger of privacy leakage on social network. We investigate privacy breaches, design attacks, show their feasibility and study their accuracies. This approach helps us to track the origin of threats and is a first step toward designing effective countermeasures. We have first introduced a subject sensitivity measure through a questionnaire survey. Then, we have designed on-line friendship and group membership link disclosure (with certainty) attacks on the largest social network “Facebook”. These attacks successfully uncover the local network of a target using only legitimate queries. We have also designed sampling techniques to rapidly collect useful data around a target. The collected data are represented by social-attribute networks and used to perform attribute inference (with uncertainty) attacks. To increase the accuracy of attacks, we have designed cleansing algorithms. These algorithms quantify the correlation between subjects, select the most relevant ones and combat data sparsity. Finally, we have used a shallow neural network to classify the data and infer the secret values of a sensitive attribute of a given target with high accuracy measured by AUC on real datasets. The proposed algorithms in this work are included in a system called SONSAI that can help end users analyzing their local network to take the hand over their privacy
When developing a software, maintenance and evolution represents an important part of the development's life-cycle, making up to 80% of the overall cost and effort. During the maintenance effort, it happens that developers have to resort to copying and pasting source code fragments in order to reuse them. Such practice, seemingly harmless is more frequent than we expect. Commonly referred to as ``clones''in the literature, these source code duplicates are a well-known and studied topic in software engineering. In this thesis, we aim at shedding some light on copy-paste practices on software artifacts. In particular, we chose to focus our contributions on two specific types of software artifacts: API documentation and build files (i.e. Dockerfiles). For both contributions, we follow a common empirical study methodology. First, We show that API documentations and software build files (i.e. Dockerfiles) actually face duplicates issues and that such duplicates are frequent. Secondly, we identify the reasons behind the existence of such duplicates. Finally, We show that both software artifacts lack reuse mechanisms to cope with duplicates, and that some developers even resort to ad-hoc tools to manage them.
This document proposes to learn the behaviour of the dialogue manager of a spoken dialogue system from a set of rated dialogues. This learning is performed through reinforcement learning. Our method does not require the definition of a representation of the state space nor a reward function. These two high-level parameters are learnt from the corpus of rated dialogues. It is shown that the spoken dialogue designer can optimise dialogue management by simply defining the dialogue logic and a criterion to maximise (e.g user satisfaction). The methodology suggested in this thesis first considers the dialogue parameters that are necessary to compute a representation of the state space relevant for the criterion to be maximized. For instance, if the chosen criterion is user satisfaction then it is important to account for parameters such as dialogue duration and the average speech recognition confidence score. The state space is represented as a sparse distributed memory. The Genetic Sparse Distributed Memory for Reinforcement Learning (GSDMRL) accommodates many dialogue parameters and selects the parameters which are the most important for learning through genetic evolution. The resulting state space and the policy learnt on it are easily interpretable by the system designer. These two algorithms consider the criterion to be the return for the entire dialogue. These functions are discussed and compared on simulated dialogues and it is shown that the resulting functions enable faster learning than using the criterion directly as the final reward. A spoken dialogue system for appointment scheduling was designed during this thesis, based on previous systems, and a corpus of rated dialogues with this system were collected. This corpus illustrates the scaling capability of the state space representation and is a good example of an industrial spoken dialogue system upon which the methodology could be applied
Given the rapid emergence of new mobile technologies and the growth of needs of a moving society in training, works are increasing to identify new relevant educational platforms to improve distant learning. The next step in distance learning is porting e-learning to mobile systems. So far, learning environment was either defined by an educational setting, or imposed by the educational content. In our approach, in m-learning, we change the paradigm where the system recommends content and adapts learning follow to learner's context.
A design platform (DP) is a total solution to build a System-On-Chip (SOC). DP consists of a set of libraries/IPs, CAD tools and design kits in conformity with the supported design flows and methodologies. The DP specifications provide a wide range of information from technology parameters like Process-Voltage-Temperature (PVT) corners to CAD tools' information for library/IP development. However, the library/IP developers have difficulties in obtaining the desired data from the existing specifications due to their informality and complexity. In this thesis, we propose methodologies, flows and tools to formalize the DP specifications for their unification and to deal with it. The proposed description is targeting to be used as a reference to generate and validate libraries (standard cells, I/O, memory) as well as complex IPs (PLL, Serdes, etc.). Furthermore, we introduce a reference-based method to create a reliable specification in LDSpecX and task-based keywords to efficiently extract data from it. On the basis of the proposed solutions, we develop a specification platform. Experimentally, we develop a standard cell library from the specification creation to library validation by using the specification platform. We show that our approach enables to create a complete and consistent specification with a considerable reduction in time. It also bridges the gap between the specification and current automatic system for rapid library/IP development.
All the work in this thesis has been developed in the context of the Cherenkov Telescope Array (CTA), which is going to be the major next-generation observatory for ground-based very-high-energy gamma-ray astronomy. The plan for this work is to use GPUs and Cloud Computing in order to speed up the computing demanding tasks, developing and optimizing data analysis pipelines. The thesis consists on two main parts: the first one is dedicated to the estimation of the future performances of CTA towards the observation of violent phenomena such as those generating Gamma Ray Bursts and Gravitational Waves, with a initial work done for the creation of the models for the First CTA Data Challenge. The second part of the thesis is related to the development of the pipelines for the reconstruction of the low-level data coming from the Monte Carlo simulations using the software library called ctapipe. In chapter 1 I go into the details of the CTA project, the telescopes and the performances of the array, together with the methods used to derive them from Monte Carlo simulations. More than 500 AGNs have been modelled for CTA. This Challenge has been important both to involve more people in the analysis of CTA data and to compute the observation time needed by the different KSP. The simulations for the gravitational waves and gamma-ray bursts Consortium papers have been created with the ctools_pipe pipeline (presented in chapter 4), implemented around the libraries ctools and gammalib. The pipeline is composed of two main parts: the task to be executed (background simulation, model creation and detection) and in which computing centre. The second part of the thesis is focused on the development and optimization of the analysis pipelines to be used for the event reconstruction of simulated raw data and for the visualization of the events in a 3D space. This analyses have been performed using ctapipe, a framework for prototyping the low-level data processing algorithms for CTA. The structure of the library is presented in chapter 5 together a focus on the reconstruction methods that are implemented in ctapipe, including the so called ImPACT. This method uses a template of images created from the Monte Carlo simulations and a seed from the standard reconstruction method to fit between the templates to find a better estimation of the shower parameters. The time profiling and the strategies adopted to optimize the ImPACT pipeline are presented in chapter 6. The implementation of the a pipeline for the analysis of the Large Size Telescope observing in monoscopic mode and its GPU implementation with PyTorch is also presented. ctapipe has also been used and developed to estimate the performances of CTA when observing using the “divergent pointing” mode, in which the pointing directions are slightly different with respect to the parallel pointing mode, so that the final hyper field-of-view of all the telescopes is larger with respect to the parallel pointing mode. The angular and energy resolutions and also the sensitivity are worse in this scenario, but having a wider hyper field-of-view can be good for other topics, such are searching for transient sources. The modifications to the reconstruction code introduced in ctapipe and some angular resolution plots for the simulated point source gammas are presented in chapter 7.The results presented in this thesis are a demonstration of the usage of advanced software techniques in very high energy astrophysics.
Adaptive discretizations are important in compressible/incompressible flow problems since it is often necessary to resolve details on multiple levels, allowing large regions of space to be modeled using a reduced number of degrees of freedom (reducing the computational time). There are a wide variety of methods for adaptively discretizing space, but Cartesian grids have often outperformed them even at high resolutions due to their simple and accurate numerical stencils and their superior parallel performances. Such performance and simplicity are in general obtained applying a finite-difference scheme for the resolution of the problems involved, but this discretization approach does not present, by contrast, an easy adapting path. In a finite-volume scheme, instead, we can incorporate different types of grids, more suitable for adaptive refinements, increasing the complexity on the stencils and getting a greater flexibility. The Laplace operator is an essential building block of the Navier-Stokes equations, a model that governs fluid flows, but it occurs also in differential equations that describe many other physical phenomena, such as electric and gravitational potentials, and quantum mechanics. So, it is a very important differential operator, and all the studies carried out on it, prove its relevance. In this work will be presented 2D finite-difference and finite-volume approaches to solve the Laplacian operator, applying patches of overlapping grids where amore fined level is needed, leaving coarser meshes in the rest of the computational domain. These overlapping grids will have generic quadrilateral shapes. Specifically, the topics covered will be: 1) introduction to the finite difference method, finite volume method, domain partitioning, solution approximation; 2) overview of different types of meshes to represent in a discrete way the geometry involved in a problem, with a focus on the octree data structure, presenting PABLO and PABLitO. The first one is an external library used to manage each single grid's creation, load balancing and internal communications, while the second one is the Python API of that library written ad hoc for the current project; 3) presentation of the algorithm used to communicate data between meshes (being all of them unaware of each other's existence) using MPI inter-communicators and clarification of the monolithic approach applied building the final matrix for the system to solve, taking into account diagonal, restriction and prolongation blocks; 4) presentation of some results; conclusions, references. It is important to underline that everything is done under Python as programming framework, using Cython for the writing of PABLitO, MPI4Py for the communications between grids, PETSc4py for the assembling and resolution parts of the system of unknowns, NumPy for contiguous memory buffer objects. The choice of this programming language has been made because Python, easy to learn and understand, is today a significant contender for the numerical computing and HPC ecosystem, thanks to its clean style, its packages, its compilers and, why not, its specific architecture optimized versions.
Biological invasions contribute to the degradation of biodiversity globally. Invasive alien plants have impacted on natural resources management and have generated substantial costs of control and economic loss. Various management options have been put in place to control the level of invasions of targeted species. The public's perception of invasive species varies among stakeholders. Controversies and conflicts emerged as a consequence of diverging opinions on the management of invasions. We conducted an inter-disciplinary study on the socio-ecological and economic dimensions related to the management of the invasive Rubus alceifolius, following a biological control programme in Réunion Island (France). Firstly, we carried out an economic analysis of the management options for R. alceifolius with future scenario on the cost of invasion. Secondly we assessed the impact of the recovery of native species post biological control. We found that the biological control programme of R. alceifolius was successful within the elevation limit of 800 m, from both an economic and ecological perspective. Given the shortfall in the decision-making process and implementation, this study demonstrated the crucial need to identify and involve stakeholders in all stages of a biological control programme. We concluded with key recommendations for successful biological programmes.
In information retrieval tasks from image databases where content representation is based on graphs, the evaluation of similarity is based both on the appearance of spatial entities and on their mutual relationships. In this thesis we present a novel scheme of Attributed Relational Adjacency Graphs representation and mining, which has been applied in content-based retrieval of comic images. We propose a graph representation that yields stable graphs and allow to retain high-level and structural information of objects of interest in comic images. Next, we extend the indexing and matching problem to graph structures representing the comic image, and apply it to the problem of retrieval. The graphs in the graph database representing the whole comic volume are then mined for frequent patterns (or frequent substructures). This step is to overcome the non-repeatability problem caused by the unavoidable errors introduced into the graph structure during the graph construction stage, which ultimately create a semantic gap between the graph and the content of the comic image. Experiments of performance measures is addressed to evaluate the performance of this CBIR system.
We study defferent types of data as a stream: Databases Social Networks Text data. For a database that follows a relational schema, an OLAP (Online Analytical Processing) analysis schema defines one of the database tables as an analysis table. We suppose that the tuples of the analysis table arrive as a stream. We study the approximation of OLAP queries, by sampling with weights the tuples of the stream without storing the analysis data. We give a preference model in this context. For the social network Twitter, we observe a stream of tweets that contain any given tag and transform it into a stream of edges of a graph. We study the existence of large clusters in the generated graph. We propose a uniform sampling method that will associate a random subgraph of the graph and study the giant components of this random subgraph as a witness of the large clusters of the original graph. For a stream of text, we consider the pairs of words in a lemmatized sentence as edges of a graph where the nodes are the words. We transform the stream of text into a stream of edges. We sample the edges proportionally to the Word2vec similarity of the words. We then analyze the giant components. We extend the classical Word2vec vectors, in order to take into account the morphology of the words, i.e. the prefixes, roots and suffixes.
This study is based on the teaching model of reading French as a foreign language by university students who are beginners in this language in Brazil. Regardless of its dissemination among Latin America students and of its continuous reformulation to account for new learning situations, this teaching method contains some gaps. The gaps refer to a critical issue for a large number of readers: the lack of lexical knowledge. This issue relates not only to a lack of vocabulary knowledge, but also to the ability of word learning. In this study, this question has been further studied based on two experimental researches. The first one, using a more prospective approach, aims at identifying the role of the dictionary–especially bilingual dictionaries–in the reading process and the effects of dictionary use in the construction of meaning. The second one compares the use of two lexicographic instruments: a bilingual dictionary and a pedagogical dictionary for learners of French as a foreign language. Both researches yield important information about the integration of a lexical study with this teaching model; additionally, they are complemented by the analysis of the dictionaries available to this target audience. In this way, we can set methodological principles to build a pedagogical dictionary that helps students in reading activities and that supports vocabulary acquisition on the basis of functional lexicography.
Our goal in this thesis is to build a system that answers a natural language question (NL) by representing its semantics as a logical form (LF) and then computing the answer by executing the LF over a knowledge base. The core part of such a system is the semantic parser that maps questions to logical forms. Our focus is how to build high-performance semantic parsers by learning from (NL, LF) pairs. We propose to combine recurrent neural networks (RNNs) with symbolic prior knowledge expressed through context-free grammars (CFGs) and automata. By integrating CFGs over LFs into the RNN training and inference processes, we guarantee that the generated logical forms are well-formed; by integrating, through weighted automata, prior knowledge over the presence of certain entities in the LF, we further enhance the performance of our models. Experimentally, we show that our approach achieves better performance than previous semantic parsers not using neural networks as well as RNNs not informed by such prior knowledge.
Automatic short text classification is more and more used nowadays in various applications like sentiment analysis or spam detection. We present two new approaches to improve short text classification. Our first approach is "Semantic Forest". The first step of this approach proposes a new enrichment method that uses an external source of enrichment built in advance. Contrarily to the methods proposed in the literature, the second step of our approach does not use traditional learning algorithm but proposes a new one based on the semantic links among words in the Random Forest classifier. Our second contribution is "IGLM" (Interactive Generic Learning Method). It is a new interactive approach that recursively updates the classification model by considering the new data arriving over time and by leveraging the user intervention to correct misclassified data. An abstraction method is then combined with the update mechanism to improve short text quality. The experiments performed on these two methods show their efficiency and how they outperform traditional algorithms in short text classification. Finally, the last part of the thesis concerns a complete and argued comparative study of the two proposed methods taking into account various criteria such as accuracy, speed, etc.
Among these pre-conceptions, many linguistic patterns have been said to be representative of male or female features, like tag questions, deference, turn-taking for example. Of all the gendered linguistic characteristics, the one which may have been the most debated is that of swear words. Because of a complex interplay between social expectations and power relations, swearing has traditionally been associated with men. Moreover, swearing is often considered as an act of power and a way of affirming oneself. Thus, the fact that one gender may be perceived as more frequent users of swear words, or on the other hand as swear word eschewers, may have an impact on other qualities related to power that we would inherently attribute to one gender or the other, whether these differences are real or not. Some studies have showed that contrary to what has long been widely believed, women do not swear less frequently than men, nor do they use a drastically different register. Some even envisioned that the use of “strong” swear words by women would increase in certain contexts, specifically on social media; this seemed especially true for younger generations of users. It was even predicted that “gender equality in swearing or a reversal in gender patterns for strong swearing, will slowly become more widespread, at least in social network sites” (Thelwall, 2008: 102), such that the use of strong swear words among young women will eventually be more frequent than among (young) men. Thus, the following question arises: has the prediction made by Thelwall in 2008 been fulfilled eight years later, in a society where computer-mediated communication in the context of social media is firmly rooted in people's everyday lives? This study is based specifically on a corpus composed of just over eighteen million tweets issued by roughly 739 000 users. The corpus was populated with tweets by British users of both genders and from different age groups throughout the United Kingdom. Corpus linguistic methodology and tools have been used to address the sociolinguistic issues raised earlier. Also, because Twitter does not provide us with a direct access to the gender or the age of the users, using computer-programming methods has been necessary to be able to study these age and gender differences.
We are currently experiencing an exceptional growth of visual data, for example, millions of photos are shared daily on social-networks. Image understanding methods aim to facilitate access to this visual data in a semantically meaningful manner. In this dissertation, we define several detailed goals which are of interest for the image understanding tasks of image classification and retrieval, which we address in three main chapters. First, we aim to exploit the multi-modal nature of many databases, wherein documents consists of images with a form of textual description. In order to do so we define similarities between the visual content of one document and the textual description of another document. These similarities are computed in two steps, first we find the visually similar neighbors in the multi-modal database, and then use the textual descriptions of these neighbors to define a similarity to the textual description of any document. Second, we introduce a series of structured image classification models, which explicitly encode pairwise label interactions. Such an interactive scenario offers an interesting trade-off between accuracy and manual labeling effort. We explore structured models for multi-label image classification, for attribute-based image classification, and for optimizing for specific ranking measures. Finally, we explore k-nearest neighbors and nearest-class mean classifiers for large-scale image classification. We propose efficient metric learning methods to improve classification performance, and use these methods to learn on a data set of more than one million training images from one thousand classes. Since both classification methods allow for the incorporation of classes not seen during training at near-zero cost, we study their generalization performances. We show that the nearest-class mean classification method can generalize from one thousand to ten thousand classes at negligible cost, and still perform competitively with the state-of-the-art.
Textual Entailment aims at capturing major semantic inference needs across applications in Natural Language Processing. Since 2005, in the Textual Entailment recognition (RTE) task, systems are asked to automatically judge whether the meaning of a portion of text, the Text - T, entails the meaning of another text, the Hypothesis - H. This thesis we focus a particular case of entailment, entailment by generality. For us, there are various types of implication, we introduce the paradigm of Textual Entailment by Generality, which can be defined as the entailment from a specific sentence towards a more general sentence, in this context, the Text T entailment Hypothesis H, because H is more general than T. We propose methods unsupervised language-independent for Recognizing Textual Entailment by Generality, for this we present an Informative Asymmetric Measure called the Simplified Asymmetric InfoSimba, which we combine with different asymmetric association measures to recognizingthe specific case of Textual Entailment by Generality. This thesis, we introduce the new concept of implication, implications by generality, in consequence, the new concept of recognition implications by generality, a new direction of research in Natural Language Processing.
This PhD thesis deals with variational inference and robustness. More precisely, it focuses on the statistical properties of variational approximations and the design of efficient algorithms for computing them in an online fashion, and investigates Maximum Mean Discrepancy based estimators as learning rules that are robust to model misspecification. In recent years, variational inference has been extensively studied from the computational viewpoint, but only little attention has been put in the literature towards theoretical properties of variational approximations until very recently. In this thesis, we investigate the consistency of variational approximations in various statistical models and the conditions that ensure the consistency of variational approximations. In particular, we tackle the special case of mixture models and deep neural networks. We also justify in theory the use of the ELBO maximization strategy, a model selection criterion that is widely used in the Variational Bayes community and is known to work well in practice. Moreover, Bayesian inference provides an attractive online-learning framework to analyze sequential data, and offers generalization guarantees which hold even under model mismatch and with adversaries. Unfortunately, exact Bayesian inference is rarely feasible in practice and approximation methods are usually employed, but do such methods preserve the generalization properties of Bayesian inference? In this thesis, we show that this is indeed the case for some variational inference algorithms. We propose new online, tempered variational algorithms and derive their generalization bounds. Our theoretical result relies on the convexity of the variational objective, but we argue that our result should hold more generally and present empirical evidence in support of this. Our work presents theoretical justifications in favor of online algorithms that rely on approximate Bayesian methods. Another point that is addressed in this thesis is the design of a universal estimation procedure. This question is of major interest, in particular because it leads to robust estimators, a very hot topic in statistics and machine learning. We tackle the problem of universal estimation using a minimum distance estimator based on the Maximum Mean Discrepancy. We show that the estimator is robust to both dependence and to the presence of outliers in the dataset. We also highlight the connections that may exist with minimum distance estimators using L2-distance. Finally, we provide a theoretical study of the stochastic gradient descent algorithm used to compute the estimator, and we support our findings with numerical simulations. We also propose a Bayesian version of our estimator, that we study from both a theoretical and a computational points of view.
In the context of a digital culture that affects the actual audiences and potential audiences of the museum, this thesis proposes to examine the ways in which the visitor's experience can be transformed. We wonder specifically about the appropriation and sharing processes that induce the museum practice in the era of digital culture. To explore this research item, we build an interdisciplinary methodological device between IT and Communication Sciences. It helps us to develop an analytical framework able to capture practices that are divided and combined between cyberspace and physical space. In this way, we propose hybrid and exploratory research protocols that provide the ability to capture, through their association, multiple practices of the visitor's experience in a triple temporality of the before, the during and the after visit of the museum. Through this thesis, we highlight how digital devices and culture play a part in the practices of the visitor's experience as a specific technical object and an exchange system and in which the visitor and potential visitor reconfigure their relationship to museum, its collection, exhibitions and other visitors and potential visitors.
This study is a new adaption of the systemic linguistic theory, created in 1987 by Sylviane CARDEY, where we propose a system of rules based on the linguistic facts and definitions and presented in the form of model that takes into account the principles of mathematical modelling. The objective of this thesis is to demonstrate to foreign language learners the mechanism of the language's operation, including its grammar, by appropriate methods and computer programs carefully formulated to provide a coherent teaching style. In this study, we propose a specific and detailed analysis for the rules of grammar, which allows us to obtain clear and comprehensive teaching material with which the learner can study the subject in its entirety, identifying its various rules and the correlation between them and between its domains. These items will subsequently be used in a practical framework which is the teaching of grammar. We have also the aim of building a web site accessible to teachers who might use it for their courses. In the second chapter, after having treated the theoretical part, we apply the theory to the conjugation of the Arabic verb. The corpus that we have chosen for this purpose consists of the 127 verbs of the ¨Bescherelle arabe¨. In this chapter, we use our system of rules to describe the Arabic conjugation system. The application of our theory has also highlighted some faults that we have found in various linguistic or other grammar manuals
Recording measurements about various phenomena and exchanging information about it, participate in the emergence of a type of data called time series. A time series is characterized by numerous points and interactions can be observed between those points. A time series is multivariate when multiple measures are recorded at each timestamp, meaning a point is, in fact, a vector of values. Even if univariate time series, one value at each timestamp, are well-studied and defined, it's not the case of multivariate one, for which the analysis is still challenging. None of the current techniques of classifying multivariate time series satisfies the following criteria, which are a low complexity of computation, dealing with variation in the number of points and good classification results. In our approach, we explored a new tool, which has not been applied before for MTS classification, which is called M-histogram. A M-histogram is a visualization tool using M axis to project the density function underlying the data. We have employed it here to produce a new representation of the data, that allows us to bring out the interactions between dimensions. Searching for links between dimensions correspond particularly to a part of learning techniques called multi-view learning. A view is an extraction of dimensions of a dataset, which are of same nature or type. Then the goal is to display the links between the dimensions inside each view in order to classify all the data, using an ensemble classifier. So we propose a multi-view ensemble model to classify multivariate time series. The model creates multiple M-histograms from differents groups of dimensions. Then each view allows us to get a prediction which we can aggregate to get a final prediction. In this thesis, we show that the proposed model allows a fast classification of multivariate time series of different sizes.
This thesis aims to demonstrate the emergence of a fourth wave of feminism in France, beginning in the early 2010s. A wave is here defined as a cycle of feminist protest that can be attested by the convergence of three interdependent and empirically testable criteria. The first criterion requires a noticeable growth in media interest for the women's cause over a given period. From a qualitative research point of view, this media interest should be met with an adoption of frames favorable to gender equality. The second criterion implies to observe a change in the ideas and / or practices of feminist movements. These changes are the sign of a sucessful adaptation of the movements to social and technical evolutions, proving that they have managed to create a discourse which is audible in a given environment – of what the existence of the first criterion attests. Finally, the third criterion is about generational renewal: its main interest is to historicize the analysis by confronting the emergence of a protest cycle to the evolution of the mobilizing discourse and of the conditions leading individuals to protest.
This research focuses on the integration of theatrical activities into the teaching and learning of oral expression in French as a foreign language. It also relies on the use of mobile phone and social network applications. The research questions the potential of the theatre as a teaching tool to help develop and improve student learners of French oral expression skills, with a particular focus on pronunciation. As part of an ethnographic action-research based on task-based learning, a blended language-learning programme combining face-to-face and distance learning was set up for French language and literature undergraduate students in Morocco. In order to evaluate the impact of the blended language-learning programme on the development of oral language skills, learners' oral productions were analyzed through phonological accuracy and fluency measures using a pre-test and a post-test. Similar measures were used to analyze audio recordings made during face-to-face learning sessions, in which learners practiced the reading and the rehearsal of extracts from a play in view of a final performance at the end of the training. The results show that the phonological accuracy developed significantly for all the learners, whereas fluidity developed for some learners only. The blended language-learning programme was perceived as a positive learning experience and students were globally satisfied with their participation. However, they did not give the same consideration for group work, which is essential to bring such a project to a successful end, nor were they all involved in the project in the same way.
Automatic speech processing is an active field of research since the 1950s. Within this field the main area of research is automatic speech recognition but simpler tasks such as speech activity detection, language identification or speaker identification are also of great interest to the community. The most recent breakthrough in speech processing appeared around 2010 when speech recognition systems using deep neural networks drastically improved the state-of-the-art. In this work, we closely look at a specific model for the RNNs: the Long Short Term Memory (LSTM) which mitigates a lot of the difficulties that can arise when training an RNN. We augment this model and introduce optimization methods that lead to significant performance gains for speech activity detection and language identification.
This research focuses on the possibility to compare prelinguistic utterances and linguistic utterances of the first words period. The definition of protoword and word notions is not clear; while we consider different approaches to determine them: historical, epistemological and experimental. The contribution of the historical approach is essential to identify the problem and to bethink how a society in different historical periods considers speech. This section allows us to highlight two elements: the question of the speech emergence implies the notion of social representation, and nowadays, the word emergence is during the first words period. The analysis of this period leads to our epistemological part which defines units of this period: protowords and words. Once units identified, we perform a longitudinal analysis of four children, from one to two years. Firstly, we identify a phenomenon of substitution of protowords in words. Secondly, we observe two common elements in these productions: prosody and phonology. We show that prosody provides a common framework to ensure the transition between protowords and words, and that phonology is the area where differences are observed: the words are the place for the development of complex phonological structures, unlike protowords. We consider that the emergence of speech is when the children prefer using words as verbal communication medium, instead of protowords, and that this feature is the object of phonological development.
As Twitter evolves into a ubiquitous information dissemination tool, understanding tweets in foreign languages becomes an important and difficult problem. Because of the inherent code-mixed, disfluent and noisy nature of tweets, state-of-the-art Machine Translation (MT) is not a viable option (Farzindar &amp; Inkpen, 2015). Indeed, at least for Hindi and Japanese, we observe that the percentage of "understandable" tweets falls from 80% for natives to below 30% for target (English or French) monolingual readers using Google Translate. Our starting hypothesis is that it should be possible to build generic tools, which would enable foreigners to make sense of at least 70% of “native tweets”, using a versatile “active reading” (AR) interface, while simultaneously determining the percentage of understandable tweets under which such a system would be deemed useless by intended users. We have thus specified a generic "SUFT" (System for Helping Understand Tweets), and implemented SUFT-1, an interactive multi-layout system based on AR, and easily configurable by adding dictionaries, morphological modules, and MT plugins. It is capable of accessing multiple dictionaries for each source language and provides an evaluation interface. For evaluations, we introduce a task-related measure inducing a negligible cost, and a methodology aimed at enabling a «continuous evaluation on open data», as opposed to classical measures based on test sets related to closed learning sets. We propose to combine understandability ratio and understandability decision time as a two-pronged quality measure, one subjective and the other objective, and experimentally ascertain that a dictionary-based active reading presentation can indeed help understand tweets better than available MT systems. In addition to gathering various lexical resources, we constructed a large resource of "word-forms" appearing in Indian tweets with their morphological analyses (viz.163221 Hindi word-forms from 68788 lemmas and 72312 Marathi word-forms from 6026 lemmas) for creating a multilingual morphological analyzer specialized to tweets, which can handle code-mixed tweets, compute unified features, and present a tweet with an attached AR graph from which foreign readers can intuitively extract a plausible meaning, if any.
Computer systems involving anomaly detection are emerging in both research and industry. Thus, fields as varied as medicine (identification of malignant tumors), finance (detection of fraudulent transactions), information technologies (network intrusion detection) and environment (pollution situation detection) are widely impacted. Machine learning offers a powerful set of approaches that can help solve these use cases effectively. It also involves several experts who will work together to find the right approaches. In addition, the possibilities opened today by the world of semantics show that it is possible to take advantage of web technologies to reason intelligently on raw data to extract information with high added value. The lack of systems combining numeric approaches to machine learning and semantic techniques of the web of data is the main motivation behind the various works proposed in this thesis. Finally, the anomalies detected do not necessarily mean abnormal situations in reality. Indeed, the presence of external information could help decision-making by contextualizing the environment as a whole. Exploiting the space domain and social networks makes it possible to build contexts enriched with sensor data. These spatio-temporal contexts thus become an integral part of anomaly detection and must be processed using a Big Data approach. Its originality lies in its ability to reason intelligently on raw data in order to infer implicit information from explicit information and assist in decision-making. This platform was developed as part of a FUI project whose main use case is the detection of anomalies in a drinking water network. RAMSSES: Hybrid machine learning system whose originality is to combine advanced numerical approaches as well as proven semantic techniques. SCOUTER: Intelligent system of "web scrapping" allowing the contextualization of singularities related to the Internet of Things by exploiting both spatial information and the web of data
The consideration of environmental issues is a very strong topic in our society nowadays. In the field of Product Development, Ecodesign is a methodology that allows the consideration of these issues by proposing to reduce the environmental impacts of products throughout their life cycle. The use phase of the life cycle is a critical step as the way how the products are managed can have significant impact on their environmental performance. We propose in this thesis to enrich the understanding of the use phase by highlighting the Kansei Information that can play a major role in the interaction between the user and the product and thus can be integrated in the early phase of the Ecodesign process. Our approach provides a better understanding the use phase, by contributing to the mastery of the environmental performance of products. Our research points out that the user can be defined not only from basic information commonly used in the User Centered Design, but also from subjective information brought by the Kansei dimension. We carry out experiments in which we implement two EcoKansei attributes corresponding to the values and emotions in order to illustrate user modeling for Ecodesign. The approach we propose is a part of EcoUse project that aims to develop a methodology for user-centered Ecodesign. Various contributions of our approach can be pointed out. For the Research, linking Ecodesign with the Kansei studies, which are basically unconnected leads to mutual enrichment between these two approaches. For Industry, an Eco-design approach supported by Kansei Information enables the design of new products which will have the benefits of both providing less impacts to the environment, and at the same time being more accepted by users as these products match with their environmental sensitivity.
All possible information sources will potentially be used in addition to the data provided by AMF, public information sources (specialized news agencies, social networks, WEB, etc.) and professionnal information sources like 'order books'(stock market transaction orders in natural language). Identification of the potentential infringement will be completed with a quantitative assessment of the considered risk and a ranking by decreasing order of importance of the documents justifying the alert, as well as the generation of a synthesis document recapitulating the extracted information that explain the alert (in a form yet to be defined, textual abstract in natural language, fact database, graphics,...) depending on the specificities of the systems using the detection algorithms (database hypercubes, watch dashboards etc.). The design of the algorithms will have to take into consideration scalability up to a Big Data context, requiring parallel computations [Bhatotia14]. The function expected of the algorithms developed during the thesis is to offer to inhouse AMF experts a preliminary analysis (weak signal detection [Sidhom11]) of the risk with all the justifying and complementary information.
Digital accessibility plays a crucial role for the education, the social inclusion and the autonomy of impaired people. This work focused on a universal component of digital documents: text formatting. Colors, fonts and text disposition are far more than just an ornament; text formatting conveys important meaning for content comprehension, and allows reader to optimize their activity. For instance, a specific set of colors and font can be enough to indicate the presence of a title, which allows a global representation of the content themes. Thus, we aimed at making text formatting meaning accessible to visually impaired people, so they can obtain the same information as sighted readers, and also benefit from the same optimizations when accessing the document with synthetic voices.
Nowadays, Artificial Intelligence (AI) is a widespread concept applied to many fields such as transportation, medicine and autonomous vehicles. The main AI algorithms are artificial neural networks, which can be divided into two families: Spiking Neural Networks (SNNs), which are bio-inspired models resulting from neuroscience, and Analog Neural Networks (ANNs), which result from machine learning. The ANNs are experiencing unprecedented success in research and industrial fields, due to their recent successes in many application contexts such as image classification and object recognition. However, they require considerable computational capacity for their deployment which is not adequate to very constrained systems. To overcome these limitations, many researchers are interested in brain-inspired computing, which would be the perfect alternative to conventional computers based on the Von Neumann architecture (CPU/GPU). This paradigm meets computing performance but not energy efficiency requirements. Hence, it is necessary to design neuromorphic hardware circuits adaptable to parallel and distributed computing. In this context, we have set criteria in terms of accuracy and hardware implementation cost to differentiate the two neural families (SNNs and ANNs). In the case of simple network topologies, we conducted a study that has shown that the spiking models have significant gains in terms of hardware cost when compared to the analog networks, with almost similar prediction accuracies. Therefore, the objective of this thesis is to design a generic neuromorphic architecture that is based on spiking neural networks. In an energy efficiency context, a thorough exploration of different neural coding paradigms for neural data representation in SNNs has been carried out. Moreover, new derivative versions of rate-based coding have been proposed that aim to get closer to the activity produced by temporal coding, which is characterized by a reduced number of spikes propagating in the network. In this way, the number of spikes can be reduced so that the number of events to be processed in the SNNs gets smaller. The aim in doing this approach is to reduce the underlying energy consumption. The proposed coding approaches are: First Spike, which is characterized using at most one single spike to present an input data, and Spike Select, which allows to regulate and minimize the overall spiking activity in the SNN.In the RTL design exploration, we quantitatively compared three SNN architectural models having different levels of computing parallelism and multiplexing. Using Spike Select coding results in a distribution regulation of the spiking data, with most of them generated within the first layer and few of them propagate into the deep layers. Such distribution benefits from a so-called 'hybrid architecture' that includes a fully-parallel part for the first layer and multiplexed parts to the other layers. Therefore, combining the Spike Select and the Hybrid Architecture would be an effective solution for embedded AI applications, with an efficient hardware and latency trade-off. Finally, based on the architectural and neural choices resulting from the previous exploration, we have designed a final event-based architecture dedicated to SNNs supporting different neural network types and sizes. The architecture supports the most used layers: convolutional, pooling and fully-connected. Using this architecture, we will be able to compare analog and spiking neural networks on realistic applications and to finally conclude about the use of SNNs for Embedded Artificial Intelligence.
This work studies a new situation of communication: communication via e-mail. Our study is more precisely focussed on e-mails sent by customers to firms and this, in the field of air tourism (concept of e-crm). To carry out our linguistic analyses, we constituted an important corpus of messages collected on Internet forums and dealing with travel. Our goal is to manage the categorization and thematisation of e-mails. We thus gathered lexical, syntactic, morpho-syntactic and semantic features which are specific to the concept of spatiality, toponymy and characteristic of air tourism sub-language. We also underline how a linguistic analysis of spatiality is linked to a temporal analysis of the sentence. Moreover, we choose to analyze emotional informations contained in our messages. In the last part of our work, we show how our work deals with mining systems. We show how statistical techniques are limited as soon as it is a question of treating linguistically complex statements such as ours. Our approach is hybrid: it is made of key words, synonyms dictionaries, scripts on the model of SCHANK and ABELSON, but especially knowledge modeling. We give some examples of computerization of our system thanks to XML, PROLOG and Perl
Video surveillance systems are of a great value for public safety. As one of the most import surveillance applications, person re-identification is defined as the problem of identifying people across images that have been captured by different surveillance cameras without overlapping fields of view. The attributes are defined as semantic mid-level descriptions of persons, such as gender, accessories, clothing etc. The same person shows very different appearances from different points of view. To deal with this issue, we consider that the images under various orientations are from different domains. The combined orientation-specific CNN feature representations are used for the person re-identification task. Thirdly, learning a similarity metric for person images is a crucial aspect of person re-identification. As the third contribution, we propose a novel listwise loss function taking into account the order in the ranking of gallery images with respect to different probe images. In this case, using only the appearance of single person leads to strong ambiguities. As the last contribution, we propose to learn a deep feature representation with displacement invariance for group context and introduce a method to combine the group context and single-person appearance.
It is important to regularly assess the technological innovation products in order to estimate the level of maturity reached by the technology and study the applications frameworks in which they can be used. For years, the different technological modules from the NLP were developed separately. The new challenge in terms of evaluation is then to evaluate the different modules while taking into account the applicative context. We describes the tasks of ASR and NER proposed during several evalution campaigns and we discuss the protocols established for their evaluation. We also point the limitations of modular evaluation approaches and we expose the alternatives measures proposed in the literature. In the second part we describe the studied task of named entities detection, classification and decomposition and we propose a new metric ETER (Entity Tree Error Rate) which allows to take into account the specificity of the task and the applicative context during the evaluation. ETER also eliminates the biases observed with the existing metrics. Rather than directly comparing reference and hypothesis transcriptions, ATENE measure how harder it becames to identify entities given the differences between hypothesis and reference by comparing an estimated likelihood of presence of entities. It is composed of two elementary measurements.
This thesis presents a generic method for automatic recognition of emotions from a bimodal system based on facial expressions and physiological signals. This data processing approach leads to better extraction of information and is more reliable than single modality. The proposed algorithm for facial expression recognition is based on the distance variation of facial muscles from the neutral state and on the classification by means of Support Vector Machines (SVM). And the emotion recognition from physiological signals is based on the classification of statistical parameters by the same classifier. In order to have a more reliable recognition system, we have combined the facial expressions and physiological signals. The direct combination of such information is not trivial giving the differences of characteristics (such as frequency, amplitude, variation, and dimensionality). To remedy this, we have merged the information at different levels of implementation. At feature-level fusion, we have tested the mutual information approach for selecting the most relevant and principal component analysis to reduce their dimensionality. For decision-level fusion we have implemented two methods; the first based on voting process and another based on dynamic Bayesian networks. The optimal results were obtained with the fusion of features based on Principal Component Analysis. These methods have been tested on a database developed in our laboratory from healthy subjects and inducing with IAPS pictures. A self-assessment step has been applied to all subjects in order to improve the annotation of images used for induction. The obtained results have shown good performance even in presence of variability among individuals and the emotional state variability for several days
A very broad definition of music structure is to consider what distinguishes music from random noise as part of its structure. In this thesis, we take interest in the macroscopic aspects of music structure, especially the decomposition of musical pieces into autonomous segments (typically, sections) and their characterisation as the result of the grouping process of jointly compressible units. An important assumption of this work is to establish a link between the inference of music structure and information theory concepts such as complexity and entropy. We thus build upon the hypothesis that structural segments can be inferred through compression schemes. In a first part of this work, we study Straight-Line Grammars (SLGs), a family of formal grammars originally used for structure discovery in biological sequences (Gallé, 2011), and we explore their use for the modelisation of musical sequences. The SLG approach enables the compression of sequences, depending on their occurrence frequencies, resulting in a tree-based modelisation of their hierarchical organisation. We develop several adaptations of this method for the modelisation of approximate repetitions and we develop several regularity criteria aimed at improving the efficiency of the method. The second part of this thesis develops and explores a novel approach for the inference of music structure, based on the optimisation of a tensorial compression criterion. This approach aims to compress the musical information on several simultaneous time-scales by exploiting the similarity relations, the logical progressions and the analogy systems which are embedded in musical segments. The proposed method is first introduced from a formal point of view, then presented as a compression scheme rooted in a multi-scale extension of the System &amp; Contrast model (Bimbot et al., 2012) to hypercubic tensorial patterns Furthermore, we generalise the approach to other, irregular, tensorial patterns, in order to account for the great variety of structural organisations observed in musical segments. The methods presented in this thesis are tested on a structural segmentation task using symbolic data, chords sequences from pop music (RWC-Pop). The methods are evaluated and compared on several sets of chord sequences, and the results establish an experimental advantage for the approaches based on a complexity criterion for the analysis of structure in music information retrieval, with the best variants offering F-measure scores around 70%.
Extracting information from linguistic data has gain more and more attention in the last decades in relation with the increasing amount of information that has to be processed on a daily basis in the world. Since the 90's, this interest for information extraction has converged to the development of researches on speech data. In fact, speech data involves extra problems to those encountered on written data. In particular, due to many phenomena specific to human speech (e.g. hesitations, corrections, etc.). But also, because automatic speech recognition systems applied on speech signal potentially generates errors. Thus, extracting information from audio data requires to extract information by taking into account the "noise" inherent to audio data and output of automatic systems. Thus, extracting information from speech data cannot be as simple as a combination of methods that have proven themselves to solve the extraction information task on written data. It comes that, the use of technics dedicated for speech/audio data processing is mandatory, and epsecially technics which take into account the specificites of such data in relation with the corresponding signal and transcriptions (manual and automatic). This problem has given birth to a new area of research and raised new scientific challenges related to the management of the variability of speech and its spontaneous modes of expressions. Furthermore, robust analysis of phone conversations is subject to a large number of works this thesis is in the continuity. More specifically, this thesis focuses on edit disfluencies analysis and their realisation in conversational data from EDF call centres, using speech signal and both manual and automatic transcriptions. This work is linked to numerous domains, from robust analysis of speech data to analysis and management of aspects related to speech expression. The aim of the thesis is to propose appropriate methods to deal with speech data to improve text mining analyses of speech transcriptions (treatment of disfluencies). To address these issues, we have finely analysed the characteristic phenomena and behavior of spontaneous speech (disfluencies) in conversational data from EDF call centres and developed an automatic method for their detection using linguistic, prosodic, discursive and para-linguistic features. The contributions of this thesis are structured in three areas of research. First, we proposed a specification of call centre conversations from the prespective of the spontaneous speech and from the phenomena that specify it. Second, we developed (i) an enrichment chain and effective processings of speech data on several levels of analysis (linguistic, acoustic-prosodic, discursive and para-linguistic) ; (ii) an system which detect automaticaly the edit disfluencies suitable for conversational data and based on the speech signal and transcriptions (manual or automatic). Third, from a "resource" point of view, we produced a corpus of automatic transcriptions of conversations taken from call centres which has been annotated in edition disfluencies (using a semi-automatic method).
Recommendation system for personalized sightseeing tours
Diffusion Magnetic Resonance Imaging (dMRI) is a meaningful technique for white matter (WM) fiber-tracking and microstructural characterization of axonal/neuronal integrity and connectivity. By measuring water molecules motion in the three directions of space, numerous parametric maps can be reconstructed. Among these, fractional anisotropy (FA), mean diffusivity (MD), and axial (λa) and radial (λr) diffusivities have extensively been used to investigate brain diseases. Overall, these findings demonstrated that WM and grey matter (GM) tissues are subjected to numerous microstructural alterations in multiple sclerosis (MS). However, it remains unclear whether these tissue alterations result from global processes, such as inflammatory cascades and/or neurodegenerative mechanisms, or local inflammatory and/or demyelinating lesions. Furthermore, these pathological events may occur along afferent or efferent WM fiber pathways, leading to antero- or retrograde degeneration. Thus, for a better understanding of MS pathological processes like its spatial and temporal progression, an accurate and sensitive characterization of WM fibers along their pathways is needed. By merging the spatial information of fiber tracking with the diffusion metrics derived obtained from longitudinal acquisitions, WM fiber-bundles could be modeled and analyzed along their profile. Such signal analysis of WM fibers can be performed by several methods providing either semi- or fully unsupervised solutions. In the first part of this work, we will give an overview of the studies already present in literature and we will focus our analysis on studies showing the interest of dMRI for WM characterization in MS. In the second part, we will introduce two new string-based methods, one semi-supervised and one unsupervised, to extract specific WM fiber-bundles. We will show how these algorithms allow to improve extraction of specific fiber-bundles compared to the approaches already present in literature. Moreover, in the second chapter, we will show an extension of the proposed method by coupling the string-based formalism with the spatial information of the fiber-tracks. In the third, and last part, we will describe, in order of complexity, three different fully automated algorithms to perform analysis of longitudinal changes visible along WM fiber-bundles in MS patients. These methods are based on Gaussian mixture model, nonnegative matrix and tensor factorisation respectively. Moreover, in order to validate our methods, we introduce a new model to simulate real longitudinal changes based on a generalised Gaussian probability density function. For those algorithms high levels of performances were obtained for the detection of small longitudinal changes along the WM fiber-bundles in MS patients. In conclusion, we propose, in this work, a new set of unsupervised algorithms to perform a sensitivity analysis of WM fiber bundle that would be useful for the characterisation of pathological alterations occurring in MS patients
Multi-day events such as conventions, festivals, cruise trips, to which we refer to as distributed events, have become very popular in recent years, attracting hundreds or thousands of participants. Their programs are usually very dense, making it challenging for the attendees to make a decision which events to join. Recommender systems appear as a common solution in such an environment. While many existing solutions deal with personalised recommendation of single items, recent research focuses on the recommendation of consecutive items that exploits user's behavioural patterns and relations between entities, and handles geographical and temporal constraints. Second, we propose an approach (ANASTASIA) to solve this problem, which aims at providing an integrated support for users to create a personalised itinerary of activities. ANASTASIA brings together three components, namely: (1) estimation of the user's interest in single items, (2) use of sequential influence on activity performance, and (3) building of an itinerary that takes into account spatio-temporal constraints. Thus, the proposed solution makes use of the methods based on sequence learning and discrete optimisation. Moreover, stating the lack of publicly available datasets that could be used for the evaluation of event and itinerary recommendation algorithms, we have created two datasets, namely: (1) event attendance on board of a cruise (Fantasy_db) based on a conducted user study, and (2) event attendance at a major comic book convention (DEvIR). This allows to perform evaluation of recommendation methods, and contributes to the reproducibility of results.
Social interactions are in the core of economic activities. Their treatment in Economies is however often limited to a focus on the market (Manski, 2000). The role social interactions themselves play for the behavior of agents as well as the formation of their attitudes is often neglected. This is despite the fact that already early contributions in economic literature have identified them as important determinants for the decision making of economic agents as for example Sherif (I936), Hyman (1942), Asch (1951), Jahoda (1959) or Merton (1968). ln consumer research, a field on the intersection between Economies, Sociology and Psychology, on the other hand social interactions (social influences) are considered to be the"... most pervasive determinants [...] of individual 's behaviour... " (Bumkrant and Cousineau, 1975). The thesis at hand bridges the gap between social interactions and their influence on agents expectation formation and behavior.
In this thesis, we present contributions to the challenging issues which are encountered in question answering and locating information in complex textual data, like log files. Question answering systems (QAS) aim to find a relevant fragment of a document which could be regarded as the best possible concise answer for a question given by a user. In this work, we are looking to propose a complete solution to locate information in a special kind of textual data, i.e., log files generated by EDA design tools. Nowadays, in many application areas, modern computing systems are instrumented to generate huge reports about occurring events in the format of log files. Log files are generated in every computing field to report the status of systems, products, or even causes of problems that can occur. Log files may also include data about critical parameters, sensor outputs, or a combination of those. Analyzing log files, as an attractive approach for automatic system management and monitoring, has been enjoying a growing amount of attention [Li et al., 2005]. Although the process of generating log files is quite simple and straightforward, log file analysis could be a tremendous task that requires enormous computational resources, long time and sophisticated procedures [Valdman, 2004]. Indeed, there are many kinds of log files generated in some application domains which are not systematically exploited in an efficient way because of their special characteristics. In this thesis, we are mainly interested in log files generated by Electronic Design Automation (EDA) systems. Electronic design automation is a category of software tools for designing electronic systems such as printed circuit boards and Integrated Circuits (IC). In this domain, to ensure the design quality, there are some quality check rules which should be verified. Verification of these rules is principally performed by analyzing the generated log files. In the case of large designs that the design tools may generate megabytes or gigabytes of log files each day, the problem is to wade through all of this data to locate the critical information we need to verify the quality check rules. We investigate throughout this work the main concern "how the specificities of log files can influence the information extraction and natural language processing methods?". In this context, a key challenge is to provide approaches that take the log file specificities into account while considering the issues which are specific to QA in restricted domains. We present different contributions as below: Within this approach, we propose an original type of descriptor to model the textual structure and layout of text documents. > Proposing an approach to locate the requested information in the log files based on passage retrieval. To improve the performance of passage retrieval, we propose a novel query expansion approach to adapt an initial query to all types of corresponding log files and overcome the difficulties like mismatch vocabularies. Our method is based on a new term weighting function, called TRQ (Term Relatedness to Query), introduced in this work, which gives a score to terms of corpus according to their relatedness to the query. We also investigate how to apply our query expansion approach to documents from general domains. > Studying the use of morpho-syntactic knowledge in our approaches. For this purpose, we are interested in the extraction of terminology in the log files. Thus, we here introduce our approach, named Exterlog (EXtraction of TERminology from LOGs), to extract the terminology of log files. To evaluate the extracted terms and choose the most relevant ones, we propose a candidate term evaluation method using a measure, based on the Web and combined with statistical measures, taking into account the context of log files.
This dissertation examines the construction of meaning and the crosslinguistic influence on the representation of the lexical signification and discursive meaning of the words work and travail. This study is based on definitional discourse and significations such as they are proposed by speakers articulating their metalinguistic semantic knowledge in the context of experimental research. The cultural concept of work is analysed by using Galatanu's theoretical framework The Semantics of Argumentative Possibilities (SAP). A French/English constractive study allowed us to determine the similarities as well as the differences between the representations of the lexical signification and discursive meaning of the words “work” and “travail” proposed by four groups of speakers. Each group of speakers having its own linguistic profile (two groups of monolingual speakers – Francophone and Anglophone, one group of bilingual speakers whose native language is French and who speak English, and one group of bilingual speakers whose native language is English and who speak French) presents the opportunity to not only compare the variations between the representations of the lexical signification and discursive meaning of the words “work” and “travail”, but also to take note of the potential influence that competence in a foreign language can have on these representations. The second part of this dissertation includes an analysis of the representation of the insult loser as used in the United States. Inspired by the presence of an element linking work and said insult, the choice to present loser lends itself to exhibiting the discursive mobilisation of the social values instilled in the words work and loser.
Knowledge bases are huge collections of primarily encyclopedic facts. They are widely used in entity recognition, structured search, question answering, and other tasks. These knowledge bases have to be curated, and this is a crucial but costly task. In this thesis, we are concerned with curating knowledge bases automatically using constraints. Our first contribution aims at discovering constraints automatically. We improve standard rule mining approaches by using (in-)completeness meta-information. We show that this information can increase the quality of the learned rules significantly. Our second contribution is the creation of a knowledge base, YAGO 4, where we statically enforce a set of constraints by removing the facts that do not comply with them. Our last contribution is a method to correct constraint violations automatically. Our method uses the edit history of the knowledge base to see how users corrected violations in the past, in order to propose corrections for the present.
Artificial Intelligence (AI) and Human-Computer Interactions (HCIs) are two research fields with relatively few common work. HCI specialists usually design the way we interact with devices directly from observations and measures of human feedback, manually optimizing the user interface to better fit users'expectations. This process is hard to optimize: ergonomy, intuitivity and ease of use are key features in a User Interface (UI) that are too complex to be simply modelled from interaction data. This drastically restrains the possible uses of Machine Learning (ML) in this design process. Currently, ML in HCI is mostly applied to gesture recognition and automatic display, e.g. advertisement or item suggestion. It is also used to fine tune an existing UI to better optimize it, but as of now it does not participate in designing new ways to interact with computers. Our main focus in this thesis is to use ML to develop new design strategies for overall better UIs. We want to use ML to build intelligent – understand precise, intuitive and adaptive – user interfaces using minimal handcrafting. We propose a novel approach to UI design: instead of letting the user adapt to the interface, we want the interface and the user to adapt mutually to each other. The goal is to reduce human bias in protocol definition while building co-adaptive interfaces able to further fit individual preferences. In order to do so, we will put to use the different mechanisms available in ML to automatically learn behaviors, build representations and take decisions. We will be experimenting on touch interfaces, as these interfaces are vastly used and can provide easily interpretable problems. The very first part of our work will focus on processing touch data and use supervised learning to build accurate classifiers of touch gestures. The second part will detail how Reinforcement Learning (RL) can be used to model and learn interaction protocols given user actions. Lastly, we will combine these RL models with unsupervised learning to build a setup allowing for the design of new interaction protocols without the need for real user data.
The proliferation of digital data has enabled scientific and practitioner communities to create new data-driven technologies to learn about user behaviors in order to deliver better services and support to people in their digital experience. The majority of these technologies extensively derive value from data logs passively generated during the human-computer interaction. A particularity of these behavioral traces is that they are structured. However, the pro-actively generated text across Internet is highly unstructured and represents the overwhelming majority of behavioral traces. To date, despite its prevalence and the relevance of behavioral knowledge to many domains, such as recommender systems, cyber-security and social network analysis,the digital text is still insufficiently tackled as traces of human behavior to automatically reveal extensive insights into behavior. Multiple original contributions are made. The only systematic study to date on the automatic modeling of asynchronous communication with speech intentions is conducted. A corpus-independent, automatic method to annotate utterances of asynchronous communication with the proposed speech intention taxonomy is designed based on supervised machine learning. For this, validated ground-truth corpora arecreated and groups of features—discourse, content and conversation-related, are engineered to be used by the classifiers. In particular, some of the discourse features are novel and defined by considering linguistic means to express speech intentions, without relying on the corpus explicit content, domain or on specificities of the asynchronous communication types. Then, an automatic method based on process mining is designed to generate process models of interrelated speech intentions from conversation turns, annotated with multiple speech intentions per sentence. As process mining relies on well-defined structured event logs, an algorithm to produce such logs from conversations is proposed. Additionally, an extensive design rationale on how conversations annotated with multiple labels per sentence could be transformed in event logs and what is the impact of different decisions on the output behavioral models is released to support future research.
We address the difficulty of creating a digitised corpus by using a crowdsourced approach for annotating comic books. The resulting XML-based encodings assist researchers, publishers and collection curators equally. To achieve our data collection goal, we develop an online crowdsourcing engine for annotating comics. The tasks are designed to mirror the page reading experience, with participants asked to identify and annotate structural (panel layout, splash pages, meta-panels) and content (characters, places, events, onomatopoeia) elements of comic books. Curators and collectors of physical or online comics collections are provided with a structured content which could enable the creation of artefacts such as comic books dictionaries, search indices and dictionaries of onomatopoeia. From a publishing perspective, current standards for digital comics are taking care exclusively of the presentation layer (i.e. rendering a publication on the screen of a device). But the artistic nature of comics and the great potential digital comics have already showcased allow us to go beyond simple content presentation. To this respect we present our contributions with enhancements to current semantic (CBML) and presentation (EPUB) open standards that will allow publishers and digital comics authors to create an improved reading experience.
The human faculties of understanding are essentially multimodal. To understand the world around them, human beings fuse the information coming from all of their sensory receptors. The aim of this thesis is to propose joint processes applying mainly to text and image for the processing of multimodal documents through two studies: one on multimodal fusion for the speaker role recognition in television broadcasts, the other on the complementarity of modalities for a task of linguistic analysis on corpora of images with captions. In the first part of this study, we interested in audiovisual documents analysis from news television channels. We propose an approach that uses in particular deep neural networks for representation and fusion of modalities. In the second part of this thesis, we are interested in approaches allowing to use several sources of multimodal information for a monomodal task of natural language processing in order to study their complementarity. We propose a complete system of correction of prepositional attachments using visual information, trained on a multimodal corpus of images with captions.
Our perspectives are educational, to create grammar exercises for French. Paraphrasing is an operation of reformulation. Our work tends to attest that sequence-to-sequence models are not simple repeaters but can learn syntax. First, by combining various models, we have shown that the representation of information in multiple forms (using formal data (RDF), coupled with text to extend or reduce it, or only text) allows us to exploit a corpus from different angles, increasing the diversity of outputs, exploiting the syntactic levers put in place. We also addressed a recurrent problem, that of data quality, and obtained paraphrases with a high syntactic adequacy (up to 98% coverage of the demand) and a very good linguistic level. We obtain up to 83.97 points of BLEU-4*, 78.41 more than our baseline average, without syntax leverage. This rate indicates a better control of the outputs, which are varied and of good quality in the absence of syntax leverage. The transition to French text was also an imperative for us. Working from plain text, by automating the procedures, allowed us to create a corpus of more than 450,000 sentence/representation pairs, thanks to which we learned to generate massively correct texts (92% on qualitative validation). Anonymizing everything that is not functional contributed significantly to the quality of the results (68.31 of BLEU, i.e. +3.96 compared to the baseline, which was the generation of text from non-anonymized data). The formal representation of information in a language-specific framework is a challenging task. This thesis offers some ideas on how to automate this operation. Moreover, we were only able to process relatively short sentences. The use of more recent neural modelswould likely improve the results. The use of appropriate output strokes would allow for extensive checks. *BLEU: quality of a text (scale from 0 (worst) to 100 (best), Papineni et al. (2002))
An innumerable number of documents is being printed, scanned, faxed, photographed every day. These documents are hybrid: they exist as both hard copies and digital copies. Moreover their digital copies can be viewed and modified simultaneously in many places. With the availability of image modification software, it has become very easy to modify or forge a document. This creates a rising need for an authentication scheme capable of handling these hybrid documents. Current solutions rely on separate authentication schemes for paper and digital documents. In order to overcome all these issues we propose to create a semantic hashing algorithm for document images. This hashing algorithm should provide a compact digest for all the visually significant information contained in the document. This digest will allow current hybrid security systems to secure all the document. This can be achieved thanks to document analysis algorithms. After defining the context of this study and what is a stable algorithm, we focused on producing stable algorithms for layout description, document segmentation, character recognition and describing the graphical parts of a document.
The goal of this thesis is to develop models linking Markov models and Neural Networks, and to study their applications, particularly for Question Answering. Then, we want to use these new models in conversational agents, also called chatbots. Today, the most used algorithms for Question Answering are based on Neural Networks. Those can be interpreted as hidden probabilistic models, where the same problem can be addressed by Markov Models. Indeed, for fifteen years, those had been extended to Pairwise and Triplet Markov Models, allowing us to obtain results sometimes impressive. The PhD student will be inspired by the steps bringing the creation of pairwise and triplet Markov models from the hidden Markov model to propose new neural network models "pairwise" and "triplet", and hybrids models with neural networks and Markov models. He will search applications for these new models, particularly for Question Answering, and will study the contribution of these new models to the classic Neural Networks.
Creating an emergency call centre dedicated to deaf and hard of hearing users has required the design and implementation of a specific socio-technical system in order to take into account the diversity of their communication practices. Our research is grounded on such technical project specifically oriented towards the accessibility to an organization acting mainly through telephonic interactions and therefrom emerged for us the opportunity to conduct a cross-reflection on socio-technical devices apprehended as a working tool and an accessibility tool. Based on elements obtained through an ethnographic study conducted in emergency call centres which has given empirical material that we are bringing into discussion with traces of the process of the product design, we are questioning the place and status of communication devices in the local environment. Along with this questioning, we are also observing how communication devices practically achieve accessibility within the social model of disability. Two main themes are therefore explored: the organization of work activities in situations of mediated communication and the process, engaged by the design and implementation of an accessibility device, by which conventional practices are reformulated. Through a refined description of the multiple courses of action and a subtle attention to the materiality of interactional practices, we demonstrate that communication technical devices are a piece of an heterogeneous assembly, that make sense in situation, and act as an invisible scaffolding that organize and stabilize communication practices. We propose to designate this form of stabilization under the name of communicational infrastructure and to use it as a conceptual tool for approaching the introduction of new communication devices in organizations and to support a reflection on accessibility-in-practices
Natural language processing systems often rely on the idea that language is compositional, that is, the meaning of a linguistic entity can be inferred from the meaning of its parts. This expectation fails in the case of multiword expressions (MWEs). For example, a person who is a "sitting duck" is neither a duck nor necessarily sitting. Modern computational techniques for inferring word meaning based on the distribution of words in the text have been quite successful at multiple tasks, especially since the rise of word embedding approaches. However, the representation of MWEs still remains an open problem in the field. In particular, it is unclear how one could predict from corpora whether a given MWE should be treated as an indivisible unit (e.g. "nut case") or as some combination of the meaning of its parts (e.g. "engine room"). This thesis proposes a framework of MWE compositionality prediction based on representations of distributional semantics, which we instantiate under a variety of parameters. We present a thorough evaluation of the impact of these parameters on three new datasets of MWE compositionality, encompassing English, French and Portuguese MWEs. Finally, we present an extrinsic evaluation of the predicted levels of MWE compositionality on the task of MWE identification. Our results suggest that the proper choice of distributional model and corpus parameters can produce compositionality predictions that are comparable to the state of the art.
In preparation for the usage of distributed infrastructures to further accelerate the computation, this study aims at exploring the possibility of executing the Frank-Wolfe algorithm in a star network with the Bulk Synchronous Parallel (BSP) model and investigating its efficiency both theoretically and empirically. In the theoretical aspect, this study revisits Frank-Wolfe's fundamental deterministic sublinear convergence rate and extends it to nondeterministic cases. In particular, it shows that with the linear subproblem appropriately solved, Frank-Wolfe can achieve a sublinear convergence rate both in expectation and with high probability. This contribution lays the theoretical foundation of using power iteration or Lanczos iteration to solve the linear subproblem for trace norm minimization. In the algorithmic aspect, within the BSP model, this study proposes and analyzes four strategies for the linear subproblem as well as methods for the line search. Moreover, noticing Frank-Wolfe's rank-1 update property, it updates the gradient recursively, with either a dense or a low-rank representation, instead of repeatedly recalculating it from scratch. All of these designs are generic and apply to any distributed infrastructures compatible with the BSP model. In the empirical aspect, this study tests the proposed algorithmic designs in an Apache SPARK cluster. According to the experiment results, for the linear subproblem, centralizing the gradient or averaging the singular vectors is sufficient in the low-dimensional case, whereas distributed power iteration, with as few as one or two iterations per epoch, excels in the high-dimensional case. The Python package developed for the experiments is modular, extensible and ready to deploy in an industrial context. This study has achieved its function as proof of concept. Following the path it sets up, solvers can be implemented for various infrastructures, among which GPU clusters, to solve practical problems in specific contexts. Besides, its excellent performance in the ImageNet dataset makes it promising for deep learning.
An ontology mapping is a set of correspondences. Each correspondence relates artifacts, such as concepts and properties, of one ontology to artifacts of another ontology. In the last few years, a lot of attention has been paid to establish mappings between source ontologies. Ontology mapping is widely and effectively used for interoperability and integration tasks (data transformation, query answering, or web-service composition, to name a few), and in the creation of new ontologies. On the one side, checking the (logical) correctness of ontology mappings has become a fundamental prerequisite of their use. On the other side, given two ontologies, there are several ontology mappings between them that can be obtained by using different ontology matching methods or just stated manually. Using ontology mappings between two ontologies in combination within a single application or for synthesizing one mapping taking the advantage of two original mappings, may cause errors in the application or in the synthesized mapping because those original mappings may be contradictory (conflicting). In both situations, correctness is usually formalized and verified in the context of fully formalized ontologies (e.g. in logics), even if some “weak” notions of correctness have been proposed when ontologies are informally represented or represented in formalisms preventing a formalization of correctness (such as UML). Verifying correctness is usually performed within one single formalism, requiring on the one side that ontologies need to be represented in this unique formalism and, on the other side, a formal representation of mapping is provided, equipped with notions related to correctness (such as consistency). In practice, there exist several heterogeneous formalisms for expressing ontologies, ranging from informal (text, UML and others) to formal (logical and algebraic). This implies that, willing to apply existing approaches, heterogeneous ontologies should be translated (or just transformed if, the original ontology is informally represented or when full translation, keeping equivalence, is not possible) in one common formalism, mappings need each time to be reformulated, and then correctness can be established. This is possible but possibly leading to correct mappings under one translation and incorrect mapping under another translation. Indeed, correctness (e.g. consistency) depends on the underlying employed formalism in which ontologies and mappings are expressed. Specifically ontologies are represented as lattices and mappings as functions between those lattices. Lattices are natural structures for directly representing ontologies, without changing the original formalisms in which ontologies are expressed. As a consequence, the (unified) notion of correctness has been reformulated by using Galois connection condition, leading to the new notion of compatible and incompatible mappings. It is formally shown that the new notion covers the reviewed correctness notions, provided in distinct state of the art formalisms, and, at the same time, can naturally cover heterogeneous ontologies. The usage of the proposed unified approach is demonstrated by applying it to upper ontology mappings. Notion of compatible and incompatible ontology mappings is also applied on domain ontologies to highlight that incompatible ontology mappings give incorrect results when used for ontology merging.
Several recent discoveries, notably from cognitive neuroscience, have had repercussions in the humanities and more particularly in the linguistics' field. They have given rise to a renewed interest in the theme of phonological iconicity, which tackles the entirety of phenomena of similarity between signifier and signified inside a language. A multitude of studies then emerged, attesting to the existence of phonosymbolic phenomena in the languages of the world. Despite this considerable growth, the content of these works, mainly written in English, remains to this day rather unknown to the French-speaking public, still relatively anchored in the tradition of Structuralism. This dissertation starts by presenting a synthesis of the international work carried out in the field of phonological iconicity in order to retain the main achievements. Building on this, the dissertation will bring new empirical evidence of the existence of iconic phenomena in a corpus of French monosyllabic verbs through two methods.
Machine Translation (MT) systems, which generate automatically the translation of a target language for each source sentence, have achieved impressive gains during the recent decades and are now becoming the effective language assistances for the entire community in a globalized world. Nonetheless, due to various factors, MT quality is still not perfect in general, and the end users therefore expect to know how much should they trust a specific translation. Building a method that is capable of pointing out the correct parts, detecting the translation errors and concluding the overall quality of each MT hypothesis is definitely beneficial for not only the end users, but also for the translators, post-editors, and MT systems themselves. Such method is widely known under the name Confidence Estimation (CE) or Quality Estimation (QE). This thesis mostly focuses on the CE methods at word level (WCE). The WCE classifier tags each word in the MT output a quality label. Nowadays, WCE shows an increasing importance in many aspects of MT. Firstly, it assists the post-editors to quickly identify the translation errors, hence improve their productivity. Secondly, it informs readers of portions of sentence that are not reliable to avoid the misunderstanding about the sentence's content. Thirdly, it selects the best translation among options from multiple MT systems. Last but not least, WCE scores can help to improve the MT quality via some scenarios: N-best list re-ranking, Search Graph Re-decoding, etc. In this thesis, we aim at building and optimizing our baseline WCE system, then exploiting it to improve MT and Sentence Confidence Estimation (SCE). Compare to the previous approaches, our novel contributions spread of these following main points. Firstly, we integrate various types of prediction indicators: system-based features extracted from the MT system, together with lexical, syntactic and semantic features to build the baseline WCE systems. We also apply multiple Machine Learning (ML) models on the entire feature set and then compare their performances to select the optimal one to optimize. Secondly, the usefulness of all features is deeper investigated using a greedy feature selection algorithm. Thirdly, we propose a solution that exploits Boosting algorithm as a learning method in order to strengthen the contribution of dominant feature subsets to the system, thus improve of the system's prediction capability. Lastly, we explore the contributions of WCE in improving MT quality via some scenarios. In N-best list re-ranking, we synthesize scores from WCE outputs and integrate them with decoder scores to calculate again the objective function value, then to re-order the N-best list to choose a better candidate. In the decoder's search graph re-decoding, the proposition is to apply WCE score directly to the nodes containing each word to update its cost regarding on the word quality. Furthermore, WCE scores are used to build useful features, which can enhance the performance of the Sentence Confidence Estimation system. In total, our work brings the insightful and multidimensional picture of word quality prediction and its positive impact on various sectors for Machine Translation. The promising results open up a big avenue where WCE can play its role, such as WCE for Automatic Speech Recognition (ASR) System, WCE for multiple MT selection, and WCE for re-trainable and self-learning MT systems.
Recent progresses in animation have allowed the use of virtual character to many extents. However, automatic generation of animation for such characters strongly rely on the lexical description of signs. Signs described through these models are usually perfect and geometric performances leading to robotic and unrealistic movements. These thesis focuses on adding information to the control skeleton of the signer to help him perform signs in a more human and realistic way. Such information are grouped under the name of "anatomic model" and are divided in five main contributions: a new computer-based description of the skeleton, an anthropometric study of the hand, the merging of articulatory dependencies, a new model of the carpo-metacarpal complex allowing an easier opposition of the thumb and finally a model computing posture comfort. These contributions are then implemented in a generation system through linguistic contraints-adapted techniques. The study ends with an evaluation of the system and a the presentation of future prospects.
In computational linguistics, the relation between different languages is often studied through automatic alignment techniques. Such alignments can be established at various structural levels. In particular, sentential and sub-sentential bitext alignments constitute an important source of information in various modern Natural Language Processing (NLP) applications, a prominent one being Machine Translation (MT). Effectively computing bitext alignments, however, can be a challenging task. Discrepancies between languages appear in various ways, from discourse structures to morphological constructions. Automatic alignments would, at least in most cases, contain noise harmful for the performance of application systems which use the alignments. To deal with this situation, two research directions emerge: the first is to keep improving alignment techniques; the second is to develop reliable confidence measures which enable application systems to selectively employ the alignments according to their needs. Both alignment techniques and confidence estimation can benefit from manual alignments. Manual alignments can be used as both supervision examples to train scoring models and as evaluation materials. The creation of such data is, however, an important question in itself, particularly at sub-sentential levels, where cross-lingual correspondences can be only implicit and difficult to capture. This thesis focuses on means to acquire useful sentential and sub-sentential bitext alignments. Chapter 1 provides a non-technical description of the research motivation, scope, organization, and introduces terminologies and notation. State-of-the-art alignment techniques are reviewed in Part I. Chapter 2 and 3 describe state-of-the-art methods for respectively sentence and word alignment. Chapter 4 summarizes existing manual alignments, and discusses issues related to the creation of gold alignment data. The remainder of this thesis, Part II, presents our contributions to bitext alignment, which are concentrated on three sub-tasks. Chapter 5 presents our contribution to gold alignment data collection. For sentence-level alignment, we collect manual annotations for an interesting text genre: literary bitexts, which are very useful for evaluating sentence aligners. We also propose a scheme for sentence alignment confidence annotation. For sub-sentential alignment, we annotate one-to-one word links with a novel 4-way labelling scheme, and design a new approach for facilitating the collection of many-to-many links. All the collected data is released on-line. Improving alignment methods remains an important research subject. We pay special attention to sentence alignment, which often lies at the beginning of the bitext alignment pipeline. Chapter 6 presents our contributions to this task. Starting by evaluating state-of-the-art aligners and analyzing their models and results, we propose two new sentence alignment methods, which achieve state-of-the-art performance on a difficult dataset. The other important subject that we study is confidence estimation. In Chapter 7, we propose confidence measures for sentential and sub-sentential alignments. Experiments show that confidence estimation of alignment links is a challenging problem, and more works on enhancing the confidence measures will be useful. Finally, note that these contributions have been employed in a real world application: the development of a bilingual reading tool aimed at facilitating the reading in a foreign language.
This dissertation is devoted to a social-media-mining problem named the activity-prediction problem. In this problem one aims to predict the number of user-generated-contents that will be created about a topic in the near future. In order to study the activity-prediction problem without referring directly to a particular social-media, a generic framework is proposed. Three defi-nitions of the activity-prediction problem are proposed. Firstly the magnitude prediction problem defines the activity-prediction as a regression problem. These three definitions of the activity prediction problem are tackled with state-of-the-art machine learning approaches applied to generic features. Indeed, these features are defined with the help of the generic framework. Therefore these features are easily adaptable to various social-media. The data was prepared so that commercial-contents and technical failure are not sources of noise. A cross-validation method that takes into account the time of observations is used. In addition an unsupervised method to extract buzz candidates is proposed. Indeed the training-sets are very ill-balanced for the buzz classification problem, and it is necessary to preselect buzz candidates. The activity-prediction problems are studied within two different experimental settings. The first experimental setting includes data from Twitter and the bulletin-board-system, on a long time-scale, and with three different languages. The second experimental setting is dedicated specifically to Twitter. This second experiment aims to increase the reproducibility of experiments as much as possible. Hence, this experimental setting includes user-generated-contents collected with respect to a list of unambiguous English terms.
This PhD thesis deals with knowledge discovery from Displacement Field Time Series (DFTS) obtained by satellite imagery. Such series now occupy a central place in the study and monitoring of natural phenomena such as earthquakes, volcanic eruptions and glacier displacements. These series are indeed rich in both spatial and temporal information and can now be produced regularly at a lower cost thanks to spatial programs such as the European Copernicus program and its famous Sentinel satellites. Our proposals are based on the extraction of grouped frequent sequential patterns. Nevertheless, they cannot use the confidence indices coming along with DFTS and the swap method used to select the most promising patterns does not take into account their spatiotemporal complementarities, each pattern being evaluated individually. Our contribution is thus double. A first proposal aims to associate a measure of reliability with each pattern by using the confidence indices. This measure allows to select patterns having occurrences in the data that are on average sufficiently reliable. We propose a corresponding constraint-based extraction algorithm. It relies on an efficient search of the most reliable occurrences by dynamic programming and on a pruning of the search space provided by a partial push strategy. A second contribution for the selection of the most promising patterns is also made. This one, based on an informational criterion, makes it possible to take into account at the same time the confidence indices and the way the patterns complement each other spatially and temporally. For this aim, the confidence indices are interpreted as probabilities, and the DFTS are seen as probabilistic databases whose distributions are only partial. The informational gain associated with a pattern is then defined according to the ability of its occurrences to complete/refine the distributions characterizing the data. On this basis, a heuristic is proposed to select informative and complementary patterns. This method provides a set of weakly redundant patterns and therefore easier to interpret than those provided by swap randomization. It has been implemented in a dedicated prototype. In addition to being constructed from different data and remote sensing techniques, these series differ drastically in terms of confidence indices, the series covering the Mont-Blanc massif being at very low levels of confidence. In both cases, the proposed methods operate under standard conditions of resource consumption (time, space), and experts' knowledge of the studied areas is confirmed and completed.
Embodied Conversational Agents are virtual characters which main purpose is to interact with a human user. They are used in various domains such as personal assistance, social training or video games for instance. The users, even if they are aware that they interact with a machine, are still capable of analyzing and identifying social behaviors through the signals produced by these virtual characters. The research in Embodied Conversational Agents has focused for a long time on the reproduction and recognition of emotions by virtual characters and now the focus is on the ability to express different social attitudes. These attitudes show a behavioral style and are expressed through different modalities of the body, like the facial expressions, the gestures or the gazes for instance. We proposed a model that allows an agent to produce different nonverbal behaviors expressing different social attitudes in a conversation. The whole set of behaviors produced by our model allows a goup of agents animated by it to simulate a conversation, without any verbal content. Two evaluations of the model were conducted, one on the Internet and one in a Virtual Reality environment, to verify that the attitudes produced are well recognized
In this thesis, we focus on how SAR images can be used to study vegetation. Vegetation lies at the core of human lives by providing both food and economic resources as well as participating in regulating climate. Traditionally, vegetation is classified into three categories: fields, flooded pastures, and forests. We follow this classification in our study. The aim of the first part is to provide a better understanding of the capabilities of Sentinel-1 radar images for agricultural land cover mapping through the use of deep learning techniques. We revealed that even with classical machine learning approaches (K nearest neighbors, random forest and support vector machines), good performance classification could be achieved with F-measure/Accuracy greater than 86% and Kappa coefficient better than 0.82. We found that the results of the two deep recurrent neural network (RNN)-based classifiers clearly outperformed the classical approaches. In the second part, the objective is to study the capabilities of multitemporal radar images for rice height and dry biomass retrievals using Sentinel-1 data. To do this, we train Sentinel-1 data against ground measurements with classical machine learning techniques (Multiple Linear Regression (MLR), Support Vector Regression (SVR) and Random Forest (RF)) to estimate rice height and dry biomass. Such results indicate that the highly qualified Sentinel-1 radar data could be well exploited for rice biomass and height retrieval and they could be used for operational tasks. Finally, reducing carbon emissions from deforestation and degradation (REDD) requires detailed insight into how the forest biomass is measured and distributed. We aim to improve on previous approaches by using radar satellite ALOS PALSAR (25-m resolution) and optical Landsat-derived tree cover (30-m resolution) observations to estimate forest biomass stocks in Madagascar, for the years 2007-2010. The radar signal and in situ biomass were highly correlated (R2 = 0.71) and the root mean square error was 30% (for biomass ranging from 0 to 500 t/ha). Combining radar signal with optical tree cover data appears to be a promising approach for using by L-band SAR to map forest biomass (and hence carbon) over broad geographical scales.
Our work is presented in three separate parts which can be read independently. Firstly we propose three active learning heuristics that scale to deep neural networks: We scale query by committee, an ensemble active learning methods. We speed up the computation time by sampling a committee of deep networks by applying dropout on the trained model. Another direction was margin-based active learning. We propose to use an adversarial perturbation to measure the distance to the margin. We also establish theoretical bounds on the convergence of our Adversarial Active Learning strategy for linear classifiers. Secondly, we focus our work on how to fasten the computation of Wasserstein distances. We propose to approximate Wasserstein distances using a Siamese architecture. From another point of view, we demonstrate the submodular properties of Wasserstein medoids and how to apply it in active learning. First, we hijack an active learning strategy to confront the relevance of the sentences selected with active learning to state-of-the-art phraseology techniques. These works help to understand the hierarchy of the linguistic knowledge acquired during the training of CNNs on NLP tasks. Secondly, we take advantage of deconvolution networks for image analysis to present a new perspective on text analysis to the linguistic community that we call Text Deconvolution Saliency.
The aim of this dissertation is to say how the linguistic system of our mind interacts with its conceptual system. I argue for the thesis that the linguistic system embodies a set of rules applying directly to semantically loaded mental representations. I first consider proposals by the formalists of the 20th century (Frege, Russell), formal semanticists (Montague, Lewis) and Grice before developing my own account. After showing that this theory could account for neuropsychological data concerning language processing, I apply it to the problem of the embedded implicatures and say how it could be used in Natural Language Processing (NLP), to clarify the Language of Thought Hypothesis and to study the semantics/pragmatics interface.
This thesis presents a novel method for ensuring cooperation between humans and robots in public spaces, under the constraint of human behavior uncertainty. The cooperation part can be solved independently from the task and executed as a finite state machine in order to contain online planning effort. The developed framework has been implemented in a real application scenario as part of the COACHES project. The thesis describes the Escort mission used as testbed application and the details of implementation on the real robots. This scenario has as well been used to carry several experiments and to evaluate our contributions.
This thesis aims to establish a discourse interpretation theory named formal hermeneutics that applies rigorous mathematical methods in studying the process of interpretation of natural language texts supposed to be written “with a good grace” as the messages intended for human understanding; we call them admissible. In the phonocentric paradigm, a natural language is described in the category of textual spaces Logos. A particular genre of texts defines there a full subcategory of formal discourse schemes. For a given admissible text X, we introduce the category Schl(X) of sheaves of fragmentary meanings, called category of Schleiermacher, in termes of which a generalized Frege's compositionality principle is formulated, and we also introduce the category Context(X) of étale bundles of contextual meanings in termes of which a generalized Frege's contextuality principle is formulated. Established by the section-functor and the germ-functor, an equivalence of categories Schl(X)?Context(X), called Frege duality, gives rise to a functional representation for fragmentary meanings that allows one to describe the process of text understanding. We consider as linguistic universals the connectidness and the Kolmogoroff's T0–separability of the phonocentric topology underlying to a text.
Lexical databases play a significant role in natural language processing (NLP), however, they require permanent development and enrichment through the exploitation of free resources from the semantic web, among others, Wikipedia, DBpedia, Geonames and Yago2. Prolexbase, which issued of numerous studies on NLP, has ten languages, three of which are well covered: French, English and Polish. It was manually designed; the first semiautomatic attempt was made by the ProlexFeeder project (Savary et al., 2013). The objective of our work was to create an automatic updating and extension tool for Prolexbase, and to introduce the Arabic language. In addition, a fully automatic system has been implemented to calculate, via Wikipedia, the notoriety of the entries of Prolexbase. This notoriety is language dependent, is the first step in the construction of an Arabic module of Prolexbase, and it takes a part in the notoriety revision currently present for the other languages in the database.
A graph is a set of nodes, together links connecting pairs of nodes. With the accumulating amount of data collected, there is a growing interest in understanding the structures and behavior of very large graphs. Nevertheless, the rapid increasing in size of large graphs makes studying the entire graphs becomes less and less efficient. Thus, there is a compelling demand for more effective methods to study large graphs without requiring the knowledge of the graphs in whole. One promising method to understand the behavior of large graphs is via exploiting specific properties of local structures, such as the size of clusters or the presence locally of some specific pattern, i.e. a given (usually small) graph. A classical example from Graph Theory (proven cases of the Erdos-Hajnal conjecture) is that if a large graph does not contain some specific pattern, then it must have a set of nodes pairwise linked or not linked of size exponentially larger than expected. This thesis will address some aspects of two fundamental questions in Graph Theory about the presence, abundantly or scarcely, of a given pattern in some large graph: - Can the large graph be partitioned into copies of the pattern? - Does the large graph contain any copy of the pattern? We will discuss some of the most well-known conjectures in Graph Theory on this topic: the Tutte's flow conjectures on flows in graphs and the Erdos-Hajnal conjecture mentioned above, and present proofs for several related conjectures -- including the Barát-Thomassen conjecture, a conjecture of Haggkvist and Krissell, a special case of Jaeger-Linial-Payan-Tarsi's conjecture, a conjecture of Berger et al, and another one by Albouker et al.
Analyse of entities representation over the Web 2.0 Every day, millions of people publish their views on Web 2.0 (social networks,blogs, etc.). These comments focus on subjects as diverse as news, politics,sports scores, consumer objects, etc. This idea carries a priori on a particular subject and is only valid in context for a given time. This perceived image is different from the entity initially wanted to broadcast (e.g. via a communication campaign). Moreover,in reality, there are several images in the end living together in parallel on the network, each specific to a community and all evolve differently over time(imagine how would be perceived in each camp together two politicians edges opposite). Finally, in addition to the controversy caused by the voluntary behavior of some entities to attract attention (think of the declarations required or shocking). It also happens that the dissemination of an image beyond the framework that governed the and sometimes turns against the entity (for example,«marriage for all» became «the demonstration for all»). The views expressed then are so many clues to understand the logic of construction and evolution of these images. The aim is to be able to know what we are talking about and how we talk with filigree opportunity to know who is speaking. In this thesis we propose to use several simple supervised statistical automatic methods to monitor entity's online reputation based on textual contents mentioning it. More precisely we look the most important contents and theirs authors (from a reputation manager point-of-view). We introduce an optimization process allowing us to enrich the data using a simulated relevance feedback(without any human involvement). We also compare content contextualization method using information retrieval and automatic summarization methods. We also propose a reflection and a new approach to model online reputation, improve and evaluate reputation monitoring methods using Partial Least Squares Path Modelling (PLS-PM).
Thiopurines are cytotoxic and immunosuppressive drugs widely prescribed, mainly in inflammatory bowel disease (IBD). Optimization of thiopurine response is challenging because of its large interindividual variability such as inefficacy and toxicities. This thesis has explored, on one hand, the relationships between TPMT activity and metabolite concentrations, and on the other hand, factors associated with thiopurine inefficacy. Furthermore, a retrospective study in pediatric IBD identified factors predicting the occurrence of lymphopenia during thiopurine therapy. Finally, using a lymphoblastoid cell line (LCL) in vitro model, we established a transcriptomic signature, including 32 genes predicting thiopurine cellular resistance. A bioinformatic functional analysis identified metabolic pathways in relation with p53 and cell cycle, as well as molecular mechanisms associated with thiopurine resistance. To conclude, this research work, focusing on the variability of thiopurine response and mainly therapeutic resistance, provides new hypotheses to individualize and optimize therapeutic response to thiopurines.
Although there are several models of visual saliency, in terms of contrast and cognition, there is no hybrid model integrating both mechanisms of attention: the visual aspect and the cognitive aspect. To carryout such a model, we have explored existing approaches in the field of visual attention, as well as several approaches and paradigms in related fields (such as object recognition, artificial learning, classification, etc.).A functional architecture of a hybrid visual attention system, combining principles and mechanisms derived from human visual attention with computational and algorithmic methods, was implemented, explained and detailed. The carried out studies and experimental validation of the proposed models confirmed the relevance of the proposed approach in increasing the autonomy of robotic systems within a real environment
Deep neural networks have achieved impressive results in many AI-related fields, ranging from computer games, to computer vision and natural language processing, to mention a few. Architecture engineering is a major part of neural network development. It allowed for the error rate on ImageNet to go down from 16.4% (AlexNet) to 3.57% (ResNet-152). Designing architectures is time consuming and difficult; automatic method have been developed to learn an architectures, and in recent years they used massive computational power for reaching state of the art performance on various problems. The aim of this PhD project, in collaboration between Paris Dauphine and Facebook AI Research Paris, includes: Contributing to the state of the art by designing and developing novel derivative-free optimization methods for architecture search. Find new architectures offering better accuracy or interesting compromises between size, computational speed and performance. Generalizing architecture to learn more efficient architectures for reinforcement learning, domain adaptation, or to handle issues arising when the distribution of the test data is different from that of the train data (covariate shift).
During the development of complex systems, several enterprises exchange a large number of heterogeneous models and requirements. During the phases of the system's life cycle, these artifacts, linked to each other and derived from different modelling tools, are constantly evolving. In such environment, it is necessary to manage the impact of the different changes occurring in the different design spaces. Traceability meets this need. However, establishing links between requirements and models in complex systems engineering requires dealing with a large volume of artifacts. For example, a specification of an autonomous vehicle with 3,000 requirements and 400 model elements, it would theoretically be necessary to check about one million of potential links. Although several approaches have been proposed for identifying traceability links, the validation process is always time-consuming and error-prone. This approach provides a quantitative confidence measure on each candidate link.
While information is abundant in the world, structured, ready-to-use information is rare. This work proposes Information Extraction (IE) as an efficient approach for producing structured,usable information on biology, by presenting a complete IE task on a model biological organism, Arabidopsis thaliana. Information Extraction is the process of extracting meaningful parts of text and identifying their semantic relations. In collaboration with experts on the plant A. Thaliana, a knowledge model was conceived. The goal of this model is providing a formal representation of the knowledge that is necessary to sufficiently describe the domain of grain development. This model contains all the entities and the relations between them which are essential and it can directly be used by algorithms. In parallel, this model was tested and applied on a set of scientific articles of the domain. These documents constitute the corpus which is needed to train machine learning algorithms. The experts annotated the text using the entities and relations of the model. This corpus and this model are the first available for grain development and among very few on A. Thaliana, despite the latter's importance in biology. This model manages to answer both needs of being complex enough to describe the domain well, and of having enough generalization for machine learning. A relation extraction approach (AlvisRE) was also elaborated and developed. After entity recognition, the relation extractor tries to detect the cases where the text mentions that two entities are in a relation, and identify precisely to which type of the model these relations belong to. AlvisRE's approach is based on textual similarity and it uses all types of information available:lexical, syntactic and semantic. In the tests conducted, AlvisRE had results that are equivalent or sometimes better than the state of the art. Additionally, AlvisRE has the advantage of being modular and adaptive by using semantic information that was produced automatically. This last feature allows me to expect similar performance in other domains.
Inflexion rules cannot envisage all possible models of the Macedonian conjugaison and their approach is too synthetic to be fully operational from a didactic point of view. For all these reasons, the purpose of this doctoral thesis is to study a large number of conjugated verbs in order to map stable patterns opening up new forays into the teaching of the Macedonian verbal system.
Supervisory Control Theory (SCT) is one of the most important formal paradigms for developing controllers for Discrete Event Systems (DESs). The large number of scientific contributions shows that SCT catches extensive academic interest and this theory has been proved to be applicable in various industrial domains such as manufacturing systems, embedded systems, transportation systems and energy systems. With SCT, the requirements which are checked afterward in conventional engineering are used as input for generation of the design of the controller the verification of this model can be eliminated. However the SCT suffers from an important lack of integration in a global design process which leads to the gaps between the theoretical development and applications of SCT within engineering practice. The Model-Based System Engineering (MBSE) provides the solutions to deal with the limitations of SCT. The objective of this study is to propose a novel framework for automatic control (AC) which integrates both SCT and MBSE to bridge the gaps formal paradigm and engineering process. In the proposed framework, different SysML diagrams are used to as complementary models which present the indispensible views of the system to be studied in the global modeling process. Secondly, in order to keep the consistency between SysML models and formal models, methods for formal modeling and verification are proposed. A case study introduced at the end prove that the proposed framework which provides a global development process from requirement analysis to controller implementation can well meet the needs of engineering practice.
Text is one of the most pervasive and persistent sources of information. Content analysis of text in its broad sense refers to methods for studying and retrieving information from documents. Nowadays, with the ever increasing amounts of text becoming available online is several languages and different styles, content analysis of text is of tremendous importance as it enables a variety of applications. To this end, unsupervised representation learning methods such as topic models and word embeddings constitute prominent tools. The goal of this dissertation is to study and address challenging problems in this area, focusing on both the design of novel text mining algorithms and tools, as well as on studying how these tools can be applied to text collections written in a single or several languages. In the first part of the thesis we focus on topic models and more precisely on how to incorporate prior information of text structure to such models. Topic models are built on the premise of bag-of-words, and therefore words are exchangeable. While this assumption benefits the calculations of the conditional probabilities it results in loss of information. To overcome this limitation we propose two mechanisms that extend topic models by integrating knowledge of text structure to them. We assume that the documents are partitioned in thematically coherent text segments. The first mechanism assigns the same topic to the words of a segment. The second, capitalizes on the properties of copulas, a tool mainly used in the fields of economics and risk management that is used to model the joint probability density distributions of random variables while having access only to their marginals. Typically, a document collection for such models is in the form of comparable document pairs. The documents of a pair are written in different languages and are thematically similar. Unless translations, the documents of a pair are similar to some extent only. Meanwhile, representative topic models assume that the documents have identical topic distributions, which is a strong and limiting assumption. To overcome it we propose novel bilingual topic models that incorporate the notion of cross-lingual similarity of the documents that constitute the pairs in their generative and inference processes. The last part of the thesis concerns the use of word embeddings and neural networks for three text mining applications. First, we discuss polylingual document classification where we argue that translations of a document can be used to enrich its representation. Using an auto-encoder to obtain these robust document representations we demonstrate improvements in the task of multi-class document classification. Second, we explore multi-task sentiment classification of tweets arguing that by jointly training classification systems using correlated tasks can improve the obtained performance. To this end we show how can achieve state-of-the-art performance on a sentiment classification task using recurrent neural networks. The third application we explore is cross-lingual information retrieval. Given a document written in one language, the task consists in retrieving the most similar documents from a pool of documents written in another language. In this line of research, we show that by adapting the transportation problem for the task of estimating document distances one can achieve important improvements.
In this thesis, we consider the problem of situation awareness, and more specifically, in crisis situation. We propose an automated situation awareness system (ASAAP system) which tries to mitigate these problems. We present a dynamic environment modelling allowing an analysis of most valuable variables for situation assessment in order to reduce the field of possibilities and maximise the information gain. In the crisis situation field, we also propose to apply our system on a more specific domain, the threat assessment. This contribution allows to define and analyse the threat level of each exposed zones with the capacity of understanding the enemy's strategy by both representing its targets and the path to reach them. Finally, we present a preliminary approach about the optimisation of the coverage of most valuable variables by the sensors to maximise the information gain. We generated scenarios inspired from real situations to evaluate ours approaches in maritime and military field. The ASAAP system results show an improvement in situation awareness in its complexity and by its capacity to describes the enemy's strategy in reasonable time.
This thesis focuses on learning methods for automatic transcription of the battery. They are based on a transcription algorithm using a non-negative decomposition method, NMD. This thesis raises two main issues: the adaptation of methods to the analyzed signal and the use of deep learning. Taking into account the information of the signal analyzed in the model can be achieved by their introduction during the decomposition steps. A first approach is to reformulate the decomposition step in a probabilistic context to facilitate the introduction of a posteriori information with methods such as SI-PLCA and statistical NMD. A second approach is to implement an adaptation strategy directly in the NMD: the application of modelable filters to the patterns to model the recording conditions or the adaptation of the learned patterns directly to the signal by applying strong constraints to preserve their physical meaning. The second approach concerns the selection of the signal segments to be analyzed. The results obtained being very interesting, the detector is trained to detect only one instrument allowing the transcription of the three main drum instruments with three CNNs. Finally, the use of a CNN multi-output is studied to transcribe the part of battery with a single network.
Since then, their propularity constantly raised in communication systems. Being images representing either an idea, a concept, or an emotion, emojis are available to the users in multiple software contexts: instant messaging, emails, forums, and other types of social medias. Their usage grew constantly and, associated to the constant addition of new emojis, there are now more than 2,789 standard emojis since winter 2018. To access a specific emoji, scrolling through huge emoji librairies or using a emoji search engines is not enough to maximize their usage and their diversity. An emoji recommendation system is required. To answer this need, we present our research work facused on the emoji recommendation topic. The objectives are to create an emoji recommender system adapted to a private and informal conversationnal context. This system must enhance the user experience, the communication quality, and take into account possible new emerging emojis. Our first contribution is to show the limits of a emoji prediction for the real usage case, and to demonstrate the need of a more global recommandation. We also veifie the correlation between the real usage of emojis representing facial expressions and a related theory on facial expressions. We also tackle the evaluation part of this system, with the metrics'limits and the importance of a dedicated user interface.
Alzheimer's disease (AD) is the first cause of dementia worldwide, affecting over 20 million people. Its diagnosis at an early stage is essential to ensure a proper care of patients, and to develop and test novel treatments. AD is a complex disease that has to be characterized by the use of different measurements: cognitive and clinical tests, neuroimaging including magnetic resonance imaging (MRI) and positron emission tomography (PET), genotyping, etc. There is an interest in exploring the discriminative and predictive capabilities of these diverse markers, which reflect different aspects of the disease and potentially carry complementary information, from an early stage of the disease. The objective of this PhD thesis was thus to assess the potential and to integrate multiple modalities using machine learning methods, in order to automatically classify patients with AD and predict the development of the disease from the earliest stages. More specifically, we aimed to make progress toward the translation of such approaches toward clinical practice. The thesis comprises three main studies. The first one tackles the differential diagnosis between different forms of dementia from MRI data. This study was performed using clinical routine data, thereby providing a more realistic evaluation scenario. The second one proposes a new framework for reproducible evaluation of AD classification algorithms from MRI and PET data. Indeed, while numerous approaches have been proposed for AD classification in the literature, they are difficult to compare and to reproduce. The third part is devoted to the prediction of progression to AD in patients with mild cognitive impairment through the integration of multimodal data, including MRI, PET, clinical/cognitive evaluations and genotyping. In particular, we systematically assessed the added value of neuroimaging over clinical/cognitive data only. Since neuroimaging is more expensive and less widely available, this is important to justify its use as input of classification algorithms.
This thesis outlines a semantic approach to the mining and analysis of ideological discourse from electronic texts. This approach integrates a qualitative social scientific method of textual analysis (Critical Discourse Analysis) with a quantitative ontology-based reasoning and information retrieval method using semi-automatic natural language processing techniques. It is applied to the analysis of Marxist discourse, as represented in the Marxists Internet Archive (http://www.marxists.org) thematic collection, containing nearly 15,000 texts. The application focuses on the acquisition of emerging schemas, which can contribute to the classification of unknown texts by ideological perspective.
However, the physical laboratories hosting these activities rely on expensive infrastructures that make very difficult for institutions to cope with the high increase of the students'population. Within this context, virtual and remote laboratories (VRL) bring an affordable alternative to provide practical activities at scale. Numerous research works have come up for the last decade; they mainly focused on technological issues such as the federation of remote laboratories, their standardization, or the pooling of the resources they provide. Nevertheless, the recent literature reviews highlight the need to pay more attention to the educational facets of these innovative learning environments. With that purpose in mind, our works make use of the learners'traces collected through their practical learning sessions to sustain socio-constructivist theories, on which practical activities rely on, and thus to engage students in their learning tasks and further their reflection. Starting from the study of scientific research, we identify as a first step a set of criteria required to design practical learning systems that support social interactions between learners. Indeed, Lab4CE builds on learning analytics to enable different forms of learning such as collaboration, cooperation, or peer assistance, but also to supply learners as well as teachers awareness and reflection tools that aim at promoting deep learning during and after practical activities. Moreover, theses experimentations suggested a significant correlation between, on the one hand, student's activity in the environment and the learning strategies they apply and, on the other hand, their academic performance. These first results allow us to assess that socio-constructivist theories leverage engagement and reflection within VRL. They also invite us to put our approach into practice in other learning settings, but also to extend the sources of information to deal with our behavioral analyses in depth, and thus to enhance our contributions regarding the adoption of practical learning within technological environments.
The most recent linguistic studies show that proper names are often translated or adapted, in contrast to the traditional theories of untranslatability of proper names. What distinguishes toponyms from the rest of onomastics are political, sociological and historical implications that affect many geographical names. Using the name "Breslau" for the Polish city "Wrocław" may convey negative connotations depending on the context. Nevertheless, this form is often found on the Internet. Moreover, due to globalization we are witnessing the multiplication of many versions of the same toponym. This work is composed of three parts. The first part presents theories of proper names,but adapted to toponyms, and the second describes their functions and their linguistic status according to the international standardization. The concept of synchronic-contrastive toponymy and methods of analysing toponyms according to this approach are introduced as well in the second part. The third part is an analysis of the corpus with the purpose of observing the structure and the integration of Polish toponyms into the French language, as well as their popular use in current publications (Internet, tourist brochures, in applications and social networks, etc.) different from the official use, which is supposed to be politically correct.
With the advent of the "big data", many repercussions have taken place in all fields of information technology, advocating innovative solutions with the best compromise between cost and accuracy. In this thesis we discuss two main problems: first, the problem of partitioning graphs is approached from a perspective big data, where massive graphs are partitioned in streaming. We study and propose several models of streaming partitioning and we evaluate their performances both theoretically and empirically. In a second step, we are interested in querying distributed / partitioned graphs.
This thesis studies the production of nabol, the narrative practices of the Nisvai language community, located in southeast Malekula, Vanuatu. Based on a request from Nisvai speakers for language resources to be produced for the local school, a corpus of oral texts was compiled to show that nabol are produced according to the situation of enunciation and local social issues related to the speaker's age set. The corpus of annotated oral texts results from research trips carried out between 2011 and2015, totalling 14 months in the field with the Nisvai community. On one hand, nabol are studied using concepts from textual linguistics to describe the discursive processes used by Nisvai speakers. From these processes, it was possible to compare the organisation of the nabol and to highlight significant variations according to the situation of enunciation. On the other hand, participant observation and guided interviews helped identify social issues that speakers associated with their narrative practices. The use of proper nouns of characters or places in the narrative is part of a regime of truth. Depending on his or her age set, the speaker must name or omit the names of the characters who take part in the plot at the risk of being criticised by their peers. Practices and standards from the language documentation program and Natural Language Processing have provided tools to develop language resources relevant for the study of narratives and their use by the Nisvai community. Two paper resources are joined as appendices: a bilingual Nisvai-French lexicon and a bilingual collection of texts from the corpus. They are designed for Nisvai speakers and their French-speaking school. In addition, two online resources, a reading-listening interface and an annotation consultation interface, have been developed to communicate the work to researchers working on oral narrative practices or Vanuatu languages.
Additionally, a custom, hardened data glove was built and used to demonstrate successful gesture recognition and real-time robot control. We finally show this work's flexibility by furthermore using it beyond robot control to drive other kinds of controllable systems.
Internet of Things (IoT) is leading to a paradigm shift within the logistics industry. The advent of IoT has been changing the logistics service management ecosystem. Logistics services providers today use sensor technologies such as GPS or telemetry to collect data in realtime while the delivery is in progress. The realtime collection of data enables the service providers to track and manage their shipment process efficiently. The key advantage of realtime data collection is that it enables logistics service providers to act proactively to prevent outcomes such as delivery delay caused by unexpected/unknown events. Data from such external sources enrich the dataset and add value in analysis. Besides, collecting them in real-time provides an opportunity to use the data for on-the-fly analysis and prevent unexpected outcomes (e.g., such as delivery delay) at run-time. However, data are collected raw which needs to be processed for effective analysis. Collecting and processing data in real-time is an enormous challenge. The main reason is that data are stemming from heterogeneous sources with a huge speed. The high-speed and data variety fosters challenges to perform complex processing operations such as cleansing, filtering, handling incorrect data, etc. The variety of data – structured, semi-structured, and unstructured – promotes challenges in processing data both in batch-style and real-time. Different types of data may require performing operations in different techniques. A technical framework that enables the processing of heterogeneous data is heavily challenging and not currently available. Therefore, in order to exploit Big Data in logistics service processes, an efficient solution for collecting and processing data in both realtime and batch style is critically important. In this thesis, we developed and experimented with two data processing solutions: SANA and IBRIDIA. SANA is built on Multinomial Naïve Bayes classifier whereas IBRIDIA relies on Johnson's hierarchical clustering (HCL) algorithm which is hybrid technology that enables data collection and processing in batch style and realtime. SANA is a service-based solution which deals with unstructured data. It serves as a multi-purpose system to extract the relevant events including the context of the event (such as place, location, time, etc.). In addition, it can be used to perform text analysis over the targeted events. IBRIDIA was designed to process unknown data stemming from external sources and cluster them on-the-fly in order to gain knowledge/understanding of data which assists in extracting events that may lead to delivery delay. According to our experiments, both of these approaches show a unique ability to process logistics data.
Scientists have developed the Virtual Observatory (VO) concept in order to make the most of the large masses of heterogeneous data produced by the modern scientific instruments of astrophysics. It is a service-oriented architecture, aiming to facilitate the identification and interoperability of astrophysical data. Despite the development and advances made by VO in the exploitation of these data, some objectives are partially such as interoperability, service selection and identification of related services, etc. In addition, the ergonomics of the tools available to the end user can be improved. Similarly, the current use of VO resources, based on human skills, would benefit from being automated. As not all the astrophysical data services are included in the VO, it would also be desirable to allow a wider use of these tools, as they also rely on services available outside the VO. In order to automate the use of online resources, information sciences have been working since 2001 on the development of the Semantic Web. This evolution provides the Web with automatic reasoning abilities, based on algorithms using a new form of content description. This new form of semantic description is expressed in computer representations called ontologies. Unfortunately, the current semantic Web development methods are not fully compatible with VO services that use data models, formats and protocols for accessing services that differ from those typically encountered in information sciences. In this context, this thesis describes a generic methodology for the composition of stateless services, based on the description of services by a global ontology, the definition of which is proposed in this document. This ontology represents both Web services and services that are not accessible via the Web. It takes into account certain specificities that may be encountered in preexisting service infrastructures. The population of this ontology, by services possibly distant from the standards usually used in the information sciences, is also treated. The methodology was applied successfully in the framework of astrophysics, and allowed to develop a Web application allowing the automatic composition of services usable by an uninformed public.
Computer-assisted translation and terminology management tools are often used to meet the needs in management of multilingual and monolingual writings. These tools facilitate the access to technical terms and expressions that are related to areas of specialty, and essential to any communication process. The understanding of technical terms can be potentiated by their “contextualization”. However, having access to a term or its translation is not enough, since it is also necessary to be able to use it properly and to understand its exact meaning. Thus, this contextualization is estabilished on two levels: in texts and in the terminology. In texts, the user must have access to information regarding the use of terms, namely linguistic knowledge-rich contexts. In the terminology, the user requires access to semantic or conceptual relationships between the terms to better understand its meaning, namely conceptual rich-knowlegde contexts. We continued our work in a bilingual phase part of specialized translation, under continuous revision. We propose a new generation of bilingual concordancers that take as input a term and its translation, and provides not parallel, but aligned Knowledge-Rich Contexts from specialized comparable corpora. The evaluation show that our concordancer can assist revisers despite the difficulty of the task.
Online Social Networks have taken a huge place in the informational space and are often used for advertising, e-reputation, propaganda, or even manipulation, either by individuals, companies or states. The amount of information makes difficult the human exploitation, while the need for social network analysis remains unsatisfied: trends must be extracted from the posted messages, the user behaviours must be characterised, and the social structure must be identified. To tackle this problem, we propose a system providing analysis tools on three levels. First, the message analysis aims to determine the opinions they bear. The first is constituted of messages published on Twitter, gathering every activity performed by a set of 5,000 accounts on a long period. The second stems from a ToR-based social network, named Galaxy2, and includes every public action performed on the platform during its uptime. We evaluate the relevance of our system on these two datasets, showing the complementarity of user account characterisation tools (influence, behaviour and role), and user account communities (interaction strength, thematic cohesion), enriching the social graph exploitation with textual content elements.
The methodology is based on three pillars: definition, search / analysis and innovation. A comprehensive definition of the main function of the industrial system delimits the research field and allows the retrieval of initial keywords through a detailed analysis of what is currently available. The iterative patent search is based on functional decomposition and physical analysis. The analysis phase uses energy functional decomposition to identify energies, transmitted functional flows and physical phenomena involved in the energy conversion process in order to select potentially relevant physical effects. To delineate the exploration field we formulate search queries from a keywords database composed by initial, physical, and technological keywords. A discovery matrix based on the intersections between these keywords allows the classification of pertinent patents. The research for innovation opportunities exploits the discovery matrix in order to decipher the evolutionary trends followed by inventions. Opportunities are deduced from an analysis of the discovery matrix empty cells, an analysis of the evolution trends, and from changing the concept by energy converter substitution. We propose evolution trends constructed from the evolution laws of TRIZ theory, design heuristics, and rules of the art of the engineering. An application case concerning the study of the evolution and the proposal of innovative biphasic separation systems in deep offshore highlights the method.
Our thesis unifes paradigms of universal darwinism, developmental psycholinguistics and computational linguistics in order to furnish a novel account of language development in human as well as artificial language-acquiring agents. Thesis is preceded by a supplementary volume called "Conceptual Foundations" which presents a so-called "Theory of Intramental Evolution" which postulates that ontogeny of an individual mind can be interpreted and even simulated as a process involving replication, variation and selection of information-encoding cognitive structures. The dissertation itself presents four distinct simulations addressing four distinct problems. Zeroth simulation illustrates how evolutionary optimization could lead to discovery of useful insights concerning the cryptological riddle known as Voynich Manuscript. The first simulation shows how theory of prototypes, vector symbolic architectures and evolutionary optimization can be mutually combined in order to yield a novel supervised machine-learning method. Second simulation uses a similiar approach in order to indicate that evolutionary optimization can discover minimalist and lightweight constellations of part-of-speech taggers. The last simulation targets the "holy grail" of computational linguistics, i.e. the problem of "grammar induction" and shows that the problem can be potentially solved by using an evolutionary strategy able to bridge the gap between subsymbolic realm of vector spaces and symbolic realm of grammar-representing regular expressions.
Internet as well as all the modern media of communication, information and entertainment entails a massive increase of digital data quantities. Automatically processing and understanding these massive data enables creating large knowledge bases, more efficient search, social medial research, etc. Natural language processing research concerns the design and development of algorithms that allow computers to process natural language in texts, audios, images or videos automatically for specific tasks. Due to the complexity of human language, natural language processing of text can be divided into four levels: morphology, syntax, semantics and pragmatics. Current natural language processing technologies have achieved great successes in the tasks of the first two levels, leading to successes in many commercial applications such as search. However, advanced structured search engine would require computers to understand language deeper than at the morphology and syntactic levels. Information extraction is designed to extract meaningful structural information from unannotated or semi-annotated resources to enable advanced search and automatically create knowledge bases for further use. This thesis studies the problem of information extraction in the specific domain of biomedical event extraction. We propose an efficient solution, which is a trade-off between the two main trends of methods proposed in previous work. This solution reaches a good balance point between performance and speed, which is suitable to process large scale data. It achieves competitive performance to the best models with a much lower computational complexity. While designing this model, we also studied the effects of different classifiers that are usually proposed to solve the multi-class classification problem. We also tested two simple methods to integrate word vector representations learned by deep learning method into our model. Even if different classifiers and the integration of word vectors do not greatly improve the performance, we believe that these research directions carry some promising potential for improving information extraction.
Following these constraints and using adapted automatic art production systems, notably based on artificial ant colony algorithms, we developed two computer programs. The first one is a virtual music instrument, allowing most people to play music and providing and automatic accompaniment. The second one is a drawing workshop with generative methods-based tools provide complex results from simple actions. This PhD thesis details the development of this two programs and their evaluations, with real users meetings.
This thesis studies the link between institutions, gender and politics. Three questions are studied: can institutions undo gender norms? In particular, we study the consequences of institutions on the perpetuation of gender norms. We study the norm according to which a woman should earn less than her husband. Using the German division as a natural experiment, we show that East German institutions have undone gender. East German women can earn more than their husband without increasing their number of housework hours, put their marriage at risk, or withdraw from the labor market. By contrast, the norm of higher male income and its consequences are still prevalent in the West. The second chapter studies whether institutions would be more gender-egalitarian if more women were heading them. In particular, I test whether female politicians have the same priorities than their male counterparts. The context studied is the French Parliament from 2001 to 2017. Using text analysis and quasi-experimental variations to randomize legislators'gender, this chapter shows that women are twice more likely to initiate women-related amendments in the Lower House. Women's issues constitute the key topic on which women are more active, followed by health and childhood issues whereas men are more active on military issues. I provide supporting evidence that these results are driven by the individual interest of legislators. Finally, I replicate these results in the Upper House by exploiting the introduction of a gender quota. The third chapter studies the reasons behind the underrepresentation of women in positions of power. I investigate whether the persistence of incumbents hinders female access to political positions when incumbents are predominantly men. Despite a context increasingly favorable to the election of women, I find that the persistence of incumbents does not block female access to the position of mayor. I investigate the mechanisms and show that it is more difficult for a woman to replace a female incumbent than a male one.
Digital system are now part of our society. They are used in a wide range of domains and in particular they have to handle delicate tasks. Already used in domains such as transportation, surgery or economy, we speak now of using digital systems for social or political matters: electronic vote, selection algorithms, electoral profiling dots. For task handled by algorithm, the responsibility is moved from the executioner to the designer, developer and tester of those algorithms. It is also the responsibility of computer scientists who study those algorithms to propose reliable techniques of verification which will be applicable in the design, the development or the testing phase. Formal verification methods provide mathematical tools to prevent executions error in all phases. Among them, fault-diagnosis consist on the construction of a diagnoser based on a formal model of the system we aim to check. The diagnoser runs in parallel with the real system and emit a warning anytime it detect a dangerous behavior. For systems modeled by timed automata, it is not always possible to construct a timed automaton to diagnose it. Indeed timed automata,introduce in the nineties by cite{AD94} and widely studied and used since to model timed systems, are not determinizable. A machine, more powerful than a timed automaton,can still be used to construct the diagnoser of a timed automaton as it is done in cite{Tripakis02}. This thesis work aim at constructing a diagnoser for any one-clock timed automata. This diagnoser is constructed with the help of a machine more powerful than timed automata, following the idea of cite{Tripakis02}. Part~I of this thesis introduce a formal framework for the modeling of quantitative systems and the study of their determinization. In this framework we introduce automata on timed structures,the model used to construct the diagnoser. Part~II study the determinization problem of automata on timed structures, and particularly the one of timed automata determinization in this framework. Part~III illustrate how automata on timed structures can be used to construct in a generic way a diagnoser for one clock timed automata. This technique is implemented in a tool, DOTA, and is compared to the technique used in cite{Tripakis02}.
Large dimensional data and learning systems are ubiquitous in modern machine learning. As opposed to small dimensional learning, large dimensional machine learning algorithms are prone to various counterintuitive phenomena and behave strikingly differently from the low dimensional intuitions upon which they are built. Nonetheless, by assuming the data dimension and their number to be both large and comparable, random matrix theory (RMT) provides a systematic approach to assess the (statistical) behavior of these large learning systems, when applied on large dimensional data. The major objective of this thesis is to propose a full-fledged RMT-based framework for various machine learning systems: to assess their performance, to properly understand and to carefully refine them, so as to better handle large dimensional problems that are increasingly needed in artificial intelligence applications. Precisely, we exploit the close connection between kernel matrices, random feature maps, and single-hidden-layer random neural networks. Under a simple Gaussian mixture modeling for the input data, we provide a precise characterization of the performance of these large dimensional learning systems as a function of the data statistics, the dimensionality, and most importantly the hyperparameters (e.g., the choice of the kernel function or activation function) of the problem. Further addressing more involved learning algorithms, we extend the present RMT analysis framework to access large learning systems that are implicitly defined by convex optimization problems (e.g., logistic regression), when optimal points are assumed reachable. To find these optimal points, optimization methods such as gradient descent are regularly used. Aiming to have a better theoretical grasp of the inner mechanism of optimization methods and their impact on the resulting learning model, we further evaluate the gradient descent dynamics in training convex and non-convex objects. These preliminary studies provide a first quantitative understanding of the aforementioned learning algorithms when large dimensional data are processed, which further helps propose better design criteria for large learning systems that result in remarkable gains in performance when applied on real-world datasets.
Emotions and their expressions by virtual characters are two important issues for future affective human-machine interfaces. Recent advances in psychology of emotions as well as recent progress in computer graphics allow us to animate virtual characters that are capable of expressing emotions in a realistic way through various modalities. Existing virtual agent systems are often limited in terms of underlying emotional models, visual realism, and real-time interaction capabilities. In our research, we focus on virtual agents capable of expressing emotions through facial expressions while interacting with the user. Our work raises several issues: How can we design computational models of emotions inspired by the different approaches to emotion in Psychology? What is the level of visual realism required for the agent to express emotions? How can we enable real-time interaction with a virtual agent? How can we evaluate the impact on the user of the emotions expressed by the virtual agent? Our work focuses on computational modeling of emotions inspired by psychological theories of emotion and emotional facial expressions by a realistic virtual character. Facial expressions are known to be a privileged emotional communication modality. Our main goal is to contribute to the improvement of the interaction between a user and an expressive virtual agent. For this purpose, our research highlights the pros and cons of different approaches to emotions and different computer graphics techniques. We worked in two complementary directions. First, we explored different approaches to emotions (categorical, dimensional, cognitive, and social). For each of these approaches, a computational model has been designed together with a method for real-time facial animation. Our second line of research focuses on the contribution of visual realism and the level of graphic detail of the expressiveness of the agent. This axis is complementary to the first one, because a greater level of visual detail could contribute to a better expression of the complexity of the underlying computational model of emotion. Our work along these two lines was evaluated by several user-based perceptual studies. The combination of these two lines of research is seldom in existing expressive virtual agents systems. Our work opens future directions for improving human-computer interaction based on expressive and interactive virtual agents. MARC has been used in various kinds of applications: games, ubiquitous intelligence, virtual reality, therapeutic applications, performance art, etc.
Thousands of neuroimaging studies are published every year. Exploiting this huge amount of results is difficult. Indeed, individual studies lack statistical power and report many spurious findings. Even genuine effects are often specific to particular experimental settings and difficult to reproduce. Meta-analysis aggregates studies to identify consistent trends in reported associations between brain structure and behavior. The standard approach to meta-analysis starts by gathering a sample of studies that investigate a same mental process or disease. Then, a statistical test delineates brain regions where there is a significant agreement among reported findings. In this thesis, we develop a different kind of metaanalysis that focuses on prediction rather than hypothesis testing. We build predictive models that map textual descriptions of experiments, mental processes or diseases to anatomical regions in the brain. Our supervised learning approach comes with a natural quantitative evaluation framework, and we conduct extensive experiments to validate and compare statistical models. We collect and share the largest existing dataset of neuroimaging studies and stereotactic coordinates. This dataset contains the full text and locations of neurological observations for over 13 000 publications. Standard meta-analysis is an essential tool to distinguish true discoveries from noise and artifacts. This thesis introduces methods for predictive metaanalysis, which complement the standard approach and help interpret neuroimaging results and formulate hypotheses or formal statistical priors.
The ever-expanding volume of available audio and multimedia data has elevated technologies related to content indexing and structuring to the forefront of research. Speaker diarization involves the detection of speaker turns within an audio document (segmentation) and the grouping together of all same-speaker segments (clustering). Indeed we first introduce a new purification component leading to competitive performance to the bottom-up approach. Moreover, while investigating the two diarization approaches more thoroughly we show that they behave differently in discriminating between individual speakers and in normalizing unwanted acoustic variation, i.e.\ that which does not pertain to different speakers. This difference of behaviours leads to a new top-down/bottom-up system combination outperforming the respective baseline system. Finally, we introduce a new technology able to limit the influence of linguistic effects, responsible for biasing the convergence of the diarization system.
With the development of e-commerce,consumers have posted large number of online reviews on the internet. These user-generated data are valuable for product designers, as information concerning user requirements and preference can be identified. The objective of this study is to develop an approach to guide product design by analyzing automatically online reviews. The proposed approach consists of two steps: data structuration and data analytics. In data structuration, the author firstly proposes an ontological model to organize the words and expressions concerning user requirements in review text. Then, a rule-based natural language processing method is proposed to automatically structure review text into the propose ontology. In data analytics, two methods are proposed based on the structured review data to provide designers ideas on innovation and to draw insights on the changes of user preference over time. In these two methods, traditional affordance-based design, conjoint analysis, the Kano model are studied and innovatively applied in the context of big data. To evaluate the practicability of the proposed approach, the online reviews of Kindle e-readers are downloaded and analyzed, based on which the innovation path and the strategies for product improvement are identified and constructed.
The primary goal of the systems engineering is the creation of a set of high quality products and services that enable the accomplishment of desired tasks and needs of the clients or user groups. A typical systems engineering project can be divided in to three phases: definition, development, and deployment. The definition phase involves the activities of requirement elicitation and refinement. By the end of system definition phase, we have all the system functional and non functional requirements. One of the results of development phase is initial working model of the system. The deployment phase consists of activities of operational implementation, operational testing and evaluation, and operational functioning and maintenance. In a project life cycle there are numerous issues to be sorted out during the various phases to finally deliver a successful product. We proposed solution to the problems of requirements engineering &amp; management, design conflict detection,and stakeholders conflict resolution. This thesis is based on the recent advances in industrial practices and research in the field of system design engineering. The objective of this thesis work is to propose an innovative and holistic conception methodology taking into account the multidisciplinary environment and multiple stakeholders. We have proposed a requirements modeling language based on the GORE techniques. We have proposed a few of tools for reducing the ambiguity of requirements such as: using negation and test cases using negation for contracting difficult requirements. Requirement management techniques are proposed to provide better requirements traceability and aid for other systems engineering activities. Using the same criteria weighting technique a flexible multi criteria multi participant decision methodology is proposed for various decision problems arising during the life cycle of systems engineering project. Finally, a comprehensive prescriptive systems engineering approach is proposed using all the previously made contributions and an illustrative case study of a real ongoing project is presented developed using the supporting tool SysEngLab, which implements majority of the methods and techniques proposed during thesis.
In the framework of speech therapy for articulatory troubles associated with tongue misplacement, providing a visual feedback might be very useful for both the therapist and the patient, as the tongue is not a naturally visible articulator. In the last years, ultrasound imaging has been successfully applied to speech therapy in English speaking countries, as reported in several case studies. The assumption that visual articulatory biofeedback may facilitate the rehabilitation of the patient is supported by studies on the links between speech production and perception. During speech therapy sessions, the patient seems to better understand his/her tongue movements, despite the poor quality of the image due to inherent noise and the lack of information about other speech articulators. We develop in this thesis the concept of augmented lingual ultrasound. We propose two approaches to improve the raw ultrasound image, and describe a first clinical application of this device. The first approach focuses on tongue tracking in ultrasound images. We propose a method based on supervised machine learning, where we model the relationship between the intensity of all the pixels of the image and the contour coordinates. The size of the images and of the contours is reduced using a principal component analysis, and a neural network models their relationship. We developed speaker-dependent and speaker-independent implementations and evaluated the performances as a function of the amount of manually annotated contours used as training data. We obtained an error of 1.29 mm for the speaker-dependent model with only 80 annotated images, which is better than the performance of the EdgeTrak reference method based on active contours. First, we build a mapping model between ultrasound images and tongue control parameters acquired on the reference speaker. We then adapt this model to new speakers referred to as source speakers. This approach is compared to a direct GMR regression between the source speaker data and the control parameters of the talking head. We show that C-GMR approach achieves the best compromise between amount of adaptation data and prediction quality. We also evaluate the generalization capability of the C-GMR approach and show that prior information of the reference speaker helps the model generalize to articulatory configurations of the source speaker unseen during the adaptation phase. Finally, we present preliminary results of a clinical application of augmented ultrasound imaging to a population of patients after partial glossectomy. The first results show an improvement of the patients' performance, especially for tongue placement.
With the rapid proliferation of data platforms collecting and curating data related to various domains such as governments data, education data, environment data or product ratings, more and more data are available online. This offers an unparalleled opportunity to study the behavior of individuals and the interactions between them. In the political sphere, being able to query datasets of voting records provides interesting insights for data journalists and political analysts. In particular, such data can be leveraged for the investigation of exceptionally consensual/controversial topics. Consider data describing the voting behavior in the European Parliament (EP). This dataset offers opportunities to study the agreement or disagreement of coherent subgroups, especially to highlight unexpected behavior. It is to be expected that on the majority of voting sessions, MEPs will vote along the lines of their European party alliance. However, when matters are of interest to a specific nation within Europe, alignments may change and agreements can be formed or dissolved. For instance, when a legislative procedure on fishing rights is put before the MEPs, the island nation of the UK can be expected to agree on a specific course of action regardless of their party alliance, fostering an exceptional agreement where strong polarization exists otherwise. In this thesis, we aim to discover such exceptional (dis)agreement patterns not only in voting data but also in more generic data, called behavioral data, which involves individuals performing observable actions on entities. These two approaches called Debunk and Deviant, ideally, enables the implementation of a sufficiently comprehensive tool to highlight, summarize and analyze exceptional comportments in behavioral data. We thoroughly investigate the qualitative and quantitative performances of the devised methods. Furthermore, we motivate their usage in the context of computational journalism.
The dematerialization of health data, which started several years ago, now generates na huge amount of data produced by all actors of health. These data have the characteristics of being very heterogeneous and of being produced at different scales and in different domains. Their reuse in the context of clinical research, public health or patient care involves developing appropriate approaches based on methods from data science. The aim of this thesis is to evaluate, through three use cases, what are the current issues as well as the place of data sciences regarding the reuse of massive health data. To meet this objective, the first section exposes the characteristics of health big data and the technical aspects related to their reuse. The second section presents the organizational aspects for the exploitation and sharing of health big data. The third section describes the main methodological approaches in data sciences currently applied in the field of health. Finally, the fourth section illustrates, through three use cases, the contribution of these methods in the following fields: syndromic surveillance, pharmacovigilance and clinical research. Finally, we discuss the limits and challenges of data science in the context of health big data.
The semantic Web proposes standards and tools to formalize and share knowledge on the Web, in the form of ontologies. A first contribution of this thesis describes a method based on pattern structures, an extension of formal concept analysis, to extract associations between adverse drug events from patient data. In this context, a phenotype ontology and a drug ontology cooperate to allow a semantic comparison of these complex adverse events, and leading to the discovery of associations between such events at varying degrees of generalization, for instance, at the drug or drug class level. A second contribution uses a numeric method based on semantic similarity measures to classify different types of genetic intellectual disabilities, characterized by both their phenotypes and the functions of their linked genes. We study two different similarity measures, applied with different combinations of phenotypic and gene function ontologies. In particular, we investigate the influence of each domain of knowledge represented in each ontology on the classification process, and how they can cooperate to improve that process. Finally, a third contribution uses the data component of the semantic Web, the Linked Open Data (LOD), together with linked ontologies, to characterize genes responsible for intellectual deficiencies. These contributions illustrates the possibility of having several ontologies cooperate to improve various data mining processes
This dissertation deals with the co-construction of meaning in interaction and the ways in which conversationalists exhibit their interpretative processes. The focus of this study is the process of explicitation, i.e. the process through which an informational content becomes explicit in conversation. By offering a multi-level analysis of conversational sequences engaged in this practice, the study approaches the co-construction of meaning from the point of view of informational transformation and inference. The analyses presented here have been conducted on a corpus of spoken French in interaction, within the setting of informal encounters between friends around a meal or a drink. The practice of making a content explicit is here being explored according to three analytical lines: (a) the sequential analysis, focusing on the deployment of the explicitation sequence and its components; (b) the analysis according to a device elaborated by means of modeling information management in these sequences; and (c) the analysis of the linguistic designs used when exhibiting the inference. One of themain challenges of the present study is that of a proposition of a conversationalist model, dealing with information management and its enforcement through analysis of talk in interaction.
Our research is related both to Second Language Acquisition Research and to Foreign Language Didactics, and has a twofold objective. First, we described crosslinguistic influences, from Spanish L1 and English L2, in the acquisition processes of some structures related to the verb construction in French L3, i.e. the verb construction with noun phrases, the selection of prepositions that potentially introduce these complements, and the acquisition of complement pronouns. To carry out this identification of syntactic transfers, we consider theoretical contributions from both multilingual acquisition research and studies that define developmental stages in French acquisition. Our study is based on written productions taken from first-year students, beginners in French, of the Translation/Interpretation university program at Universidad de Concepción in Chile. The second objective of our research consists of a didactic transposition of empirical results in order to better support the learning of French in our training context. Our teaching proposals deal with the selection and sequencing of grammatical contents, the methodological approach of grammar instruction, and the correction/evaluation of learners' grammatical competence. In the end, our didactic purpose is to contribute to multilingual learners training and self-conscience of their acquisition processes, to support them with the reflexive use of their linguistic knowledge, in particular their grammatical knowledge.
Artificial intelligence (AI) with recent progress in statistical machine learning (ML) is currently aiming to revolutionise how experimental science is conducted. In physics, chemistry, biology, neuroscience or medicine, data is now the driver of new theoretical insights and new scientific hypotheses. Supervised learning and predictive models are now used to assess if something is *predictable*: Can I predict what people *think* from neural signals? ML is now used as a replacement for classical statistical hypothesis testing. In healthcare, one talks about precision medicine, virtual patients with the vision that artificial intelligence will allow to have individualised predictions from genomic, physiological or imaging data. After pioneering breakthroughs in computer vision, speech processing or natural language processing, ML has now to face new challenges in order to impact various scientific disciplines and in particular health related applications. When considering medical applications, statistical and computational problems emerge. The particular problem investigated in this project is related to the absence or limited amount supervision for algorithms: supervised predictive models need so-called annotations or labels to be trained and tested, and unfortunately too few medical applications can provide enough of these. The approach considered will be based on self-supervised learning and non-linear ICA.
The increase of textual sources over the Web offers an opportunity for knowledge extraction and knowledge base creation. Recently, several research works on this topic have appeared or intensified. They generally highlight that to extract relevant and precise information from text, it is necessary to define a collaboration between linguistic approaches, e.g., to extract certain concepts regarding named entities, temporal and spatial aspects, and methods originating from the field of semantics'processing. Moreover, successful approaches also need to qualify and quantify the uncertainty present in the text. Finally, in order to be relevant in the context of the Web, the linguistic processing need to be consider several sources in different languages. This PhD thesis tackles this problematic in its entirety since our contributions cover the extraction, representation of uncertain knowledge as well as the visualization of generated graphs and their querying.
Bioinformatic analyses of transcriptomic data aims to identify genes with variations in their expression level in different tissue samples, for example tissues from healthy versus seek patients, and to characterize these genes on the basis of their functional annotation. In this thesis, I present four contributions for taking into account domain knowledge in these methods. Firstly, I define a new semantic and functional similarity measure which optimally exploits functional annotations from Gene Ontology (GO). Then, I show, thanks to a rigorous evaluation method, that this measure is efficient for the functional classification of genes. In the third contribution, I propose a differential approach with fuzzy assignment for building differential expression profiles (DEPs). I define an algorithm for analyzing overlaps between functional clusters and reference sets such as DEPs here, in order to point out genes that have both similar functional annotation and similar variations in expression. This method is applied to experimental data produced from samples of healthy tissue, colorectal tumor and cancerous cultured cell line. Finally the similarity measure IntelliGO is generalized to another structured vocabulary organized as GO as a rooted directed acyclic graph, with an application concerning the semantic reduction of attributes before mining.
Based on a case study built around the analysis of the speeches produced by the various actors in the institutional fields concerned (the field of tourist accommodation for Airbnb, that of passenger transportation for Uber), I seek to highlight the strategies of the platforms to build their place in the social space. I use different analytical grids to understand the strategies implemented by Airbnb and Uber: the dynamics of platform business ecosystems developed by strategic management research, the megamarketing grid defined by Kotler in 1986, neo-institutional theory and its latest developments concerning institutional work and the question of legitimacy. I find that Airbnb and Uber have each mobilized their megamarketing skills in their own way to build their business ecosystem and legitimacy system, which is a real support for their institutional conquest. These different expressions of their strategies are also embodied in the institutional work process, which is oriented towards negotiation for Airbnb and confrontation for Uber. The results of the institutional process have similarities between the two cases: constitution of the legitimacy systems necessary to interpret the role of these two platforms, legal recognition of the activities permitted by the platforms and their producer sides, adjustment of the offers of established professionals. This work provides a glimpse of a platform life cycle model, taking into account the dynamics of these organizational forms and those resulting from institutional work and their quest for legitimacy.
This project aims to develop expert tools able to automatically detect or suggest (portions of) texts suited to children's understanding abilities. These tools will rely on the analysis of time and emotions linguistic expression in texts for children, in relation to findings of psycholinguistic works on developmental stages of these two dimensions'understanding.
However, this signal requires extensive fitting and noise reduction steps to extract useful information. The complexity of these analysis pipelines yields results that are highly dependent on the chosen parameters. The computation cost of this data deluge is worse than linear: as datasets no longer fit in cache, standard computational architectures cannot be efficiently used. To speed-up the computation time, we considered dimensionality reduction by feature grouping. We use clustering methods to perform this task. We introduce a linear-time agglomerative clustering scheme, Recursive Nearest Agglomeration (ReNA). Unlike existing fast agglomerative schemes, it avoids the creation of giant clusters. We then show empirically how this clustering algorithm yields very fast and accurate models, enabling to process large datasets on budget. In neuroimaging, machine learning can be used to understand the cognitive organization of the brain. The idea is to build predictive models that are used to identify the brain regions involved in the cognitive processing of an external stimulus. However, training such estimators is a high-dimensional problem, and one needs to impose some prior to find a suitable model. To handle large datasets and increase stability of results, we propose to use ensembles of models in combination with clustering. We study the empirical performance of this pipeline on a large number of brain imaging datasets. Finally, we show that ensembles of models improve the stability of the weight maps and reduce the variance of prediction accuracy.
We propose two novel approaches for recommender systems and networks. In the first part, we first give an overview of recommender systems and concentrate on the low-rank approaches for matrix completion. Building on a probabilistic approach, we propose novel penalty functions on the singular values of the low-rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefficients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low-rank matrix completion. In the second part, we first introduce some background on Bayesian nonparametrics and in particular on completely random measures (CRMs) and their multivariate extension, the compound CRMs. We then propose a novel statistical model for sparse networks with overlapping community structure. The model is based on representing the graph as an exchangeable point process, and naturally generalizes existing probabilistic models with overlapping block-structure to the sparse regime. Our construction builds on vectors of CRMs, and has interpretable parameters, each node being assigned a vector representing its level of affiliation to some latent communities. We develop methods for simulating this class of random graphs, as well as to perform posterior inference. We show that the proposed approach can recover interpretable structure from two real-world networks and can handle graphs with thousands of nodes and tens of thousands of edges.
Medical imaging is a principal data source for different applications. Even though medical images represent a lot of knowledge concerning the studied case, all the a priori knowledge known by the specialist remains implicit. Nevertheless this a priori knowledge has a major role in the interpretation and the use of the images. In this thesis, anatomical a priori knowledge is integrated in two medical applications. First, an automatic processing pipeline is proposed in order to detect, quantify and localize aneurysms on a segmented cerebrovascular tree. Centerlines of blood vessels are extracted and then used to automatically detect aneurysms and quantify them. To localize aneurysm, a matching is made between the cerebrovascular tree of the patient and a healthy one. The a priori knowledge, in this case, is represented by a graph. A new algorithm using this ontology to accomplish the segmentation task is then proposed.
Potentially avoidable hospitalizations (PAHs) are the hospital admissions that could have been prevented with timely and effective treatments. The high rates of PAHs are associated with many factors. These preventable hospitalizations are associated with a cost of several hundred million Euros for the Health Insurance. In other words, reducing PAHs not only enhances patients' quality of life but also could save substantial costs due to patient treatments. Therefore, health authorities are highly interested in solutions improving health care services to reduce PAHs. Some recent studies in France have suggested that increasing the number of nurses in selected geographic areas could lead to the reduction of the rates of PAHs in those areas. In our approach, after evaluating some common regression methods, we extended the support vector machine for regression to spatial information. This approach allows us to select not only the geographic areas but also the number of to-be-added nurses in these areas for the biggest reduction in the number of PAHs. Specifically, our approach is applied in the Occitanie region, France and geographic areas mentioned above are the cross-border living areas (fr. Bassins de vie-BVs). On the other side, the extreme temperature could be one potential factor associated with high rates of PAHs. Therefore, a part of our works is to measure the impact of the extreme temperature to PAHs as well as to include this environmental data in our approach above. In our works, we used the temperature values measured hourly by sensors at the weather stations. However, these values are sometimes discontinuous and we need an imputation method for these missing values. In the literature, two most popular approaches dealing with this processing step exploit either the spatial component or temporal component of the temperature data. Respectively, these approaches are spatial interpolation methods such as Inverse Distance Weighted (IDW) and time-series models such as Autoregressive Integrated Moving Average (ARIMA). The results show that compared with IDW and ARIMA methods, our approach performs better at 100% and 99.8% (604 over 605) weather stations respectively. In addition, as mentioned at the beginning, improving the coordination between the health care providers could lead to the reduction of the PAHs. Moreover, in the cases that the patients change hospitals for treatments, to ensure efficient and high-quality treatments, doctors would need access to the patients' medical records at the previous hospitals. Therefore, we propose a graph-based approach to address this problem. Particularly, we first model data flows of patients between hospitals as an undirected weighted graph in which nodes and edges present the hospitals and the amount of patient flows respectively. Then, after evaluating two common graph clustering methods, we customize the more suitable one for our needs. Our result provides interesting insights compared with approaches based on administrative boundaries
This study focuses on language acquisition of a bilingual child growing up in a French-Russian speaking family. Recent research has shown that a range of factors such as parental input frequency, family discourse strategies can explain the language development processes which take place withing a bilingual family (Döpke, 1998; Lanza, 1997, 2004; De Houwer, 2009; King et Fogle, 2013, etc.) The bilingual child was being recorded during spontaneous and natural interaction with both of her parents for the period of two years (from 2;00 to 4;00). The overall corpora are composed of 68 recording hours, while the analysed sample is based on 28 hours of transcribed data. The data gathered in this study strongly suggest the existence of clear correlation between input frequency, parental discourse strategies and child's linguistic competence in both languages. The findings from the research show a shift from dominant bilingualism to the harmonious use of both languages at the age of 3. This shift is accompanied by the changes of the child's linguistic soundscape, the use of parental discourse strategies and input frequency in Russian. The grammatical-categories emergence is characterized by a strong discrepancy in both languages: the acquisition of French follows developmental paths of French monolingual children, while Russian is acquired with a substantial time delay. The crosslinguistic influences, lexical, morphological and syntactic, support the idea of a common underlying proficiency.
The main topics of this thesis involve the development of stochastic algorithms for optimization under uncertainty, the study of their theoretical properties and applications. We study their convergence using the tools developed in the theory of Markov processes: we use properties of infinitesimal generators and functional inequalities to measure the distance between their probability law and a target one. The first part is concerned with quantum graphs endowed with a probability measure on their vertex set. Quantum graphs are continuous versions of undirected weighted graphs. The starting point of the present work was the question of finding Fréchet means on such a graph. The Fréchet mean is an extension of the Euclidean mean to general metric spaces and is defined as an element that minimizes the sum of weighted square distances to all vertices. Our method relies on a Langevin formulation of a noisy simulated annealing dealt with using homogenization. In order to establish the convergence in probability of the process, we study the evolution of the relative entropy of its law with respect to a convenient Gibbs measure. Using functional inequalities (Poincare and Sobolev) and Gronwall's Lemma, we then show that the relative entropy goes to zero. We test our method on some real data sets and propose an heuristic method to adapt the algorithm to huge graphs, using a preliminary clustering. In the same framework, we introduce a definition of principal component analysis for quantum graphs. This implies, once more, a stochastic optimization problem, this time on the space of the graph's geodesics. We suggest an algorithm for finding the first principal component and conjecture the convergence of the associated Markov process to the wanted set. On the second part, we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem on a finite space. We prove the algorithm's convergence in probability towards the optimal set, provide convergence rate and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1. This work is completed with a set of numerical experiments and the assessment of the practical performance both on benchmark test cases and on real world examples.
Aligning macromolecules such as proteins, DNAs and RNAs in order to reveal, or conversely exploit, their functional homology is a classic challenge in bioinformatics, with far-reaching applications in structure modelling and genome annotation. In the specific context of complex RNAs, featuring pseudoknots, multiple interactions and non-canonical base pairs, multiple algorithmic solutions and tools have been proposed for the structure sequence alignment problem. However, such tools are seldom used in practice, due in part to their extreme computational demands, and because of their inability to support general types of structures. Recently, Rinaudo et al. gave a fully general parameterised algorithm for structure-sequence comparison, which is able to take as input any type of pseudoknotted structures. The parameterised algorithm is a tree decomposition based dynamic programming. To accelerate the dynamic programming algorithm without losing two much accuracy, we introduced a banded dynamic programming. Then three algorithms are introduced to get the suboptimal structure-sequence alignments. The algorithms are implemented in a software named LiCoRNA (aLignment of Complex RNAs). We first evaluate the performance of LiCoRNA on the seed alignment in the pseudoknotted RFAM families. Compared to the state-of-the-art algorithms, LiCoRNA shows generally equivalent or better results than its competitors. With the high accuracy showed by LiCoRNA, we further curate RFAM full pseudoknotted alignment.
The biological brain is an ensemble of individual components which have evolved over millions of years. Neurons and other cells interact in a complex network from which intelligence emerges. Many of the neural designs found in the biological brain have been used in computational models to power artificial intelligence, with modern deep neural networks spurring a revolution in computer vision, machine translation, natural language processing, and many more domains. However, artificial neural networks are based on only a small subset of biological functionality of the brain, and often focus on global, homogeneous changes to a system that is complex and locally heterogeneous. In this work, we examine the biological brain, from single neurons to networks capable of learning. We examine individually the neural cell, the formation of connections between cells, and how a network learns over time. For each component, we use artificial evolution to find the principles of neural design that are optimized for artificial neural networks. We then propose a functional model of the brain which can be used to further study select components of the brain, with all functions designed for automatic optimization such as evolution. Our goal, ultimately, is to improve the performance of artificial neural networks through inspiration from modern neuroscience. However, through evaluating the biological brain in the context of an artificial agent, we hope to also provide models of the brain which can serve biologists.
In the past years, robots have been a part of our every day lives. And, with the growth of autonomous cars, we see them driving autonomously on highways and cities. Another area of growth is social robotics. We can see a lot of studies such as robots helping children with autism. Other robots are being used to receive people in hotels or to interact with people in shopping centers. In the latter examples, robots need to understand people behavior. In addition, in the case of mobile robots, they need to know how to navigate in human environments. In the context of human environments, this thesis explores socially acceptable navigation of robots towards people. The human is an entity that needs to be considered based on social norms that we (humans) use on a daily basis. In a first time, we explore how a robot can approach one person. A person is an entity that can be bothered if someone or something approaches invading her personal space. The person also will feel distressed when she is approached from behind. These social norms have to be respected by the robot. For this reason, we decided to model the behavior of the robot through learning algorithms. We manually approach a robot to a person several times and the robot learns how to reproduce this behavior. In a second time, we present how a robot can understand what is a group of people. We, humans, have the ability to do this intuitively. However, for a robot, a mathematical model is essential. Lastly, we address how a robot can approach a group of people. We use exemplary demonstrations to teach this behavior to the robot. We evaluate then the robot's movements by for example, observing if the robot invades people's personal space during the trajectory.
This dissertation presents a novel approach to automatic text summarization, one of the most challenging tasks in Natural Language Processing (NLP). Until now, no one had ever created a summarization method capable of producing summaries comparable in quality with those produced by humans. Even many of state-of-the-art approaches form the summary by selecting a subset of sentences from the original text. Since some of the selected sentences might still contain superfluous information, a finer analysis is needed. We propose an Automatic Sentence Compression method based on the elimination of intra-phrase discourse segments. After applying both algorithms in documents in Spanish, our method is able to produce high quality results. Finally, we evaluate the produced summaries using the Turing test to determine if human judges can distinguish between human-produced summaries and machine-produced summaries.
Opinion mining has emerged as a hot topic in the machine learning community due to the recent availability of large amounts of opinionated data expressing customer's attitude towards merchandisable goods. Yet, predicting opinions is not easy due to the lack of computational models able to capture the complexity of the underlying objects at hand. Current approaches consist in predicting simple representations of the affective expressions, for example by restricting themselves to the valence attribute. We propose a deep learning based approach able to take advantage of the different labeled parts of the output objects by learning to jointly predict them. We propose a novel hierarchical architecture composed of different state-of-the-art multimodal neural layers and study the effect of different learning strategies in this joint prediction context. The resulting model is shown to improve over the performance of separate opinion component predictors and raises new questions concerning the optimal treatment of hierarchical labels in a structured prediction context. We specifically analyzed the case of preference based learning and joint entity and valence detection under a 2 layer binary tree representation in order to derive excess risk bounds and an analysis of the learning procedure algorithmic complexity. A second aspect of this thesis is to handle a newly released multimodal dataset containing entity and valence annotations at different granularity levels providing a complex representation of the underlying expressed opinions. Hence, we propose a deep learning based approach able to take advantage of the different labeled parts of the output objects by learning to jointly predict them. We propose a novel hierarchical architecture composed of different state-of-the-art multimodal neural layers and study the effect of different learning strategies in this joint prediction context. The resulting model is shown to improve over the performance of separate opinion component predictors and raises new questions concerning the optimal treatment of hierarchical labels in a structured prediction context.
The Spanish verb dar is a lexical item particularly rich in nuances of meaning. Some of its senses have been studied under the «semantic bleaching» hypothesis consequent upon the «grammaticalization» of the verb. This process would reveal mainly in the discourse marker dale and in the use of dar as a «support verb» in nominal predicates like dar un golpe, dar un paseo, dar una mano de pintura, etc. According to the theoretical framework for this research, which accounts for the distinction between language and discourse as well as the unicity of the linguistic sign, an existential notion underlies every discourse realization of this verb. The notion conveyed by the signified of dar is that an entity B gains access to existence in the sphere of an entity C as a result of the action of an entity A. Hence, this existential meaning appears in every use of a lexeme whose semantic content is never affected.
Quality of service is a huge issue for telecommunications operators since they have to master and evaluate it in order to satisfy their customers. To replace expensive and time-consuming human judgment methods, objective methods, integrating objective models providing a prediction of the perceived quality, have been conceived. Our research aimed at developing a technical diagnostic method, complementary to objective voice quality models, which provides specific information about the nature of the perceived voice quality impairments and identifies the underlying technical causes. Assuming that speech quality is a multidimensional phenomenon, our technical diagnostic method is built on the modelling of the four perceptual dimensions identified in the literature: “Noisiness” relative to the perceived background noise, “Continuity” linked to discontinuity, “Coloration” related to frequency–response degradations and “Loudness” corresponding to the impact of the speech level, each one being quantified by quality degradation indicators based on audio signal analysis. A crucial step of our research was to find and/or to develop relevant quality degradation indicators to perfectly characterize each dimension. To do so, we identified quality degradation indicators in the most recent objective voice quality models (particularly the ITU-T P.863 recommendation, known as POLQA) and we analysed the performance of identified indicators. Finally, for each dimension, we proposed a detection block which automatically classifies a perceived degradation according to the nature of the defect detected in the audio signal, and an additional block providing information about the impact of degradations on speech quality. The proposed technical diagnostic method is designed to cover three bandwidths (Narrowband, Wideband and Super Wideband) used in telecommunications systems with a priority investigation to Super Wideband speech signals which remain very useful for future telephony applications.
Modern nuclear reactors utilize core calculations that implement a thermo-hydraulic feedback requiring accurate homogenized few-group cross sections. They describe the interactions of neutrons with matter, and are endowed with the properties of smoothness and regularity, steaming from their underling physical phenomena. This thesis is devoted to the modeling of these functions by industry state-of-theart and innovative machine learning techniques. Convenient is intended in terms of computational performance, such as the model's size, evaluation speed, accuracy, robustness to numerical noise, complexity,etc; always with respect to the engineering modeling objectives that specify the multidimensional spaces of interest. In this thesis, a standard UO₂ PWR fuel assembly is analyzed for three state-variables, burnup,fuel temperature, and boron concentration. A full grid is used as usually donein the industry. Kernel methods, that are a very general machine learning framework able to pose in a normed vector space, a large variety of regression or classification problems. Kernel functions can reproduce different function spaces using an unstructured support,which is optimized with pool active learning techniques. The approximations are found through a convex optimization process simplified by the kernel trick. The intrinsic modular character of the method facilitates segregating the modeling phases: function space selection, application of numerical routines and support optimization through active learning. Artificial neural networks which are“model free” universal approximators able Artificial neural networks which are“model free” universal approximators able to approach continuous functions to an arbitrary degree without formulating explicit relations among the variables. With adequate training settings, intrinsically parallelizable multi-output networks minimize storage requirements offering the highest evaluation speed. These strategies are compared to each other and to multi-linear interpolation in a Cartesian grid, the industry standard in core calculations. The data set, the developed tools, and scripts are freely available under aMIT license.
Since 2004, online social medias have grown hugely. This fast development had interesting effects to increase the connection and information exchange between users, but some negative effects also appeared, including fake accounts number growing day after day. Sockpuppets are multiple fake accounts created by a same user. They are the source of several types of manipulation such as those created to praise, defend or support a person or an organization, or to manipulate public opinion. Experiments have been performed on the adaptation stage using real data crawled from English Wikipedia. In order to find the best machine learning algorithm for sockpuppet's detection phase, the results of six machine learning algorithms are compared. In addition, they are compared with the literature, and the results show that our proposition improves the accuracy of the detection of sockpuppets. Furthermore, the results of five community detection algorithms are compared for sockpuppet's grouping phase, in order to find the best community detecton algorithm that will be used in real-time stage.
Affective communication plays a major role in our interpersonal interactions. We communicate emotions through multiple non-verbal channels. Researches on human-computer interaction have exploited these communication channels in order to design systems that automatically recognize and display emotional signals. Touch has receivers less interest then other non-verbal modalities in this area of research. The intrusive aspect of current haptic interfaces is one of the main obstacles to their use in mediated emotional communication. In fact, the user is must physically connected to mechanical systems to receive the stimulation. This configuration affects the transparency of the mediated interaction and limits the perception of certain emotional dimensions as the Valence. On the basis of the state of the art of haptic interfaces, we proposed a strategy of tactile stimulation based on the use of a mobile air jet. This technique provides a non-intrusive tactile stimulation on different areas of the body. In addition, this tactile device would allow effective stimulation of some mechanoreceptors that play an important role in perceptions of positive affect. We conducted an experimental study to understand the relationships between the physical characteristics of tactile stimulation by air jet and the emotional perception of the users. The results highlight the main effects of the intensity and the velocity of movement of the air stream on the subjective evaluation measured in space affective (namely, Valence, Arousal and Dominance).The communication of emotions is clearly multi-modal. We use touch jointly with other modalities to communicate different emotional messages. We conducted two experimental studies to examine the combination of air jet tactile stimulation with facial and vocal expressions for perception of the valence. These experiments were conducted in a theoretical and experimental framework called integration of information theory. This framework allows modelling the integration of information from multiple sources using a cognitive algebra. Our work suggests that tactile stimulation by air jet can be used to transmit emotional signals in the context of the human-machine interactions. Perceptual bimodal integration models can be exploited to build computational models to display affects by combining tactile stimulation to facial expressions or the voice.
Safety arguments, also called Safety Cases, are commonly used to present that adequate efforts have been made to achieve the safety goals. Thus, the system safety is often justified through assessing the safety arguments. The assessment of such arguments is usually implemented by experts without any dedicated tool or method. This leads to a questionable validity of the results. In this thesis, a quantitative framework is proposed based on Dempster-Shafer theory (D-S theory) to assess our confidence in Safety Cases. An application in railway domain realises the parameter estimation of the framework by a survey with safety experts.
This dissertation proposes a method and a System for the identification of entities (persons, locations, organizations) mentioned in the textual production of the news agency Agence France Presse, in the prospect of the automatic content enrichment. The various fields concerned by this task are viewed through their relationship: Semantic Web, Information Extraction and in particular Named Entity Recognition (NER), Semantic Annotation, Entity Linking. Following this study, the industrial need expressed by the Agence France Presse is the subject of specifications, useful for the development of a solution relying on Natural Language Processing tools. The approach adopted for the identification of the target entities is then described: we propose a System taking charge of the NER step using any existing module, whose results, possibly combined with those of other modules, are evaluated by a linking module able to (i) align a given mention with the entity it denotes among an inventory, built prior to the task, (ii) to spot denotations without alignment in the inventory and (iii) to reconsider denotational readings of mentions (false positive detection). The Nomos System is developed to this end for the processing of French data. Its conception also gives rise to the building and use of resources integrated into the Linked Data network, as well as a rich knowledge base about the target entities.
In the context of Industry 4.0 and the More than Moore's paradigm, delivery precision and short cycle times are essential to the competitiveness of High Mix Low Volume semiconductor manufacturing and future industries in general. We first conducted, in the first part of the manuscript, an in-depth study of “variability”: we approached the notion through its consequences in manufacturing systems, clarified that the variability was about the workflow, introducing the notion of workflow variability and measures that come with it, and identified the main sources of variability through a literature review and real-world examples. We focused in the second part of this manuscript on the integration of workflow variability in production management tools: We showed how integrating the stable consequences of workflow variability can improve WIP projections in complex systems and increase the control on such systems, proposed a new tool (the Concurrent WIP) to better measure the performances of systems subject to high workflow variability, and showed that complex “dependency” mechanisms play a key role in workflow variability yet are not integrated in any model. Finally, the third and last part of the manuscript organized perspectives for variability reduction: based on the work of this manuscript, we showed a framework for variability reduction on the short term, and proposed a direction for medium and long-term research.
The PhD student will take part in the ANR JCJC MAOI (Multimodal Analysis of Opinions in Interactions) at Telecom-ParisTech. The role of the PhD will consist in developing machine learning methods for the multimodal (i.e. speech and text) analysis of the user's opinion during his/her interaction with an agent.
In the first algorithm, we exploit the joint autoregressive model that models short and long (periodic) correlations of Gaussian speech signals to formulate a state space model with unknown parameters. The EM-Kalman algorithm is then used to estimate jointly the sources (involved in the state vector) and the parameters of the model. In the second algorithm, we use the same speech model but this time in the frequency domain (quasi-periodic Gaussian sources with AR spectral envelope). Classical frequency domain asymptotic methods replace linear convolution by circulant convolution leading to approximation errors. We show how the introduction of windows can lead to slightly more complex frequency domain techniques, replacing diagonal covariance matrices by banded covariance matrices, but with controlled approximation error. The sources are then estimated using the Wiener filtering. Alternating MAP/ML which suffers from inconsistent parameter bias, EM which converges to ML and VB that we prove converges asymptotically to the ML solution for parameter estimation.
Language diversity is under considerable pressure: half of the world's languages could disappear by the end of this century. This realization has sparked many initiatives in documentary linguistics in the past two decades, and 2019 has been proclaimed the International Year of Indigenous Languages by the United Nations, to raise public awareness of the issue and foster initiatives for language documentation and preservation. Yet documentation and preservation are time-consuming processes, and the supply of field linguists is limited. Consequently, the emerging field of computational language documentation (CLD) seeks to assist linguists in providing them with automatic processing tools. The Breaking the Unwritten Language Barrier (BULB) project, for instance, constitutes one of the efforts defining this new field, bringing together linguists and computer scientists. This thesis examines the particular problem of discovering words in an unsegmented stream of characters, or phonemes, transcribed from speech in a very-low-resource setting. This primarily involves a segmentation procedure, which can also be paired with an alignment procedure when a translation is available. Using two realistic Bantu corpora for language documentation, one in Mboshi (Republic of the Congo) and the other in Myene (Gabon), we benchmark various monolingual and bilingual unsupervised word discovery methods. We then show that using expert knowledge in the Adaptor Grammar framework can vastly improve segmentation results, and we indicate ways to use this framework as a decision tool for the linguist. We also propose a tonal variant for a strong nonparametric Bayesian segmentation algorithm, making use of a modified backoff scheme designed to capture tonal structure. To leverage the weak supervision given by a translation, we finally propose and extend an attention-based neural segmentation method, improving significantly the segmentation performance of an existing bilingual method.
We develop a data mining and visualisation toolkit to study how the information is shared on online social network services. This software allows to observe relationships between conversational, semantical, temporal and geographical dimensions of online communication acts. Internet memes are short messages that spread quickly through the Web. Following models that remain largely unknown, they articulate personal discussions, societal debates and large communication campaign. We analyse a set of Internet memes by using methods from social network analysis and Chinese natural language processing on a large corpus of 200 million tweets which represents/reflects the overall activity on the Chinese social network Sina Weibo in 2012. An interactive visualisation interface showing networks of words, user exchanges and their projections on geographical maps provides a detailed understanding of actual and textual aspects of each meme spread. An analysis of hashtags in the corpus shows that the main content from Sina Weibo is largely similar to the ones in traditional media (advertisement, entertainment, etc.). Therefore, we decided to not consider hashtags as memes representatives, being mostly byproducts of well planned strategic or marketing campaigns. Our final approach studies a dozen of memes selected for the diversity of their topic: humor, political scandal, breaking news and marketing.
Most of current recommendation systems are based on ratings (i.e. numbers between 0 and 5) and try to suggest a content (movie, restaurant...) to a user. These systems usually allow users to provide a text review for this content in addition to ratings. It is hard to extract useful information from raw text while a rating does not contain much information on the content and the user. In this thesis, we tackle the problem of suggesting personalized readable text to users to help them make a quick decision about a content. More specifically, we first build a topic model that predicts personalized movie description from text reviews. We evaluate our model on an IMDB dataset and illustrate its performance through comparison of topics. We then study parameter inference in large-scale latent variable models, that include most topic models. We propose a unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods. Finally, we propose a new class of determinantal point processes (DPPs) which can be manipulated for inference and parameter learning in potentially sublinear time in the number of items. This class, based on a specific low-rank factorization of the marginal kernel, is particularly suited to a subclass of continuous DPPs and DPPs defined on exponentially many items. We apply this new class to modelling text documents as sampling a DPP of sentences, and propose a conditional maximum likelihood formulation to model topic proportions, which is made possible with no approximation for our class of DPPs. We present an application to document summarization with a DPP on 2 to the power 500 items, where the summaries are composed of readable sentences.
Billions of “things” connected to the Internet constitute the symbiotic networks of communication devices (e.g., phones, tablets, and laptops), smart appliances (e.g., fridge, coffee maker and so forth) and networks of people (e.g., social networks). So, the concept of traditional networks (e.g., computer networks) is expanding and in future will go beyond it, including more entities and information. These networks and devices are constantly sensing, monitoring and generating a vast amount of data on all aspects of human life. One of the main challenges in this area is that the network consists of “things” which are heterogeneous in many ways, the other is that their state of the interconnected objects is changing over time, and there are so many entities in the network which is crucial to identify their interdependency in order to better monitor and predict the network behavior. In this research, we address these problems by combining the theory and algorithms of event processing with machine learning domains. Our goal is to propose a possible solution to better use the information generated by these networks. It will help to create systems that detect and respond promptly to situations occurring in urban life so that smart decision can be made for citizens, organizations, companies and city administrations. Social media is treated as a source of information about situations and facts related to the users and their social environment. At first, we tackle the problem of identifying the public opinion for a given period (year, month) to get a better understanding of city dynamics. The second challenge is combing network data with various properties and characteristics in common format that will facilitate data sharing among services. To solve it we created common event model that reduces the representation complexity while keeping the maximum amount of information. This model has two major additions: semantic and scalability. The semantic part means that our model is underlined with an upper-level ontology that adds interoperability capabilities. While the scalability part means that the structure of the proposed model is flexible in adding new entries and features. We validated this model by using complex event patterns and predictive analytics techniques. To deal with the dynamic environment and unexpected changes we created dynamic, resilient network model. It always chooses the optimal model for analytics and automatically adapts to the changes by selecting the next best model. We used qualitative and quantitative approach for scalable event stream selection, that narrows down the solution for link analysis, optimal and alternative best model.
Based on electronic death certificates from 2012 to 2016 in France, this thesis aims to implement and evaluate the performance of natural language processing methods, to classify medical causes of death in free text format into mortality syndromic groups (MSG) relevant for reactive mortality surveillance and health impact assessment. Close to 100 MSGs meeting the objectives of the surveillance were defined. Two classification methods were developed: a rule-based method and a supervised machine learning method (SVM). Two SVM models were developed using different combinations of features. The development and evaluation of the performances of the methods were based on 4,500 annotated death certificates. The evaluation was initially based on 7 MSGs and was then extended to 60 other MSGs. The variations of the monthly number of MSGs assigned by the two methods were compared using the whole death certificates from 2012 to 2016 (204,000 deaths). The rule-based method and the SVM model including all the features obtained high performances (F-mesure≥0.95) for the classification of causes into 31 MSGs. The monthly variations of those MSGs were comparable. In average, the causes of death within a certificate are classified into 3.7 MSG. We proposed a balanced weighting method of the MSG to take these multiple causes into account in the routine analysis of mortality for alert and impact assessment. These results complete and enrich the reactive surveillance currently based on administrative data.
Lexical knowledge acquisition of verbal constructions is an important issue for natural language processing as well as lexicography, which aims at referencing emerging linguistic usages. Such a task implies numerous challenges, technical as well as theoretical. In this thesis, we had a closer look at two fundamental aspects of the description of the verb: the notion of lexical item and the distinction between arguments and adjuncts. Following up on studies in natural language processing and linguistics, we embrace the hypothesis that there is no clear distinction between homonyms and quasi-synonyms, and the hypothesis of a continuum between arguments and adjuncts. We provide a complete approach to lexical knowledge acquisition of verbal constructions from an untagged news corpus. The acquisition process makes use of the notion of argumenthood, and builds models of the two continuums. Our lexicon has been evaluated on a qualitative and comparative basis. Siding with lexicography anchored in the theoretical framework of corpus linguistics, we show the difficulty of using lexical resources to describe as yet unseen data.
Despite its recent successes in computer vision or machine translation, the use of deep learning in the biomedical field faces many challenges. Among them, we have the difficult access to data in sufficient quantity and quality, as well as the need of having interoperable and the interpretable models. In this thesis, we are interested in these different issues from the perspective of the creation of models predicting future glucose values of diabetic patients. Such models would allow patients to anticipate daily glucose variations, helping its regulation in order to avoid states of hypoglycemia or hyperglycemia. To this end, we use three datasets. While the first was collected during this thesis on several type-2 diabetic patients, the other two are composed of type-1 diabetic patients, both real and virtual. Across the studies, we use each patient's past glucose, insulin, and carbohydrate data to build personalized models that predict the patient's glucose values 30 minutes into the future. First, we do a detailed state-of-the-art analysis by building an open-source benchmark of glucosepredictive models. While promising, we highlight the difficulty deep models have in making predictions that are at the same time accurate and safe for the patient. In order to improve the clinical acceptability of the models, we investigate the integration of clinical constraints within the training of the models. We explore its practical use through an algorithm that enables the training of a model maximizing the precision of the predictions while respecting the clinical constraints set beforehand. Then, we study the use of transfer learning to improve the performance of glucose-predictive models. It eases the learning of personalized models by reusing the knowledge learned on other patients. In particular, we propose the adversarial multi-source transfer learning framework. It significantly improves the performance of the models by allowing the learning of a priori knowledge which is more general, by being agnostic of the patients that are the source of the transfer. We investigate different transfer scenarios through the use of our three datasets. We show that it is possible to transfer knowledge using data coming from different experimental devices, from patients of different types of diabetes, but also from virtual patients. Finally, we are interested in improving the interpretability of deep models through the attention mechanism. In particular, we explore the use of a deep and interpretable model for the prediction of glucose. It implements a double attention mechanism enabling the estimation of the contribution of each input variable to the model to the final prediction. We empirically show the value of such a model for the prediction of glucose by analyzing its behavior in the computation of its predictions.
This work deals with the automatic identification of Romance Languages. The aim of our study is to provide linguistic patterns potentially robust for the discrimination of 5 languages from the latin family (i. E., Spanish, French, Italian, Portuguese and Romanian). The Romance Languages have the advantage of a secular linguistic tradition and represents official languages in several countries of the world, the study of the taxonomist approaches devoted to this linguistic family shows a spécial relevance of the typological classification. The first group includes languages with prototypical vocalic systems, whereas the second group, languages with complex vocalic systems in terms of number of oppositions. In addition to the vocalic criteria, these hierarchy is supported by consonantal and prosodic particularities. We conducted two experimental paradigms to test the correspondence between the perceptual patterns used by nai͏̈f listeners to differentiate the Romance languages and the linguistic patterns employed by the typological classification. Romance native language] (i. E., French, Romanian vs. Japanese, Americans), showed different perceptual strategies related both to the native language and to the familiarity with the Romance languages. The linguistic strategies lead to a macro-discrimination of the languages in two groups similar to those obtained via the typological taxonomy based on vocalic particularities (i. E., Spanish, Italian vs. Romanian, French, Portuguese). The second series of perceptual experiments on two groups of subjects (French and American) consisted in the evaluation of the acoustic similarity of the have languages. The results confirmed the division of Romance Languages in the same two groups as those via the discrimination experiments. We concluded that the vocalic patterns may be a robust clue for the discrimination of the Latin idioms into two major linguistic groups: Italian, Spanish vs. Romanian, French,
In the field of chemistry, it is interesting to be able to estimate the physicochemical properties of molecules, especially for industrial applications. These are difficult to estimate by physical simulations, as their implementation often present prohibitive time complexity. However, the emergence of data (public or private) opens new perspectives for the treatment of these problems by statistical methods and machine learning. The main difficulty lies in the characterization of molecules: these are more like a network of atoms (in other words a colored graph) than a vector. The aim of this thesis is to take advantage of public corpora to learn the best possible representations of these structures, and to transfer this global knowledge to smaller datasets. We adapted methods used in automatic processing of natural languages to achieve this goal. To implement them, more theoretical work was needed, especially on the graph isomorphism problem. The results obtained on classification / regression tasks are at least competitive with the state of the art, and even sometimes better, in particular on restricted data sets, attesting some opportunities for transfer learning in this field.
Is it possible to reconcile the need for language assistance to all non-French speakers with the lack of standardized language resources for translating combinations of languages which are genetically and culturally remote? This is the issue raised by Hindi and Urdu translation in France, in the judicial context. The judicial systems in which they are used come from the British colonial heritage based upon common law. Through the analysis of a corpus of various documents, this work is aimed at producing terms and phraseological resources in order to assist the translator-interpreter in finding out translation equivalences between languages. First, we will explore the differences between the three countries' judiciaries, as well as the status of the Hindi, Urdu and French languages. Eventually, we will propose a method for term extraction and sub-corpus alignment in order to stress term or translation equivalences between these languages in the judicial genre. This work, which sheds light on the relations between text, words and context, provides actual field-specific resources for judicial translation and interpretation professionals.
This thesis comes within the scope of talking heads. We are particularly interested in the prediction of labial and jaw coarticulation movements. After analyzing intra and inter speaker variability using two corpora, we defined a prediction algorithm for anticipatory coarticulation based on phonetic rules which takes into account interactions between articulators. It consists in concatenating elementary VC...CV sequences selected by our prediction algorithm and either extracted from the corpus or rebuilt by completion. With the aim of estimating the quality of our synthesis process, we measured differences between real and predicted data for all the sentences of the corpus et we compared our solution with Cohen and Massaro 's algorithm. It turns out that our solution is better for specific VCCV sequences in which anticipation is more complex.
Large variations in writing styles and difficulties in segmenting cursive words are the main reasons for handwritten cursive words recognition for being such a challenging task. An Indian postal document reading system based on a segmentation-free context based stochastic model is presented. The originality of the work resides on a combination of high-level perceptual features with the low-level pixel information considered by the former model and a pruning strategy in the Viterbi decoding to reduce the recognition time. While the low-level information can be easily extracted from the analyzed form, the discriminative power of such information has some limits as describes the shape with less precision. For that reason, we have considered in the framework of an analytical approach, using an implicit segmentation, the implant of high-level information reduced to a lower level. This enrichment can be perceived as a weight at pixel level, assigning an importance to each analyzed pixel based on their perceptual properties. The challenge is to combine the different type of features considering a certain dependence between them. To reduce the decoding time in the Viterbi search, a cumulative threshold mechanism is proposed in a flat lexicon representation. Instead of using a trie representation where the common prefix parts are shared we propose a threshold mechanism in the flat lexicon where based just on a partial Viterbi analysis, we can prune a model and stop the further processing. The cumulative thresholds are based on matching scores calculated at each letter level, allowing a certain dynamic and elasticity to the model. As we are interested in a complete postal address recognition system, we have also focused our attention on digit recognition, proposing different neural and stochastic solutions. To increase the accuracy and robustness of the classifiers a combination scheme is also proposed.
Programs are everywhere in our daily life: computers and phones but also fridges, planes and so on. The main actor in the process of creating these programs is human beings. As thorough as they can be, humans are known to make involuntary errors without their awareness. All long their development task, developers have to continuously face their (or their colleagues) errors. This key observation arises the need of aiding developers in their development/maintenance tasks.
Professionals who have to peruse documents in a limited amount of time or private individuals who want to be informed about a specific topic without having the time to read all the texts about it both need summaries. The increase in electronic documents available have made there search in automatic summarization an important domain in the field of natural language processing. We propose a method based on a sentence classification in semantic clusters, using similarity calculation between sentences. This step allows us to identify the sentences which convey the same information and to remove redundancy from the automatically generated summaries. This method has been evaluated on the # opinion summarization # task of TAC2008 evaluation campaing, and on the # news summarization # task of TAC2008 and TAC2009 campaigns. Our system ranks itself among the first quarter of the participating systems. We also propose to integrate newswire articles structure to our summarization system in order to improve the quality of the summaries it generates. Our summarization method has also been integrated to a larger application which aims to help the user to visualize the main topics of a corpus and to automatically extract the essential information.
Depending on the input representation, this dissertation investigates issues from two classes: meaning representation (MR) to text and text-to-text generation. In the first class (MR-to-text generation, "Generating Sentences"), we investigate how to make symbolic grammar based surface realisation robust and efficient. We propose an efficient approach to surface realisation using a FB-LTAG and taking as input shallow dependency trees. To further improve our robustness, we propose two error mining algorithms: one, an algorithm for mining dependency trees rather than sequential data and two, an algorithm that structures the output of error mining into a tree to represent them in a more meaningful way. We show that our realisers together with these error mining algorithms improves on both efficiency and coverage by a wide margin. In the second class (text-to-text generation, "Simplifying Sentences"), we argue for using deep semantic representations (compared to syntax or SMT based approaches) to improve the sentence simplification task. We use the Discourse Representation Structures for the deep semantic representation of the input. We propose two methods: a supervised approach (with state-of-the-art results) to hybrid simplification using deep semantics and SMT, and an unsupervised approach (with competitive results to the state-of-the-art systems) to simplification using the comparable Wikipedia corpus
Multiple sclerosis (MS) is a chronic disease of the central nervous system, leading cause of nontraumatic disability in young adults. MS is characterized by inflammation, demyelination and neurodegenrative pathological processes which cause a wide range of symptoms, including cognitive deficits and irreversible disability. Concerning the diagnosis of the disease, the introduction of Magnetic Resonance Imaging (MRI) has constituted an important revolution in the last 30 years. In particular, new approaches based on the representation of MR images of the brain as graph have been used to study and quantify damages in the brain white matter network, achieving promising results. Due to their effectiveness in analyzing large amount of data, detecting latent patterns and establishing functional relationships between input and output, these artificial intelligence techniques have gained particular attention in the scientific community and is nowadays widely applied in many context, including computer vision, speech recognition, medical diagnosis, among others. In this work, deep learning methods were developed to support biomedical image analysis, in particular for the classification and the characterization of MS patients based on structural connectivity information. Graph theory, indeed, constitutes a sensitive tool to analyze the brain networks and can be combined with novel deep learning techniques to detect latent structural properties useful to investigate the progression of the disease. In the first part of this manuscript, an overview of the state of the art will be given. We will focus our analysis on studies showing the interest of DTI for WM characterization in MS. An overview of the main deep learning techniques will be also provided, along with examples of application in the biomedical domain. In a second part, two deep learning approaches will be proposed, for the generation of new, unseen, MRI slices of the human brain and for the automatic detection of the optic disc in retinal fundus images. In the third part, graph-based deep learning techniques will be applied to the study of brain structural connectivity of MS patients. Semisupervised and unsupervised approaches were also investigated with the aim of reducing the human intervention in the pipeline
They can represent a basis for automated risk assessment, relying on an identification and valuation of critical assets in the network. This allows to design pro-active and reactive counter-measures for risk mitigation and can be leveraged for security monitoring and network hardening. Our thesis aims to apply a similar approach in Cloud environments, which implies to consider new challenges incurred by these modern infrastructures, since the majority of attack graph methods were designed with traditional environments in mind. Novel virtualization attack scenarios, as well as inherent properties of the Cloud, namely elasticity and dynamism are a cause for concern. To realize this objective, a thorough inventory of virtualization vulnerabilities was performed, for the extension of existing vulnerability templates. Based on an attack graph representation model suitable to the Cloud scale, we were able to leverage Cloud and SDN technologies, with the purpose of building Cloud attack graphs and maintain them in an up-to-date state. Algorithms able to cope with the frequent rate of change occurring in virtualized environments were designed and extensively tested on a real scale Cloud platform for performance evaluation, confirming the validity of the methods proposed in this thesis, in order to enable Cloud administrator to dispose of an up-to-date Cloud attack graph.
The analysis of airborne and satellite images is one of the core subjects in remote sensing. In recent years, technological developments have facilitated the availability of large-scale sources of data, which cover significant extents of the earth's surface, often at impressive spatial resolutions. In addition to the evident computational complexity issues that arise, one of the current challenges is to handle the variability in the appearance of the objects across different geographic regions. For this, it is necessary to design classification methods that go beyond the analysis of individual pixel spectra, introducing higher-level contextual information in the process. In this thesis, we first propose a method to perform classification with shape priors, based on the optimization of a hierarchical subdivision data structure. We then delve into the use of the increasingly popular convolutional neural networks (CNNs) to learn deep hierarchical contextual features. We investigate CNNs from multiple angles, in order to address the different points required to adapt them to our problem. Among other subjects, we propose different solutions to output high-resolution classification maps and we study the acquisition of training data. We also created a dataset of aerial images over dissimilar locations, and assess the generalization capabilities of CNNs. Finally, we propose a technique to polygonize the output classification maps, so as to integrate them into operational geographic information systems, thus completing the typical processing pipeline observed in a wide number of applications. Throughout this thesis, we experiment on hyperspectral, atellite and aerial images, with scalability, generalization and applicability goals in mind.
This PHD thesis offers a synchronic description of Reunion Creole's determiner system as well as an analysis of the interpretation of its noun phrases. The thesis includes new data from two kinds of sources: a small collection of oral corpora, and grammaticality/felicity judgments. We investigate the distribution of the different kinds of NP, the morphosyntactic status of pre- and postnominal elements, the number system, and the expression of definiteness in Reunion Creole.
The present thesis offers a corpus study of the alternation between do it, do this and do that in their use as 'Verb Phrase anaphors' (VPAs), in which they refer to a salient action mentioned in previous discourse, typically by means of a VP, or exophorically to a salient action in the speech situation that is not explicitly mentioned in previous discourse. Do it/this/that have been little studied in the otherwise extant literature on anaphora and especially VP ellipsis (VPE, e.g., Kim knows the answer and Pat does too). This is because it has long been assumed that they are largely interchangeable with each other as well as with do so and VPE, so that detailed analysis of their discourse properties was not deemed worth pursuing. The examples below show that this assumption is flawed: in (1), an attested example from the BNC, do this/that so could be used instead of do it, but in (3), do that is strongly preferred. As for VPE, it is unnatural in (1) and prefers a context of the type in (2). 1. They've been rescuing companies for so long they do it automatically now, I expect. (AB9, ok: they do this/that/so automatically…) 2. They've been rescuing companies for so long that whenever they do, it's always a success. 3. He closes his eyes when he speaks and I don't trust anyone who does that. (…anyone who #does this/#it/#so) Based on a sample of annotated data from the British National corpus (BNC, Davies 2004-), our study will examine the factors driving the alternation between do it/this/that. Amongst others, VPA choice is influenced by register, the presence of an adjunct after the VPA, whether or not the antecedent has already been mentioned prior to the antecedent clause, and, to a lesser extent, the saliency of the antecedent and its presumed familiarity to the addressee. Do it typically refers to highly salient actions which are then further described by means of an adjunct. Do that typically occurs without an adjunct, and sometimes bears much resemblance to VPE in its usage.
Learning-a vital principle of evolution-ensures the transformation of primary data captured by our senses into useful knowledge or abstract and general ideas that can be used in new situations and contexts. Cognitive neuroscience shows that the mechanisms of learning are stimulated by cognitive (e.g. wondering, evaluating errors), physical (e.g. manipulating, moving) and social (e.g. debating, collaborating) engagement. The learner builds knowledge through experience, by exploring his environment, formulating hypotheses and experimenting. Learning is crucial in a context where the exponential evolution of information and communication technologies is changing objects, practices and uses. The development of the Internet of Things (IoT) transforms common objects (e.g. light bulbs, watches, cars) into connected devices (CD) that can collect data and act on the user's environment. Learning becomes both biological and artificial and allows the creation of artificial intelligence systems (AIS) that analyse large volumes of data to automate tasks and assist individuals. Technologies can support learning when the technical possibilities they offer are used to support the process of knowledge construction. Thus, this thesis focuses on learning in the context of IoT and examines how the specificities of CD can be articulated with the mechanisms of learning. In order to identify the characteristics of learning in the context of IoT, we studied existing uses of CD. Based on the state of the art, we proposed a conceptual tool describing the IoT through four dimensions of analysis: Data, Interfaces, Agents and Pervasiveness. This tool enabled us to identify, list, classify and ultimately analyse the uses of CD for learning. In the context of these uses, learning is characterised by physical commitment, contextualisation of knowledge and bringing pedagogical activities closer to reality. Building on the results of this initial work, we have developed an approach to put the specificities of CD to learn sciences. The abstract and often counter-intuitive aspect of scientific knowledge hinders their learning, partly because our perception of reality is subjective and limited by our senses. However, data collected by CD and analysed by AIS provide information about the environment that can be used to extend human perception. Therefore, the objective of our approach, translated by the Data-Representations-Interactions (DRI) model, aims at exploiting OCs and SIAs to facilitate the observation of physical phenomena. According to the DRI model, the learner interacts with representations of a physical phenomenon generated by CD and AIS. In accordance with the mechanisms of learning (e.g. constructivism, role of experience), the learner is led to make observations and manipulations, formulate hypotheses and test them. In order to evaluate the effects and constraints of the DRI model, we have designed LumIoT devices dedicated to the learning of photometric quantities (e.g. luminous flux, luminous intensity, illuminance). Then, we conducted an experiment with 17 students of the Master 1 Multimedia Products and Services of the University of Franche-Comté (Montbéliard). The results of the experiment show that the LumIoT devices, based on the DRI model, have facilitated the observation and understanding of photometric quantities. By making abstract knowledge accessible, the DRI model paves the way for learning devices using CD and AIS to mediate knowledge.
This thesis follows on from a recent study conducted by a few researchers from University of Montpellier, with the aim of proposing to the scientific community an inversion procedure capable of noninvasively estimating patient-specific blood pressure in cerebral arteries. Its first objective is, on the one hand, to examine the accuracy and robustness of the inversion procedure proposed by these researchers with respect to various sources of uncertainty related to the models used, formulated assumptions and patient-specific clinical data, and on the other hand, to set a stopping criterion for the ensemble Kalman filter based algorithm used in their inversion procedure. For this purpose, uncertainty analysis and several sensitivity analyses are carried out. The second objective is to illustrate how machine learning, mainly focusing on convolutional neural networks, can be a very good alternative to the time-consuming and costly inversion procedure implemented by these researchers for cerebral blood pressure estimation. An approach taking into account the uncertainties related to the patient-specific medical images processing and the blood flow model assumptions, such as assumptions about boundary conditions, physical and physiological parameters, is first presented to quantify uncertainties in the inversion procedure outcomes. Uncertainties related to medical images segmentation are modelled using a Gaussian distribution and uncertainties related to modeling assumptions choice are analyzed by considering several possible hypothesis choice scenarii. From this approach, it emerges that the uncertainties on the procedure results are of the same order of magnitude as those related to segmentation errors. Furthermore, this analysis shows that the procedure outcomes are very sensitive to the assumptions made about the model boundary conditions. In particular, the choice of the symmetrical Windkessel boundary conditions for the model proves to be the most relevant for the case of the patient under study. Next, an approach for ranking the parameters estimated during the inversion procedure in order of importance and setting a stopping criterion for the algorithm used in the inversion procedure is presented. The results of this strategy show, on the one hand, that most of the model proximal resistances are the most important parameters for blood flow estimation in the internal carotid arteries and, on the other hand, that the inversion algorithm can be stopped as soon as a certain reasonable convergence threshold for the most influential parameter is reached. Finally, a new numerical platform, based on machine learning and allowing to estimate the patient-specific blood pressure in the cerebral arteries much faster than with the inversion procedure but with the same accuracy, is presented. The application of this platform to the patient-specific data used in the inversion procedure provides noninvasive and real-time estimate of patient-specific cerebral pressure consistent with the inversion procedure estimation.
Context: One of the major challenges for treating neuro-psychiatric pathologies is the follow-up of chronic patients in order to measure early relapses as well as observance and compliance to the treatment. Such a monitoring is possible thanks to the use of connected medical devices (measuring for instance weight, blood pressure or physical activities) but crucial information about how the patients feel are difficult to measure. Regular in-person appointments between doctors and patients are thus required. The growing number of patients however increases the queuing time and often results in episodic followups with unevenly spaced interviews. Apart from the clinical interviews, it is nonetheless possible to measure some symptoms (e.g.: sadness or sleepiness) with a range of techniques: looking at eye movements, with electroencephalographic measurements, or by watching verbal expressions or body movements. Thanks to recent advances in speech processing, it is now possible to detect precise cues in the voice allowing to characterise the state of a speaker (e.g. to measure the sleepiness level). This method has the following advantages: recording voice data is not invasive and does not require specific sensors nor complex calibration processes. It can thus be set up in various environments, outside laboratories, and allows regular and non-restrictive monitoring of patients. In this project, we wish to propose a solution using speech processing that, associated with Artificial Intelligence algorithms, will help us define new biomarkers. These biomarkers will then be used for personalised at-home monitoring of the quality of life of the patients. Aims: Since 2016, we considered using speech analysis for the followup of patients suffering for excessive sleepiness problems. This study, carried out with a collaboration between the SANSPY Lab (CNRS USE 3413) and the LaBRI (CNRS UMR 5800), is currently funded by the Nouvelle Aquitaine region in the framework of the IS-OSA project. We thus could set up a recording procedure allowing us to be the only teams to possess an audio database in French of patients along with their clinical diagnoses. During this preliminary study, we devised an analysis method that employs a reduced number of interpretable features while keeping a good performance level. These results will soon be published in an international journal. These promising results open the path for improvements to be made to the system in order to alleviate the technological barriers of the project: 1. Study the coherence of the different clinical sleepiness measurements and how they are related to voice features. 2. Optimise the research for relevant voice features and link them to clinical symptoms. 3. Increase the size of the database to improve the quality of the automatic classification by using artificial intelligence and deep learning techniques. 4. Embed the system to the virtual companion developed at SANPSY and test it in real conditions at the patient's home.
A Textometric method, however, uses several statistical models to study the distribution of words in large corpora, with the goal of shedding light on significant characteristics of the textual data. In this research, Textometry, an approach traditionally considered incompatible with information extraction methods, is applied to the same corpus as an information extraction procedure in order to obtain information on economic events. Several textometric analyses (characteristic elements, co-occurrences) are examined on a corpus of online news feeds. Both approaches contribute differently to processing textual data, producing complementary analyses of the corpus. Following the comparison, this research presents the advantages for these two text mining methods in strategic monitoring of current events.
The discovery of elementary linguistic units (phonemes, words) only from sound recordings is an unresolved problem that arouses a strong interest from the community of automatic speech processing, as evidenced by the many recent contributions of the state of the art. During this thesis, we focused on using neural networks to answer the problem. We approached the problem using neural networks in a supervised, poorly supervised and multilingual manner. We have developed automatic phoneme segmentation and phonetic classification tools based on convolutional neural networks. The automatic segmentation tool obtained 79% F-measure on the BUCKEYE conversational speech corpus. This result is similar to a human annotator according to the inter-annotator agreement provided by the creators of the corpus. In addition, it does not need a lot of data (about ten minutes per speaker and 5 different speakers) to be effective. In addition, it is portable to other languages (especially for poorly endowed languages such as xitsonga). The phonetic classification system makes it possible to set the various parameters and hyperparameters that are useful for an unsupervised scenario. In the unsupervised context, the neural networks (Auto-Encoders) allowed us to generate new parametric representations, concentrating the information of the input frame and its neighboring frames. We studied their utility for audio compression from the raw signal, for which they were effective (low RMS, even at 99% compression). We also carried out an innovative pre-study on a different use of neural networks, to generate vectors of parameters not from the outputs of the layers but from the values of the weights of the layers. These parameters are designed to mimic Linear Predictive Coefficients (LPC). In the context of the unsupervised discovery of phoneme-like units (called pseudo-phones in this memory) and the generation of new phonetically discriminative parametric representations, we have coupled a neural network with a clustering tool (k-means). The iterative alternation of these two tools allowed the generation of phonetically discriminating parameters for the same speaker: low rates of intra-speaker ABx error of 7.3% for English, 8.5% for French and 8, 4% for Mandarin were obtained. These results allow an absolute gain of about 4% compared to the baseline (conventional parameters MFCC) and are close to the best current approaches (1% more than the winner of the Zero Resource Speech Challenge 2017). The inter-speaker results vary between 12% and 15% depending on the language, compared to 21% to 25% for MFCCs.
Temporal aspects to allow Senegalese communities to share and to co-construct their sociocultural knowledge. Indeed, with the globalization it is very common to meet Senegalese youth knowing more about the geography of the West than its own country. Thus, to refresh the memory of our fellow citizens, we initiated the establishment of an online application that allows them to share and coconstruct their cultural heritage. Our proposals are based on social and semantic web technologies. Indeed, social web proposes a framework where value is created by the aggregation of many individual user contributions. The semantic web enables to find, to combine and to share resources, not only between humans but also between machines. The combination of these two technologies enables Senegalese communities to share and co-construct their cultural heritage in a collaborative and semantic framework. Our contributions include to (i) propose ontologies to annotate sociocultural resources and (ii) provide a collaborative framework to Senegalese communities. Ontologies are backbone of the semantic web and allow to characterize a domain. Thus, we defined two ontologies: 1) a sociocultural ontology based on cultural-historical activity theory and 2) a temporal ontology to annotate temporally sociocultural resources. We also defined a virtual community called cultural knowledge-building community and proposed a prototype that integrates our contributions.
Corpora which are text collections selected for specific purposes, are playing an increasing role in Linguistics and Natural Language Processing (NLP). They are conceived as knowledge sources on natural language use, as much as knowledge on the entities designated by linguistic expressions, and they are used in particular to evaluate NLP application performances. The criteria prevailing on their constitution have an obvious, though still delicate to characterize, impact on (i) the major linguistic structures they contain, (ii) the knowledge conveyed, and, (iii) computational systems' success on a give task. This thesis studies methodologies of automatic extraction of semantic relations on written text corpora. Such a topic calls for a detailed examination of the context in which a given expression holds, as well as for the discovery of the features which determine its meaning, in order to be able to link semantic units. Generally, contextual models are built from the co-occurrence analysis of linguistic informations, drawn from resources and NLP tools. The benefits and limits of these informations are evaluated in a task of relation extraction from corpora belonging to different genres (press article, fairy tale, biography). The results show that these informations are insufficient to reach a satisfying semantic representation as well as to design robust systems. Two problems are particularly addressed. On the one hand, it seems indispensable to add informations related to text genre. So as to characterize the impact of genre on semantic relations, an automatic classification method, which relies on the semantic restrictions holding between verbs and nouns, is proposed. The method is experimented on a fairy tale corpus and on a press corpus. On the other hand, contextual models need to deal with problems which come under discourse surface variation. In a text, related linguistic expressions are not always close to one another and it is sometimes necessary to design complex algorithms in order to detect long dependencies. To answer this problem in a coherent manner, a method of discourse segmentation based on surface structure triggers in written corpora, is proposed. It paves the way for grammars operating on macro-syntactic categories in order to structure the discursive representation of a sentence. This method is applied prior to a syntactic analysis and its improvement is evaluated. The solutions proposed to these problems help us to approach Information Extraction from a particular angle: the implemented system is evaluated on a task of Named Entity correction in the context of a Question-Answering System. This specific need entails the alignment of a category definition on the type of answer expected by the question.
In the case of a software version upgrade, a technical migration or the implementation of new connectors to ensure a proper communication with third-party services, it is crucial to be able to keep the clients data, ensure its consistency and communicate it according to heterogeneous structures (i.e. metamodels) that are not known in advance. In such a context, it is then important to efficiently handle the data 'conversion'from on metamodel to another. This PhD thesis concerns the implementation of an automated model transformation to ensure a federate interoperability: the objective is to on-the-fly infer a set of rules that allow convert data from a source metamodel to a target metamodel. For this, this thesis aims to study techniques coming from various disciplines such as semantic web, machine learning, natural language processing or datagraph exploitation thanks to opimization algorithms, for example. This PhD thesis is funded by the Forterro Sylob company that will also bring real case studies.
This document presents the problems I have been interested in during my PhD thesis. I begin with a concise presentation of the main results, followed by three relatively independent parts. In the first part, I consider statistical inference problems on an i.i.d. sample from an unknown distribution over a countable alphabet. The first chapter is devoted to the concentration properties of the sample's profile and of the missing mass. This is a joint work with Stéphane Boucheron and Mesrob Ohannessian. After obtaining bounds on variances, we establish Bernstein-type concentration inequalities and exhibit a vast domain of sampling distributions for which the variance factor in these inequalities is tight. The second chapter presents a work in progress with Stéphane Boucheron and Elisabeth Gassiat, on the problem of universal adaptive compression over countable alphabets. We give bounds on the minimax redundancy of envelope classes, and construct a quasi-adaptive code on the collection of classes defined by a regularly varying envelope. In the second part, I consider random walks on random graphs with prescribed degrees. I first present a result obtained with Justin Salez, establishing the cutoff phenomenon for non-backtracking random walks. Under certain degree assumptions, we precisely determine the mixing time, the cutoff window, and show that the profile of the distance to equilibrium converges to the Gaussian tail function. Then I consider the problem of comparing the mixing times of the simple and non-backtracking random walks. The third part is devoted to the concentration properties of weighted sampling without replacement and corresponds to a joint work with Yuval Peres and Justin Salez.
For food security systems, data on cultivated surfaces and yields are a prerequisite for agricultural production forecast. Moderate resolution satellite remote-sensing systems offer a synoptic vision that makes them a particularly appropriate information source for the estimation of such data. However, the estimation of cultivated surfaces is still challenging in West Africa, because of highly fragmented farmland, specific weather conditions resulting in high regional variability in terms of agricultural systems and practices, and synchronized phenology of crops and natural vegetation due to the rainfall regime. In this context, this thesis presents three original methodological approaches for the characterization of agricultural systems in West Africa by remote sensing. These methods were developed using MODIS time series (from 250 to 500 m spatial resolution) acquired for Mali. (i) The mapping of cultivated areas was carried out with spectral, spatial and textural indices derived from the images. Two approaches were chosen: one of ISODATA type following a segmentation of the territory based on MODIS imagery, and the other of data mining type based on 'sequential patterns'. The crop map obtained showed a better precision than that of the existing land cover global products (70% vs 50% in average). Furthermore, it was shown that a significant part of user and producer errors (20 to 40%) could not be compressed due to farmland fragmentation. (ii) The mapping of agricultural system types first required the definition of a typology derived from an IER (Institute of Rural Economy in Mali) field survey data base on 100 villages. Three types of agricultural systems were determined at the village scale: mainly cereals (millet, sorghum), mainly intensive crops (maize, cotton) and a mixture of sorghum and cotton. The classification of agricultural systems using the aforementioned remote sensing indicators was carried out by a Random Forest type algorithm with an overall accuracy of 62%. Results bring to light the important part played by temporal NDVI and texture in agricultural system characterization. (iii) Finally, for crop monitoring, the MODIS phenological product was tested and assessed using phenological variables obtained from agro-meteorological simulations made by the SARRA-H plant model. Results show that this product contains inconsistencies due to the significant cloud cover linked with the start of the raining season. After the suppression of incongruous data, the phenological transition dates for crop land derived from MODIS were shown to be earlier by 20 days than the SARRA-H-simulated transition dates, due mainly to the 'agro-ecosystem'mixed nature of surfaces at MODIS pixels scale. The results of this thesis highlight new possibilities for the combinination of remote sensing, field data and agro-meteorological modelling, delivering nonstop information in time and space on the characterization of “Sahel” farmland.
While speech communication is a faculty that seems natural, a lot remainsto be understood about the nature of the cognitive representations and processes that are involved. Central to this PhD research is the study of interactions between perception and action during production or perception of syllables. We choose Bayesian Programming as a rigorous framework within which we provide a mathematical definition of the COSMO model ("Communicating Objects using Sensori-Motor Operations"), which allows to formalize motor, auditory and perceptuo-motor theories of speech communication and to study them quantitatively. This approach first leads to a strong theoretical result:we prove an indistinguishability theorem, according to which, given some ideal learning conditions, motor and auditory theories make identical predictions for perception tasks, and therefore cannot be distinguished empirically. This algorithm, which learns by mimicking acoustic targets, allows to acquire motor skills from acoustic inputs only, with the remarkable property of focusing its learning on the adequate regions. We use syllables synthesized by a vocal tract model (VLAM) to analyse how thedifferent models evolve through learning and how robust they are to degradations.
Metagenomic data from human microbiome is a novel source of data for improving diagnosis and prognosis in human diseases. Machine Learning has obtained great achievements on important metagenomics problems linked to OTU-clustering, binning, taxonomic assignment, etc. The contribution of this PhD thesis is multi-fold: 1) a feature selection framework for efficient heterogeneous biomedical signature extraction, and 2) a novel deep learning approach for predicting diseases using artificial image representations. The framework is efficient on a real and heterogeneous datasets containing metadata, genes of adipose tissue, and gut flora metagenomic data with a reasonable classification accuracy compared to the state-of-the-art methods. The second approach is a method to visualize metagenomic data using a simple fill-up method, and also various state-of-the-art dimensional reduction learning approaches. The new metagenomic data representation can be considered as synthetic images, and used as a novel data set for an efficient deep learning method such as Convolutional Neural Networks. The results show that the proposed methods either achieve the state-of-the-art predictive performance, or outperform it on public rich metagenomic benchmarks.
Online reviewing websites help users decide what to buy or places to go. These platforms allow users to express their opinions using numerical ratings as well as textual comments. The numerical ratings give a coarse idea of the service. On the other hand, textual comments give full details which is tedious for users to read. In this dissertation, we develop novel methods and algorithms to generate personalized, aspect-based summaries of movie reviews for a given user. The first problem we tackle is extracting a set of related words to an aspect from movie reviews. Our evaluation shows that our method is able to extract even unpopular terms that represent an aspect, such as compound terms or abbreviations, as opposed to the methods from the related work. We then study the problem of annotating sentences with aspects, and propose a new method that annotates sentences based on a similarity between the aspect signature and the terms in the sentence. The third problem we tackle is the generation of personalized, aspect-based summaries. We propose an optimization algorithm to maximize the coverage of the aspects the user is interested in and the representativeness of sentences in the summary subject to a length and similarity constraints. Finally, we perform three user studies that show that the approach we propose outperforms the state of art method for generating summaries.
Chapter 1 introduces the dissertation, establishes the research questions and the methodology, questions the stakes of studying the markers um and uh, and lays out the study organization. Chapter 2 defines the main types of disfluencies, clinical and naturally occurring, summarizes the state of the art on the topic, and presents the different positions on their discourse role. Chapter 3 establishes the challenges regarding the fillers um and uh and summarizes studies that support the idea of different pragmatic and functional roles, suggesting that they are markers rather than just fillers. Chapter 5 introduces the two corpora used in this dissertation, ATAROS and Switchboard (SWB), and establishes their contribution. This chapter presents the methodologies for the annotations, the two versions of SWB, as well as the methodology adopted to construct an interoperability between the corpora to analyze um and uh. Chapter 6 analyzes the distribution and the duration of the two markers in SWB and ATAROS depending on speaker and dyad gender, on the conversationÕs naturalness, and on speaker participation. This chapter shows that um and uh are different from each other, that they have different distribution and duration cues depending on the variables, and therefore indicates that they are not used randomly. Chapter 7 focuses on the production of um and uh in SWB, and on the perception of the two markers by comparing two transcription versions of the corpus. The results of this chapter show that um and uh are more often missed than other frequent words such as function words, and that SWB transcribers make more transcription errors on uh than on um, suggesting that um plays a more important role in discourse than uh. Chapter 8 investigates the relationship between stance and the presence and the position of um and uh in an utterance, and reveals that the presence and the position of the two markers is dependent with stance. Chapter 9 looks at the relationship between stance and the acoustic realization of the vowel of the markers, compared to the vowel of other monosyllabic words. The results indicate that the stance values affect the vowel realization to different extents. Chapter 10 consists of a classification experiment that incorporates the findings from previous experiments to find out which features pertinent to um and uh (lexical, position, and acoustics) improve the systemÕs performance. The results also indicate that different acoustic features improve the scores. Chapter 11 concludes the dissertation by summarizing the results from chapters 6 through 10, underlying the impact of this study, and addressing the future directions of this project.
In this thesis, we harvest knowledge of two different types from online resources. The first one is commonsense knowledge, i.e. intuitive knowledge shared by most people like "the sky is blue". We extract salient statements from query logs and question-answering by carefully designing question patterns. Next, we validate our statements by querying other web sources such as Wikipedia, Google Books, or image tags from Flickr. We aggregate these signals to create a final score for each statement. We obtain a knowledge base, QUASIMODO, which, compared to its competitors, has better precision and captures more salient facts. The other kind of knowledge we investigate is hidden knowledge, i.e. knowledge not directly given by a data provider. More concretely, some Web services allow accessing the data only through predefined access functions. To answer a user query, we have to combine different such access functions, i.e., we have to rewrite the query in terms of the functions. We study two different scenarios: In the first scenario, the access functions have the shape of a path, the knowledge base respects constraints called "Unary Inclusion Dependencies", and the query is atomic. We show that the problem is decidable in polynomial time, and we provide an algorithm with theoretical evidence. In the second scenario, we remove the constraints and create a new class of relevant plans called "smart plans". We show that it is decidable to find these plans and we provide an algorithm.
This dissertation addresses the study of French regional accents based on two large corpora totalling over 100 hours of face-to-face speech (PFC) and telephone speech. We investigated the perception, acoustic characteristics as well as automatic classification of French varieties (standard French, French spoken in the South of France, Alsace, Belgium and Switzerland) in order to model segmental and prosodic levels. First, perceptual experiments were conducted to determine French listeners' capacities to discriminate between French accents. Results were analysed using scaling and clustering techniques. Next, acoustic analyses relying on automatic phonemic alignment were carried out measuring formants, fundamental frequency, duration and intensity so as to produce linguistically-motivated segmental and prosodic features. Furthermore, region-specific pronunciation tendencies were quantified exploiting new alignments with augmented pronunciation dictionaries including relevant variants. The different methods allowed us to highlight characteristic pronunciation traits such as the realisation of nasal vowels, /O/ fronting, the devoicing of voiced consonants and word-initial stress.
Most successful deep neural models are the ones with many layers which highly increases their number of parameters. Training such models requires a large number of training samples which is not always available. One of the fundamental issues in neural networks is overfitting which is the issue tackled in this thesis. Such problem often occurs when the training of large models is performed using few training samples. Many approaches have been proposed to prevent the network from overfitting and improve its generalization performance such as data augmentation, early stopping, parameters sharing, unsupervised learning, dropout, batch normalization, etc. In this thesis, we tackle the neural network overfitting issue from a representation learning perspective by considering the situation where few training samples are available which is the case of many real world applications. We propose three contributions. The first one presented in chapter 2 is dedicated to dealing with structured output problems to perform multivariate regression when the output variable y contains structural dependencies between its components. Our proposal aims mainly at exploiting these dependencies by learning them in an unsupervised way. Validated on a facial landmark detection problem, learning the structure of the output data has shown to improve the network generalization and speedup its training. The second contribution described in chapter 3 deals with the classification task where we propose to exploit prior knowledge about the internal representation of the hidden layers in neural networks. This prior is based on the idea that samples within the same class should have the same internal representation. We formulate this prior as a penalty that we add to the training cost to be minimized. Empirical experiments over MNIST and its variants showed an improvement of the network generalization when using only few training samples. Our last contribution presented in chapter 4 showed the interest of transfer learning in applications where only few samples are available. In this application, the task consists in localizing the third lumbar vertebra in a 3D CT scan. A pre-processing of the 3D CT scan to obtain a 2D representation and a post-processing to refine the decision are included in the proposed system.
With the development of information and Internet technology, human society has stepped into an era of information overload. When people exchange information and resources, they leave traces in some way or other. The modelled traces represent knowledge as well as experience concerning the interactive actions among users and resources. Such traces can be defined, modelled and exploited in return to offer a clue on a variety of deductions. This thesis focuses on implementing a recommender system by exploiting various collaborative traces in the group shared/collaborative workspace. As a practical experience we tested our prototype in the context of the E-MEMORAe collaborative platform.
This work aims at presenting methods of construction of electronic dictionaries of frozen nominal sequences in Korean and their inflected forms, and at justifying their validity by applying our dictionary in applied fields of automatic analysis of Korean texts. For lexicon-based recognition of frozen nominal sequences, we classified them in three categories according to typographical conventions: compact nouns (CN), frozen nouns with optional spacing (FNO) and frozen nouns with obligatory spacing (FNS). Since inflected forms of frozen nominal sequences appear in Korean texts, we built (i) an electronic dictionary of FNO with 45000 entries and (ii) a transducer of sequences of nominal postpositions with their segmentation, and finally these two data sets were merged into a single dictionary through the aid of inflectional codes attached to each frozen nominal sequence and of the inflection module in INTEX. Our dictionary built according to these methods has the principal following advantages compared to the preexistent systems: 1) The dictionary of inflected forms of FNO allows automatic recognition of all the alternatives of FNO related to spacing 2) The dictionary of inflected forms of FNO allows segmentation of inflected forms of FNO into a FNO and a sequence of nominal postpositions without error 3) The dictionary of sequences of nominal postpositions represented by graphs allows their segmentation in nominal postpositions without error 4) The dictionary of FNO is used for segmentation of the free nominal sequences written without delimiters 5) The dictionary of FNO can be extended into a bilingual dictionary for machine translation 6) Each entry of the dictionary of FNO comprises useful codes for the natural language processing applications: codes indicating semantic features, status of predicative noun, head noun of each entry, origin and grammatical
In this study I examine different types of reciprocal nominal predicates or noun of reciprocity in order to give a full account on their syntactic and semantic properties. These nouns can be found in constructions with the structure "These is Npred between A and B", correlating obligatory at least two elements (humans, concrets, ideas, etc.). They can permute within the sentence without any effect of meaning. My analysis aims to contribute to a more general research working out a semantic typology of a part of the lexicon, the predicative nouns in the perspective of a natural language processing.
The last years witnessed the success of the Linked Open Data (LOD) project as well as a significantly growing amount of semantic data sources available on the web. However, there are still a lot of data not being published as fully materialized knowledge bases like as sensor data, dynamic data, data with limited access patterns, etc. Such data is in general available through web APIs or web services. Integrating such data to the LOD or in mashups would have a significant added value. However, discovering such services requires a lot of efforts from developers and a good knowledge of the existing service repositories that the current service discovery systems do not efficiently overcome. In this thesis, we propose novel approaches and frameworks to search for semantic web services from a data integration perspective. Firstly, we introduce LIDSEARCH, a SPARQL-driven framework to search for linked data and semantic web services. To achieve such a purpose, we apply natural language processing techniques and deep-learning-based text similarity techniques to leverage I/O relations from text to ontologies.
For instance in visual scene understanding, structured prediction allows to provide a natural language sentence describing the visual scene. Other tasks can be found in many other areas such bioinformatics (prediction of a biomolecule) or natural language processing (machine translation, sequence labelling)... Another angle to structured prediction is to consider it as a regression task in some structured output space.
This work examines the effects of different kinds of training for the acquisition of the British English monophthongs /æ, ʌ, ɑː, ɪ and iː/ by French late learners. Behavioral measures were computed before and after training in order to evaluate changes occurring on perception and production abilities. Furthermore, an Event Related Potentials study (MMN) was conducted in order to observe the neural correlates of the acquisition of the trained /æ-ʌ/ contrast. A first High Variability Phonetic Training including identification and discrimination tasks with feedback was designed. It was proposed to 16 French learners constituting the so-called PE-Group. The feedback was based on the analysis of the acoustic parameters of the vowel produced by the learners, allowing them to evaluate their own production. A third group, who did not receive any training program was also included (C-Group). The PE-group was shown to perform better after training in identification and discrimination compared to the C-Group, and the PR-Group improved in identification only. Production performances were evaluated through two different methods: 1) one based on the acoustic parameters of the vowels; 2) the other based on the classification of the produced vowels by native English speakers. It was shown that the HVPT was better to improve production performances. However, it was observed that production improvement was restricted to intelligibility, as evaluated by native speakers, and that foreign accent reflected by acoustic measures was not significantly reduced in experimental groups (PE and PR) compared to the C-Group. The expected Event Related Potentials were not observed, but we discuss potential improvements to our protocol.
Weakly-supervised learning studies the problem of minimizing the amount of human effort required for training state-of-the-art models. However, in practice weakly-supervised methods perform significantly worse than their fully-supervised counterparts. This is also the case in deep learning, where the top-performing computer vision approaches remain fully-supervised, which limits their usage in real world applications. This thesis attempts to bridge the gap between weakly-supervised and fully-supervised methods by utilizing motion information. It also studies the problem of moving object segmentation itself, proposing one of the first learning-based methods for this task. This is especially challenging due to the need to precisely capture object boundaries and avoid local optima, as for example segmenting the most discriminative parts. In contrast to most of the state-of-the-art approaches, which rely on static images, we leverage video data with object motion as a strong cue. In particular, our method uses a state-of-the-art video segmentation approach to segment moving objects in videos. The approximate object masks produced by this method are then fused with the semantic segmentation model learned in an EM-like framework to infer pixel-level semantic labels for video frames. Thus, as learning progresses, the quality of the labels improves automatically. We then integrate this architecture with our learning-based approach for video segmentation to obtain a fully trainable framework for weakly-supervised learning from videos. In the second part of the thesis we study unsupervised video segmentation, the task of segmenting all the objects in a video that move independently from the camera. This task presents challenges such as strong camera motion, inaccuracies in optical flow estimation and motion discontinuity. We address the camera motion problem by proposing a learning-based method for motion segmentation: a convolutional neural network that takes optical flow as input and is trained to segment objects that move independently from the camera. It is then extended with an appearance stream and a visual memory module to improve temporal continuity. The appearance stream capitalizes on the semantic information which is complementary to the motion information. The visual memory module is the key component of our approach: it combines the outputs of the motion and appearance streams and aggregates a spatio-temporal representation of the moving objects. The final segmentation is then produced based on this aggregated representation. The resulting approach obtains state-of-the-art performance on several benchmark datasets, outperforming the concurrent deep learning and heuristic-based methods.
Human centered and concurring product design is based on the collaboration between mechanical engineers, human factor experts and industrial designers. This collaboration is often difficult and can be eased through the use of intermediary objects such as Virtual Reality (VR). Nevertheless, even though VR is widely used in the industry, it suffers from a lack of acceptance by product designers. In the context of this research work, we suggest to use VR in the form of immersive multidisciplinary convergence support tools. These tools are developed in accordance with an anthropocentered approach, as a function of each specific product design project's needs. In order to optimize development times, we propose a dedicated immersive software design methodology: the ASAP methodology (As Soon As Possible). A first experiment, aiming to demonstrate the feasibility of the ASAP methodology and the effectiveness of the implemented immersive tools, has been conducted in the context of industrial product design projects. A second experiment, involving more than 50 participants, has been conducted in the context of educational product design projects and led to the development of 12 immersive tools. It demonstrated quantitatively the contribution of immersive tools to the perceived effectiveness of interdisciplinary convergence phases and the contribution of the ASAP methodology on the acceptation of VR by product designers. This research work describes a first approach that could, according to us, allow a better integration of VR within product design processes.
Manual corpus annotation has become a key issue for Natural Langage Processing (NLP), as manually annotated corpora are used both to create and to evaluate NLP tools. However, the process of manual annotation remains underdescribed and the tools used to support it are often misused. This situation prevents the campaign manager from evaluating and guarantying the quality of the annotation. We propose in this work a unified vision of manual corpus annotation for NLP. It results from our experience of annotation campaigns, either as a manager or as a participant, as well as from collaborations with other researchers. We first propose a global methodology for managing manual corpus annotation campaigns, that relies on two pillars: an organization for annotation campaigns that puts evaluation at the heart of the process and an innovative grid for the analysis of the complexity dimensions of an annotation campaign. A second part of our work concerns the tools of the campaign manager. We evaluated the precise influence of automatic pre-annotation on the quality and speed of the correction by humans, through a series of experiments on part-of-speech tagging for English. Furthermore, we propose practical solutions for the evaluation of manual annotations, that proche che vide the campaign manager with the means to select the most appropriate measures. Finally, we brought to light the processes and tools involved in an annotation campaign and we instantiated the methodology that we described.
Speaker diarization is the task of determining "who speaks when" in an audio stream that usually contains an unknown amount of speech from an unknown number of speakers. Speaker diarization systems are usually built as the combination of four main stages. First, non-speech regions such as silence, music, and noise are removed by Voice Activity Detection (VAD). Next, speech regions are split into speaker-homogeneous segments by Speaker Change Detection (SCD), later grouped according to the identity of the speaker thanks to unsupervised clustering approaches. Finally, speech turn boundaries and labels are (optionally) refined with a re-segmentation stage. In this thesis, we propose to address these four stages with neural network approaches. In the speech turn clustering stage, we propose to use affinity propagation on top of neural speaker embeddings. Experiments on a broadcast TV dataset show that affinity propagation clustering is more suitable than hierarchical agglomerative clustering when applied to neural speaker embeddings. The LSTM-based segmentation and affinity propagation clustering are also combined and jointly optimized to form a speaker diarization pipeline. Compared to the pipeline with independently optimized modules, the new pipeline brings a significant improvement. In addition, we propose to improve the similarity matrix by bidirectional LSTM and then apply spectral clustering on top of the improved similarity matrix. The proposed system achieves state-of-the-art performance in the CALLHOME telephone conversation dataset. To better understand its behavior, the analysis is based on a proposed encoder-decoder architecture. Our proposed systems bring a significant improvement compared with traditional clustering methods on toy examples.
This research paper, situated at the crossroads of linguistics, language teaching and automated language processing tools, focuses firstly on the classification of the textual materials used in the teaching of French as a Foreign Language (FFL), and secondly on the analysis of the various criteria required for such a classification. Our analysis centres chiefly on the lexical criterion, the determining factor in the reading process. Starting from a pre-selected sample of textual materials drawn from FFL materials, we therefore set out to determine whether a classification along these lines was viable. To this end, we analysed these texts from the point of view of their correspondence with the levels drawn up by the Common European Framework of References (CEFR), levels A1 to B2, using the inventories developed in the Reference Frameworks for French. In the first part, we consider the questions of interdisciplinarity, textual materials, text typologies and reference frameworks, by seeking to determine to what extent the lexical approach may justifiably be adopted as the chief criterion of this classification. In the second part, we use information technologies to analyse the correspondence between the textual materials and the CEFR levels based on the lexical criterion, with a view to examining the scope for developing a dynamic corpus of textual materials for use in FFL.
Nowadays, there are a lot of applications related to machine vision and hearing which tried to reproduce human capabilities on machines. These problems are mainly amenable to a temporal signals classification problem, due our interest to this subject. In fact, we were interested to two distinct problems, humain gait recognition and audio signal recognition including both environmental and music ones. In the former, we have proposed a novel method to automatically learn and select the dynamic human body-parts to tackle the problem intra-class variations contrary to state-of-art methods which relied on predefined knowledge. To achieve it a group fused lasso algorithm is applied to segment the human body into parts with coherent motion value across the subjects.
The method we propose takes into account various linguistic and discursive cues calling on different levels of analysis. As obsolescence is a non-linguistic phenomenon, our hypothesis is that linguistic and discursive cues must be considered in terms of combinations. A machine learning system is then implemented to bring out relevant cue configurations in the obsolescent segments characterized by the experts. Both our objectives have been achieved: we propose a detailed description of obsolescence in our corpus of encyclopaedic texts as well as a prototype aid to updating. A double evaluation was carried out: by cross validation on the corpus used for machine learning and by experts on a test corpus.
Radiologists use medical imaging solutions on a daily basis for diagnosis. Improving user experience is a major line of the continuous effort to enhance the global quality and usability of software products. Monitoring applications enable to record the evolution of various software and system parameters during their use and in particular the successive actions performed by the users in the software interface. These interactions may be represented as sequences of actions. Based on this data, this work deals with two industrial topics: software crashes and software usability. Both topics imply on one hand understanding the patterns of use, and on the other developing prediction tools either to anticipate crashes or to dynamically adapt software interface according to users'needs. For this purpose, we propose to use a binomial test to determine which type of patterns is the most appropriate to represent crash signatures. The improvement of software usability through customization and adaptation of systems to each user's specific needs requires a very good knowledge of how users use the software. In order to highlight the trends of use, we propose to group similar sessions into clusters. We compare 3 session representations as inputs of different clustering algorithms. The second contribution of our thesis concerns the dynamical monitoring of software use. Both methodologies take advantage of the recurrent structure of LSTM neural networks to capture dependencies among our sequential data as well as their capacity to potentially handle different types of input representations for the same data.
This doctoral dissertation presents, discusses and proposes tools for the automatic information retrieval in big musical databases. The main application is the supervised classification of musical themes to generate thematic playlists. The first chapter introduces the different contexts and concepts around big musical databases and their consumption. The second chapter focuses on the description of existing music databases as part of academic experiments in audio analysis. This chapter notably introduces issues concerning the variety and unequal proportions of the themes contained in a database, which remain complex to take into account in supervised classification. The third chapter explains the importance of extracting and developing relevant audio features in order to better describe the content of music tracks in these databases. This chapter explains several psychoacoustic phenomena and uses sound signal processing techniques to compute audio features. New methods of aggregating local audio features are proposed to improve song classification. The fourth chapter describes the use of the extracted audio features in order to sort the songs by themes and thus to allow the musical recommendations and the automatic generation of homogeneous thematic playlists. This part involves the use of machine learning algorithms to perform music classification tasks. The contributions of this dissertation are summarized in the fifth chapter which also proposes research perspectives in machine learning and extraction of multi-scale audio features.
This linguistic approach has been implemented by a generator of expert system called "snark".
Opinion mining is a sub-discipline within Information Retrieval (IR) and Computational Linguistics. It refers to the computational techniques for extracting, classifying, understanding, and assessing the opinions expressed in various online sources like news articles, social media comments, and other user-generated content. It is also known by many other terms like opinion finding, opinion detection, sentiment analysis, sentiment classification, polarity detection, etc. Defining in more specific and simpler context, opinion mining is the task of retrieving opinions on an issue as expressed by the user in the form of a query. There are many problems and challenges associated with the field of opinion mining. In this thesis, we focus on some major problems of opinion mining.
Location Based Services (LBS) had been involved to deliver relevant geospatial information based on a geographic position or address. We assume that integrating geospatial data from several sources may improve the quality of information offered to users.
The increasing development of networks and especially the Internet has greatly expanded the gap between heterogeneous information systems. In a review of studies of interoperability of heterogeneous information systems, we find that all the work in this area tends to be in solving the problems of semantic heterogeneity. The W3C (World Wide Web Consortium) standards proposed to represent the semantic ontology. Ontology is becoming an indispensable support for interoperability of information systems, and in particular the semantics. The structure of the ontology is a combination of concepts, properties and relations. This combination is also called a semantic graph. The OWL (Ontology Web Language) and RDF (Resource Description Framework) are the most important languages of the Semantic Web, and are based on XML. RDF is the first W3C standard for enriching resources on the Web with detailed descriptions, and increases the facility of automatic processing of Web resources. Descriptions may be characteristics of resources, such as the author or the content of a website. These descriptions are metadata. Enriching the Web with metadata allows the development of the so-called Semantic Web. RDF is used to represent semantic graphs corresponding to a specific knowledge modeling. RDF files are typically stored in a relational database and manipulated using SQL, or derived languages such as SPARQL. This solution is well suited for small RDF graphs, but is unfortunately not well suited for large RDF graphs. These graphs are rapidly evolving, and adapting them to change may reveal inconsistencies. Driving the implementation of changes while maintaining the consistency of a semantic graph is a crucial task, and costly in terms of time and complexity. An automated process is essential. For these large RDF graphs, we propose a new way using formal verification entitled "Model Checking". Model Checking is a verification technique that explores all possible states of the system. In this way, we can show that a model of a given system satisfies a given property. This thesis provides a new method for checking and querying semantic graphs. We propose an approach called ScaleSem which transforms semantic graphs into graphs understood by the Model Checker (The verification Tool of the Model Checking method). It is necessary to have software tools to perform the translation of a graph described in a certain formalism into the same graph (or adaptation) described in another formalism
The behaviour of machines is difficult to define, especially when machines have to adapt to a changing environment. For example, this is the case when human-machine interactions are concerned. Indeed, the machine has to deal with several sources of uncertainty to exhibit a consistent behaviour to the user. First, it has to deal with the different behaviours of the users and also with a change in the behaviour of a user when he gets used to the machine. Secondly, the communication between the user and the machine can be noisy, which makes the transfer of information more complicated. The objective is thus to deal with the different sources of uncertainty to show a consistent behaviour. Usually, dealing with uncertainties is performed by introducing models: models of the users, the task concerned or the decision. However, the accuracy of the solution depends on the accuracy of expert knowledge used to build the models. If machine learning, through reinforcement learning, has successfully avoided the use of model for the decision and removed \textit{ad hoc} knowledge about it, expert knowledge is still necessary. The thesis presented in this work is that some constraints related to human expertise can be slackened without a loss of generality related to the introduction of models
Online Social Networks are increasing and piercing our lives such that almost every person in the world has a membership at least in one of them. Among famous social networks, there are online shopping websites such as Amazon, eBay and other ones which have members and the concepts of social networks apply to them. This thesis is particularly interested in the online shopping websites and their networks. One of the challenging issues is providing useful information to help the customers in their shopping. Thus, an underlying question the thesis seeks to answer is how to provide comprehensive information to the customers in order to help them in their shopping. This is important for the online shopping websites as it satisfies the customers by this useful information and as a result increases their customers and the benefits of both sides. To do so, the users are ranked based on two scores namely optimist and pessimist. In the second part, a novel opinion propagation methodology is presented to reach an agreement and maintain the consistency among users which subsequently, makes the aggregation feasible. The propagation is conducted considering the impacts of the influential users and the neighbors. Ultimately, in the third part, the opinion aggregation is proposed to gather the existing opinions and present it as the valuable information to the customers regarding each product of the online shopping website. To this end, the weighted averaging operator and fuzzy techniques are used. The thesis presents a consensus opinion model in signed and unsigned networks. This solution can be applied to any group who needs to find a plenary opinion among the opinions of its members.
This action-research in applied linguistics investigates the evolution of adolescent learners of German as a foreign language during a project involving corpus exploration. It focuses on the role played by resources, tools, face-to-face learning versus distance learning and by the pedagogical relationship. My research analyzes how the learners perceived and exploited three small specialized corpora containing texts representing each one sub-genre of the «tourist information» genre. Secondly, I describe how the language and the discourse used in the texts changed during the project. The tasks and tools of the project helped some of the learners to develop reading habits which focus on information retrieval more than on complete comprehension of the text. The effects are texts written in a German ha! ving linguistic features that resemble those attested in the explored corpora. The intertextual writing method offered by the project supports acculturation to the foreign language. By means of concordancing and the import of collocations, the learners are now able to deal with some language features, especially with the domain of German adjective inflection, better than before.
The task of speaker diarization, as defined by NIST, considers the recordings from a corpus as independent processes. The recordings are processed separately, and the overall error rate is a weighted average. In this context, detected speakers are identified by anonymous labels specific to each recording. Therefore, a speaker appearing in several recordings will be identified by a different label in each of the recordings. Yet, this situation is very common in broadcast news data: hosts, journalists and other guests may appear recurrently. Consequently, speaker diarization has been recently considered in a broader context, where recurring speakers must be uniquely identified in every recording that compose a corpus. This generalization of the speaker partitioning problem goes hand in hand with the emergence of the concept of collections, which refers, in the context of speaker diarization, to a set of recordings sharing one or more common characteristics. The work proposed in this thesis concerns speaker clustering of large audiovisual collections (several tens of hours of recordings). The main objective is to propose (or adapt) clustering approaches in order to efficiently process large volumes of data, while detecting recurrent speakers. The effectiveness of the proposed approaches is discussed from two point of view: first, the quality of the produced clustering (in terms of error rate), and secondly, the time required to perform the process. For this purpose, we propose two architectures designed to perform cross-show speaker diarization with collections of recordings. We propose a simplifying approach to decomposing a large clustering problem in several independent sub-problems. Solving these sub-problems is done with either of two clustering approaches which takeadvantage of the recent advances in speaker modeling.
A coreference chain is the set of linguistic expressions — or mentions — that refer to the same entity or discourse object in a given document. Coreference resolution consists in detecting all the mentions in a document and partitioning their set into coreference chains. Coreference chains play a central role in the consistency of documents and interactions, and their identification has applications to many other fields in natural language processing that rely on an understanding of language, such as information extraction, question answering or machine translation. Natural language processing systems that perform this task exist for many languages, but none for French — which suffered until recently from a lack of suitable annotated resources — and none for spoken language. In this thesis, we aim to fill this gap by designing a coreference resolution system for spoken French. To this end, we propose a knowledge-poor system based on an end-to-end neural network architecture, which obviates the need for the preprocessing pipelines common in existing systems while maintaining performances comparable to the state-of-the art. We then propose extensions on that baseline, by augmenting our system with external knowledge obtained from resources and preprocessing tools designed for written French. Finally, we propose a new standard representation for coreference annotation in corpora of written and spoken languages, and demonstrate its use in a new version of ANCOR, the first coreference corpus of spoken French.
As a first step, theoretical issues of such a research are clarified. It leads to focus on the concept of semantic invariant to give an insight into the semantic identity of a given noun regardless of its contextual variations. As a second step, the empirical object of this research – human body part nouns in contemporary French – is delimited. The rest of the dissertation consists in the empirical investigation itself. First of all, an overall description of the semantic variation of the French human body part nouns is provided. Eventually, four more nouns (artère “artery”, épaule “shoulder”, bouche “mouth” and pied “foot”) are studied from a semantic point of view. Each of these four studies offers a new opportunity to test the relevance of the semantic invariant concept in order to give an account of the polysemy in the nominals.
In certain sensitive environments, such as the healthcare domain, where users are generally trusted and where particular events may occur, such as emergencies, the implemented security controls in the corresponding information systems should not block certain decisions and actions of users. This could have serious consequences. Indeed, it is important to be able to identify and trace these actions and decisions in order to detect possible violations of the security policy put in place and fix responsibilities. These functions are ensured by the a posteriori access control that lies on a monitoring mechanism based on logs. In the literature, this type of access control hasbeen divided into three stages: log processing, log analysis, and accountability. In this thesis, we cover these three areas of the a posteriori access control by providing new solutions, and we introduce new aspects that have not been addressed before.
Criminal analysis is a discipline that supports investigations practiced within the National Gendarmerie. However, criminal analysis relies on entities to formalize its practice. The presentation of the research context details the practice of criminal analysis as well as the constitution of judicial procedure files as textual corpora. We then propose perspectives for the adaptation of natural language processing(NLP) and information extraction methods to the case study, including a comparison of the concepts of entity in criminal analysis and named entity in NLP. This comparison is done on the conceptual and linguistic plans. A first approach to the detection of entities in witness interviews is presented. Finally, since textual genre is a parameter to be taken into account when applying automatic processing to text, we develop a structure of the «legal» textual genre into discourse, genres, and sub-genres through a textometric study aimed at characterizing different types of texts (including witness interviews) produced by the field of justice
Computer vision is an interdisciplinary field that investigates how computers can gain a high level of understanding from digital images or videos. In artificial intelligence, and more precisely in machine learning, the field in which this thesis is positioned,computer vision involves extracting characteristics from images and then generalizing concepts related to these characteristics. This field of research has become very popular in recent years, particularly thanks to the results of the convolutional neural networks that form the basis of so-called deep learning methods. Today, neural networks make it possible, among other things, to recognize different objects present in an image, to generate very realistic images or even to beat the champions at the Go game. Their performance is not limited to the image domain, since they are also used in other fields such as natural language processing (e. g. machine translation) or sound recognition. In this thesis, we study convolutional neural networks in order to develop specialized architectures and loss functions for low-level tasks (color constancy) as well as high-level tasks (semantic segmentation). In computer vision, the main approach consists in estimating the color of the illuminant and then suppressing its impact on the perceived color of objects. Our experience shows that our method makes it possible to obtain competitive performances with the state of the art. Nevertheless, our architecture requires a large amount of training data. In order to partially correct this problem and improve the training of neural networks, we present several techniques for artificial data augmentation. We are also making two contributions on a high-level issue: semantic segmentation. This task, which consists of assigning a semantic class to each pixel of an image, is a challenge in computer vision because of its complexity. On the one hand, it requires many examples of training that are costly to obtain. On the other hand, it requires the adaptation of traditional convolutional neural networks in order to obtain a so-called dense prediction, i. e., a prediction for each pixel present in the input image. To do this, we define a selective loss function that has the advantage of allowing the training of a convolutional neural network from data from multiple databases. We also developed self-context approach that captures the correlations between labels in different databases. Finally, we present our third contribution: a new convolutional neural network architecture called GridNet specialized for semantic segmentation. Unlike traditional networks, implemented with a single path from the input (image) to the output (prediction), our architecture is implemented as a 2D grid allowing several interconnected streams to operate at different resolutions. In addition, we empirically demonstrate that our architecture generalize many of well-known stateof- the-art networks. We conclude with an analysis of the empirical results obtained with our architecture which, although trained from scratch, reveals very good performances, exceeding popular approaches often pre-trained
Oedipus, the character in sophocle's tragedy, solves the sphinx's enigma by + his own intelligence ;. This is the starting point of a general reflection on the linguistic status of language games, the practice of which could be seen throughout all periods and in all cultures. Oedipus's intelligence is based on a capacity for + calculating ; the interpretation of the enigma by giving up inductive reasoning (by recurrence) so as to adopt analogical reasoning instead. In the second part, it is shown that the calculation of the meaning of the polysemous messages enables us to propose a pattern of a combinatory analysis which is a tool for the automatic treatment of language (atl), able to help calculate riddles and to interpret coded definitions of crosswords. This pattern is used as a touchstone for an analysis of the semantic structures underlying interpretations and shows which lexical items are concerned by isotopy. Isotopy is not in that case considered to be an element of the message but a process of the interpretation. The whole approach is then based on interpretative semantics. The third part is the developement of the reflection including the treatment of enigmatic messages in the issues of the man-machine dialogue (mmd) which enables us to deal with the ambiguities of some utterances and is able to understand + strange messages ; on the basis of propositions of interpretation extrapolated from the pattern. Then little by little we analyse the calculation of the one who gets messages like an activity which consists in analysing graphematic and acoustic signs. Taking the signs into account is a confrontation with what is expected in the linguistic system and it enables us to carry out a series of decisions leading to the identification of a coherent analysis. This coherence and the analysis are compared to the approach adopted when + reading ; an anamorphosis (in art painting) or when decoding the organisation rules in suites of cards in eleusis'game. We find a similar approach when we have to interpret the + scriptio continua ; on paleographic inscriptions, the technique of which serves as a basis for some literary experiences under duress and for hidden puns.
Nowadays, social media has widely affected every aspect of human life. The most significant change in people's behavior after emerging Online Social Networks (OSNs) is their communication method and its range. Having more connections on OSNs brings more attention and visibility to people, where it is called popularity on social media. Depending on the type of social network, popularity is measured by the number of followers, friends, retweets, likes, and all those other metrics that is used to calculate engagement. Studying the popularity behavior of users and published contents on social media and predicting its future status are the important research directions which benefit different applications such as recommender systems, content delivery networks, advertising campaign, election results prediction and so on. This thesis addresses the analysis of popularity behavior of OSN users and their published posts in order to first, identify the popularity trends of users and posts and second, predict their future popularity and engagement level for published posts by users. To this end, i) the popularity evolution of ONS users is studied using a dataset of 8K Facebook professional users collected by an advanced crawler. The collected dataset includes around 38 million snapshots of users'popularity values and 64 million published posts over a period of 4 years. Clustering temporal sequences of users'popularity values led to identifying different and interesting popularity evolution patterns. The identified clusters are characterized by analyzing the users'business sector, called category, their activity level, and also the effect of external events. Then ii) the thesis focuses on the prediction of user engagement on the posts published by users on OSNs. A novel prediction model is proposed which takes advantage of Point-wise Mutual Information (PMI) and predicts users'future reaction to newly published posts. Finally, iii) the proposed model is extended to get benefits of representation learning and predict users'future engagement on each other's posts. The proposed prediction approach extracts user embedding from their reaction history instead of using conventional feature extraction methods. The performance of the proposed model proves that it outperforms conventional learning methods available in the literature. The models proposed in this thesis, not only improves the reaction prediction models to exploit representation learning features instead of hand-crafted features but also could help news agencies, advertising campaigns, content providers in CDNs, and recommender systems to take advantage of more accurate prediction results in order to improve their user services
This thesis deals with the application of artificial intelligence to the automatic classification of audio sequences according to the emotional state of the customer during a commercial phone call. The goal is to improve on existing data preprocessing and machine learning models, and to suggest a model that is as efficient as possible on the reference IEMOCAP audio dataset. We draw from previous work on deep neural networks for automatic speech recognition, and extend it to the speech emotion recognition task. We are therefore interested in End-to-End neural architectures to perform the classification task including an autonomous extraction of acoustic features from the audio signal. Traditionally, the audio signal is preprocessed using paralinguistic features, as part of an expert approach. We choose a naive approach for data preprocessing that does not rely on specialized paralinguistic knowledge, and compare it with the expert approach. In order to apply a neural network to a prediction task, a number of aspects need to be considered. On the one hand, the best possible hyperparameters must be identified. On the other hand, biases present in the database should be minimized (non-discrimination), for example by adding data and taking into account the characteristics of the chosen dataset. We study these aspects in order to develop an End-to-End neural architecture that combines convolutional layers specialized in the modeling of visual information with recurrent layers specialized in the modeling of temporal information. We propose a deep supervised learning model, competitive with the current state-of-the-art when trained on the IEMOCAP dataset, justifying its use for the rest of the experiments. Our model is evaluated on two English audio databases proposed by the scientific community: IEMOCAP and MSP-IMPROV. A first contribution is to show that, with a deep neural network, we obtain high performances on IEMOCAP, and that the results are promising on MSP-IMPROV. Another contribution of this thesis is a comparative study of the output values ​​of the layers of the convolutional module and the recurrent module according to the data preprocessing method used: spectrograms (naive approach) or paralinguistic indices (expert approach). We analyze the data according to their emotion class using the Euclidean distance, a deterministic proximity measure. We try to understand the characteristics of the emotional information extracted autonomously by the network. The idea is to contribute to research focused on the understanding of deep neural networks used in speech emotion recognition and to bring more transparency and explainability to these systems, whose decision-making mechanism is still largely misunderstood.
In natural language processing, two main approaches are used: machine learning and data mining. In this context, cross-referencing data mining methods based on patterns and statistical machine learning methods is apromising but hardly explored avenue.
Formal approaches to representing the signs of Sign Languages are traditionally parametric and this work shows that they are inappropriate for use in computer science. The main reasons are: the parameters used are neither all necessary nor do they form a sufficient set; parameters take on fixed values whereas signs are dynamic in nature and values change through time; parametric descriptions do not account for the signs'adaptability to context, hence are not reusable, which brings them to disregard the power in concision of sign languages. We propose a model called Zebedee, which describes signs in a sequence of timing units, each of which specifies a set of necessary and sufficient constraints to apply to a skeleton. The signing space is regarded as a Euclidean geometric space where any auxiliary geometric object may be built. Dependencies between elements of the descriptions or indeed on context are not only possible but also made relevant, and are based on articulatory, semantic and cognitive issues. We then give two complementary processes for evaluation: in computer science with the implementation of Zebedee in a signing avatar animation platform and an information display system for train stations, and in linguistics with a data base and new possibilities of queries that linguists may want to test. As prospects, we discuss different computational fields in which Zebedee should be useful, and several present linguistic problems for which it holds pieces of solutions.
Wireless sensing has evolved since the discovery of radio wave echo detection and radar in 1886. However, for the longest time, its usefulness was seldom for human-centric applications because of technical limitations, impracticality or costliness. Introducing wireless networks awakened a newfound interest in developing new wireless sensing services for their seamlessness and versatility. Integrating such functionalities would contribute to resolving some prominent societal issues. Localization, motion detection, and vital signs monitoring have great potential for promoting healthy aging, public safety, and retail. Contactless sensing offers an appreciable degree of freedom, enabling remote monitoring of the isolated elderly without hampering their daily lives. It could assist public safety services for crowd counting and detection of survivors inside buildings during emergencies. Retail and public facilities would benefit from passive and active localization to offer an enhanced experience to their visitors and to help their logistical efforts. While other works provide coarse-grained solutions for resolving such issues, we use MIMO radar techniques to provide an accurate orientation estimation system for Wi-Fi infrastructures. To be more precise, we analyze the phase information of signals received on the antenna array to compute the heading of a Wi-Fi terminal. Current solutions are complex, costly, or not energy-efficient. To address this problem, we introduce MIMO capabilities to LoRa LPWAN systems that provide accurate localization with limited startup costs. We enable the angle of arrival estimation by leveraging a second antenna on the LoRaWAN gateway. We also prove the usefulness of such information for wireless communication efficiency. A third challenge for wireless localization is the inefficiency of current model-based approaches in case of non-line-of-sight conditions and the rigidity of data-driven approaches in case of propagation environment changes. To address this challenge, we propose a new data-driven solution for passive localization to address the limitations of model-based localization techniques.
The ongoing climate change is modifying the environmental conditions and species have to adapt to these new constraints, either on the same site or migrating in new suitable sites leading to a modification of distribution area. This repositioning has two main dimensions: (i) the species capacity to adapt to the new conditions (modification of life history traits) which is linked to the species resilience and (ii) the species capacity to explore new suitable habitats. The objective of this study was to build a mechanistic model incorporating these two dimensions in order to evaluate, understand and predict the repositioning possibilities of European diadromous fish facing climate change. In their life cycles, diadromous fish species have to use freshwater, estuarine and marine ecosystems. These specific life history strategies represent a great repositioning potential in comparison to freshwater fish species. A database of diadromous fish life history traits, incorporating those that could be influenced by climate change and those that could have an importance in the species repositioning potential, has been built. An Analytic Hierarchy Process has been suggested to develop a composite score based on life traits aiming at assessing the diadromous species repositioning potential. Then, the GR3D model (Global Repositioning Dynamics for Diadromous fish Distribution) has been developed in order to study with a dynamic approach the repositioning potential of diadromous fish, at a large scale, in a context of climate change. This model is a simulation, stochastic and individual-based model incorporating the main population dynamics processes of a diadromous fish (reproduction, mortality, growth, upstream migration with dispersal and downstream migration). A first exploratory application case simulating the repositioning of a virtual allis shad (Alosa alosa) population between two river catchments under a scenario of temperature increase has been carried out and the associated global sensitivity analysis has been performed in order to determine the influence of uncertain population dynamics parameters and of parameters defining the landscape stucture. The results showed that dispersal distance and parameters related to sea lifespan and to survival at sea were crucial to determine the success of colonization. Finally, the use of GR3D in a real application case allowed improving the understanding of allis shad persistence at the scale of its distribution area (i.e. the Atlantic coast) in a context of climate change. Over time, simulation results of GR3D should be relevant and useful in management and conservation of diadromous fish species.
In this thesis, we propose different model-checking techniques for pushdown system models. Pushdown systems (PDSs) are indeed known to be a natural model for sequential programs, as they feature an unbounded stack that can simulate the assembly stack of an actual program. Our first contribution consists in model-checking the logic HyperLTL that adds existential and universal quantifiers on path variables to LTL against pushdown systems (PDSs). The model-checking problem of HyperLTL has been shown to be decidable for finite state systems. We prove that this result does not hold for pushdown systems nor for the subclass of visibly pushdown systems. Therefore, we introduce approximation algorithms for the model-checking problem, and show how these can be used to check security policies. In the second part of this thesis, as pushdown systems can fail to accurately represent the way an assembly stack actually operates, we introduce pushdown systems with an upper stack (UPDSs), a model where symbols popped from the stack are not destroyed but instead remain just above its top, and may be overwritten by later push rules. We prove that the sets of successors post* and predecessors pre* of a regular set of configurations of such a system are not always regular, but that post* is context-sensitive, hence, we can decide whether a single configuration is forward reachable or not. Finally, in order to analyse multi-threaded programs, we introduce in this thesis a model called synchronized dynamic pushdown networks (SDPNs) that can be seen as a network of pushdown processes executing synchronized transitions, spawning new pushdown processes, and performing internal pushdown actions. We then apply this abstraction framework to a iterative abstraction refinement scheme.
The Arabic language is poor in electronic semantic resources. Among those resources there is Arabic WordNet which is also poor in words and relationships. This thesis focuses on enriching Arabic WordNet by synsets (a synset is a set of synonymous words) taken from a large general corpus. In order to validate the method, we had to create a gold standard corpus (there are none in Arabic for this area) from Arabic WordNet, and then compare the GraPaVec method with Word2Vec and Glove ones. The result shows that GraPaVec gives for this problem the best results with a F-measure 25 % higher than the others. The generated classes will be used to create new synsets to be included in Arabic WordNet.
This thesis studies the use of ontology and knowledge base for guiding various steps of the Knowledge Discovery in Databases (KDD) process in the domain of pharmacogenomics. This approach has been implemented using semantic Web technologies and leads finally to populating a pharmacogenomic knowledge base. As a result, data to analyze are represented in the knowledge base, which is a benefit for guiding following steps of the knowledge discovery process. Firstly, I study this benefit for feature selection by illustrating how the knowledge base can be used for this purpose. Secondly, I describe and apply to pharmacogenomics a new method named Role Assertion Analysis (or RAA) that enables knowledge discovery directly from knowledge bases. This method uses data mining algorithms over assertions of our pharmacogenomic knowledge base and results in the discovery of new and relevant knowledge.
With the rise of Web 2.0 and collaborative technologies that are attached to,the Web has now become a broad platform of exchanges between users. The majority of websites is now dedicated to social interactions of their users, or offers tools to develop these interactions. Our work focuses on the understanding of these exchanges, as well as emerging community structures arising, through a semantic approach. To meet the needs of web analysts, we analyze these community structures to identify their essential characteristics as their thematic centers and central contributors. Our semantic analysis is mainly based on reference light ontologies to define several new metrics such as the temporal semantic centrality and the semantic propagation probability. We employ an online approach to monitor user activity in real time in our community analysis tool WebTribe. We have implemented and tested our methods on real data from social communication systems on the Web
The present dissertation focuses on the acquisition of highly polysemous verbs by French as a second language learners. I am particularly interested in the polysemy of the verb prendre. This dissertation has two complementary objectives. On the one hand, the first objective is to describe the polysemy of prendre through a lexical semantic analysis within a cognitive framework. On the other hand, the second objective is to evaluate the impact of this polysemy on learners'knowledge of the verb's different senses in order to isolate those which are the most problematic for French L2 learners. Furthermore, I also investigate whether the problems associated with polysemy for Anglophone learners can be explained through cross-linguistic influence. Prior to carrying out the semantic analysis, a battery of syntactic tests was used in order to categorize the different uses of the verb prendre into the following categories: support verb constructions, idiomatic expressions and predicative uses. I use the cognitive semantic concept of the windowing of attention in order to explain how each of the parts of the core meaning can be windowed in order to obtain the different senses of the verb. Moreover, this analysis allowed me to formulated hypotheses about the differences between the verb prendre and its equivalents in English. In order to meet the second objective of this dissertation, I conducted an experiment on 191 learners of French as a second language. All participants completed three tasks: a cloze test to measure their proficiency level in French, a guided written production task and an acceptability judgement task. The results of the logistic and multinomial regression analyses show not only that the semantic analysis proposed can predict participants' knowledge of the different senses of the verb, but also that participants whose first language is English behave differently from those whose first language is another language, confirming the presence of cross-linguistic influence for the English-speaking participants.
Statistical physics, originally developed to describe thermodynamic systems, has been playing for the last decades a central role in modelling an incredibly large and heterogeneous set of different phenomena taking for instance place on social, economical or biological systems. Such a vast field of possible applications has been found also for networks, as a huge variety of systems can be described in terms of interconnected elements. After an introductory part introducing these themes as well as the role of abstract modelling in science, in this dissertation it will be discussed how a statistical physics approach can lead to new insights as regards three problems of interest in network theory: how some quantity can be optimally spread on a graph, how to explore it and how to reconstruct it from partial information. Some final remarks on the importance such themes will likely preserve in the coming years conclude the work.
This thesis offers a contrastive analysis of the notion of definiteness as conveyed by the system of the article in English and Standard Arabic. The corpus, The Brook Kerith, by the Irish writer, George Moore, is chosen for geo-historical and literary reasons: the story takes place in the Holy Land at the dawn of this Christian era. A contrastive analysis of the first chapter along with its translation is analyzed from a pragmatic and semantic perspective. The analysis is followed by statistical and computational analyses. It is found that the article “the” and the Arabic article “al'are used for seemingly the same purpose in the proportion of 76%. The occurrence of the article “a/an” is 96% consistent with indefiniteness in Arabic. However, the use of the “zero article” shows discrepancy as whether to use the article “al” or no article in Arabic. In the last analysis, the cognitive operations underlying usage in both languages are similar. The differences are on the level of the semiotic transformation of these deep operations.
This thesis addresses the subject of event detection in temporal signals for elderly monitoring by the use of a floor pressure sensor. We first show that most proposed systems do not meet main practical issues and that floor systems constitute promising candidates for monitoring tasks. Since complex signals require sophisticated models, we propose a random-forest-based approach that detects falls with state-of-the-art accuracy and meets hardware constraints with a feature selection procedure. The model performance is improved with data augmentation and time aggregation of the random forest outputs. Methods are tested on several data sets, showing interesting potential continuation, and a Python implementation is made available. Finally, motivated by the issue of elderly monitoring while dealing with one-dimensional signals for a large areas, we propose to distinguish elderly persons from younger individuals with a model based on convolutional neural network and convolutional dictionary learning. Since signals are mainly made of walks, the first part of the model is trained to recognize steps, and the last part of the model is trained with all previous layers frozen. This novel approach to gait classification allows to isolate elderly-generated signals with very high accuracy.
Recommender Systems aim at pre-selecting and presenting first the information in which users may be interested. This has raised the attention of the e-commerce, where the interests of users are analysed in order to predict future interests and to personalize the offers (a.k.a. items). Recommender systems exploit the current preferences of users and the features of items/users in order to predict their future preference in items. Although they demonstrate accuracy in many domains, these systems still face great challenges for both academia and industry: they require distributed techniques to deal with a huge volume of data, they aim to exploit very heterogeneous data, and they suffer from cold-start, situation in which the system has not (enough) information about (new) users/items to provide accurate recommendations. Among popular techniques, Matrix Factorization has demonstrated high accurate predictions and scalability to parallelize the analysis among multiple machines. However, it has two main drawbacks: (1) difficulty of integrating external heterogeneous data such as items'features, and (2) the cold-start issue. Our contributions to this area are given by four aspects: (1) we improve the distribution of a matrix factorization recommendation algorithm in order to achieve better scalability, (2) we enhance recommendations performed by matrix factorization by studying the implicit interest of the users in the attributes of the items, (3) we propose an accurate and low-space binary vector based on Bloom Filters for representing users/items through a high quantity of features in low memory-consumption, and (4) we cope with the new user cold-start in collaborative filtering by using active learning techniques. The experimentation phase uses the publicly available MovieLens dataset and IMDb database, what allows to perform fair comparisons to the state of the art. Our contributions demonstrate their performance in terms of accuracy and efficiency.
In this thesis, we present a methodology for interactive and iterative extracting knowledge from texts-the KESAM system: A tool for Knowledge Extraction and Semantic Annotation Management. KESAM is based on Formal Concept Analysis for extracting knowledge from textual resources that supports expert interaction. In the KESAM system, knowledge extraction and semantic annotation are unified into one single process to benefit both knowledge extraction and semantic annotation. Semantic annotations are used for formalizing the source of knowledge in texts and keeping the traceability between the knowledge model and the source of knowledge. The knowledge model is, in return, used for improving semantic annotations. The KESAM process has been designed to permanently preserve the link between the resources (texts and semantic annotations) and the knowledge model. The core of the process is Formal Concept Analysis that builds the knowledge model, i.e. the concept lattice, and ensures the link between the knowledge model and annotations. In order to get the resulting lattice as close as possible to domain experts' requirements, we introduce an iterative process that enables expert interaction on the lattice. Experts are invited to evaluate and refine the lattice; they can make changes in the lattice until they reach an agreement between the model and their own knowledge or application's need. Thanks to the link between the knowledge model and semantic annotations, the knowledge model and semantic annotations can co-evolve in order to improve their quality with respect to domain experts' requirements. Moreover, by using FCA to build concepts with definitions of sets of objects and sets of attributes, the KESAM system is able to take into account both atomic and defined concepts, i.e. concepts that are defined by a set of attributes. In order to bridge the possible gap between the representation model based on a concept lattice and the representation model of a domain expert, we then introduce a formal method for integrating expert knowledge into concept lattices in such a way that we can maintain the lattice structure. The expert knowledge is encoded as a set of attribute dependencies which is aligned with the set of implications provided by the concept lattice, leading to modifications in the original lattice. The method also allows the experts to keep a trace of changes occurring in the original lattice and the final constrained version, and to access how concepts in practice are related to concepts automatically issued from data. The method uses extensional projections to build the constrained lattices without changing the original data and provide the trace of changes. From an original lattice, two different projections produce two different constrained lattices, and thus, the gap between the representation model based on a concept lattice and the representation model of a domain expert is filled with projections.
The vast majority of skin cancer deaths are due to malignant melanoma. It is considered as one of the most dangerous cancers. In its early stages, malignant melanoma is completely curable with a simple biopsy. Therefore, an early detection is the best solution to improve skin cancer prognostic. Medical imaging such as dermoscopy and standard camera images are the most suitable tools available to diagnose melanoma at early stages. To help radiologists in the diagnosis of melanoma cases, there is a strong need to develop computer aided diagnosis (CAD) systems. The accurate segmentation and classification of pigment skin lesions still remains a challenging task due to the various colors and structures developed randomly inside the lesions. The model is adapted to segment the variations of the pigment inside the lesion and not only the main border. The method has been implemented on DermIs and DermQues, two databases of standard camera images, and achieved an accuracy of 86.54% with a sensitivity of 80% and a specificity of 95.45%. The method has been implemented on the PH2 database for dermoscopy images with 1000-random sampling cross validation.
Metric distance learning is a branch of re-presentation learning in machine learning algorithms. We summarize the development and current situation of the current metric distance learning algorithm from the aspects of the flat database and nonflat database. For a series of algorithms based on Mahalanobis distance for the flat database that fails to make full use of the intersection of three or more dimensions, we propose a metric learning algorithm based on the submodular function. For the lack of metric learning algorithms for relational databases in non-flat databases, we propose LSCS(Relational Link-strength Constraints Selection) for selecting constraints for metric learning algorithms with side information and MRML (Multi-Relation Metric Learning) which sums the loss from relationship constraints and label constraints. Through the design experiments and verification on the real database, the proposed algorithms are better than the current algorithms.
The issue at stake is the automated meaning allocation. In a first time, a theoretical scheme is elaborated to describe meaning change for a lexical unit already defined in a lexical resource. Our model relies on quantitative evidence and it is inspired from text semantics. The preexisting meaning is represented as a structured set of semantic features. The context modifies it dueto salient semantic featuresin texts. These dynamic change is comprehended through description strata ranging from coarse-grained to fine-grained semantic units. In a second time, we dwell on relevant resources and tools from corpus linguistics. Concretely, we use the Trésor de la Langue Française informatisé as a dictionary. Its entries are automatically converted into bags of semantic features. The textual dataconsists in three recent journalistic corpus. The resources are considered are mathematic spaces and statistical tools are used to extract significant units and to structure information. In a last time, we give an outline of a process to allocate automatically a new meaning. Experiments illustrate each step. Through this approach, it is possible to qualify the new meaning in a precise and structured way.
The use of multiple choice questions to assess knowledge is a reliable and widely used method, even in official contexts. Such a method offers many advantages, including equality of marking between candidates, or, more pragmatically, the possibility of automatic correction. With the emergence of MOOCs (courses delivered in a digital format), the need for automatic evaluation has increased. The scope of this thesis is part of this context, by proposing a solution that enables automatic thematic question generation. The work presented in this thesis uses knowledge bases as data sources to automatically generate thematic multiple-choice questions. This thesis presents a method based on Wikipedia metadata to identify and sort knowledge base entities according to predefined themes.- In order to be intelligible, a question must be grammatically correct, and must include enough information to remove any ambiguity about the correct answer. In a last contribution, we present the method used to select distractors that are not only relevant to the question's statement, but also to its context.
Script is a structure describes an appropriate sequence of events or actions in our daily life. A story, is invoked a script with one or more interesting deviations, which allows us to deeper understand about what were happened in routine behaviour of our daily life. Therefore, it is essential in many ambient intelligence applications such as healthmonitoring and emergency services. Hence, human activity recognition (HAR) has become a hot topic interest of research over the past decades. In order to do HAR, most researches used machine learning approaches such as Neural network, Bayesian network, etc. Therefore, the ultimate goal of our thesis is to generate such kind of stories or scripts from activity data of wearable sensors using machine learning approach. However, to best of our knowledge, it is not a trivial task due to very limitation of information of wearable sensors activity data. Hence, there is still no approach to generate script/story using machine learning, even though many machine learning approaches were proposed for HAR in recent years (e.g., convolutional neural network, deep neural network, etc.) to enhance the activity recognition accuracy. Secondly, we introduce a novel scheme to automatically generate scripts from wearable sensor human activity data using deep learning models, and evaluate the generated method performance. Finally, we proposed a neural event embedding approach that is able to benefit from semantic and syntactic information about the textual context of events. The approach is able to learn the stereotypical order of events from sets of narrative describing typical situations of everyday life.
The framework of this thesis is speaker-independent automatic speech recognition. This document begins with a short survey of speech signal processing applied to speech recognition. Then, we describe several common architectures: dynamic time warping of acoustic shapes, artificial intelligence, neural networks and hidden Markov models. This document is made of two main parts. The first part is devoted to the recognition of words. We are using finite state automata for modeling the eleven American spoken digits. As the speech database TiDigits has been used by other teams we can compare our results against thoose obtained with other approaches. The first step is concerned with isolated word recognition. Then, we describe how sentences of the database have been segmented. Last sections of this part describe continuous speech recognition of word sequences, as well as a discussion about strong and weak points of our approach. The second part treats of the use of production models for speech recognition. We begin with a description of the acoustic equations which drive the air flow inside the vocal tract and we present several articulatory models. Then, we justify the choice of Maeda's model. We describe the adaptation of this model to a male speaker. Next, we describe the variational method used for recovering articulatory trajectories from the speech. Finally, the software we built, is described. In the conclusion, we give the results obtained and we exhibit sorne perspectives for future work towards a better speaker indepedent continuous speech recognition system.
Nowadays, remotely sensed images constitute a rich source of information that can be leveraged to support several applications including risk prevention, land use planning, land cover classification and many other several tasks. In this thesis, Satellite Image Time Series (SITS) are analysed to depict the dynamic of natural and semi-natural habitats. We introduce an object-oriented method to analyse SITS that consider segmented satellites images. Firstly, we identify the evolution profiles of the objects in the time series. Then, we analyse these profiles using machine learning methods. To analyse these evolution graphs, we introduced three contributions. The first contribution explores annual SITS. It analyses the evolution graphs using clustering algorithms, to identify similar evolutions among the spatio-temporal entities. We consider several study areas described by multi-annual SITS. In the third contribution, we introduce à semi-supervised method based on constrained clustering. We propose a method to select the constraints that will be used to guide the clustering and adapt the results to the user needs. Our contributions were evaluated on several study areas. The experimental results allow to pinpoint relevant landscape evolutions in each study sites. We also identify the common evolutions among the diﬀerent sites. In addition, the constraint selection method proposed in the constrained clustering allows to identify relevant entities. Thus, the results obtained using the unsupervised learning were improved and adapted to meet the user needs.
Pharmacovigilance suffers from chronic underreporting of drug's adverse effects from health professional's part. The FDA (US Food and Drug Administration), The EMA (European Medicines Agency), and other health agencies, suggest that social media could constitute an additional data source for detection of weak pharmacovigilance signals. The WHO (World Health Organization) published a report in 2003 outlining the problem of non-compliance with treatment over long term and its prejudicial effectiveness on health systems worldwide. The necessary data for development of an information extraction system from patient's forums are made available by the company Kappa Sante. The first proposed approach fits into a context of pharmacovigilance case detection from patient's online discussions on health forums. We propose a filter based on the number of words separating the name of the mentioned drug in the message from the term considered as a potential adverse effect. We propose a second approach based on topic models to target groups of messages addressing topics dealing with non-compliance. In terms of pharmacovigilance, the proposed Gaussian filter identifies 50.03% of false positives with a precision of 95.8% and a recall of 50%. The case detection approach of non-compliance allows the identification of messages describing this kind of behaviors with a precision of 32.6% and a recall of 98.5%.
Behavioral Strategy suggests that an explanation may be found in the psychology of decision makers, and particularly in their cognitive biases. This, however, calls for a link between individual-level cognition and affects, and organization-level choices. We propose “Strategic Choice Routines” as a middle level of analysis to bridge this gap, and identify three broad types of Strategic Choice Routines. This leads us to formulate hypotheses on how Strategic Choice Routines can be modified to minimize strategic errors. We illustrate these hypotheses through case studies; test some of them quantitatively; and analyze preferences that drive their adoption by executives. Finally, we discuss theoretical and managerial implications.
Recent developments in information technologies have made the web an important data source. However, the web content is very unstructured. Therefore, it is a difficult task to automatically process this web content in order to extract relevant information. This is a reason why research work related to Information Extraction (IE) on the web are growing very quickly. Our research work is at the crossroads of both areas. The main goal of our work is to develop strategies and techniques for crawling the web in order to extract complex Named Entities (NEs) (NEs with several properties that may be text or other NEs). We then propose to index them and to query them in order to answer information needs. This work was carried out within the T2I team of the LIUPPA laboratory, in collaboration with Cogniteev, a company which core business is focused on the analysis of web content. The issues we had to deal with were the extraction of complex NEs on the web and the development of IR services supplied by the extracted data. Our first contribution is related to complex NEs extraction from text content. For this contribution, we take into consideration several problems, in particular the noisy context characterizing some properties (the web page describing an event for example, may contain more than one dates: the event's date and the date of ticket's sales opening). For this particular problem, we introduce a block detection module that focuses property's extraction on relevant text blocks. Our experiments show an improvement of system's performances. We also focused on address extraction where the main issue arises from the fact that there is not a standard way for writing addresses in general and on the web in particular. We therefore propose a pattern-based approach which uses some lexicons for extracting addresses from text, regardless of proprietary resources. Our second contribution deals with similarity computation between complex NEs. In the state of the art, this similarity computation is generally performed in two steps: (i) first, similarities between properties are calculated; (ii) then the obtained similarities are aggregated to compute the overall similarity. Our main proposals focuses on the second step. We propose three techniques for aggregating property's similarities. The first two are based on the weighted sum of these property's similarities (simple linear combination and logistic regression). The third technique however, uses decision trees for the aggregation. We also propose a similarity computation function between spatial EN, one represented by a point and the other by a polygon.
The approach adopted to build COATIS system aims at finding an operational method for constructing representations from a text, taking into account the causality notion.
Underwater robots can nowadays operate in complex environments in a broad scope of missions where the use of human divers is difficult for cost or safety reasons. However the complexity of aquatic environments requires to give the robotic vector an autonomy sufficient to perform its mission while preserving its integrity. This requires to design control laws according to application requirements. They are built on knowledge from several scientific fields, underlining the interdisciplinarity inherent to robotics. Once the control law designed, it must be implemented as a control Software working on a real-time Software architecture. Nonetheless the current conception of control laws, as "monolithic" blocks, makes difficult the adaptation of a control from an application to another and the integration of knowledge from various scientific fields which are often not fully understood by control engineers. Moreover this will allow us a more efficient projection on the Software architecture. We thus propose a new formalism for control laws description as a modular composition of basic entities named Atoms used to encapsulate the knowledge items. We propose as well a methodology relying on our formalism to guide the implementation of control on a real-time Middleware. We will focus on the ContrACT Middleware developed at LIRMM.Finally we illustrate our approach on several robotic functionalities that can be used during aquatic environments exploration and especially for wall avoidance during the exploration of a karst aquifer.
At the time of the beginning of this work presented in this document (2003), genome annotation was a long and tedious task. With the advent of new sequencing technologies, many tools have been developed to facilitate and accelerate this process. At best, the annotation of an automatic genome can take less than 3 minutes, making manual annotation the more time consuming activity. Thus, many genomes are deposited into sequence banks without expert manual annotation. It quickly became clear that annotators needed to be provided with the possibility of accessing consolidated databases specific to their field of expertise. This paper presents a modular annotation and visualization tool, GenoBrowser, which we created as part of the research in our microbiology team. This allows us to easily integrate new functionalities related to Omics data generated in the team. The architecture of our tool and the creation of a specific API (Application Programming Interface) enabled us to develop and make available to the scientific community two databases (P2CS and P2TF) dedicated to regulation networks in bacteria, as well as the associated web server for prediction of these systems for genomes sequenced de novo. This work has led to the development of a set of tools within a research team to support expertise in environmental genomics research. It allowed us to work on the consolidation and reuse of the growing amount of Omics data and to carry out a new research theme to help team members: bibliomics, the study of all scientific publications using NLPs (Natural Language Processing) approaches.
Application of spoken language understanding aim to extract relevant items of meaning from spoken signal. There is two distinct types of spoken language understanding: understanding of human/human dialogue and understanding in human/machine dialogue. Given a type of conversation, the structure of dialogues and the goal of the understanding process varies. However, in both cases, most of the time, automatic systems have a step of speech recognition to generate the textual transcript of the spoken signal. Speech recognition systems in adverse conditions, even the most advanced one, produce erroneous or partly erroneous transcript of speech. Those errors can be explained by the presence of information of various natures and functions such as speaker and ambience specificities. They can have an important adverse impact on the performance of the understanding process. The first part of the contribution in this thesis shows that using deep autoencoders produce a more abstract latent representation of the transcript. This latent representation allow spoken language understanding system to be more robust to automatic transcription mistakes. In the other part, we propose two different approaches to generate more robust representation by combining multiple views of a given dialogue in order to improve the results of the spoken language understanding system. The second one introduce new autoencoders architectures that use supervision in the denoising autoencoders.
By means of a qualitative and inductive method, this study acknowledges a special interest in ordinary speakers' metalinguistic evaluative statements and attempts to determine the different meanings and functions attached to those evaluating terms.
Machine learning (ML) algorithms are designed to learn models that have the ability to take decisions or make predictions from data, in a large panel of tasks. However, when data distribution is complex (e.g. high-dimensional with nonlinear interactions between observed variables), the simplifying assumptions can be counterproductive. In this situation, a solution is to feed the model with an alternative representation of the data. Recently, a branch of ML called deep learning (DL) completely shifted the paradigm. In this thesis, we are interested in learning representations of multivariate time series (MTS) and graphs. MTS and graphs are particular objects that do not directly match standard requirements of ML algorithms. They can have variable size and non-trivial alignment, such that comparing two MTS or two graphs with standard metrics is generally not relevant. Hence, particular representations are required for their analysis using ML approaches. The contributions of this thesis consist of practical and theoretical results presenting new MTS and graphs representation learning frameworks. Two MTS representation learning frameworks are dedicated to the ageing detection of mechanical systems. First, we propose a model-based MTS representation learning framework called Sequence-to-graph (Seq2Graph). We show that using this method, under few assumptions, we identify the true state underlying the studied mechanical system, up-to monotone scalar transform. Two graph representation learning frameworks are dedicated to the classification of graphs. We show that this feature matches minimal requirements to classify graphs when all the meaningful information is contained in the structure of the graphs.
This study is mainly concerned with the description of the portuguese noun phrase, in a scientific and technical context provided by medical texts. Whilst being traditionally linguistic in nature, the analysis also means to bring to light a certain number of linguistic resources which may ultimately serve in langage processing activities, and in particular those of document processing a machine-aided translation.
Sign Languages (SLs) have developed naturally in Deaf communities. With no written form, they are oral languages, using the gestural channel for expression and the visual channel for reception. These poorly endowed languages do not meet with a broad consensus at the linguistic level. These languages make use of lexical signs, i.e. conventionalized units of language whose form is supposed to be arbitrary, but also - and unlike vocal languages, if we don't take into account the co-verbal gestures - iconic structures, using space to organize discourse. Other corpora consist of interpreted SL, which may also differ significantly from natural SL, as it is strongly influenced by the surrounding vocal language. In this thesis, we wish to show the limits of this approach, by broadening this perspective to consider the recognition of elements used for the construction of discourse or within illustrative structures. We then propose the redesign of a French Sign Language dialogue corpus, Dicta-Sign-LSF-v2, with rich and consistent annotations, following an annotation scheme shared by many linguists. We then propose a redefinition of the problem of automatic SLR, consisting in the recognition of various linguistic descriptors, rather than focusing on lexical signs only. At the same time, we discuss adapted metrics for relevant performance assessment. In order to perform a first experiment on the recognition of linguistic descriptors that are not only lexical, we then develop a compact and generalizable representation of signers in videos. This is done by parallel processing of the hands, face and upper body, using existing tools and models that we have set up. Besides, we preprocess these parallel representations to obtain a relevant feature vector. We then present an adapted and modular architecture for automatic learning of linguistic descriptors, consisting of a recurrent and convolutional neural network. Finally, we show through a quantitative and qualitative analysis the effectiveness of the proposed model, tested on Dicta-Sign-LSF-v2. The study of the model predictions then demonstrates the merits of the proposed approach, with a very interesting performance for the continuous recognition of four linguistic descriptors, especially in view of the uncertainty related to the annotations themselves. The segmentation of the latter is indeed subjective, and the very relevance of the categories used is not strongly demonstrated. Indirectly, the proposed model could therefore make it possible to measure the validity of these categories. With several areas for improvement being considered, particularly in terms of signer representation and the use of larger corpora, the results are very encouraging and pave the way for a wider understanding of continuous Sign Language Recognition.
This thesis focuses on SMS language and information extraction from the point of view of natural language processing. The starting point of our study is the observation of the differences that most short messages have, using the alpes4science corpora, in comparison with the standard language. The differences are highlighted by the particular morphology of words and by the syntactic and grammar rules that are not respected when the issuer considers that it would not impair the intelligibility of the message. Because of the deviations from the standard language, processing and analyzing noisy messages is still a challenge for any NLP task. The obtained result from this model was evaluated, afterwards, for named entities recognition through a series of tests applied thanks to three other systems. The results have shown that these performances of named entity recognition systems are significantly improved when applied to automatically normalized SMS in comparison with raw and manually normalized corpora. Keywords: computer-mediated communication, SMS language, SMS normalization
This thesis work focuses on audio-visual detection of emotional (laugh and smile) and attentional markers for elderly people in social interaction with a robot. To effectively understand and model the pattern of behavior of very old people in the presence of a robot, relevant data are needed. I participated in the collection of a corpus of elderly people in particular for recording visual data. The system used to control the robot is a Wizard of Oz, several daily conversation scenarios were used to encourage people to interact with the robot. These scenarios were developed as part of the ROMEO2 project with the Approche association. We described at first the corpus collected which contains 27 subjects of 85 years'old on average for a total of 9 hours, annotations and we discussed the results obtained from the analysis of annotations and two questionnaires. My research then focuses on the attention detection and the laughter and smile detection. The motivations for the attention detection are to detect when the subject is not addressing to the robot and adjust the robot's behavior to the situation. After considering the difficulties related to the elderly people and the analytical results obtained by the study of the corpus annotations, we focus on the rotation of the head at the visual index and energy and quality vote for the detection of the speech recipient. The laughter and smile detection can be used to study on the profile of the speaker and her emotions. My interests focus on laughter and smile detection in the visual modality and the fusion of audio-visual information to improve the performance of the automatic system. Spontaneous expressions are different from posed or acted expression in both appearance and timing. Designing a system that works on realistic data of the elderly is even more difficult because of several difficulties to consider such as the lack data for training the statistical model, the influence of the facial texture and the smiling pattern for visual detection, the influence of voice quality for auditory detection, the variety of reaction time, the level of listening comprehension, loss of sight for elderly people, etc. The systems of head-turning detection, attention detection and laughter and smile detection are evaluated on ROMEO2 corpus and partially evaluated (visual detections) on standard corpus Pointing04 and GENKI-4K to compare with the scores of the methods on the state of the art. We also found a negative correlation between laughter and smile detection performance and the number of laughter and smile events for the visual detection system and the audio-visual system. This phenomenon can be explained by the fact that elderly people who are more interested in experimentation laugh more often and therefore perform more various poses. The variety of poses and the lack of corresponding data bring difficulties for the laughter and smile recognition for our statistical systems. The experiments show that the head-turning can be effectively used to detect the loss of the subject's attention in the interaction with the robot. For the attention detection, the potential of a cascade method using both methods in a complementary manner is shown.
Source camera identification has recently received a wide attention due to its important role in security and legal issue. The problem of establishing the origin of digital media obtained through an imaging device is important whenever digital content is presented and is used as evidence in the court. Source camera identification is the process of determining which camera device or model has been used to capture an image. Our first contribution for digital camera model identification is based on the extraction of three sets of features in a machine learning scheme. These features are the co-occurrences matrix, some features related to CFA interpolation arrangement,and conditional probability statistics computed in the JPEG domain. These features give high order statistics which supplement and enhance the identification rate. The experiments prove the strength of our proposition since it achieves higher accuracy than the correlation-based method. The second contribution is based on using the deep convolutional neural networks(CNNs). Unlike traditional methods, CNNs can automatically and simultaneously extract features and learn to classify during the learning process. A layer of preprocessing is added to the CNN model, and consists of a high pass filter which is applied to the input image. The obtained CNN gives very good performance for a very small learning complexity. Experimental comparison with a classical two steps machine learning approach shows that the proposed method can achieve significant detection performance. The well known object recognition CNN models, AlexNet and GoogleNet, are also examined.
Any buzz has one or more starting point(s), i.e. primary source(s). The information is then passed on by secondary sources which may speed or slow down its spreading depending on their influence. Throughout the buzz lifecycle, the semantic content can evolve. To understand a buzz on the Internet, one needs to analyze what is said and qualify who speaks. This thesis will focus on two main points: a topological analysis of the sources (graph theory and networks), and an analysis of the textual content (corpus linguistics).
Clustering is the task of finding a partition of items, such that items in the same cluster are more similar than items in different clusters. One challenge consists in designing a system capable of taking benefit of the different sources of data. Objects can also be described by a graph which captures the relationships objects have with each others. In addition to this, some constraints can be known about the data. It can be known that an object is of a certain type or that two objects share the same type or are of different types. It can also be known that on a global scale, the different types of objects appear with a known frequency. In this thesis, we focus on clustering with three different types of constraints: label constraints, pairwise constraints and power-law constraint. A label constraint specifies in which cluster an object belong. Pairwise constraints specify that pairs of object should or should not share the same cluster. Finally, the power-law constraint is a cluster-level constraint that specifies that the distribution of cluster sizes are subject to a power-law. We want to show that introducing semi-supervision to clustering algorithms can alter and improve the solutions returned by unsupervised clustering algorithms. We contribute to this question by proposing algorithms for each type of constraints. Our experiments on UCI data sets and natural language processing data sets show the good performance of our algorithms and give hints towards promising future works.
The different dialects of the arabic language have a large phonological, morphological, lexical and syntactic variations when compared to the standard written arabic language called MSA (Modern Standard Arabic). Until recently, these dialects were presented only in their oral form and most of the existing resources for the Arabic language is limited to the Standard Arabic (MSA), leading to an abundance of tools for the automatic processing of this variety. Given the significant differences between the MSA and DA, the performance of these tools fall down when processing AD. The presence of the latter in a disorderly manner in the discourse poses a serious problem for NLP (Natural Language Processing) and makes this oral a less resourced language. However, the resources required to model this oral are almost nonexistent. Thus, the objective of this thesis is to fill this gap in order to build a language model dedicated to an automatic recognition system for the oral spoken in the Tunisian media. For this reason, we describe in this thesis a resource generation methodologyand we evaluate it relative to a language modeling task. The results obtained are encouraging.
This work belongs to the field of performative control of voice synthesis, and more precisely of real-time modification of pre-recorded voice signals. In a context where such systems were only capable of modifying parameters such as pitch, duration and voice quality, our work was carried around the question of performative modification of voice rhythm. One significant part of this thesis has been devoted to the development of Vokinesis, a program for performative modification of pre-recorded voice. It has been developed under 4 goals: to allow for voice rhythm control, to obtain a modular system, usable in public performances situations as well as for research applications. To achieve this development, a reflexion about the nature of voice rhythm and how it should be controlled has been carried out. It appeared that the basic inter-linguistic rhtyhmic unit is syllable-sized, but that syllabification rules are too language-dependant to provide a invariant inter-linguistic rhythmic pattern. We showed that accurate and expressive sequencing of vocal rhythm is performed by controlling the timing of two phases, which together form a rhythmic group: the rhythmic nucleus and the rhythmic link. We developed several rhythm control methods, tested with several control interfaces. An objective evaluation showed that one of our methods allows for very accurate control of rhythm. New strategies for voice pitch and quality control with a graphic tablet have been established. The use of Vokinesis as a musical instrument has been successfully evaluated in public representations of the Chorus Digitalis ensemble, for various singing styles (from pop to contemporary music). Application perspectives are diverse: scientific studies (research in prosody, expressive speech, neurosciences), sound and music production, language learning and teaching, speech therapies.
The present research takes issue with the supposed dichotomy between alternations on the one hand and surface generalisations on the other, within the framework of construction grammar. More specifically the aim of this thesis is threefold. Through the careful analysis of a large dataset, we aim to provide a thorough description of the causative alternation in English (The fabric stretched vs. Joan stretched the fabric), suggest a method that allows for a solid measure ofa verb's alternation strength and of the amount of shared meaning between two constructions,and finally, show that in order to capture constraints at the level of the construction, one must pay attention to lower level generalisations such as the interaction between verb and arguments within the scope of each construction. In an effort to add to the discussion on alternation vs. surface generalisations, we propose a detailed study of the two constructions that make up the causative alternation: the intransitive non-transitive causative construction and the transitive causative construction. Our goal is to measure the amount of meaning shared by the two constructions and also show the differences between the two. In order to do so we take three elements into account: construction, verb and theme (i.e. the entity that undergoes the event denoted by the verb). We use distributional semantics to measure the semantic similarity of the various themes found with each verb and each construction in our corpus. This grouping highlights the different verb senses used with each construction and allows us to draw generalisations as to the constraints on the theme in each construction.
The amount of available scientific literature is constantly growing. If the experts of a domain want to easily access this information, it must be extracted and structured. To obtain structured data, both entities and relations of the texts must be detected. Our research is about the problem of complex relation extraction which represent experimental results, and detection and classification of binary relations between biomedical entities. We are interested in experimental results presented in scientific papers. These results are important for biology experts, for example for doing modelization. In the domain of renal physiology, a database was created to centralize these experimental results, but the base is manually populated, therefore the population takes a long time. We propose a solution to automatically extract relevant knowledge for the database from the scientific papers, that is experimental results which are represented by a n-ary relation. The method proceeds in two steps: automatic extraction from documents and proposal of information extracted for approval or modification by the experts via an interface. We also proposed a method based on machine learning for extraction and classification of binary relations in specialized domains. We focused on the variations of the expression of relations, and how to represent them in a machine learning system. We studied the way to take into account syntactic structure of the sentence and the sentence simplification guided by the task of relation extraction. In particular, we developed a simplification method based on machine learning, which uses a series of classifiers.
For a decade now, convolutional deep neural networks have demonstrated their ability to produce excellent results for computer vision. For this, these models transform the input image into a series of latent representations. In this thesis, we work on improving the "quality''of the latent representations of ConvNets for different tasks. Then, we propose to structure the information in two complementary latent spaces, solving a conflict between the invariance of the representations and the reconstruction task. This structure allows to release the constraint posed by classical architecture, allowing to obtain better results in the context of semi-supervised learning. Finally, we address the problem of disentangling, i.e. explicitly separating and representing independent factors of variation of the dataset. We pursue our work on structuring the latent spaces and use adversarial costs to ensure an effective separation of the information. This allows to improve the quality of the representations and allows semantic image editing.
Seismic probabilistic risk assessment (SPRA) is one of the most widely used methodologies to assess and to ensure the performance of critical infrastructures, such as nuclear power plants (NPPs), faced with earthquake events. The thesis provides discussions on the following aspects: (i) Construction of meta-models with ANNs to build the relations between seismic IMs and engineering demand parameters of the structures, for the purpose of accelerating the fragility analysis. The uncertainty related to the substitution of FEMs models by ANNs is investigated. (ii) Proposal of a Bayesian-based framework with adaptive ANNs, to take into account different sourcesof information, including numerical simulation results, reference values provided in the literature and damage data obtained from post-earthquake observations, in the fragility analysis. (iii) Computation of GMPEs with ANNs. The epistemic uncertainties of the GMPE input parameters, such as the magnitude and the averaged thirty-meter shear wave velocity, are taken into account in the developed methodology. (iv) Calculation of the annual failure rate by combining results from the fragility and hazard analyses. The fragility curves are determined by the adaptive ANN, whereas the hazard curves are obtained from the GMPEs calibrated with ANNs. The proposed methodologies are applied to various industrial case studies, such as the KARISMA benchmark and the SMART model.
The relations between sets of paraphrases can be described as series of textual transformations. To rephrase, an initial lexical substitution starts, then triggers other syntactic, lexical and morphological changes. After having described the frequent paraphrasing mechanisms in our corpus, we propose two formalisations. The first one is theoretical, explaining the different paraphrasing relationships maintained by the paraphrases between each other. The second formalises paraphrase structures as predicate-argument ones. We consider the latter suitable for paraphrase processing. Finally we have implemented a paraphrase structures extraction system. This is a compact operational system for the volume of data within our domain, the aim of which is to provide a concrete example of a possible use of our formalisation.
For the past two decades, electronic devices have revolutionized the traceability of social phenomena. Social dynamics now leave numerical footprints, which can be analyzed to better understand collective behaviors. The development of large online social networks (like Facebook, Twitter and more generally mobile communications) and connected physical structures (like transportation networks and geolocalised social platforms) resulted in the emergence of large longitudinal datasets. These new datasets bring the opportunity to develop new methods to analyze temporal dynamics in and of these systems. Nowadays, the plurality of data available requires to adapt and combine a plurality of existing methods in order to enlarge the global vision that one has on such complex systems. The purpose of this thesis is to explore the dynamics of social systems using three sets of tools: network science, statistical physics modeling and machine learning. The third chapter shows the added value of using longitudinal data. We study the behavior evolution of bike sharing system users and analyze the results of an unsupervised machine learning model aiming to classify users based on their profiles. The fourth chapter explores the differences between global and local methods for temporal community detection using scientometric networks. The last chapter merges complex network analysis and supervised machine learning in order to describe and predict the impact of new businesses on already established ones. We explore the temporal evolution of this impact and show the benefit of combining networks topology measures with machine learning algorithms.
The aim of this thesis is to build a piece of French grammar explaining the use of the infinitive syntax within the framework of polychrome tree grammars. The thesis comprises two parts. The first part studies problems associated with the infinitive. The infinitive, like a verb, has complements and the component it forms with its complements can in turn be the complement to a verb, a noun or an adjective. Traditional grammars express this as the double nature of an infinitive: it has the properties of a verb and of a noun/ it has both verbal and nominative properties. It is therefore difficult to insert the infinitive syntax into a formal model allowing the use of computational linguistics. We define the infinitive component as the unit comprising an infinitive verb and its complements. We explain why this term is preferable to “infinitive subordinated proposition”. The second part examines the syntactic analysis of infinitive components within the framework of polychrome tree grammars and is organized around the contexts in which the infinitive is used (a verb, a noun, an adjective). A polychrome tree grammar representation makes it possible to test this formalism and demonstrates the benefits of separating the syntactic function of each category. One can thus explain the cases in which the component of a non-nominal category performs a role generally performed by a noun. Finally, this work on the syntax of the infinitive, which revisits certain studies from traditional grammars, enriches the formalism constructed within the framework of computational linguistics.
The present thesis aims to develop an efficient strategy for impact detection and classification in the presence of modeling uncertainties of the robot and its environment and using a minimum number of sensors, in particular in the absence of force/torque sensor. The first part of the thesis deals with the detection of an impact that can occur at any location along the robot arm and at any moment during the robot trajectory. Impact detection methods are commonly based on a dynamic model of the system, making them subject to the trade-off between sensitivity of detection and robustness to modeling uncertainties. In this respect, a quantitative methodology has first been developed to make explicit the contribution of the errors induced by model uncertainties. This methodology has been applied to various detection strategies, based either on a direct estimate of the external torque or using disturbance observers, in the perfectly rigid case or in the elastic-joint case. A comparison of the type and structure of the errors involved and their consequences on the impact detection has been deduced. In a second step, novel impact detection strategies have been designed: the dynamic effects of the impacts are isolated by determining the maximal error range due to modeling uncertainties using a stochastic approach. Once the impact has been detected and in order to trigger the most appropriate post-impact robot reaction, the second part of the thesis focuses on the classification step. In particular, the distinction between an intentional contact (the human operator intentionally interacts with the robot, for example to reconfigure the task) and an undesired contact (a human subject accidentally runs into the robot), as well as the localization of the contact on the robot, is investigated using supervised learning techniques and more specifically feedforward neural networks. The challenge of generalizing to several human subjects and robot trajectories has been investigated.
This PhD dissertation is dedicated to the issue of 'exclusivist' growth and vulnerability characterizing Vietnam as well as developing Asia today. The chapters address three important aspects of vulnerability and inclusive development, namely: Informality (chapter 1), Education dilemma (chapter 2) and Non-standard employment (chapter 3). Chapter 2 focuses on the variation of the returns to higher education across the Vietnamese population with different estimation models. Chapter 3 is the first study that systematically examines the wage differentials induced by temporary job status in Asian developing countries. Overall, the whole thesis implies that human capital, employment, and income are interrelated facets of individual well-being, and that some development phenomena should be analyzed in their heterogeneity.
The protection of the natural environment is a key issue for the future of humanity. SSE, which shares the principles of sustainable development, is particularly well suited to implement more environmentally friendly development alternatives. The thesis looks at SSE organisations from the perspective of organisational identity and focuses on environmental communication on the one hand, and concrete actions on the other. The study of environmental communication is based on the social network Twitter. It is based on a program coded in Python, and on automatic text mining techniques. It highlights several rhetorical strategies. A second study deals with seven cases, based on semi-directive interviews. It sheds light on the role of individual commitment but also on collective logic in environmental action. This work makes a methodological contribution by developing the approach of automatic text mining, which is rarely used in Management Sciences. On the theoretical level, the thesis introduces the collective dimension as anborganisational identity of the SSE. We then adapt an environmental action model by identifying an additional determinant specific to these organizations. Finally, the research invites the SSE to put ecological issues back at the centre, and gives suggestions for supporting organisations in their efforts to protect the environment.
However, the majority of this research has focused on static datasets, and the analysis and visualisation tasks tend to be carried out by trained expert users. In more recent years, social changes and technological advances have meant that data have become more and more dynamic, and are consumed by a wider audience. Examples of such dynamic data streams include e-mails, status updates, RSS 1 feeds, versioning systems, social networks and others. These new types of data are used by populations that are not specifically trained in information visualization. Some of these people might consist of casual users, while others might consist of people deeply involved with the data, but in both cases, they would not have received formal training in information visualization. For simplicity, throughout this dissertation, I refer to the people (casual users, novices, data experts) who have not been trained in information visualisation as non-experts. These social and technological changes have given rise to multiple challenges because most existing visualisation models and techniques are intended for experts, and assume static datasets. Few studies have been conducted that explore these challenges. In this dissertation, with my collaborators, I address the question: Can we empower non-experts in their use of visualisation by enabling them to contribute to data stream analysis as well as to create their own visualizations? The first step to answering this question is to determine whether people who are not trained in information visualisation and the data sciences can conduct useful dynamic analysis tasks using a visualisation system that is adapted to support their tasks. In the first part of this dissertation I focus on several scenarios and systems where different sized crowds of InfoVis non-experts users (20 to 300 and 2 000 to 700 000 people) use dynamic information visualisation to analyse dynamic data. Another important issue is the lack of generic design principles for the visual encoding of dynamic visualization. In this dissertation I design, define and explore a design space to represent dynamic data for non-experts. This design space is structured by visual tokens representing data items that provide the constructive material for the assembly over time of different visualizations, from classic represen-tations to new ones. This paradigm is inspired by well-established developmental psychological theory as well as past and existing practices of visualisation authoring with tangible elements. I describe the simple conceptual components and processes underlying this paradigm, making it easier for the human computer interaction community to study and support this process for a wide range of visualizations. Finally, I use this paradigm and tangible tokens to study if and how non-experts are able to create, discuss and update their own visualizations. This study allows us to refine our previous model and provide a first exploration into how non-experts perform a visual mapping without software. In summary, this thesis contributes to the understanding of dynamic visualisation for non-expert users.
In the future, robots will become our companions and co-workers. However, we are still far from a real autonomous robot, which would be able to act in a natural, efficient and secure manner with humans. To endow robots with the capacity to act naturally with human, it is important to study, first, how humans act together. Consequently, this manuscript starts with a state of the art on joint action in psychology and philosophy before presenting the implementation of the principles gained from this study to human-robot joint action. We will then describe the supervision module for human-robot interaction developed during the thesis. Part of the work presented in this manuscript concerns the management of what we call a shared plan. Here, a shared plan is a a partially ordered set of actions to be performed by humans and/or the robot for the purpose of achieving a given goal. First, we present how the robot estimates the beliefs of its humans partners concerning the shared plan (called mental states) and how it takes these mental states into account during shared plan execution. It allows it to be able to communicate in a clever way about the potential divergent beliefs between the robot and the humans knowledge. Second, we present the abstraction of the shared plans and the postponing of some decisions. Indeed, in previous works, the robot took all decisions at planning time (who should perform which action, which object to use…) which could be perceived as unnatural by the human during execution as it imposes a solution preferentially to any other. This work allows us to endow the robot with the capacity to identify which decisions can be postponed to execution time and to take the right decision according to the human behavior in order to get a fluent and natural robot behavior. The complete system of shared plans management has been evaluated in simulation and with real robots in the context of a user study. Thereafter, we present our work concerning the non-verbal communication needed for human-robot joint action. This work is here focused on how to manage the robot head, which allows to transmit information concerning what the robot's activity and what it understands of the human actions, as well as coordination signals. Finally, we present how to mix planning and learning in order to allow the robot to be more efficient during its decision process. The idea, inspired from neuroscience studies, is to limit the use of planning (which is adapted to the human-aware context but costly) by letting the learning module made the choices when the robot is in a "known" situation. The first obtained results demonstrate the potential interest of the proposed solution.
Microblog service (such as Twitter and Sina Weibo) have become an important platform for Internet content sharing. The analysis of the information diffusion in Microblogs involves the data collection from Microblog, the modeling on information spreading and using the resulting models. Dealing with the huge amount of data flowing through microblogs is by itself a challenge. Designing an efficient and unbiased sampling algorithm for Microblog is therefore essential. Besides, the retweeting process in Microblog is complex because of the ephemerality of information, the topology of Microblog network and the particular features (such as number of followers) of publisher and retweeters. Two traditional models have been used for information diffusion : Independent Cascades and Linear Threshold models. However no one of them can describe completely the retweeting process in Microblog accurately. The analysis and design of new models to characterize the information diffusion in Microblog is therefore necessary. Moreover, a comprehensive description of the correlation between the information diffusion in Microblog and the searching trends of keywords on search engines is lacking although some work has been found some preliminary relationships. This work presnets a complete analysis of information diffusion in Microblog from. The contributions and innovations of this thesis are as follows: 1)There are two popular unbiased Online Social Network (OSN) sampling algorithms, Metropolis-Hastings Random Walk (MHRW) and Unbiased Sampling for Directed Social Graph (USDSG) method. However they are both likely to yield considerable self-sampling probabilities when applied to Microblogs where there is local. To solve this problem, I have modelled the process of OSN sampling as a Markov process and have deduced the sufficient and necessary conditions of unbiased sampling. The experimental evaluation demonstrate that the average node degree of samples of MHRW and USDSG is 2 - 4 times as high as the ground truth while USDE can provide the approximation of ground truth when the sampling repetitions are removed. 2)A second contribution targets the shortages of Independent Cascades (IC) and Linear Threshold (LT) models in characterizing the retweeting process in Microblogs. I achieve this by introducing a Galton Watson with Killing (GWK) model which considers all the three important factors including the ephemerality of information, the topology of network and the features of publisher and retweeters accurately. Besides, the GWK model is useful for revealing the endogenous and exogenous factors which affect the popularity of tweets. 3) Motivated by the correlation between popularity and trendiness of topics in Microblog and search trends, I have developed an economic analysis of the market involving a third-party ad broker, which is a popular market in current SEM, and finds that the adwords augmenting strategy with the trending and popular topics in Twitter enables the broker to achieve, on average, four folds larger return on investment than with a non-augmented strategy, while still maintaining the same level of risk.
These last years, with the advent of sites such as Youtube, Dailymotion or Blip TV, the number of videos available on the Internet has increased considerably. The size and their lack of structure of these collections limit access to the contents. Summarization is one way to produce snippets that extract the essential content and present it as concisely as possible. In this work, we focus on extraction methods for video summary, based on audio analysis. We treat various scientific problems related to this objective: content extraction, document structuring, definition and estimation of objective function and algorithm extraction. On each of these aspects, we make concrete proposals that are evaluated. On content extraction, we present a fast spoken-term detection. The main novelty of this approach is that it relies on the construction of a detector based on search terms. We show that this strategy of self-organization of the detector improves system robustness, which significantly exceeds the classical approach based on automatic speech recogntion. We then present an acoustic filtering method for automatic speech recognition based on Gaussian mixture models and factor analysis as it was used recently in speaker identification. The originality of our contribution is the use of decomposition by factor analysis for estimating supervised filters in the cepstral domain. We then discuss the issues of structuring video collections. We show that the use of different levels of representation and different sources of information in order to characterize the editorial style of a video is principaly based on audio analysis, whereas most previous works suggested that the bulk of information on gender was contained in the image. The third focus of this work concerns the summary itself. As part of video summarization, we first try, to define what a synthetic view is. Is that what characterizes the whole document, or what a user would remember (by example an emotional or funny moment)? We then propose an algorithm for finding the sum of the maximum interest that derives from the one introduced in previous works, based on integer linear programming.
Bilingual lexicographic resources for multiword expressions (MWE) are still rare and they are even more so for the French / Arabic language pair that interests us. There is little research on this language pair. In our research project, we are interested in extracting, from specialized parallel and comparable fr-ar corpuses, MWE, in particular the verbally based patterns. These constructions, from our point of view, constitute a relevant aspect to study in the specialized texts. The resulting corpus will relate to the medical field, a strategic field at the present time where the whole world is facing a pandemic. We will therefore take advantage of the availability of popular scientific articles which abound at this time. In our study we will adopt a contrastive approach which consists in identifying and analyzing the MWE and their translational equivalences, and this in a corpus based approach. Our main objective is to further research on this still under-studied language pair in this field, and to provide resources for translators and learners of specialized translation. Ultimately, it will be about exploiting the results in the field of specialized computer-assisted translation learning, drawing inspiration from the Data Driven Learning approach. All the steps will be carried out using the NLP tools appropriate for each language and each procedure.
This lead to a hierarchical division of the various tasks performed in order to analyze a text statement. The traditional approach considers task-specific models which are subsequently arranged in cascade within processing chains (pipelines). This approach has a number of limitations: the empirical selection of models features, the errors accumulation in the pipeline and the lack of robusteness to domain changes. These limitations lead to particularly high performance losses in the case of non-canonical language with limited data available such as transcriptions of conversations over phone. Disfluencies and speech-specific syntactic schemes, as well as transcription errors in automatic speech recognition systems, lead to a significant drop of performances.
Limestone is a historical building material widely used in Loire Valley of France. Time passed, it is facing to health problems, mainly due to water content. To evaluate this indicator, stepped-frequency radar is an efficient technique among all non-destructive testing techniques. It can be combined with a frequency-domain full-waveform inversion technique, which is an efficient signal processing method to achieve permittivity that is the observable of water content, after a calibration process. To do signal inversion, an analytical model of ultrawide band radar wave propagation through limestone is built in the forward study. This model is based on one-dipole Green's function, for the EM propagation, and 4-parameters' Jonscher dispersion model, for the EM characterization of the medium. This direct model is validated by a numerical approach using HFSS and an experimental approach using an experimental platform of Cerema. The inverse problem, including objective function and minimization algorithm, to achieve the permittivity, is then studied. Finally, this method is validated on several practical applications including geophysical information acquisition (water front estimation) and material characterization (water gradient estimation and hydric mapping).
This thesis consists of two parts, a theoretical part and a practical one. The study will also tackle the link between terminography and specialized translation on both didactic and professional levels. Secondly, using term extractors, the operation of terms extraction from the delimited corpus will be conducted. After sorting the generated terms, according to well-defined linguistic criteria, these terms will be the subject of lexical and morphological analyses in order to highlight the way Arabic language adopt to form terms as well as the effect of its contact with other languages. In addition, we will conduct a field study through a questionnaire, addressed to translators and conference interpreters, in order to examine their urgent medical terminology needs and assess their use of available terminology management tools. This study has several purposes: meeting the urgent needs of finding new terms in the medical field and providing translators and conference interpreters with an easily accessible terminology management tool to assist them in their work that requires relevance and rapidity. The study attempts also to explore new horizons in specialized translation teaching methods by highlighting the considerable contribution of terminotics in the acquisition of professional skills in this field. The study aims also to assess the use of NLP software in order to underline the extent of their use as well as their limitations in terminology extraction, which should help developers better improve the performance of these tools.
While the benefits of early bilingual education programs are widely praised and acknowledged, relatively little is known about the processes involved in child second language acquisition. The aim of this research is to investigate L2 development in young children while school is their only source of exposure to the L2. Relying on corpora collected in two French-American kindergarten and primary schools in California, this work focuses on how French is acquired by English-speaking children from 5 to 7 through immersion education. Following interactionist theories of L2 acquisition, it is shown how this specific context bears on the course of acquisition in terms of interaction opportunities and input addressed to the children, and also how it provides them with scaffolding patterns that are instrumental in the first stages of L2 acquisition. Instead, it seems that child second language acquisition in this context is a rather complex phenomenon involving many factors such as linguistic and metalinguistic awareness, interaction strategies, or influence from L1. Based on these findings, pedagogical guidelines for bilingual teaching in kindergarten are provided.
The present thesis offers both apparent-time and real-time analyses of two sub-corpora of the Diachronic Electronic Corpus of Tyneside English (DECTE): one from the 1970s and another one compiled in the 1990s (Corrigan et al., 2012). It comprises two main parts: (1) an analysis of inter and intra-speaker variation in the lexical sets FACE, GOAT, PRICE and MOUTH (Wells 1982) based on a multiple factor analysis (MFA, Escofier 2008, Husson et al. 2011) (2) a dynamic acoustic analysis of formant trajectories of these vowels using Generalised additive mixed models (GAMMs, Wood 2015) followed by a static analysis of onsets in PRICE. The first part establishes the sociolinguistic profiles of 44 speakers from Gateshead and Newcastle based on the original phonetic transcriptions of the Tyneside Linguistic Survey (TLS, Strang 1968). Although the profiling analysis are based on the entire phonetic system transcribed by the original TLS team, the main focus is on FACE, GOAT, PRICE and MOUTH only. Results indicate that FACE the main determinant of TE speech. The symmetry between FACE et GOAT as found by Watt 1999, was also observed in PRICE et MOUTH among women. While middle-class women clearly favour a closing diphthong in FACE et GOAT and have a low onset in PRICE and MOUTH, working-class women tend to have higher frequency scores of pan-northern monophthongs in the first pair of lexical sets. They also exhibit more frequent raised onsets in PRICE and MOUTH. In addition, the central monophthong GOAT is more often used by men with a less traditional accent in the 1970s corpus, which is in line with Watt's findings for the 1990s corpus (Watt 1998). The second part analyses formant trajectories in FACE, GOAT and PRICE. The main aim was to compare the original phonetic transcriptions with the corresponding formant trajectories. Results confirm the pertinence of the transcriptions in the wordlist section of the corpora (TLS and PVC). Differences between the two main variants of PRICE ([aɪ] vs. [eɪ]) appeared to be strikingly different be in terms of both onsets / offset heights and trajectory shape.
Advances in information technologies, such as communication technology and database technology, allow us to access to vast amounts of information and are causing an exponential increase in the number of documents available online. Effective retrieval and mining are getting more and more difficult. Document representation aims to represent document input into a fixed-length vector to reduce the complexity of the documents and make them easier to handle. Therefore, document representation is playing an important role in many real-world applications, e.g., information retrieval, text clustering and classification. Simple weighted averaging of word vectors is an effective way to represent sentences. However, when the document is long, it may be not effective and would lose fine grained distinctions. The reason is when the document is long, it is likely to contain words from many different topics and hence creating a single vector would ignore the thematic structure. To overcome the above mentioned limitation, we want to do something a little bit different. That is to say, instead of getting representations by simple average, we can represent a document by a set of vectors and we can get novel representation. It turns out to be more reasonable because the set of vectors can cover different parts of the documents. By this mean, the representation can have the ability to cover different topics. The core part of document representation model is the hierarchical document encoder. We want to proposed a method to firstly pre-train document level hierarchical bidirectional transformer encoders on unlabeled data. Further the pretrained document models can be adapted to different tasks (information retrieval, text clustering and text classification) and collections via fine-tuning.
In this thesis, we study the expressivity of read speech with a particular type of data, which are audiobooks. Audiobooks are audio recordings of literary works made by professionals (actors, singers, professional narrators) or by amateurs. These recordings may be intended for a particular audience (blind or visually impaired people). The availability of this kind of data in large quantities with a good enough quality has attracted the attention of the research community in automatic speech and language processing in general and of researchers specialized in expressive speech synthesis systems. We propose in this thesis to study three elementary entities of expressivity that are conveyed by audiobooks: emotion, variations related to discursive changes, and speaker properties. We treat these patterns from a prosodic point of view.
Information now occupies a central place in our daily lives, it is both ubiquitous and easy to access. Yet extracting information from data is often an inaccessible process. Indeed, even though data mining methods are now accessible to all, the results of these mining are often complex to obtain and exploit for the user. Pattern mining combined with the use of constraints is a very promising direction of the literature to both improve the efficiency of the mining and make its results more apprehensible to the user. However, the combination of constraints desired by the user is often problematic because it does not always fit with the characteristics of the searched data such as noise. In this thesis, we propose two new constraints and an algorithm to overcome this issue. The robustness constraint allows to mine noisy data while preserving the added value of the contiguity constraint. The extended closedness constraint improves the apprehensibility of the set of extracted patterns while being more noise-resistant than the conventional closedness constraint. The C3Ro algorithm is a generic sequential pattern mining algorithm that integrates many constraints, including the two new constraints that we have introduced, to provide the user the most efficient mining possible while reducing the size of the set of extracted patterns. C3Ro competes with the best pattern mining algorithms in the literature in terms of execution time while consuming significantly less memory. C3Ro has been experienced in extracting competencies from web-based job postings
Many successes of deep learning rely on the availability of massive annotated datasets that can be exploited by supervised algorithms. Obtaining those labels at a large scale, however, can be difficult, or even impossible in many situations. Designing methods that are less dependent on annotations is therefore a major research topic, and many semi-supervised and weakly supervised methods have been proposed. Meanwhile, the recent introduction of deep generative networks provided deep learning methods with the ability to manipulate complex distributions, allowing for breakthroughs in tasks such as image edition and domain adaptation. In this thesis, we explore how these new tools can be useful to further alleviate the need for annotations. Firstly, we tackle the task of performing stochastic predictions. It consists in designing systems for structured prediction that take into account the variability in possible outputs. Then, we study adversarial methods to learn a factorized latent space, in a setting with two explanatory factors but only one of them is annotated. We propose models that aim to uncover semantically consistent latent representations for those factors. One model is applied to the conditional generation of motion capture data, and another one to multi-view data. Finally, we focus on the task of image segmentation, which is of crucial importance in computer vision. Building on previously explored ideas, we propose a model for object segmentation that is entirely unsupervised.
This thesis uses multi-scalar data to create a three-dimensional (3D) representation and, to generate a complete digital record of the early hominin-bearing fossil assemblage from the lithostratigraphic Unit P at Kromdraai in the Cradle of Humankind World Heritage Site (Gauteng Province, South Africa). We provided a multi-scalar analysis of various aspects of the study site, with the application of methods such as multi-image land and aerial photogrammetry. In alignment with the principles and guidelines for the management of archaeological heritage mandated by international agencies such as UNESCO, we also present a protocol for heritage documentation. We used 3D data capture technologies to record the Kromdraai site and the archaeological evidence discovered between 2014 and 2018 from its main excavation. This research presents an original technique developed for the quantification and visualization of the volume sediments removed from the site during each excavation period. Volume estimations computed using 3D photogrammetry and digitization, provided a temporal and spatial context to the volume and location of material removed by each excavator and, a more precise and virtual repositioning of the fossil material discovered ex situ. Furthermore, we implemented metadata modelling to demonstrate the use of 4D relational database management systems for the fusion, organisation and dissemination of the Kromdraai site dataset and the sharing of intellectual property. We also introduce one of the first statistical approaches of 3D spatial patterning in Plio-Pleistocene early hominin-bearing assemblages in South Africa. Implementing classic statistical testing methods such as k-means and Density-Based Spatial Clustering and Application with Noise (DBSCAN) cluster computation in 3D, we investigated the spatial patterns of the fossil assemblage within Unit P, a sample of 810 individually catalogued specimens recovered between 2014 and 2018. The clustering of bovids, carnivores, hominins, and non-human primates revealed a non-uniform spatial distribution pattern of fossils in-situ.
The amount of information on the Internet today overwhelms most users. Discovering relevant information (e.g. news to read or videos to watch) is time-consuming and tedious and yet it is part of the daily job of at least 80% of the employees in North America. Several information filtering systems for the web can ease this task for users. Examples fall into families such as Social Networks, Social Rating Systems and Social Bookmarking Systems. All these systems require user engagement to work (e.g. submission or rating of content). They work well in an Internet-wide community but suffer in the case smaller communities. Indeed, in smaller communities, the users' input is more scarce. We focus on communities of a place that are communities that group people who live, work or study in the same area. Examples of communities of a place are: (i) the students of a campus, (ii) the people living in a neighborhood or (iii) researchers working in the same site. Anecdotally we know that only 0.3% of workers contribute daily to their corporate social network. This information shows that there is a lack of user engagement in communities of a place.
This thesis is dedicated to non-convex modeling and the optimization based on the DC programming and DCA for certain classes of problems of two important domains: the Data Mining and the Cryptology. They are non-convex optimization problems of very large dimensions for which the research of good solution methods is always of actuality. Our work is based mainly on the DC programming and DCA that have been successfully applied in various fields of applied sciences, including machine learning. It is motivated and justified by the robustness and the good performance of DC programming and DCA in comparison with the existing methods. This thesis is devised in three parties. The first part, entitling Methodology, serves as a reference for other chapters. The first chapter concerns the programming of DC and DCA while the second chapter describes the genetic algorithms. In the second part, we develop the DC and DCA programming to solve two classes of problems in Data Mining. In the chapter four, we take consideration into the model of classification FCM and develop the programming DC and DCA for their resolution. Many formulations DC in correspondence to different decompositions DC are proposed. Our work in hierarchic classification (chapter 5) is motivated by one of its interesting and very important applications, known as muliticast communication. It's a non-convex, non differentiable, non-convex problem in a very big dimension with which we have reformulated in the forms of 3 different DC programs and developed the DCA relative. The 3rd part focuses on the Cryptology. We propose a method of resolving two problems PP and PPA by DCA and a cutting plan method in the last chapter
This thesis explores the construction of sense in English particle and prepositional verbs. It departs from the premise that meaning is something constructed during the process of situated usage. A corpus of 286 combinations formed through the association of a verbal element with over are analysed in context in order to identify the various factors which influence final semantic interpretation. The relationship between syntactic configuration and semantic interpretation is investigated and the various ways in which the number and nature of the verbal and/or prepositional arguments can impact semantic interpretation is explored. The study also explores several more general areas of linguistic investigation including the conceptualisation of movement, resultativity, transitivity and polysemy. During the course of the study a wide range of factors which influence the final semantic interpretation of particle and prepositional verbs in English are identified.
In a context of development and maintenance of the enterprise competitiveness, Business and Competitive Intelligence feeds the enterprise with informations which may be used as reference objects for environment analysis and decision-making support. The ALSEM project aims at the design of an automatic system assisting the Competitive Intelligence professional in the exploration and combination of informational resources dealing with the enterprise's environment. The project's goal is the integration of technics offering a methodologic framework for an information management more suitted to a specific and complex task like Business and Competitive Intelligence. Our work stands at the bridge of three disciplines: natural language processing, semantics and knowledge engineering. It aims at the development of information extraction technics with a semantic analysis of texts and information interpretation and modelling technics to make it operational.
Semantic Web is the vision of next generation of Web proposed by Tim Berners-Lee in 2001. Traditional Semantic Web querying and reasoning tools are designed to run in stand-alone environment. Therefor, Processing large-scale bulk data computation using traditional solutions will result in bottlenecks of memory space and computational performance inevitably. Large volumes of heterogeneous data are collected from different data sources by different organizations. In this context, different sources always exist inconsistencies and uncertainties which are difficult to identify and evaluate. To solve these challenges of Semantic Web, the main research contents and innovative approaches are proposed as follows. For these purposes, we firstly developed an inference based semantic entity resolution approach and linking mechanism when the same entity is provided in multiple RDF resources described using different semantics and URIs identifiers. We also developed a MapReduce based rewriting engine for Sparql query over big RDF data to handle the implicit data described intentionally by inference rules during query evaluation. The rewriting approach also deal with the transitive closure and cyclic rules to provide a rich inference language as RDFS and OWL. The second contribution concerns the distributed inconsistency processing. We extend the approach presented in first contribution by taking into account inconsistency in the data. The third contribution concerns the reasoning and querying over large-scale uncertain RDF data. We propose an MapReduce based approach to deal with large-scale reasoning with uncertainty. Unlike possible worlds semantic, we propose an algorithm for generating intensional Sparql query plan over probabilistic RDF graph for computing the probabilities of results within the query.
In this thesis, we study the problem of detection of visual relations of the form (subject, predicate, object) in images, which are intermediate level semantic units between objects and complex scenes. Our work addresses two main challenges in visual relation detection: (1) the difficulty of obtaining box-level annotations to train fully-supervised models, (2) the variability of appearance of visual relations. We first propose a weakly-supervised approach which, given pre-trained object detectors, enables us to learn relation detectors using image-level labels only, maintaining a performance close to fully-supervised models. Experimental results demonstrate the improvement of our hybrid model over a purely compositional model and validate the benefits of our transfer by analogy to retrieve unseen triplets.
This Ph.D. thesis is about the establishment of textual data similarities in the client relation domain. Two subjects are mainly considered: - the automatic analysis of short messages in response of satisfaction surveys ; - the search of products given same criteria expressed in natural language by a human through a conversation with a program. The first subject concerns the statistical information from the surveys answers. The ideas recognized in the answers are identified, organized according to a taxonomy and quantified. The second subject concerns the transcription of some criteria over products into queries to be interpreted by a database management system. The number of criteria under consideration is wide, from simplest criteria like material or brand, until most complex criteria like color or price. The two subjects meet on the problem of establishing textual data similarities thanks to NLP techniques. The main difficulties come from the fact that the texts to be processed, written in natural language, are short ones and with lots of spell checking errors and negations. Establishment of semantic similarities between words (synonymy, antonymy,...) and syntactic relations between syntagms (conjunction, opposition,...) are other issues considered in our work. We also study in this Ph. D. thesis automatic clustering and classification methods in order to analyse answers to satisfaction surveys.
Bourdieu, a sociologist, defines social capital as: "The set of current or potential ressources linked to the possession of a lasting relationships network". On Twitter,the friends, followers, users mentionned and retweeted are considered as the relationships network of each user, which ressources are the chance to get relevant information, to be read, to satisfy a narcissist need, to spread information or advertisements. We observe that some Twitter users that we call social capitalists aim to maximize their follower numbers to maximize their social capital. We introduce their methods, based on mutual subscriptions and dedicated hashtags. In order to study them, we first describe a large scale detection method based on their set of followers and followees. Then, we show with an automated Twitter account that their methods allow to gain followers and to be retweeted efficiently. Afterwards, we bring to light that social capitalists methods allows these users to occupy specific positions in the network allowing them a high visibility. Furthermore, these methods make these users influent according to the major tools. We thus set up a classification method to detect accurately these user and produce a new influence score.
The information cycle, from collection to dissemination is a cornerstone in competitive intelligence. Our work is to study about the impact that web 2.0 has on the famous cycle and propose methods and tools to take advantage of this new paradigm, and this for each stage of the cycle
The thesis has been prepared within a co-supervision agreement with the Professors Jean-Hugues Chauchat (ERIC-Lyon2) and N.V. Charonova (National Polytechnic University of Kharkov in Ukraine).The results obtained can be summarized as follows:1. State of the art:Retrospective of theoretical foundations concerning the formalization of knowledge and natural language as precursors of ontology engineering. Update of the state of the art on general approaches in the field of ontology learning, and on methods for extracting terms and semantic relations. Methodological proposals:Learning morphosyntactic patterns and implementing partial taxonomies of terms. Finding semantic classes representing concepts and relationships for the field of radiological safety. Building a frame for the various stages of the work leading to the construction of the ontology in the field of radiological safety.3. Implementation of the three previous methods and analysis of the results obtained.
The Data Quality Monitoring of High Energy Physics experiments is a crucial and demanding task to deliver high-quality data used for physics analysis. At the Compact Muon Solenoid experiment operating at the CERN Large Hadron Collider, the current quality assessment paradigm, is based on the scrutiny of a large number of statistical tests. However, the ever increasing detector complexity and the volume of monitoring data call for a growing paradigm shift. Here, Machine Learning techniques promise a breakthrough. This dissertation deals with the problem of automating Data Quality Monitoring scrutiny with Machine Learning Anomaly Detection methods. Anomalies caused by detector malfunctioning are difficult to enumerate a priori and rare, limiting the amount of labeled data. This thesis explores the landscape of existing algorithms with particular attention to semi-supervised problems and demonstrates their validity and usefulness on real test cases using the experiment data. As part of this project, the monitoring infrastructure was further optimized and extended, delivering methods with higher sensitivity to various failure modes.
In order to open their innovation process and co-create value with consumers, companies use idea crowdsourcing platforms, allowing them to gather innovative ideas from the crowd. This doctoral thesis studies a new platform model mixing competition with cooperation, called the «co-opetition» model. Our research question is: does the co-opetition model enhance consumer's creative performance, comparing with classical models based on competition or cooperation? To answer this question, we have conducted two experiments aiming at quantitatively comparing co-opetition, competition and cooperation effects on participant's creative performance. Results show that co-opetition and competition enhance creativity, while the hypothesized positive effect of cooperation is not supported. There is a significant interaction effect between competition and cooperation: co-opetition increases creativity more than competition alone. We find also a significant mediating effect of motivational ambivalence, while the hypothesized mediation effect of emotional ambivalence is not supported. Attitude towards independance moderates the direct effects of co-opetition and competition on idea quality, while attitudes towards cooperation and competition do not.
This dissertation revolves around two wider topics: social norms and production networks. The first chapter investigates a specific modern-day case study where social norms are leveraged in the fight against online hate speech to shed light on how norms shape political behavior more broadly. Using machine learning techniques, I show that speaking out against hateful views is an effective way of deterring further hate speech. The mechanism that most likely explains this effect is that vociferous contradiction in fact serves as a form of non-monetary punishment that raises the salience of a social norm. The second chapter focuses on the crucial role of image concerns in explaining the effect of social norms on behavior. While there are now plenty of studies showing that image concerns affect people on average, we still know very little about which individuals specifically drive that effect. I introduce a novel laboratory experiment designed to fill this gap. It generates an individual-specific measure of image concern, shows that there is substantial heterogeneity even in a small laboratory sample, and investigates how it correlates with other social preferences. Vertical integration give rise to anticompetitive behavior or indeed be a motive for it. I discuss one such mechanism, called vertical foreclosure, by which vertically integrating firms disrupt the supply of critical inputs to competitors. I leverage novel production network data to identify mergers and acquisitions between vertically related firms and show taht these mergers affect the supply chains of their rivals, which I interpret as evidence for foreclosure.
This thesis builds upon the work on the Social Semantic Web, a research perspective on the complementarity and coevolution of two aspects of the Web, the social and semantic one. Web development in recent years has given rise to a huge graph of semantically structured data, partly resulting from user activity. We are particularly interested in the use of this graph in order to facilitate access to information found on the Web, in a useful, informative manner. This problem is particularly studied in scenarios related to innovation on the Web - practices to use Web technologies to contribute to the emergence of innovation. A notable specificity of this context, so far little discussed in literature, is the need to encourage serendipity and discovery. Beyond the simple relevance sought in any search and recommendation situation on the Web, the context of innovation requires a certain openness to allow the user to access information relevant yet unexpected, and should also open opportunities to learn and translate ideas from one domain to another.
The thesis leans on the concept of media literacy. After defined this concept, we have tried to implement it for non-reading children (2 - 7 years) of Togo and\or deaf persons (7 in 12 years) of France. Assuming that the media contents can contribute to the development, to the apprenticeships and to the culture of these children, an information-communication analysis of the mechanisms of appropriation of these contents is made for these two target populations. The study has mobilized experiments in Togo and in France. This preliminary review of the grounds of study shows that the public politics and plans of communication appropriation for our target remain insufficient on one hand, and that the field is little approached on the previous works of the other one. The second part bends over the theoretical frame of the media literacy and specifies its conceptualization in the field of information communication. The protocol VI.A.G.E is then elaborated to estimate the process of appropriation of media contents with the children. The third part is dedicated to the perusal of three experiments of ground led to Togo (experimental viewing of the movie Kirikou and the witch) and in France (visio-guide on DVD and iPad, and interaction on tactile big screen, in particular at Quai Branly Museum). Their respective results are explained.
From the hypothesis which the form informs about the meaning, a corpus of attested utterances has been constituted and collected in a database so as to allow the observation of syntactic, lexical and semantic combinatorial of prepositional groups complements ("argumentaux") introduced by French preposition dans with a verb. The research on one hand identifies and describes all the verbs which subcategorize this preposition and on one other hand finds the interpretations which emerge from these combinations so as to further in the semantic identity of the French preposition dans and of the complement in general. From the syntactic point of view, the result of the investigation consists of a list (of uses) of definable verbs by their complementation with dans and characterized by a certain number of properties: the study brought to a successful conclusion in the definition of a lexical class, a paradigm also united on the semantic plan. The French preposition dans is analyzable in every case as a marker of "coi͏̈ncidence", whatever is the particular value of a complement. The contribution strictly linguistics of the study is defined and presented by sort to allow us the using in the computational linguistics field.
The anaphora traditionally handled in man-machine dialogues are pronominal anaphora. Next, having exploited the corpus on the retrieval level (query typology, search strategies,...) and on the linguistic level (suraces structures), anaphora, reference and coherence,...), we express our doubts about the usability of natural language in certain situations and we also state the hypothesis of the need for a new "phraseology" for man-machine communication. Concerning natural language processing (nlp, we explain the different types of knowledge necessary for dialogue management and anaphora resolution (dialogue modeling, task modeling, dialogue dynamics,...), on the basis of different linguistic approaches of anaphorical phenomena and existing work in nlp. Within the framework of the nlp work done at criss (centre for research in informatics applied to the social sciences), we then derive some rules for identifying anaphoric units.
This thesis will be devoted to the use of natural language processing methods in finance. We will first study sentiment analysis methods with the aim of applying them on financial news. Before the era of artificial intelligence, feature engineering and statistical approaches were the dominant tools in this area, e.g., Naïve Bayes Classifier, TF-IDF. However, most of these approaches are based on static embeddings and ignore contextual information in a sentence. Hence, we are going to study possible improvements in accuracy by introducing contextualized embeddings into the models. We will also methods enabling us to generate contextualized embedding, either by defining and training an embedding model from scratch or fine-tuning an existing model, such as BERT or XLNet. We will then focus on sentiment analysis on long texts. In finance, there are different types of such documents, for example annual reports or conference transcripts. All these documents being much longer than a title or a summary, we cannot directly apply standard methods on these corpus. Thus, we propose summarizing the text in the first place and then training a classification model. Existing summarization methods such as, LDA (Latent Dirichlet Allocation) or BERTSum work well on somehow uniform texts. However they have limited performance on these financial documents with various formats and characteristics. We will investigate the transfer learning approach in this task. We will first train a general model based on all available texts. Then, for each type of corpus, we will consider a much more parsimonious model with limited amount of data. Our final result will be obtained by aggregating the outputs of the general model and the specialized model. Finally, we will be interested in the possibility of applying NLP methods on time series dimension reduction task. Widely used methods in NLP such as Auto-encoder, Generative Adversarial Network (GAN) and Variational Auto-Encoder (VAE) all have an encoder part, which transforms high-dimensional embedding into multiple vectors as a summary. Hence, based on this idea, we would like to design a model structure which takes in high-dimensional time series instead of texts and outputs a lower-dimensional time series.
Patents are industrial property titles that give their holders a monopoly over the patented invention. It is possible to find a sort of history of the evolution of the artifact. In this context the designer often like to do research in patent documents in order to benefit from the knowledge contained inside to structure the inventive process. Developed to assist designers in their innovation approach, the Inventive Design Method (IDM) is part of the pattern of dialectic. IDM has clarified the concepts at stake in the description of the evolution of technical systems and artifact. These items often interest designers and are essential to understanding the underlying problem and collecting of all features on which to act; and the effect of variations on the artifact. This thesis, firstly, deals with patent document analysis from a linguistic point of view, in order to know its typology. Then, it is possible to identify in the patent document, the knowledge likely to be useful to IDM and formalize it as a computer program. The approach proposed in this paper is based on text mining techniques. It uses a method based on linguistic markers using lexical and syntactic patterns from the field of natural language processing. This method of extraction of useful concepts for IDM allows the establishment of a kind of initial mapping of past and possible changes in the future of the artifact characteristics. The interest is also to greatly facilitate the preliminary analysis of knowledge on the said artifact.
This thesis deals with the question of behavioral changes, and in particular with the way this question applies to the computer domain through persuasive technologies. In a particular application context, that of the renovation of housing, we are interested in the role that the information available to users can play in the way they develop their renovation project. One way to change user behavior is to change the goals they pursue, either explicitly or implicitly. Although the effectiveness of the former has been shown in an experimental context, it seems less suitable for natural situations. We therefore propose an approach aimed at modifying the goals pursued by the users implicitly. With this in mind, we are working first on the use of injunctive social norms to encourage users to work particularly on energy renovation. In a first study, we compare injunctive social norm and goal setting to a control condition. We are interested in the performance of the participants in the task (improving the energy performance of a home) as well as the way in which the project is set up throughout the study. The results show that social norm and explicit goal have a similar effect on task performance but different on temporal organization. We also observe a more stable behavior in the case where the social norm is activated, and an effect that seems globally less artificial than in the case where we set an explicit objective to the user. This first study also highlights the need for the norm to be salient, or activated. In a second study, we focus on what characterizes the salience of the normative message. In the first study, we used two different types of information: the normative message and concrete cues of desirable behavior. This second study aims to distinguish these two types of information and test their respective effect. The results show that the normative message seems to have a slightly greater effect on performance but also more artificial on user behavior. In a third study, we are interested in the characteristics of the message, assuming that a better perceived message could support the salience of the norm it carries. As part of a collaboration with artificial intelligence researchers, we tested different types of framing to assess their respective effect on the perception of the argument to which they applied. The mixed results essentially show that the argumentative style (rational and factual rather than emotional or moral) seems to have a significant weight on the perception of the argument. In addition, the theme addressed by the argument seems to play a significant role and should therefore be given special attention for the development of similar interventions. At the application level, our results first highlight the relevance of the use of injunctive social norms in a context of persuasive technology. They also show that social standard messages must be carefully crafted, taking into account multiple factors. On the theoretical level, we show that a social norm can have an effect comparable to that of an explicitly fixed objective, but that both generate the setting up of different cognitive processes. Eventually, methodologically, we apply the analysis of traces of activity to the field of social influence, which, to our knowledge, had not yet been put in place.
The use of drugs is often necessary during pregnancy despite a lack of knowledge of their adverse effects on pregnancy or the fetus. The identification of adverse reactions to marketed drugs using statistical signal detection tools classically relies on the use of large spontaneous reporting databases in which pregnancy status is unfrequent. Health claims databases are increasingly used in pharmacoepidemiology studies, including those of pregnant women. They are therefore a potential resource for automated pharmacovigilance. In France, the National Health Data System covers almost the entire French population and the thesis focuses on the use of its permanent sample, the Permanent Beneficiaries Sample (EGB). The first axis describes the prescription of drugs for pregnant women and more particularly teratogenic or fetotoxic drugs and supplementations recommended during pregnancy. The results of this work are that pregnant women are dispensed a lot of medications. Known risk drugs are rarely prescribed during pregnancy and recommended supplementation is increased during the study period. Pregnant women on low incomes have more drug prescribed but fewer supplementations. The second axis focuses on the development of a signal detection methodology based on the use of the propensity score or the prognostic score in high dimension. This methodology is applied to identify drugs possibly associated with an increased risk of prematurity. The results show that the use of the p-RD derived from the prognostic score makes it possible to better take into account confusion, and to limit indication and protopathic bias. For the third axis, the p-RD is applied to twenty specific or non-specific pregnancy pathologies and combined with an automated query of MeSH keywords from MEDLINE articles. Automated query allows easy annotation of known adverse drug reactions. In conclusion, this thesis shows the value of medico-administrative data for analyzing the evolution of drug prescription during pregnancy and for the detection of pharmacovigilance signals in pregnancy.
Gradually, digital technologies are becoming more important in research on sociocultural phenomena. Equipment projects are developing in all the social sciences and the humanities (SSH) and movements advocating an instrumental revolution are multiplying. This thesis proposes to question the advent of a digitally equipped research in the SSH on the basis of a general reflection on the links between science, technology and writing. What are the epistemological and political issues that underlie these digital instrumentation logics as they institute new writing techniques at the heart of research practices? The thesis is composed of three main parts. The first part questions the fundamental relationships between technical instruments and scientific knowledge. It is also about estimating the specificities of a communication approach to scientific instrumentation. Based on a theory of digital writing, and on a techno-semiotic analysis approach, the thir part questions the forms and powers of the digital instrumentation. On a morphological level, what do the design and implementation of such instruments consist of? On a political level, what are the normative effects of these "dispositifs" on the epistemology of the disciplines that seize them?
The success of the Open Access movement during the last decade shows the relevance of this model for the research community. The new practices in scientific publishing aim to offer free access to information about research and to foster the dissemination of scientific knowledge. Furthermore, Open Science advocates for the free accessibility of not only research publications, but also their dataset, methods and results in order to enhance the reproducibility of scientific research, facilitate collaboration between researchers and accelerate innovation. Nowadays, scientific research benefits from the information society and Big Data, through the exploitation of big datasets, which are integral part of the current tools for the generation of new knowledge. This thesis aims to analyse and to process scientific articles in order to extract new meta-data regarding the datasets and the research results related to these datasets. We will study the challenges facing Open Science and the different phenomena related to scientific papers and their data, in order to propose a typology for datasets in the form of an ontology. We will propose an approach for the automatic identification of textual segments referring to datasets in scientific articles. The new metadata, obtained from these automatic analyses of scientific corpora, will be aggregated in the form of Open Data in order to propose new tools for the exploitation and analysis of the scientific output of a given domain.
Recommender systems play a leading role in user's choice guidance. The search of accuracy in such systems is generally done through an optimization of a function between the items and the users. It has been proved that maximizing only the accuracy does not produce the most useful recommendation for the users. This can confine individuals inside the bubble of their own choices. Additionally, it tends to emphasize the agglomaration of the users' behavior on few popular items. Thus, it produces a lack of diversity and novelty in recommendations and a limited coverage of the platform catalog. This non-discovery is even more crucial if the platform wants to be fair in its recommendations with all contents' producers (e.g, music artists, writers, video game developers or videographers). The non diversity, and novelty problem is more important for the users because it has been shown that human mind appreciates when moved outside of its comfort zone. Our two models are based on a user profile understanding prior to bring diversification. They capture the diversity in the user profile and respond to thisdiversity by looking to create a diverse list of recommendation without loosing to much accuracy. The first model is mainly built upon a clustering approach, while the second model is based on an wavelet function. This wavelet function in our model helps us delimit an area where the user will find item slightly different from what he liked in the past. Our proposals are tested on a common experimental design that consider well-known datasets and state-of-the-art algorithm. The results of our experiments show that our approaches indeed bring diversity and novelty and are also competitive against state-of-the-art method. We also propose a user-experiment to validate our model based on the wavelet. The results of user centered experiments conclude that this model corresponds with human cognitive and perceptual behavior.
At the beginning the concept of a parallel corpus is defined. French and Czech texts forming the parallel Fratchèque corpus come from literature; only texts after the year 1945 have been selected. Fratchèque is not marked up explicitly by XML tags because the tagging is not necessary for the proper functioning of the corpus manager ParaConc. The building-up of the corpus is thoroughly described following all steps and settings of the software used. The process starts with the optical character recognition program FineReader and, after checking the accuracy of numerical texts by using MS Word 2002, it goes on building up a corpus managed by ParaConc. The linguistic investigations of the thesis rely primarily on the realization of a parallel corpus. The main purpose is to tackle a phenomenon that is known in Czech as částice but has no direct equivalent in French. The most frequent terms used in the French approach are mots du discours and particules énonciatives. The existing descriptions suggest a close relationship between these words and the discourse. It is demonstrated on two Czech částice - přece, vždyt̕ and their variants - using huge Czech corpora (Analysis A) and Fratchèque (Analysis B). The study continues analysing systematically all kind of usage of vždyt̕, přece in order to present lexicographical description for a bilingual Czech-French dictionary. Finally, some issues concerning automatic evaluation of translation quality are discussed taking into account the work with částice
The research work presented in this thesis concerns the development of unsupervised learning approaches adapted to large relational and dynamic data-sets. The combination of these three characteristics (size, complexity and evolution) is a major challenge in the field of data mining and few satisfactory solutions exist at the moment, despite the obvious needs of companies. This is a real challenge, because the approaches adapted to relational data have a quadratic complexity, unsuited to the analysis of dynamic data. We propose here two complementary approaches for the analysis of this type of data. The second proposes to use support points among the objects in order to build a representation space to define representative prototypes of the clusters. Finally, we apply the proposed approaches to real-time profiling of connected users.
Computer based teaching which grew out of automated teaching has been one of the central issues of the past twenty years. The increasing use of computers in education has opened up a new series of possibilities for both the teacher and the learner of english. In order to provide the learner with original exercises any development of computer tools must take into account the latest possibilities offered by information technology such as direct access, simulation and interactivity. An analysis taking into account on the one hand the processing of speech by machine and on the other hand the salient features of aural comprehension will allow us to construct a structure for the teaching and the learning of the latter competence which will be based on quality, quantity, strategy and communication. For each of these categories interactive multimedia computer based teaching offers a number of advantages. It is particularly at this last level that a wider range of innovative activities most of which only possible using information technology can now be designed.
The aim of this study is propose a typology of predicates of motion in Hungarian. The typology reflects a simple objective perception of motion and space. Our classification is based on semantic properties such as directionality, mood, destination, goal, place and the aspectual properties. These semantic properties are completed by morpho-syntactic properties needed for natural language processing. The contrastive component of our study has made it possible to propose a better description of the classes of predicates in Hungarian and to bring out the morpho-syntactic and combinatory differences specific to both languages in the expression of motion, such as the role of verb prefixes, locative complements, and to underline the importance of noun predicates.
In this thesis, we propose a collaborative model driven methodology for designing Autonomic Cognitive IoT systems to deal with IoT design complexity. We defined within this methodology a set of autonomic cognitive design patterns that aim at (1) delineating the dynamic coordination of the autonomic processes to deal with the system's context changeability and requirements evolution at run-time, and (2) adding cognitive abilities to IoT systems to understand big data and generate new insights. To address challenges related to big data and scalability, we propose a generic semantic big data platform that aims at integrating heterogeneous distributed data sources deployed on the cloud and generating knowledge that will be exposed as a service (Knowledge as a Service--KaaS). As an application of the proposed contributions, we instantiated and combined a set of patterns for the development of prescriptive cognitive system for the patient treatment management. Thus, we elaborated two ontological models describing the wearable devices and the patient context as well as the medical knowledge for decision-making. The proposed system is evaluated from the clinical prescriptive through collaborating with medical experts, and from the performance perspective through deploying the system within the KaaS following different configurations
Agents having to take a collective decision are often motivated by individual goals. In such scenarios, two key aspects need to be addressed. The first is defining how to select a winning alternative from the expressions of the agents. The second is making sure that agents will not manipulate the outcome. This dissertation studies the aggregation and the strategic component of multi-agent collective decisions where the agents use a compactly represented language. The languages we study are all related to logic: from propositional logic, to generalized CP-nets and linear temporal logic (LTL). Our main contribution is the introduction of the framework of goal-based voting, where agents submit individual goals expressed as formulas of propositional logic. Classical aggregation functions from voting, judgment aggregation, and belief merging are adapted to this setting and studied axiomatically and computationally. Desirable axiomatic properties known in the literature of social choice theory are generalized to this new type of propositional input, as well as the standard complexity problems aimed at determining the result. Another important contribution is the study of the aggregation of generalized CP-nets coming from multiple agents, i.e., CP-nets where the precondition of the preference statement is a propositional formula. We use different aggregators to obtain a collective ordering of the possible outcomes. Thanks to this thesis, two lines of research are thus bridged: the one on the aggregation of complete CP-nets, and the one on the generalization of CP-nets to incomplete preconditions. The focus is on three majoritarian voting rules which are found to be manipulable. Therefore, we study restrictions on both the language of the goals and on the strategies allowed to the agents to discover islands of strategy-proofness. We also present a game-theoretic extension of a recent model of opinion diffusion over networks of influence. In the influence games defined here, agents hold goals expressed as formulas of LTL and they can choose whether to use their influence power to make sure that their goal is satisfied. Classical solution concepts such as weak dominance and winning strategy are studied for influence games, in relation to the structure of the network and the goals of the agents. Finally, we introduce a novel class of concurrent game structures (CGS) in which agents can have shared control over a set of propositional variables. Such structures are used for the interpretation of formulas of alternating-time temporal logic, thanks to which we can express the existence of a winning strategy for an agent in a repeated game (as, for instance, the influence games mentioned above). The main result shows by means of a clever construction that a CGS with shared control can be represented as a CGS with exclusive control. In conclusion, this thesis provides a valuable contribution to the field of collective decision-making by introducing a novel framework of voting based on individual propositional goals, it studies for the first time the aggregation of generalized CP-nets, it extends a framework of opinion diffusion by modelling rational agents who use their influence power as they see fit, and it provides a reduction of shared to exclusive control in CGS for the interpretation of logics of strategic reasoning. By using different logical languages, agents can thus express their goals and preferences over the decision to be taken, and desirable properties of the decision process can be ensured.
The increasing utilization of sensor devices in addition to human-given data make it possible to capture real world systems complexity through rich temporal descriptions. One core characteristic of such configurations is heterogeneity that appears at different levels of the data generation process: data sources, time models and data models. In such context, one challenging task for monitoring systems is to discover non-trivial temporal knowledge that is directly actionable and suitable for human interpretation. In this thesis, we firstly propose to use a Temporal Abstraction (TA) approach to express information given by heterogeneous raw data streams with a unified interval-based representation, called state streams. Such approach solves problems introduced by heterogeneity, provides a high level pattern vocabulary and also permits also to integrate expert(s) knowledge into the discovery process. Second, we introduced the Complex Temporal Dependencies (CTD) that is a quantitative interval-based pattern model. Third, we proposed CTD-Miner a first efficient CTD mining framework. CTD-Miner performs an incremental dependency construction.
This research focuses on the lexicological description of pragmatemes. They are non-free compositional phrasems constrained by the context of communication in which they are used. In this work we adopt a contrastive French-Spanish approach. The most common expressions of everyday life involve many constraints of which we are not aware. Greeting someone with a Hello!, or finishing a letter by Sincerely, Regards, does not represent any difficulty for a native speaker. These utterances, which appear very simple in terms of their content, their form and the contexts of ordinary life in which they are used, are very singular. They are ritually used in everyday situations to which they are prototypically associated. Pragmatemes often go unnoticed in the language, as phraseological units, and it is during the translation that we realize that they can not be translated literally into another language. We must find an equivalent expression. There is a link between pragmatemes and culture. Within a linguistic community,the speakers understand each other because they share a linguistic and cultural competence. However, in communicating in a foreign language, we must consider the cultural elements that condition the situation in which the exchange takes place.
Vertical search engines, which focus on a specific segment of the Web, become more and more present in the Internet landscape. Topical search engines, notably, can obtain a significant performance boost by limiting their index on a specific topic. In this thesis, we tackle the first inevitable step of a all topical search engine: focused document gathering from the Web. A thorough study of the state of art leads us to consider two strategies to gather topical documents from the Web: either relying on an existing search engine index (focused search) or directly crawling the Web (focused crawling).The first part of our research has been dedicated to focused search. In this context, a standard approach consists in combining domain-specific terms into queries, submitting those queries to a search engine and downloading top ranked documents. After empirically evaluating this approach over 340 topics, we propose to enhance it in two different ways: Upstream of the search engine, we aim at formulating more relevant queries in order to increase the precision of the top retrieved documents. To do so, we define a metric based on a co-occurrence graph and a random walk algorithm, which aims at predicting the topical relevance of a query. Downstream of the search engine, we filter the retrieved documents in order to improve the document collection quality. We do so by modeling our gathering process as a tripartite graph and applying a random walk with restart algorithm so as to simultaneously order by relevance the documents and terms appearing in our corpus. Then, we consider the problem of crawl frontier ordering, which is at the very heart of a focused crawler. Such ordering strategy allows the crawler to prioritize its fetches, maximizing the number of in-domain documents retrieved while minimizing the non relevant ones. We propose to apply learning to rank algorithms to efficiently order the crawl frontier, and define a method to learn a ranking function from existing crawls.
The last years have known an unprecedented growth in the use of mobile devices especially smartphones. They became omnipresent in our daily life because of the features they offer. They allow the user to install third-party apps to achieve numerous tasks. Smartphones are mostly governed by the Android operating system. Mobile apps collect a huge amount of data such as email addresses, contact list, geolocation, photos and bank account credentials. Consequently, Android has become a favorable target for cyber criminals. Thus, understanding the issue, i.e., how Android malware operates and how to detect it, became an important research challenge. Android malware frequently tries to bypass static analysis using multiple techniques such as code obfuscation and dynamic code loading. To overcome these limitations, many analysis techniques have been proposed to execute the app and monitor its behavior at runtime. Nevertheless, malware developers use time and logic bombs to prevent the malicious code from executing except under certain circumstances. Recent approaches try to automatically characterize the malicious behavior by identifying the most suspicious locations in the code and forcing them to execute. These approaches solely analyze the application code and miss the execution paths that occur when the application calls a framework method that in turn calls another application method. It also gives key information about the analyzed application in order to understand how the suspicious code was injected into the application. To validate our approach, we use GPFinder to study a collection of 14,224 malware samples, and we evaluate that 72.69% of the samples have at least one suspicious code location which is only reachable through implicit calls. Triggering approaches mainly use one of the following strategies to run a specific portion of the application's code: the first approach heavily modifies the app to launch the targeted code without keeping the original behavioral context. The second approach generates the input to force the execution flow to take the desired path without modifying the app's code. However, it is sometimes hard to launch a specific code location just by fuzzing the input. We propose in this dissertation a tool, TriggerDroid, that has a twofold goal: force the execution of the suspicious code and keep its context close to the original one. It crafts the required framework events to launch the right app component and satisfies the necessary triggering conditions to take the desired execution path. To validate our approach, we led an experiment on a dataset of 135 malware samples from 71 different families. Results show that our approach needs more refinement and adaptation to handle special cases due to the highly diverse malware dataset that we analyzed. Finally, we give a feedback on the experiments we led on different malware datasets, and we explain our experimental process. Finally, we present the Kharon dataset, a collection of well documented Android malware that can be used to understand the malware landscape.
The objective of this thesis is to develop semantic methods of reassembly in the complicated framework of heritage collections, where some blocks are eroded or missing. The reassembly of archaeological remains is an important task for heritage sciences: it allows to improve the understanding and conservation of ancient vestiges and artifacts. However, some sets of fragments cannot be reassembled with techniques using contour information or visual continuities. It is then necessary to extract semantic information from the fragments and to interpret them. These tasks can be performed automatically thanks to deep learning techniques coupled with a solver, i.e., a constrained decision making algorithm. This thesis proposes two semantic reassembly methods for 2D fragments with erosion and a new dataset and evaluation metrics. The first method, Deepzzle, proposes a neural network followed by a solver. The neural network is composed of two Siamese convolutional networks trained to predict the relative position of two fragments: it is a 9-class classification. The solver uses Dijkstra's algorithm to maximize the joint probability. Deepzzle can address the case of missing and supernumerary fragments, is capable of processing about 15 fragments per puzzle, and has a performance that is 25% better than the state of the art. The second method, Alphazzle, is based on AlphaZero and single-player Monte Carlo Tree Search (MCTS). It is an iterative method that uses deep reinforcement learning: at each step, a fragment is placed on the current reassembly. Two neural networks guide MCTS: an action predictor, which uses the fragment and the current reassembly to propose a strategy, and an evaluator, which is trained to predict the quality of the future result from the current reassembly. Alphazzle takes into account the relationships between all fragments and adapts to puzzles larger than those solved by Deepzzle. Moreover, Alphazzle is compatible with constraints imposed by a heritage framework: at the end of reassembly, MCTS does not access the reward, unlike AlphaZero. Indeed, the reward, which indicates if a puzzle is well solved or not, can only be estimated by the algorithm, because only a conservator can be sure of the quality of a reassembly.
Surface realisation is a subtask of natural language generation. It may be viewed as the inverse of parsing, that is, given a grammar and a representation of meaning, the surface realiser produces a natural language string that is associated by the grammar to the input meaning. This thesis presents three extensions to GenI, a realisation algorithm for Feature-Based Tree Adjoining Grammar (FB-LTAG). The first extension improves the efficiency of the realiser with respect to lexical ambiguity. It is an adaptation from parsing of the “electrostatic tagging” optimisation, in which lexical items are associated with a set of polarities, and combinations of those items with non-neutral polarities are filtered out. The second extension deals with the number of outputs returned by the realiser. Normally, the GenI algorithm returns all of the sentences associated with the input logical form. Whilst these inputs can be seen as having the same core meaning, they often convey subtle distinctions in emphasis or style. The extension builds off the fact that the FB-LTAG grammar used by the generator was constructed from a “metagrammar”, explicitly putting to use the linguistic generalisations that are encoded within. The final extension provides a means for the realiser to act as a metagrammar-debugging environment. Mistakes in the metagrammar can have widespread consequences for the grammar. Since the realiser can output all strings associated with a semantic input, it can be used to find out what these mistakes are, and crucially, their precise location in the metagrammar.
Our contributions cover three different applications but share a common denominator: the extraction of relevant user representations. Our first application is the item recommendation task, where recommender systems build user and item profiles out of past ratings reflecting user preferences and item characteristics. Nowadays, textual information is often together with ratings available and we propose to use it to enrich the profiles extracted from the ratings. Our hope is to extract from the textual content shared opinions and preferences. The models we propose provide another opportunity: predicting the text a user would write on an item. Our second application is sentiment analysis and, in particular, polarity classification. Our idea is that recommender systems can be used for such a task. Recommender systems and traditional polarity classifiers operate on different time scales. We propose two hybridizations of these models: the former has better classification performance, the latter highlights a vocabulary of surprise in the texts of the reviews. The third and final application we consider is urban mobility. It takes place beyond the frontiers of the Internet, in the physical world. Using authentication logs of the subway users, logging the time and station at which users take the subway, we show that it is possible to extract robust temporal profiles.
Word Sense Disambiguation (WSD), which is a central task in natural language processing and that can improve applications such as machine translation or information extraction. Researches in word sense disambiguation predominantly concern the English language, because the majority of other languages lacks a standard lexical reference for the annotation of corpora, and also lacks sense annotated corpora for the evaluation, and more importantly for the construction of word sense disambiguation systems. In English, the lexical database wordnet is a long-standing de-facto standard used in most sense annotated corpora and in most WSD evaluation campaigns. Our contribution to this thesis focuses on several areas:first of all, we present a method for the automatic creation of sense annotated corpora for any language, by taking advantage of the large amount of wordnet sense annotated English corpora, and by using a machine translation system. This method is applied on Arabic and is evaluated, to our knowledge, on the only Arabic manually sense annotated corpus with wordnet: the Arabic OntoNotes 5.0, which we have semi-automatically enriched. Its evaluation is performed thanks to an implementation of two supervised word sense disambiguation systems that are trained on the corpora produced using our method. We hence propose a solid baseline for the evaluation of future Arabic word sense disambiguation systems, in addition to sense annotated Arabic corpora that we provide as a freely available resource. Secondly, we propose an in vivo evaluation of our Arabic word sense disambiguation system by measuring its contribution to the performance of the machine translation task.
The detection of block structures in matrices is an important challenge. First in data analysis where matrices are a key tool for data representation, as data tables or adjacency matrices. Indeed, for the first one, finding a co-clustering is equivalent to finding a row and column block structure of the matrix. For the second one, finding a structure of diagonal dominant blocks leads to a clustering of the data. In this dissertation, we focus our analysis on the detection of dominant diagonal block structures by symmetrically permuting the rows and columns of matrices. Lots of algorithms have been designed that aim to highlight such structures. The first one consists of algorithms that first project the matrix rows onto a low-dimensional space generated by the matrix leading eigenvectors, and then apply a procedure such as a k-means on the reduced data. Their main drawbacks is that the knowledge of number of clusters to uncover is required. The second kind consists of iterative procedures that look for the k-th best partition into two subblocks of the matrix at step k. However, if the matrix structure shows more than two blocks, the best partition into two blocks may be a poor fit to the matrix groundtruth structure. Hence, we propose a spectral algorithm that deals with both issues described above. To that end, we preprocess the matrix with a doubly-stochastic scaling, which leverages the blocks. First we show the benefits of using such a scaling by using it as a preprocessing for the Louvain's algorithm, in order to uncover community structures in networks. We also investigate several global modularity measures designed for quantifying the consistency of a block structure. We generalise them to make them able to handle doubly-stochastic matrices, and thus we remark that our scaling tends to unify these measures. Then, we describe our algorithm that is based on spectral elements of the scaled matrix. Our method is built on the principle that leading singular vectors of a doubly-stochastic matrix should have a staircase pattern when their coordinates are sorted in the increasing order, under the condition that the matrix shows a hidden block structure. Tools from signal processing-that have been initially designed to detect jumps in signals-are applied to the sorted vectors in order to detect steps in these vectors, and thus to find the separations between the blocks. However, these tools are not specifically designed to this purpose. Hence procedures that we have implemented to answer the encountered issues are also described. We then propose three applications for the matrices block structure detection. For these applications, we compare the results of our algorithm with those of algorithms that have been designed on purpose. Finally, we deal with the dialogue act detection in a discorsre, using the STAC database that consists in a chat of online players of " The Settlers of Catan ". To that end we connect classical clustering algorithms with a BiLSTM neural network taht preprocesses the dialogue unities. Finally, we conclude by giving some preliminary remarks about the extension of our method to rectangular matrices.
This dissertation explores the topic of generative modelling of natural images,which is the task of fitting a data generating distribution. Such models can be used to generate artificial data resembling the true data, or to compress images. Latent variable models, which are at the core of our contributions, seek to capture the main factors of variations of an image into a variable that can be manipulated. Unfortunately these models struggle to capture all the modes of the original distribution, ie they do not cover the full variability of the dataset. Conversely, likelihood based models such as VAEs typically cover the full variety of the data well and provide an objective measure of coverage. However these models produce samples of inferior visual quality that are more easily distinguished from real ones. The work presented in this thesis strives for the best of both worlds: to obtain compelling samples while modelling the full support of the distribution. We propose a training procedure relying on an auxiliary loss function to control what information is captured by the latent variables and what information is left to an autoregressive decoder. Unlike previous approaches to such hybrid models, ours does not need to restrict the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables. The second contribution builds on the standard GAN model, which trains a discriminator network to provide feedback to a generative network. The discriminator usually assesses the quality of individual samples, which makes it hard to evaluate the variability of the data. Instead we propose to feed the discriminator with emph{batches} that mix both true and fake samples, and train it to predict the ratio of true samples in the batch. In our third contribution, we show that usual parametric assumptions made in VAEs induce a conflict between them, leading to lackluster performance of hybrid models. We propose a solution based on deep invertible transformations, that trains a feature space in which usual assumptions can be made without harm. Our approach provides likelihood computations in image space while being able to take advantage of adversarial training. It obtains GAN-like samples that are competitive with fully adversarial models while improving likelihood scores over existing hybrid models at the time of publication, which is a significant advancement.
This thesis focuses on acoustic model structuring for improving HMM-Based automatic speech recognition. The structuring relies on unsupervised clustering of speech utterances of the training data in order to handle speaker and channel variability. The idea is to split the data into acoustically similar classes. In conventional multi-Modeling (or class-Based) approach, separate class-Dependent models are built via adaptation of a speaker-Independent model. When the number of classes increases, less data becomes available for the estimation of the class-Based models, and the parameters are less reliable. One way to handle such problem is to modify the classification criterion applied on the training data, allowing a given utterance to belong to more than one class. This is obtained by relaxing the classification decision through a soft margin. This is investigated in the first part of the thesis. In the main part of the thesis, a novel approach is proposed that uses the clustered data more efficiently in a class-Structured GMM. Instead of adapting all HMM-GMM parameters separately for each class of data, the class information is explicitly introduced into the GMM structure by associating a given density component with a given class. To efficiently exploit such structured HMM-GMM, two different approaches are proposed. The first approach combines class-Structured GMM with class-Dependent mixture weights. In this model the Gaussian components are shared across speaker classes, but they are class-Structured, and the mixture weights are class-Dependent. For decoding an utterance, the set of mixture weights is selected according to the estimated class. In the second approach, the mixture weights are replaced by density component transition probabilities. The approaches proposed in the thesis are analyzed and evaluated on various speech data, which cover different types of variability sources (age, gender, accent and noise)
Documentary production in a professional context often involves a revising process in which documents need to be proofread before validation and publication. This important task faces new challenges when dealing with digital documents. As an advanced digital writing technology, XML publishing chains are a relevant framework for studying proofreading of digital documents. Part of the contribution presented here has led to the development of prototypes that have been experimented in the use of Scenari publishing chains in a pedagogical context. These prototypes rely on linear proofreading views allowing in particular the comparison between two versions of the document based on a diff algorithm.
This thesis focuses on two Natural Language Processing tasks that require to extract semantic information from raw texts: Sentiment Analysis and Text Summarization. Accordingly, this dissertation is composed of two parts: the first part (Neural Sentiment Analysis) deals with the computational study of people's opinions, sentiments, and the second part (Neural Text Summarization) tries to extract salient information from a complex sentence and rewrites it in a human-readable form. Neural Sentiment Analysis. Similar to computer vision, numerous deep convolutional neural networks have been adapted to sentiment analysis and text classification tasks. However, unlike the image domain, these studies are carried on different input data types and on different datasets, which makes it hard to know if a deep network is truly needed. We thus propose a new adaptation of the deepest convolutional architecture (DenseNet) for text classification and study the importance of depth in convolutional models with different atom-levels (word or character) of input. Besides, to further improve sentiment classifiers and contextualize them, we propose to model them jointly with dialog acts, which are a factor of explanation and correlate with sentiments but are nevertheless often ignored. We have manually annotated both dialogues and sentiments on a Twitter-like social medium, and train a multi-task hierarchical recurrent network on joint sentiment and dialog act recognition. We show that transfer learning may be efficiently achieved between both tasks, and further analyze some specific correlations between sentiments and dialogues on social media. Neural Text Summarization. Detecting sentiments and opinions from large digital documents does not always enable users of such systems to take informed decisions, as other important semantic information is missing. People also need the main arguments and supporting reasons from the source documents to truly understand and interpret the document. To capture such information, we aim at making the neural text summarization models more explainable. We propose a model that has better explainability properties and is flexible enough to support various shallow syntactic parsing modules. More specifically, we linearize the syntactic tree into the form of overlapping text segments, which are then selected with reinforcement learning (RL) and regenerated into a compressed form. Hence, the proposed model is able to handle both extractive and abstractive summarization. We thus provide a detailed comparison of both RL-based and syntax-aware approaches and of their combination along several dimensions that relate to the perceived quality of the generated summaries such as number of repetitions, sentence length, distribution of part-of-speech tags, relevance and grammaticality. We show that when there is a resource constraint (computation and memory), it is wise to only train models with RL and without any syntactic information, as they provide nearly as good results as syntax-aware models with less parameters and faster training convergence.
Since progress in automatic natural language analysis has made it possible to support fully finalized dialogue in a fully automatic style (e.g. travel reservation [1]), the systems are able to answer to questions about complex tasks and more open areas (eg IBM's WATSON system that won the jeopardy game show in 2011 [2]). Recently, standalone chat agents (known as chatbots) have become essential elements in customer relationship management [3]. In this context, the conversational agent function evolves more and more towards that of a virtual adviser from whom we expect an increasingly intelligent behavior going beyond the collection of information or providing simple answers, but also being able to make decisions such as taking actions and in particular recognizing one's own limitations and knowing when to hand over to a human advisor in order to preserve the quality of service. Technological lock A large number of chatbots exist on the market, from simple ones often elaborated with weakly contextual rules, to more complex based on machine learning. The former: quickly become silent if one goes beyond the limits of their field of competence, while the latter are capable of a little more generalization, but this is usually paid at the price of greater inaccuracy in their answers. Regardless of the type of system considered, everything is based on knowledge, that is, the memory of the conversational agent. In humans, memory is divided into several types [4] for example: episodic (lived events), semantics (knowledge), procedural (know-how), perceptive (recognition of voices, smells, etc.) or work (short-term notepad). It can be of explicit nature (conscious access and restitution), implicit (subconscious, emotions) or autobiographical (personal combining semantic and episodic memory). But these characteristics of the memory of an autonomous agent condition the representations that he constructs [5] and therefore its actions. In addition to the conversational agent's memory functions and their impact on the management of the dialogue, the scientific investigations will address the problem of adaptation to the application domain1 [8] and will have to address recent advances in machine learning (deep neural approaches[6]), this at different levels of granularity (analysis of a statement, acquisition of background knowledge from corpus etc.). The validation of the intermediate experimental results will be carried out through the evaluation of demonstrators (prototype dialogue system) according to a corpus-based quantitative evaluation procedure with a set of performance measures defined from the classical evaluation protocols. for the domain [7].
Statistical learning aims to modelize a functional link between two variables X and Y thanks to a random sample of realizations of the couple (X,Y). When the variable Y takes a binary number of values, learning is named classification and learn the functional link is equivalent to learn the boundary of a manifold in the feature space of the variable X. In this PhD thesis, we are placed in the context of active learning, i.e. we suppose that learning sample is not random and that we can, thanks to an oracle, generate points for learning the manifold. In the case where the variable Y is continue (regression), previous works show that criterion of low discrepacy to generate learning points is adequat. We show that, surprisingly, this result cannot be transfered to classification talks. In this PhD thesis, we propose the criterion of dispersion for classification problems. This criterion being difficult to realize, we propose a new algorithm to generate low dispersion samples in the unit cube. After a first approximation of the manifold, successive approximations can be realized in order to refine its knowledge. Two methods of sampling are possible: the «selective sampling» which selects points to present to the oracle in a finite set of candidate points, and the «adaptative sampling» which allows to select any point in the feature space of the variable X. The second sampling can be viewed as the infinite limit of the first. Nevertheless, in practice, it is not reasonable to use this method. Then, we propose a new algorithm, based on dispersion criterion, leading both exploration and exploitation to approximate a manifold.
From XVth to XVIIth century, Portugal has been ranking first among the most advanced nations of its time. But any contact with peoples and cultures has always been a source of various and multifaceted reciprocal influences. In this thesis, we will study the lusitanian impressions in the Guinea Gulf. The research has been conducted in the southern part of Ivory Coast, Ghana, Togo and Benin and is based on a corpus made up of some hundreds of words that we have listed in a bibliography and an investigation that we have carried out in the field during eight years. The analysis of data is done according to a bipolar method which combines history and structuralism in its contrastive approach because, in reality, we are comparing two linguistic systems: portuguese, kru and kwa languages of the Niger-Congo family.
The entropy of a probability distribution on a set of discrete random variables is always bounded by the entropy of its factorisable counterpart. This is due to the submodularity of entropy on the set of discrete random variables. Submodular functions are also generalisation of matroid rank function; therefore, linear functions may be optimised on the associated polytopes exactly using a greedy algorithm. In this manuscript, we exploit these links between the structures of graphical models and submodular functions: we use greedy algorithms to optimise linear functions on the polytopes related to graphic and hypergraphic matroids for learning the structures of graphical models, while we use inference algorithms on graphs to optimise submodular functions. The first main contribution of the thesis aims at approximating a probabilistic distribution with a factorisable tractable distribution under the maximum likelihood framework. Since the tractability of exact inference is exponential in the treewidth of the decomposable graph, our goal is to learn bounded treewidth decomposable graphs, which is known to be NP-hard. We pose this as a combinatorial optimisation problem and provide convex relaxations based on graphic and hypergraphic matroids. This leads to an approximate solution with good empirical performance. As third contribution, we propose and analyse algorithms aiming at minimizing submodular functions that can be written as sum of simple functions. Our algorithms only make use of submodular function minimisation and total variation oracles of simple functions.
In this thesis, we approach these questions by defining and implementing a multi-scale model for music segment structure description, called Polytopic Graph of Latent Relations (PGLR). In our work, a segment is the macroscopic constituent of the global piece. Under the PGLR scheme, relationships between musical elements within a musical segment are assumed to be developing predominantly between homologous elements within the metrical grid at different scales simultaneously. This approach generalises to the multi-scale case the System&amp;Contrast framework which aims at describing, as a 2×2 square matrix, the logical system of expectation within a segment and the surprise resulting from that expectation. Each vertex in the polytope corresponds to a low-scale musical element, each edge represents a relationship between two vertices and each face forms an elementary system of relationships. The aim of the PGLR model is to both describe the time dependencies between the elements of a segment and model the logical expectation and surprise that can be built on the observation and perception of the similarities and differences between elements with strong relationships. The approach is presented conceptually and algorithmically, together with an extensive evaluation of the ability of different models to predict unseen data, measured using the cross-perplexity value. Our results illustrate the efficiency of the proposed model in capturing structural information within such data.
Cultural heritage is the legacy of physical artefacts and intangible attributes of a group or society that is inherited from past generations. Vases are among the most iconic objects of cultural heritage. Although some of these collections have been digitised, they are rarely accessible in an open format and remain isolated. In addition, the lack of clearly identified terminologies is an obstacle to communication and knowledge sharing. Our work aims to respond to this issue by implementing practices drawn from the semantic web and knowledge engineering, and more particularly by building in a W3C format an ontology dedicated to the Chinese vases of the Ming and Qing dynasties. The construction of the TAO CI ("ceramic" in Chinese) ontology respects the experts'way of thinking in their conceptualization of the field, and takes into account the international standards in Terminology (ISO 1087 and ISO 704). Both approaches are based on the notion of essential characteristic and define a concept as a unique combination of characteristics. The search for differences between objects, combined with a morphological analysis of Chinese terms whose characters carry meaning in relation to knowledge of the field, allows to identify essential characteristics. The definition of concept is based on the idea that a concept is a set of essential characteristics stable enough to be named in language. We have thus proposed a specific method for building ontologies guided by the terms and essential characteristics of the domain. We have introduced new terms (neologisms) in English and concepts without any designation in language for ontology structuring purposes. The construction of the ontology was done using Protégé, the most widely used environment for building ontologies in the W3C format (RDF/OWL). The terminological dimension was reduced, as is often the case, to annotations (in SKOS, RDFS) on the concepts. The TAO CI ontology is linked to external resources such as CIDOC CRM and ATT Getty for the conceptual part, and to museums for the objects. Finally, the TAO CI ontology was evaluated from the point of view of the domain (coverage) and its implementation. The ontology is in open access at the following address: http://www.dh.ketrc.com/otcontainer/data/OTContainer.owlThe last phase of the project consisted in the creation of a dedicated website. This site provides access to the different resources of the project and in particular to a bilingual (English, Chinese) electronic dictionary of the vases of the Ming and Qing dynasties. The dictionary entries correspond to the OWL classes of the ontology: http://www.dh.ketrc.com/The TAO CI ontology is, to our knowledge, the first open and reusable ontology in the format of the semantic web of Chinese ceramic vases. It is an illustration of an approach guided by terms and essential characteristics that can be applied to the construction of ontologies in other areas of Chinese cultural heritage.
Emotion recognition is one of the most complex scientific domains. In the last few years, various emotion recognition systems are developed. We focus on facial emotion recognition specially the six basic emotions namely happiness, anger, fear, disgust, sadness and surprise. A comparative study between geometric method and appearance method is performed on CK+ database as the posed emotion database, and FEEDTUM database as the spontaneous emotion database. We consider different constraints in this study such as different image resolutions, the low number of labelled images in learning step and new subjects. We evaluate afterward various fusion schemes on new subjects, not included in the training set. Good recognition rate is obtained for posed emotions (more than 86%), however it is still low for spontaneous emotions. These ones increase spontaneous emotions recognition rates.
Analysis of textual data is facilitated by the use of text mining (TM) allowing to automate content analysis, and is implemented in several application in healthcare. These include the use of TM to explore the content of posts shared online. We performed a systematique literature review to identify the application of TM in psychiatry. In addition, we used TM to explore users' concerns of an online forum dedicated to antidepressants and anxiolytics between 2013 and 2015 analysing words frequency, cooccurences, topic models (LDA) and popularity of topics. The four TM applications in psychiatry retrieved are the analysis of patients'narratives (psychopathology), feelings expressed online, content of medical records, and biomedical literature screening. Four topics are identified on the forum: withdrawals (most frequent), escitalopram, anxiety related to treatment effect and secondary effects. While concerns around secondary effects of treatment declined, questions about withdrawals effects and changing medication increased related to several antidepressants.
The goal of this thesis dissertation is to show that, contrary to preconceived ideas, one can efficiently take advantage of low frequency words in natural language processing. We use them in sub-sentential alignment, which constitutes the first step of most data-driven machine translation systems (statistical or example-based machine translation). We show that rare words can be used as a foundation in the design of a multilingual sub-sentential alignment method, using differential techniques similar to those found in example-based machine translation. This method is truly multilingual, in that sense, it allows the simultaneous processing of any number of languages. Moreover, it is very simple, anytime, and scales up naturally. We compare our implementation, Anymalign, to two statistical tools proven in the domain. Although its current results are in average slightly behind those of state of the art methods in phrase-based statistical machine translation, we show that the intrinsic quality of our lexicons is actually superior to that of lexicons produced by state of the art methods.
In the study of the use of referring expressions, numerous studies have looked into the relationship between linguistic forms and informationally defined functions, and have put forth the early sensitivity of young children to certain aspects of information structuring, notably with regard to attentional status of referents and the topic-comment dimension. Interactional linguistics have adopted a complementary approach on referring expressions and have shown how speakers signal and accomplish, by their choice of a referring expression, not only reference, but also various tasks pertaining to the management of interaction proper. Our study aims to shed further light, in a multidimensional perspective, on the use of referring expressions, and especially on the contrast between week, strong and dislocated forms, by studying two languages which exploit different linguistic means to mark sentence topic. Our analysis is based on a cross-sectional database of 12 French- and German-speaking children, aged 2 to 3 years. We analyzed the forms and uses of referring expressions in the children's and the adult's speech, by considering morph-syntactic, informational and interactional factors. Results show the complementary nature of these different factors when explaining referring expression use in children and in adults. We were also able to put forth specific uses of certain linguistic means, such as French dislocations, German demonstrative der/die/das and null forms, and we described their functioning within interactional routines that aid the young children to join in on discourse and interaction.
This thesis presents Adetoa, a system designed to automatically locate temporal expressions in Web pages and tag them with semantic annotations, in the field of e-tourism. A detailed linguistic study has revealed that the expression of temporal information in Web tourism pages is complex and has specific properties. These analyses have led to the development of a large number of transducers (under Unitex) for the extraction and mark-up tasks. Other tourist information is also extracted, such as tourist objects and addresses. Linking transducers have been developed to group all the information concerning one tourist destination. The annotation scheme is based on a tourism ontology but is not a direct replica, thus enabling the expressions to be accurately characterized on a linguistic level. The ontology has then been adapted accordingly, so that the information can more easily be included in the corresponding knowledge base. The evaluation of Adetoa, which is detailed in the last chapter, showed satisfying results, both on a theoretical level and for industrial purposes.
Brain-Computer Interface (BCI) allows communication between a user and a machine, by converting the user's brain activity into commands that control external devices. Many limitations prevent the diffusion of BCI systems in real applications, such as the calibration phase that is a consequence of the issue of variability across sessions and among users. The calibration phase is fundamental because it allows to set the main parameters to extract the relevant information from the electroencephalograpy (EEG) signal of the subject, but it is considered time consuming and tedious for the user. The objective of this thesis is to overcome these limitations by novel methods based on the improvement or even replacement of the traditional calibration phase, proposing the development of a user-centered BCI system. Firstly, we present a design to develop an adaptive BCI system for two different applications. The former deals with a code-modulated Visual Evoked Potential (c-VEP) speller where an adaptive parameter setting phase is proposed to replace the standard calibration phase. The latter application concerns the development of a Mental Imagery (MI) BCI for a disabled user, characterized by a long user-centered multi-stage training phase, in the context of a international BCI competition. The proposed methods showed promising results and open new perspectives to the diffusion of BCI
Clinical data are produced as part of the practice of medicine by different health professionals, in several places and in various formats. They therefore present an heterogeneity both in terms of their nature and structure and are furthermore of a particularly large volume, which make them considered as Big Data. The work carried out in this thesis aims at proposing an effective information retrieval method within the context of this type of complex and massive data. First, the access to clinical data constrained by the need to model clinical information. This can be done within Electronic Health Records and, in a larger extent, within data Warehouses. In this thesis, I proposed a proof of concept of a search engine allowing the access to the information contained in the Semantic Health Data Warehouse of the Rouen University Hospital. In order to provide search functionalities adapted to this generic representation of data, a query language allowing access to clinical information through the various entities of which it is composed has been developed and implemented as a part of this thesis's work. Second, the massiveness of clinical data is also a major technical challenge that hinders the implementation of an efficient information retrieval. The initial implementation of the proof of concept highlighted the limits of a relational database management systems when used in the context of clinical data. A migration to a NoSQL key-value store has been then completed. Finally, the contribution of this work within the general context of the Semantic Health Data Warehouse of the Rouen University Hospital was evaluated. The proof of concept proposed in this work was used to access semantic descriptions of information in order to meet the criteria for including and excluding patients in clinical studies. In this evaluation, a total or partial response is given to 72.97% of the criteria. In addition, the genericity of the tool has also made it possible to use it in other contexts such as documentary and bibliographic information retrieval in health.
In natural language processing, each analysis step has improved the way in which language can be modeled by machines. Another step of analysis still poorly mastered resides in semantic parsing. This type of analysis can provide information which would allow for many advances, such as better human-machine interactions or more reliable translations. There exist several types of meaning representation structures, such as PropBank, AMR and FrameNet. FrameNet corresponds to the frame semantic framework whose theory has been described by Charles Fillmore (1971). In this theory, each prototypical situation and each different elements involved are represented in such a way that two similar situations are represented by the same object, called a semantic frame. The work that we will describe here follows the work already developed for machine prediction of frame semantic representations. We will present four prediction systems, and each one of them allowed to validate another hypothesis on the necessary properties for effective prediction. We will show that semantic parsing can also be improved by providing prediction models with refined information as input of the system, with firstly a syntactic analysis where deep links are made explicit and secondly vectorial representations of the vocabulary learned beforehand.
Requirements analysis is the first step of the design process. It is also an important source of innovation in companies, particularly when it is shared and fulfilled by the multidisciplinary design team. The prospection and anticipation of future needs are therefore an important challenge for the development of new products adapted to their user. The goal of this thesis is to optimize the anticipation of future needs in order to foster innovation. Our assumptions include methodological and technological factors to improve the collaboration of the multidisciplinary team and the performance of requirements anticipation. These assumptions are operationalized through three different ways of running the Persona method and tested in the context of three industrial projects. We show that a method combining several reasoning modes adapted to the various professional backgrounds in the multidisciplinary team,the anticipation of needs is improved quantitatively and qualitatively (i.e. usefulness assessed by users). We also show that the technological support plays an important role in the effectiveness of methods: a collaborative and playful technology such as an interactive tabletop can increase the number of strategic ideas for the company (i.e. useful and technically feasible); an immersive and playful technology such as a virtual world can shape needs anticipation inaccordance with the project priorities (Techno-Centered or User-Centered). These results open many opportunities for the methodological and technological evolution of front-end innovation phases towards the anticipation of future needs.
This thesis focuses on speaker voice transformation in the aim to indicate the distance of it: a spoken-to-whispered voice transformation to indicate a close distance and a spoken-to-shouted voice transformation for a rather far distance. We perform at first, in-depth analysis to determine most relevant features in whispered voices and especially in shouted voices (much harder). The main contribution of this part is to show the relevance of prosodic parameters in the perception of vocal effort in a shouted voice. Then, we propose some descriptors to better characterize the prosodic contours. For the actual transformation, we propose several new transformation rules which importantly control the quality of transformed voice. The results showed a very good quality of transformed whispered voices and transformed shouted voices for relatively simple linguistic structures (CVC, CVCV, etc.).
Decision-making is a highly researched field in science, be it in neuroscience to understand the processes underlying animal decision-making, or in robotics to model efficient and rapid decision-making processes in real environments. In neuroscience, this problem is resolved online with sequential decision-making models based on reinforcement learning. In robotics, the primary objective is efficiency, in order to be deployed in real environments. However, in robotics what can be called the budget and which concerns the limitations inherent to the hardware, such as computation times, limited actions available to the robot or the lifetime of the robot battery, are often not taken into account at the present time. We propose in this thesis to introduce the notion of budget as an explicit constraint in the robotic learning processes applied to a localization task by implementing a model based on work developed in statistical learning that processes data under explicit constraints, limiting the input of data or imposing a more explicit time constraint. In this context, the alternation between information retrieval for location and the decision to move for a robot may be indirectly linked to the notion of exploration-exploitation compromise.
Social conventions are learned mostly at a young age, but are quite different from other domains, like for example sensorimotor skills. The first people to define conventions just picked an arbitrary alternative between several options: a side of the road to drive on, the design of an electric plug, or inventing a new word. Because of this, while setting a new convention in a population of interacting individuals, many competing options can arise, and lead to a situation of growing complexity if many parallel inventions happen. How do we deal with this issue?Humans often exhert an active control on their learning situation, by for example selecting activities that are neither too complex nor too simple. This behavior, in cases like sensorimotor learning, has been shown to help learn faster, better, and with fewer examples. Could such mechanisms also have an impact on the negotiation of social conventions? A particular example of social convention is the lexicon: which words we associated with given meanings. Computational models of language emergence, called the Language Games, showed that it is possible for a population of agents to build a common language through only pairwise interactions. In particular, the Naming Game model focuses on the formation of the lexicon mapping words and meanings, and shows a typical burst of complexity before starting to discard options and find a final consensus. Several strategies were introduced, and have a different impact on both the time needed to converge to a consensus and the amount of memory needed by individual agents. Firstly, we artificially constrain the memory of agents to avoid the local complexity burst. A few strategies are presented, some of which can have similar convergence speed as in the standard case. Secondly, we formalize what agents need to optimize, based on a representation of the average state of the population. A couple of strategies inspired by this notion help keep the memory usage low without having constraints, but also result in a faster convergence process. We then show that the obtained dynamics are close to an optimal behavior, expressed analytically as a lower bound to convergence time. Eventually, we designed an online user experiment to collect data on how humans would behave in the same model, which shows that they do have an active topic choice policy, and do not choose randomly. Contributions from this thesis also include a classification of the existing Naming Game models and an open-source framework to simulate them.
In this context, we aimed at enhancing the quality of the interaction between users and Embodied Conversational Agents ECAs by (1) endowing the ECA with the capacity to express social attitudes, such as being friendly or dominant depending its role or relationship with its interaction partners; (2) adapting the agent's behavior according to the user's behavior, hence, the conversation partners influence each others through an interaction loop, thus, enhancing the interaction quality; (3) predicting the user's engagement level and adapting the agent's behavior accordingly. We take advantage of the recent advances in machine learning, more specifically, temporal sequence mining and neural networks to model these capacities in the ECA. The first model is used to learn relevant patterns (sequences) of non-verbal signals that best represent attitude variations, and then reproduce them on the agent. The latter is used to encompass the dynamics of non-verbal signals. Two use cases have been explored using the well-known LSTM model: agent's behavior adaptation based on both agent's and user's behavior history, and user's engagement prediction based on his/her own behavior history. The implemented models and algorithms have been validated through a number of perceptive studies as well as through rigorous quantitative analysis of the obtained results. In addition, the realized models have been integrated into a virtual-agent platform.
In recent years, the enterprise applications interoperability has become the leitmotiv of developers and designers in systems engineering. Most approaches to interoperability in the company have the primary objective of adjustment and adaptation of types and data structures necessary for the implementation of collaboration between companies. In the field of manufacturing, the product is a central component. Scientific works propose solutions taking into account information systems derived from products technical data throughout their life cycle. But this information is often uncorrelated. The management of product data (PDM) is commonly implemented to manage all information concerning products throughout their life cycle. However, these models are generally independent “islands” ignoring the problem of interoperability between applications that support these models. The objective of this thesis is to study the problem of interoperability applied to applications used in the manufacturing environment and to define a model of the ontological knowledge of enterprises related to the products they manufacture, based on technical data, ensuring the interoperability of enterprise systems. The outcome of this research concerns the formalization of a methodology for identifying a product-centric information system in the form of an ontology, for the interoperability of applications in manufacturing companies, based on existing standard such as ISO 10303 and IEC 62264.
Randomized controlled trials and systematic reviews are essential for the practice of evidence-based medicine (EBM). EBM will be compromised if the evidence issued from biomedical research proves to be biased. Many authors previously denounced the poor quality of research, but in 2009 I.Chalmers and P.Glasziou integrate these criticisms into a more global concept, waste of research. It could be defined as research that fails to help patients and their clinicians to make informed decisions as without accessible, honest and usable reports, research cannot help research end users. They estimated that it could represent up to 85% of health research. In this thesis, we focused on the avoidable waste of research in clinical trials. We had a particular interest in waste of research due to poor trial planning (trials methods or choice of outcomes) or to selective and incomplete reporting of outcomes. Then we estimated to what extent this waste could have been avoided by simple and inexpensive methodological adjustments. Our results suggest that 1) simple and inexpensive methodological adjustments could have limited the risk of bias in 50% of clinical trials, and thus partially reduce the burden of waste of research, and 2) that many trials did not measure or report completely the important outcomes, but that this waste could have been partially avoided for the majority of the trials.
On the other hand, interest in languages defined over large and infinite alphabets has increased in recent years. Although many theories and properties generalize well from the finite case, learning such languages is not an easy task. As the existing methods for learning regular languages depends on the size of the alphabet, a straightforward generalization in this context is not possible. In this thesis, we present a generic algorithmic scheme that can be used for learning languages defined over large or infinite alphabets, such as bounded subsets of N or R or Boolean vectors of high dimensions. We restrict ourselves to the class of languages accepted by deterministic symbolic automata that use predicates to label transitions, forming a finite partition of the alphabet for every state. Our learning algorithm, an adaptation of Angluin's L*, combines standard automaton learning by state characterization, with the learning of the static predicates that define the alphabet partitions. We use the online learning scheme, where two types of queries provide the necessary information about the target language. The first type, membership queries, answer whether a given word belongs or not to the target. We study language learning over large or infinite alphabets within a general framework but our aim is to provide solutions for particular concrete instances. For this, we focus on the two main aspects of the problem. Initially, we assume that equivalence queries always provide a counter-example which is minimal in the length-lexicographic order when the conjecture automaton is incorrect. Then, we drop this ``strong''equivalence oracle and replace it by a more realistic assumption, where equivalence is approximated by testing queries, which use sampling on the set of words. Such queries are not guaranteed to find counter-examples and certainly not minimal ones. All proposed algorithms have been implemented and their performance, as a function of automaton and alphabet size, has been empirically evaluated.
In many context of statistics it is common to rely on (generalized) linear model to perform prediction or variable selection. Nevertheless, it is often realistic to assume that second or even third order interactions of variables might help and improve the power of the decision taken. The same need for interactions is something occurring frequently in the context of online advertising, and especially in real time bidding/ Click Through Rate (CTR). Indeed, due to real time prediction constraints, despite many samples can be collected, only few variables are obtained by the players in this field, eg OS type, browser type, time, etc. Moreover, the signal to recover is weak, with only around 0.1% of click through rate (CTR) in the best cases. Hence, it might be beneficial to incorporate interactions to work with such drastic constraints. Last but not least, for categorical data, such interaction can model hierarchical refinements of the categories: this is for instance of high interest when dealing with natural language processing applications. Hence, proposing efficient methods for dealing with such scenarios might have a huge impact in various applications of machine learning. In the high dimension scenario we are aiming at, Lasso methods for interactions was proposed by Radchenko James(2010) and a variant designed to handle hierarchical interactions was studied by Bien Taylor Tibshirani (2013). So far we believe that such methods remained under used due to their computational limits. We believe that adapting the recent developments proposed by the last two teams mentioned, along with the refined active sets strategies investigated by Massias Gramfort Salmon(2018) could lead to implementations order of magnitude faster than the solutions currently available. On top of the intrinsic interest of such a normalization process, the interaction model we plan to address could be an interesting prototype for understanding batch normalization
A socially assistive robot (SAR) is meant to engage people into situated interaction such as monitoring physical exercise, neuropsychological rehabilitation or cognitive training. While the interactive behavioral policies of such systems are mainly hand-scripted, we discuss here key features of the training of multimodal interactive behaviors in the framework of the SOMBRERO project. In our work, we used learning by demonstration in order to provide the robot with adequate skills for performing collaborative tasks in human centered environments. There are three main steps of learning interaction by demonstration: we should (1) collect representative interactive behaviors from human coaches; (2) build comprehensive models of these overt behaviors while taking into account a priori knowledge (task and user model, etc.); and then (3) provide the target robot with appropriate gesture controllers to execute the desired behaviors. Multimodal HRI (Human-Robot Interaction) models are mostly inspired by Human-Human interaction (HHI) behaviors. Transferring HHI behaviors to HRI models faces several issues: (1) adapting the human behaviors to the robot's interactive capabilities with regards to its physical limitations and impoverished perception, action and reasoning capabilities; (2) the drastic changes of human partner behaviors in front of robots or virtual agents; (3) the modeling of joint interactive behaviors; (4) the validation of the robotic behaviors by human partners until they are perceived as adequate and meaningful. In this thesis, we study and make progress over those four challenges. In particular, we solve the two first issues (transfer from HHI to HRI) by adapting the scenario and using immersive teleoperation. We also build and evaluate a proof-of-concept autonomous robot to perform the tasks.
This thesis addresses problems of security in the French grid operated by RTE, the French ``Transmission System Operator''(TSO). Progress in sustainable energy, electricity market efficiency, or novel consumption patterns push TSO's to operate the grid closer to its security limits. To this end, it is essential to make the grid ``smarter''. To tackle this issue, this work explores the benefits of artificial neural networks. We propose novel deep learning algorithms and architectures to assist the decisions of human operators (TSO dispatchers) that we called “guided dropout”. This allows the predictions on power flows following of a grid willful or accidental modification. This is tackled by separating the different inputs: continuous data (productions and consumptions) are introduced in a standard way, via a neural network input layer while discrete data (grid topologies) are encoded directly in the neural network architecture. This architecture is dynamically modified based on the power grid topology by switching on or off the activation of hidden units. The main advantage of this technique lies in its ability to predict the flows even for previously unseen grid topologies. The "guided dropout" achieves a high accuracy (up to 99% of precision for flow predictions) with a 300 times speedup compared to physical grid simulators based on Kirchoff's laws even for unseen contingencies, without detailed knowledge of the grid structure. We also showed that guided dropout can be used to rank contingencies that might occur in the order of severity. In this application, we demonstrated that our algorithm obtains the same risk as currently implemented policies while requiring only 2% of today's computational budget. The ranking remains relevant even handling grid cases never seen before, and can be used to have an overall estimation of the global security of the power grid.
The importance of cultural heritage documentation increases in parallel with the risks to which it is exposed, such as wars, uncontrolled urban development, natural disasters, neglect and inappropriate conservation techniques or strategies. In addition, this documentation is a fundamental tool for the assessment, the conservation, and the management of cultural heritage. Consequently, this tool allows us to estimate the historical, scientific, social and economic value of this heritage. According to several international institutions dedicated to the preservation of cultural heritage, there is an urgent need to develop computer solutions to facilitate and support the documentation of poorly documented cultural heritage especially in developing countries where there is a lack of resources. Among these countries, Palestine represents a relevant case study in this issue of lack of documentation of its heritage. To address this issue, we propose an approach of knowledge acquisition and extraction in the context of poorly documented heritage. We take as a case study the church of the Nativity in Palestine and we put in place our theoretical approach by the development of a platform for the acquisition and extraction of heritage knowledge. Our solution is based on the semantic technologies, which gives us the possibility, from the beginning, to provide a rich ontological description, a better structuring of the information, a high level of interoperability and a better automatic processing without additional efforts. Therefore, the interaction between the two components of our system as well as the heritage knowledge develop and improve over time especially that our system uses manual contributions and validations of the automatic results (in both components) by the experts to optimize its performance.
Graph theory has long been studied in mathematics and probability as a tool for describing dependence between nodes. However, only recently it has been implemented on data, giving birth to the statistical analysis of real networks. The topology of economic and financial networks is remarkably complex: it is generally unobserved, thus requiring adequate inferential procedures for it estimation, moreover not only the nodes, but the structure of dependence itself evolves over time. Statistical and econometric tools for modelling the dynamics of change of the network structure are lacking, despite their increasing requirement in several fields of research. At the same time, with the beginning of the era of “Big data” the size of available datasets is becoming increasingly high and their internal structure is growing in complexity, hampering traditional inferential processes in multiple cases. This thesis aims at contributing to this newborn field of literature which joins probability, economics, physics and sociology by proposing novel statistical and econometric methodologies for the study of the temporal evolution of network structures of medium-high dimension.
We show that musically-trained children and young professional musicians outperform controls in a series of experiments, with faster brain plasticity and stronger functional connectivity, as measured by electroencephalography. By contrast, advantages for old adult musicians are less clear-cut, suggesting a limited impact of music training to counteract cognitive decline. Finally, young musicians show better long-term memory for novel words, which possibly contributes, along with better auditory perception and attention, to their advantage in word learning. By showing transfer effects from music training to semantic processing and long-term memory, results reveal the importance of domain-general cognitive functions and open new perspectives for education and rehabilitation.
The concept of automata, central to language theory, is the natural and efficient tool to apprehendvarious practical problems. The intensive use of finite automata in an algorithmic framework is illustrated by numerous researchworks. The correctness and the evaluation of performance are the two fundamental issues of algorithmics. A classic method to evaluate an algorithm is based on the controlled random generation of inputs. The work described in this thesis lies within this context and more specifically in the field of theuniform random generation of finite automata. This design builds on the symbolic method. Theoretical results and an experimental study are given. A random generator of non deterministic automata then illustrates the flexibility of the Markov ChainMonte Carlo methods (MCMC) as well as the implementation of the Metropolis-Hastings algorithm tosample up to isomorphism. A result about the mixing time in the general framework is given. The MCMC sampling methods raise the problem of the mixing time in the chain. By drawing on worksalready completed to design a random generator of partially ordered automata, this work shows howvarious statistical tools can form a basis to address this issue.
Recent computing trends have been advocating for more distributed paradigms, namely Fog computing, which extends the capacities of the Cloud at the edge of the network, that is close to end devices and end users in the physical world. The Fog is a key enabler of the Internet of Things (IoT) applications as it resolves some of the needs that the Cloud fails to provide such as low network latencies, privacy, QoS, and geographical requirements. For this reason, the Fog has become increasingly popular and finds application in many fields such as smart homes and cities, agriculture, healthcare, transportation, etc. The Fog, however, is unstable because it is constituted of billions of heterogeneous devices in a dynamic ecosystem. IoT devices may regularly fail because of bulk production and cheap design. When failures occur in such an ecosystem, the resulting inconsistencies in the application affect the physical world by inducing hazardous and costly situations. In this Thesis, we propose an end-to-end autonomic failure management approach for IoT applications deployed in the Fog. The approach manages IoT applications and is composed of four functional steps: (i) state saving, (ii) monitoring, (iii) failure notification,and (iv) recovery. Each step is a collection of similar roles and is implemented, taking into account the specificities of the ecosystem (e.g., heterogeneity, resource limitations). State saving aims at saving data concerning the state of the managed application. These include runtime parameters and the data in the volatile memory, as well as messages exchanged and functions executed by the application. Monitoring aims at observing and reporting information on the lifecycle of the application. When a failure is detected, failure notificationsare propagated to the part of the application which is affected by that failure. The propagation of failure notifications aims at limiting the impact of the failure and providinga partial service. In order to recover from a failure, the application is reconfigured and thedata saved during the state saving step are used to restore a cyber-physical consistent state of the application. Cyber-physical consistency aims at maintaining a consistent behaviour of the application with respect to the physical world, as well as avoiding dangerous and costly circumstances. The approach was validated using model checking techniques to verify important correctness properties. It was then implemented as a framework called F3ARIoT. This framework was evaluated on a smart home application. The results showed the feasibility of deploying F3ARIoT on real Fog-IoT applications as well as its good performances in regards to end user experience.
In the professional aeronautical field (one of the safest in the world), human error management must be improved to reach a better safety level. However, due to the complexity of socio technical systems, the implementation of an efficient user centred design process could be challenging. To ease this process, our study aims to develop and validate specific tools, particularly for processing large amounts of textual data. Secondly, we will confront the control condition with results obtained automatically. The identification of a methodology to extract risk situations that could be included in specific studies. This step is very important for the user centred design process. Links that we have established between our results and incident/accident studies allow us to consider positive impacts on aviation safety.
Estimating human pose and recognizing human activities are important steps in many applications, such as human computer interfaces (HCI), health care, smart conferencing, robotics, security surveillance etc. Despite the ongoing effort in the domain, these tasks remained unsolved in unconstrained and non cooperative environments in particular. As a second problem, we tackled the human pose estimation task in particular settings where multiple visual sensors are available and allowed to collaborate. In the first part, we focused on indoor action recognition from videos and we consider complex activities. To this end, we explored several methodologies and eventually introduced a 3D spatio-temporal representation for a video sequence that is viewpoint independent. A 3D feature descriptor was employed afterwards to build a codebook and classify the actions with the bag-of-words approach. As for the second part, we concentrated on articulated pose estimation, which is often an intermediate step for activity recognition. Our motivation was to incorporate information from multiple sources and views and fuse them early in the pipeline to overcome the problem of self-occlusion, and eventually obtain robust estimations. In addition to the single-view appearance of the human body and its kinematic priors, we demonstrated that geometrical constraints and appearance-consistency parameters are effective for boosting the coherence between the viewpoints in a multi-view setting. Both methods that we proposed was evaluated on public benchmarks and showed that the use of view-independent representations and integrating information from multiple viewpoints improves the performance of action recognition and pose estimation tasks, respectively.
The extracted named entities and their textual contexts will be categorized using Natural Language Processing approaches in order to account for the semantics of the relationships expressed in the texts. The precise description of documents through encoding formats such as XML-EAD, Dublin Core or XML-TEI thus facilitates the design of finding aids, data mining tasks but also sharing online or within a community of researchers. An ontology will be proposed to meet these needs and allow us to formalize this issue from the point of view of the Semantic Web. This project will be based on the use and extension of already existing ontologies such as DBPedia or ISA Programme Person Core Vocabulary, whose interest in tasks of disambiguation of named entities has already been widely demonstrated. The digitized documents will be processed for Optical Character Recognition (OCR) and finally encoded in XML-TEI format. The data produced will be recorded following the RDF standard, thus ensuring interoperability with all services belonging to OpenLinkedData, in order to produce text inferences based on SPARQL. The methodology for populating the ontology will be developed specifically for the task of extracting and categorizing named entities and spatio-temporal data, in order to be able to automatically extract the relationships between these entities in documents.
My thesis concerns a study of continuous word representations applied to the automatic detection of speech recognition errors. Our study focuses on the use of a neural approach to improve ASR errors detection, using word embeddings. The exploitation of continuous word representations is motivated by the fact that ASR error detection consists on locating the possible linguistic or acoustic incongruities in automatic transcriptions. The aim is therefore to find the appropriate word representation which makes it possible to capture pertinent information in order to be able to detect these anomalies. Our contribution in this thesis concerns several initiatives. First, we start with a preliminary study in which we propose a neural architecture able to integrate different types of features, including word embeddings. Second, we propose a deep study of continuous word representations. This study focuses on the evaluation of different types of linguistic word embeddings and their combination in order to take advantage of their complementarities. On the other hand, it focuses on acoustic word embeddings. Then, we present a study on the analysis of classification errors, with the aim of perceiving the errors that are difficult to detect. Finally, we exploit the linguistic and acoustic embeddings as well as the information provided by our ASR error detection system in several downstream applications.
This thesis is a study of Wolof verbal constructions in a typological perspective. Based on available descriptions of Wolof verbal conjugation, I first provide a summary of the system of verbal predication in the light of the typological literature. The typological analysis of these periphrastic constructions provides us with the empirical basis to propose a new approach to the notion of “auxiliary”. I argue that auxiliaries should not be cross-linguistically defined as items belonging to a specific lexical class or as items on a grammaticalisation path but rather as autonomous predicative elements with a specific function. In addition, I propose a constructional analysis of the organisation of the verbal predication system of Wolof. The entirety of Wolof verbal constructions is not assumed to form an unstructured set of independent entities, but it is instead taken to constitute a highly structured system (a network of constructions). Furthermore, some apparent idiosyncrasies in the conjugation paradigm of Wolof can be explained from a diachronic point of view. Finally, I provide a comparative analysis of verbal constructions in Atlantic languages in order to determine which elements of the Wolof conjugation are inherited from Proto-Atlantic.
This thesis proposes a practical model making it possible to implement a case-based reasoning system that adapts processes represented as natural language text in response to user queries. While the cases and the solutions are in textual form, the adaptation itself is performed on networks of temporal constraints expressed with a qualitative algebra, using a belief revision operator. Natural language processing methods are used to acquire case representations and to regenerate text based on the adaptation result
This research work aims to propose a creative support system according to a multi-agent architecture in order to manage the knowledge needed and produced during a creative workshop. This work contributes to the scientific research with regard to various aspects. Beforehand designing any system, a review of the current creative supports systems is carried out in order to determine their limits concerning the creative process and collaboration mode. To fix these limits, a knowledge engineering approach is adopted. Based on this organizational modeling of a creative workshop, the organization of computational agents that would contribute to manage knowledge is deduced. Based on the same modeling, creative workshop ontology is created in order to provide a representation of the environment and shared knowledge to agents. Multi-agent architecture for creative support system permits to explore new knowledge processing approach notably for idea evaluation. An idea evaluation methodology based on multi-criteria analysis methods is suggested. In addition to this methodology, automatic idea processing based on naturel language processing is experimented in order to assist evaluators.
Identifying satisfaction triggers among customers is a crucial need in today's business world, as a strong customer relationship is now a most vital asset. The domain of opinion mining, in which this thesis falls into, offers several methods to answer this need. These methods, however, require a continuous update of specialized resources which are the cornerstone of many opinion mining tools. The objective of this work is to develop acquisition and structuration strategies for these resources, which can be lexicons, morphosyntactic rules or annotated data. We then compare the benefits of each type of resource through a benchmark of several opinion mining methods, and conclude that the best performing strategy is a hybrid approach. Finally, we present results for resource acquisition methods that answer not only the needs of opinion mining but also the constraints from the industrial setting in which this work has been conducted.
Digital content is increasingly produced nowadays in a variety of media such as news and social network sites, personal Web sites, blogs etc. In particular, a large and dynamic part of such content is related to media-worthy events, whether of general interest (e.g., the war in Syria) or of specialized interest to a sub-community of users (e.g., sport events or genetically modified organisms). While such content is primarily meant for the human users (readers), interest is growing in its automatic analysis, understanding and exploitation. Within the ANR project ContentCheck, we are interested in developing textual and semantic tools for analyzing content shared through digital media. The proposed PhD project takes place within this contract, and will be developed based on the interactions with our partner from Le Monde. Information and relation extraction from a text which may comprise a statement to be fact-checked, with a particular focus on capturing the time dimension ; a sample statement is for instance « VAT on iron in France was the highest in Europe in 2015 ». Building structured queries from extracted information and relations, to be evaluated against reference databases used as trusted information against which facts can be checked.
This thesis deals with sequence matching techniques, applied to word spotting (locating keywords in document images without interpreting the content). Several sequence matching techniques exist in the literature but very few of them have been evaluated in the context of word spotting. This thesis begins by a comparative study of these methods for word spotting on several datasets of historical images. Thus, FSM is able to skip outliers from target sequence, which can be present at the beginning, at the end or in the middle of the target sequence. Moreover it can perform one-to-one, one-to-many and many-to-one correspondences between query and target sequence without considering noisy elements in the target sequence. We then also extend these characteristics to the query sequence by defining a new algorithm (ESC: Examplary Sequence Cardinality). Finally, we propose an alternative word matching technique by using an inexact chain codes (shape code), describing the words.
This work investigates practical methods to ease training and improve performances of neural language models with large vocabularies. The main limitation of neural language models is their expensive computational cost: it depends on the size of the vocabulary, with which it grows linearly. Despite several training tricks, the most straightforward way to limit computation time is to limit the vocabulary size, which is not a satisfactory solution for numerous tasks. Most of the existing methods used to train large-vocabulary language models revolve around avoiding the computation of the partition function, ensuring that output scores are normalized into a probability distribution. Here, we focus on sampling-based approaches, including importance sampling and noise contrastive estimation. These methods allow an approximate computation of the partition function. After examining the mechanism of self-normalization in noise-contrastive estimation, we first propose to improve its efficiency with solutions that are adapted to the inner workings of the method and experimentally show that they considerably ease training. Finally, we aim at improving performances on full vocabulary language models, by augmenting output words representation with subwords.
Many urban cities in Southeast Asia witness severe flooding associated to increasing rainfall intensity and rapid urbanization often due to poor urban planning. Two important inputs required in flood hazard assessment are: (1) high accuracy Digital Elevation Model (DEM), and (2) long rainfall record. High accuracy DEM is both expensive and time consuming to acquire. Long rainfall records for areas of interest are often not available or not sufficiently long to determine the probable extremes. This thesis presents a notably cost-effective and efficient approach to derive high accuracy DEM, and suggests proxies for long rainfall data. DEM data from a publicly accessible satellite, Shuttle Radar Topography Mission (SRTM), and Sentinel 2 multispectral imagery are selected and used to train the Artificial Neural Network (ANN) to improve the quality of the DEM. In the training of ANN, high quality observed DEM is the key leading to a well-trained ANN. The trained ANN will then be ready to efficiently and effectively generate high quality DEM, at low cost, for places where DEM data is not available. The DEM resulting from the latest version of improved SRTM (iSRTM_v2 DEM) shows (1) significantly better than the original SRTM DEM, a 34 % to 57 % RMSE reduction; (2) the visual clarity is so much clearer as well; and (3) much closer drainage network with the actual. The much improved DEM allows flood modelling to proceed with high confidence. Rainfall data resulting from a high spatial resolution Regional Climate Model (RCM), Weather Research and Forecasting driven by ERA-Interim (WRF/ERAI) dataset, is extracted, analyzed, and compared its accuracy with high quality observed rainfall data of Singapore. The comparisons are performed, among others, on their Intensity-Duration-Frequency (IDF) curves, the essential design curves for flood risk assessment; they matched quite well. MIKE 21 Flow Model Flexible Mesh (MIKE 21 FM) is applied to Greater Jakarta, with input data from the above mentioned much improved DEM and precipitation proxy data, for flood simulations of 2 return periods (50-and 100-years). Finally flood maps are generated. This demonstrates the applications of the approaches/methodologies, proposed in this thesis, on catchments where most essential data for flood risk assessment (high resolution and high accuracy DEM and long and high accuracy rainfall data) are not available.
In Natural Language Processing vectorization of words is a key that enables the use of algorithms based on mathematical models. Recently new methods have appeared, and evaluating their quality is a necessity. At present, evaluations are mostly effective on English, which introduces the question of multilingual evaluations. We worked on generalizing methods, on comparing them, on devising new evaluations, and on WordNet as a multilingual resource used for evaluation. We choose six vectorization methods: CBOW, SkipGram, GloVe, an older method as baseline, and two more recent methods. As an indirect method, we choose semantic clustering of words for comparing the underlying vectorizations. The chosen clustering algorithms were: the most used Kmeans, a neuronal one (SOM) and a probabilistic one (EM).Our system applies evaluation methods on big corpora in English, French and Arabic, then compares underlying vectorizations. We propose five new evaluation methods, with four based on WordNet, and one new protocol for polling. Our results yield three different vectorization orderings agreeing on decisive points, and invalidate some existing evaluations. As for our own evaluations, the protocol is validated, one method is invalidated and the reason analyzed, one is validated for English and French, but not Arabic, two are validated on the three languages, and one is left for further exploration.
Advances in technology, in particular the democratization of mobile devices (PCs, smartphones and tablets), has made information accessible to anyone at any time and from anywhere while facilitating the capture of physical contextual data, thereby justifying the growing interest for pervasive computing. The classical approach of pervasive computing has been affected by the introduction of the social dimension. Ubiquitous systems do not meet the needs of users independently from each other but do take into account their social context. Fostering the social dimension has given rise to a fast growing research field called Pervasive Social Computing. Applications in this area are increasingly concerned by communities. A community is considered in our approach as a set of distinct social entities that should be supported with services as a single user is. In this thesis, we look into different aspects of existing centered communities applications and we identify several weaknesses and shortcomings in the notion of community, the community models, and the architecture of communities'applications. To overcome these shortcomings, we propose three main contributions: The ontological representation allows us to organize and represent social data, to make information searches easier for users and to infer new knowledge. A dynamically reconfigurable architecture for fostering spontaneous communities in order to facilitate the user access to communities, information exchange between community members and service discovery. The proposed architecture for community and service discovery have been validated through a prototype called Taldea and have been tested through several scenarios characterized by mobility and ubiquity.
The catalog takes up a special position in the supply of services of academic libraries, as a pivot for the intermediary between users and information professionals who carry the responsibility for building up collections. For 10 years, through a serious crisis, they've been seeing their patrons preferring the general or commercial search engines. The Web is more than a serious competitor today, ahead of the document information systems, and became the main access point for information retrieval. Libraries are trying to structure an information space that is temporarily or permanently inhabited by users, in which the service offering is developed, but it is still presented as a series of silos, with few opportunities of navigation between them despite considerable engineering efforts and a perspective of evolution towards discovery tools. The profession, having become aware of this deep crisis after accusing eddies caused by the breakdown of the digital switch, looking for ways to adapt and diversify its offering, streamlines the dissemination of information, and reinvents its roles, trying to take advantage of new practices of users, new expectations and new prospects. Libraries put their hope in new data models, trying to add a level of abstraction promoting links with the world of knowledge. The evolution towards the Semantic Web seems to be a valuable opportunity to enhance the collections and make them usable in another context, at the expense of significant efforts sized up by this analysis. A constructivist approach based on participant observation and data collection offers a vision of the outcome within the library community on the development of catalogs and intermediation tools, and an outlook on their issues.
The term paraphrase is now commonly used to refer to textual units of equivalent meaning, down to the level of sub-sentential fragments. Although one can envisage to manually build high-coverage lists of synonyms, enumerating meaning equivalences at the level of phrases is too daunting a task for humans. Consequently, acquiring this type of knowledge by automatic means has attracted a lot of attention and significant research efforts have been devoted to this objective. In this thesis we use parallel monolingual corpora for a detailed study of the task of sub-sentential paraphrase acquisition. We argue that the scarcity of this type of resource is compensated by the fact that it is the most suited corpus type for studies on paraphrasing. We propose a large exploration of this task with experiments on two languages with five different acquisition techniques, selected for their complementarity, their combinations, as well as four monolingual corpus types of varying comparability. We report, under all conditions, a significant improvement over all techniques by validating candidate paraphrases using a maximum entropy classifier. An important result of our study is the identification of difficult-to-acquire paraphrase pairs, which are classified and quantified in a bilingual typology.
Credit card fraud has emerged as major problem in the electronic payment sector. In this thesis, we study data-driven fraud detection and address several of its intricate challenges by means of machine learning methods with the goal to identify fraudulent transactions that have been issued illegitimately on behalf of the rightful card owner. In particular, we explore several means to leverage contextual information beyond a transaction's basic attributes on the transaction level, sequence level and user level. On the transaction level, we aim to identify fraudulent transactions which, in terms of their attribute values, are globally distinguishable from genuine transactions. We provide an empirical study of the influence of class imbalance and forecasting horizons on the classification performance of a random forest classifier. We augment transactions with additional features extracted from external knowledge sources and show that external information about countries and calendar events improves classification performance most noticeably on card-not-present transaction. On the sequence level, we aim to detect frauds that are inconspicuous in the background of all transactions but peculiar with respect to the short-term sequence they appear in. We use a Long Short-term Memory network (LSTM) for modeling the sequential succession of transactions. Our results suggest that LSTM-based modeling is a promising strategy for characterizing sequences of card-present transactions but it is not adequate for card-not-present transactions. On the user level, we elaborate on feature aggregations and propose a flexible concept allowing us define numerous features by means of a simple syntax. We provide a CUDA-based implementation for the computationally expensive extraction with a speed-up of two orders of magnitude. Our feature selection study reveals that aggregates extracted from users'transaction sequences are more useful than those extracted from merchant sequences. Regarding future work, we motivate the usage of simple and transparent machine learning methods for credit card fraud detection and we sketch a simple user-focused modeling approach.
The studies of diachronic allow us to study the changes and analyze the regularities in the change to understand the faculty of language. It is a qualitative study using descriptive and comparative methodology for analyzing the corpus. Formalization is a field of automation and simplification of language law without disturbing the language grammatical rules for the application in the field of natural language processing. By passing the morphonology and the morphology, we are able to look clearly into the structure and the mechanism of the Malay grammar in the search for a solution for the affix-lexical combination. This study allows us to understand clearly the internal structure of allomorph, verbal and nominal derivation from Malay verbs in helping us to formulate the model of natural language processing or as an equipment of pedagogy by a schema and the rules. We are presenting the correlation between verbs and the system of derivation by using a study of linguistic-statistic by showing and explaining the situation of Malay linguistic. The result of the statistics and the corpus show us the reliability of our model because there is a coherence between result and
Because of its critical impacts on performance and competitivity, organizations' knowledge is today considered to be an invaluable asset. In this context, the development of methods and frameworks aiming at improving knowledge preservation and exploitation is of major interest. For the purpose of highlighting this potential causal link to infer general learnings, we envisage relying on inductive reasoning techniques. The considered work will be developed and validated through the scope of a humanitarian organization, Médecins Sans Frontières, with a focus on the logistical response in emergency situations
The LegalCluster platform proposes to define legal "clusters" in an ecosystem approach, in order to guarantee the sustainability of the information produced by legal actors. In this context, LegalCluster seeks to provide a legal recommendation service for firms, departments and clients of legal functions, locally, in a given cluster or on a larger scale. Moreover, several major constraints must be noted in order to deliver a relevant service that is fully integrated into the legal environment: professional secrecy, the user's legal context and the consideration of case law. It is then necessary to propose a general solution that can be applied in a local cluster, combined with external resources and knowledge. To do so, this approach is based on a combination of natural language processing (NLP) and information retrieval (IR). Language processing, based on learning models, aims to categorize legal issues. Due to the fact that we have local clusters and different legal codes to associate, our approach is fully integrated into the Topic Modeling and abstract summary generation process. In order to integrate both the business and professional secrecy protection aspects, the model will have to be trained on a global corpus, then locally, which implies the characterization of the models in these two components while relying on implicit data such as legislative texts and jurisprudence. The Information Retrieval section aims to facilitate the analysis of legal corpora while providing new functionalities in order to develop a dedicated legal search engine. Thus, traditional indexing makes it possible to match keywords with the most relevant documents associated with them, but also with the metadata of the business and will make it possible to produce a synthetic "local" vision of business data. The first step of integration is to evolve the data structure model to adapt it to metadata, but also to modify the core of the search engine to allow it to provide scoring functions adapted to the needs of LegalCluster. The search engine will facilitate the analysis and association of these texts necessary for the Learning brick.
Modelling user preferences is crucial in many real-life problems, ranging from individual and collective decision-making to strategic interactions between agents for example. Since agents don't come with their preferences transparently given in advance, we have only two means to determine what they are if we wish to exploit them in reasoning: we can infer them from what an agent says or from his nonlinguistic actions. In this work, we propose a new approach to extract and reason on preferences expressed in negotiation dialogues. After having extracted the preferences expressed in each dialogue turn, we use the discursive structure to follow their evolution as the dialogue progresses. We use CP-nets, a model used for the representation of preferences, to formalize and reason about these extracted preferences. The method is first evaluated on different negotiation corpora for which we obtain promising results. We then apply the end-to-end method with principles from Game Theory to predict trades in the win-lose game The Settlers of Catan. This work thus presents a new approach at the intersection of several research domains: Natural Language Processing (for the automatic preference extraction and the reasoning on their verbalisation), Artificial Intelligence (for the modelling and reasoning on the extracted preferences) and Game Theory (for strategic action prediction in a bargaining game)
More than 8 morphological schemas can derive event nouns from verbs in French:-age,-ment,-ion,-ure,-ance,-ade,-aison and-erie suffixations, as well as verb to noun conversion (défendre 'to defend'-&gt; défense 'defence'). These schemas have similar semantic functions and sometimes select the same verbal bases, which causes the emergence of doublets (gouverner 'to govern'-&gt; gouvernance 'governance', gouvernement 'government', gouverne 'guidance'). Conducted in the field of lexematic morphology (Matthews 1974; Anderson 1992; Aronoff 1994; Fradin 2003; Booij 2005), this contribution aims to shed light on the dynamics that support the coexistence of these rival nominalization schemas by searching for interactional constraints on multiple levels: phonological, morphological, syntactic, semantic and organizational. These constraints are assessed using computational methods on massive web corpora (word embeddings, statistical models, analogical modeling; Arndt-Lappe 2014, Lapraye 2017, Wauquier et al. 2018, Bonami and Thuilier 2019), in order to propose a predictive model of the construction of deverbal nominalizations.
Such an enrichment which is based on Natural Language Processing and Information Retrieval technologies has several applications. As an example, flling in the gap between a scientifc paper and a collection of highly cited papers in a domain helps the paper to be better acknowledged by the community that refers to that collection. We present a comprehensive study over different approaches of each component. The enrichment is performed by recommending discriminative sets of semantically relevant keywords, i.e. topics, to a user. The topics are labeled with representative keywords and have a level of granularity that is easily interpretable. It is also knowledge-poor and domain-independent. To evaluate their robustness, we studied them on 10 topically diverse domains. Our results on the keyword extraction approach showed that the statistical features are not adequate for capturing words importance within a web page. We showed that our approach out performs a baseline approach, since the widely-used co-occurrence feature between keywords is notivenough for capturing their semantic similarity and consequently for detecting semantically consistent topics.
This thesis is part of a descriptive work in acoustic phonetics, with the aim of studying the productions of Luxembourgish vowels in native and non-native speech. Its objective is to conciliate the variation of Luxembourgish, mainly a spoken language, composed of many regional varieties, evolving in a multilingual context, and the learning of Luxembourgish as a foreign language in the Grand-Duchy of Luxembourg. As we assume the fact that language learning implies knowledge of sound contrast in speech, we investigate the productions of speakers whose mother tongues have different features than Luxembourgish, such as French, to see whether if the contrast are reproduced in non-native speech. Productions of French speakers are compared to those of native speakers from the region around the capital city of the Grand-Duchy of Luxembourg, whose variety serves as a reference to the teaching of Luxembourgish as a foreign language. The purpose of the study is the following:-to extend the descriptions on the acoustic properties of vowels produced in a regional variety of the Grand-Duchy of Luxembourg,-to highlight the specific difficulties of productions by French learners of Luxembourgish,-to interpret the results regarding the teaching of Luxembourgish as a foreign language. Fieldwork and the creation of a corpus through recordings of 10 Luxembourg speakers and 10 French speakers are an important part of the empirical work. We obtained a corpus of 12 hours and a half of spoken and spontaneous speech, including native speech and not native of Luxembourgish and also native speech of French. This corpus represents a first corpus containing native and non-native speech of Luxembourgish and enables to conduct different comparative studies. In our thesis, we did comparative analyses of the data in read speech. The methodology we used made it possible to compare data of native and non-native speech and also data of the L1 and L2 of French speakers. The results gave information about native and non-native productions of vowels. These results as well as thorough descriptions of the vowels in native speech, extend knowledge not only of Luxembourgish, but also of the variety which serves as the reference for Luxembourgish as a foreign language. In addition, they open up prospects for studying Luxembourgish by problematizing the introduction of rules for this type of education, despite the absence of language instruction in schools and the evolution of regional varieties in a concentrated geographical area.
We also show that our CNN prediction remarkably predicts the shape of the WER distribution on a collection of speech recordings. Then, we analyze factors impacting both prediction approaches. We also assess the impact of the training size of prediction systems as well as the robustness of systems learned with the outputs of a particular ASR system and used to predict performance on a new data collection. Our experimental results show that both prediction approaches are robust and that the prediction task is more difficult on short speech turns as well as spontaneous speech style. Finally, we try to understand which information is captured by our neural model and its relation with different factors. Our experiences show that intermediate representations in the network automatically encode information on the speech style, the speaker's accent as well as the broadcast program type. To take advantage of this analysis, we propose a multi-task system that is slightly more effective on the performance prediction task.
Nowadays, the Web is formed by two types of content which are linked: structured data of the so-called Semantic Web and users' contributions of the Social Web. The ViewpointS approach was de-signed as an integrative formalism capable of mixing these two types of content while preserving the subjectivity of the interactions of the Social Web. ViewpointS is a subjective knowledge repre-sention approach. The approach also provides a second level of subjectivity. Indeed, the viewpoints can be interpreted differently according to the user through the perspective mechanism. In our frame-work, resources from the Web are tied by viewpoints in a Knowledge Graph. From the Knowledge Graph containing viewpoints and Web resources a Knowledge Map consisting of “synapses” and re-sources is created as a result of the interpretation and aggregation of viewpoints. The evolution of the ViewpointS synapses may be considered analog to the ones in the brain in the very simple sense that each viewpoint contributes to the establishment, strengthening or weakening of a syn-apse that connects two resources. The exchange of viewpoints is the selection process ruling the synapses evolution like the selectionist process within the brain. We investigate in this study the potential impact of our subjective representation of knowledge in various fields: information search, recommendation, multilingual ontology alignment and methods for calculating semantic distances.
We define a fragment of constraints and propose two approaches: the naive and the rewriting, which allows us to filter dynamically valid answers at the query time instead of validating them at the data source level. These two approaches have been evaluated and have shown the feasibility of our system. This is our main contribution: we extend the set of well-known query-rewriting systems (Chase, Chase&amp; backchase, PerfectRef, Xrewrite, etc.) with a new effective solution for the new purpose of filtering query results based on constraints in user context. Moreover, we also enlarge the trigger condition of the constraint compared with other works by using the notion of one-way MGU.
Tabular data often contain columns with categorical variables, usually considered as non-numerical entries with a fixed and limited number of unique elements or categories. As many statistical learning algorithms require numerical representations of features, an encoding step is necessary to transform categorical entries into feature vectors, using for instance one-hot encoding. However, non-curated data give rise to string categorical variables with a very high cardinality and redundancy: the string entries share semantic and/or morphological information, and several entries can reflect the same entity. Without any data cleaning or feature engineering step, common encoding methods break down, as they tend to lose information in their vectorial representation. Also, they can create high-dimensional feature vectors, which prevent their usage in large scale settings. In this work, we study a series of categorical encodings that remove the need for preprocessing steps on high-cardinality string categorical variables. Experiments on real and simulated data show that the methods we propose improve supervised learning, are adapted to large-scale settings, and, in some cases, create feature vectors that are easily interpretable. Hence, they can be applied in Automated Machine Learning (AutoML) pipelines in the original string entries without any human intervention.
In this thesis, we address the specific problem of probabilistic graphical model structure learning, that is, finding the most efficient structure to represent a probability distribution, given only a sample set D ∼ p(v). In the first part, we review the main families of probabilistic graphical models from the literature, from the most common (directed, undirected) to the most advanced ones (chained, mixed etc.). Then we study particularly the problem of learning the structure of directed graphs (Bayesian networks), and we propose a new hybrid structure learning method, H2PC (Hybrid Hybrid Parents and Children), which combines a constraint-based approach (statistical independence tests) with a score-based approach (posterior probability of the structure). In the second part, we address the multi-label classification problem, which aims at assigning a set of categories (binary vector y P (0, 1)m) to a given object (vector x P Rd). In this context, probabilistic graphical models provide convenient means of encoding p(y|x), particularly for the purpose of minimizing general loss functions. We review the main approaches based on PGMs for multi-label classification (Probabilistic Classifier Chain, Conditional Dependency Network, Bayesian Network Classifier, Conditional Random Field, Sum-Product Network), and propose a generic approach inspired from constraint-based structure learning methods to identify the unique partition of the label set into irreducible label factors (ILFs), that is, the irreducible factorization of p(y|x) into disjoint marginal distributions. We establish several theoretical results to characterize the ILFs based on the compositional graphoid axioms, and obtain three generic procedures under various assumptions about the conditional independence properties of the joint distribution p(x, y).
The increasing use of social and sensor networks generates a large quantity of data that can be represented as complex graphs. There are many tasks from information analysis, to prediction and retrieval one can imagine on those data where relation between graph nodes should be informative. All the proposed models use the representation learning framework in its deterministic or Gaussian variant. First, we proposed two algorithms for the heterogeneous graph labeling task, one using deterministic representations and the other one Gaussian representations. Contrary to other state of the art models, our solution is able to learn edge weights when learning simultaneously the representations and the classifiers. Second, we proposed an algorithm for relational time series forecasting where the observations are not only correlated inside each series, but also across the different series. We use Gaussian representations in this contribution. This was an opportunity to see in which way using Gaussian representations instead of deterministic ones was profitable. At last, we apply the Gaussian representation learning approach to the collaborative filtering task. This is a preliminary work to see if the properties of Gaussian representations found on the two previous tasks were also verified for the ranking one. The goal of this work was to then generalize the approach to more relational data and not only bipartite graphs between users and items.
In the medical field, the computerization of health professions and development of the personal medical file (DMP) results in a fast increase in the volume of medical digital information. The need to convert and manipulate all this information in a structured form is a major challenge. This is the starting point for the development of appropriate tools where the methods from the natural language processing (NLP) seem well suited. The work of this thesis are within the field of analysis of medical documents and address the issue of representation of biomedical information (especially the radiology area) and its access. We show the interest of the hypothesis of no separation between different types of knowledge through a document analysis. This network combines weight and annotations on typed relationships between terms and concepts as well as an inference mechanism which aims to improve quality and network coverage. We describe how from semantic information in the network, it is possible to define an increase in gross index built for each records to improve information retrieval. We present then a method of extracting semantic relationships between terms or concepts. This extraction is performed using lexical patterns to which we added semantic constraints. The results show that the hypothesis of no separation between different types of knowledge to improve the relevance of indexing. The index increase results in an improved return while semantic constraints improve the accuracy of the relationship extraction.
Molecular evolution proceeds not only by divergence from a common ancestor, but also by combining parts from evolving objects of different origins, through processes that are called introgressive. Lateral gene transfers are probably the most well-known of these processes, but introgression has been shown to also happen at various levels of biological organization. As a result, most biological evolving objects (genes, genomes, communities) can be composed of parts from different phylogenetic origins and can be described as composites. Such modular evolution is inadequately modeled by trees, since composite objects are not merely the result of divergence from a common ancestor only. Networks on the other hand are much more suited for handling modularity, and graph theory can be used to search networks for patterns that are characteristic of such reticulate evolution. During this PhD, I developed a piece of software, CompositeSearch, that can efficiently detect composite genes in massive sequence dataset, comprising up to millions of sequences. This algorithm was used to identify and quantify the abundance of composite genes in polluted soil environments, and in prokaryotic plasmids. These studies show that important biological novelties and adaptations can result from processes acting at subgenic levels. However, as shown in this manuscript, networks provide a framework that goes well beyond the boundaries of molecular evolution and I have applied them to other evolving entities, such as animals (trait networks) morphology and languages (word networks). In both cases, modularity appears to be a major evolutionary outcome, following rules that remain to be investigated.
The last decade has seen the re-emergence of machine learning methods based on formal neural networks under the name of deep learning. Although these methods have enabled a major breakthrough in machine learning, several obstacles to the possibility of industrializing these methods persist, notably the need to collect and label a very large amount of data as well as the computing power necessary to perform learning and inference with this type of neural network. In this thesis, we propose to study the adequacy between inference and learning algorithms derived from biological neural networks and massively parallel hardware architectures. We show with three contribution that such adequacy drastically accelerates computation times inherent to neural networks. We also propose the introduction of a coarse-to-fine architecture based on complex cells. We show that GPU portage accelerates processing by a factor of seven, while the coarse-to-fine architecture reaches a factor of one thousand. The second contribution presents three algorithms for spike propagation adapted to parallel architectures. We study exhaustively the computational models of these algorithms, allowing the selection or design of the hardware system adapted to the parameters of the desired network. In our third axis we present a method to apply the Spike-Timing-Dependent-Plasticity rule to image data in order to learn visual representations in an unsupervised manner. We show that our approach allows the effective learning a hierarchy of representations relevant to image classification issues, while requiring ten times less data than other approaches in the literature.
Using gesture commands is a new way of interacting with touch sensitive interfaces. In order to facilitate user memorization of several commands, it is essential to let the user customize the gestures. This applicative context gives rise to a crosslearning situation, where the user has to memorize the set of commands and the system has to learn and recognize the different gestures. This situation implies several requirements, from the recognizer and from the system that supervizes its learning process. For instance, the recognizer has to be able to learn from few data samples, to keep learning during its use and to follow indefinitely any change of the data now. The supervisor has to optimize the cooperation between the recognizer and the system to minimize user interactions while maximizing recognizer learning. This thesis presents on the one hand the evolving recognition system Evolve oo, that is capable of fast teaming from few data samples, and that follows concept drifts. The use of forgetting in the learning process allows to maintain the learning gain indefinitely, enabling class adding at any stage of system learning, and guaranteeing lifelong evolving capacity. The on line active supervisor IntuiSup optimizes user interactions to train a classifier when the user is in the training loop. The proportion of data that is labeled by the user evolves to adapt to problem difficulty and to follow environment evolution (concept drift s). The use of a boosting method optimizes the timing of user interactions to maximize their impact on classifier learning process.
The work presented here is for a first part at the cross section of deep learning and anonymization. A full framework was developed in order to identify and remove to a certain extant, in an automated manner, the features linked to an identity in the context of image data. Two different kinds of processing data were explored. They both share the same Y-shaped network architecture despite components of this network varying according to the final purpose. The first one was about building from the ground an anonymized representation that allowed a trade-off between keeping relevant features and tampering private features. This framework has led to a new loss. Therefore the anonymized representation shares the same nature as the initial data (e.g. an image is transformed into an anonymized image). This task led to another type of architecture (still in a Y-shape) and provided results strongly dependent on the type of data. The second part of the work is relative to another kind of relevant information: it focuses on the monitoring of predictor behavior. In the context of black box analysis, we only have access to the probabilities outputted by the predictor (without any knowledge of the type of structure/architecture producing these probabilities). This monitoring is done in order to detect abnormal behavior that is an indicator of a potential mismatch between the data statistics and the model statistics. Two methods are presented using different tools. The first one is based on comparing the empirical cumulative distribution of known data and to be tested data. The second one introduces two tools: one relying on the classifier uncertainty and the other relying on the confusion matrix. These methods produce concluding results.
The constantly changing customers'and users'needs require fast response from software teams. This creates strong demand for seamlessness of the software processes. Continuous integration, delivery and deployment, also known as DevOps, made a huge progress in making software processes responsive to change. This progress had little effect on software requirements, however. Specifying requirements still relies on the natural language, which has an enormous expressive power, but inhibits requirements'traceability, verifiability, reusability and understandability. Promoting the problematic qualities without inhibiting the expressiveness too much introduces a challenge. This approach has motivated and inspired the work on the present thesis. While multirequirements focus on traceability and understandability, the Seamless Object-Oriented Requirements approach presented in the dissertation takes care of verifiability, reusability and understandability. The dissertation explores the Martin Glinz'hypothesis that software requirements should be objects to support seamlessness. The exploration confirms the hypothesis and results in a collection of tool-supported methods for specifying, validating, verifying and reusing object-oriented requirements. The most significant reusable technical contribution of the dissertation is a ready-to-use Eiffel library of template classes that capture recurring software requirement patterns. Concrete seamless object-oriented requirements inherit from these templates and become clients of the specified software. The dissertation reflects on several experiments and shows that the new approach promotes requirements'verifiability, reusability and understandability while keeping expressiveness at an acceptable level. The experiments rely on several examples, some of which are used as benchmarks in the requirements literature. Each experiment illustrates a problem through an example, proposes a general solution, and shows how the solution fixes the problem. While the experimentation relies on Eiffel and its advanced tool support, such as automated proving and testing, each idea underpinning the approach scales conceptually to any statically typed object-oriented programming language with genericity and elementary support for contracts.
In this thesis, we propose to use techniques based on factor analysis to build acoustic models for automatic speech processing, especially Automatic Speech Recognition (ASR). Firstly, we were interested in reducing the footprint memory of acoustic models. Our factor analysis-based method demonstrated that it is possible to pool the parameters of acoustic models and still maintain performance similar to the one obtained with the baseline models. We propose as an alternative a vector representation of states: the factors of states. These factors of states enable us to accurately measure the similarity between the states of the HMM by means of an euclidean distance for example. Using this vector representation, we propose a simple and effective method for building acoustic models with shared states. This procedure is even more effective when applied to under-resourced languages. Finally, we concentrated our efforts on the robustness of the speech recognition systems to acoustic variabilities, particularly those generated by the environment. In our various experiments, we examined speaker variability, channel variability and additive noise. Through our factor analysis-based approach, we demonstrated the possibility of modeling these different types of acoustic variability as an additive component in the cepstral domain. By compensation of this component from the cepstral vectors, we are able to cancel out the harmful effect it has on speech recognition
Landmarks are presented in the applications of different domains such as biomedical or biological. It is also one of the data types which have been usedin different analysis, for example, they are not only used for measuring the form of the object, but also for determining the similarity between two objects. In biology, landmarks are used to analyze the inter-organisms variations, however the supply of landmarks is very heavy and most often they are provided manually. In recent years, several methods have been proposed to automatically predict landmarks, but it is existing the hardness because these methods focused on the specific data. This thesis focuses on automatic determination of landmarks on biological images, more specifically on two-dimensional images of beetles. In our research, we have collaborated with biologists to build a dataset including the images of 293 beetles. For each beetle in this dataset, 5 images correspond to 5 parts have been taken into account, e.g., head, body, pronotum, left and right mandible. Along with each image, a set of landmarks has been manually proposed by biologists. First step, we have brought a method which was applied on fly wings, to apply on our dataset with the aim to test the suitability of image processing techniques on our problem. Secondly, we have developed a method consisting of several stages to automatically provide the landmarks on the images. These two first steps have been done on the mandible images which are considered as obvious to use the image processing methods. Thirdly, we have continued to consider other complex remaining parts of beetles. Accordingly, we have used the help of Deep Learning. We have designed a new model of Convolutional Neural Network, named EB-Net, to predict the landmarks on remaining images. In addition, we have proposed a new procedure to augment the number of images in our dataset, which is seen as our limitation to apply deep learning. Finally, to improve the quality of predicted coordinates, we have employed Transfer Learning, another technique of Deep Learning. In order to do that, we trained EB-Net on a public facial key points. Then, they were transferred to fine-tuning on beetle's images. The obtained results have been discussed with biologists, and they have confirmed that the quality of predicted landmarks is statistically good enough to replace the manual landmarks for most of the different morphometry analysis.
This thesis adresses the issue of accessing scientific and technical information conveyed by large sets of documents. The resulting model enables a documentary immersion, thanks to three types of complementary processes: endogenous processes (exploiting the corpus to analyze the corpus), exogenous processes (using external resources) and anthropogenous ones (in which the user's skills are considered as a resource) are combined. They all contribute to granting the user a fundamental role in the system, as an interpreting agent and as a knowledge creator, provided that he is placed in an industrial or specialised context.
The present work deals with the problem of the semantic complexity in natural language, proposing an hypothesis based on some features of natural language sentences that determine their difficulty for human understanding. We aim at introducing a general framework for semantic complexity, in which the processing difficulty depends on the interaction between two components: a Memory component, which is responsible for the storage of corpus-extracted event representations, and a Unification component, which is responsible for combining the units stored in Memory into more complex structures. We propose that semantic complexity depends on the difficulty of building a semantic representation of the event or the situation conveyed by a sentence, that can be either retrieved directly from the semantic memory or built dynamically by solving the constraints included in the stored representations. In order to test our intuitions, we built a Distributional Semantic Model to compute a compositional cost for the sentence unification process. Our tests on several psycholinguistic datasets showed that our model is able to account for semantic phenomena such as the context-sensitive update of argument expectations and of logical metonymies.
The large use of CAD systems in many industrial fields, such as automotive, naval, and aerospace, has generated a number of 3D databases making available a lot of 3D digital models. Within enterprises, which make use of these technologies, it is common practice to access to CAD models of previously developed products. Therefore, it is useful to have technological solutions that are able to evaluate the similarities of different products in such a way that the user can retrieve existing models and thus have access to the associated useful information for the new design. The concept of similarity has been widely studied in literature and it is well known that two objects can be similar under different perspectives. These multiple possibilities make complicate the assessment of the similarity between two objects. So far, many methods are proposed for the recognition of different parts similarities, but few researches address this problem for assembly models. Based on these requirements, we propose a system for retrieving similar assemblies according to different similarity criteria. To achieve this goal, it is necessary having an assembly description including all the information required for the characterizations of the possible different similarity criteria between the two assemblies. Therefore, one of the main topics of this work is the definition of a descriptor capable of encoding the data needed for the evaluation of similarity adaptable to different objectives. In addition, some of the information included in the descriptor may be available in CAD models, while other has to be extracted appropriately. Therefore, algorithms are proposed for extracting the necessary information to fill out the descriptor elements. Finally, for the evaluation of assembly similarity, several measures are defined, each of them evaluating a specific aspect of their similarity.
Machine Translation (MT) has made significant progress in the recent years and continues to improve. Today, MT is successfully used in many contexts, including professional translation environments and production scenarios. However, the translation process requires knowledge larger in scope than what can be captured by machines even from a large quantity of translated texts. Since injecting human knowledge into MT is required, one of the potential ways to improve MT is to ensure an optimized human-machine collaboration. To this end, many questions are asked by modern research in MT: How to detect where human assistance should be proposed? How to make machines exploit the obtained human knowledge so that they could improve their output? And, not less importantly, how to optimize the exchange so as to minimize the human effort involved and maximize the quality of MT output? Various solutions have been proposed depending on concrete implementations of the MT process. In this thesis we have chosen to focus on Pre-Edition (PRE), corresponding to a type of human intervention into MT that takes place ex-ante, as opposed to Post-Edition (PE), where human intervention takes place ex-post. In particular, we study targeted PRE scenarios where the human is to provide translations for carefully chosen, difficult-to-translate, source segments. Targeted PRE scenarios involving pre-translation remain surprisingly understudied in the MT community. Moreover, in a multilingual setting common difficulties can be resolved at one time and for many languages. Such scenarios thus perfectly fit standard production contexts, where one of the main goals is to reduce the cost of PE and where translations are commonly performed simultaneously from one language into many languages. A representative production context - an automatic translation of systematic medical reviews - is the focus of this work. Given this representative context, we propose a system-independent methodology for translation difficulty detection. We define the notion of translation difficulty as related to translation quality: difficult-to-translate segments are segments for which an MT system makes erroneous predictions. We cast the problem of difficulty detection as a binary classification problem and demonstrate that, using this methodology, difficulties can be reliably detected without access to system-specific information. We integrate the results of our difficulty detection procedure into a PRE protocol that enables resolution of those difficulties by pre-translation. We assess the protocol in a simulated setting and show that pre-translation as a type of PRE can be both useful to improve MT quality and realistic in terms of the human effort involved. Moreover, indirect effects are found to be genuine. Results of those pilot experiments confirm the results in the simulated setting and suggest an encouraging beginning of the test phase.
This research work deals with the interdisciplinary field of the information and communication sciences (CIS) and aims to explore the use of the semantic web in digital libraries. The web requires libraries to rethink their organizations, activities, practices and services in order to reposition themselves as reference institutes for the dissemination of knowledge. In this thesis, we wish to understand the contexts of use of the semantic web in French digital libraries. It questions the contributions of the semantic web within these libraries, as well as on the challenges and the obstacles that accompany its implementation. We are also interested in documentary practices and their evolutions following the introduction of the semantic web in digital libraries. The problem is related to the role that information professionals can play in the implementation of the semantic web in digital libraries. After selecting 98 digital libraries following an analysis of three censuses, a questionnaire survey aims to collect data on the use of the semantic web in these libraries. Then, a second interview-based survey consists of highlighting the representations that the information professionals have of the semantic web and its use in the library, as well as on the evolution of their professional practices. The results show that the representation of knowledge within the semantic web requires human intervention to provide the conceptual framework to determine the links between the data. Finally, information professionals can become actors of the semantic web, in the sense that their roles are not limited to the use of the semantic web but also to the development of its standards to ensure better organization of knowledge.
The work synthesized in this thesis aims at studying the coordination between manual gestures and speech during multimodal utterances production. More precisely, the temporal relationship between the two modalities is considered. The coordination is studied in a designation framework since designating is possible both manually (pointing gesture) and using speech (one can "show with the voice" using focus and/or demonstratives for example). All the studies presented in this work are done in a lab setting thus allowing to get precise and reproducible measurements while minimizing potential external sources of variation (either between or within participants). Participants'productions were then compared to each other focusing on factors of interest while keeping other sources of variation as low as possible. A part of the work consisted in designing rather natural experimental protocols so as to ensure productions were not too artificial. The first two experiments studied to co-production of manual gestures and speech containing a focused part. Different types of gestures were compared (pointing gesture, beat, button-push) in a designation task. It has been shown that producing focus did temporally attract manual gesture whichever its type but that this attraction was finer and less variable for pointing gesture. Another interesting finding was that the apex of pointing gesture seems to be cooccurring with articulatory targets rather than acoustic ones. The second study manipulates the designation link between manual gestures and speech. By showing that participants can be split up into two groups using different multimodal coordination strategies, it put forward the complexity of underlying mechanisms of this coordination. The last experiment focuses on the coordination in a more natural interactive and collaborative task. Results show a co-ocurrence of the part of the gesture that shows and with the complementary information in speech (ie. the name of the object to be placed at the spot pointed at by the manual gesture) rather than with the part of speech that shows (ie. demonstrative). The work presented in this manuscript moreover put forward a systematic way of labeling semi-constrained interactive tasks which can be generalized. The conclusion puts in perspective the results so as to improve some manual gestures/speech co-production models and indicates paths for reflection about embodied conversational agents and early detection of pathological cases.
This dissertation work is realised as a contrastive analysis which aims the identification of the aspectual differences between two linguistic systems, French and Russian. Our methodology is based on the analysis of two types of data corpora: comparable and parallel. The subject of this research concerns the study of aspectuel values of Nouns of emotion and their collocative verbs, especially in the Verb+Noun constractions. The identification of the aspectual values of these combinations comes from their lexical and syntactic combinatory. It is composed of different parameters (settings): aspectuel features of the Noun (bi-nominal structures, adjectives-modifiers and determinants) and aspectual features of the Verb (grammatical aspect, lexical aspect and phases).
This thesis focuses on mismatches in peripheral ellipsis (RNR) and proposes an analysis based on lexeme identity between the missing material and the peripheral material. In this thesis, we challenge this hypothesis. We analyzed 5 types of mismatches in peripheral ellipsis: polarity mismatch, possessive mismatch, voice mismatch and verbal form mismatch. Mismatches are quite numerous even in careful writings. In all cases, the mismatches are resolved by the form that corresponds to the second conjunct. The results of acceptability judgment tests and eye tracking experiments allow the integration of these mismatches into the grammar. The results are compatible with analyses postulating semantic identity between the missing material and the antecedent for ellipsis. We formalize peripheral ellipsis with mismatch within HPSG.We finally compare our results with lexical coordination. We show that it obeys closest conjunct agreement (Villavicencio et al (2005)) and propose a HPSG analysis for coordination of verbs and prepositions.
The development of correct formal specifications for systems and software begins with the analysis and understanding of client requirements. Between these requirements described in natural language and their specification defined in a specific formal language, a gap exists and makes the task of development more and more difficult to accomplish. We are facing two different worlds. This thesis aims to clarify and establish interactions between these two worlds and to evolve them together. By interaction, we mean all the links, exchanges and activities taking place between the different documents. Among these activities, we present the validation as a rigorous process that starts from the requirements analysis and continues throughout the development of their formal specification. As development progresses, choices are made and feedbacks from verification and validation tools can detect shortcomings in requirements as well as in the specification. The evolution of the two worlds is described via the introduction of a new requirement into an existing system and through the application of development patterns. They facilitate the task of development and help to avoid the risk of oversights. Whatever the choice, the proposed approach is guided by questions accompanying the evolution of the whole system and makes it possible to detect imperfections, omissions or ambiguities in the existing.
This work aims at providing efficient access to relevant information among the increasing volume of digital data. Thus, we proposed a mixed method which combines natural language processing techniques for extracting knowledge from text and the reuse of existing semantic resources for the conceptualization step. We have also developed a method for aligning terms in English and French in order to enrich terminologically the resulting ontology. The application of our methodology resulted in a bilingual ontology dedicated to Alzheimer's disease. We then proposed algorithms for supporting ontology-based semantic IR. Thus, we used concepts from ontology for describing documents automatically and for query reformulation. We were particularly interested in: 1) the extraction of concepts from texts, 2) the disambiguation of terms, 3) the vectorial weighting schema adapted to concepts and 4) query expansion. These algorithms have been used to implement a semantic portal about Alzheimer's disease. Further, because the content of documents are not always fully available, we exploited incomplete information for identifying the concepts, which are relevant for indexing the whole content of documents. Toward this end, we have proposed two classification methods: the first is based on the k nearest neighbors' algorithm and the second on the explicit semantic analysis. The two methods have been evaluated on large standard collections of biomedical documents within an international challenge.
Document processing is the transformation of a human understandable data in a computer system understandable format. Document analysis and understanding are the two phases of document processing. Considering a document containing lines, words and graphical objects such as logos, the analysis of such a document consists in extracting and isolating the words, lines and objects and then grouping them into blocks. The subsystem of document understanding builds relationships (to the right, left, above, below) between the blocks. A document processing system must be able to: locate textual information, identify if that information is relevant comparatively to other information contained in the document, extract that information in a computer system understandable format. For the realization of such a system, major difficulties arise from the variability of the documents characteristics, such as: the type (invoice, form, quotation, report, etc.), the layout (font, style, disposition), the language, the typography and the quality of scanning. This work is concerned with scanned documents, also known as document images. We are particularly interested in locating textual information in invoice images. Invoices are largely used and well regulated documents, but not unified. They contain mandatory information (invoice number, unique identifier of the issuing company, VAT amount, net amount, etc.) which, depending on the issuer, can take various locations in the document. The present work is in the framework of region-based textual information localization and extraction. First, we present a region-based method guided by quadtree decomposition. Our method allows to determine accurately in document images, the regions containing text information that one wants to locate and retrieve quickly and efficiently. In another approach, we propose a textual information extraction model consisting in a set of prototype regions along with pathways for browsing through these prototype regions. The life cycle of the model comprises five steps: - Produce synthetic invoice data from real-world invoice images containing the textual information of interest, along with their spatial positions. - Partition the produced data. - Derive the prototype regions from the obtained partition clusters. - Derive pathways for browsing through the prototype regions, from the concept lattice of a suitably defined formal context. - Update incrementally the set of protype regions and the set of pathways, when one has to add additional data.
The study of continental surfaces is a major global challenge for the monitoring and management of territories, particularly in terms of the distribution between urban expansion, agricultural land and natural areas. In this context, land cover maps characterizing the biophysical cover of land are an essential asset for the analysis of continental surfaces. Supervised classification algorithms allow, from annual time series of satellite images and reference data, to automatically produce the map of the corresponding period. However, reference data is expensive information to obtain, especially over large areas. Indeed, field survey campaigns require a high human cost, and databases are associated with long update times. In addition, these reference data are valid only for the corresponding period due to changes in land use. These changes mainly concern urban expansion at the expense of natural areas, and agricultural land subject to crop rotation. The general objective of the thesis is to propose methods for producing land cover maps without exploiting the reference data of the corresponding period. The work carried out is based on the creation of a land cover history. This history includes all the information available for the area of interest: land cover maps, image time series, reference data, classification models, etc. A first part of the work considers that the history contains only one period. Thanks to this history we proposed a \g{naïve} classification approach allowing to use a classifier already trained, over a new period. The performances obtained shown that this approach is insufficient, thus requiring more efficient methods. Domain adaptation makes it possible to address this type of problem. We considered two approaches: data projection via canonical correlation analysis and optimal transport. These two approaches allow the historical data to be projected in order to reduce differences with the year to be processed. Nevertheless, these approaches offer results equivalent to the naive classification for much more significant production costs.
In many areas where it exists human risks, such as medicine, nuclear or avionics, it is necessary to go through a certification stage to ensure the proper functioning of a system or product. Certification is based on normative documents that express the justification requirements to which the product and the development process must conform. A certification audit then consists of producing documentation certifying compliance with this regulatory framework. To cope with this need for justifications to ensure compliance with the standards in force and the completeness of the justifications provided, it must therefore be able to target the justification requirements to be claimed for a project and produce justifications during the development of the project. In this context, eliciting the justification requirements from the standards and producing the necessary and sufficient justifications are issues to ensure compliance with standards and avoid over-justification. In these works we seek to structure the justification requirements and then help to produce the associated justifications while remaining attentive to the confidence that can be placed in them. To address these challenges, we have defined a formal semantics for an existing model of justifications: Justification Diagrams. From this semantics, we have been able to define a set of operations to control the life cycle of the justifications to ensure that the justifications regarding the justification requirements. Through this semantics, we have also been able to guide, and even automate in some cases, the production of justifications and the verification of conformance. These contributions were applied in the context of medical technologies for the company AXONIC, the bearer of this work.
This thesis takes place in Natural Langage Processing and aims to help users in such situation. Existing systems (as search engines on Internet) do not fully satisfied their users in repeated tasks, taking few into consideration their point of view and their interactions with the textual material. In this thesis, we propose to consider personalization and interaction as the core of new tools to access the content of sets of texts. Thus, we represent users'point of view on domains of their interest with sets of lexical units described and structured according to a differential lexical semantic model. We then use such representations to build cartographic supports allowing interactions between users and theirs sets of texts in order to visualize gatherings, links and differences between texts of a set and, by this way, to reach their content. To computerize such propositions, we have developed the ProxiDocs plat-form.
«What do I need to know about something to know it?». It is no wonder that such a general, hard to grasp and riddle-like question remained the exclusive domain of a single discipline for centuries: Philosophy. In this context, the distinction of the primitive components of reality – the so called "world's furniture" – and their relations is called an Ontology. This book investigates the emergence of similar questions in two different though related fields, namely: Artificial Intelligence and Knowledge Engineering. We show here that the way these disciplines apply an ontological methodology to either cognition or knowledge representation is not a mere analogy but raises a bunch of relevant questions and challenges from both an applied and a speculative point of view. More specifically, we suggest that some of the technical answers to the issues addressed by Big Data invite us to revisit many traditional philosophical positions concerning the role of language or common sense reasoning in the thought or the existence of mind-independent structure in reality.
Developing natural language processing tools usually requires a large number of resources (lexica, annotated corpora,...), which often do not exist for less-resourced languages. Another approach is to exploit existing resources of closely related languages. Taking advantage of the closeness of standard Arabic and its dialects, one way to solve the problem of limited resources, consists in performing a conversion of Arabic dialects into standard Arabic in order to use the tools developed to handle the latter. We propose a conversion system of Tunisian into a closely form of standard Arabic for which the application of natural language processing tools designed for the latter provides good results. In order to validate our approach, we focused on part-of-speech tagging. Our system achieved an accuracy of 89% which presents ∼20% of absolute improvement over a standard Arabic tagger baseline.
Children with language impairment, such as dyslexia, are often faced with important difficulties when learning to read and during any subsequent reading tasks. Over the past fifteen years, general tools developed in the field of Natural Language Processing have been transformed into specific tools for that help with and compensate for language impaired students'difficulties. At the same time, the use of concept maps or heuristic maps to encourage dyslexic children express their thoughts, or retain certain knowledge, has become popular. It was important that this piece of software facilitate reading comprehension while including functionalities that are adapted to dyslexic teenagers.
A collection of documents is generally represented as a set of documents but this simple representation does not take into account cross references between documents, which often defines their context of interpretation. This standard document model is less adapted for specific professional uses in specialized domains in which documents are related by many various references and the access tools need to consider this complexity. We propose two models based onformal and relational concept analysis and on semantic web techniques.
The aim of this thesis is to understand how the brain computes and represents symbolic structures, such like those encountered in language or mathematics. The existence of parts in structures like morphemes, words and phrases has been established through decades of linguistic analysis and psycholinguistic experiments. Nonetheless the neural implementation of the operations that support the extreme combinatorial nature of language remains unsettled. Some basic composition operations that allow the stable internal representation of sensory objects in the sensory cortex, like hierarchical pattern recognition, receptive fields, pooling and normalization, have started to be understood[5]. But models of the binding operations required for construction of complex, possibly hierarchical, symbolic structures on which precise manipulation of its components is a requisite, lack empirical testing and are still unable to predict neuroimaging signals. In this sense, bridging the gap between experimental neuroimaging evidence and the available modelling solutions to the binding problem is a crucial step for the advancement of our understanding of the brain computation and representation of symbolic structures. From the recognition of this problem, the goal of this PhD became the identification and experimental test of the theories, based on neural networks, capable of dealing with symbolic structures, for which we could establish testable predictions against existing fMRI and ECoG neuroimaging measurements derived from language processing tasks. We identified two powerful but very different modelling approaches to the problem. The first is in the context of the tradition of Vectorial Symbolic Architectures (VSA) that bring precise mathematical modelling to the operations required to represent structures in the neural units of artificial neural networks and manipulate them. This is Smolensky's formalism with tensor product representations (TPR)[10], which he demonstrates can encompass most of the previous work in VSA, like Synchronous Firing[9], Holographic Reduced Representations[8] and Recursive Auto-Associative Memories[1]. Instead of solving binding by assuming precise and particular algebraic operations on vectors, the NBA proposes the establishment of transient connectivity changes in a circuit structure of neural assemblies, such that the potential _ow of neural activity allowed by working memory mechanisms after a binding process takes place, implicitly represents symbolic structures. The first part of the thesis develops in more detail the theory behind each of these models and their relationship from the common perspective of solving the binding problem. Both models are capable of addressing most of the theoretical challenges posed currently for the neural modelling of symbolic structures, including those presented by Jackendo_[3]. For the second part of the thesis, we identified the superposition principle, which consists on the addition of the neural activations of each of the sub-parts of a symbolic structure, as one of the most crucial assumptions of Smolensky's TPR.
Computer assistance became indispensable part of modern surgical procedures. Desire of creating new generation of intelligent operating rooms incited researchers to explore problems of automatic perception and understanding of surgical situations. A great progress was achieved in recognition of surgical phases and gestures. Yet, there is still a blank between these two granularity levels in the hierarchy of surgical process. Very few research is focused on surgical activities carrying important semantic information vital for situation understanding. Two important factors impede the progress. First, automatic recognition and prediction of surgical activities is a highly challenging task due to short duration of activities, their great number and a very complex workflow with multitude of possible execution and sequencing ways. Secondly, very limited amount of clinical data provides not enough information for successful learning and accurate recognition. In our opinion, before recognizing surgical activities a careful analysis of elements that compose activity is necessary in order to chose right signals and sensors that will facilitate recognition. We used a deep learning approach to assess the impact of different semantic elements of activity on its recognition. Through an in-depth study we determined a minimal set of elements sufficient for an accurate recognition. Information about operated anatomical structure and surgical instrument was shown to be the most important. We also addressed the problem of data deficiency proposing methods for transfer of knowledge from other domains or surgeries. The methods of word embedding and transfer learning were proposed. They demonstrated their effectiveness on the task of next activity prediction offering 22% increase in accuracy. In addition, pertinent observations about the surgical practice were made during the study.
Despite the importance of an international nomenclature, the field of chemistry still suffers from some linguistic problems, linked in particular to its simple and complex terminological units, which can hinder scientific communication. This is in addition to the recurring use of borrowings. The problematic is how to represent the simple and complex terminological units of this specialized language. In other words, formalize the terminological characteristics by studying the mechanisms of themorphosyntactic construction of the chemistry'terms in Arabic. This study should lead to the establishment of a semantic-disambiguation tool that aims to create a tool for extracting the terms of Arabic chemistry and their relationships. The construction of this identification grammar requires modelling of morphosyntactic patterns from their observation in corpus and leads to the definition of rules of grammar and constraints.
Technological development has its pros and cons. Nowadays, we can easily share, download, and upload digital content using the Internet. Also, malicious users can illegally change, duplicate, and distribute any kind of information, such as images and documents. Therefore, we should protect such contents and arrest the perpetrator. The goal of this thesis is to protect PDF documents and images using the Spread Transform Dither Modulation (STDM), as a digital watermarking technique, while taking into consideration the main requirements of transparency, robustness, and security. STDM watermarking scheme achieved a good level of transparency and robustness against noise attacks. The key to this scheme is the projection vector that aims to spreads the embedded message over a set of cover elements. However, such a key vector can be estimated by unauthorized users using the Blind Source Separation (BSS) techniques. In our first contribution, we present our proposed CAR-STDM (Component Analysis Resistant-STDM) watermarking scheme, which guarantees security while preserving the transparency and robustness against noise attacks. STDM is also affected by the Fixed Gain Attack (FGA). In the second contribution, we present our proposed N-STDM watermarking scheme that resists the FGA attack and enhances the robustness against the Additive White Gaussian Noise (AWGN) attack, JPEG compression attack, and variety of filtering and geometric attacks. Experimentations have been conducted distinctly on PDF documents and images in the spatial domain and frequency domain. Recently, Deep Learning and Neural Networks achieved noticeable development and improvement, especially in image processing, segmentation, and classification. Diverse models such as Convolutional Neural Network (CNN) are exploited for modeling image priors for denoising. CNN has a suitable denoising performance, and it could be harmful to watermarked images. In the third contribution, we present the effect of a Fully Convolutional Neural Network (FCNN), as a denoising attack, on watermarked images. STDM and Spread Spectrum (SS) are used as watermarking schemes to embed the watermarks in the images using several scenarios. This evaluation shows that such type of denoising attack preserves the image quality while breaking the robustness of all evaluated watermarked schemes.
In-vivo reflectance confocal microscopy (RCM) is a powerful tool to visualize the skin layers at cellular resolution. Aging descriptors have been highlighted from confocal images. However, it requires visual assessment of images by experienced dermatologists to assess those descriptors. The objective of this thesis is the development of an innovative technology to automatically quantify the phenomenon of skin aging using in vivo reflectance confocal microscopy. First, the quantification of the epidermal state is addressed. The proposed measurements show significant difference among groups of age and photo-exposition. Finally, the proposed methods are validated through both clinical and cosmetic product efficacy studies
Nowadays, the use of business process management techniques within companies allows a significant improvement in the efficiency of the operational systems. These techniques assist business experts in modelling business processes, implementation, analytics, and enhancements. The execution context of a business process contains information to identify and understand the interactions between itself and other processes. The first process is triggered following a customer need (operational process) and the others, following the need of the operationnal process (support processes). The satisfaction of a customer need depends on an effective interaction between an operational process and the related support processes. These interactions are defined through operational data, manipulated by the operational process, and support data, manipulated by the support processes. Our work is based on the framework of Model Driven Engineering. The approach proposed in this thesis is based on the annotation of operational or support process models. This annotation is performed with the help of an ontology defining the business domain described by these processes. These annotations are then exploited to constitute a set of data, called contextual data. The analysis of the traces of the execution of the operational process and of these contextual data makes it possible to select the best sub set of contextual data, in the business sense. Thus, an operational process can be associated with a set of support processes via the contextual data.
On a global scale, migration numbers have evolved right after the Cold War period. Refugee populations grew and rates changed from one year to another ranging from 2.4 million in 1975 and reaching 12.1 million in the year of 2000 (UNHCR 1995; UNHCR 2000). Today, the migration rates in Europe show a total of more than 4 million immigrants to different European countries. Furthermore, France and The United Kingdom are considered as two of the major migration targets in Europe, according to the statistical office of the European Union situated in Luxembourg " Germany reported the largest total number of immigrants (917.1 thousand) in 2017, followed by the United Kingdom (644.2 thousand), Spain (532.1 thousand), France (370.0 thousand) and Italy (343.4 thousand)", an attraction for migration and asylum alike. In general, the majority of migrants and refugees experience difficult living conditions; however, the impact on women is more critical due to gender and socioeconomic inequalities, despite the fact that female migrants represent 52% of the entire migrants of Europe. Some female immigrants and refugees on the other hand, took it on themselves to express their turmoil and draw the public eye's attention to their predicament. The aim of this research is, on the one hand, to delve deeper into the inner workings and apparatuses that are shaping the European culture and heritage as a result of migrants' cultural contributions, and on the other hand, to study and analyze the means used to channel their thoughts and share their stories with the rest of the world between the XX and the XXI centuries. This research covers several artistic and literary disciplines - ranging from visual arts such as theatrical and cinematic productions, filmography and photography to literary works as novels, creative writings and comic books- which could be used for various ends such as sociological assertiveness, a quest for identity and belonging, and therapy. On theater, the study covers the pioneer theatrical texts and performances that have shaped migration history within the two European regions over the 20 and 21st centuries, as well as the troupes and groups that target and tend to include refugees. We could refer, therefore to the Franco-Algerian playwright Salika Amara who worked within the French theater crew called Kahina founded by women of Aubervilliers in 1976 and mainly consisted of female actors and their counterparts in the United Kingdom Winsomme Pinnock and contemporary English- Nigerian Bola Agbaje. It goes without saying that the work on theater is not only limited to theater stages but covers all types of representations, formal and informal, such as the work Ornina, a company founded by individuals of French, Syrian and multiple origins that help the refugees coming to the city of Dijon in France, of Le Bureau d'Accueil et d'Accompagnement des Migrants (BAAM), theater company CK Point, The Good Chance Theatre in the UK and even plays represented in international Festivals (migrant'scène, Avignon, Edinburgh). When it comes to cinema and filmmaking, I will over the minority cinema that reflects the conditions of exile and alienation and that are mostly produced in the margins. This is better described by Naficy as films with open form and closed-form visual style; fragmented, multilingual, epistolary, self reflexive, and critically juxtaposed narrative structure; amphibolic, doubled, crossed, and lost characters; subject matter and themes that involve journeying, historicity, identity, and displacement; dysphoric, euphoric, nostalgic, synaesthetic, liminal, and politicized structures of feeling, interstitial and collective modes of production; and inscription of the biographical, social, and cinematic (dis)location of the filmmakers. (Naficy 2001: 4) All those characteristics are usually present in the migration filmmaking; moreover, these films represent the state of mind and psychological condition of the immigrant or refugee especially if the producers or main actors are themselves migrants or refugees such as in the short film Unbroken Paradise played by a genuine Syrian refugee. Other migration movies on the other hand tend to raise awareness about certain matters and act as advocates for human or civil rights to call for improvement for the sake of the young women's welfare, a sort of deconstruction of the "male gaze" and an adoption of a defensive self-reflexive approach. Pertaining to the field of visual arts, the role of photography is undeniable. For ages, graphics have been a medium for loud yet silent expressions. In the case of migration, the task and the burden are heavier than usual. It takes only one click to redirect a vision, an idea and shift a perspective of a nation, to a positive or a negative side alike. In this research we believe that the role of graphics is very prominent and pertinent to the migrants' journey. Just like the aforementioned works of art, photography is a means of re-identification and re-telling of migrants' stories. Omar Imam for instance, a Syrian photographer working on the Syrian migration crisis, with his project Live, Love, Refugee within refugee camps and tells stories of refugees with creative and surrealistic shots and still pictures of different actions that intend to reflect certain emotions and thoughts. Many other photography projects will be covered in the research with the aim of re-writing female migrants identities such as the Refugees Learning and Storytelling through Participatory Photography project launched by three female leaders in the field of migration and photography and that aims at teaching women migrants the basics of photography in order to empower them and help them re-tell their own stories (Brigham, 2018). The literature of migration entails the analysis of women's presence within literary texts which means also novels and works of creative writings such as poems, journals, travel accounts etc. and comic books with a major reference to postcolonial theory and Orientalism in regard to refugees as well. According to Franz Fanon "Europe is literally the creation of the Third World" (Fanon, 2002, p. 99), which refers to the continuous writing from the periphery as a means for re-writing and re-inventing identity 'the experience of migration acts as a catalyst and conduit for nascent feelings, a re-conception of our sense of self and our relationships with others' (Jacobs, 2011:142). However, the creative writing has an important share is this research mainly because of its developing and evolving use within refugee camps and by different companies in order to bridge gaps between locals and migrants as a sociological and psychological therapy. Comic books are also a new and fast growing trend used by journalists and artists is order to depict the real experiences of migrants from different locations in the world. Through a minimized text but expressive graphics, they attract readers with different backgrounds and different societal status to the conditions and hardships and horrors encountered by individuals, families and children seeking shelter and refuge on foreign soils, such as the outstanding digital comic Over Under Sideways Down by Karrie Fransman, and Fleeing to the Unknown created by the organization PositiveNegative. The analysis covers all the works created by female and male migrants as well as others pertaining to local or international artists/writers with a reference to female migrants and migration. Following a historically and pluri-disciplinary approach, the comparative analysis aims at identifying the new trends and methods used in representing female migrants and answer the below questions: is there a difference between the discourse used by migrants on both regions? What is the difference between the usual discourse in artistic female representation and that of the migrant female representation? How is it used and to which ends? Is there a difference between masculine discourse in reference to women, and the feminine discourse in reference to women (self-referentiality)? Do all those disciplines share the same objective vis-à-vis migrants' situation and image?
In the recent years, the Web has undergone a tremendous growth regarding both content and users. This has led to an information overload problem in which people are finding it increasingly difficult to locate the right information at the right time. Recommender systems have been developed to address this problem, by guiding users through the big ocean of information. The recommendation approaches have multiplied and have been successfully implemented, particularly through approaches such as collaborative filtering. However, there are still challenges and limitations that offer opportunities for new research. Among these challenges, the design of reading recommendation systems has become a new expanding research focus following the emergence of digital libraries. Traditionally, libraries play a passive role in interaction with users due to the lack of effective search and recommendation tools. In this manuscript, we will study the creation of a reading recommendation system in which we'll try to exploit the possibilities of digital access to scientific information.
This thesis deals with deep learning applied to image classification tasks. The primary motivation for the work is to make current deep learning techniques more efficient and to deal with changes in the data distribution. We work in the broad framework of continual learning, with the aim to have in the future machine learning models that can continuously improve. We first look at change in label space of a data set, with the data samples themselves remaining the same. We consider a semantic label hierarchy to which the labels belong. We investigate how we can utilise this hierarchy for obtaining improvements in models which were trained on different levels of this hierarchy. The second and third contribution involve continual learning using a generative model. We analyse the usability of samples from a generative model in the case of training good discriminative classifiers. We propose techniques to improve the selection and generation of samples from a generative model. Following this, we observe that continual learning algorithms do undergo some loss in performance when trained on several tasks sequentially. We analyse the training dynamics in this scenario and compare with training on several tasks simultaneously. We make observations that point to potential difficulties in the learning of models in a continual learning scenario. Finally, we propose a new design template for convolutional networks. This architecture leads to training of smaller models without compromising performance. In addition the design lends itself to easy parallelisation, leading to efficient distributed training. In conclusion, we look at two different types of continual learning scenarios. We propose methods that lead to improvements. Our analysis also points to greater issues, to over come which we might need changes in our current neural network training procedure.
Understanding data is the main purpose of data science and how to achieve it is one of the challenges of data science, especially when dealing with big data. One existing technology that has been shown as particularly relevant for modeling, simulating and solving problems in complex systems are Multi-Agent Systems. The AMAS (Adaptive Multi-Agent Systems) theory proposes to solve complex problems for which there is no known algorithmic solution by self-organization. The cooperative behavior of the agents enables the system to self-adapt to a dynamical environment so as to maintain the system in a functionality adequate state. In this thesis, we apply this theory to Big Data Analytics. The aim of this thesis is to present the AMAS4BigData analytics framework based on the Adaptive Multi-agent systems technology, which uses a new data similarity metric, the Dynamics Correlation, for dynamic data relations discovery and dynamic display. This framework is currently being applied in the neOCampus operation, the ambient campus of the University Toulouse III - Paul Sabatier.
The focus of visual content is often people. Automatic analysis of people from visual data is therefore of great importance for numerous applications in content search, autonomous driving, surveillance, health care, and entertainment. The goal of this thesis is to learn visual representations for human understanding. Particular emphasis is given to two closely related areas of computer vision: human body analysis and human action recognition.
The impressive increase in the quantity of textual data makes it difficult today to analyze them without the assistance of tools. However, a text written in natural language is unstructured data, i.e. it cannot be interpreted by a specialized computer program, without which the information in the texts remains largely under-exploited. To accomplish this task, we propose a new approach by aligning two types of vector representations of entities that capture part of their meanings: word embeddings for text mentions and concept embeddings for concepts, designed specifically for this work. The alignment between the two is done through supervised learning. The developed methods have been evaluated on a reference dataset from the biological domain and they now represent the state of the art for this dataset. These methods are integrated into a natural language processing software suite and the codes are freely shared.
The rational drug discovery process has limited success despite all the advances in understanding diseases, and technological breakthroughs. Indeed, the process of drug development is currently estimated to require about 1.8 billion US dollars over about 13 years on average. We focus in this thesis on statistical approaches which virtually screen a large set of compounds against a large set of proteins, which can help to identify drug candidates for known therapeutic targets, anticipate potential side effects or to suggest new therapeutic indications of known drugs. This thesis is conceived following two lines of approaches to perform drug virtual screening: data-blinded feature-based approaches (in which molecules and proteins are numerically described based on experts'knowledge), and data-driven feature-based approaches (in which compounds and proteins numerical descriptors are learned automatically from the chemical graph and the protein sequence). We discuss these approaches, and also propose applications of virtual screening to guide the drug discovery process.
Business processes are everywhere and, as such, we must acknowledge them. Among all of them, hospital processes are of vital importance. Healthcare organizations invest huge amount of efforts into keeping these processes under control, as the allowed margin of error is so slight. This research work seeks to develop a methodology to endorse improvement of patient pathways inside healthcare organizations. Along the former functions, the contribution of this thesis are: The DIAG methodology which, through four different states, extracts knowledge from location data; the DIAG meta-model which supports both the interpretation of location data (from raw data to usable information) and the alignment of the domain knowledge (which are used for the diagnosing methods); two process discovery algorithms which explore statistical stability in event logs, application of Statistical Process Control (SPC) for the “enhancement notation” of Process Mining; the ProDIST algorithm for measuring the distance between process models; two automatic process diagnosing methods to detect causes of structural deviations in individual cases and common processes. The state of the art in this dissertation endorses the necessity for proposing such solutions. A case study within this research work illustrates the applicability of the DIAG methodology and its mentioned functions and methods.
The aim of this thesis is to develop techniques for segmenting strongly-structured scenes (e.g. building images) and weakly-structured scenes (e.g. natural images). Building images can naturally be expressed in terms of grammars and inference is performed using grammars to obtain the optimal segmentation. However, it is difficult and time consuming to write such grammars. To alleviate this problem, a novel method to automatically learn grammars from a given training set of image and ground-truth segmentation pairs is developed. Experiments suggested that such learned grammars help in better and faster inference. Surprisingly, even with out using any domain specific knowledge, we observed significant improvements in terms of performance on several benchmark datasets. Lastly,a novel technique based on convolutional neural networks is developed to segment images without any high-level structure. Image-adaptive filtering is performed within a CNN architecture to facilitate long-range connections. Experiments on different large scale benchmarks show significant improvements in terms of performance
Information k-means is a new mathematical framework that extends the classical k-means criterion, using the Kullback divergence as a distortion measure. The fragmentation criterion is an even broader extension where each signal is approximated by a combination of fragments instead of a single center. Using the fragmentation criterion as a distortion measure, we propose a new fragmentation algorithm for digital signals, conceived as a lossy data compression scheme. We tested the method on grey level digital images, where it was possible to label successfully translated patterns and rotated patterns. This lets us hope that transformation invariant pattern recognition could be approached in a flexible way using a general purpose data compression criterion. From a mathematical point of view, we derived two kinds of generalization bounds. This explains why our syntax trees may be meaningful. Second, combining PAC-Bayesian lemmas with the kernel trick, we proved non asymptotic dimension-free generalization bounds for the various information k-means and information fragmentation criteria we introduced. Using a new kind of PAC-Bayesian chaining, we also proved a bound of order O(log(n/k) sqrt{k log(k)/n}).
In the Big Data era, companies are moving away from traditional data-warehouse solutions whereby expensive and timeconsumingETL (Extract, Transform, Load) processes are used, towards data lakes in order to manage their increasinglygrowing data. Yet the stored knowledge in companies' databases, even though in the constructed data lakes, can never becomplete and up-to-date, because of the continuous production of data. Local data sources often need to be augmentedand enriched with information coming from external data sources. Unfortunately, the data enrichment process is one of themanual labors undertaken by experts who enrich data by adding information based on their expertise or select relevantdata sources to complete missing information. Such work can be tedious, expensive and time-consuming, making itvery promising for automation. We present in this work an active user-centric data integration approach to automaticallyenrich local data sources, in which the missing information is leveraged on the fly from web sources using data services. In doing so, we take into consideration a set of user preferences such as the cost threshold and the responsetime necessary to compute the desired answers, while ensuring a good quality of the obtained results.
Our thesis research is concerned with the estimation of motion saliency in image sequences. First, we have defined an original method to detect frames in which a salient motion is present. For this, we propose a framework relying on a deep neural network, and on the compensation of the dominant camera motion. Second, we have designed a method for estimating motion saliency maps. This method requires no learning. The motion saliency cue is obtained by an optical flow inpainting step, followed by a comparison with the initial flow. Third, we consider the problem of trajectory saliency estimation to handle progressive saliency over time. We have built a weakly supervised framework based on a recurrent auto-encoder that represents trajectories with latent codes. Performance of the three methods was experimentally assessed on real video datasets.
Effective management of large amounts of information has become a challenge increasingly important for information systems. Everyday, new information sources emerge on the web. Someone can easily find what he wants if (s)he seeks an article, a video or a specific artist. However,it becomes quite difficult, even impossible, to have an exploratory approach to discover new content. Recommender systems are software tools that aim to assist humans to deal with information overload. The work presented in this Phd thesis proposes an architecture for efficient recommendation of news. The ontology contains a formal vocabulary modeling a view on the domain knowledge. Carried out in collaboration with the company Actualis SARL, this work has led to the marketing of a new highly competitive product, FristECO Pro'fil.
Facing the significant complexity of the mammography area and the massive changes in its data, the need to contextualize knowledge in a formal and comprehensive modeling is becoming increasingly urgent for experts. It is within this framework that our thesis work focuses on unifying different sources of knowledge related to the domain within a target ontological modeling. On the one hand, there is, nowadays, several mammographic ontological modeling, where each resource has a distinct perspective area of interest. On the other hand, the implementation of mammography acquisition systems makes available a large volume of information providing a decisive competitive knowledge. However, these fragments of knowledge are not interoperable and they require knowledge management methodologies for being comprehensive. In this context, we are interested on the enrichment of an existing domain ontology through the extraction and the management of new knowledge (concepts and relations) derived from two scientific currents: ontological resources and databases holding with past experiences. Our approach integrates two knowledge mining levels: The first module is the conceptual target mammographic ontology enrichment with new concepts extracting from source ontologies. The goal is to reduce the alignment task from two full ontologies to two reduced conceptual clusters. The second stage consists on aligning the two hierarchical structures of both source and target ontologies. Thirdly, the validated alignments are used to enrich the reference ontology with new concepts in order to increase the granularity of the knowledge base. The second level of management is interested in the target mammographic ontology relational enrichment by novel relations deducted from domain database. The latter includes medical records of mammograms collected from radiology services. The latter is to filter and classify the rules in order to facilitate their interpretation and validation by expert, vi) The enrichment of the ontology by new associations between concepts. This approach has been implemented and validated on real mammographic ontologies and patient data provided by Taher Sfar and Ben Arous hospitals.
In this thesis we explored the ability of magnetic models of statistical physics to extract the essential information contained in texts. Documents are represented as sets of interacting magnetic units, the intensity of such interactions are measured and they are used to calculate quantities that are evidence of the importance of information scope. We propose two new methods. Several adaptations were necessary to adapt the energy calculation to a wide range of tasks such as summarisation, information retrieval, document classification and thematic segmentation. Furthermore, and even exploratory, we propose a second algorithm that defines a grammatical coupling between types of terms to retain the important terms and produce contractions. In this way, the compression of a sentence is the ground state of the chain of terms. As this compression is not necessarily good, it was interesting produce variants by thermal fluctuations. We have done simulations Metropolis Monte-Carlo with the aim of finding the ground state of this system that is analogous to spin glass.
Underlying competing mechanisms drive the evolution of gas in the interstellar medium. Electronic relaxation from the brightest excited state has been simulated for neutral polyacenes with 2 to 7 aromatic cycles. The results display a striking alternation in decay times of the brightest singlet state computed for polyacenes with up to 6 aromatic cycles, which is correlated with a qualitatively similar alternation of energy gaps between the brightest state and the state lying just below in energy. The results show that the electronic population of the brightest excited state in chrysene decays an order-of-magnitude faster than that in tetracene. This is correlated with a significant difference in energy gaps between the brightest state and the state lying just below in energy, which is consistent with the previous conclusions for polyacenes. A last major development concerns the use of Machine Learning (ML) algorithms that have been proposed as a way to avoid most of the computationally-demanding electronic structure calculations. It aims to assess the performance of neural networks algorithms applied to excited-state dynamics. Electronic relaxation in neutral phenanthrene has been chosen as a test case due to the diversity of available experimental results. Several neural networks have been trained with different parameters and their respective accuracy and efficiency analyzed. In addition, approximate trajectory surface hopping schemes have been interfaced to ML-based PESs and gradients, resulting in non-adiabatic dynamics simulations at a negligible cost. Various simplified hopping approaches have been compared with FSSH. Overall, ML is found to be a highly promising tool for nanosecond-long molecular dynamics in excited states. This PhD research opens new avenues to investigate theoretical photophysics of large molecular complexes. Last but not least, the theoretical tools developed and implemented in deMon-Nano in a modular way can be further combined with other advanced (such as Configuration Interaction) DFTB techniques better adapted to charge-transfer states.
Moreover, NLP researchers have focused much of their effort on training NLP models on the news domain, due to the availability of training data. However, many research works have highlighted that models trained on news fail to work efficiently on out-of-domain data, due to their lack of robustness against domain shifts. This thesis presents a study of transfer learning approaches, through which we propose different methods to take benefit from the pre-learned knowledge from high-resourced domains to enhance the performance of neural NLP models in low-resourced settings. Indeed, despite the importance of its valuable content for a variety of applications (e.g. public security, health monitoring, or trends highlight), this domain is still lacking in terms of annotated data. We~present different contributions. First, we propose two methods to transfer the knowledge encoded in the neural representations of a source model -- pretrained on large labelled datasets from the source domain -- to the target model, further adapted by a fine-tuning on few annotated examples from the target domain. Second, we perform a series of analysis to spot the limits of the above-mentioned proposed methods. We find that even though transfer learning enhances the performance on social media domain, a hidden negative transfer might mitigate the final gain brought by transfer learning. Besides, an interpretive analysis of the pretrained model shows that pretrained neurons may be biased by what they have learnt from the source domain, thus struggle with learning uncommon target-specific patterns. Third, stemming from our analysis, we propose a new adaptation scheme which augments the target model with normalised, weighted and randomly initialised neurons that beget a better adaptation while maintaining the valuable source knowledge. Finally, we propose a model that, in addition to the pre-learned knowledge from the high-resource source-domain, takes advantage of various supervised NLP tasks.
It is expected to diagnose its early stage, Mild Cognitive Impairment (MCI), then interventions can be applied to delay the onset. Fluorodeoxyglucose positron emission tomography (FDG-PET) is considered as a significant and effective modality to diagnose AD and the corresponding early phase since it can capture metabolic changes in the brain thereby indicating abnormal regions. For this purpose, three independent novel methods are proposed. Such connectivities are represented by either similarities or graph measures among regions. Then combined with each region's properties, these features are fed into a designed ensemble classification framework to tackle problems of AD diagnosis and MCI conversion prediction. The spatial gradient is quantified by a 2D histogram of orientation and expressed in a multiscale manner. The results are given by integrating different scales of spatial gradients within different regions. Such an architecture can facilitate convolutional operations, from 3D to 2D, and meanwhile consider spatial relations, which is benefited from a novel mapping layer with cuboid convolution kernels. Experiments conducted on public dataset show that the three proposed methods can achieve significant performance and moreover, outperform most state-of-the-art approaches.
This thesis addresses the problems of phonemic variability and confusability from the pronunciation modeling perspective for an automatic speech recognition (ASR) system. In particular, several research directions are investigated. Since the addition of alternative pronunciation may introduce homophones (or close homophones), there is an increase of the confusability of the system. A novel measure of this confusability is proposed to analyze it and study its relation with the ASR performance. This pronunciation confusability is higher if pronunciation probabilities are not provided and can potentially severely degrade the ASR performance. It should, thus, be taken into account during pronunciation generation. Discriminative training approaches are, then, investigated to train the weights of a phoneme confusion model that allows alternative ways of pronouncing a term counterbalancing the phonemic confusability problem. The objective function to optimize is chosen to correspond to the performance measure of the particular task. In this thesis, two tasks are investigated, the ASR task and the KeywordSpotting (KWS) task. For experiments conducted on KWS, the Figure of Merit (FOM), a KWS performance measure, is directly maximized.
The present study is a terminological analysis of the financial domain, in a natural language processing perspective. The derived and compounded forms of a corpus are analysed, emphasis being put on the syntactic and semantic relations between their elements, in order to pick out the constituents that are most productive in the economical language. The research also includes the idiomatic and recurrent expressions of the corpus. As a conclusion, korean and french economical terms are contrasted, so as to supply the translater with an editing and translation toolbox.
Vibrational spectroscopy (VS) relates to the specific optical techniques of infrared and Raman spectroscopy (RS). These techniques probe molecular vibrations of the sample when light interacts with it, which present 'fingerprints' of the global biochemistry. Both techniques hold great promise in disease diagnostics, especially with 'liquid biopsies' for biofluids. This study developed bio-spectroscopic methodologies to query the serum biochemistry towards rapid diagnosis and detection of diseases. Beyond proof of concept with investigations in to preanalytical variation (which proved no effect is seen on the serum profile) via serum freeze-thawing and environmental drying, three diagnostic studies were sought; from patient cases, i.e., hepatocellular carcinoma from cirrhotic sera, cirrhosis from fibrotic sera, and varying degrees of gliomas from brain tumour sera. Throughout, a suite of FTIR and Raman spectroscopy techniques were employed/developed, such serum ATR-FTIR, HT-FTIR (high throughput screening), Raman microspectroscopy on liquid and dried human sera, and Raman microspectroscopy on liquid sera. Advanced multivariate analysis and chemometric approaches were employed such as PCA, HCA, PLS-DA, forward LDA, radial basis function SVM-LOOCV, Random forest classifiers, all towards developing a robust disease classifier. Across all diagnostic studies, results showed moderate-good diagnostic ability with one method succeeding the other in various cases. It was shown that spectroscopy combined with advanced chemometric methods can provide a good adjunct to clinical screening settings, such as point-of-care areas.
This thesis considers the role of lexical cohesion in various approaches of discourse analysis. Two main hypotheses are studied: - distributional analysis, which allows to bring together lexical units based on the syntactic contexts they share, highlights diverse semantic relations which can be employed in the detection of lexical cohesion in texts; - lexical cues are involved in discourse signalization and can be used both at a local level (identification of rhetorical relations between elementary discourse units) and at a global level (detection or characterization of higher level segments). In reference to the first hypothesis, we show that a distributional resource is strongly relevant in the analysis of a wide panel of relations having lexical cohesion roles in texts. We introduce projection and filtering methods for this distributional resource. In reference to the second hypothesis, we provide a series of outlooks showing the improvement brought by careful consideration of lexical cohesion in a large panel of settings within the study of textual organisation and its automatic detection: thematic segmentation of texts, enumerative structures characterization, study of the correlation between lexicon and the rhetorical structure of discourse, and finally detection of realisations of a specific discourse relation, the Elaboration relation.
Despite the development of syndromic surveillance and of modern decision making methods, outbreak surveillance within hospitals remains mainly manual. Yet, the use of detection algorithms could improve surveillance efficiency, broaden its scope and overall reduce the time spent to accomplish this task. However, the methodological quality of the evaluations published to this date is too low to conclude about the real utility of these tools. We propose in this memoir several key elements of an adequate methodological framework for early outbreak detection, as well as a first data set containing labelled potential outbreaks. This data set can be used by researchers to develop, evaluate and compare detection algorithms. However, as research on surveillance systems and healthcare artificial intelligence tools has shown, implementing these tools can be difficult, and the results observed in real life can be quite different from those of the development phase. It is therefore necessary to use alternative indicators and evaluation methods to assess the real utility of these tools in real life, measuring for example their acceptability, ease of use, costs and impacts on daily practices.
In the context of the aging population, the aim of this thesis is to include in the living environment of the elderly people an automatic speech recognition (ASR) system, which can recognize calls to alert the emergency services. The acoustic models of ASR systems are mostly learned with non-elderly speech, delivered in a neutral way, and read. However, in our context, we are far from these ideal conditions (aging and expressive voice). From these corpora, a study on the differences between young and old voices, and between neutral and emotional voice permit to develop an ASR system adapted to the task. This system was then evaluated on data recorded during an experiment in realistic situation, including falls played by volunteers.
The acoustic-to-articulatory inversion of speech consist in the recovery of the vocal tract shape from the speech signal. This problem is tackled with an analysis-by-synthesis method depending on a physical model of speech production controlled by a small number of parameters describing the vocal tract shape: the jaw opening, the shape and the position of the tongue and the position of lips and larynx. In order to approach the geometry of the speaker, the articulatory model is built with articulatory contours from cineradiographic images of the sagittal view of the vocal tract. This articulatory synthesizer allows us to create a table made up with couples associating a articulatory vector with the corresponding acoustic vector. The formants (resonance frequency of the vocal tract shape) are not used as acoustic vector because their extraction is not always reliable causing errors during inversion. The cepstral coefficients are used as acoustic vector. Moreover, the source effect and the mismatch between the speaker vocal tract and the articulatory model are considered explicitly comparing the natural spectrum with those produced by the synthesizer because we have the both signals
This thesis proposes a method for supporting flexible coordination in multi-agent systems (MASs). In other words, we aim at influencing societies of artificial agents such that they can handle complex or evolving environments and collective goals (e.g. robots providing an emergency support capable of handling various hazards, climatic conditions, status of victims). Towards achieving this goal, we first investigated why in human societies, for which MASs can be seen as an "artificial" counterpart, humans manage to coordinate relatively flexibly comparatively with artificial agents in MASs. We discovered that culture is a key factor of this relative success. Briefly, when humans share a cultural background, they manage to coordinate more flexibly because they share a common idea about what ``working together''means. The lack raises our goal: we want to better understand how culture can be integrated within and used for coordinating artificial societies. This goal raises the following research question: how can human-like culture be used as a tool for supporting coordination in artificial societies? As a preliminary step for answering this question, we need first to answer this question: (how) can the influence human-like cultures be integrated within artificial societies? In turn, this question raises a third one to be answered first: how does culture influence coordination in human societies? As a first step, we expand general theories of culture for conceptualizing its influence in the context of coordination. From a generic perspective, we explain that culture influences individual decisions that support matching expectations and coherent interaction patterns, leading in turn to (generally) better collective performance. From a more specific perspective, we specify how the core acknowledged patterns of the influence of culture (e.g. cultural importance given to power status, to rules) apply in the context of coordination (e.g. culture influences the likeliness that leaders are (made) responsible for making decisions for subordinates vs. proposing alternatives). As a second step, we study how to replicate human-like influences of culture on coordination within artificial societies. First, since culture is grounded within individual decisions, we investigate the core culturally-sensitive decision aspects that impact the most (flexible) coordination in human societies. We discover that values, what people consider as ``good''or ``important''(e.g. honesty, obedience, autonomy), constitute such an aspect by deeply supporting a wide range of (interaction-related) decisions. Then, for illustrating how to replicate influence of culture within artificial societies, we build an value-sensitive agent decision architecture that can make coordination-related decisions. Finally, we illustrate that our architecture can replicate the influence of culture on coordination through two simulations that replicate known coordination-related cultural phenomena. As a third step, we study how human-like values can be used for supporting coordination in artificial societies. First, we investigate the range of coordination problems for which values can offer an operational means for supporting coordination. As in human societies, values are particularly adequate for problems with complex and dynamic environments, requiring agents to make coordination-related decisions. Then, towards concretely implementing values, we study the technical details to consider when using values for supporting flexible coordination (e.g. how to concretely design values and integrating them within decision processes).
Psychiatry is a medical speciality that aims at providing diagnosis and treating mental disorders. Despite internationally acknowledged criteria leading to diagnostic categories, most psychiatric disorders are syndromes with common symptoms or dimensions between these diagnostic categories. In addition, the analysis of the prevalence and incidence of social and environmental risk factors of diseases is crucial to understand and treat them and might have significant impacts on policy decisions (therapeutic as well as the length or the cost of hospitalisation). This overlap between diagnoses and the heterogeneity within the defined diagnoses stresses the need to improve our capability to detect, to quantify the behaviour and to model the symptoms and the social and environmental risk factors associated to psychiatric disorders. To that end, we propose OntoPsychia, an ontology for psychiatry, divided in two modules: social and environmental factors of mental disorders and mental disorders. OntoPsychia associated with dedicated tools will help to perform semantic research in Patient Discharges Summaries (PDS), to represent comorbidity, to reach a consensus on descriptive categories of mental disorders. In a first step, we developed two ontological modules using two different methods. The first proposes an analysis of PDS, while the second proposes an alignment of psychiatric classifications to meet the need for consensus. In a second step, we have developed a methodological framework to validate the structure and semantics of ontologies.
However, multimodality remains limited in current human-computer interfaces. For example, posture is less explored than other modalities, such as speech and facial expressions. The postural expressions of others have a huge impact on how we situate and interpret an interaction. Devices and interfaces for representing full-body interaction are available (e.g., Kinect and full-body avatars), but systems still lack computational models relating these modalities to spatial and emotional communicative functions. The goal of this thesis is to lay the foundation for computational models that enable better use of posture in human-computer interaction. How can these representations inform the design of virtual characters' postural expressions? In our approach, we start with the manual annotation of video corpora featuring postural expressions. We define a coding scheme for the manual annotation of posture at several levels of abstraction and for different body parts. These representations were used for analyzing the spatial and temporal relations between postures displayed by two human interlocutors during spontaneous conversations. Next, representations were used to inform the design of postural expressions displayed by virtual characters. For studying postural expressions, we selected one promising, relevant component of emotions: the action tendency. Finally, postural expressions were designed for a virtual character used in an ambient interaction system. These postural and spatial behaviors were used to help users locate real objects in an intelligent room (iRoom).
The goal of this thesis is to show the various aspects of corpus annotation in the Arabic language. We present our publications on corpus annotation and lexical resources creation in the Arabic language. First, we discuss the methods, the language difficulties, the annotation guidelines, the annotation effort optimization limits and how we adapted some of the existing annotation procedures to the Arabic language. Furthermore, we show the complementarity between the different layers of annotations. Finally, we illustrate the importance of our work for natural language processing by illustrating some examples of resources and applications.
Since the introduction of Google's PageRank method for Web searches in the late 1990s, graph algorithms have been part of our daily lives. In the mid 2000s, the arrival of social networks has amplified this phenomenon, creating new use-cases for these algorithms. Relationships between entities can be of multiple types: user-user symmetric relationships for Facebook or LinkedIn, follower-followee asymmetric ones for Twitter or even user-content bipartite ones for Netflix or Amazon. They all come with their own challenges and the applications are numerous: centrality calculus for influence measurement, node clustering for knowledge discovery, node classification for recommendation or embedding for link prediction, to name a few. In the meantime, the context in which graph algorithms are applied has rapidly become more constrained. On the one hand, the increasing size of the datasets with millions of entities, and sometimes billions of relationships, bounds the asymptotic complexity of the algorithms for industrial applications. On the other hand, as these algorithms affect our daily lives, there is a growing demand for explanability and fairness in the domain of artificial intelligence in general. For example, the European Union has published a set of ethics guidelines for trustworthy AI. This calls for further analysis of the current models and even new ones. This thesis provides specific answers via a novel analysis of not only standard, but also extensions, variants, and original graph algorithms. Scalability is taken into account every step of the way. Following what the Scikit-learn project does for standard machine learning, we deem important to make these algorithms available to as many people as possible and participate in graph mining popularization. Therefore, we have developed an open-source software, Scikit-network, which implements and documents the algorithms in a simple and efficient way. With this tool, we cover several areas of graph mining such as graph embedding, clustering, and semi-supervised node classification.
One of the core human abilities is that of interpreting symbols. Notwithstanding decades of neuropsychological and neuroimaging work on the cognitive and neural substrate of semantic representations, many questions are left unanswered. In the first part, I review the different theoretical positions and empirical findings on the cognitive and neural correlates of semantic representations. Crucially, I propose an operational distinction between motor-perceptual dimensions (i.e., those attributes of the objects referred to by the words that are perceived through the senses) and conceptual ones (i.e., the information that is built via a complex integration of multiple perceptual features). In the second part, I present the results of the studies I conducted in order to investigate the automaticity of retrieval, topographical organization, and temporal dynamics of motor-perceptual and conceptual dimensions of word meaning. The results suggest that the neural substrates of different components of symbol meaning can be dissociated in terms of localization and of the feature of the signal encoding them, while sharing a similar temporal evolution.
Continuous word representations (word type embeddings) are at the basis of most modern natural language processing systems, providing competitive results particularly when input to deep learning models. However, important questions are raised concerning the challenges they face in dealing with the complex natural language phenomena and regarding their ability to capture natural language variability. To better handle complex language phenomena, much work investigated fine-tuning the generic word type embeddings or creating specialized embeddings that satisfy particular linguistic constraints. While this can help distinguish semantic similarity from other types of semantic relatedness, it may not suffice to model certain types of relations between texts such as the logical relations of entailment or contradiction. The first part of the thesis investigates encoding the notion of entailment within a vector space by enforcing information inclusion, using an approximation to logical entailment of binary vectors. We further develop entailment operators and show how the proposed framework can be used to reinterpret an existing distributional semantic model. Evaluations are provided on hyponymy detection as an instance of lexical entailment. Another challenge concerns the variability of natural language and the necessity to disambiguate the meaning of lexical units depending on the context they appear in. For this, generic word type embeddings fall short of being successful by themselves, with different architectures being typically employed on top to help the disambiguation. As type embeddings are constructed from and reflect co-occurrence statistics over large corpora, they provide one single representation for a given word, regardless of its potentially numerous meanings. Furthermore, even given monosemous words, type embeddings do not distinguish between the different usages of a word depending on its context. In that sense, one could question if it is possible to directly leverage available linguistic information provided by the context of a word to adjust its representation. Would such information be of use to create an enriched representation of the word in its context? And if so, can information of syntactic nature aid in the process or is local context sufficient? In the second part of the thesis, we investigate one possible way to incorporate contextual knowledge into the word representations themselves, leveraging information from the sentence dependency parse along with local vicinity information. We propose syntax-aware token embeddings (SATokE) that capture specific linguistic information, encoding the structure of the sentence from a dependency point of view in their representations. This enables moving from generic type embeddings (context-invariant) to specific token embeddings (context-aware). While syntax was previously considered for building type representations, its benefits may have not been fully assessed beyond models that harvest such syntactical information from large corpora. The obtained token representations are evaluated on natural language understanding tasks typically considered in the literature: sentiment classification, paraphrase detection, textual entailment and discourse analysis. We empirically demonstrate the superiority of the token representations compared to popular distributional representations of words and to other token embeddings proposed in the literature. The work proposed in the current thesis aims at contributing to research in the space of modelling complex phenomena such as entailment as well as tackling language variability through the proposal of contextualized token embeddings.
This work introduces the Kabyle language to the field of Natural Language Processing by giving it a database for the NooJ software that allows the automatic recognition of linguistic units in a written corpus. We have divided the work in four parts. The first part is the place to give a snapshot on the history of formal linguistics, to present the field of NLP and the NooJ software and the linguistic units that have been treated. The second part is devoted to the description of the process that has been followed for the treatment and the integration of Kabyle verbs in NooJ. We have built a dictionary that contains 4508 entries and 8762 derived components and some models of flexion for each type which have been linked with each entry. In the third part, we have explained the processing of nouns and other units. We have built, for the nouns, a dictionary (3508 entries, 501 derived components) that have been linked to the models of flexion and for the other units (870 entries including adverbs, prepositions, conjunctions, interrogatives, personal pronouns, etc.). The second and third part are completed by examples of applications on a text, this procedure has allowed us to show with various sort of annotations the ambiguities. Regarding the last part we have devoted it to ambiguities, after having identified a list of various types of amalgams, we have tried to show, with the help of some examples of syntactic grammars, some of the tools used by NooJ for disambiguation.
Introduced as part of the last Message Understanding Conferences dedicated to information extraction, Named Entity extraction is a well-studied task in Natural Language Processing. Following this success, named entity treatment is moving towards new research prospects with, among others, disambiguation,and fined-grained annotation. However, this new challenges make even more crucial the question of named entity definition, which was not much discussed until now. Two main lines were explored during this PhD project: first we tried to propose a definition of named entities and then we experimented disambiguation methods. After a presentation and a state of the art of the named entity recognition task, we had to examine, from a methodological point of view, how to tackle the question of the definition of named entities. Our approach led us to study, firstly, the linguistic side, with proper names and definite descriptions and, secondly, the Computing side, this development aiming at, finally, proposing a named entity definition that takes into account language aspects but also informatic Systems capacities and requirements. The continuation of the dissertation is about more experimental works, with a presentation of experiments about fined-grained named entity annotation and metonymy resolution methods.
A research on the etymology and history of linguistics terminology will provide an important gain in historical and etymological knowledge. This gain concerning the description of particular scientific terms has broader implications: improved knowledge on the coining process of the linguistics vocabulary, as well as indications about the possible consequences that a massive progress of the description of an important portion of the internationalisms could have on the etymological sub-discipline itself. This thesis also contributes to a methodological progress in a weak area of the etymological science, a progress all the more significant as it is crucial to our understanding of the current functioning of lexical creation in French. An outstanding characteristic of this thesis is that it lays at the intersection of several fields, covering lexicology (and lexicography, since it encompasses a section devoted to lexicographical entries), terminology (since the subject of these articles is linguistic terms) and etymology. Furthermore, the philological aspect of this research is important because the work on the beginning of the existence of terms led us to analyze various types of texts belonging to different periods. We then exploit the new data offered by the lexicographical entries in order to provide a synthetic view of the constitution of the French linguistics terminology.
The main goal of this thesis is the development of event-based algorithms for visual detection and tracking. This algorithms are specifically designed to work on the output of neuromorphic event-based cameras. This type of cameras are a new type of bioinspired sensors, whose principle of operation is based on the functioning of the retina: every pixel is independent and generates events asynchronously when a sufficient amount of change is detected in the luminance at the corresponding position on the focal plane. This new way of encoding visual information calls for new processing methods. The resulting virtual mechanical system is simulated with every incoming event. Next, a line and segment detection algorithm is introduced, which can be employed as an event-based low level feature. Two event-based methods for 3D pose estimation are then presented. The first of these 3D algorithms is based on the assumption that the current estimation is close to the true pose of the object, and it consequently requires a manual initialization step. The second of the 3D methods is designed to overcome this limitation. All the presented methods update the estimated position (2D or 3D) of the tracked object with every incoming event.
This thesis investigates acoustic-to-articulatory inversion, i.e. recovering articulatory movements from the speech signal. In this work, we present an important evolution of codebooks methods, i.e. methods using acoustic-articulatory tuples precomputed using an acoustic synthesis model. Apart from the inversion method, we present the introduction of two types of constraints: generic phonetic constraints, derived from the analysis by human experts of articulatory invariance for vowels, and visual constraints, i.e. constraints derived automatically from a video signal, in our case a stereo video signal, thus allowing us to perform multimodal inversion.
Recent years have witnessed a growing renewed interest in the use of graphs as a reliable means for representing and modeling data. Thereby, graphs enable to ensure efficiency in various fields of computer science, especially for massive data where graphs arise as a promising alternative to relational databases for big data modeling. In this regard, querying data graph proves to be a crucial task to explore the knowledge in these datasets. In this dissertation, we investigate two main problems. In the first part we address the problem of detecting patterns in larger graphs, called the top-k graph pattern matching problem. We introduce a new graph pattern matching model named Relaxed Graph Simulation (RGS), to identify significant matches and to avoid the empty-set answer problem. We formalize and study the top-k matching problem based on two classes of functions, relevance and diversity, for ranking the matches according to the RGS model. We also consider the diversified top-k matching problem, and we propose a diversification function to balance relevance and diversity. Moreover, we provide efficient algorithms based on optimization strategies to compute the top-k and the diversified top-k matches according to the proposed model. The proposed approach is optimal in terms of search time and flexible in terms of applicability. The analyze of the time complexity of the proposed algorithms and the extensive experiments on real-life datasets demonstrate both the effectiveness and the efficiency of these approaches. In the second part, we tackle the problem of graph querying using aggregated search paradigm. Firstly, we give the motivation behind the use of such a paradigm, and we explain the potential benefits compared to traditional querying approaches. Furthermore, we propose a new method for aggregated tree search, based on approximate tree matching algorithm on several tree fragments, that aims to build, the extent possible, a coherent and complete answer by combining several results. The proposed solutions are shown to be efficient in terms of relevance and quality on different real-life datasets
Multi-objective problems arise in many real world scenarios where one has to find an optimal solution considering the trade-off between different competing objectives. Having motivated our work, we study two multi-objective learning tasks in detail. We study the problem as finding an optimal trade-off between different classification errors, and propose an algorithm based on cost-sensitive classification. In the second task, we study the problem of diverse ranking in information retrieval tasks, in particular recommender systems.
Secondly, we built a corpus of translations done by domain experts and we studied it to reinforce the analysis. The most obvious difference are the use of machine translation (MT) and the production context. By looking at current MT technologies, it appears that they can either post-edit the texts that are in target language, after the translation process or pre-edit the texts that are in source language, before the translation process. We propose to take advantage of the unprecedented situation of having the "writer" and the "translator" working together, to use the writer expertise during the translation process by creating a new MT feature that allow editing during the process.
This thesis deals with environmental scene analysis, the auditory result of mixing separate but concurrent emitting sources. The sound environment is a complex object, which opens the field of possible research beyond the specific areas that are speech or music. For a person to make sense of its sonic environment, the involved process relies on both the perceived data and its context. For each experiment, one must be, as much as possible,in control of the evaluated stimuli, whether the field of investigation is perception or machine learning. Nevertheless, the sound environment needs to be studied in an ecological framework, using real recordings of sounds as stimuli rather than synthetic pure tones. We therefore propose a model of sound scenes allowing us to simulate complex sound environments from isolated sound recordings. The high level structural properties of the simulated scenes -- such as the type of sources, their sound levels or the event density -- are set by the experimenter. The usefulness of the proposed model is assessed on two areas of investigation. The first is related to the soundscape perception issue, where the model is used to propose an innovative experimental protocol to study pleasantness perception of urban soundscape. The second tackles the major issue of evaluation in machine listening, for which we consider simulated data in order to powerfully assess the generalization capacities of automatic sound event detection systems.
This work focuses on the non-standard linguistic usages of less-literate writers during the First World War, based on their private correspondence.
As of 2017, we live in a data-driven world where data-intensive applications are bringing fundamental improvements to our lives in many different areas such as business, science, health care and security. This has boosted the growth of the data volumes (i.e., deluge of Big Data). To extract useful information from this huge amount of data, different data processing frameworks have been emerging such as MapReduce, Hadoop, and Spark. Traditionally, these frameworks run on largescale platforms (i.e., HPC systems and clouds) to leverage their computation and storage power. Usually, these largescale platforms are used concurrently by multiple users and multiple applications with the goal of better utilization of resources. Though benefits of sharing these platforms exist, several challenges are raised when sharing these large-scale platforms, among which I/O and failure management are the major ones that can impact efficient data processing. To this end, we first focus on I/O related performance bottlenecks for Big Data applications on HPC systems. We start by characterizing the performance of Big Data applications on these systems. We identify I/O interference and latency as the major performance bottlenecks. Next, we zoom in on I/O interference problem to further understand the root causes of this phenomenon. Then, we propose an I/O management scheme to mitigate the high latencies that Big Data applications may encounter on HPC systems. Second, we focus on the impact of failures on the performance of Big Data applications by studying failure handling in shared MapReduce clusters. We introduce a failure-aware scheduler which enables fast failure recovery while optimizing data locality thus improving the application performance.
The thesis takes part within works on the automated interpretation of complex manifestations of discourse facts that are not graspable by the usual methods of discourse analysis. Based on political interviews transcripts, the study focuses on the linguistic mechanism of "nomination". As part of the ANR TALAD project, the aim is to do theoretical and descriptive works on discourse facts to carry out prototyping, implementation, experimentation and validation of approaches for the detection and the characterization of nominations. In particular, the purpose is to study the outputs of entity and co-reference recognition systems to determine their contribution to the nomination detection. One of the challenges of the thesis is to propose a classification system for the Reticular company - which is interested in the qualification of political actors as "doctrine designers" or "influencers".
This thesis focus on the automatic translation approaches for User-Generated Contents (e.g. texts found in social media). It addresses and explores the state-of-the-art methods on automatic translation to cope with the two key scientific challenges: 1) the need to overcome limitations of a phrase-by-phrase translation using a larger context information, and 2) the very noisy characteristics of such texts compared to the canonical and edited contents usually used in NLP, the former being extremely diverse, abounding in abbreviations, orthographic or typographic mistakes and grammatical errors.
This thesis comes within the framework of information retrieval on the Web with the help of methods inspired by the computational linguistics researchs of LaLICC laboratory. The purpose of our work is to develop a tool that allows to assist in an interactive way, during an information retrieval session, a user who wants to collect some information available on the Web on a given notion or topic. The main idea which is realized through this tool called RAP (Research, Analyse, Propose), consists in foàcusing the search according to one or more points of view which allow to satisfy, in a gradual way, the user information needs. Conceptually, a great part of our work consisted to study how to characterize the user need notion that constitute the intuitive foundation on which rely the notion of points de view. In this order, the linguistic knowledge we use allows as to not see the need notion as necessarily related to a particular community of users. Our reflexions led us to pose the elementary or complex informational need notions as a theoritical framework of our research. To these needs correspond the points of view that a user can select to focus his search. Technically, direct the search according to a point of view means that we reformulate the user query by integrating there the linguistics markers relating to the chosen point of view, for example that of Causality or that of Quotation. The purpose of this reformulation is in one part, to reduce significantly the noise, and in the other part to target Web pages having rich semantic contents. The realization of the points of view by this reformulation technique implies the use of linguistics markers resulting from the team LaLICC works on the semantic filtering of the texts. Each class of these markers relating to the selected point of view intervenes in the process of user query reformulation through the reformulation technique that we developed; then, in the part of extraction of the paragraphs or textual segments of the document where signs of the chosen point of view are detected, thus to help the user to make a better choice of Web pages among the resulted pages computed by the search engine (AltaVista in our case). The whole of the processes was realized by the construction of the RAP tool written in Java and including a convivial interface, in which, 27 points of view resulting from the various approaches of the sixth principal points of view: Causality, Descriptive Relations, Quotation, Theme/Position, Problem/Solution and Actors.
Probabilistic parsing is one of the most attractive research areas in natural language processing. Current successful probabilistic parsers require large treebanks which are difficult, time consuming, and expensive to produce. Therefore, we focused our attention on less-supervised approaches. We suggested two categories of solution: active learning and semi-supervised algorithm. Active learning strategies allow one to select the most informative samples for annotation. Most existing active learning strategies for parsing rely on selecting uncertain sentences for annotation. We show in our research, on four different languages (French, English, Persian, and Arabic), that selecting full sentences is not an optimal solution and propose a way to select only subparts of sentences. As our experiments have shown, some parts of the sentences do not contain any useful information for training a parser, and focusing on uncertain subparts of the sentences is a more effective solution in active learning.
We are interested in multimodal human-computer communication systems that use the following modes: speech, gesture and vision. The user communicates with the system by oral utterance in natural language and/or by gesture. The user's request contains his/her goal and the designation of objects (referents) required to the goal realisation. The system should identify in a precise and non ambiguous way the designated objects. The main aspects of the realisation consist in modeling the natural language processing in speech environment, the gesture processing and the visual context (visual salience use) while taking into account the difficulties in multimodal context: speech recognition errors, natural language ambiguity, gesture imprecision due to the user performance, designation ambiguity due to the perception of the displayed objects or to the display topology. To complete the interpretation of the user's request, we propose a method for fusion/verification of modalities processing results to find the designated objects by the user.
Automatic emotion recognition in speech is a relatively recent research subject in the field of natural language processing considering that the subject has been proposed for the first time about ten years ago. This subject is nowadays the object of much attention, not only in academia but also in industry, thank to the increased models performance and system reliability. The first studies were based on acted data and non spontaneous speech. Up until now, most experiments carried out by the research community on emotions were realized pre-segmented sequences and with a unique speaker and not on spontaneous speech with several speaker. With this methodology the models built on acted data are hardly usable on data collected in natural context The studies we present in this thesis are based on call center's conversation with about 1620 hours of dialogs and with at least two human speakers (a commercial agent and a client) for each conversation. Our aim is the detection, via emotional expression, of the client satisfaction. In the first part of this work we present the results we obtained from models using only acoustic or linguistic features for emotion detection. We show that to obtain correct results an approach taking into account only one of these features type is not enough. To overcome this problem we propose the combination of three type of features (acoustic, lexical and semantic). We show that the use of models with features fusion allows higher score for the recognition step in all case compared to the model using only acoustic features. This gain is also obtained if we use an approach without manual pre-processing (automatic segmentation of conversation, transcriptions based on automatic speech recognition). In the second part of our study we notice that even if models based on features combination are relevant for emotion detection the amount of data we use in our training set is too small if we used it on large amount of data test. To overcome this problem we propose a new method to automatically complete training set with new data. These additions allow us to double the amount of data in our training set and increase emotion recognition rate compare to the non-enrich models. Finally, in the last part we choose to evaluate our method on entire conversation and not only on conversations turns as in most studies. To define the classification of a dialog we use models built on the previous steps of this works and we add two new features group: i) structural features including information like the length of the conversation, the proportion of speech for each speaker in the dialog ii) dialogic features including informations like the topic of a conversation and a new concept we call “affective implication”. The aim of the affective implication is to represent the impact of the current speaker's emotional production on the other speakers. We show that if we combined all information we can obtain results close to those of humans.
The vision of pervasive computing of building interactive smart spaces in the physical environment is gradually heading from the research domain to reality. Computing capacity is moving beyond personal computers to many day-to-day devices, and these devices become, thanks to multiple interfaces, capable of communicating directly with one another or of connecting to the Internet. In this thesis, we are interested in a kind of pervasive computing environment that forms what we call an Intermittently Connected Hybrid Network (ICHN). An ICHN is a network composed of two parts: a fixed and a mobile part. The fixed part is formed of some fixed infostations (potentially connected together with some fixed infrastructure, typically the Internet). The mobile part, on the other hand, is formed of smartphones carried by nomadic people. While the fixed part is mainly stable, the mobile part is considered challenging and form what is called an Opportunistic Network. Indeed, relying on short-range communication means coupled with the free movements of people and radio interferences lead to frequent disconnections. To perform a network-wide communication, the "store, carry and forward" approach is usually applied. With this approach, a message can be stored temporarily on a device, in order to be forwarded later when circumstances permit. Any device can opportunistically be used as an intermediate relay to facilitate the propagation of a message from one part of the network to another. In this context, the provisioning of pervasive services is particularly challenging, and requires revisiting important components of the provisioning process, such as performing pervasive service discovery and invocation with the presence of connectivity disruptions and absence of both end-to-end paths and access continuity due to user mobility. This thesis addresses the problems of providing network-wide service provisioning in ICHNs and proposes solutions for pervasive service discovery, invocation and access continuity. Concerning service discovery challenge, we propose TAO-DIS, a service discovery protocol that performs an automatic and fast service discovery mechanism. TAO-DIS takes into account the hybrid nature of an ICHN and that the majority of services are provided by infostations. It permits mobile users to discover all the services in the surrounding environment in order to identify and choose the most convenient ones. To allow users to interact with the discovered services, we introduce TAO-INV. TAO-INV is a service invocation protocol specifically designed for ICHNs. It relies on a set of heuristics and mechanisms that ensures performing efficient routing of messages (both service requests and responses) between fixed infostations and mobile clients while preserving both low values of overhead and round trip delays. Since some infostations in the network might be connected, we propose a soft handover mechanism that modifies the invocation process in order to reduce service delivery delays. This handover mechanism takes into consideration the opportunistic nature of the mobile part of the ICHN. We have performed various experiments to evaluate our solutions and compare them with other protocols designed for ad hoc and opportunistic networks. The obtained results tend to prove that our solutions outperform these protocols, namely thanks to the optimizations we have developed for ICHNs. In our opinion, building specialized protocols that benefit from techniques specifically designed for ICHNs is an approach that should be pursued, in complement with research works on general-purpose communication protocols
This thesis presents new results in three fundamental areas of public-key cryptography: integrity, authentication and confidentiality. In each case we design new primitives or improve the features of existing ones. The first chapter, dealing with integrity, introduces a non-interactive proof for proper RSA public key generation and a contract co-signature protocol in which a breach in fairness provides the victim with transferable evidence against the cheater. The second chapter, focusing on authentication, shows how to use time measurements to shorten zeroknowledge commitments and how to exploit bias in zero-knowledge challenges to gain efficiency. This chapter also generalizes Fiat-Shamir into a one-to-many protocol and describes a very sophisticated smart card fraud illustrating what can happen when authentication protocols are wrongly designed. The third chapter is devoted to confidentiality. We propose public-key cryptosystems where traditional hardness assumptions are replaced by refinements of the CAPTCHA concept and explore the adaptation of honey encryption to natural language messages.
This research work is positioned in the continuity of the thesis [21] carried out in the LRI laboratory. As part of this thesis, we will establish methods which can extract and analyse the social networks datas but also from other data sources with a view to their reuse in the Octopeek platform. Nevertheless, the realization of such a system remains a scientific challenge which have to take into account large data sets, the time complexity of the reseach process and the language semantic in order to provide the best answers. Indeed, for each information sought, it will be necessary to find the method, the optimized algorithm which takes into account the speed of calculation, and the associated scoring. We will implement algorithms for a on the fly usage and on several people in parallel.
Our assumption in this research project is that scenarios and personas can be used as support methods to handle above-mentioned obstacles. An experiment is designed and conducted in a laboratory environment in order to test this assumption. Some qualitative and quantitative indicators are proposed to evaluate these impacts. Based on the analysis of seven observed collaborative design sessions, the findings of research study are discussed. However, the findings were not adequate to conclude that they have a significant impact on the perspective clarification and convergence. Hence, the main contribution of this research lies from one part in the evaluation of the impacts of these methods in requirement elicitation activity.
This dissertation is a contribution to both the professional branch of English for Specific Purposes and English as a lingua franca. The research takes place in the corporate world where employees exchange emails during the course of their professional routines. In this context, English is considered as an international language and, in the situations where employees are natives of other languages than English, the lingua franca. In the first part, the four fundamental concepts used in this study are introduced: (1) English as an international language, (2) register, (3) phraseology, and (4) professional discourse. The second part deals with the methodological approach which consists in building a corpus comprising 500 messages extracted from a larger database which was collected while we did fieldwork in the corporate world. The corpus is defined by the four following linguistic situations: 1. native professionals writing to native professionals 2. native professionals writing to non-native professionals 3. non-native professionals writing to native professionals 4. non-native professionals writing to non-native professionals It is also defined by four professional situations, namely: 1. selling and purchasing 2. team management 3. human resources management 4. technical problem solving The situations are then used to conduct a corpus-based, register analysis alongthree linguistic and paralinguistic dimensions. Each dimension seeks to characterise professional emails as a form of minimal, embedded, and interpersonal discourse. More generally, this thesis explores and challenges the solidity of traditional norms and that of the concept of discourse community by presenting the English used in global, ephemeral and professional networks as a fluid variety.
In this thesis, we investigate a new paradigm for text-to-speech synthesis (TTS) allowing to deliver synthetic speech while the text is being inputted: incremental text-to-speech synthesis. Contrary to conventional TTS systems, that trigger the synthesis after a whole sentence has been typed down, incremental TTS devices deliver speech in a "piece-meal" fashion (i.e. word after word) while aiming at preserving the speech quality achievable by conventional TTS systems. By reducing the waiting time between two speech outputs while maintaining a good speech quality, such a system should improve the quality of the interaction for speech-impaired people using TTS devices to express themselves. The main challenge brought by incremental TTS is the synthesis of a word, or of a group of words, with the same segmental and supra-segmental quality as conventional TTS, but without knowing the end of the sentence to be synthesized. In this thesis, we propose to adapt the two main modules (natural language processing and speech synthesis) of a TTS system to the incremental paradigm. For the natural language processing module, we focused on part-of-speech tagging, which is a key step for phonetization and prosody generation. We propose an "adaptive latency algorithm" for part-of-speech tagging, that estimates if the inferred part-of-speech for a given word (based on the n-gram approach) is likely to change when adding one or several words. If the Part-of-speech is considered as likely to change, the synthesis of the word is delayed. In the other case, the word may be synthesized without risking to alter the segmental or supra-segmental quality of the synthetic speech. The proposed method is based on a set of binary decision trees trained over a large corpus of text. We achieve 92.5% precision for the incremental part-of-speech tagging task and a mean delay of 1.4 words. Objective and subjective evaluation show that the proposed method outperforms the baselines for French. Finally, we describe a prototype developed during this thesis implementing the proposed solution for incremental part-of-speech tagging and speech synthesis. A perceptive evaluation of the word grouping derived from the proposed adaptive latency algorithm as well as the segmental quality of the synthetic speech tends to show that our system reaches a good trade-off between reactivity (minimizing the waiting time between the input and the synthesis of a word) and speech quality (both at segmental and supra-segmental levels).
Although communication between languages has without question been made easier thanks to Machine Translation (MT), especially given the recent advances in statistical MT systems, the quality of the translations produced by MT systems is still well below the translation quality that can be obtained through human translation. This gap is partly due to the way in which statistical MT systems operate; the types of models that can be used are limited because of the need to construct and evaluate a great number of partial hypotheses to produce a complete translation hypothesis. Using these features in a reranking framework does often provide a better modelization of certain aspects of the translation. However, this approach is inherently limited: reranked hypothesis lists represent only a small portion of the decoder's search space, tend to contain hypotheses that vary little between each other and which were obtained with features that may be very different from the complex features to be used during reranking. In this work, we put forward the hypothesis that such translation hypothesis lists are poorly adapted for exploiting the full potential of complex features. The aim of this thesis is to establish new and better methods of exploiting such features to improve translations produced by statistical MT systems. Our first contribution is a rewriting system guided by complex features. Sequences of rewriting operations, applied to hypotheses obtained by a reranking framework that uses the same features, allow us to obtain a substantial improvement in translation quality. The originality of our second contribution lies in the construction of hypothesis lists with a multi-pass decoding that exploits information derived from the evaluation of previously translated hypotheses, using a set of complex features. Our system is therefore capable of producing more diverse hypothesis lists, which are globally of a better quality and which are better adapted to a reranking step with complex features. What is more, our forementioned rewriting system enables us to further improve the hypotheses produced with our multi-pass decoding approach. Our third contribution is based on the simulation of an ideal information type, designed to perfectly identify the correct fragments of a translation hypothesis. This perfect information gives us an indication of the best attainable performance with the systems described in our first two contributions, in the case where the complex features are able to modelize the translation perfectly. Through this approach, we also introduce a novel form of interactive translation, coined "pre-post-editing", under a very simplified form: a statistical MT system produces its best translation hypothesis, then a human indicates which fragments of the hypothesis are correct, and this new information is then used during a new decoding pass to find a new best translation.
Automatic speech recognition systems are still vulnerable to non native accents. Their precision drastically drops as non native speakers commit acoustic and pronunciation errors. We have proposed a new approach for non native ASR based on pronunciation modelling. This approach uses a non native speech corpus and two sets of acoustic models: the first set stands for the canoncial target language accent and the second stands for the non native accent. It is an automated approach that associates, to each phoneme from the first set of models, one or several non native pronunciations each expressed as a sequence of phonemes from the second set of models. These pronunciations are taken into account through adding new HMM paths to the models of each phoneme from the first set of models. We have developed a new approach for the automatic detection of the mother tong of non native speakers. This approach is based on the detection of discriminative phoneme sequences, and is used as a first step of the ASP process. Besides, we have developed an approach of automatic phoneme-grapheme alignment in order to take into account the graphemic constraints within the non native pronunciation modeling. We have studied some fast likelihood computation techinques, and we have proposed three novel appraoches that aim at enhancing likelihood computation speed without harming ASR precision.
Current recommender systems need to recommend items that are relevant to users (exploitation), but they must also be able to continuously obtain new information about items and users (exploration). This is the exploration / exploitation dilemma. Such an environment is part of what is called "reinforcement learning". In the statistical literature, bandit strategies are known to provide solutions to this dilemma. The contributions of this multidisciplinary thesis the adaptation of these strategies to deal with some problems of the recommendation systems, such as the recommendation of several items simultaneously, taking into account the aging of the popularity of an items or the recommendation in real time.
In numerous fields such as machine learning, operational research or circuit design, a task is modeled by a set of parameters to be optimized in order to take the best possible decision. Formally, the problem amounts to minimize a function describing the desired objective with iterative algorithms. The development of these latter depends then on the characterization of the geometry of the function or the structure of the problem. In a first part, this thesis studies how sharpness of a function around its minimizers can be exploited by restarting classical algorithms. Optimal schemes are presented for general convex problems. They require however a complete description of the function that is rarely available. Adaptive strategies are therefore developed and shown to achieve nearly optimal rates. A specific analysis is then carried out for sparse problems that seek for compressed representation of the variables of the problem. Their underlying conic geometry, that describes sharpness of the objective, is shown to control both the statistical performance of the problem and the efficiency of dedicated optimization methods by a single quantity. A second part is dedicated to machine learning problems. These perform predictive analysis of data from large set of examples. Systematic algorithmic approaches are developed by analyzing the geometry induced by partitions of the data. A theoretical analysis is then carried out for grouping features by analogy to sparse methods.
This thesis discusses the use of deep generative models for symbolic music generation. We will be focused on devising interactive generative models which are able to create new creative processes through a fruitful dialogue between a human composer and a computer. Recent advances in artificial intelligence led to the development of powerful generative models able to generate musical content without the need of human intervention. I believe that this practice cannot be thriving in the future since the human experience and human appreciation are at the crux of the artistic production. However, the need of both flexible and expressive tools which could enhance content creators'creativity is patent; the development and the potential of such novel A.I.-augmented computer music tools are promising. In this manuscript, I propose novel architectures that are able to put artists back in the loop. The proposed models share the common characteristic that they are devised so that a user can control the generated musical contents in a creative way. In order to create a user-friendly interaction with these interactive deep generative models, user interfaces were developed. I believe that new compositional paradigms will emerge from the possibilities offered by these enhanced controls.
The World Wide Web is a proliferating source of multimedia objects described using various natural languages. In order to use semantic Web techniques for retrieval of such objects (images, videos, etc.), we propose a content extraction method in multilingual text collections, using one or several ontologies as parameters. The content extraction process is used on the one hand to index multimedia objects using their textual content, and on the other to build formal requests from spontaneous user requests. The process is based on an interlingual annotation of texts, keeping ambiguities (polysemy and segmentation) in graphs. This first step allows using common desambiguation processes at th elevel of a pivot langage (interlingual lexemes). Passing an ontology as a parameter of the system is done by aligning automatically its elements with the interlingual lexemes of the pivot language. It is thus possible to use ontologies that have not been built for a specific use in a multilingual context, and to extend the set of languages and their lexical coverages without modifying the ontologies. A demonstration software for multilingual image retrieval has been built with the proposed approach in the framework of the OMNIA ANR project, allowing to implement the proposed approaches. It has thus been possible to evaluate the scalability and quality of annotations produiced during the retrieval process.
Even if they are uncommon, Rare Diseases (RDs) are numerous and generally sever, what makes their study important from a health-care point of view. Few databases provide information about RDs, such as Orphanet and Orphadata. Despite their laudable effort, they are incomplete and usually not up-to-date in comparison with what exists in the literature. Indeed, there are millions of scientific publications about these diseases, and the number of these publications is increasing in a continuous manner. This thesis aims at extracting information from texts and using the result of the extraction to enrich existing ontologies of the considered domain. We studied three research directions (1) extracting relationships from text, i.e., extracting Disease-Phenotype (D-P) relationships; (2) identifying new complex entities, i.e., identifying phenotypes of a RD and (3) enriching an existing ontology on the basis of the relationship previously extracted, i.e., enriching a RD ontology. First, we mined a collection of abstracts of scientific articles that are represented as a collection of graphs for discovering relevant pieces of biomedical knowledge. We focused on the completion of RD description, by extracting D-P relationships. This could find applications in automating the update process of RD databases such as Orphanet. Accordingly, we developed an automatic approach named SPARE*, for extracting D-P relationships from PubMed abstracts, where phenotypes and RDs are annotated by a Named Entity Recognizer. SPARE* is a hybrid approach that combines a pattern-based method, called SPARE, and a machine learning method (SVM). It benefited both from the relatively good precision of SPARE and from the good recall of the SVM. Second, SPARE* has been used for identifying phenotype candidates from texts. We selected high-quality syntactic patterns that are specific for extracting D-P relationships only. Then, these patterns are relaxed on the phenotype constraint to enable extracting phenotype candidates that are not referenced in databases or ontologies. These candidates are verified and validated by the comparison with phenotype classes in a well-known phenotypic ontology (e.g., HPO). This comparison relies on a compositional semantic model and a set of manually-defined mapping rules for mapping an extracted phenotype candidate to a phenotype term in the ontology. This shows the ability of SPARE* to identify existing and potentially new RD phenotypes. We applied SPARE* on PubMed abstracts to extract RD phenotypes that we either map to the content of Orphanet encyclopedia and Orphadata; or suggest as novel to experts for completing these two resources. Finally, we applied pattern structures for classifying RDs and enriching an existing ontology. First, we used SPARE* to compute the phenotype description of RDs available in Orphadata. We propose comparing and grouping RDs in regard to their phenotypic descriptions, and this by using pattern structures.
This thesis presents a new approach to automatic prosodic boundary and prosodic structure detection based on a theoretical hierarchical representation of prosodic organization of speech in French. We used a descriptive theory of the French prosodic System to create a rule based linguistic prosodic model suitable for the automatic treatment of spontaneous speech. This model allows finding automatically prosodic group boundaries and structuring them hierarchically. The prosodic structure of every phrase is thus represented in the form of a prosodic tree. This representation proved to be efficient for automatic processing of continuous speech in French. The resulting prosodic segmentation was compared to manual prosodic segmentation. Prosodic structure accuracy was also verified manually by an expert. We applied our model to different kinds of continuous spontaneous speech data with different phonemic and lexical segmentations: manual segmentation and different kinds of automatic segmentations. In particular, the application of our prosodic model to the output of a speech recognition System showed a satisfactory performance. There also bas been established a correlation between the level of the prosodic tree node and the boundary detection accuracy. Thus, it is possible to improve the precision of boundary detection by attributing a degree of confidence to the boundary according to its level in prosodic tree.
With the development of capture devices and the Internet, people access to an increasing amount of images. Assessing visual aesthetics has important applications in several domains, from image retrieval and recommendation to enhancement. Image aesthetic quality assessment aims at determining how beautiful an image looks to human observers. Many problems in this field are not studied well, including the subjectivity of aesthetic quality assessment, explanation of aesthetics and the human-annotated data collection. Conventional image aesthetic quality prediction aims at predicting the average score or aesthetic class of a picture. However, the aesthetic prediction is intrinsically subjective, and images with similar mean aesthetic scores/class might display very different levels of consensus by human raters. Recent work has dealt with aesthetic subjectivity by predicting the distribution of human scores, but predicting the distribution is not directly interpretable in terms of subjectivity, and might be sub-optimal compared to directly estimating subjectivity descriptors computed from ground-truth scores. Furthermore, labels in existing datasets are often noisy, incomplete or they do not allow more sophisticated tasks such as understanding why an image looks beautiful or not to a human observer. In this thesis, we first propose several measures of subjectivity, ranging from simple statistical measures such as the standard deviation of the scores, to newly proposed descriptors inspired by information theory. We evaluate the prediction performance of these measures when they are computed from predicted score distributions and when they are directly learned from ground-truth data. We find that the latter strategy provides in general better results. We also use the subjectivity to improve predicting aesthetic scores, showing that information theory inspired subjectivity measures perform better than statistical measures. Then, we propose an Explainable Visual Aesthetics (EVA) dataset, which contains 4070 images with at least 30 votes per image. EVA has been crowd-sourced using a more disciplined approach inspired by quality assessment best practices. It also offers additional features, such as the degree of difficulty in assessing the aesthetic score, rating for 4 complementary aesthetic attributes, as well as the relative importance of each attribute to form aesthetic opinions. The publicly available dataset is expected to contribute to future research on understanding and predicting visual quality aesthetics. Additionally, we studied the explainability of image aesthetic quality assessment. A statistical analysis on EVA demonstrates that the collected attributes and relative importance can be linearly combined to explain effectively the overall aesthetic mean opinion scores. We found subjectivity has a limited correlation to average personal difficulty in aesthetic assessment, and the subject's region, photographic level and age affect the user's aesthetic assessment significantly.
This work aims toward an improved understanding of the seismic signals derived from the inter-receiver correlation functions of seismic noise, which is valuable and critical for a reliable noise-based deep Earth imaging. The thesis consists of seven chapters. Chapter 1 introduces background knowledge on seismic noise, from its classifications to various origins. Chapter 2 provides a literature overview on the history and development of the emerging noise correlation method, and reviews various techniques for the pre-processing of seismic noise data and post-processing of noise correlation functions. Statistics-based noise processing methods and a modified scheme for computing correlation function are developed in this chapter. Chapter 3 proposes several Radon-based techniques to analyze the slownesses of correlated wavefields and to unveil the origin of noise-derived seismic signals. Chapter 6 discusses several situations that bring ambiguities into the noise-derived seismic signals and can potentially bias the noise-based imaging of subsurface structure. The last chapter provides a summarization over the contributions of this thesis and an outlook of several ongoing and prospected works.
Dialogue systems in natural language are error-prone. We advocate the resolution of these errors within the collaborative model of Clark &amp; Schaefer (1989): each participant is willing to reach the mutual understanding by providing evidence he understands his partner's utterances. This raises the problem of recursive acceptance as noted by Traum (1994,1999) but existing models that solve this problem either make important simplifications or are too complex to implement. We propose to consider shared beliefs of understanding in order to guide the grounding process. We implemented the model and made an evaluation of its accuracy within the simulation paradigm. Eventually, the evaluation results show the efficiency of grounding while measuring the gain of understanding.
In this thesis, we present a study about Web image results visualization on mobile devices. Our main findings were inspired by the recent advances in two main research areas-Information Retrieval and Natural Language Processing. In the former, we considered different topics such as search results clustering, Web mobile interfaces, query intent mining, to name but a few. In the latter, we were more focused in collocation measures, high order similarity metrics, etc. Particularly in order to validate our hypothesis, we performed a great deal of different experiments with task specific datasets. Many characteristics are evaluated in the proposed solutions. First, the clustering quality in which classical and recent evaluation metrics are considered. Secondly, the labeling quality of each cluster is evaluated to make sure that all possible query intents are covered. Thirdly and finally, we evaluate the user's effort in exploring the images in a gallery-based interface. An entire chapter is dedicated to each of these three aspects in which the datasets-some of them built to evaluate specific characteristics-are presented. For the final results, we can take into account two developed algorithms, two datasets and a SRC evaluation tool. From the algorithms, Dual C-means is our main product. It can be seen as a generalization of our previously developed algorithm, the AGK-means. Both are based in text-based similarity metrics. A new dataset for a complete evaluation of SRC algorithms is developed and presented. Similarly, a new Web image dataset is developed and used together with a new metric to measure the users effort when a set of Web images is explored. Finally, we developed an evaluation tool for the SRC problem, in which we have implemented several classical and recent SRC metrics. Our conclusions are drawn considering the numerous factors that were discussed in this thesis. However, additional studies could be motivated based in our findings. Some of them are discussed in the end of this study and preliminary analysis suggest that they are directions that have potential.
When expressing a concrete relationship, they are generally categorized as topological localizers which express an inclusion or a contact relationship between two entities. When expressing a metaphorical meaning, all of these localizers can express an abstract framework, although each one has its own specific uses. They are sometimes interchangeable for both concrete and metaphorical meanings. Our comparative study in contemporary Chinese is based on a corpus of texts of different styles: written, oral and written with oral characteristics. The statistics show the importance of text styles in the choice of localizer. Pragmatic and semantic contexts are also determining factors in the uses of localizers. The diachronic analysis performed on a corpus of 26 documents representing the archaic, medieval and pre-modern periods shows the evolution of each localizer and their tendency of use throughout History, both in concrete and metaphorical expressions.
In this thesis, I investigate the role of expectations in business cycles by studying three different kinds of expectations. First, I focus on a theoretical explanation of business cycles generated by changes in expectations which turn out to be self-fulfilling. This chapter improves a puzzle from the sunspot literature, thereby giving more evidence towards an interpretation of business cycles based on self-fulfilling prophecies. Second, I empirically analyze the propagation mechanisms of central bank announcements through changes in market participants'beliefs. This chapter shows that credible announcements about future unconventional monetary policies can be used as a coordination device in a sovereign debt crisis framework. Third, I study a broader concept of expectations and investigate the predictive power of political climate on the pricing of sovereign risk. This chapter shows that political climate provides additional predictive power beyond the traditional determinants of sovereign bond spreads. In order to interrogate the role of expectations in business cycles from multiple angles, I use a variety of methodologies in this thesis, including theoretical and empirical analyses, web scraping, machine learning, and textual analysis. In addition, this thesis uses innovative data from the social media platform Twitter. Regardless of my methodology, all my results convey the same message: expectations matter, both for economic research and economically sound policy-making.
In this thesis, we are interested in how we can leverage fuzzy logic to improve the interactions between relational database systems and humans. Cooperative answering techniques aim to help users harness the potential of DBMSs. These techniques are expected to be robust and always provide answer to users. Empty set (0,00 sec) is a typical example of answer that one may wish to never obtain. The informative nature of explanations is higher than that of actual answers in several cases, e.g. empty answer sets and plethoric answer sets, hence the interest of robust cooperative answering techniques capable of both explaining and improving an answer set. Using terms from natural language to describe data --- with labels from fuzzy vocabularies --- contributes to the interpretability of explanations. Offering to define and refine vocabulary terms increases the personalization experience and improves the interpretability by using the user's own words. These axes define cooperative techniques where the interest of explanations is to enable users to understand how results are computed in an effort of transparency. The informativeness of the explanations brings an added value to the direct results, and that in itself represents a cooperative answer.
Translation techniques constitute an important subject in translation studies and in linguistics. When confronted with a certain word or segment that is difficult to translate, human translators must apply particular solutions instead of literal translation, such as idiomatic equivalence, generalization, particularization, syntactic or semantic modulation, etc. However, this subject has received little attention in the field of Natural Language Processing (NLP). Our research problem is twofold: is it possible to automatically recognize translation techniques? To verify our hypothesis, we annotated a parallel English-French corpus with translation techniques, while establishing an annotation guide. Our typology of techniques is proposed based on previous typologies, and is adapted to our corpus. The inter-annotator agreement (0.67) is significant but slightly exceeds the threshold of a strong agreement (0.61), reflecting the difficulty of the annotation task. Based on annotated examples, we then worked on the automatic classification of translation techniques. Even if the dataset is limited, the experimental results validate our working hypothesis regarding the possibility of recognizing the different translation techniques. We have also shown that adding context-sensitive features is relevant to improve the automatic classification. In order to test the genericity of our typology of translation techniques and the annotation guide, our studies of manual annotation have been extended to the English-Chinese language pair. This pair shares far fewer linguistic and cultural similarities than the English-French pair. The annotation guide has been adapted and enriched. With the aim to validate the benefits of these studies, we have designed a tool to help learners of French as a foreign language in reading comprehension. An experiment on reading comprehension with Chinese students confirms our working hypothesis and allows us to model the tool. Other research perspectives include helping to build paraphrase resources, evaluating automatic word alignment and evaluating the quality of machine translation.
AI nowadays relies largely on using large data and enhanced machine learning methods which consist in developing classification and inference algorithms leveraging large datasets of large sizes. These large dimensions induce many counter-intuitive phenomena, leading generally to a misunderstanding of the behavior of many machine learning algorithms often designed with small data dimension intuitions. By taking advantage of (rather than suffering from) the multidimensional setting, random matrix theory (RMT) is able to predict the performance of many non-linear algorithms as complex as some random neural networks as well as many kernel methods such as Support Vector Machines, semi-supervised classification, principal component analysis or spectral clustering. To characterize the performance of these algorithms theoretically, the underlying data model is often a Gaussian mixture model (GMM) which seems to be a strong assumption given the complex structure of real data (e.g., images). Furthermore, the performance of machine learning algorithms depends on the choice of data representation (or features) on which they are applied. Once again, considering data representations as Gaussian vectors seems to be quite a restrictive assumption. Relying on random matrix theory, this thesis aims at going beyond the simple GMM hypothesis, by studying classical machine learning tools under the hypothesis of Lipschitz-ally transformed Gaussian vectors also called concentrated random vectors, and which are more generic than Gaussian vectors. This hypothesis is particularly motivated by the observation that one can use generative models (e.g., GANs) to design complex and realistic data structures such as images, through Lipschitz-ally transformed Gaussian vectors. This notably suggests that making the aforementioned concentration assumption on data is a suitable model for real data and which is just as mathematically accessible as GMM models. Therefore, we demonstrate through this thesis, leveraging on GANs, the interest of considering the framework of concentrated vectors as a model for real data. In particular, we study the behavior of random Gram matrices which appear at the core of various linear models, kernel matrices which appear in kernel methods and also classification methods which rely on an implicit solution (e.g., Softmax layer in neural networks), with concentrated random inputs. Analyzing these methods for concentrated data yields to the surprising result that they have asymptotically the same behavior as for GMM data (with the same first and second order statistics). This result strongly suggest the universality aspect of large machine learning classifiers w.r.t.
Named Entity (NE) Recognition is a recurring problem in the different domain of Natural Language Processing. With these processes, Nemesis performance achieves about 90% of precision and 80% of recall. To increase the recall, we put forward optional modules (analysis of the wide context and utilization of the Web as a source of new contexts) and investigate in setting up a disambiguation and grammar rules inference module.
Reading is a crucial learning in primary school. Initially focused on decoding and accuracy during the first years, reading teaching is then based on automaticity and comprehension. However this method gives only an assessment of accuracy and automaticity skills, while flluency includes also the abilility to read with apropriate phrasing and expressivity, that means to read with a prosody adapted to the text. Omitting the prosodic dimensions of fluency results in confusion between speed and fluency. Prosodic dimensions of reading have long been neglected in reading studies. Only recently, a few new studies have been interested in reading prosody development in various langages, but not in French. That's why the prosodic dimensions of fluency deserve more interest, especially while developping in young readers, to better understand its connection with written comprehension. For this purpose, we used three complementary assessment methods for reading prosody : a subjective assessment, using a multidimensionnal scale, and two objective assessments: one using acoustic markers and another one, automatic, based on raw speech signal analyses. These scores enabled us to characterize subjectively reading prosody development and to highlight, in French, the link between reading prosody and comprehension, mentionned in various studies. These data were then used to explore the link between acoustic markers and subjective scoring and have cues of which acoustic markers have an influence on our perception of readings. Then we used the pausing and breathing data to characterize the link between reading prosody and comprehension. Using these data, we built growth model for each dimension of reading fluency and studied the causal link between automaticity, prosody and comprehension. The work presented here, on the development of reading prosody and its link to comprehension in French speaking children, enables us to promote new fluency assessment tools including reading prosody and to consider how to develop training tools. It also gives us new prospect on reading teaching and on identifying and helping children who need reading intervention.
This thesis aims at defining a unified adaptation of the document selection and answer extraction strategies, based on the document and question types, in a Question-Answering (QA) context. We develop and investigate a method based on an Information Retrieval approach for the selection of relevant documents in QA. The method is based on a language model and a binary model of textual classification in relevant or irrelevant category. It is used to filter unusable documents for answer extraction by matching lists of a priori relevant documents to the question type automatically. First, we present the method along with its underlying models and we evaluate it on the QA task with RITEL in French. The evaluation is done on a corpus of 500,000 unsegmented web pages with factoid questions provided by the Quaero program (i.e. evaluation at the document level or D-level). Then, we evaluate the methodon segmented web pages (i.e. evaluation at the segment level or S-level). The idea is that information content is more consistent with segments, which facilitates answer extraction. D-filtering brings a small improvement over the baseline (no filtering). S-filtering outperforms both the baseline and D-filtering but not significantly. Finally, we study at the S-level the links between RITEL's performances and the key parameters of the method. In order to apply the method on segments, we created a system of web page segmentation. We present and evaluate it on the QA task with the same corpora used to evaluate our document selection method. This evaluation follows the former hypothesis and measures the impact of natural web page variability (in terms of size and content) on RITEL in its task. In general, the experimental results we obtained suggest that our IR-based method helps a QA system in its task, however further investigations should be conducted – especially with larger corpora of questions – to make them significant.
Projects in digital humanities increasingly employ public-oriented collaboration methods such as crowdsourcing to achieve objectives that include research, conservation and scholarly editing in the humanities and social sciences. For example, crowdsourcing presents an opportunity to quicken the pace of progress for transcription projects for research communities that have traditionally operated within closed circuits. Meanwhile, literature that evaluates the efficacy of crowdsourcing for digital humanities projects is insufficient. Questions as to whether the public can produce material that can be used for scholarly editions, in which cases, for which types of projects, and how much post-processing or corrections will be required, continue to occupy discussions on the matter. This doctoral thesis will examine the potential benefits of crowdsourced transcription for scholarly editing projects in the digital humanities. Firstly, by exploring the technologies and techniques available to render online transcription in XML possible. Secondly, by developing and testing an online transcription platform, which will allow to examine user needs for collaborative work environments based on user responses and existing industrial crowdsourcing environments. Thirdly, the data collected will be subjected to digital analysis to compare the productions of non-expert transcribers to those of expert transcribers on the basis of document distance measurements. The results will be interpreted to determine the potential benefits of crowdsourcing for digital scholarly editing projects. Finally, the work will conclude by discussing the implications of current work and presenting opportunities for future research in the field.
The research work in this thesis is related to Topic Map construction and their use in semantic annotation of web resources in order to help users find relevant information in these resources. The amount of information sources available today is very huge and continuously increasing, for that, it is impossible to create and maintain manually a Topic Map to represent and organize all these information. Many Topic Maps building approaches can be found in the literature [Ellouze et al. 2008a]. However, none of these approaches takes as input multilingual document content. In addition, although Topic Maps are basically dedicated to users navigation and information search, no one approach takes into consideration users requests in the Topic Map building process. In this context, we have proposed ACTOM, a Topic Map building approach based on an automated process taking into account multilingual documents and Topic Map evolution according to content and usage changes. To enrich the Topic Map, we are based on a domain thesaurus and we propose also to explore all potential questions related to source documents in order to represent usage in the Topic Map. In our approach, we extend the Topic Map model that already exists by defining the usage links and a list of meta-properties associated to each Topic, these meta-properties are used in the Topic Map pruning process. In our approach ACTOM, we propose also to precise and enrich semantics of Topic Map links so, except occurrences links between Topics and resources, we classify Topic Map links in two different classes, those that we have called “ontological links” and those that we have named “usage links”.
The present dissertation deals with didactic description of linguistic variation in a constraint-based approach. This approach allows us to consider variation in a functional perspective rather than in a normative perspective and to describe “non-standard” variants as more or less appropriate to certain tasks rather than deviations from the norm. To illustrate our approach, we are applying it to the description of clitic left dislocation in French. We claim that these constraints are all pragmatic in nature and that their interaction weight on the use of clitic left dislocation in French. These claims are tested empirically via a corpus studies, a series of acceptability judgment tests and a matched guise test. Furthermore, we argue that the learning of pragmatic constraints in foreign language is dependent of their explicit teaching and repeated expositions to the construction in felicitous contexts. Following the dynamic interface hypothesis (Ellis, 2005), we suggest that explicit learning of the constraints of clitic left dislocation in the context of the classroom facilitates their implicit learning when the learners find themselves in a situation of communication with French native speakers. The role of exposition is explored empirically by replicating an acceptability judgment test and the matched guise test with non-native participants. Stylistics constraints are described using existential competencies and sociolinguistics registers (European Framework, 2001).
Starting from a dynamic and plural conception of emotions, this thesis reflects on the discursive inscription of affects in digital interactions like WhatsApp. It is part of the field of analysis of discourses in interaction (Kerbrat-Orecchioni, 2005) by making the framework of digital discourse (Paveau, 2017) echo with the theoretical propositions of information and communication sciences (Allard, 2017). If linguists have long stressed the infinity of the linguistic marks of emotions (Kerbrat-Orecchioni 2000) or even their heterogeneity (Plantin, 2011 ; Micheli, 2014), this postulate is even more relevant indigital ecosystems where the expression of affects is distributed throughout the digital interface integrating words and gestures indifferently (Jeanneret and Souchier, 1999). Following a methodological reflection on the constitution of the WhatsApp digital corpus, this work focuses on the semiotic and discursive resources in the expression of emotions. The latter are materialised in new verbal forms such as the emotiwords lol and mdr, understood as a sociolect of the affect. Nevertheless, in the context of digital messaging, the expression of emotions goes beyond the verbal to be embodied in iconic forms marked by photo discourses, where the photographic capture is woven with theverbal to co-elaborate meaning. Based on qualitative analyses of the collected observables, this thesis shows how the speakers constantly renew the forms of emotional expressions and how they deal with the digital affordances of the system, to explore new versions in the interactional management of affects.
Humans and robots working safely and seamlessly together in a cooperative environment is one of the future goals of the robotics community. When humans and robots can work together in the same space, a whole class of tasks becomes amenable to automation, ranging from collaborative assembly to parts and material handling to delivery. Proposed standards exist for collaborative human-robot safety, but they focus on limiting the approach distances and contact forces between the human and the robot. A key enabler for human-robot safety in cooperative environments involves the field of intention recognition, in which the robot attempts to understand the intention of an agent (the human) by recognizing some or all of their actions to help predict the human's future actions. We present an approach to inferring the intention of an agent in the environment via the recognition and representation of state information. This approach to intention recognition is different than many ontology-based intention recognition approaches in the literature as they primarily focus on activity (as opposed to state) recognition and then use a form of abduction to provide explanations for observations.
In specialised domains, the applications such as information retrieval for machine translation rely on terminological resources for taking into account terms or semantic relations between terms or groupings of terms. In order to face up to the cost of building these resources, automatic methods have been proposed. Among those methods, the distributional analysis uses the repeated information in the contexts of the terms to detect a relation between these terms. While this hypothesis is usually implemented with vector space models, those models suﬀer from a high number of dimensions and data sparsity in the matrix of contexts. In specialised corpora, this contextual information is even sparser and less frequent because of the smaller size of the corpora. Likewise, complex terms are usually ignored because of their very low number of occurrences. Semantic relations acquired from corpora are used to generalise and normalise those contexts. We evaluated the method robustness on four corpora of different sizes, different languages and different domains. The analysis of the results shows that, while taking into account complex terms in distributional analysis, the abstraction of distributional contexts leads to defining semantic clusters of better quality, that are also more consistent and more homogeneous.
Natural Language Generation is the task of automatically producing natural language text to describe information present in non-linguistic data. The latter task is known as Surface Realisation (SR). In my thesis, I study the SR task in the context of input data coming from Knowledge Bases (KB). I present two novel approaches to surface realisation from knowledge bases: a supervised approach and a weakly supervised approach. In the first, supervised, approach, I present a corpus-based method for inducing a Feature Based Lexicalized Tree Adjoining Grammar from a parallel corpus of text and data. In the weakly supervised approach, I explore a method for surface realisation from KB data which does not require a parallel corpus. Instead, I build a corpus from heterogeneous sources of domain-related text and use it to identify possible lexicalisations of KB symbols and their verbalisation patterns. I evaluate the output sentences and analyse the issues relevant to learning from non-parallel corpora. In both these approaches, the proposed methods are generic and can be easily adapted for input from other ontologies
This research takes an interest the syntax of exceptive constructions (ECs) and its correspondence with semantic within two languages: French and Arabic. We will focus our analysis on the markers sauf, excepté, hormis, etc. and analyze them as particular case of paradigmatic lists/piles, in which two segments of the utterance pile up on the same syntactic position and whose most famous case is coordination. This analysis is different from the one generally associated with these markers in French grammars and dictionaries which consider them as prepositions. Our analyzes lead us to consider the markers ʾillā, ġayr and siwā in Arabic as coordinating conjunctions. These items, like their French counterparts, relate two elements where X on the right of the marker and Y on the left form paradigmatic lists/piles, in the sense that they fulfill the same syntactic function in the utterance. We analyze the lexical items ʿadā (mā-ʿadā), ẖalā (mā-ẖalā), ḥāšā (mā-ḥāšā) as verbs. These verbs introduce a clause that maintains a parataxic relation with the preceding clause. Finally, we consider the items bistiṯnā'i and biẖilāfi as prepositive phrases introducing a sequence that maintains a subordinate relationship with the main clause.
Exchanged information in emails' texts is usually concerned by complex events or business processes in which the entities exchanging emails are collaborating to achieve the processes' final goals. Thus, the flow of information in the sent and received emails constitutes an essential part of such processes i.e. the tasks or the business activities. Extracting information about business processes from emails can help in enhancing the email management for users. It can be also used in finding rich answers for several analytical queries about the employees and the organizations enacting these business processes. None of the previous works have fully dealt with the problem of automatically transforming email logs into event logs to eventually deduce the undocumented business processes. Towards this aim, we work in this thesis on a framework that induces business process information from emails. We introduce approaches that contribute in the following: (1) discovering for each email the process topic it is concerned by, (2) finding out the business process instance that each email belongs to, (3) extracting business process activities from emails and associating these activities with metadata describing them, (4) improving the performance of business process instances discovery and business activities discovery from emails by making use of the relation between these two problems, and finally (5) preliminary estimating the real timestamp of a business process activity instead of using the email timestamp. Using the results of the mentioned approaches, an event log is generated which can be used for deducing the business process models of an email log. The efficiency of all of the above approaches is proven by applying several experiments on the open Enron email dataset.
This thesis is a part of the emergence of deep learning and focuses on spoken language understanding assimilated to the automatic extraction and representation of the meaning supported by the words in a spoken utterance. We study a semantic concept tagging task used in a spoken dialogue system and evaluated with the French corpus MEDIA. For the past decade, neural models have emerged in many natural language processing tasks through algorithmic advances or powerful computing tools such as graphics processors. Many obstacles make the understanding task complex, such as the difficult interpretation of automatic speech transcriptions, as many errors are introduced by the automatic recognition process upstream of the comprehension module. We present a state of the art describing spoken language understanding and then supervised automatic learning methods to solve it, starting with classical systems and finishing with deep learning techniques. The contributions are then presented along three axes. Then we study the management of automatic recognition errors and solutions to limit their impact on our performances. Finally, we envisage a disambiguation of the comprehension task making the systems more efficient.
From these descriptions, items were created to develop a reception task using videotaped utterances of FSL, and a production task using drawings. The production of a story from a cartoon was also proposed in order to assess the narrative skills as well as these predicative constructs in a narrative situation.
The recent trend toward Network Softwarization is driving an unprecedented technoeconomic shift in the Telecom and ICT (Information and Communication Technologies) industries. SDN and NFV paradigms add more flexibility and enable more control over networks, thus, related technologies are expected to dominate a large part of the networking market in the next few years (estimated at USD 3.68B in 2017 and forecasted by some to reach 54B USD by 2022 at a Compound Annual Growth Rate (CAGR) of 71.4%). However, one of the major operators' concerns about Network Softwarization is security. In this thesis, we have first designed and implemented a pentesting (penetration testing) framework for SDN controllers. We have proposed a set of algorithms to fingerprint a remote SDN controller without having direct connection to it. Using our framework, network operators can evaluate the security of their SDN deployments (including Opendaylight, Floodlight and Cisco Open SDN Controller) before putting them into production. Also, sOFTDP outperforms OFDP by several orders of magnitude which we confirmed by extensive experiments. The second axis of our research in this thesis is smart management in softwarized networks. Inspired by the recent breakthroughs in machine learning techniques, notably, Deep Neural Networks (DNNs), we have built a traffic engineering engine for SDN called NeuRoute, entirely based on DNNs. Current SDN/OpenFlow controllers use a default routing based on Dijkstra's algorithm for shortest paths, and provide APIs to develop custom routing applications.
After September 2008, due to a frozen interbank market, shortage of liquidity, loss of confidence, and collapsing financial institutions, the monetary policy transmission in the euro area was severely impaired. Under thus exceptional circumstances, the European Central Bank (ECB) had to turn to non-standard monetary policy measures. Considering, in the euro area, the constrained range of actions and fragmented financial markets, the objective of this empirical thesis is to assess the transmission channels of ECB standard and non-standard monetary policies and their effects on both financial markets and the economy. As banks' lending behaviors are related to their financing costs, the first essay focuses on bank lending channel. It studies the evolution of lending activities of European financial institutions on the syndicated loan market and its reaction to the ECB standard and non-standard policies. The communication of the central bank is of utmost importance in a monetary union with heterogeneous, in terms of economic situations and cultures, countries. The second and third essays study the signaling channel of monetary policy. The second essay focuses on the communication during monthly press conferences and their effects on the predictability of monetary policy decisions and on financial markets returns and volatility. The last essay concentrates exclusively on the use of \textit{forward guidance} on interest rate, a non-standard central bank communication providing information on future short-term interest rates. It discusses its effectiveness and ability to lower market participants expected interest rates.
The current trend in Artificial Intelligence (AI) is to heavily rely on systems capable of learning from examples, such as Deep learning (DL) models, a modern embodiment of artificial neural networks. While numerous applications have made it to market in recent years (including self-driving cars, automated assistants, booking services, and chatbots, improvements in search engines, recommendations, and advertising, and heath-care applications, to name a few) DL models are still notoriously hard to deploy in new applications. In particular, the require massive numbers of training examples, hours of GPU training, and highly qualified engineers to hand-tune their architectures. This thesis will contribute to reduce the barrier of entry in using DL models for new applications, a step towards 'democratizing AI'. The angle taken will be to develop new Transfer Learning (TL) approaches, based on modular DL architectures. Transfer learning encompasses all techniques to speed up learning by capitalizing on exposure to previous similar tasks. For instance, using pre-trained networks is a key TL tactic used by winners of the recent AutoDL challenge https://autodl.chalearn.org/. The doctoral candidate will push forward the notion of reusability of pre-trained networks in whole or in part (modularity). There are several important questions raised in this context. From a technical standpoint, the current limitations of pre-training include that: (T1) In many domains, there are no available pre-trained networks, due to lack of massive datasets in related domains; (T2) Novel architectures of networks such as 'Graph Neural Networks'(GNNs) do not easily lend themselves to pre-training; (T3) Besides merely retraining the last layer and fine-tuning inner layers, means of re-using pre-trained networks in new contexts are under-developed. These three issues offer challenging research opportunities to efficiently use prior knowledge, data simulators, and/or data augmentation, and develop novel algorithms and architectures that learn in a modular re-usable way. From the fundamental research point of view, modularity and inheritance of pre-trained learning modules in biologically-inspired learning systems is a burning topic in AI. Unanswered questions include: (F1) Does modularity of the brain increase its effectiveness or is this a legacy of evolution that plays no particular role; (F2) Likewise, in which context and how does modularity help in artificial systems (e.g. to implement invariances, to help transfer learning, etc.); (F3) Does module specialization hinder or help generalization to new data modalities (e.g. new sensor data), and if so, how? In this context, the doctoral student will investigate a novel approach to transfer learning that we call 'Deep Modular Learning'. The candidate will tackle the problem of training large artificial neural networks whose architectures are modular and whose modules are eventually reusable. A possible method to approach the problem will be to use multi-level optimization algorithms, addressing the optimization of the overall system (achieving a higher level objective) under the constraint that the modules achieve a lower level objective (reusability). One scientific aim will be to challenge the hypothesis that modularity is essential for learning systems, in that it accelerates learning by making possible an effective form of transfer learning, a central functionality in AI. Several principles/conjectures/hypotheses may be guiding this research including: (P1) The principle of parsimony or 'Ockham's razor'embodied in modern learning theory as 'regularization', which in layman's words states that ``of two theories equivalently powerful in reproducing observations, one should prefer the simpler one''; indeed modular architectures sharing identical sub-modules have fewer adjustable parameters and therefore can be considered less complex than e.g. fully connected networks. (P2) The innateness hypothesis: Task solving capabilities are a combination of innate vs. acquired skills. Is it a characteristic of intelligent systems to rely more on learned skills such as language rather than inherit them? Is it true that language can be completely learned 'from scratch'? (P3) Induction, deduction, conceptualisation, and causality: do intelligent learning systems rest upon modularity for conceptualisation, language acquisition, and causal inference? To put this framework in practice, the student will choose practical applications from domains including biomedicine (e.g. molecular toxicity or efficacy), ecology, econometrics, speech recognition, natural language processing, image or video processing, etc. See for instance http://snap.stanford.edu/data/.
The automatic summarization is generally based on extractive methods that select relevant sentences from the source document and merge them to create a summary. These methods are not really adapted to the problem of the summary for the spoken conversations because of the spontaneous nature of these dialogs and the importance of the interactions between the speakers. By selecting only a few sentences, the final summary will contain only one verbatim of what has been said, and not a synthetic description of what happened during the conversation. This is why abstractive approaches based on concepts detection would be able to overcome those difficulties. Then we study the interest if using semantic models in the automatic summarization task. Finally, we propose a method of summay based on patterns. Filling patterns methods have shown their interest in specific areas for automatic text summary. In our case, It allows to deal with the difference in spoken data document style (transcripts of conversations) and the nature if the summaries to be generated (synthetic narration). However, it requires manual writing of patterns and manual annotations of quantities of source data into concepts to be detected to fill these patterns.
This thesis deals with automatic Dialogue Act (DA) recognition in Czech and in French. Dialogue acts are sentence-level labels that represent different states of a dialogue, such as questions, statements, hesitations, etc. The experimental results confirmed that every type of feature (lexical, prosodic and word positions) bring relevant and somewhat complementary information. The proposed methods that take into account word positions are especially interesting, as they bring global information about the structure of a sentence, at the opposite of traditional n-gram models that only capture local cues. One of the main issue in the domain of automatic dialogue act recognition concerns the design of a fast and cheap method to label new corpora. Experimental results showed that the proposed method is an efficient approach to create new dialogue act corpora at low costs.
Word Sense Disambiguation (WSD) and Machine Translation (MT) are two central and among the oldest tasks of Natural Language Processing (NLP). Although they share a common origin, WSD being initially conceived as a fundamental problem to be solved for MT, the two tasks have subsequently evolved very independently of each other. Indeed, on the one hand, MT has been able to overcome the explicit disambiguation of terms thanks to statistical and neural models trained on large amounts of parallel corpora, and on the other hand, WSD, which faces some limitations such as the lack of unified resources and a restricted scope of applications, remains a major challenge to allow a better understanding of the language in general. Today, in a context in which neural networks and word embeddings are becoming more and more important in NLP research, the recent neural architectures and the new pre-trained language models offer not only some new possibilities for developing more efficient WSD and MT systems, but also an opportunity to bring the two tasks together through joint neural models, which facilitate the study of their interactions. In this thesis, our contributions will initially focus on the improvement of WSD systems by unifying the ressources that are necessary for their implementation, constructing new neural architectures and developing original approaches to improve the coverage and the performance of these systems. Then, we will develop and compare different approaches for the integration of our state of the art WSD systems and language models into MT systems for the overall improvement of their performance. Finally, we will present a new architecture that allows to train a joint model for both WSD and MT, based on our best neural systems.
This work aims to study grammaticalization, the process by which the functional items of a language come to be replaced with time by content words or constructions, usually providing a more substantial meaning. Grammaticalization is therefore a particular type of semantic replacement. However, language emerges as a social consensus, so that it would seem that semantic change is at odds with the proper working of communication. Despite of this, the phenomenon is attested in all languages, at all times, and pervades all linguistic categories, as the very existence of grammaticalization shows. These frequencies of use are extracted from the textual database Frantext, which covers a period of seven centuries. The statistical distribution of the different observables related to these two phenomenal features are extracted. A random walk model is then proposed to account for this two-sided frequency pattern. The latency period appears as a critical phenomenon in the vicinity of a saddle-node bifurcation, and quantitatively matches its empirical counter-part. Finally, an extension of the model is sketched, in which the relationship between the structure of the semantic network and the outcome of the evolution could be discussed.
This thesis aims to study the argumental function for finding an efficient method to automatically acquire the terms. We start with a discussion on the problematic of the corpus which is: what kind of corpus should we choose and how should we construct the web corpus. Then, three methods are developed which are based on the morphological characteristics of lexical units and the relation between the appropriate predicates and their arguments. The distributional method tries to exploit the predicate-argument structures for identifying the arguments of the given semantic class. The morph-semantic method is developed on the basis of the morphological characteristics of the lexical units in order to extend the list of terms. The mixed method which combines the two precedent approaches permit to improve the result. Finally, we try to develop a statement on the natural language character, on the semantic class, on the specialized language and on the recursive nature of language in the perspective of natural language processing.
On the other hand, when it comes to dealing with under-resourced languages, there is often a lack of tools and data. In this thesis, we are interested in some of the vernacular forms of Arabic used in Maghreb. These forms are known as dialects, which can be classified as poorly endowed languages. Except for raw texts, which are generally extracted from social networks, there is not plenty resources allowing to process Arabic dialects. The latter, compared to other under-resourced languages, have several specificities that make them more difficult to process. We can mention, in particular the lack of rules for writing these dialects, which leads the users to write the dialect without following strict rules, so the same word can have several spellings. Words in Arabic dialect can be written using the Arabic script and/or the Latin script (arabizi). For the Arab dialects of the Maghreb, they are particularly impacted by foreign languages such as French and English. In addition to the borrowed words from these languages, another phenomenon must be taken into account in automatic dialect processing. This is the problem known as code-switching. This phenomenon is known in linguistics as diglossia. This gives free rein to the user who can write in several languages in the same sentence. He can start in Arabic dialect and in the middle of the sentence, he can switch to French, English or modern standard Arabic. In addition to this, there are several dialects in the same country and a fortiori several different dialects in the Arab world. It is therefore clear that the classic NLP tools developed for modern standard Arabic cannot be used directly to process dialects. The main objective of this thesis is to propose methods to build automatically resources for Arab dialects in general and more particularly for Maghreb dialects. This represents our contribution to the effort made by the community working on Arabic dialects. We have thus produced methods for building comparable corpora, lexical resources containing the different forms of an input and their polarity. In addition, we developed methods for processing modern standard Arabic on Twitter data and also on transcripts from an automatic speech recognition system operating on Arabic videos extracted from Arab television channels such as Al Jazeera, France24, Euronews, etc. We compared the opinions of automatic transcriptions from different multilingual video sources related to the same subject by developing a method based on linguistic theory called Appraisal.
Qualitative Spatial and Temporal Reasoning is a major field of study in Artificial Intelligence and, particularly, in Knowledge Representation, which deals with the fundamental cognitive concepts of space and time in an abstract manner. In our thesis, we focus on qualitative constraint-based spatial and temporal formalisms and make contributions to several aspects. In particular, given a knowledge base of qualitative spatial or temporal information, we define novel local consistency conditions and related techniques to efficiently solve the fundamental reasoning problems that are associated with such knowledge bases. Further, we enrich the field of spatio-temporal formalisms that combine space and time in an interrelated manner by making contributions with respect to a qualitative spatio-temporal logic that results by combining the propositional temporal logic (PTL) with a qualitative spatial constraint language, and by investigating the task of ordering a temporal sequence of qualitative spatial configurations to meet certain transition constraints.
In this dissertation we tackle the problem of multilingual epidemic surveillance. We present an approach which is differential, endogenous and non-compositionnal. Using genre properties and communication principles, we maximise the factorization in order to get a system as generic as possible. Our local analysis does not rely on classical linguistic analyzers for morphology, syntax or semantics but on the distribution of character strings at key positions thus avoiding the problem of the definition of a "word". We implemented a system using this approach, this system is called DAnIEL (Data Analysis for Information Extraction in any Language). DanIEL analyzes press articles in order to check if they contain epidemic events and classifies them according to disease-location pair in order to reduce redundancy for the end-user. DanIEL is fast, efficient in comparison to state-of-the-art systems. It needs very few additional knowledge for processing new languages.
Attention involves distinguishing information which is useful for an activity from that which is not. Augmented reality (AR) highlighting guides this process of information selection by increasing the salience of high-value elements. In such systems, “value” is typically seen as linked to the overall activity of driving (e.g. traffic signs in case of poor visibility, or indications of direction). However, several studies have shown that during an activity eye movements are specific to the immediate goal. AR which does not respect this “natural” prioritizing of information thus risks interfering with information-processing. The first objective of this study is to determine to what extent the allocation of visual attention in automobile driving is focused on information related to a maneuver. The second objective is to study the impact of AR on this allocation of attention. We set up three experiments in which participants viewed static and dynamic driving situations and had to decide whether they could perform a maneuver. We analyzed the variations in allocation of visual attention in relation to the maneuvers in question and the AR conditions using eye movements recording. Our results show that visual attention is strongly linked to cues which permit decision-making, but that it does not overlook cues allowing a general comprehension of the situation. AR optimizes visual attention when it highlights cues related to the maneuver, but it disturbs visual attention in other cases. These findings make it possible to identify and categorize various risks inherent in AR highlighting, and to discuss possible approaches to dealing with them through more effective design.
Rare Neurodegenerative Diseases with Motor Expression (RNDME) are a very heterogeneous group of neurodegenerative pathologies of acquired or innate physiopathology that can occur at any age. They have a targeted impact on the central or peripheral neurological structures involved in motor activity. These structures are functionally and topographically close to the centers of cognition. Thus, under certain conditions, the neurodegeneration of motor structures impacts cognitive functions. They result in gait and/or balance disorders, a decrease or absence of movement, a varied cognitive impairment that can go as far as dementia. RNDME are sporadic or family based. The scientific literature indicates an important number of genetic pathogenic variation responsible for RNDMEs, which allows classification but also makes it more complex because it can be observed that the same gene can be involved in several pathologies and a pathology can be caused by different genes. Consequently, in RNDMEs, atypical symptoms, clinical overlaps or allelic diseases are observed. In the French West Indies, the data in the literature on RNDMEs are limited but rich in certain observations confirming the atypical RNDMEs seach as the atypical Caribbean Parkinson's syndrome. On the genetic level, diagnostic investigations are limited to medical fields based in metropolitan France and most often about the comparison of genomic data of Caucasian populations. The inventory of RNDMEs in Martinique and the experimental work of data analysis of NGS, are the first works of this type, in the field and in our region. It had almost never been used in the analysis of RNDMEs. We used it because it is a method of choice for searching for both simple and structural variants. The results show a diagnostic cost-effectiveness of 58% since we identified a variant probably pathogenic in 7 patients out of 12 tested. We found these variants in known RNDMEs genes sometimes described in Asian populations, but also of African or Caucasian descent. Some of these genes may be involved in several symptomatologies, confirming the finding of overlap in these diseases. This work lays the epidemiological basis for RNDMEs in the West Indies and provides a basis for a registry in this area. On the experimental level, it allow to propose a molecular cause associated with previously described or new variations. It is also a proof of concept regarding the bioinformatics means of analyzing NGS data in Martinique. It paves the way for other work of the same kind, susceptible to draw up the gene specificities of the RNDMEs in our region and to expand the data in the scientific and medical literature.
The thesis is focused on learning a complex manipulation robotics task using little knowledge. More precisely, the concerned task consists in reaching an object with a serial arm and the objective is to learn it without camera calibration parameters, forward kinematics, handcrafted features, or expert demonstrations. Deep reinforcement learning algorithms suit well to this objective. Indeed, reinforcement learning allows to learn sensori-motor mappings while dispensing with dynamics. Besides, deep learning allows to dispense with handcrafted features for the state space representation. However, it is difficult to specify the objectives of the learned task without requiring human supervision. Some solutions imply expert demonstrations or shaping rewards to guide robots towards its objective. The latter is generally computed using forward kinematics and handcrafted visual modules. Another class of solutions consists in decomposing the complex task. Learning from easy missions can be used, but this requires the knowledge of a goal state. Alternate approaches which use several agents in parallel to increase the probability of success can be used but are costly. Indeed, humans first look at an object before reaching it. The first learned task is an object fixation task which is aimed at localizing the object in the 3D space. This is learned using deep reinforcement learning and a weakly supervised reward function. The second task consists in learning jointly end-effector binocular fixations and a hand-eye coordination function. This is also learned using a similar set-up and is aimed at localizing the end-effector in the 3D space. In addition, without using additional priors, an object reach ability predictor is learned in parallel. The main contribution of this thesis is the learning of a complex robotic task with weak supervision.
This thesis focuses on machine learning for data classification. We propose a new uncertainty measure that characterizes the importance of data and improves the performance of active learning compared to the existing uncertainty measures. This measure determines the smallest instance weight to associate with new data, so that the classifier changes its prediction concerning this data. The existing stream-based active learning methods are initialized with some labelled instances that cover all possible classes. However, in many applications, the evolving nature of the stream implies that new classes can appear at any time. We propose an effective method of active detection of novel classes in a multi-class data stream. This method incrementally maintains a feature space area which is covered by the known classes, and detects those instances that are self-similar and external to that area as novel classes. Finally, it is often difficult to get a completely reliable labelling because the human labeller is subject to labelling errors that reduce the performance of the learned classifier. This problem was solved by introducing a measure that reflects the degree of disagreement between the manually given class and the predicted class, and a new informativeness measure that expresses the necessity for a mislabelled instance to be re-labeled by an alternative labeller
Recommender Systems (RS) have become essential tools to deal with an endless increasing amount of data available on the Internet. Their goal is to provide items that may interest users before they have to find them by themselves. After being exclusively focused on the precision of users'interests prediction task, RS had to evolve by taking into account other criteria like human factors involved in the decision-making process while computing recommendations, so as to improve their quality and usefulness of recommendations. Nevertheless, the way some human factors, such as context and diversity needs, are managed remains open to criticism. While context-aware recommendations relies on exploiting data that are collected without any consideration for users'privacy, diversity has been coming down to a dimension which has to be maximized. Thereby, we argue that analyzing the evolution of diversity over time would be a promising way to define a user's context, under the condition that context is now defined by item attributes. Indeed, we support the idea that a sudden variation of diversity can reflect a change of user's context which requires to adapt the recommendation strategy. We present in this manuscript the first approach to model the evolution of diversity over time and a new kind of context, called ``implicit contexts'', that are respectful of privacy (in opposition to explicit contexts). We confirm the benefits of implicit contexts compared to explicit contexts from several points of view. As a first step, using two large music streaming datasets we demonstrate that explicit and implicit context changes are highly correlated. As a second step, a user study involving many participants allowed us to demonstrate the links between the explicit contexts and the characteristics of the items consulted in the meantime. Based on these observations and the advantages offered by our models, we also present several approaches to provide privacy-preserving context-aware recommendations and to take into account user's needs
The objective of this research is to explore and develop machine learning methods for the analysis of continuous electroencephalogram (EEG). Continuous EEG is an interesting modality for functional evaluation of cerebral state in the intensive care unit and beyond. The subparts of this work hinge around post-anoxic coma prognostication, chosen as pilot application. A small number of long-duration records were performed and available existing data was gathered from CHU Grenoble. First, we validate the effectiveness of deep neural networks for EEG analysis from raw samples. For this we choose the supervised task of sleep stage classification from single-channel EEG. We use a convolutional neural network adapted for EEG and we train and evaluate the system on the SHHS (Sleep Heart Health Study) dataset. This constitutes the first neural sleep scoring system at this scale (5000 patients). Classification performance reaches or surpasses the state of the art. In real use for most clinical applications, the main challenge is the lack of (and difficulty of establishing) suitable annotations on patterns or short EEG segments. Available annotations are high-level (for example, clinical outcome) and therefore they are few. We search how to learn compact EEG representations in an unsupervised/semi-supervised manner. The field of unsupervised learning using deep neural networks is still young. To compare to existing work we start with image data and investigate the use of generative adversarial networks (GANs) for unsupervised adversarial representation learning. The quality and stability of different variants are evaluated. We then apply Gradient-penalized Wasserstein GANs on EEG sequences generation. The system is trained on single channel sequences from post-anoxic coma patients and is able to generate realistic synthetic sequences. We also explore and discuss original ideas for learning representations through matching distributions in the output space of representative networks. Finally, multichannel EEG signals have specificities that should be accounted for in characterization architectures. Each EEG sample is an instantaneous mixture of the activities of a number of sources. Based on this statement we propose an analysis system made of a spatial analysis subsystem followed by a temporal analysis subsystem. The spatial analysis subsystem is an extension of source separation methods built with a neural architecture with adaptive recombination weights, i.e. weights that are not learned but depend on features of the input. We show that this architecture learns to perform Independent Component Analysis if it is trained on a measure of non-gaussianity. For temporal analysis, standard (shared) convolutional neural networks applied on separate recomposed channels can be used.
Knowledge discovery and inference are concepts tackled in different ways in the scientific literature. Indeed, a large number of domains are interested such as: information retrieval, textual inference or knowledge base population. Theses concepts are arousing increasing interest in both academic and industrial fields, promoting development of new methods. This manuscript proposes an automated approach to infer and evaluate knowledge from extracted relations in non-structured texts. Its originality is based on a novel framework making possible to exploit (i) the linguistic uncertainty thanks to an uncertainty detection method described in this manuscript (ii) a generated partial ordering of studied objects (e.g. noun phrases) taking into account of syntactic implications and a prior knowledge defined into taxonomies, and (iii) an evaluation step of extracted and inferred relations by selection models exploiting a specific partial ordering of relations. This partial ordering allows to compute some criteria in using information propagation rules in order to evaluate the belief associated to a relation in taking into account of the linguistic uncertainty. The proposed approach is illustrated and evaluated through the definition of a system performing question answering by analysing texts available on the Web. This case study shows the benefits of structuring processed information (e.g. using prior knowledge), the impact of selection models and the role of the linguistic uncertainty for inferring and discovering new knowledge. These contributions have been validated by several international and national publications and our pipeline can be downloaded at https: //github.com/PAJEAN/.
This work is situated in the context of information retrieval (IR) using machine learning (ML) and deep learning (DL) techniques. It concerns different tasks requiring text matching, such as ad-hoc research, question answering and paraphrase identification. The objective of this thesis is to propose new approaches, using DL methods, to construct semantic-based models for text matching, and to overcome the problems of vocabulary mismatch related to the classical bag of word (BoW) representations used in traditional IR models. Indeed, traditional text matching methods are based on the BoW representation, which considers a given text as a set of independent words. The process of matching two sequences of text is based on the exact matching between words. The main limitation of this approach is related to the vocabulary mismatch. This problem occurs when the text sequences to be matched do not use the same vocabulary, even if their subjects are related. For example, the query may contain several words that are not necessarily used in the documents of the collection, including relevant documents. BoW representations ignore several aspects about a text sequence, such as the structure the context of words. These characteristics are important and make it possible to differentiate between two texts that use the same words but expressing different information. Another problem in text matching is related to the length of documents. The relevant parts can be distributed in different ways in the documents of a collection. This is especially true in large documents that tend to cover a large number of topics and include variable vocabulary. A long document could thus contain several relevant passages that a matching model must capture. Unlike long documents, short documents are likely to be relevant to a specific subject and tend to contain a more restricted vocabulary. Assessing their relevance is in principle simpler than assessing the one of longer documents. In this thesis, we have proposed different contributions, each addressing one of the above-mentioned issues. First, in order to solve the problem of vocabulary mismatch, we used distributed representations of words (word embedding) to allow a semantic matching between the different words. These representations have been used in IR applications where document/query similarity is computed by comparing all the term vectors of the query with all the term vectors of the document, regardless. Unlike the models proposed in the state-of-the-art, we studied the impact of query terms regarding their presence/absence in a document. We have adopted different document/query matching strategies. The intuition is that the absence of the query terms in the relevant documents is in itself a useful aspect to be taken into account in the matching process. Indeed, these terms do not appear in documents of the collection for two possible reasons: either their synonyms have been used or they are not part of the context of the considered documents.
This thesis relates to Text-To-Speech synthesis and more particularly deals with a corpus based approach that is the unit selection speech synthesis. We propose the Kullback-Leibler divergence to guide the iterative selection of candidate sentences: indeed, this criterion gives the possibility to control the unit distribution at each step of the algorithm. The aim of this method is to build a corpus whose unit distribution approximates a given target distribution. We also propose an efficient implementation of this method which incrementally update the KL divergence in the sentence selection process. We then use our method for database reduction adapted to a specific domain TTS synthesis applications. We show that this adaptive database pruning method is a promising reduction method.
One daunting challenge of Content Based Image Retrieval systems is the requirement of annotated databases. To limit the burden of annotation, this thesis proposes a system of image annotation based on gaze data. The purpose is to classify a small set of images according to a target category (binary classification) in order to classify a set of unseen images. Among the gaze features known to be informative about the intentions of the participants, we have determined a Gaze-Based Intention Estimator (GBIE), computable in real-time; independent from both the participant and the target category. This implicit annotation is better than random annotation but is inherently uncertain. In a second part, the images annotated by the GBIE from the participants' gaze data are used to classify a bigger set of images with an algorithm that handles label uncertainty: P-SM combining classification and regression SVM. We have determined among different strategies a criterion of relevance in order to discriminate the most reliable labels, involved in the classification part, from the most uncertain labels, involved in the regression part. The average accuracy of P-SVM is evaluated in different contexts and can compete with the performances of standard classification algorithm trained with true-class labels. These evaluations were first conducted on a standard benchmark for comparing with state-of-the-art results and later conducted on food image dataset.
France has a large nationwide longitudinal database with claims and hospital data, the Système National des Données de Santé (French National healthcare database – SNDS), which currently covers almost the complete French population, from birth or immigration to death or emigration, and includes all reimbursed medical and paramedical encounters. Since SNDS systematically and prospectively captures drug dispensings, death, and events leading to hospital stays, it has a strong potential for drug assessment in real life settings. Following the worldwide withdrawal of rofecoxib in 2004, several initiatives aiming to develop and evaluate methodologies for drug safety monitoring on healthcare databases emerged. The EU-ADR alliance (Exploring and Understanding Adverse Drug Reactions by integrative mining of clinical records and biomedical knowledge) and OMOP (Observational Outcomes Partnership) were respectively launched in Europe and in the Unites-States. These experiments demonstrated the usefulness of pharmacoepidemiological approaches in drug safety signal detection. However the SNDS had never been tested in this scope. Self-controlled case series showed the best performances in both outcomes, ALI and UGIB, with AUCs reaching respectively 0.80 and 0.94 and MSEs 0.07 and 0.12. For UGIB optimal performances were observed when adjusting for multiple drugs and using a risk window corresponding to the 30 first days of exposure. For ALI, optimal performances were also observed when adjusting for multiple drugs but using a risk window corresponding to the overall period covered by drug dispensings. Negative drug control implementation highlighted that a low systematic error seemed to be generated by the optimum variants in the SNDS but that protopathic bias and confounding by indication remained unaddressed issues. These results showed that self-controlled case series were well suited to detect drug safety alerts associated with UGIB and ALI in the SNDS in an accurate manner. A clinical perspective remains necessary to rule out potential false positive signals from residual confounding. The application in routine of such approaches extended to other outcomes of interest could result in substantial progress in pharmacovigilance in France.
Restricted Boltzmann machines (RBM) are graphical models that learn jointly a probability distribution and a representation of data. Despite their simple architecture, they can learn very well complex data distributions such the handwritten digits data base MNIST. However, not all variants of RBM perform equally well, and little theoretical arguments exist for these empirical observations. In the first part of this thesis, we ask how come such a simple model can learn such complex probability distributions and representations. By analyzing an ensemble of RBM with random weights using the replica method, we have characterised a compositional regime for RBM, and shown under which conditions (statistics of weights, choice of transfer function) it can and cannot arise. Both qualitative and quantitative predictions obtained with our theoretical analysis are in agreement with observations from RBM trained on real data. In a second part, we present an application of RBM to protein sequence analysis and design. Owe to their large size, it is very difficult to run physical simulations of proteins, and to predict their structure and function. It is however possible to infer information about a protein structure from the way its sequence varies across organisms. For instance, Boltzmann Machines can leverage correlations of mutations to predict spatial proximity of the sequence amino-acids. Here, we have shown on several synthetic and real protein families that provided a compositional regime is enforced, RBM can go beyond structure and extract extended motifs of coevolving amino-acids that reflect phylogenic, structural and functional constraints within proteins. Moreover, RBM can be used to design new protein sequences with putative functional properties by recombining these motifs at will.
Monitoring animal health worldwide, especially the early detection of outbreaks of emerging and exotic pathogens, is one of the means of preventing the introduction of infectious diseases in France. Recently, there is an increasing awareness among health authorities for the use of unstructured information published on the Web for epidemic intelligence purposes. Our approach is generic; however, it was elaborated using five exotic animal infectious diseases: african swine fever, foot-and-mouth disease, bluetongue, Schmallenberg, and avian influenza. We show that the text mining techniques, supplemented by the knowledge of domain experts, are the foundation of an efficient and reactive system for monitoring animal health emergence on the Web. Our tool will be used by the French epidemic intelligence team for international monitoring of animal health, and will facilitate the early detection of events related to emerging health hazards identified from media reports on the Web.
Main results are: (1) the perception and production of the vowels are greatly influenced by the L1 of the learners, both at the beginning and after nine months of learning; (2) the perception and production performances of the learners are better predicted by the identity of the vowel rather than its status in L2 compared to L1 (new, similar or identical vowels); (3) the phonetic training we gave showed no benefit on the perceptual or production performances of the children.
Today users can interact with popular virtual assistants such as Siri to accomplish their tasks on a digital environment. In these systems, links between natural language requests and their concrete realizations are specified at the conception phase. A more adaptive approach would be to allow the user to provide natural language instructions or demonstrations when a task is unknown by the assistant. An adaptive solution should allow the virtual assistant to operate a much larger digital environment composed of multiple application domains and providers and better match user needs. We have previously developed robotic systems, inspired by human language developmental studies, that provide such a usage-based adaptive capacity. Here we extend this approach to human interaction with a virtual assistant that can first learn the mapping between verbal commands and basic action semantics of a specific domain. Then, it can learn higher level mapping by combining previously learned procedural knowledge in interaction with the user. The flexibility of the system is demonstrated as the virtual assistant can learn actions in a new domains (Email, Wikipedia,...), and can then learn how email and Wikipedia basic procedures can be combined to form hybrid procedural knowledge
Current musical improvisation systems are able to generate unidimensional musical sequences by recombining their musical contents. However, considering several dimensions (melody, harmony...) and several temporal levels are difficult issues. In this thesis, we propose to combine probabilistic approaches with formal language theory in order to better assess the complexity of a musical discourse, both from a multidimensional and multi-level point of view in the context of improvisation where the amount of data is limited. First, we present a system able to follow the contextual logic of an improvisation modelled by a factor oracle whilst enriching its musical discourse with multidimensional knowledge represented by interpolated probabilistic models. Then, this work is extended to create another system using a belief propagation algorithm representing the interaction between several musicians, or between several dimensions, in order to generate multidimensional improvisations. Finally, we propose a system able to improvise on a temporal scenario with multi-level information modelled with a hierarchical grammar. We also propose a learning method for the automatic analysis of hierarchical temporal structures. Every system is evaluated by professional musicians and improvisers during listening sessions
The increasing quantity of video material available requires automatic structuring techniques to facilitate access to the information contained in documents, generic enough to be able to structure different kinds of videos. To this end, we develop two kinds of thematic structuring of TV shows, linear or hierarchical, based on the automatic transcripts of the speech pronounced in the programs. These transcripts, independent of the type of documents considered, are exploited using natural language processing methods adapted to the peculiarities of professional videos transcripts – transcription errors, limited number of repetition – by taking into account linguistic knowledge and automatic speech recognition and signal information. Experiments conducted on three corpora composed of broadcast news and reports on current affairs, manually and automatically transcribed, show that the proposed adjustments lead to improved performance of the structuring methods developed
The purpose of this PhD thesis is to study how neology manifests itself in a french language dictionary. I have chosen to work on the electronic version of the "Nouveau Petit Robert", since it is the epitome of a reference dictionary. I studied the new attested words that were added between 1990 and 2012 in the "Nouveau Petit Robert Électronique" 2012. In doing so, the corpus of this study is now composed of 477 new attested words added between 1990 and 2012.This work intends to bring out the formation modes of these new words as well as the experience areas to which they are linked. In this section the lexicographical particularities of the "Nouveau Petit Robert Électronique" 2012 are highlighted by comparing a few words extracted from its corpus with the same words in two other dictionaries: "Le Petit Larousse Illustré" 2016 and the "Wiktionnaire". The most efficient formation modes for the corpus' words are those pertaining to the internal matrix with a total of 266 words, as well as those pertaining to the external matrix with a total of 186 words. The corpus'words belonging to the internal matrix are the most numerous, and it indicates that the lexicon renews itself on its own. There is a distinct decline of the social sciences in favor of technical sciences to be observed.
This thesis is a study of the prosodic interlanguage of students of English as a foreign language whose native language is French or Spanish. It is organized in two main parts. The first part is a study of the methods of conception and representation of prosody for the analysis of interlanguage – a hybrid linguistic system which includes characteristics of the student's native language, characteristics of the target language, and intermediate developmental or characteristics. This provides a methodological framework for the phonetic analysis and phonological interpretation of this type of prosodic systems. The results show traces of the influence of their respective native languages at the phonetic and phonological levels, as well as developmental characteristics common to both groups of learners. The results serve as a basis for reflection on the levels of abstraction in the study of prosody and on the didactic priorities for teaching oral English at a university level.
Computationnal linguistics have questioned former studies on fixation. Indeed, it has become essential to describe set expressions given their frequency in texts. This study sets about examining predicate adjectivals in this setting. Taking object classes as a model, a semantico-syntactic typology has been worked out for these sequences. The final objective of our analysis is to create an electronic dictionary of predicate adjectivals. This dissertation communicates three essential points: fixation of adjectivals, their definitions and their classification based on common distributional properties.
This thesis focus on a proposition that helps humans during the exploration of database. The particularity of this proposition relies on a co-evolution principle between the user and an intelligent interface. It provides a support to the understanding of the domain represented by the data. A metaphor of living virtual museum is adopted. This museum evolves incrementally according to the user's interactions. It incarnates both the data and the semantic information which are expressed by a knowledge model specific to the domain of the data. Through the topological organization and the incremental evolution, the museum personalizes online the user's exploration. The approach is insured by three main mechanisms: the evaluation of the user profile modelled by a dynamical weighting of the semantic information, the use of this dynamic profile to establish a recommendation as well as the incarnation of the data in the living museum. The approach has been applied to the heritage domain as part of the ANTIMOINE project, funded by the National Research Agency (ANR). The genericity of the latter has been demonstrated through its application to a database of publications but also using various types of interfaces (website, virtual reality).Experiments have validated the hypothesis that our system adapts itself to the user behavior and that it is able, in turn, to influence him. They also showed the comparison between a 2D interface and a 3D interface in terms of quality of perception, guidance, preference and efficiency.
This hypothesis is particularly convenient when it comes to developing theoretical guarantees that the learning is accurate. However, it is not realistic from the point of view of applicative domains that have emerged in the last years. In this thesis, we focus on four distinct problems in artificial intelligence, that have mainly one common point: All of them imply knowledge transfer from one domain to the other. The first problem is analogical reasoning and concerns statements of the form "A is to B as C is to D". The second one is transfer learning and involves classification problem in situations where the training data and test data do not have the same distribution (nor even belong to the same space). The third one is data stream mining, ie. managing data that arrive one by one in a continuous and high-frequency stream with changes in the distributions. The last one is collaborative clustering and focuses on exchange of information between clustering algorithms to improve the quality of their predictions. This framework is based on the notion of Kolmogorov complexity, which measures the inner information of an object. This tool is particularly adapted to the problem of transfer, since it does not rely on probability distributions while being able to model the changes in the distributions. Apart from this modeling effort, we propose, in this thesis, various discussions on aspects and applications of the different problems of interest. These discussions all concern the possibility of transfer in multiple domains and are not based on complexity only.
Anticipating clients'needs is crucial to any business — this is particularly true for corporate and institutional banks such as BNP Paribas Corporate and Institutional Banking due to their role in the financial markets. This thesis addresses the problem of future interests prediction in the financial context and focuses on the development of ad hoc algorithms designed for solving specific financial challenges. This manuscript is composed of five chapters: - Chapter 1 introduces the problem of future interests prediction in the financial world. The goal of this chapter is to provide the reader with all the keys necessary to understand the remainder of this thesis. These keys are divided into three parts: a presentation of the datasets we have at our disposal to solve the future interests prediction problem and their characteristics, an overview of the candidate algorithms to solve this problem, and the development of metrics to monitor the performance of these algorithms on our datasets. This chapter finishes with some of the challenges that we face when designing algorithms to solve the future interests problem in finance, challenges that will be partly addressed in the following chapters; - Chapter 2 proposes a benchmark of some of the algorithms introduced in Chapter 1 on a real-word dataset from BNP Paribas CIB, along with a development on the difficulties encountered for comparing different algorithmic approaches on a same dataset and on ways to tackle them. This benchmark puts in practice classic recommendation algorithms that were considered on a theoretical point of view in the preceding chapter, and provides further intuition on the analysis of the metrics introduced in Chapter 1; - Chapter 3 introduces a new algorithm, called Experts Network, that is designed to solve the problem of behavioral heterogeneity of investors on a given financial market using a custom-built neural network architecture inspired from mixture-of-experts research. In this chapter, the introduced methodology is experimented on three datasets: a synthetic dataset, an open-source one and a real-world dataset from BNP Paribas CIB. The chapter provides further insights into the development of the methodology and ways to extend it; - Chapter 4 also introduces a new algorithm, called History-augmented Collaborative Filtering, that proposes to augment classic matrix factorization approaches with the information of users and items'interaction histories. This chapter provides further experiments on the dataset used in Chapter 2, and extends the presented methodology with various ideas. Notably, this chapter exposes an adaptation of the methodology to solve the cold-start problem and applies it to a new dataset; - Chapter 5 brings to light a collection of ideas and algorithms, successful or not, that were experimented during the development of this thesis. This chapter finishes on a new algorithm that blends the methodologies introduced in Chapters 3 and 4.
Multivalued Decision Diagrams (MDDs) are efficient data structures widely used in several fields like verification, optimization and dynamic programming. In this thesis, we first focus on improving the main algorithms such as the reduction, allowing MDDs to potentially exponentially compress set of tuples, or the combination of MDDs such as the intersection of the union. We go further by designing parallel algorithms, and algorithms handling non-deterministic MDDs. We then investigate relaxed MDDs, that are more and more used in optimization, and define the notions of relaxed reduction or operation and design efficient algorithms for them. The sampling of solutions stored in a MDD is solved with respect to probability mass functions or Markov chains. These new propagators outperform the existing ones and allow the reformulation of several other constraints such as the dispersion constraint, and even to define new ones easily. We finally apply our algorithm to several real world industrial problems such as text and music generation and geomodeling of a petroleum reservoir.
The extraction of structured information from a document is one of the main parts of natural language processing (NLP). This extraction usually consists in three steps: named entities recognition relation extraction and event extraction. The notion of event covers a broad list of different phenomena which are characterized through a varying number of roles. Thereupon, Event extraction consists in detecting the occurrence of an event then determining its argument, that is, the different entities filling specific roles. The current best approaches, based on neural networks, focus on the direct neighborhood of the target word in the sentence. Information in the rest of the document is then usually ignored. This thesis presents different approaches aiming at exploiting this document-level context. We begin by reproducing a state of the art convolutional neural network and analyze some of its parameters. We then present an experiment showing that, despite its good performances, our model only exploit a narrow context at the intra-sentential level. Subsequently, we present two methods to generate and integrate a representation of the inter-sentential context in a neural network operating on an intra-sentential context. We also show that this task-specific representation is better than an existing generic representation of the inter-sentential context. Our second contribution, in response to the limitations of the first one, allows for the dynamic generation of a specific context for each target word. This method yields the best performances for a single model on multiples datasets. Finally, we take a different tack on the exploitation of the inter-sentential context. We try a more direct modelisation of the dependencies between multiple event instances inside a document in order to produce a joint prediction. To do so, we use the PSL (Probabilistic Soft Logic) framework which allows to model such dependencies through logic formula.
Antimicrobial resistance (AMR) is a major global public health concern. The Viet Nam National Action Plan on AMR recognised surveillance as one of critical components for control. However, the current AMR surveillance system (AMRSS) in Viet Nam is likely to be over-representing severe and hospital acquired infections (HAI), potentially resulting in an overestimation of resistance among community acquired infection (CAI). This thesis aims to evaluate the AMRSS in Viet Nam and to make suggestions to optimize the AMRSS effectiveness in providing accurate and representative AMR data for CAI patients in this setting. A systematic litterature review was conducted to generate an overview of the AMRSSs that have been implemented globally and any evaluations of such systems. There is no standardized framework or guidelines for conducting evaluation of AMRSS. Less than 10% of the systems reported some system evaluation, focusing on few attributes such as representativeness, timeliness, bias, cost, coverage, and sensitivity. The sensitivity of the AMRSS was in the 2-5% range and remained similar between the two periods. There was a delay in data submission from the hospitals, which affected surveillance timeliness. No evaluation of the surveillance system was carried out to identify problems and implement prompt resolutions. Overall, the results showed that the accuracy of AMR data is enhanced when the number of hospitals increases (0.6% decrease in mean squared error for one additional hospital (CI 0.6% - 0.7%)). For a given amount of budget, the optimal numbers of hospitals by type can be determined using this modelling approach to identify a system with the best values for each performance attribute. The results indicate that the current AMRSS can increase the proportions of specialized and provincial hospitals to increase accuracy of data and system representativeness. The models were based on VINARES data, therefore the results are likely to be valid for an AMRSS with similar organizational structures and data collection protocols. The amount of budget that the government and foreign development partners are willing to spend on AMR surveillance is also an important factor in identifying the optimal hospital combination for the AMRSS.
The present study deals with word frequencies distributions and their relation to probabilistic Information Retrieval (IR) models. We examine the burstiness phenomenon of word frequencies in textual collections. We propose to model this phenomenon as a property of probability distributions and we study the Beta Negative Binomial and Log-Logistic distributions to model word frequencies. We then focus on probabilistic IR models and their fundamental properties. Our analysis reveals that probability distributions underlying most state-of-the-art models do not take this phenomenon into account, even if fundamental properties of IR models such as concavity enable implicitly to take it into account. Lastly, we study empirically and theoretically pseudo relevance feedback models. We propose a theoretical framework which explain well the empirical behaviour and performance of pseudo relevance feedback models. Overall, this highlights interesting properties for pseudo relevance feedback and shows that some state-of-the-art model are inadequate.
This thesis deals with the detection of credit card fraud. According to the European Central Bank, the value of frauds using cards in 2016 amounted to 1.8 billion euros. The challenge for institutions is to reduce these frauds. In general, fraud detection systems consist of an automatic system built with "if-then" rules that control all incoming transactions and trigger an alert if the transaction is considered suspicious. An expert group checks the alert and decides whether it is true or not. The criteria used in the selection of the rules that are kept operational are mainly based on the individual performance of the rules. This approach ignores the non-additivity of the rules. We propose a new approach using power indices. This approach assigns to the rules a normalized score that quantifies the influence of the rule on the overall performance of the group. The indexes we use are the Shapley Value and Banzhaf Value. Their applications are 1) Decision support to keep or delete a rule; 2) Selection of the number k of best-ranked rules, in order to work with a more compact set. Using real credit card fraud data, we show that: 1) This approach performs better than the one that evaluates the rules in isolation. 2) The performance of the set of rules can be achieved by keeping one-tenth of the rules. We observe that this application can be considered as a task of selection of characteristics: We show that our approach is comparable to the current algorithms of the selection of characteristics. It has an advantage in rule management because it assigns a standard score to each rule. This is not the case for most algorithms, which focus only on an overall solution. We propose a new version of Banzhaf Value, namely k-Banzhaf; which outperforms the previous in terms of computing time and has comparable performance. Finally, we implement a self-learning process to reinforce the learning in an automatic learning algorithm. We compare these with our power indices to rank credit card fraud data. In conclusion, we observe that the selection of characteristics based on the power indices has comparable results with the other algorithms in the self-learning process.
Incremental dialogue systems are able to process the user's speech as it is spoken (without waiting for the end of a sentence before starting to process it). This makes them able to take the floor whenever they decide to (the user can also speak whenever she wants, even if the system is still holding the floor). As a consequence, they are able to perform a richer set of turn-taking behaviours compared to traditional systems. Several contributions are described in this thesis with the aim of showing that dialogue systems'turn-taking capabilities can be automatically improved from data. First, human-human dialogue is analysed and a new taxonomy of turn-taking phenomena in human conversation is established. Based on this work, the different phenomena are analysed and some of them are selected for replication in a human-machine context (the ones that are more likely to improve a dialogue system's efficiency). Then, a new architecture for incremental dialogue systems is introduced with the aim of transforming a traditional dialogue system into an incremental one at a low cost (also separating the turn-taking manager from the dialogue manager). To be able to perform the first tests, a simulated environment has been designed and implemented. It is able to replicate user and ASR behaviour that are specific to incremental processing, unlike existing simulators. Combined together, these contributions led to the establishement of a rule-based incremental dialogue strategy that is shown to improve the dialogue efficiency in a task-oriented situation and in simulation. A new reinforcement learning strategy has also been proposed. It is able to autonomously learn optimal turn-taking behaviours throughout the interactions. The simulated environment has been used for training and for a first evaluation, where the new data-driven strategy is shown to outperform both the non-incremental and rule-based incremental strategies. In order to validate these results in real dialogue conditions, a prototype through which the users can interact in order to control their smart home has been developed.
Comparing and matching probability distributions is a crucial in numerous machine learning (ML) algorithms. Optimal transport (OT) defines divergences between distributions that are grounded on geometry: starting from a cost function on the underlying space, OT consists in finding a mapping or coupling between both measures that is optimal with respect to that cost. Despite those advantages, the applications of OT in data sciences have long been hindered by the mathematical and computational complexities of the underlying optimization problem. To circumvent these issues, one approach consists in focusing on particular cases that admit closed-form solutions or that can be efficiently solved. In particular, OT between elliptical distributions is one of the very few instances for which OT is available in closed form, defining the so-called Bures-Wasserstein (BW) geometry. This thesis builds extensively on the BW geometry, with the aim to use it as basic tool in data science applications. To do so, we consider settings in which it is alternatively employed as a basic tool for representation learning, enhanced using subspace projections, and smoothed further using entropic regularization. In a first contribution, the BW geometry is used to define embeddings as elliptical probability distributions, extending on the classical representation of data as vectors in R^d. In the second contribution, we prove the existence of transportation maps and plans that extrapolate maps restricted to lower-dimensional projections, and show that subspace-optimal plans admit closed forms in the case of Gaussian measures. Our third contribution consists in deriving closed forms for entropic OT between Gaussian measures scaled with a varying total mass, which constitute the first non-trivial closed forms for entropic OT and provide the first continuous test case for the study of entropic OT. Finally, in a last contribution, entropic OT is leveraged to tackle missing data imputation in a non-parametric and distribution-preserving way.
Voice based personal assistants are part of our daily lives. Their performance suffers in the presence of signal distortions, such as noise, reverberation, and competing speakers. This thesis addresses the problem of extracting the signal of interest in such challenging conditions by first localizing the target speaker and using the location to extract the target speech. In a first stage, a common situation is considered when the target speaker utters a known word or sentence such as the wake-up word of a distant-microphone voice command system. A method that exploits this text information in order to improve the speaker localization performance in the presence of competing speakers is proposed. The proposed solution uses a speech recognition system to align the wake-up word to the corrupted speech signal. A model spectrum representing the aligned phones is used to compute an identifier which is then used by a deep neural network to localize the target speaker. Results on simulated data show that the proposed method reduces the localization error rate compared to the classical GCC-PHAT method. Similar improvements are observed on real data. Given the estimated location of the target speaker, speech separation is performed in three stages. In the first stage, a simple delay-and-sum (DS) beamformer is used to enhance the signal impinging from that location which is then used in the second stage to estimate a time-frequency mask corresponding to the localized speaker using a neural network. This mask is used to compute the second-order statistics and to derive an adaptive beamformer in the third stage. A multichannel, multispeaker, reverberated, noisy dataset---inspired from the famous WSJ0-2mix dataset---was generated and the performance of the proposed pipeline was investigated in terms of the word error rate (WER). To make the system robust to localization errors, a Speaker LOcalization Guided Deflation (SLOGD) based approach which estimates the sources iteratively is proposed. At each iteration the location of one speaker is estimated and used to estimate a mask corresponding to that speaker. The estimated source is removed from the mixture before estimating the location and mask of the next source. The proposed method is shown to outperform Conv-TasNet. Finally, we consider the problem of explaining the robustness of neural networks used to compute time-frequency masks to mismatched noise conditions. We employ the so-called SHAP method to quantify the contribution of every time-frequency bin in the input signal to the estimated time-frequency mask. We define a metric that summarizes the SHAP values and show that it correlates with the WER achieved on separated speech. To the best of our knowledge, this is the first known study on neural network explainability in the context of speech separation.
This thesis presents several contributions on the theme of recognising textual entailment (RTE). The RTE is the human capacity, given two texts, to determine whether the meaning of the second text could be deduced from the meaning of the first or not. One of the contributions made to the field is a hybrid system of RTE taking analysis of an existing stochastic parser to label them with semantics roles, then turning obtained structures in logical formulas using rewrite rules to finally test the entailment using proof tools. Another contribution of this thesis is the generation of finely annotated tests suites with a uniform distribution of phenomena coupled with a new methodology of systems evaluation using error minning techniques developed by the community of parsing allowing better identification of systems limitations. For this, we create a set of formulas, then we generate annotated syntactics realisations corresponding by using an existing generation system. Then, we test whether or not there is an entailment between each pair of possible syntactics realisations. Finally, we select a subset of this set of problems of a given size and a satisfactory a certain number of constraints using an algorithm that we developed
This thesis is dedicated to the study of Recommendation Systems for implicit feedback (clicks) mostly using Learning-to-rank and neural network based approaches. In this line, we derive a novel Neural-Network model that jointly learns a new representation of users and items in an embedded space as well as the preference relation of users over the pairs of items and give theoretical analysis. In addition we contribute to the creation of two novel, publicly available, collections for recommendations that record the behavior of customers of European Leaders in eCommerce advertising, Kelkoofootnote{url{https://www.kelkoo.com/}} and Purchfootnote{label{purch}url{http://www.purch.com/}}. Both datasets gather implicit feedback, in form of clicks, of users, along with a rich set of contextual features regarding both customers and offers. Therefore, we propose a simple yet effective strategy on how to overcome the popularity bias introduced while designing an efficient and scalable recommendation algorithm by introducing diversity based on an appropriate representation of items. Further, this collection contains contextual information about offers in form of text. Keywords. Recommendation Systems, Data Sets, Learning-to-Rank, Neural Network, Popularity Bias, Diverse Recommendations, Contextual information, Topic Model.
As semantically non-compositional phrases, idioms are lexical units. Consequently, they must have their own entries in a lexical resource, with a lexicographic definition and grammatical characteristics. Furthermore, because of their phrasal signifier, idioms show – to varying degrees – a formal flexibility (passivization, attachment of modifiers, substitution of components, etc.) Our thesis defends the view that a description of idioms that combine identification of their lexical components and identification of dependency links between these components will permit to predict their formal variations. Such a description is possible only in a model of lexicon that describes precisely combinatorial proprieties of lexical units. The thesis makes two principal contributions to the study of phraseology. The first contribution is the development of a precise description of idioms' lexico-syntactic structures. The second contribution is the identification and the study of structural, syntactic and lexical variations linked to idioms' formal flexibility. Idioms' formal variations are correlated with their lexico-syntactic structures, but also with their lexicographic definitions. This work leads us to introduce the notion of structural projection, that plays a central role in the continuum of idiom's formal flexibility
The overall objective of this thesis is to foster the deployment of supervised learning in detection systems to strengthen detection. To that end, we consider the whole machine learning pipeline (data annotation, feature extraction, training, and evaluation) with security experts as its core since it is crucial to pursue real-world impact. First, we provide methodological guidance to help security experts build supervised detection models that suit their operational constraints. Moreover, we design and implement DIADEM, an interactive visualization tool that helps security experts apply the methodology set out. DIADEM deals with the machine learning machinery to let security experts focus mainly on detection. Besides, we propose a solution to effectively reduce the labeling cost in computer security annotation projects. We design and implement an end-to-end active learning system, ILAB, tailored to security experts needs. Our user experiments on a real-world annotation project demonstrate that they can annotate a dataset with a low workload thanks to ILAB. Finally, we consider automatic feature generation as a means to ease, and thus foster, the use of machine learning in detection systems. We define the constraints that such methods should meet to be effective in building detection models. We compare three state-of-the-art methods based on these criteria, and we point out some avenues of research to better tailor automatic feature generation to computer security experts needs.
Learning a new language means making use, more or less consciously, of the sphere ofproximity between the target linguistic system and the other languages. We focus onthese transfers and on their influence over interlanguage, in the case of the Romanian learnersstudying French as a foreign language, but also Italian and Spanish. The data analysis enables us, on the one hand, to give an account of the connectionexisting between the conformity to the norm and the complexity of interlanguage and, on theother hand, to ascertain the role of both positive and negative linguistic transfers against thesetwo dimensions of interlanguage (the complexity and the conformity)
The main goal of this thesis resides in using rich and efficient profiling to improve the adequation between the retrieved information and the user's expectations. We focus on exploiting as much feedback as we can (being clicks, ratings or written reviews) as well as context. In the meantime, the tremendous growth of ubiquitous computing forces us to rethink the role of information access platforms. Therefore, we took interest not solely in performances but also in accompanying users through their access to the information. Not only it improves the system performances but it also brings some kind of explicativity to the recommendation. Thus, we propose to accompany the user through his experience accessing information instead of constraining him to a given set of items the systems finds fitting.
They have given rise to numerous studies in Natural Language Processing. Indeed, their study and precise identification are essential, both from a theoretical and applicative perspective. However, most of the researches about the subject relate to everyday uses of language: "small talk" dialogs, requests for schedule, speeches, etc. But what about spontaneous speech production made in a restrained framework? To our knowledge, no study has ever been carried out in this context. However, we know that using a "language specialty" in the framework of a given task leads to specific behaviours. Our thesis work is devoted to the linguistic and computational study of disfluencies within such a framework. These dialogs concern air traffic control, which entails both pragmatic and linguistic constraints. We carry out an exhaustive study of disfluencies phenomena in this context. At first we conduct a subtle analysis of these phenomena. Then we model them to a level of abstraction, which allows us to obtain the patterns corresponding to the different configurations observed. Finally we propose a methodology for automatic processing. It consists of several algorithms to identify the different phenomena, even in the absence of explicit markers. It is integrated into a system of automatic processing of speech. Eventually, the methodology is validated on a corpus of 400 sentences.
Taking into account the semantic aspect of the textual data during the classification task has become a real challenge in the last ten years. This difficulty is in addition to the fact that most of the data available on social networks are short texts, which in particular results in making methods based on the "bag of words" representation inefficient. The approach proposed in this research project is different from the approaches proposed in previous work on the enrichment of short messages for three reasons. First, we do not use external knowledge like Wikipedia because typically short messages that are processed by the company come from specific domains. Secondly, the data to be processed are not used for the creation of resources because of the operation of the tool. In this thesis, we propose the creation of resources enabling to enrich the short messages in order to improve the performance of the tool of the semantic grouping of the company Succeed Together. The tool implements supervised and unsupervised classification methods. To build these resources, we use sequential data mining techniques.
This thesis aimed at conducting research on the synthesis and expressive transformations of the singing voice, towards the development of a high-quality synthesizer that can generate a natural and expressive singing voice automatically from a given score and lyrics. This thesis provides some contributions in each of those 3 directions. First, a fully-functional synthesis system has been developed, based on diphones concatenations. The modular architecture of this system allows to integrate and compare different signal modeling approaches. Then, the question of the control is addressed, encompassing the automatic generation of the f0, intensity, and phonemes durations. The modeling of specific singing styles has also been addressed by learning the expressive variations of the modeled control parameters on commercial recordings of famous French singers. Finally, some investigations on expressive timbre transformations have been conducted, for a future integration into our synthesizer.
Semantic information retrieval (SIR) aims to propose models that allow us to rely, beyond statistical calculations, on the meaning and semantics of the words of the vocabulary, in order to better represent relevant documents with respect to user's needs, and better retrieve them. The aim is therefore to overcome the classical purely statistical («bag of wordsé») approaches, based on strings'matching and the analysis of the frequencies of the words and their distributions in the text. To do this, existing SIR approaches, through the exploitation of external semantic resources (thesauri, ontologies, etc.), proceed by injecting knowledge into the classical IR models (such as the vector space model) in order to disambiguate the vocabulary or to enrich the representation of documents and queries. The semantic resources thus exploited are «ﬂattened», the calculations are generally confined to calculations of semantic similarities. In order to better exploit the semantics in RI, we propose a new model, which allows to unify in a coherent and homogeneous way the numerical (distributional) and symbolic (semantic) information without sacrificing the power of the analyzes of the one for the other. The semantic-documentary network thus modeled is translated into a weighted graph. The matching mechanism is provided by a Spreading activation mechanism in the graph. This new model allows to respond to queries expressed in the form of key words, concepts or even examples of documents. The propagation algorithm has the merit of preserving the well-tested characteristics of classical information retrieval models while allowing a better consideration of semantic models and their richness. Depending on whether semantics is introduced in the graph or not, this model makes it possible to reproduce a classical IR or provides, in addition, some semantic functionalities. The co-occurrence in the graph then makes it possible to reveal an implicit semantics which improves the precision by solving some semantic ambiguities. The explicit exploitation of the concepts as well as the links of the graph allow the resolution of the problems of synonymy, term mismatch, semantic coverage, etc. These semantic features, as well as the scaling up of the model presented, are validated experimentally on a corpus in the medical field.
The analyser performs automatic Chinese word segmentation based on linguistic rules and dictionaries, part-of-speech tagging based on n-gram statistics and dependency grammar parsing. The module allows to extract information around named entities and activities. In order to achieve these goals, we have tackled the following main issues: segmentation and part-of-speech ambiguity; unknown word identification in Chinese text; attachment ambiguity in parsing. Chinese texts are analysed sentence by sentence. Given a sentence, the analyzer begins with typographic processing to identify sequences of Latin characters and numbers. Then, dictionaries are used for preliminary segmentation into words. An n-gram language model is created from a training corpus and selects the best word segmentation and parts-of-speech. Dependency grammar parsing is used to annotate relations between words. A first step of named entity recognition is performed after parsing. Its goal is to identify single-word named entities and noun-phrase-based named entities and to determine their semantic type. These named entities are then used in knowledge extraction. Knowledge extraction rules are used to validate named entities or to change their types. Knowledge extraction consists of two steps: automatic content extraction and tagging from analysed text; extracted contents control and ontology-based co-reference resolution.
The work presented in this thesis is about TTS speech synthesis and, more particularly, about statistical speech synthesis for French. We present an analysis on the impact of the linguistic contextual factors on the synthesis achieved by the HTS statistical speech synthesis system. To conduct the experiments, two objective evaluation protocols are proposed. The first one uses Gaussian mixture models (GMM) to represent the acoustical space produced by HTS according to a contextual feature set. By using a constant reference set of natural speech stimuli, GMM can be compared between themselves and consequently acoustic spaces generated by HTS. The second objective evaluation that we propose is based on pairwise distances between natural speech and synthetic speech generated by HTS. Results obtained by both protocols, and confirmed by subjective evaluations, show that using a large set of contextual factors does not necessarily improve the modeling and could be counter-productive on the speech quality.
This thesis presents a methodology to solve certain classification problems, particularly those involving sequential classification for Natural Language Processing tasks. It proposes the use of an iterative, error-based approach to improve classification performance, suggesting the incorporation of expert knowledge into the learning process through the use of knowledge rules. We applied and evaluated the methodology to two tasks related with the detection of hedging in scientific articles: those of hedge cue identification and hedge cue scope detection. Additionally, this thesis proposes a class schema for representing sentence analysis in a unique structure, including the results of different linguistic analysis. This allows us to better manage the iterative process of classifier improvement, where different attribute sets for learning are used in each iteration. We also propose to store attributes in a relational model, instead of the traditional text-based structures, to facilitate learning data analysis and manipulation.
The inevitable emergence of the necessity to estimate the size of a software thus estimating the probable cost and effort is a direct outcome of increasing need of complex and large software in almost every conceivable situation. Furthermore, due to the competitive nature of the software development industry, the increasing reliance on accurate size estimation at early stages of software development becoming a commonplace practice. Traditionally, estimation of software was performed a posteriori from the resultant source code and several metrics were in practice for the task. However, along with the understanding of the importance of code size estimation in the software engineering community, the realization of early stage software size estimation, became a mainstream concern. Once the code has been written, size and cost estimation primarily provides contrastive study and possibly productivity monitoring. On the other hand, if size estimation can be performed at an early development stage (the earlier the better), the benefits are virtually endless. This research focuses on functional size estimation metrics commonly known as Function Point Analysis (FPA) that estimates the size of a software in terms of the functionalities it is expected to deliver from a user's point of view. One significant problem with FPA is the requirement of human counters, who need to follow a set of standard counting rules, making the process labour and cost intensive (the process is called Function Point Counting and the professional, either analysts or counters). Moreover, these rules, in many occasion, are open to interpretation, thus they often produce inconsistent counts. Furthermore, the process is entirely manual and requires Function Point (FP) counters to read large specification documents, making it a rather slow process. Automation of the process of identifying the FPs in a document accurately, will at least reduce the reading requirement of the counters, making the process faster and thus shall significantly reduce the cost. Moreover, consistent identification of FPs will allow the production of consistent raw function point counts. To the best of our knowledge, the works presented in this thesis is an unique attempt to analyse specification documents from early stages of the software development, using a generic approach adapted from well established Natural Language Processing (NLP) practices.
Future applications of robotics, especially personal service robots, will require continuous adaptability to the environment, and particularly the ability to recognize new objects and learn new words through interaction with humans. Though having made tremendous progress by using machine learning, current computational models for object detection and representation still rely heavily on good training data and ideal learning supervision. In contrast, two year old children have an impressive ability to learn to recognize new objects and at the same time to learn the object names during interaction with adults and without precise supervision. Therefore, following the developmental robotics approach, we develop in the thesis learning approaches for objects, associating their names and corresponding features, inspired by the infants'capabilities, in particular, the ambiguous interaction with humans, inspired by the interaction that occurs between children and parents. The general idea is to use cross-situational learning (finding the common points between different presentations of an object or a feature) and to implement multi-modal concept discovery based on two latent topic discovery approaches: Non Negative Matrix Factorization (NMF) and Latent Dirichlet Association (LDA). This thesis highlights the algorithmic solutions required to be able to perform efficient learning of these word-referent associations from data acquired in a simplified but realistic acquisition setup that made it possible to perform extensive simulations and preliminary experiments in real human-robot interactions. We also gave solutions for the automatic estimation of the number of topics for both NMF and LDA.We finally proposed two active learning strategies, Maximum Reconstruction Error Based Selection (MRES) and Confidence Based Exploration (CBE), to improve the quality and speed of incremental learning by letting the algorithms choose the next learning samples. We compared the behaviors produced by these algorithms and show their common points and differences with those of humans in similar learning situations.
In a first part, we consider the problem of similarity measure learning for two tasks where sequential structure is at stake: (i) the multivariate change-point detection and (ii) the time warping of pairs of time series. The methods generally used to solve these tasks rely on a similarity measure to compare timestamps. Using standard structured prediction methods, we present algorithmically efficient ways for learning. We consider the score as a symbolic representation giving: (i) a complete information about the order of events or notes played and (ii) an approximate idea about the expected shape of the alignment. We propose to learn a classifier for each note using this information. Our learning problem is based onthe optimization of a convex function that takes advantage of the weak supervision and of the sequential structure of data. Our approach is validated through experiments on the task of audio-to-score on real musical data.
The rapid growth in population, combined with the increased mobility of people has created a need for sophisticated identity management systems. For this purpose, biometrics refers to the identification of individuals using behavioral or biological characteristics. The most popular approaches, i.e. fingerprint, iris or face recognition, are all based on computer vision methods. The adoption of deep convolutional networks, enabled by general purpose computing on graphics processing units, made the recent advances incomputer vision possible. These advances have led to drastic improvements for conventional biometric methods, which boosted their adoption in practical settings, and stirred up public debate about these technologies. In this respect, biometric systems providers face many challenges when learning those networks. In this thesis, we consider those challenges from the angle of statistical learning theory, which leads us to propose or sketch practical solutions. First, we answer to the proliferation of papers on similarity learningfor deep neural networks that optimize objective functions that are disconnected with the natural ranking aim sought out in biometrics. Precisely, we introduce the notion of similarity ranking, by highlighting the relationship between bipartite ranking and the requirements for similarities that are well suited to biometric identification. We then extend the theory of bipartite ranking to this new problem, by adapting it to the specificities of pairwise learning, particularly those regarding its computational cost. The thesis tackles all of those three examplesby proposing their careful statistical analysis, as well as practical methods that provide the necessary tools to biometric systems manufacturers to address those issues, without jeopardizing the performance of their algorithms.
This work deals with the confrontation between data-processing tools for natural language processing and the problem arising from automatic treatment to construct sense. The language is always moving and the linguistic descriptions must be ajusted. Our work deals with a process of representation which does not predefines all the knowledge that can be associated to the words. We use the prototype as part of prototype-based language as a tool for representation of linguistic facts, in so far as we think that it can propose an answer for the problem we face. This tool leads us to build simple and ajustable structures of representation to suit to the adjustement dimension of natural languages. Prototype-based language encourage an approach for representation which consists in successive leaps and which improves the quality of the representation that is produced. Futhermore, these processes go together with the 'home-made" but necessary- work of the linguist and his will to describe the way language events work.
The study compares the functioning of non-verbal utterances in German and Kabyle (Berber) using the Zemb's (1978) semantico-logical triad as a theoretical framework, i.e. the theme (what is being talked about), the rheme (what is said about the theme) and the phème (place of articulation of modalisation and negation), applied by Behr and Quintin (1996) and Behr (2013) to categorisation of German non-verbal utterances. We posit that each language has morphosyntactic, contextual and situational means allowing the construction of non-verbal utterances and that these means are more extensive in Kabyle. We also hypothesise that there are unique semantico-logical structures which could be expressed through varied morphosyntactical structures. Finally, we presume that non-verbal utterances express all the modalities; they have morphological and/ or contextual possibilities which locate them within the temporal framework. We have observed, among other results, that the frequency of non-verbal utterances is higher in Kabyle due to grammaticalized predicative structures, except for those depending syntactically on a main sentence, which could be explained by the scrambling-process. At the syntactic level, the pre-/postposition of the rheme in relation to the theme is subject to language specific constraints, i.e. changes in the noun state in Kabyle, the determination and definiteness in German; constraints concerning non-verbal utterances appear in the preference of the rheme-theme order in German. Non-verbal utterances express all modalities; they are located in time by circumstances, by some demonstratives or by the context, and nominalisations as existential rheme express telic and atelic aspectuality.
Multidisciplinary oncology teams meet regularly in tumor board meetings, in which medical experts of all specialties review patient cases to determine the best therapeutic option. Data preparation for these meetings is often conducted manually, which increases the risk of extraction errors and represents a burden for health professionals. The ASIMOV project uses natural language processing and data integration to mine clinical data warehouses and narrative clinical reports to simplify and accelerate data collection, with a focus on temporal information extraction. We address the problem of machine learning in the context of limited annotated training data and non-English languages. We will leverage rule-based systems and machine learning approaches, and develop new methods towards generic approaches.
Energy is fundamental to maintain comfort and it shapes our modern life. With the excess demand for energy, home energy management systems are appearing with time. They aim at reducing or modulating energy consumption while keeping an acceptable level of comfort. Efficient home energy management systems should embed a behavioral representation of a home system, including inhabitants. It establishes relationships between different environmental variables and heterogeneous phenomena present in a home. Therefore, those systems are complex to build and to understand for inhabitants. This was justified as it was nearly impossible to implicate occupants and to create a relation between occupants and energy systems. This concept does create different problems as occupants are detached from the energy system and they don't understand its functionality nor how it is working. To overcome this difficulty this work promotes the concept of “doing with” as it tries to implicate the occupant in the loop with their energy management system. This is where the explanation is needed to allow occupants to discover the knowledge in the energy system and to develop their capacity of understanding how the system is working and why it is recommending different actions. The explanation is the way to discover new knowledge and consequently, to involve occupants. For humans, explanation plays an important role in life. It is one of the main tools for learning and understanding. It is even used in communication and social aspects. People tend to use it besides learning to show their knowledge about a subject to gain the confidence of others or to clarify a situation. But generating explanations is not an easy task. It is one of the ongoing scientific problems from several decades. Explanations have numerous forms, types, and level of clearness. This study is focusing on the causal explanations. As it is the most intuitive form of explanation to be understood by occupants and is adapted to transfer the knowledge from complex systems like energy models. The scientific challenge is how to construct causal explanations for the inhabitants from a flow of observed sensor data.
Recent studies show an increasing proportion of queries with geographic criteria on Web search engines. This part is even bigger on specific corpora like cultural heritage collection (e. G. Travelogues). We admit that the geographic information is composed of three facets: spatial, temporal and thematic. The goal of this thesis is to combine these three facets to support multicriteria searches. This work concerns several fields: Natural Language Processing (NLP), Geographic Information System (GIS), classic Information Retrieval (IR) and Geographic Information Retrieval (GIR). Our first contribution is about an original combination approach of specific indexes. In order to realize this combination, we propose to imitate the homogenization approaches used in classical IR strategies that represent terms with corresponding lemmas. So, our second contribution concerns a generic standardization approach implemented on spatial and temporal information. The last contribution relates to an evaluation framework for GIR systems. Thanks to this framework, we verified and quantified the benefit of combining the different geographic information facets and also have compared several combination approaches.
Business environmental scanning and collective intelligence (VASIC) as proposed by Lesca is a method to help companies tune in to their environment to anticipate opportunities or risks. This method requires collecting information, yet with the development of information technology, employees face a glut of information. To help sustain VASIC, it is necessary to develop tools to manage information overload. In this thesis, we propose a nearness measurement to estimate if two pieces of information are similar and we have created a prototype, called Alhena, based on this measurement. We demonstrate the properties of our measurement and its relevance in the context of VASIC. We also show that the prototype can be used in other fields such as literature, computer science and psychology. This work is multidisciplinary as it covers aspects of business environmental scanning (management science), research information, computer linguistics and mathematics. We focus on a concrete problem in management science to provide a tool that operationalizes computational and mathematical techniques with a goal of providing decision making support (time saving, reading assistance,...).
This thesis presents some contributions in three research domains: case-based reasoning, knowledge discovery and knowledge representation. Case-based reasoning consists in solving new problems by reusing a set of previous problem-solving experiences, called cases. In this thesis, a language is introduced to represent variations between cases. We first show how this language can be used to represent adaptation knowledge and to model the adaptation phase in case-based reasoning. This language is then applied to the task of adaptation knowledge learning. A knowledge discovery process, called CabamakA, is proposed, that learns adaptation knowledge by generalization from a representation of variations between cases. A discussion follows on how to make this knowledge discovery process operational in a knowledge acquisition process. The discussion leads to the proposition of a new approach for adaptation knowledge acquisition, in which the knowledge discovery process is triggered in an opportunistic manner at problem-solving time. The concepts introduced in the thesis are illustrated in the cooking domain through their application in the case-based reasoning system Taaable, that constitutes the application domain of the study.
Our dissertation belongs to a recently initiated line of studies seeking to characterise the advanced English L2 variety. We present an integrated analysis of some semantic, discourse and cross-linguistic factors underlying the use of verb forms by advanced French and Catalan learners of English as a foreign language. Using a corpus of oral picture book narratives, we explore the distribution of tense-aspect morphology in relation to the aspectual class of predicates (the Aspect Hypothesis) and the temporal information they encode in narrative discourse (the Discourse Hypothesis). The use of tense-aspect forms is also considered from the perspective of the so called L2 rhetorical style, the systematic linguistic choices learners make in a given communicative task drawing on their learnt repertoire of L2 devices but also on information selection and organisation patterns unconsciously transferred from their L1. While English, Catalan and French grammaticalise aspectual distinctions, this does not ensure a nativelike use of aspectual marking in English L2. Prototypical predicate/form coalitions in learner production were found to remain strong in the use of tense-aspect morphology with durative (a)telic predicates and to lead to an across-the-board reliance on aspectual marking, often in tension with the plot-advancing role of the predicate. The degree of grammaticalisation of the progressive aspect in learners'L1 seems to interfere with the hypotheses of use concerning the progressive form in English L2. Only a subset of the learners, the most advanced ones, employ tense-aspect forms in a way which is genuinely liberated from the semantic congruence with the predicate, similar to what was observed in English L1. In this case, the progressive has a discourse-specific function and becomes optional when viewpoint information can be retrieved from other elements in the context. Form-function mappings in the domain of tense-aspect morphology remain, nevertheless, more limited or do not match the ones observed in English L1. These findings invite to a reflection on the margins of grammaticalised contrasts, where atypical coalitions arise, and how learners can grasp such peripheral uses in an instructional setting. They also indicate that L2 oral production at the advanced stages remains bound to a way of thinking the world which is the legacy of our L1.
The coastal environment in Normandy is conducive to a convergence of multiple hazards (erosion, marine submersion, flooding by overflowing streams or upwelling of a water table, turbid flooding by runoff, coastal or continental slope movement). This interaction will occur within the slopes and valleys where coastal populations and their activities have tended to become more densified since the 19th century. As part of this thesis, as well as in the ANR RICOCHET program, three study sites were selected at the outlet of the coastal rivers: from Auberville to Pennedepie, from Quiberville to Dieppe and from Criel-sur-Mer to Ault due to significant issues and strong interactions between hydrological and gravitational phenomena.
This study deals with pedestrian detection in high-density crowds from a mono-camera system. We cast the detection problem as a Multiple Classifier System (MCS), composed by two different ensembles of classifiers, the first one based on SVM (SVM-ensemble) and the second one based on CNN (CNN-ensemble), combined relying on the Belief Function Theory (BFT) to exploit their strengths for pixel-wise classification. BFT allows us to take into account the imprecision in addition to the uncertainty value provided by each classifier, which we consider coming from possible errors in the calibration procedure and from pixel neighbor's heterogeneity in the image space. However, scarcity of labeled data for specific dense crowd contexts reflects in the impossibility to obtain robust training and validation sets. By exploiting belief functions directly derived from the classifiers'combination, we propose an evidential Query-by-Committee (QBC) active learning algorithm to automatically select the most informative training samples. On the other side, we explore deep learning techniques by casting the problem as a segmentation task with soft labels, with a fully convolutional network designed to recover small objects thanks to a tailored use of dilated convolutions. Finally, we show that the output map given by the MCS can be employed to perform people counting.
This thesis focuses on entity recognition in documents recognized by OCR, driven by a database. An entity is a homogeneous group of attributes such as an enterprise in a business form described by the name, the address, the contact numbers, etc. or meta-data of a scientific paper representing the title, the authors and their affiliation, etc. Given a database which describes entities by its records and a document which contains one or more entities from this database, we are looking to identify entities in the document using the database. This work is motivated by an industrial application which aims to automate the image document processing, arriving in a continuous stream. We addressed this problem as a matching issue between the document and the database contents. The difficulties of this task are due to the variability of the entity attributes representation in the database and in the document and to the presence of similar attributes in different entities. Added to this are the record redundancy and typing errors in the database, and the alteration of the structure and the content of the document, caused by OCR. To deal with these problems, we opted for a two-step approach: entity resolution and entity recognition. The first step is to link the records referring to the same entity and to synthesize them in an entity model. For this purpose, we proposed a supervised approach based on a combination of several similarity measures between attributes. These measures tolerate character mistakes and take into account the word permutation. The second step aims to match the entities mentioned in documents with the resulting entity model. We proceeded by two different ways, one uses the content matching and the other integrates the structure matching. For the content matching, we proposed two methods: M-EROCS and ERBL. M-EROCS, an improvement / adaptation of a state of the art method, is to match OCR blocks with the entity model based on a score that tolerates the OCR errors and the attribute variability. ERBL is to label the document with the entity attributes and to group these labels into entities. The structure matching is to exploit the structural relationships between the entity labels to correct the mislabeling. The proposed method, called G-ELSE, is based on local structure graph matching with a structural model which is learned for this purpose. This thesis being carried out in collaboration with the ITESOFT-Yooz society, we have experimented all the proposed steps on two administrative corpuses and a third one extracted from the web
This dissertation studies the relationship between the expression of controlled aggressive attitudes and the perception of dominance, based on extracts from televised sessions of the Municipal Council of Montreuil during 2013; a period marked by a lively and hostile political climate. We constituted a corpus of spontaneous speech extracts from the Mayor, Dominique Voynet, and four of her opponents. During subsequent recording sessions, the five speakers were asked to read transcriptions of their own speech extracts in a neutral tone (25 stimuli per speaker). They also participated in a self-evaluation questionnaire that focused on the perception of emotional profiles in their own stimuli. The original and re-read extracts were compared in their prosodic-syntactic structure as well as their temporal and melodic characteristics. We show that: 1) some speakers seem to rely mostly on melodic parameters whereas others primarily use temporal parameters, 2) nevertheless, general trends emerge regarding the speech correlates of hostility and dominance in our corpus, notably: a) discrepancies between the syntactic and the prosodic structure of the extracts, b) reduction or absence of pre-pausal final syllabic lengthening, c) large variations in F0 range on both sides of silent pauses.
Nowadays, more and more data of different kinds is becoming available. Formal concept analysis (FCA) and pattern structures are theoretical frameworks that allow dealing with an arbitrary structured data. But the number of concepts extracted by FCA is typically huge. To deal with this problem one can either simplify the data representation, which can be done by projections of pattern structures, or by introducing constraints to select the most relevant concepts. The manuscript starts with application of FCA to mining important pieces of information from molecular structures. With the growth of dataset size good constraints begin to be essential. For that we explore stability of a concept, a well-founded formal constraint. Finding stable concepts in this dataset allows us finding new possible mutagenetic candidates that can be further interpreted by chemists. However for more complex cases, the simple attribute representation of data is not enough. Correspondingly, we turn to pattern structures that can deal with many different kinds of descriptions. We extend the original formalism of projections to have more freedom in data simplification. We show that this extension is essential for analyzing patient trajectories, describing patients hospitalization histories. Finally, the manuscript ends by an original and very efficient approach that enables to mine stable patterns directly.
In the Web of data, an increasing number of knowledge graphs are concurrently published, edited, and accessed by human and software agents. Their wide adoption makes key the two tasks of matching and mining. First, matching consists in identifying equivalent, more specific, or somewhat similar units within and across knowledge graphs. This task is crucial since concurrent publication and edition may result in coexisting and complementary knowledge graphs. However, this task is challenging because of the inherent heterogeneity of knowledge graphs, e.g., in terms of granularities, vocabularies, and completeness. Motivated by an application in pharmacogenomics, we propose two approaches to match n-ary relationships represented in knowledge graphs: a symbolic rule-based approach and a numeric approach using graph embedding. We experiment on PGxLOD, a knowledge graph that we semi-automatically built by integrating pharmacogenomic relationships from three distinct sources of this domain. Second, mining consists in discovering new and useful knowledge units from knowledge graphs. Their increasing size and combinatorial nature entail scalability issues, which we address in the mining of path patterns. We also propose Concept Annotation, a refinement approach extending Formal Concept Analysis, a mathematical framework that groups entities based on their common attributes. Throughout all our works, we particularly focus on taking advantage of domain knowledge in the form of ontologies that can be associated with knowledge graphs. We show that, when considered, such domain knowledge alleviates heterogeneity and scalability issues in matching and mining approaches.
This thesis work aims at bridging the gap between the fields of Web Semantics and Experimental Particle Physics. Taking as a use case a specific type of physics experiments, namely the irradiation experiments used for assessing the resistance of components to radiation, a domain model, what in Web Semantics is called an ontology, has been created for describing the main concepts underlying the data management of irradiation experiments.
The first part of this thesis aims at exploring deep kernel architectures for complex data. One of the known keys to the success of deep learning algorithms is the ability of neural networks to extract meaningful internal representations. However, the theoretical understanding of why these compositional architectures are so successful remains limited, and deep approaches are almost restricted to vectorial data. The deep kernel architecture we propose consists in replacing the basic neural mappings functions from vector-valued Reproducing Kernel Hilbert Spaces (vv-RKHSs). Although very different at first glance, the two functional spaces are actually very similar, and differ only by the order in which linear/nonlinear functions are applied. Apart from gaining understanding and theoretical control on layers, considering kernel mappings allows for dealing with structured data, both in input and output, broadening the applicability scope of networks. We finally expose works that ensure a finite dimensional parametrization of the model, opening the door to efficient optimization procedures for a wide range of losses. The second part of this thesis investigates alternatives to the sample mean as substitutes to the expectation in the Empirical Risk Minimization (ERM) paradigm. Indeed, ERM implicitly assumes that the empirical mean is a good estimate of the expectation. However, in many practical use cases (e.g. heavy-tailed distribution, presence of outliers, biased training data), this is not the case. The Median-of-Means (MoM) is a robust mean estimator constructed as follows: the original dataset is split into disjoint blocks, empirical means on each block are computed, and the median of these means is finally returned. We propose two extensions of MoM, both to randomized blocks and/or U-statistics, with provable guarantees. The (randomized) MoM minimizers are shown to be robust to outliers, while MoM tournament procedure are extended to the pairwise setting. We close this thesis by proposing an ERM procedure tailored to the sample bias issue.
Today, social networking has considerably changed why people are taking pictures all the time everywhere they go. More than 500 million photos are uploaded and shared every day, along with more than 200 hours of videos every minute. More particularly, with the ubiquity of smartphones, social network users are now taking photos of events in their lives, travels, experiences, etc. and instantly uploading them online. Such public data sharing puts at risk the users' privacy and expose them to a surveillance that is growing at a very rapid rate. Furthermore, new techniques are used today to extract publicly shared data and combine it with other data in ways never before thought possible. However, social networks users do not realize the wealth of information gathered from image data and which could be used to track all their activities at every moment (e.g., the case of cyberstalking). Thus, the aim of this work is to provide a privacy-preserving constraint (de-linkability) to bound the amount of information that can be used to re-identify individuals using online profile information. Firstly, we provide a framework able to quantify the re-identification threat and sanitize multimedia documents to be published and shared. Secondly, we propose a new approach to enrich the profile information of the individuals to protect. Specifically, our approach is able to detect and link users' elementary events using photos (and related metadata) shared within their online social networks. A prototype has been implemented and several experiments have been conducted in this work to validate our different contributions.
The massification of the Internet and computers has changed several aspects of our daily life and the way we apply to a job is not the exception. Since the last 15 years, the researchers in Natural Language Processing have been studying how to improve the performance of recruiters with the help of the e-Recruitment. Several systems have been developed in this field, from the job and applicants search engines to the automatic ranking of applicants. In the last case, most of the developed systems consist in the comparison between the résumés of applicants and a job offer. Only one system makes use of résumés from past selection processes to rank newer applicants. In this thesis we study whether and how we can use the résumés, without having to use past selection processes, to develop new methods for e-Recruitment systems. More specifically, we start with the automatic treatment of a large set of résumés used during real recruitment and selection processes. Then, we analyze and apply different proximity measures to know which are the most adequate to study the résumés of applicants. We introduce, after, an innovative method which consists on the Relevance Feedback and the use of proximity measures over uniquely the résumés to rank applicants. Along this thesis we show that résumés have enough information about the selection processes, in order to rank the applicants. Nonetheless, it is important to choose correctly the proximity measure to use. As well, we present interesting outcomes from the triple comparison between résumés and job offers. The results obtained in this thesis are the basis for a new prototype of an e-Recruitment system and hopefully, the beginning of a new way to create these.
Earliest records of a wheeled chair used to transport a person with disability dates back to the 6m century in China. With the exception of the collapsible X-frame wheelchairs invented in 1933, 1400 years of human scientific evolution has not radically changed the initial wheelchair design. Meanwhile, advancements in computing and the development of artificial intelligence since the mid-1980s has inevitably led to research on Intelligent Wheelchairs. Rather than focusing on improving the underlying design, the core objective of making a wheelchair intelligent is to make it more accessible. Even though the invention of the powered wheelchairs have partially mitigated a user's dependence on other people for their daily routines, some disabilities that affect limb movements, motor or visual coordination, make il impossible for a user to operate a common powered wheelchair. Accessibility can also thus be thought of as the idea, where the wheelchair adapts to the user malady such that he/she is able to utilize its assistive capabilities. While it is certain that intelligent robots are poised to address a growing number of issues in the service and medical care industries, it is important to resolve how humans and users interact with robots in order to accomplish common objectives. Particularly in the assistive intelligent wheelchair domain, preserving a sense of autonomy with the user is required, as individual agency is essential for his/her physical and social well-being. This work thus aims to globally characterize the idea of assistive shared control while particularly devoting the attention to two issues within the intelligent assistive wheelchair domain viz. vision-based assistance and human-aware navigation. Recognizing the fundamental tasks that a wheelchair user may have to execute in indoor environments, we design low­cost vision-based assistance framework for corridor navigation. The framework provides progressive assistance for the tasks of safe corridor following and doorway passing. Evaluation of the framework is carried out on a robotized off-the­shelf wheelchair. From the proposed plug and play design, we infer an adaptive formulation for sharing control between user and robot. Furthermore, keeping in mind that wheelchairs are assistive devices that operate in human environments, it is important to consider the issue of human-awareness within wheelchair mobility. We leverage spatial social conventions from anthropology to surmise wheelchair navigation in human environments. Moreover, we propose a motion strategy that can be embedded on a social robot (such as an intelligent wheelchair) that allows il to equitably approach and join a group of humans in interaction. Based on the lessons learnt from the proposed designs for wheelchair mobility assistance, we can finally mathematically formalize adaptive shared control for assistive motion planning. In closing, we demonstrate this formalism in order to design a general framework for assistive wheelchair navigation in human environments.
Named entity recognition is a crucial discipline of NLP. It is used to extract relations between named entities, which allows the construction of knowledge bases (Surdeanu and Ji, 2014), automatic summary (Nobata et al., 2002) and so on. Our interest in this thesis revolves around structuration phenomena that surround them. We distinguish here two kinds of structural elements in named entities. The first one are recurrent substrings, that we will call the caracteristic affixes of a named entity. The second type of element is tokens with a good discriminative power, which we call trigger tokens of named entities. We will explain here the algorithm we provided to extract such affixes, which we will compare to Morfessor (Creutz and Lagus, 2005b). We will then apply the same algorithm to extract trigger tokens, which we will use for French named entity recognition and postal address extraction. We propose a novel kind of linear tagger cascade which have not been used before for structured named entity recognition, generalising other previous methods that are only able to recognise named entities of a fixed depth or being unable to model certain characteristics of the structure.
This thesis, entitled: ''French as a writing academic language in Algeria. Contrastive study between scientific fields and humanities''addresses the issue of teaching and/or learning foreign languages in Algeria through the characteristics of scientific genre. The aim of this research is to discover if genre retains its stability in the academic writing when it is a question of practice of a foreign language, as well as French. The French language does not exist for itself. It is the language of studying at the Algerian university, and poses, among other factors, an obstacle to success. Based on a heterogeneous corpus which is made up of twelve dissertations presented in Algeria, we would like to contrast, among these writings, the writing of the human and social sciences with the writing of the hard and natural sciences. Adopting an automatic language processing through the software Hyperbase is necessary because the corpus is very large. This tool continues to develop new techniques for scientific research. Also, in order to acquaint ourselves with the context of using French language in Algeria, we conducted a questionnaire survey with Algerian students and teachers at the university. The main result obtained from this research shows that genre is always dominant even in a specific context like the use of a foreign language.
This Ph.D. took the shape of a partnership between the VORTEX team in the computer science research laboratory IRIT and the company Andil, which specializes in software for e-learning. This partnership was concluded around a CIFRE Ph.D. This plan is subsidized by the French state through the ANRT. The Ph.D. student, Angela Bovo, worked in Université Toulouse 1 Capitole. Another partnership was built with the training institute Juriscampus, which gave us access to data from real trainings for our experiments. Our main goal for this project was to improve the possibilities for monitoring students in an e-learning training to keep them from falling behind or giving up. We proposed ways to do such monitoring with classical machine learning methods, with the logs from students'activity as data. We also proposed, using the same data, indicators of students'behaviour. With Andil, we designed and produced a web application called GIGA, already marketed and sold, and well appreciated by training managers, which implements our proposals and served as a basis for first clustering experiments which seem to identify well students who are failing or about to give up. However, our implementations did not get to the point of conclusive results.
This thesis focuses on the phonological aspect of the /R/ in French language spoken in Niamey, the capital of Niger, a Sub-Saharan country of Africa. In Niamey, French coexists with others national and local languages: haousa, songhaï-zarma, touareg, peul, kanuri et arabic. In the proposed work at first we have illustrated a phonetic and phonology classification of rhotics class, then we have classified and analyzed our data. We have analyzed all allophones of /R/ produced by the interviewed speakers. These data show that the largest part of the speaker pronounce a vibrant alveolar [r], followed by a fricative uvular [ʁ], and then by [ɰ], [χ], [ɻ] and [ø]. Furthermore, we have compared our results with other PFC studies conducted all around the francophone word. Additionally, we have focused on fall of /R/ in cluster group, and we concluded that this fall depends on the lexicon, and concerns especially numbers pronunciation (for example, quatre [katR]&gt; [kat]).
Microcirculation refers to the subset of the circulatory system where extracellular gas and fluid exchanges take place. It is composed of arterioles, capillaries and venules. The objectives of this work are to study, understand and identify new iatrogenic etiologies to these microvascular diseases, as well as to evaluate and compare the effectiveness and safety of treatments used in these diseases. We therefore conducted several studies using pharmacovigilance databases, clinical trial data and the literature. This thesis work allowed us to explore the role of drugs in these microvascular pathologies, fields that were poorly studied in the literature yet. This work has allowed us to identify many pharmacological classes whose role was unknown in these diseases. The study of the pharmacological mechanisms underlying these adverse drug reactions also makes it possible to develop new pathophysiological hypotheses underlying these diseases. The treatments used in these different microvascular diseases are currently not specific and important research work still needs to be carried out in order to personalize patient care.
Our thesis focuses on controlled natural languages (CNL) for software engineering. It aims at facilitating the adoption of the business rule approach (BRA) by companies by creating a CNL in order to help business experts in the specification of their business rules. Our solution will allow reducing the semantic gap between business experts and system experts to meet not only the need for mutual understanding between them but also to achieve an automatic transfer of the description of business rules to information systems (IS). The CNL that we have created will also ensure the consistency and the traceability of these rules together with their implementation
Many accidents in transport, industry or healthcare result from a causal chain of events where inadvertent human errors have not been corrected in time. In such socio-technical and dynamic systems where complexity and unpredictability widespread, training teams to risk management in real-life like situations is crucial. This thesis aims to provide a virtual multi-player environment designed for inter-professional team training to risk management. To that end, a method to design risk management interactive and controlled scenario has been described. A communication system, a group decision making system and a team tracing model have been created. They all together enable the virtual team to be free enough to manage the educational situations. These coherent and innovative environment allows us to control the team activity and automate the edition of a personalized, individual and corporate debriefing at the end of a team training session.
The number of connected objects continues to grow to the point that billions of objects are expected in the near future. The approach of this thesis sets up an autonomic management architecture for systems based on connected objects, combining them with other services such as weather services accessible on the Internet. Parameters such as execution time or consumed energy are also considered in order to optimize the choices of actions to be performed and of services used. A concrete prototype was realized in a smart city scenario with connected buses in the investment for future project: S2C2.
False information are multiplying and are spreading quickly on social networks. In this thesis, we analyze the publications from a multimodal point of view between the text and the associated image. Several studies were conducted during this thesis. The first compares several types of media present on social networks and aims to discriminate them automatically. The second one allows the detection and the localization of modifications in an image thanks to the comparison with an old version of this image. Finally, we focused on merged knowledge based on the predictions of other research teams to create a single system.
Description of the principles of the mutual comprehension and the research projects referring itself to it, in the European Union (and in Latin America) and their applications. They are classified: Passive knowledge (receptive competences) and interactive communication. We exploited the results of work of linguistic engineering of Patrice Pognan and Diana Lemay. A total diagram of teaching in conformity with the concept of mutual comprehension is presented, and the list of the international, governmental or not governmental organizations concerned. To conclude a priority list from strategies, methods and tools is established.
This thesis deals with the notion of budget to study problems of complexity (it can be computational complexity, a complex task for an agent, or complexity due to a small amount of data). Indeed, the main goal of current techniques in machine learning is usually to obtain the best accuracy, without worrying about the cost of the task. The concept of budget makes it possible to take into account this parameter while maintaining good performances. We first focus on classification problems with a large number of classes: the complexity in those algorithms can be reduced thanks to the use of decision trees (here learned through budgeted reinforcement learning techniques) or the association of each class with a (binary) code. We then deal with reinforcement learning problems and the discovery of a hierarchy that breaks down a (complex) task into simpler tasks to facilitate learning and generalization. Here, this discovery is done by reducing the cognitive effort of the agent (considered in this work as equivalent to the use of an additional observation).
The first part of the work analyses the set of constraints involved in the exercise of sign language interpreting, as distinguished from those generally observed to apply between spoken languages (including languages syntactically far apart), such as socio-economic constraints, linguistic constraints and, finally, spatial constraints. There follows a cognitive analysis of the interpreting process with reference to Gile's Effort model of simultaneous interpreting (Listening and Analysis Effort, Memory Effort, Production Effort, Effort of Coordination of these three simultaneous activities), with an attempt to envisage transposing its application to sign language. In order to gain better understanding of the constituent mechanisms of the process, initial analysis of the cognitive load of the interpreter in action accords particular attention to the concept of scénarisation (scene-staging) (Séro-Guillaume, 2008). Is this capacity for creating a visual picture from sequential meaning greater or lesser when factors such as the degree of abstraction of the speech, the technicality of its content, a lack of lexical correspondence, the interpreting context (educational setting, conference setting, etc), and the amount of preparation are taken into account? Analysis of the process is based upon a corpus comprising several empirical studies of interpreting into sign language: a semi-experimental study, a naturalistic case study, and an experimental study, as well as on interpreter interviews and a focus group. The observations drawn from all of these studies have enabled cross-referencing of our data and the identification of the relevant elements of our research results in order to advance understanding of the cognitive process of sign language interpreting.
Many natural language processing applications rely on word representations (also called word embeddings) to achieve state-of-the-art results. These numerical representations of the language should encode both syntactic and semantic information to perform well in downstream tasks. However, common models (word2vec, GloVe) use generic corpus like Wikipedia to learn them and they therefore lack specific semantic information. Moreover it requires a large memory space to store them because the number of representations to save can be in the order of a million. I developed dict2vec, a model that uses additional information from online lexical dictionaries when learning word representations. The dict2vec word embeddings perform ∼15% better against the embeddings learned by other models on word semantic similarity tasks. The second part of my work is to reduce the memory size of the embeddings. I developed an architecture based on an autoencoder to transform commonly used real-valued embeddings into binary embeddings, reducing their size in memory by 97% with only a loss of ∼2% in accuracy in downstream NLP tasks.
Our objective is to describe the formal properties of the sentences containing support verbs in French and Malay and to compare them from the syntactic and semantic point of view. We have bracket four tests of recognition of the support verb in order to determine the statue of these three verbs in the studied construction. Our study is structured in six chapters. The second chapter is a general presentation of the Malay language. The third chapter is the study of the verb support “membuat/faire” (to do) in Malay. The forth chapter is a study of the support verb “memberi/donner” (to give) in Malay. The fifth chapter is a study of the support verb “mengambil/prendre” (to take) in Malay. The sixth chapter is a comparative study between the three support verbs membuat/faire (to do), memberi/donner (to give) and mengambil/prendre (to take) in Malay and French. The results obtained have shown that the support verbs in French and Malay share the same general characteristics. These results also enabled us to show that in spite of the universality of the phenomenon, each language has its own mechanism concerning the function of the support verb and the system of determination.
Our work aims to build, from a deep automatic or neuronal analysis method, semantic representations of aspecto-temporal values ​​(associated with linguistic indices) of the French compound past tense.
Bitext alignment is the task of aligning a text in a source language and its translation in the target language. Aligning amounts to finding the translational correspondences between textual units at different levels of granularity. Many practical natural language processing applications rely on bitext alignments to access the rich linguistic knowledge present in a bitext. While the most predominant application for bitexts is statistical machine translation, they are also used in multilingual (and monolingual) lexicography, word sense disambiguation, terminology extraction, computer-aided language learning andtranslation studies, to name a few. Bitext alignment is an arduous task because meaning is not expressed seemingly across languages. Current practices in bitext alignment model the alignment as a hidden variable in the translation process. In order to reduce the complexity of the task, such approaches suppose that a word in the source sentence is aligned to one word at most in the target sentence. However, this over-simplistic assumption results in asymmetric, one-to-many alignments, whereas alignments are typically symmetric and many-to-many. To achieve symmetry, two one-to-many alignments in opposite translation directions are built and combined using a heuristic. In order to use these word alignments in phrase-based translation systems which use phrases instead of words, a heuristic is used to extract phrase pairs that are consistent with the word alignment. We improve the state of the art in several ways using discriminative learning techniques. The interaction between alignment decisions is approximated using stackingtechniques, which allows us to account for a part of the structural dependencies without increasing the complexity. This formulation can be seen as an alignment combination method,in which the union of several input alignments is used to guide the output alignment. Our MaxEnt aligner obtains state of the art results in terms of alignment quality as measured by thealignment error rate, and translation quality as measured by BLEU on large-scale Arabic-English NIST'09 systems. We reformulate the problem in the supervised framework in which we decide for each phrase pair whether we keep it or not in the translation model. This offers a principled way to combine several features to make the procedure more robust to alignment difficulties. We use a simple and effective method, based on oracle decoding,to annotate phrase pairs that are useful for translation. Using machine learning techniques based on positive examples only,these annotations can be used to learn phrase alignment decisions. Using this approach we obtain improvements in BLEU scores for recall-oriented translation models, which are suitable for small training corpora.
The development of a speech recognition system requires the availability of a large amount of resources namely, large corpora of text and speech, a dictionary of pronunciation. Nevertheless, these resources are not available directly for Arabic dialects. As a result, the development of a SRAP for Arabic dialects is fraught with many difficulties, namely the lack of large amounts of resources and the absence of a standard spelling as these dialects are spoken and not written. In this perspective, the work of this thesis is part of the development of a SRAP for the Tunisian dialect. A first part of the contributions consists in developing a variant of CODA (Conventional Orthography for Arabic Dialectal) for the Tunisian dialect. In fact, this convention is designed to provide a detailed description of the guidelines applied to the Tunisian dialect. Given the guidelines of CODA, we have created our corpus TARIC: Corpus of the interaction of the railways of the Tunisian Arab in the field of SNCFT. In addition to these resources, the pronunciation dictionary is indispensable for the development of a peech recognition system. In this regard, in the second part of the contributions, we aim at the creation of a system called conversion(Grapheme-Phonème) G2P which allows to automatically generate this phonetic dictionary. All these resources described before are used to adapt a SRAP for the MSA of the LIUM laboratory to the Tunisian dialect in the field of SNCFT. The evaluation of our system gave rise to WER of 22.6% on the test set.
This accident is the result of an unprecedented discrepancy between the state of the art of drilling engineers'heuristics and that of pollution response engineers. Deepwater Horizon is in this sense a case of engineering facing extreme situation, as defined by Guarnieri and Travadel. First, we propose to return to the overall concept of accident by means of an in-depth linguistic analysis presenting the semantic spaces in which the accident takes place. This makes it possible to enrich its "core meaning" and broaden the shared acceptance of its definition. Then, we bring that the literature review must be systematically supported by algorithmic assistance to process the data taking into account the available volume, the heterogeneity of the sources and the requirements of quality and relevance standards. In fact, more than eight hundred scientific articles mentioning this accident have been published to date and some twenty investigation reports, constituting our research material, have been produced. Our method demonstrates the limitations of accident models when dealing with a case like Deepwater Horizon and the urgent need to look for an appropriate way to formalize knowledge. As a result, the use of upper-level ontologies should be encouraged. The DOLCE ontology has shown its great interest in formalizing knowledge about this accident and especially in elucidating very accurately a decision-making process at a critical moment of the intervention. The population, the creation of instances, is the heart of the exploitation of ontology and its main interest, but the process is still largely manual and not without mistakes. This thesis proposes a partial answer to this problem by an original NER algorithm for the automatic population of an ontology. Finally, the study of accidents involves determining the causes and examining "socially constructed facts". This thesis presents the original plans of a "semantic pipeline" built with a series of algorithms that extract the expressed causality in a document and produce a graph that represents the "causal path" underlying the document. It is significant for scientific or industrial research to highlight the reasoning behind the findings of the investigation team. As a conclusion, this thesis is a work of a fitter, an architect, which offers both a prime insight into the Deepwater Horizon case and proposes the data drilling, an original method and means to address an event, in order to uncover answers from the research material for questions that had previously escaped understanding.
Our thesis falls under the enunciation theory following work of Antoine Culioli and Jean-Pierre Desclés. Any utterance requires a enunciator. According to whether the enunciator refers or not to the enunciative situation several types of enouncements appear. We compared two types of utterances described by Aristotle with the representations established by Jean-Pierre Desclés in Grammar Applicative and Cognitive. In both cases, a change is represented between two stable situations. In both cases, a movement spreading itself is described in its non-accomplishment. We proposed to understand the concept of entelecheia used by Aristotle when he defines the movement, starting from the concept of unaccomplished process described by Jean-Pierre Desclés. Whereas the utterance of a change requires to validate two stable states for a single temporal reference, the utterance of a movement spreading itself requires that the enonciator constates indeed a movement that he knew possible. A temporal parameter remains common to these two types of utterances. The writing of a data-processing program made it possible automatically to extract the values of this temporal parameter and to distinguish the various utterances from a digitized textual corpus. The data-processing application implements theoretical description in a software component usable by other programs to temporally organize a text for the automatic treatment of the natural languages.
The automatic processing of the Quechua language (APQL) lacks an electronic dictionary of French­ Quechua verbs. However, any NLP project requires this important linguistic resource. The realization of such a resource couId also open new perspectives on different domains such as multilingual access to information, distance learning,inthe areas of annotation /indexing of documents, spelling correction and eventually in machine translation. The first challenge was the choice of the French dictionary which would be used as our basic reference. Among the numerous French dictionaries, there are very few which are presented in an electronic format, and even less that may be used as an open source. Among the latter, we found the dictionary Les verbes français (LVF}, of Jean Dubois and Françoise Dubois-Charlier, edited by Larousse en 1997. It is a remarkably complete dictionary. It contains 25 610 verbal senses and with open source license. It is entirely compatible with the Nooj platform. That's why we have chosen this dictionary to be the one to translate into Quechua. However, this task faces a considerable obstacle: the Quechua lexicon of simple verbs contains around 1,500 entries. How to match 25,610 French verbal senses with only 1,500 Quechua verbs? Are we condemned to produce many polysemies? For example, in LVF, we have 27 verbal senses of the verb "tourner" to turn; should we translate them all by the Quechua verb muyuy to turn? Or, can we make use of a particular and remarkable Quechua strategy that may allow us to face thischallenge: the generation of new verbs by suffix derivation? As a first step, we have inventoried ail the Quechua suffixes that make possible to obtain a derived verbal form which behaves as if it was a simple verb. This set of suffixes, which we call IPS_DRV, contains 27 elements. Thus each Quechua verb, transitive or intransitive, gives rise to at least 27 derived verbs. Next, we need to formalize the paradigms and grammars that will allow us to obtain derivations compatible with the morphology of the language. This was done with the help of the NooJ platform. The application of these grammars allowed us to obtain 40,500 conjugable atomic linguistic units (CALU) out of 1,500 simple Quechua verbs. This encouraging first result allows us to hope to get a favorable solution to our project of translation of the 25,000 verbal senses of French into Quechua. In order to obtain the translation of these CALUs, we first needed to know the modalities of enunciation that each IPS have and transmits to the verbal radical when it is agglutinated to it. Each suffix can have several modalities of enunciation. We have obtained an inventory of them from the corpus, our own experience and some recordings obtained in fieldwork. We constructed an indexed table containing all of these modalities. Next, we used NooJ operators to program grammars that present automatic translation into a glossed form of enunciation modalities. Finally, we developed an algorithm that allowed us to obtain the reciprocal translation from French to Quechua of more than 8,500 Verbal senses of Level 3 and a number of verbal senses of Levels 4 and 5.
Social Media, and Twitter in particular, has become a privileged source of information for journalists in recent years. Most of them monitor Twitter, in the search for newsworthy stories. This thesis aims to investigate and to quantify the effect of this technological change on editorial decisions. Second, we study different types of algorithms to automatically discover tweets that relate to the same stories. We test several vector representations of tweets, looking at both text and text-image representations, Third, we design a new method to group together Twitter events and media events. Finally, we design an econometric instrument to identify a causal effect of the popularity of an event on Twitter on its coverage by traditional media. We show that the popularity of a story on Twitter does have an effect on the number of articles devoted to it by traditional media, with an increase of about 1 article per 1000 additional tweets.
This PhD thesis focuses on the automatic speech synthesis field, and more specifically on unit selection. A deep analysis and a diagnosis of the unit selection algorithm (lattice search algorithm) is provided. The importance of the solution optimality is discussed and a new unit selection implementation based on a A* algorithm is presented. Three cost function enhancements are also presented. The first one is a new way – in the target cost – to minimize important spectral differences by selecting sequences of candidate units that minimize a mean cost instead of an absolute one. This cost is tested on a phonemic duration distance but can be applied to others. Our second proposition is a target sub-cost addressing intonation that is based on coefficients extracted through a generalized version of Fujisaki's command-response model. This model features gamma functions modeling F0 called atoms. Finally, our third contribution concerns a penalty system that aims at enhancing the concatenation cost. It penalizes units in function of classes defining the risk a concatenation artifact occurs when concatenating on a phone of this class. This system is different to others in the literature in that it is tempered by a fuzzy function that allows to soften penalties for units presenting low concatenation costs.
The need for personalized recommendations is motivated by the overabundance of online information, products, social connections. This typically tackled by recommender systems (RS) that learn users interests from past recorded activities. Another context where recommendation is desirable is when estimating the relevance of an item requires complex reasoning based on experience. Machine learning techniques are good candidates to simulate experience with large amounts of data. The present thesis focuses on the cold-start context in recommendation, i.e. the situation where either a new user desires recommendations or a brand-new item is to be recommended. Since no past interaction is available, RSs have to base their reasoning on side descriptions to form recommendations. The problem of choosing an optimization algorithm in a portfolio can be cast as a recommendation problem. We propose a two components system combining a per-instance algorithm selector and a sequential scheduler to reduce the optimization cost of a brand-new problem instance and mitigate the risk of optimization failure. Both components are trained with past data to simulate experience, and alternatively optimized to enforce their cooperation. The final system won the Open Algorithm Challenge 2017.Automatic job-applicant matching (JAM) has recently received considerable attention in the recommendation community for applications in online recruitment platforms. We develop specific natural language (NL) modeling techniques and combine them with standard recommendation procedures to leverage past user interactions and the textual descriptions of job positions. The appropriateness of various RSs on applications similar to the JAM problem are discussed.
In this thesis, we study two problems of machine learning: (I) community detection and (II) adaptive matching. I) It is well-known that many networks exhibit a community structure. Finding those communities helps us understand and exploit general networks. In this thesis we focus on community detection using so-called spectral methods based on the eigenvectors of carefully chosen matrices. We analyse their performance on artificially generated benchmark graphs. Instead of the classical Stochastic Block Model (which does not allow for much degree-heterogeneity), we consider a Degree-Corrected Stochastic Block Model (DC-SBM) with weighted vertices, that is able to generate a wide class of degree sequences. We consider this model in both a dense and sparse regime. In the dense regime, we show that an algorithm based on a suitably normalized adjacency matrix correctly classifies all but a vanishing fraction of the nodes. In the sparse regime, we show that the availability of only a small amount of information entails the existence of an information-theoretic threshold below which no algorithm performs better than random guess. On the positive side, we show that an algorithm based on the non-backtracking matrix works all the way down to the detectability threshold in the sparse regime, showing the robustness of the algorithm. This follows after a precise characterization of the non-backtracking spectrum of sparse DC-SBM's. We further perform tests on well-known real networks. II) Online two-sided matching markets such as Q&amp;A forums and online labour platforms critically rely on the ability to propose adequate matches based on imperfect knowledge of the two parties to be matched. We develop a model of a task / server matching system for (efficient) platform operation in the presence of such uncertainty. For this model, we give a necessary and sufficient condition for an incoming stream of tasks to be manageable by the system. We further identify a so-called back-pressure policy under which the throughput that the system can handle is optimized. We show that this policy achieves strictly larger throughput than a natural greedy policy. Finally, we validate our model and confirm our theoretical findings with experiments based on user-contributed content on an online platform.
Approximate Numerical Expressions (ANE) are imprecise linguistic expressions implying numerical values, illustrated by "about 100". We first focus on ANE interpretation, both in its human and computational aspects. In a second step, we proposed two interpretation models, based on the same principle of a compromise between the cognitive salience of the endpoints and their distance to the ANE reference value, formalized by Pareto frontiers. The experimental validation of the models, based on real data, show that they offer better performances than existing models. We also show the relevance of the fuzzy model by implementing it in the framework of flexible database queries. We then show, by the mean of an empirical study, that the semantic context has little effect on the collected intervals. Finally, we focus on the additions and products of ANE, for instance to assess the area of a room whose walls are "about 10" and "about 20 meters" long. We conducted an empirical study whose results indicate that the imprecisions associated with the operands are not taken into account during the calculations.
We set out to collect and combine large datasets enabling 1) the study of the spatial, temporal, linguistic and network dependencies of socioeconomic inequalities and 2) the inference of socioeconomic status (SES) from these multimodal signals. The study of these questions is important, as much is still unclear about the root causes of SES inequalities and the deployment of ML/DL solutions to pinpoint them is still very much in its infancy.
This thesis analyses the problem of building well-founded domain ontologies for reasoning and decision support purposes.
This thesis in computer science focuses on the problem of capitalizing analysis processes of elearning traces within the Learning Analytics (LA) community. The aim is to allow these analysis processes to be shared, adapted and reused. This prevents them from being shared, but also from being simply reused outside their original contexts, even if the new contexts are similar. The objective of this thesis is to provide models and methods for the capitalisation of analysis processes of elearning traces, as well as to assist the various actors involved in the analysis, particularly during the reuse phase. Our second contribution responds to the first barrier related to the technical dependence of current analysis processes and their sharing. We propose a meta-model that allows to describe the analysis processes independently of the analysis tools. This meta-model formalizes the description of the operations used in the analysis processes, the processes themselves and the traces used, in order to avoid the technical constraints caused by these tools. This formalism, common to the analysis processes, also makes it possible to consider their sharing. It has been implemented and evaluated in one of our prototypes. Our third contribution deals with the second lock on the reuse of analysis processes. We propose an ontological framework for analysis processes, which allows semantic elements to be directly introduced, in a structured way, during the description of analysis processes. This narrative approach thus enriches the previous formalism and makes it possible to satisfy the properties of understanding, adaptation and reuse necessary for capitalisation. This ontological approach was implemented and evaluated in another of our prototypes. Finally, our last contribution responds to the last lock identified and concerns new assistances to actors, in particular a new method of researching analysis processes, based on our previous proposals. We also use the semantic network underlying this ontological modeling to strengthen assistance to actors by providing them with inspection and understanding tools during the research. This assistance was implemented in one of our prototypes, and empirically evaluated.
Located in the heart of the Indian Ocean and having a surface area of 2 040 square kilometers, the Republic of Mauritius is an insular country that includes four islands: Mauritius (the main island), Rodrigues, Agaléga and Saint-Brandon. Altogether, its nearly 1.3 million inhabitants make up a plurilingual speech community in which more than 10 languages are spoken. In spite of this linguistic richness, shaped by its history conducive to language contact, and the plurilingual language skills of Mauritians, the most commonly spoken language at home, by 86.50% of the Mauritian population (according to the 2011's Housing and Population Census), is the Mauritian Creole: a French-based creole language. Studies on the Mauritian Creole began during the period of colonization. In the nineteenth century, Baissac (1880) proposed a *study on the creole patois in Mauritius* and in the twentieth century, after independence in March 1968, Baker (1972) published a book on its linguistic description. The only contemporary grammar available is that of Police-Michel et al. (2012). Also, various PhD research works studied and laid emphasis on the syntactic categories of the Mauritian Creole, particularly on the noun phrase (Alleesaib, 2012), the verb (Henri, 2010) and the adverb (Hassamal, 2017). However, among all of these studies, none of them took a real interest in the field of Natural Language Processing (NLP). David (2019) has accomplished a research work on the Part-Of-Speech (POS) tagging of the Mauritian Creole, but overall there is a lack of scientific literature in the automatic processing of this resource-poor language. Based on these research works on the Mauritian Creole's syntax and in a view of harnessing the methods and NLP tools developed by David (2019), this thesis aims at building and exploiting a Treebank (a syntactically annotated written corpus) for the Mauritian Creole, such as the French Treebank for French (Abeillé et al., 2003) and the Penn Treebank for English (Taylor et al., 2003). The methodological framework of this work will be organized around 5 main phases. The first phase will focus on the constitution, standardization, and structuring of a written electronic corpus. During the second phase, the unavailable computer tools required for this work will be newly developed while existing ones will be optimized. The third phase will focus on the annotation of the corpus, based on a coherent and structured annotation scheme. Then, experiments will be conducted during the fourth phase. Finally, the fifth phase will evaluate the quality of the annotations produced and the performance of the tools developed. Ultimately, the goal of this thesis is to achieve automatic syntactic parsing in Mauritian Creole, while providing this language with all the elements which are necessary for its automatic processing, especially under a syntactic analysis perspective.
The objective of this work is to introduce a new robust approach to treat the problem of finding the correct answer to a question. Our first contribution is the design and implementation of a robust representation model for information. The aim is to represent the structural information of sentences of documents and questions structural information. This representation is composed of typed groups of words (typed segments) and relations between these groups. This model has been evaluated on several corpus (written, oral, web) and achieved good resultats, which proves his robustness. Our second contribution consisted is the design of a re-ranking method of a set of the candidate answers output by the question-answering system. This re-ranking method is based on the structural information representation. The general idea is to compare a question and a passage from where a candidate answer was extracted, and to compute a similarity score by using a modified edit distance we proposed. Our re-ranking method has been evaluated on the data of several evaluation campaigns. The results are quite good on long and complex questions. These results show the interest of our method: our approach is quite adapted to treat long question, whatever the type of the data. The re-ranker has been officially evaluated on the 2010 edition of the Quaero evaluation campaign, with positives results.
We are interested in the recovery and prediction of multiple time series from partially observed and/or aggregate data. After examining kriging from spatio-temporal statistics and a hybrid method based on the clustering of individuals, we propose a general framework based on nonnegative matrix factorization. This frameworks takes advantage of the intrisic correlation between the multivariate time series to greatly reduce the dimension of the parameter space. Once the estimation problem is formalized in the nonnegative matrix factorization framework, two extensions are proposed to improve the standard approach. The first extension takes into account the individual temporal autocorrelation of each of the time series. This increases the precision of the time series recovery. The second extension adds a regression layer into nonnegative matrix factorization. This allows exogenous variables that are known to be linked with electricity consumption to be used in estimation, hence makes the factors obtained by the method to be more interpretable, and also increases the recovery precision. We produce a theoretical analysis on the framework which concerns the identifiability of the model and the convergence of the algorithms that are proposed. The performance of proposed methods to recover and forecast time series is tested on several multivariate electricity consumption datasets at different aggregation level.
This pluridisciplinary thesis, at the interface between computer science and historical linguistics, aims to computationally model phonetic changes, based on cognate identification and prediction. Phonological differences between cognates partly capture the divergences between the phonetic evolution of related languages. This work is structured in three main steps. The first one consists in the creation of an etymological lexical database large enough to allow us to train neural models of the phonetic correspondences between languages via a cognate prediction task. The design and training of such networks is the second step of the work. We rely on sequence-to-sequence neural architectures, similar what constitutes today the state of the art in machine translation, yet adapted to the specificities of this work, notably the small amount of available training data. The third step is dedicated to the validation and analysis of the results produced by our neural models, in collaboration with historical linguists.
Named entities have been the topic of many researches during the 90's. For instance, knowing that a text contains the words “Google” and “Youtube” can be relevant but being able to link them and detect an acquisition relation can be more interesting (Google has bought Youtube in 2006). Our work is focusing on two different aspects: to define a finer perimeter around the relation between named entities definition, with linguistic aspect in mind, and to explore new techniques that make use of linguists in order to build a relation between named entities recognition system.
They play an important role in many successful applications of Natural Language Processing, such as Automatic Speech Recognition, Machine Translation and Information Extraction. In this way, language models predict a word based on its n-1 previous words. In spite of their prevalence, conventional n-gram based language models still suffer from several limitations that could be intuitively overcome by consulting human expert knowledge. One critical limitation is that, ignoring all linguistic properties, they treat each word as one discrete symbol with no relation with the others. This kind of model is constructed based on the count of n-grams in training data. Therefore, the pertinence of these models is conditioned only on the characteristics of the training text (its quantity, its representation of the content in terms of theme, date). These representations and the associated objective function (the likelihood of the training data) are jointly learned using a multi-layer neural network architecture. This approach has shown significant and consistent improvements when applied to automatic speech recognition and statistical machine translation tasks. A major difficulty with the continuous space neural network based approach remains the computational burden, which does not scale well to the massive corpora that are nowadays available. For this reason, the first contribution of this dissertation is the definition of a neural architecture based on a tree representation of the output vocabulary, namely Structured OUtput Layer (SOUL), which makes them well suited for large scale frameworks. The second contribution is to provide several insightful analyses on their performances, their pros and cons, their induced word space representation. Finally, the third contribution is the successful adoption of the continuous space neural network into a machine translation framework. New translation models are proposed and reported to achieve significant improvements over state-of-the-art baseline systems.
This thesis proposes a study the linguistic relationships between nominal predicates and causative verbs the names of emotions and causative verbs according to the methodology established in the ANR-DFG Emolex project (www.emolex.eu).
In the era of digitilization, and with the emergence of several semantic Web applications, many new knowledge bases (KBs) are available on the Web. These KBs contain (named) entities and facts about these entities. They also contain the semantic classes of these entities and their mutual links. In addition, multiple KBs could be interconnected by their entities, forming the core of the linked data web. A distinctive feature of these KBs is that they contain millions to trillions of unreliable RDF triples. This uncertainty has multiple causes. It can result from the integration of data sources with various levels of intrinsic reliability or it can be caused by some considerations to preserve confidentiality. Furthermore, it may be due to factors related to the lack of information, the limits of measuring equipment or the evolution of information. The goal of this thesis is to improve the usability of modern systems aiming at exploiting uncertain KBs. In particular, this work proposes cooperative and intelligent techniques that could help the user in his decision-making when his query returns unsatisfactory results in terms of quantity or reliability. The approach proposed to handle this problem is query-driven and offers a two fold advantage: (i) it provides the user with a rich explanation of the failure of his query by identifying the MFS (Minimal Failing Sub-queries) and (ii) it allows the computation of alternative queries called XSS (maXimal Succeeding Sub-queries), semantically close to the initial query, with non-empty answers. Moreover, from a user's point of view, this solution offers a high level of flexibility given that several degrees of uncertainty can be simultaneously considered. All our propositions have been validated with a set of experiments on different uncertain and large-scale knowledge bases (WatDiv and LUBM). We have also used several Triplestores to conduct our tests.
Lexicon-Grammar tables, whose development was initiated by Gross (1975), are a very rich syntactic lexicon for the French language. They cover various lexical categories such as verbs, nouns, adjectives and adverbs. This linguistic database is nevertheless not directly usable by computer programs, as it is incomplete and lacks consistency. To use these tables, we must make explicit the essential features appearing in each one of them. In addition, many features must be renamed for consistency sake. Our aim is to adapt the tables, so as to make them usable in various Natural Language Processing (NLP) applications, in particular parsing. We describe the problems we encountered and the approaches we followed to enable their integration into a parser. We propose LGExtract, a generic tool for generating a syntactic lexicon for NLP from the Lexicon-Grammar tables. It relies on a global table in which we added the missing features and on a single extraction script including all operations related to each property to be performed for all tables We also present LGLex, the new generated lexicon of French verbs, predicative nouns, frozen expressions and adverbs. Then, we describe how we converted the verbs and predicatives nouns of this lexicon into the Alexina framework, that is the one of the Lefff lexicon (Lexique des Formes Fléchies du Français)(Sagot, 2010), a freely available and large-coverage morphological and syntactic lexicon for French. This enables its integration in the FRMG parser (French MetaGrammar) (Thomasset et de La Clergerie, 2005), a large-coverage deep parser for French, based on Tree-Adjoining Grammars (TAG), that usually relies on the Lefff. This conversion step consists in extracting the syntactic information encoded in Lexicon-Grammar tables. We describe the linguistic basis of this conversion process, and the resulting lexicon. We evaluate the FRMG parser on the reference corpus of the evaluation campaign for French parsers Passage (Produire des Annotations Syntaxiques à Grande Échelle)(Hamon et al., 2008), by comparing its Lefff-based version to our version relying on the converted Lexicon-Grammar tables.
Surveillance systems are important tools for law enforcement agencies for fighting crimes. Surveillance control rooms have two main duties: live monitoring the surveillance areas, and crime solving by investigating the archives. To support these difficult tasks, several significant solutions from the research and market fields have been proposed. However, the lack of generic and precise models for video content representation make the building of fully automated intelligent video analysis and description system a challenging task. Furthermore, the application domain still shows a big gap between the research field and the real practical needs, it also shows a lack between these real needs and the on-market video analytics tools. Consequently, in conventional surveillance systems, live monitoring and investigating the archives still rely mostly on human operators. This thesis proposes a novel approach for textual describing important contents in videos surveillance scenes, based on new generic context-free "VSSD ontology", with focus on two objects interactions. The proposed ontology presents a new generic flexible and extensible ontology dedicated for video surveillance scenes description. While analysing and understanding variety of video scenes, our approach introduces many new concepts and methods concerning mediation and action at a distant, abstraction in the description, and a new manner of categorizing the scenes. It introduces a new heuristic way to discriminate between deformable and non-deformable objects in the scenes.
Spoken dialog systems enable users to interact with computer systems via natural dialogs, as they would with human beings. These systems are deployed into a wide range of application fields from commercial services to tutorial or information services. However, the communication skills of such systems are bounded by their spoken language understanding abilities. Our work focus on the spoken language understanding module which links the automatic speech recognition module and the dialog manager. DBN-based models allow to infer and then to compose semantic frame-based tree structures from speech transcriptions. First, we developed a semantic knowledge source covering the domain of our experimental corpus (MEDIA, a French corpus for tourism information and hotel booking). The inference process extracts all possible sub-trees according to lower level information and composes the hypothesized branches into a single utterance-span tree. This work investigates a stochastic process for generating and composing semantic frames using DBNs. The proposed approach offers a convenient way to automatically derive semantic annotations of speech utterances based on a complete frame hierarchical structure.
The amount and complexity of data generated by information systems keep increasing in Warehouses. The domain of Business Intelligence (BI) aims at providing methods and tools to better help users in retrieving those data. Looking for new information could be a tedious task, because business users try to reduce their work overload. To tackle this problem, Enterprise Search is a field that has emerged in the last few years, and that takes into consideration the different corporate data sources as well as sources available to the public (e.g. World Wide Web pages). However, corporate retrieval systems nowadays still suffer from information overload. We believe that such systems would benefit from Natural Language (NL) approaches combined with Q&amp;A techniques. Indeed, NL interfaces allow users to search new information in their own terms, and thus obtain precise answers instead of turning to a plethora of documents. Major challenges for designing such a system are to interface different applications and their underlying query languages on the one hand, and to support users' vocabulary and to be easily configured for new application domains on the other hand. This thesis outlines an end-to-end Q&amp;A framework for corporate use-cases that can be configured in different settings. In traditional BI systems, user-preferences are usually not taken into account, nor are their specific contextual situations. State-of-the art systems in this field, Soda and Safe do not compute search results on the basis of users' situation. This thesis introduces a more personalized approach, which better speaks to end-users' situations. Our main experimentation, in this case, works as a search interface, which displays search results on a dashboard that usually takes the form of charts, fact tables, and thumbnails of unstructured documents. Depending on users' initial queries, recommendations for alternatives are also displayed, so as to reduce response time of the overall system. This process is often seen as a kind of prediction model. Our work contributes to the following: first, an architecture, implemented with parallel algorithms, that leverages different data sources, namely structured and unstructured document repositories through an extensible Q&amp;A framework, and this framework can be easily configured for distinct corporate settings; secondly, a constraint-matching-based translation approach, which replaces a pivot language with a conceptual model and leads to more personalized multidimensional queries; thirdly, a set of NL patterns for translating BI questions in structured queries that can be easily configured in specific settings. In addition, we have implemented an iPhone/iPad™ application and an HTML front-end that demonstrate the feasibility of the various approaches developed through a series of evaluation metrics for the core component and scenario of the Q&amp;A framework. To this end, we elaborate on a range of gold-standard queries that can be used as a basis for evaluating retrieval systems in this area, and show that our system behave similarly as the well-known WolframAlpha™ system, depending on the evaluation settings.
This thesis studies the problem of machine learning under budget constraints, in particular we propose to focus on the cost of the information used by the system to predict accurately. Most methods in machine learning usually defines the quality as the performance (e.g accuracy) on the task at hand, but ignores the cost of the model itself: for instance, the number of examples and/or labels needed during learning, the memory used, or the number of features required to predict at test-time. We present three models that learn to predict under such constraint, i.e that learn a strategy to gather only the necessary information in order to predict well but with a small cost. We rely on representation learning techniques, along with recurrent neural networks architecture and gradient descent algorithms for learning. In the last part of the thesis, we propose to study the problem of active-learning, where one aims at constraining the amount of labels used to train a model. We present our work for a novel approach of the problem using meta-learning, with an instantiation using bi-directional recurrent neural networks.
One of the major research areas in computer vision is visual surveillance. The scientific challenge in this area includes the implementation of automatic systems for obtaining detailed information about the behavior of individuals and groups. Particularly, detection of abnormal individual movements requires sophisticated image analysis. This thesis focuses on the problem of the abnormal events detection, including feature descriptor design characterizing the movement information and one-class kernel-based classification methods. In this thesis, three different image features have been proposed: (i) global optical flow features, (ii) histograms of optical flow orientations (HOFO) descriptor and (iii) covariance matrix (COV) descriptor. Based on these proposed descriptors, one-class support vector machines (SVM) are proposed in order to detect abnormal events. Two online strategies of one-class SVM are proposed:
The notion of metric plays a key role in machine learning problems, such as classification, clustering and ranking. Learning metrics from training data in order to make them adapted to the task at hand has attracted a growing interest in the past years. This research field, known as metric learning, usually aims at finding the best parameters for a given metric under some constraints from the data. The learned metric is used in a machine learning algorithm in hopes of improving performance. Most of the metric learning algorithms focus on learning the parameters of Mahalanobis distances for feature vectors. Current state of the art methods scale well for datasets of significant size. On the other hand, the more complex topic of multivariate time series has received only limited attention, despite the omnipresence of this type of data in applications. An important part of the research on time series is based on the dynamic time warping (DTW) computing the optimal alignment between two time series. The current state of metric learning suffers from some significant limitations which we aim to address in this thesis. The most important one is probably the lack of theoretical guarantees for the learned metric and its performance for classification. The theory of (ℰ, ϓ, τ)-good similarity functions has been one of the first results relating the properties of a similarity to its classification performance. A second limitation in metric learning comes from the fact that most methods work with metrics that enforce distance properties, which are computationally expensive and often not justified. In this thesis, we address these limitations through two main contributions. The first one is a novel general framework for jointly learning a similarity function and a linear classifier. This formulation is inspired from the (ℰ, ϓ, τ)-good theory, providing a link between the similarity and the linear classifier. It is also convex for a broad range of similarity functions and regularizers. We derive two equivalent generalization bounds through the frameworks of algorithmic robustness and uniform convergence using the Rademacher complexity, proving the good theoretical properties of our framework. Our second contribution is a method for learning similarity functions based on DTW for multivariate time series classification. The formulation is convex and makes use of the(ℰ, ϓ, τ)-good framework for relating the performance of the metric to that of its associated linear classifier. Using uniform stability arguments, we prove the consistency of the learned similarity leading to the derivation of a generalization bound.
The purpose of this research is to investigate a linguistic phenomenon that has been developing in Tunisia in the Social Networks cyber-territory. This study has two main aims: (1) to examine the Tunisian language of the capital's young people, and its realization in Arabish/Arabizi (Latin script). (2) To build an Arabish corpus with different annotation levels (such as: tokenization, Part-of-Speech tagging) exploiting NLP approaches.
An electronic grammar is one of the most important elements in the natural language processing. Since traditional manual grammar development is a time-consuming and labor-intensive task, many efforts for automatic grammar development have been taken during last décades. Automatic grammar development means that a System extracts a grammar from a Treebank. Full-scale syntactic tags and morphological analysis in Sejong Korean Treebank allow us to extract syntactic features automatically and to develop FB-LTAG. During extraction experiments, we modify thé Treebank to improve extracted grammars and extract five différent types of grammars; four lexicalized grammars and one feature-based lexicalized grammar. Extracted grammars are evaluated by ils size, ils coverage and ils average ambiguity. The number of tree schemata is not stabilized at thé end of the extraction process, which seems to indicate that thé size of a Treebank is not enough to reach thé convergence of extracted grammars. However, the number of tree schemata appeared at least twice in the Treebank is nearly stabilized at the end of the extraction process, and the number of superior grammars (the ones which are extracted after thé modification of Treebank) is also much stabilized than inferior grammars. We also evaluate extracted grammars using LLP2 and our extracting System using other Treebank. Finally, we compare extracted grammars with the one of Han et al.
In the last few decades, many scientists were concerned with the fast extinction of languages. Faced with this alarming decline of the world's linguistic heritage, action is urgently needed to enable fieldwork linguists, at least, to document languages by providing them innovative collection tools and to enable them to describe these languages. Machine assistance might be interesting to help them in such a task. This is what we propose in this work, focusing on three pillars of the linguistic fieldwork: collection, transcription and analysis. Recordings are essential, since they are the source material, the starting point of the descriptive work. Speech recording is also a valuable object for the documentation of the language. LIG-AIKUMA proposes a range of different speech collection modes (recording, respeaking, translation and elicitation) and offers the possibility to share recordings between users. Through these modes, parallel corpora are built such as "under-resourced speech - well-resourced speech", "speech - image", "speech - video", which are also of a great interest for speech technologies, especially for unsupervised learning. We propose to use automatic techniques to help the fieldwork linguist to take advantage of all his speech collection. Along these lines, automatic speech recognition (ASR) is a way to produce transcripts of the recordings, with a decent quality. Once the transcripts are obtained (and corrected), the linguist can analyze his data. In order to analyze the whole collection collected, we consider the use of forced alignment methods. We demonstrate that such techniques can lead to fine evaluation of linguistic features. In return, we show that modeling specific features may lead to improvements of the ASR systems.
Asthma results from multiple genetic and environmental factors and from interactions between these factors. The global aim of this thesis was to propose gene-gene and gene-environment interaction strategies of analysis to identify new genes associated with the risk of asthma and atopy. The tests of interactions between genetic variants are applied to the selected gene pairs. These analyzes, conducted in three family studies (n = 3,244), identified an interaction between two genes (ADGRV1 and DNAH5) involved in ciliary mobility, an emerging mechanism in asthma. A meta-analysis of GWAS of the time-to-asthma onset, conducted in nine studies (n = 19,348), identified a new locus associated with the risk of asthma (16q12) and confirmed four more. Five of these nine studies included environmental factor data on early-life tobacco smoke (ELTS) exposure.
Given the amount of Arabic textual information available on the web, developing effective Information Retrieval Systems (IRS) has become essential to retrieve relevant information. Most of the current Arabic SRIs are based on the bag-of-words representation, where documents are indexed using surface words, roots or stems. Accordingly, we propose four contributions to improve Arabic content representation, indexing, and retrieval. The first contribution consists of representing Arabic documents using Multi-Word Terms (MWTs). The latter is motivated by the fact that MWTs are more precise representational units and less ambiguous than isolated SWTs. Hence, we propose a hybrid method to extract Arabic MWTs, which combines linguistic and statistical filtering of MWT candidates. The linguistic filter uses POS tagging to identify MWTs candidates that fit a set of syntactic patterns and handles the problem of MWTs variation. Then, the statistical filter rank MWT candidate using our proposed association measure that combines contextual information and both termhood and unithood measures. In the second contribution, we explore and evaluate several IR models for ranking documents using both SWTs and MWTs. Additionally, we investigate a wide range of proximity-based IR models for Arabic IR. Then, we introduce a formal condition that IR models should satisfy to deal adequately with term dependencies. The third contribution consists of a method based on Distributed Representation of Word vectors, namely Word Embedding (WE), for Arabic IR. It relies on incorporating WE semantic similarities into existing probabilistic IR models in order to deal with term mismatch. The last contribution is a method to incorporate WE similarity into Pseud-Relevance Feedback PRF for Arabic Information Retrieval. The main idea is to select expansion terms using their distribution in the set of top pseudo-relevant documents along with their similarity to the original query terms. The experimental validation of all the proposed contributions is performed using standard Arabic TREC 2002/2001 collection.
Probabilistic graphical models encode the hidden dependencies between random variables for data modelling. Parameter estimation is a crucial and necessary part of handling such probabilistic models. These very general models have been used in plenty of fields such as computer vision, signal processing, natural language processing and many more. We mostly focused on log-supermodular models, which is a specific part of exponential family distributions, where the potential function is assumed to be negative of a submodular function. This property will be very handy for the maximum a posteriori and parameter learning estimations. Despite the seemed restriction of the models of interest, they cover a broad part of exponential families, since there are plenty of functions that are submodular, e.g., graph cuts, entropy and others. It is well known that probabilistic treatment is a challenging way for most of all models, however we were able to tackle some of the challenges at least approximately. In this manuscript, we exploit the perturb-and-MAP ideas for the partition function approximation and thus an efficient parameter learning. Moreover, the problem can be also interpreted as a structure learning task, where each estimated parameter or weight represents the importance of the corresponding term. We propose a way of approximate parameter estimation and inference for the models where exact learning and inference is intractable in general case due to the partition function calculation complexity. The first part of the thesis is dedicated to theoretical guarantees. Given the log-supermodular models, we take advantage of the efficient minimization property related to submodularity. Introducing and comparing two existing upper bounds of the partition function, we are able to demonstrate their relation by proving a theoretical result. We introduce an approach for missing data as a natural subroutine of probabilistic modelling. It appears that we can apply a stochastic technique over proposed perturb-and-map approximation approach and still maintain convergence while make it faster in practice. The second main contribution of this thesis is an efficient and scalable generalization of the parameter learning approach. In this section we develop new algorithms to perform the parameter estimation for various loss functions, different levels of supervision and we also work on the scalability. In particular, working with mostly graph cuts, we were able to incorporate various acceleration techniques. As a third contribution we deal with a general problem of learning continuous signals. In this part, we focus on the sparse graphical models representations. We consider common sparsity-inducing regularizers as prior-based potentials. The proposed denoising techniques do not require choosing any precise regularizer in advance. To perform sparse representation learning, community often use symmetric losses as l1, but we propose to parameterize the loss and learn the weight of each loss component from the data. This is feasible via the approach we proposed in the previous sections. For all sides of the parameter estimation mentioned above we performed computational experiments to approve the idea or compare with existing benchmarks, and demonstrate its performance in practice.
The three main results are the following. First, we study how argumentation graphs are obtained from knowledge bases expressed in existential rules, the structural properties of such graphs and show several insights as to how their generation can be improved. Second, we propose a framework that generates an argumentation graph with a special feature called sets of attacking arguments instead of the regular binary attack relation and show how it improves upon the current state-of-the-art using an empirical analysis. In the latter, we set up the foundation theory for semantics that rank arguments in argumentation graphs with sets of attacking arguments.
The expansion of the radio and the development of new standards enrich the diversity and the amount of data carried by the broadcast radio waves. It becomes wise to develop a search engine that has the capacity to make these accessible as do the search engines on the internet like Google. Such an engine can offer many possibilities. In that vein, the SurfOnHertz project, which was launched in 2010 and ended in 2013, aimed to develop a browser that is capable of indexing audio streams of all radio stations. This indexing would result, among others, in the detection of keywords in the audio streams, the detection of commercials, the classification of musical genres. The browser once developed would become the first search engine of its kind to address the broadcast content. Taking up such a challenge requires to have a device to capture all the stations being broadcasted in the geographical area concerned, demodulate them and transmit the audio contents to the indexing engine. Thus, the work of this thesis aim to provide digital architectures carried on a SDR platform for extracting, demodulating, and making available the audio content of each broadcast stations in the geographic area of the receiver. Before the large number of radio standards which exist today, the thesis focuses FM and DRM30 standards. However the proposed methodologies are extensible to other standards. The choice of this type of component is justified by the great opportunities it offers in terms of parallelism of treatments, mastery of available resources, and embeddability. The development of algorithms was done for the sake of minimizing the amount of the used calculations blocks. Moreover, many implementations have been performed on a Stratix II technology which has limited resources compared to those of the FPGAs available today on the market. This attests to the viability of the presented algorithms. The proposed algorithms thus operate simultaneous extraction of all radio channels when the stations can only occupy uniformly spaced locations like FM in Western Europe, and also for standards of which the distribution of stations in the spectrum seems rather random as the DRM30. Another part of the discussion focuses on the means of simultaneously demodulating it.
The applicant get insights from Parawitz'grounds (2009), dialogical Logic of Lorenzen (1961) ludic by Girard (2001) and proof theoretical semantics by Francez (2015). It enables to draw a distinction between hitherto similar expressions 'little'/ 'a little'or 'each'/ 'every'. The method should be developed as a module of the wide scale grammatical and logical parser for French Grail. Such an analysis and tools are of course especially welcome for the automated analysis of argumentation, argumentative dialogues and debates.
In this thesis, we propose a new approach WCUM (Web Content and Usage Mining based approach) for linking content analysis to usage analysis of a website to better understand the general behavior of the web site visitors. To mitigate the problem of determination of the number of clusters on rows and columns, we suggest to generalize the use of some indices originally proposed to evaluate the partitions obtained by clustering algorithms to evaluate bipartitions obtained by simultaneous clustering algorithms. To evaluate the performance of these indices on data with biclusters structure, we proposed an algorithm for generating artificial data to perform simulations and validate the results. Experiments on artificial data as well as on real data were realized to estimate the efficiency of the proposed approach.
Extracting Meaningful substructures from graphs has always been a key part in graph studies. In machine learning frameworks, supervised or unsupervised, as well as in theoretical graph analysis, finding dense subgraphs and specific decompositions is primordial in many social and biological applications among many others. In this thesis we aim at studying graph degeneracy, starting from a theoretical point of view, and building upon our results to find the most suited decompositions for the tasks at hand. Hence the first part of the thesis we work on structural results in graphs with bounded edge admissibility, proving that such graphs can be reconstructed by aggregating graphs with almost-bounded-edge-degree. We also provide computational complexity guarantees for the different degeneracy decompositions, i.e. if they are NP-complete or polynomial, depending on the length of the paths on which the given degeneracy is defined. In the second part we unify the degeneracy and admissibility frameworks based on degree and connectivity. Within those frameworks we pick the most expressive, on the one hand, and computationally efficient on the other hand, namely the 1-edge-connectivity degeneracy, to experiment on standard degeneracy tasks, such as finding influential spreaders. Following the previous results that proved to perform poorly we go back to using the k-core but plugging it in a supervised framework, i.e. graph kernels. Thus providing a general framework named core-kernel, we use the k-core decomposition as a preprocessing step for the kernel and apply the latter on every subgraph obtained by the decomposition for comparison. We are able to achieve state-of-the-art performance on graph classification for a small computational cost trade-off. Finally we design a novel degree degeneracy framework for hypergraphs and simultaneously on bipartite graphs as they are hypergraphs incidence graph. This decomposition is then applied directly to pretrained neural network architectures as they induce bipartite graphs and use the coreness of the neurons to re-initialize the neural network weights. This framework not only outperforms state-of-the-art initialization techniques but is also applicable to any pair of layers convolutional and linear thus being applicable however needed to any type of architecture.
Corpora are the main material of computer linguistics and natural language processing. Web resources contain lots of noise (menus, ads, etc.). Filtering boilerplate and repetitive data requires a large-scale manual cleaning by the researcher. This thesis presents an automatic system that construct web corpus with a low level of noise. The system is evaluated in terms of the efficacy of noise filtering and of computing time. Our experiments, made on four languages, are evaluated using our own gold standard corpus. We compare our method with three methods dealing with the same problem, Nutch, BootCat and JusText. The performance of our system is better as regards the extraction quality, even if for computing time, Nutch and BootCat dominate.
In using Modeling and Simulation for the system Verification &amp; Validation activities, often the difficulty is finding and implementing consistent abstractions to model the system being simulated with respect to the simulation requirements. A proposition for the unified design and implementation of modeling abstractions consistent with the simulation objectives based on the computer science, control and system engineering concepts is presented. It addresses two fundamental problems of fidelity in simulation, namely, for a given system specification and some properties of interest, how to extract modeling abstractions to define a simulation product architecture and how far does the behaviour of the simulation model represents the system specification. A general notion of this simulation fidelity, both architectural and behavioural, in system verification and validation is explained in the established notions of the experimental frame and discussed in the context of modeling abstractions and inclusion relations. A semi-formal ontology based domain model approach to build and define the simulation product architecture is proposed with a real industrial scale study. A formal approach based on game theoretic quantitative system refinement notions is proposed for different class of system and simulation models with a prototype tool development and case studies. Challenges in research and implementation of this formal and semi-formal fidelity framework especially in an industrial context are discussed.
It is not possible for a science computing system to process a text when sequences, like words or sentences, are not annotated. However, to date, no system has been able to automatically produce a perfect annotation of a text. This report poses the folowing question; which is the better natural language processing system: a system designed to integrate imperfect annotations in its reasoning process or a system designed to work with perfect annotation but dealing with imperfect annotations? To answer this, we have proposed a probabilistic inference model based on Bayesian Networks (BN), a formalism well adapted to reasoning from imperfect data. We have worked on the resolution of the anaphoric pronoun "it" and validate our model in evaluating two BN on different corpora: a BN dedicated to the resolution of the impersonal pronoun recognition problem and a BN dealing with the choice of he antecedent problem.
Stimulated by the heavy use of smartphones, the joint use of textual and spatial data in space-textual objects (e.g., tweets, Flickr photos, POI reviews) became the mainstay of many applications, such as crisis management, tourist assistance or the finding of places of interest. We propose to leverage geographic contexts and distributional semantics to resolve the semantic location prediction task. Our work consists of two main contributions: (1) improving word embeddings which can be combined to construct object representations using spatial word distributions; (2) exploiting deep neural networks to perform semantic matching between tweets and POIs. Regarding the improvement of text representations, we propose to regularize word embeddings that can be combined to construct object representations. One based on a spatial partitioning method using the k-means algorithm, and the other one based on a probabilistic partitioning using a kernel density estimation (KDE). Unlike existing architectures, our approach is based on joint learning of local and global interactions between tweet-POI pairs. According to the proposed model, the exact matching signals of the local word-to-word interactions are corrected by a spatial damping factor.
This thesis studies the structuring and exploration of news collections. While its main focus is on natural language processing and multimedia retrieval, it also deals with social studies through the study of the production of news and ergonomy through the conduct of user tests. Hyperlinking consists in automatically finding relevant links between multimedia segments. We apply this concept to whole news collections, resulting in the creation of a hypergraph, and study the topological properties and their influence on the explorability of the resulting structure. In this thesis, we provide improvements beyond the state of the art along three main {axes:} a structuring of news collections by means of mutli-sources and multimodal graphs based on the creation of inter-document links, its association with a large diversity of links allowing to represent the variety of interests that different users may have, and a typing of the created links in order to make the nature of the relation between two documents explicit. Extensive user studies confirm the interest of the methods developped in this thesis.
The web technology is in an on going growth, and a huge volume of data is generated in the social web, where users would exchange a variety of information. In addition to the fact that social web text may be rich of information, the writers are often guided by provoked sentiments reflected in their writings. Based on that concept, locating sentiment in a text can play an important role for information extraction. The purpose of this thesis is to improve the book search and recommendation quality of the Open Edition's multilingual Books platform. The Books platform also offers additional information through users generated information (e.g. book reviews) connected to the books and rich in emotions expressed in the users' writings. Therefore, the previous analysis, concerning locating sentiment in a text for information extraction, plays an important role in this thesis, and can serve the purpose of quality improvement concerning book search, using the shared users generated information. Accordingly, we choose to follow a main path in this thesis to combine sentiment analysis (SA) and information retrieval (IR) fields, for the purpose of improving the quality of book search. Two objectives are summarised in the following, which serve the main purpose of the thesis in the IR quality improvement using SA: • An approach for SA prediction, easily applicable on different languages, low cost in time and annotated data. • New approaches for book search quality improvement, based on SA employment in information filtering, retrieving and classifying
Languages in Malaysia are dying in an alarming rate. As of today, 15 languages are in danger while two languages are extinct. One of the methods to save languages is by documenting languages, but it is a tedious task when performed manually. Automatic Speech Recognition (ASR) system could be a tool to help speed up the process of documenting speeches from the native speakers. However, building ASR systems for a target language requires a large amount of training data as current state-of-the-art techniques are based on empirical approach. The main aim of this thesis is to investigate the effects of using data from closely-related languages to build ASR for low-resource languages in Malaysia. Past studies have shown that cross-lingual and multilingual methods could improve performance of low-resource ASR. In this thesis, we try to answer several questions concerning these approaches: How do we know which language is beneficial for our low-resource language? How does the relationship between source and target languages influence speech recognition performance? We study the effects of using data from Malay, a local dominant language which is close to Iban, for developing Iban ASR under different resource constraints. We have proposed several approaches to adapt Malay data to obtain pronunciation and acoustic models for Iban speech. It was based on bootstrapping techniques for improving Malay data to match Iban pronunciations. To increase the performance of low-resource acoustic models we explored two acoustic modelling techniques, the Subspace Gaussian Mixture Models (SGMM) and Deep Neural Networks (DNN). Results show that using Malay data is beneficial for increasing the performance of Iban ASR. We also tested SGMM and DNN to improve low-resource non-native ASR. We proposed a fine merging strategy for obtaining an optimal multi-accent SGMM. In addition, we developed an accent-specific DNN using native speech data. After applying both methods, we obtained significant improvements in ASR accuracy. From our study, we observe that using SGMM and DNN for cross-lingual strategy is effective when training data is very limited.
This research main aim is to study reference and reference disturbance concepts, according to the dynamic and interlocutory aspects of the reference dialogical process. Starting from a critical review of the literature related to the theoretico-methodological approach of the reference process, our first study aim to examine the actual approach of this phenomena according to structural, actional, and representational aspects of verbal interaction. Our results lead us to the development of a new model, qualitatively more sensitive. Its heuristic potential is tested in additional works, henceforth aiming to study referential disturbance in verbal interaction accomplished with schizophrenics. Results obtained from two studies providing criterions giving us the possibility to characterize, in a dynamic way, the expression and emergence contexts of reference disturbance. In addition, within the massive modularity theory framework, our results lead us to build an interpretation of the cognitive processes which are implicated. Furthermore, results provided by a complementary study shows that reference disturbance can be considered as a relatively independent phenomena from symptomatological intensity. Emphasizing the interest of a dynamic approach of troubles, the contribution of our results is in relation with the objectives of the pragmatic approach in psychopathology, providing trails to the renewal of mental disorders classifications and also, to a clinic of cognitive efficiency.
In 2015, the number of new cases of breast cancer in France is 54,000. The survival rate after 5 years of cancer diagnosis is 89%. If the modern treatments allow to save lives, some are difficult to bear. Many clinical research projects have therefore focused on quality of life (QoL), which refers to the perception that patients have on their diseases and their treatments. QoL is an evaluation method of alternative clinical criterion for assessing the advantages and disadvantages of treatments for the patient and the health system. In this thesis, we will focus on the patients stories in social media dealing with their health. The aim is to better understand their perception of QoL. This new mode of communication is very popular among patients because it is associated with a great freedom of speech, induced by the anonymity provided by these websites. The originality of this thesis is to use and extend social media mining methods for the French language. The main contributions of this work are: (1) construction of a patient/doctor vocabulary; (2) detection of topics discussed by patients; (3) analysis of the feelings of messages posted by patients and (4) combinaison of the different contributions to quantify patients discourse. Firstly, we used the patient's texts to construct a patient/doctor vocabulary, specific to the field of breast cancer, by collecting various types of non-experts'expressions related to the disease, linking them to the biomedical terms used by health care professionals. We combined several methods of the literature based on linguistic and statistical approaches. To evaluate the relationships, we used automatic and manual validations. Then, we transformed the constructed resource into human-readable format and machine-readable format by creating a SKOS ontology, which is integrated into the BioPortal platform. Secondly, we used and extended literature methods to detect the different topics discussed by patients in social media and to relate them to the functional and symptomatic dimensions of the QoL questionnaires (EORTC QLQ-C30 and EORTC QLQ-BR23). In order to detect the topics discussed by patients, we applied the unsupervised learning LDA model with relevant preprocessing. Then, we applied a customized Jaccard coefficient to automatically compute the similarity distance between the topics detected with LDA and the items in the auto-questionnaires. Thus, we detected new emerging topics from social media that could be used to complete actual QoL questionnaires. This work confirms that social media can be an important source of information for the study of the QoL in the field of cancer. Thirdly, we focused on the extraction of sentiments (polarity and emotions). For this, we evaluated different methods and resources for the classification of feelings in French. These experiments aim to determine useful characteristics in the classification of feelings for different types of texts, including texts from health forums. Finally, we used the different methods proposed in this thesis to quantify the topics and feelings identified in the health social media. In general, this work has opened promising perspectives on various tasks of social media analysis for the French language and in particular the study of the QoL of patients from the health forums.
In this thesis, we focus on bridging the semantic gap between the documents and queries representations, hence improve the matching performance. We propose two models that integrate relational semantics into the distributed representations: a) an offline model that combines two types of pre-trained representations to obtain a hybrid representation of the document; b) an online model that jointly learns distributed representations of documents, concepts and words. To better integrate relational semantics from knowledge resources, we propose two approaches to inject these relational constraints, one based on the regularization of the objective function, the other based on instances in the training text. Our neural model relies on: a) a representation of raw-data that models the relational semantics of text by jointly considering objects and relations expressed in a knowledge resource, and b) an end-to-end neural architecture that learns the query-document relevance by leveraging the distributional and relational semantics of documents and queries.
Development and spread of connected devices, in particular smartphones, requires the implementation of authentication methods. In this thesis, the evaluation of operationnal biometric systems has been studied, and an implementation is presented. A second contribution studies the quality estimation of speech samples, in order to predict recognition performances.
This thesis is part of the deep learning applied to spoken language understanding. Until now, this task was performed through a pipeline of components implementing, for example, a speech recognition system, then different natural language processing, before involving a language understanding system on enriched automatic transcriptions. Recently, work in the field of speech recognition has shown that it is possible to produce a sequence of words directly from the acoustic signal. First, we present a state of the art describing the principles of deep learning, speech recognition, and speech understanding. Then, we describe the contributions made along three main axes. We propose a first system answering the problematic posed and apply it to a task of named entities recognition. Then, we propose a transfer learning strategy guided by a curriculum learning approach. This strategy is based on the generic knowledge learned to improve the performance of a neural system on a semantic concept extraction task. Then, we perform an analysis of the errors produced by our approach, while studying the functioning of the proposed neural architecture. Finally, we set up a confidence measure to evaluate the reliability of a hypothesis produced by our system.
This thesis explores the usage of Abstract Categorial Grammars (ACG) for Natural Language Generation (NLG) in an industrial context. While NLG system based on linguistic theories have a long history, they are not prominent in industry, which, for the sake of simplicity and efficiency, usually prefer more ``pragmatic" methods. This study shows that recent advances in computational linguistics allow to conciliate the requirements of soundness and efficiency, by using ACG to build the main elements of a production grade NLG framework (document planner and microplanner), with performance comparable to existing, less advanced methods used in industry
This research project aims at better understanding the valuation of green assets as well as the financial risk embedded in sustainable investment strategies. To achieve this, I will use machine learning algorithms, especially text mining and natural language processing in order to label green assets from brown ones and supervised algorithms to build a sustainable asset pricing model.
The motor capacities (motor skills)'evaluation is an essential activity for movement analysis. This activity aims is to quantify the human's motor performance to be able to follow-up and control the evolution of the patient's pathology thus allowing an adapted treatment. The physiotherapists need accurate tools able to measure this performance. They developed their own tools based on observations and normalized exercises. This activity can be supported and enhanced by the technological advances. A category of motion tracking tools exists to track and record those movements. Their use in motor evaluation system could refine the therapist's evaluations and increase their reproducibility. To insure the correct development and use of such tools it is necessary to answer the following question: “what are the development stakes and criteria related to a system for measure and evaluation of motor capacities?” This thesis work refined this question into the 3 following research axis: “how to measure motor capacities?”, “how to analyze and communicate the result?”, “how to integrate this system in the medical practice?” For each of those axis the key criteria for development were investigated and contributions are presented. To illustrate those criteria a case study was conducted: the instrumentation, with new motion capture technologies, of an assessment protocol for motor capacities also called MFM (The Motor Function Measure).
The aim of the National Cancer Registry (NCR) in Luxembourg is to collect data about cancer and the quality of cancer treatment. To obtain high quality data that can be compared with other registries or countries, the NCR follows international coding standards and rules, such as the International Classification of Diseases for Oncology (ICD-O). These standards are extensive and complex, which complicates the data collection process. The operators, i.e. the people in charge of this process, are often confronted with situations where data is missing or contradictory, preventing the application of the provided guidelines. To assist in their effort, the coding experts of the NCR answer coding questions asked by operators. This assistance.is time consuming for experts. To help reduce this burden on experts and to facilitate the operators' task, this project aims at implementing a coding assistant that would answer coding questions. From a scientific point of view, this thesis tackles the problem of extracting the information from a set of data sources under a given set of rules and guidelines. Case-based reasoning has been chosen as the method for solving this problem given its similarity with the reasoning process of the coding experts. The method designed to solve this problem relies on arguments provided by coding experts in the context of previously solved problems. This document presents how these arguments are used to identify similar problems and to explain the computed solution to both operators and coding experts. A preliminary evaluation has assessed the designed method and has highlighted key areas to improve. While this work focused on cancer registries and medical coding, this method could be generalized to other domains.
Medical sector is a dynamic domain that requires continuous improvement of its business processes and assistance to the actors involved. This research focuses on the medical treatment process requiring prosthesis implantation. The specificity of such a process is that it makes in connection two lifecycles belonging to medical and engineering domains respectively. This implies several collaborative actions between stake holders from heterogeneous disciplines. However, several problems of communication and knowledge sharing may occur because of the variety of semantic used and the specific business practices in each domain. To do so, a conceptual framework is proposed for the analysis of links between the disease (medical domain) and the prosthesis (engineering domain) lifecycles. Based on this analysis, a semantic ontology model for medical domain is defined as part of a global knowledge-based PLM approach proposition. The application of the proposition is demonstrated through an implementation of useful function in the AUDROS PLM software.
This thesis aims at illustrating the realization and the use of a model for anaphora resolution in oral dialogs in the field of computational linguistics. We don't pretend having discovered the most successful solution for the problem of anaphora in discourse. However, we propose that the study and analysis of anaphoric structures needs the use of complex formal theories such as government and binding. Oral language has it's own logic, but standard forms of language appears largely in it
Natural language processing and more particularly automatic understanding of documents aims to propose methods for extracting relevant information from them. The most effective approaches today use supervised machine learning approaches and very large amounts of manually annotated examples. This thesis topic proposes to answer two problems: 1) how to generate synthetic data?
The main objective of this thesis is to propose a recommendation system allowing retailers to improve their assortments of products distributed through numerous stores. In this context, the problem addressed is the assortment planning which consists in eliciting the best products, e.g., those with the highest turnover. To this end, we first propose a comparison of assortment planning with the pragmatic methods which are commonly used in the industry and the state of the art. This comparison highlights the problem of cross-functionality of the knowledge used today to improve the assortment. To overcome this problem, we propose knowledge structures specific to mass distribution. Thanks to these structures, an Agile assortment optimisation method that can be integrated into a continuous improvement process is formalised. This method makes possible to integrate human expertise, which we deem essential, in the various levers currently adopted. To underline the modularity of our approach, we then propose a semantic analysis of the stores which, in addition to improving the accuracy of our simulations, allows us to define a new axis of assortment improvement. This analysis is based on our proposals both for domain ontologies which are specific to each brand and on semantic similarity measures. Finally, to perfect our method and go further in the exploitation of those structures, we propose a semantic analysis of the consumers who are the final targets of the assortment. This second semantic analysis allows us to bring new knowledge to retailers and new constraints on assortments. In parallel to these scientific contributions, different applications have been developed to highlight the interoperability of our contributions with concepts specific to different types of retailers (e.g. Food, DIY...). These applications are presented in the manuscript within the limits of respect for confidentiality and intellectual property.
Pronoun resolution is the process in which an anaphoric pronoun is linked to its antecedent. In a normal situation, humans do not experience much cognitive effort due to this process. However, automatic systems perform far from human accuracy, despite the efforts made by the Natural Language Processing community. Experimental research in the field of psycholinguistics has shown that during pronoun resolution many linguistic factors are taken into account by speakers. An important question is thus how much influence each of these factors has and how the factors interact with each-other. A second question is how linguistic theories about pronoun resolution can incorporate all relevant factors. In this thesis, we propose a new approach to answer these questions: computational simulation of the cognitive load of pronoun resolution. On the other hand, robust computational systems can be run on uncontrolled data such as eye movement corpora and thus provide an alternative to hand-constructed experimental material. First, we simulated the cognitive load of pronouns by learning the magnitude of impact of various factors on corpus data. Second, we tested whether concepts from Information Theory were relevant to predict the cognitive load of pronoun resolution. Finally, we evaluated a theoretical model of pronoun resolution on a corpus enriched with eye movement data. Our research shows that multiple factors play a role in pronoun resolution and that their influence can be estimated on corpus data. We also demonstrate that the concepts of Information Theory play a role in pronoun resolution. We conclude that the evaluation of hypotheses on corpus data enriched with cognitive data ---- such as eye movement data --- play an important role in the development and evaluation of theories.
The work of this thesis focuses on the consequences of digital technology development on research practice in the humanities in the broad sense and particularly in history. The introduction of digital technology disrupts historical research practices by making available to the researcher a large volume of digitized sources as well as numerous analysis and writing tools. These new capacities of research allow the discipline to adopt new approaches and renew certain points of view, but they also raise methodological and epistemological questions. Given this observation, we have chosen to study in more detail the impact of information retrieval tools, digital libraries and search engines on historical research activity. These systems offer access to a large volume of historical documents but they depend on computer processes that are mostly invisible to users and acting as black boxes. The main objective of this work is to give users the means to observe and understand these processes in order to allow them to integrate their side effects in a suitable methodology. In order to better position our object of study, we propose a conceptual framework based on the notion of digital resource. Based on this conceptual framework, we propose an analysis of digital libraries and historical sources search engines according to each context. These indicators are then crossed with the functioning of the system, in its contexts of production and execution, to reveal the potential methodological biases. Following these analyzes, we propose a reinvestment of these results in the form of a software tool dedicated to teaching a critical approach to online information retrieval for student in history. This work is evaluated by an experimental approach. This prototype has been the subject of several experimental phases related to its development, the evaluation of these features and its impact on practice in a training context.
In this thesis, we explore inertial-based gesture recognition on Smartphones, where gestures holding a semantic value are drawn in the air with the device in hand. In our research, speed and delay constraints required by an application are critical, leading us to the choice of neural-based models. Thus, our work focuses on metric learning between gesture sample signatures using the "Siamese" architecture (Siamese Neural Network, SNN), which aims at modelling semantic relations between classes to extract discriminative features, applied to the MultiLayer Perceptron. Indeed, after a preprocessing step where the data is filtered and normalised spatially and temporally, the SNN is trained from sets of samples, composed of similar and dissimilar examples, to compute a higher-level representation of the gesture, where features are collinear for similar gestures, and orthogonal for dissimilar ones. While the original model already works for classification, multiple mathematical problems which can impair its learning capabilities are identified. Consequently, as opposed to the classical similar or dissimilar pair; or reference, similar and dissimilar sample triplet input set selection strategies, we propose to include samples from every available dissimilar classes, resulting in a better structuring of the output space. Furthermore, the notion of polar sine enables a redefinition of the angular problem by maximising a normalised volume induced by the outputs of the reference and dissimilar samples, which effectively results in a Supervised Non-Linear Independent Component Analysis. Finally, we assess the unexplored potential of the Siamese network and its higher-level representation for novelty and error detection and rejection. To summarise, the proposed SNN allows for supervised non-linear similarity metric learning, which extracts discriminative features, improving both inertial gesture classification and rejection.
In this thesis, we focus on the automatic recognition and translation of the speech of Arabic and dialectal videos. The statistical approaches proposed in the literature for automatic speech recognition are language independent and they are applicable to modern standard Arabic. However, this language presents some characteristics that we need to take into consideration in order to boost the performance of the speech recognition system. Among these characteristics we can mention the absence of short vowels in the text, which makes their training by the acoustic model difficult. We proposed several approaches to acoustic and/or language modeling in order to better recognize the Arabic speech. In the Arab world, modern standard Arabic is not the mother tongue, that is why daily conversations are carried out with dialect, an Arabic inspired from modern standard Arabic, but not only. We worked on the adaptation of the speech recognition system developed for the modern standard Arabic to the Algerian dialect, which is one of the most difficult variants of the Arabic language to recognize by automatic speech recognition systems. This is mainly due to the borrowed words from other languages, the code-switching and the lack of resources. Our approach to overcome all these problems is to take advantage from oral and textual data of other languages that have an impact on the dialect in order to train the required models for dialect speech recognition. The resulting text from Arabic speech recognition system was then used for machine translation. As a starting point, we conducted a comparative study between the phrase based approach and the neural approach used in machine translation. Then, we adapted these two approaches to translate the code-switched text. Our study focused on the mix of Arabic and English in a parallel corpus extracted from official documents of the United Nations. In order to prevent the error propagation in the pipeline system, we worked on the adaptation of the vocabulary of the automatic speech recognition system and on the proposition of a new model that directly transforms a speech signal in language A into a sequence of words in another language B.
Making sense of textual data is an essential requirement in order to make computers understand our language. To extract actionable information from text, we need to represent it by means of descriptors before using knowledge discovery techniques. The goal of this thesis is to shed light into heterogeneous representations of words and how to leverage them while addressing their implicit sparse nature. First, we propose a hypergraph network model that holds heterogeneous linguistic data in a single unified model. In other words, we introduce a model that represents words by means of different linguistic properties and links them together accordingto said properties. Our proposition differs to other types of linguistic networks in that we aim to provide a general structure that can hold several types of descriptive text features, instead of a single one as in most representations. This representationmay be used to analyze the inherent properties of language from different points of view, or to be the departing point of an applied NLP task pipeline. Secondly, we employ feature fusion techniques to provide a final single enriched representation that exploits the heterogeneous nature of the model and alleviates the sparseness of each representation. These types of techniques are regularly used exclusively to combine multimedia data. In our approach, we consider different text representations as distinct sources of information which can be enriched by themselves. This approach has not been explored before, to the best of our knowledge. Thirdly, we propose an algorithm that exploits the characteristics of the network to identify and group semantically related words by exploiting the real-world properties of the networks. In contrast with similar methods that are also based on the structure of the network, our algorithm reduces the number of required parameters and more importantly, allows for the use of either lexical or syntactic networks to discover said groups of words, instead of the singletype of features usually employed. We focus on two different natural language processing tasks: Word Sense Induction and Disambiguation (WSI/WSD), and Named Entity Recognition (NER). In total, we test our propositions on four different open-access datasets. Specifically, our experiments are twofold: first, we show that using fusion-enriched heterogeneous features, coming from our proposed linguistic network, we outperform the performance of single features' systems and other basic baselines. While based on previous work, we improve it by obtaining better overall performance and reducing the number of parameters needed. Contrary to other similar resources, insteadof just storing its part of speech tag and its dependency relations, we also take into account the constituency-tree information of each word analyzed. The hope is for this resource to be used on future developments without the need to compile suchresource from zero.
In recent years, Deep Learning techniques have swept the state-of-the-art of many applications of Machine Learning, becoming the new standard approach for them. The architectures issued from these techniques have been used for transfer learning, which extended the power of deep models to tasks that did not have enough data to fully train them from scratch. This thesis'subject of study is the representation spaces created by deep architectures. First, we study properties inherent to them, with particular interest in dimensionality redundancy and precision of their features. Our findings reveal a strong degree of robustness, pointing the path to simple and powerful compression schemes. Then, we focus on refining these representations. We choose to adopt a cross-modal multi-task problem, and design a loss function capable of taking advantage of data coming from multiple modalities, while also taking into account different tasks associated to the same dataset. In order to correctly balance these losses, we also we develop a new sampling scheme that only takes into account examples contributing to the learning phase, i.e. those having a positive loss. Finally, we test our approach in a large-scale dataset of cooking recipes and associated pictures. Our method achieves a 5-fold improvement over the state-of-the-art, and we show that the multi-task aspect of our approach promotes a semantically meaningful organization of the representation space, allowing it to perform subtasks never seen during training, like ingredient exclusion and selection. The results we present in this thesis open many possibilities, including feature compression for remote applications, robust multi-modal and multi-task learning, and feature space refinement. For the cooking application, in particular, many of our findings are directly applicable in a real-world context, especially for the detection of allergens, finding alternative recipes due to dietary restrictions, and menu planning.
Robot swarms are systems composed of a large number of rather simple robots. Due to the large number of units, these systems, have good properties concerning robustness and scalability, among others. However, it remains generally difficult to design controllers for such robotic systems, particularly due to the complexity of inter-robot interactions. Consequently, automatic approaches to synthesize behavior in robot swarms are a compelling alternative. In this thesis, we focus on online behavior adaptation in a swarm of robots using distributed Embodied Evolutionary Robotics (EER) methods. To this end, we provide three main contributions: (1) We investigate the influence of task-driven selection pressure in a swarm of robotic agents using a distributed EER approach. We evaluate the impact of a range of selection pressure strength on the performance of a distributed EER algorithm. The results show that the stronger the task-driven selection pressure, the better the performances obtained when addressing given tasks. (2) We investigate the evolution of collaborative behaviors in a swarm of robotic agents using a distributed EER approach. We perform a set of experiments for a swarm of robots to adapt to a collaborative item collection task that cannot be solved by a single robot. Our results show that the swarm learns to collaborate to solve the task using a distributed approach, and we identify some inefficiencies regarding learning to choose actions. (3) We propose and experimentally validate a completely distributed mechanism that allows to learn the structure and parameters of the robot neurocontrollers in a swarm using a distributed EER approach, which allows for the robot controllers to augment their expressivity. Our experiments show that our fully-decentralized mechanism leads to similar results as a mechanism that depends on global information
Smart-phones and tablets are nowadays one of the most used objects in everyday life (personal or professional usage). The diversity of mobile applications'sources and the easiness of downloading/using new applications make it hard for a typical user to manage or care about the security of his device. To deal with this issue, different kinds of methods have been developed recently. We can mention: static analysis, dynamic analysis or behavioral analysis. Pradeo offers to its clients a solution based on advanced static analysis. This product was developed during several PHD programs in partnership with LIRMM. However, this solution does not include automatic learning from data collected over devices in order to prevent risky situations on mobile devices. During this PHD program, we will apply data analysis and artificial intelligence methods in order to correlate data coming from several data sources and thus implement a robust behavioral analysis. We think that the methods present in current literature can't be applied directly to our problem because of the velocity and diversity of received data and also the limitations imposed by mobile environments. The following points detail what needs to be considered during the design of this solution: - OS shouldn't be modified and the agent should be executed using simple users - Energy consumption is an important parameter to consider ; it should be minimal during the collection of data over devices - During the collection process, the receiving system should be capable to handle, index and query data coming from +1 million devices - The error margin of the final model should be minimal-All mobile devices should be taken into account (iOS, Android, etc.) The goal of this project is to enrich the Pradeo product in order to identify risky situations on mobile devices. We aim to add an intelligent model to analyze in (almost) real-time: - Events coming in real-time from devices - Reports of static analysis generated by the core product of Pradeo-External sources of vulnerabilities.
Our research concerns the public policy analysis on how Cloud Computing and Big data are adopted by French and Moroccan States with a comparative approach between the two models. The impact of the digital on the organization of States and Government ; The concept related to the data protection, data privacy ; The limits between security, in particular home security, and the civil liberties ; The future and the governance of the Internet ; A use case on how the Cloud could change the daily work of a public administration ; Our research aims to analyze how the public sector could be impacted by the current digital (re) evolution and how the States could be changed by emerging a new model in digital area called Cyber-State. We tried to analyze the digital transformation by looking on how the public authorities treat the new economics, security and social issues and challenges based on the Cloud Computing and Big Data as the key elements on the digital transformation. We tried also to understand how the States – France and Morocco-face the new security challenges and how they fight against the terrorism, in particular, in the cyberspace. We studied the recent adoption of new laws and legislation that aim to regulate the digital activities. We analyzed the limits between security risks and civil liberties in context of terrorism attacks. We analyzed the concepts related to the data privacy and the data protection.
This rnodel gives the opportunity to extract most lipreading information according to a in depth bibliographical study. On these images, automatic lip location without external constraints is still unsolved. To label lips automatically, we use two repetitions of the same sentence by the same subject, with and without blue make up: onceagain, the blue sequence enables easy lip location and dynamic time warping (dtw) allows to estimate lip shape on natural images using the extracted shapes on blue images. The appearance model obtained is very similar to the one obtained when training the same initial model with hand-Iabeled images and is quite better than other models relying on hue.
The rise of work in affective computing sees the emergence of various research questions to study agent / human interactions. Among them raises the question of the impact of interpersonal relations on the strategies of communication. Human/agent interactions usually take place in collaborative environments in which the agent and the user share common goals. The interpersonal relations which individuals create during their interactions affects their communications strategies. Moreover, individuals who collaborate to achieve a common goal are usually brought to negotiate. This type of negotiation allows the negotiators to efficiently exchange information and their respective expertise in order to better collaborate. The objective of this thesis is to study the impact of the interpersonal relationship of dominance on collaborative negotiation strategies between an agent and a human. This work is based on studies from social psychology to define the behaviours related to the manifestation of dominance in a negotiation. We propose a collaborative negotiation model whose decision model is governed by the interpersonal relation of dominance. Depending on its position in the dominance spectrum, the agent is able to express a specific negotiation strategy. In parallel, the agent simulates an interpersonal relationship of dominance with his interlocutor. To this aim, we provided the agent with a model of theory of mind that allows him to reason about the behaviour of his interlocutor in order to predict his position in the dominance spectrum. Afterwards, the agent adapts his negotiation strategy to complement the negotiation strategy detected in the interlocutor. Our results showed that the dominance behaviours expressed by our agent are correctly perceived by human participants. Furthermore, our model of theory of mind is able de make accurate predictions of the interlocutor behaviours of dominance with only a partial representation of the other's mental state. Finally, the simulation of the interpersonal relation of dominance has a positive impact on the negotiation: the negotiators reach a good rate of common gains and the negotiation is perceived comfortable which increases the liking between the negotiators.
Previous research demonstrates that having access to the syntactic structure of sentences helps children to discover the meaning of novel words. This implies that infants need to get access to aspects of syntactic structure before they know many words. Since in all the world's languages the prosodic structure of a sentence correlates with its syntactic structure, and since function words/morphemes are useful to determine the syntactic category of words, infants might use phrasal prosody and function words to bootstrap their way into lexical and syntactic acquisition. In this thesis, I empirically investigated the role of phrasal prosody and function words to constrain syntactic analysis in young children (PART 1) and whether infants exploit this information to learn the meanings of novel words (PART 2). In part 1, I constructed minimal pairs of sentences in French and in English, testing whether children exploit the relationship between syntactic and prosodic structures to drive their interpretation of noun-verb homophones. I demonstrated that preschoolers use phrasal prosody online to constrain their syntactic analysis. When listening to French sentences such as [La petite ferme][… – [The little farm][…, children interpreted ferme as a noun, but in sentences such as [La petite][ferme…] – [The little girl][closes…, they interpreted ferme as a verb (Chapter 3). This ability was also attested in English-learning preschoolers who listened to sentences such as 'The baby flies…': they used prosodic information to decide whether “flies” was a noun or a verb (Chapter 4). Importantly, in further studies I demonstrated that even infants around 20-months use phrasal prosody to recover syntactic structures and to predict the syntactic category of upcoming words (Chapter 5), an ability which would be extremely useful to discover the meaning of unknown words. This is what I tested in part 2: whether the syntactic information obtained from phrasal prosody and function words could allow infants to constrain their acquisition of word meanings. A first series of studies relied on right-dislocated sentences containing a novel verb in French: [ili dase], [le bébéi]-'hei is dasing, the babyi'(meaning 'the baby is dasing') which is minimally different from the transitive sentence [il dase le bébé] (he is dasing the baby). 28-month-olds were shown to exploit prosodic information to constrain their interpretation of the novel verb meaning (Chapter 6). In a second series of studies, I investigated whether phrasal prosody and function words constrain the acquisition of nouns and verbs. I used sentences like 'Regarde la petite bamoule', which can be produced either as [Regarde la petite bamoule!]-Look at the little bamoule!, where 'bamoule'is a noun, or as [Regarde], [la petite] [bamoule!] -Look, the little (one) is bamouling, where bamoule is a verb. 18-month-olds correctly parsed such sentences and attributed a noun or verb meaning to the critical word depending on its position within the syntactic-prosodic structure of the sentences (Chapter 7). Taken together, these studies show that infants exploit function words and the prosodic structure of an utterance to recover the sentences' syntactic structure, which in turn constrains the possible meaning of novel words. This powerful mechanism might be extremely useful for infants to construct a first-pass syntactic structure of spoken sentences even before they know the meanings of many words. Although prosodic information and functional elements can surface differently across languages, our studies suggest that this information may represent a universal and extremely useful tool for infants to access syntactic information through a surface analysis of the speech stream, and to bootstrap their way into language acquisition.
Even if intangible capital represents an increasingly important part of the value of our enterprises, it's not always possible to store, trace or capture knowledge and expertise, for instance in middle sized projects. Email it still widely used in professional projects especially among geographically distributed teams. In this study we present a novel approach to detect zones inside business emails where elements of knowledge are likely to be found. We define an enhanced context taking into account not only the email content and metadata but also the competencies of the users and their roles. Also linguistic pragmatic analysis is added to usual natural language processing techniques. After describing our model and method KTR, we apply it to a real life corpus and evaluate the results based on machine learning, filtering and information retrieval
Facial expression analysis is an important problem in many biometric tasks, such as face recognition, face animation, affective computing and human computer interface. In this thesis, we aim at analyzing facial expressions of a face using images and video sequences. We divided the problem into three leading parts. First, we study Macro Facial Expressions for Emotion Recognition and we propose three different levels of feature representations. Then, we incorporate the time dimension to extract spatio-temporal features with the objective to describe subtle feature deformations to discriminate ambiguous classes. Second, we direct our research toward transfer learning, where we aim at Adapting Facial Expression Category Models to New Domains and Tasks. Thus we study domain adaptation and zero shot learning for developing a method that solves the two tasks jointly. Our method is suitable for unlabelled target datasets coming from different data distributions than the source domain and for unlabelled target datasets with different label distributions but sharing the same context as the source domain. Therefore, to permit knowledge transfer between domains and tasks, we use Euclidean learning and Convolutional Neural Networks to design a mapping function that map the visual information coming from facial expressions into a semantic space coming from a Natural Language model that encodes the visual attribute description or use the label information. The consistency between the two subspaces is maximized by aligning them using the visual feature distribution. Then, a statistical based model for estimating the probability density function of normal facial behaviours while associating a discriminating score to spot micro-expressions is learned based on a Gaussian Mixture Model. Finally, an adaptive thresholding technique for identifying micro expressions from natural facial behaviour is proposed. Our algorithms are tested over deliberate and spontaneous facial expression benchmarks.
The aim of this research is to investigate multi-modality biometric image quality assessment methods for unconstrained samples. Studies of biometrics noted the significance of sample quality for a recognition system or a comparison algorithm because the performance of the biometric system depends mainly on the quality of the sample images. The need to assess the quality of multi-modality biometric samples is increased with the requirement of a high accuracy multi-modality biometric systems. Following an introduction and background in biometrics and biometric sample quality, we introduce the concept of biometric sample quality assessment for multiple modalities. Recently established ISO/IEC quality standards for fingerprint,iris, and face are presented. In addition, sample quality assessment approaches which are designed specific for contact-based and contactless fingerprint, near infrared-based iris and visible wavelength iris, as well as face are surveyed. Following the survey, approaches for the performance evaluation of biometric sample quality assessment methods are also investigated. Based on the knowledge gathered from the biometric sample quality assessment challenges, we propose a common framework for the assessment of multi-modality biometric image quality. We review the previous classification of image-based quality attributes for a single biometric modality and investigate what are the common image-based attributes for multi-modality. Then we select and re-define the most important image-based quality attributes for the common framework. In order to link these quality attributes to the real biometric samples, we develop anew multi-modality biometric image quality database which has both high quality sample images and degraded images for contactless fingerprint, visible wavelength iris, and face modalities. The degradation types are based on the selected common image-based quality attributes. Another important aspect in the proposed common framework is the image quality metrics and their applications in biometrics. We first introduce and classify the existing image quality metrics and then conducted a brief survey of no-reference image quality metrics, which can be applied to biometric sample quality assessment. Plus, we investigate how no-reference image quality metrics have been used for the quality assessment for fingerprint, iris, and face biometric modalities. The experiments for the performance evaluation of no-reference image quality metrics for visible wavelength face and iris modalities are conducted. The experimental results indicate that there are several no-reference image quality metrics that can assess the quality of both iris and face biometric samples. Through the work carried out in this thesis we have shown the applicability of no-reference image quality metrics for the assessment of unconstrained multi-modality biometric samples.
The topic of this Ph.D. thesis lies on the borderline between signal processing, statistics and computer science. This framework is by nature suited for learning from distributed collections or data streams, and has already been instantiated with success on several unsupervised learning tasks such as k-means clustering, density fitting using Gaussian mixture models, or principal component analysis. We improve this framework in multiple directions. First, it is shown that perturbing the sketch with additive noise is sufficient to derive (differential) privacy guarantees. Sharp bounds on the noise level required to obtain a given privacy level are provided, and the proposed method is shown empirically to compare favourably with state-of-the-art techniques. Then, the compression scheme is modified to leverage structured random matrices, which reduce the computational cost of the framework and make it possible to learn on high-dimensional data. Lastly, we introduce a new algorithm based on message passing techniques to learn from the sketch for the k-means clustering problem.
They enable users to pay or sign numeric documents for example. Because they contain sensible information about their user and secrets, attackers are interested in them. In particular, these attackers can use fuzzing. This attack consists in sending the most possible communication messages to a program in order to detect its vulnerabilities. This thesis aims at protecting smart cards against fuzzing. Two approaches for detecting implementation errors are proposed. The first one is from the state of the art, and it is adapted and improved for Java. It is based on an automated source code analysis. The second approach analyses source codes too, but it takes into account limitations of the first one. In particular, the precision and the dimension reduction is improved by using Natural Language Processing techniques. In addition, it studies plagiarsm techniques in order to reinforce its analysis against different implementations choices. Both approaches are evaluated against three manually created oracles from OpenPGP and AES implementations for the neighborhood discovery and the anomaly detection. Results show that the second approach is improved in precision, recall and with less execution time than the first one. Its implementation, Confiance can be used in companies to secure source codes.
This dissertation examines C-to-V coarticulation in French and its interaction with others sources of variation in order to better understand what modulates and governs variation in speech. Based on data from large speech corpora, we tested how C-to-V coarticulation is a function of: 1) the articulatory properties of the tested segments, i.e. 18.5k vowels /i, e, ɛ, a, x, u, o, ɔ/ (/x/=/ø, œ, ə/) in ALVeolar, UVular et VELar contexts; 2) the prosodic position occupied by the vowels, comparing the degree of coarticulation of 17k CV and VC sequences V=/i, e, a, ɔ/ and C=ALV|UV in initial position of intonational phrases, to similar sequences in internal word position; 3) the speech style, by analyzing the degree of coarticulation in 22k CV and VC sequences, V = /i, E, a, u, ɔ/ (/E/ = /e, ɛ/) and C = ALV|UV, in journalistic and conversational speech. However, some results suggest that the modulation of coarticulation by prosodic position and speech style have different linguistic functions whose implications for speech variation will be discussed. Finally, a reflection on sound changes related to the universal preference for the anteriorization of back closed vowels will be proposed from the observed differences between the vowels.
In this regard, incoherent interpretive schemes and majority influence are examples for the former and performance drawbacks as well as learning difficulties associated to hierarchical methodologies are instances of the latter. Based on the results of the literature review, two experiments were conducted. The first experiment inquired into the impact of disciplinary group composition (H1) as well as of the applied methodology (H2) on the creative group problem solving process and its outcomes. In a laboratory experiment 60 participants, 45 with a life science background and 15 with a mechanical engineering background were trained either in instances of intuitive approaches (Brainstorming, Mind Mapping) or in analytical, hierarchical methodology (TRIZ/USIT). Then, they had to solve an ill-defined medical problem in either mono- or multidisciplinary teams. Statistical analyses (ANOVA, Correlation parameters and Attraction rates), to a certain extent, support H1 and H2. More importantly however, the experiment shows differences related to method performance in general and as a function of disciplinary group composition in particular. The model incorporates two of the most important concepts of TRIZ, and is sought to facilitate creative problem solving attempts in both, mono- and multidisciplinary teams. The said model was tested during an industrial NCD study in the roller bearing industry. After the case study, the participating engineers were asked to compare the applied model and the associated technology integration process with existing approaches used in the company. The results of the experiment point to superior performance of the presented model in terms of knowledge transfer-related and idea quality-related criteria. However, required resources for process conduction and necessary effort for the learning of the approach were considered comparable to existing approaches. The present Ph.D. work contributes to the understanding of creative problem solving in interdisciplinary groups in general and related to technology integration in particular. Especially the comparison of more pragmatic intuitive methods with more hierarchical analytical approaches depending on disciplinary group composition provided relevant insight for R&amp;D processes.
The goal of this thesis is to model the semantic and topical context of new proper names in order to retrieve those which are relevant to the spoken content in the audio document. Training context models is a challenging problem in this task because several new names come with a low amount of data and the context model should be robust to errors in the automatic transcription. Probabilistic topic models and word embeddings from neural network models are explored for the task of retrieval of relevant proper names. A thorough evaluation of these contextual representations is performed. The proposed Neural Bag-of-Weighted-Words (NBOW2) model learns to assign a degree of importance to input words and has the ability to capture task specific key-words. Experiments on automatic speech recognition on French broadcast news videos demonstrate the effectiveness of the proposed models. Evaluation of the NBOW2 model on standard text classification tasks shows that it learns interesting information and gives best classification accuracies among the BOW models
Those latest decades, the development of information end communication technologies has deeply modified die way we access knowledge. Facing the volume end the diversity of date, it is necessary to work out robust end efficient technologies to retrieve information. We question named entities status (related notions, typologies, evaluation end annotation) and propose properties to define their linguistic nature. We conclude this part by describing state-of-the-art approaches end by presenting our contribution, focused on markers (tags) diet begin or end an annotation. In die second part, we present die formalism used to mine date. The framework we use to enrich date, explore sequences and extract annotation rules is formalized. The lest part describes the implemented system (mXS) and the obtained results. Specific implementation details are given and results about rule extraction from data are reported. Finally, we provide quantitative results of the performance of mXS on Ester2 end Etape datasets, among with various indications about die behaviour of die system from diverse points of view and in diverse configurations. They show diet our approach gives competitive results end that it opens up new perspectives for natural language processing and automatic annotation.
This thesis proposes to develop new methods to learn dialogue strategies with reinforcement learning with the objective of solving the dual challenge of task-oriented dialog systems: Designing conversational agents both efficient to solve a task and that reach human-level language communication.
As the production of digital texts grows exponentially, a greater need to analyze text corpora arises in various domains of application, insofar as they constitute inexhaustible sources of shared information and knowledge. We therefore propose in this thesis a novel visual analytics approach for the analysis of text corpora, implemented for the real and concrete needs of investigative journalism. Motivated by the problems and tasks identified with a professional investigative journalist, visualizations and interactions are designed through a user-centered methodology involving the user during the whole development process. Specifically, investigative journalists formulate hypotheses and explore exhaustively the field under investigation in order to multiply sources showing pieces of evidence related to their working hypothesis. Carrying out such tasks in a large corpus is however a daunting endeavor and requires visual analytics software addressing several challenging research issues covered in this thesis. First, the difficulty to make sense of a large text corpus lies in its unstructured nature. We resort to the Vector Space Model (VSM) and its strong relationship with the distributional hypothesis, leveraged by multiple text mining algorithms, to discover the latent semantic structure of the corpus. Although the exploration of the coarse-grained topics helps locate topic of interest and its neighborhood, the identification of specific facts, viewpoints or angles related to events or stories requires finer level of structuration to represent topic variants. This nested structure, revealed by Bimax, a pattern-based overlapping biclustering algorithm, captures in biclusters the co-occurrences of terms shared by multiple documents and can disclose facts, viewpoints or angles related to events or stories. This thesis tackles issues related to the visualization of a large amount of overlapping biclusters by organizing term-document biclusters in a hierarchy that limits term redundancy and conveys their commonality and specificities. We evaluated the utility of our software through a usage scenario and a qualitative evaluation with an investigative journalist. In addition, the co-occurrence patterns of topic variants revealed by Bima. are determined by the enclosing topical structure supplied by the coarse-grained topic extraction method which is run beforehand. Nonetheless, little guidance is found regarding the choice of the latter method and its impact on the exploration and comprehension of topics and topic variants. Therefore we conducted both a numerical experiment and a controlled user experiment to compare two topic extraction methods, namely Coclus, a disjoint biclustering method, and hierarchical Latent Dirichlet Allocation (hLDA), an overlapping probabilistic topic model.
Over the past three decades, millions of people have been producing and sharing information on the Web, Currently, billions of RDF descriptions are available on the Web through the Linked Open Data cloud projects (e.g., DBpedia and LinkedGeoData). Also, several data providers have adopted the principles and practices of the Linked Data to share, connect, enrich and publish their information using the RDF standard, e.g., Governments (e.g., Canada Government), universities (e.g., Open University) and companies (e.g., BBC and CNN). As a result, both individuals and organizations are increasingly producing huge collections of RDF descriptions and exchanging them through different serialization formats (e.g., RDF/XML, Turtle, N-Triple, etc.). For that purpose, we have defined a framework entitled R2NR which normalizes different RDF descriptions pertaining to the same information into one normalized representation, which can then be tuned both at the graph level and at the serialization level, depending on the target application and user requirements. We illustrate this approach by introducing use cases (real and synthetics) that need to be normalized. The contributions of the thesis can be summarized as follows: i. Producing a normalized (output) RDF representation that preserves all the information in the source (input) RDF descriptions, ii. Eliminating redundancies and disparities in the normalized RDF descriptions, both at the logical (graph) and physical (serialization) levels, iii. Computing a RDF serialization output adapted w.r.t. the target application requirements (faster loading, better storage, etc.), iv. Providing a mathematical formalization of the normalization process with dedicated normalization functions, operators, and rules with provable properties, and v. Providing a prototype tool called RDF2NormRDF (desktop and online versions) in order to test and to evaluate the approach's efficiency. In order to validate our framework, the prototype RDF2NormRDF has been tested through extensive experimentations. Experimental results are satisfactory show significant improvements over existing approaches, namely regarding loading time and file size, while preserving all the information from the original description.
Some natural language processing applications have to deal with textual data streams characterized by the use of an evolving vocabulary, whether at the creation of words as at the change in the meaning of already existing words. In light of those observations, we have developed an incremental algorithm which can build automatically an evolving lexical database for identifying lexical units observed in a textual data stream. This structured representation is completed with a cartographic model taking into account the continuous aspects of meaning and semantic proximity between concepts. This property is exploited to propagate the classification of a small number of named entities (NEs: lexical units which usually refer to people, places, organizations...) to others NEs observed in unlabelled data streams during the incremental construction of the lattice. Once the lexical database is built, the concepts are enriched with NEs labels observed in a training corpus. The concepts and their attached labels are then respectively used for unsupervised annotation and supervised classification of NEs in test corpus.
Modeling time series has practical applications in many domains: speech, gesture and handwriting recognition, synthesis of realistic character animations etc... The starting point of our modeling is that an important part of the variability between observation sequences may be the consequence of a few contextual variables that remain fixed all along a sequence or that vary slowly with time. For instance a sentence may be uttered quite differently according to the speaker emotion, a gesture may have more amplitude depending on the height of the performer etc... This method relies on sharing information between classes where in generative models classes are normally considered independent.
The structure of a neural network determines to a large extent its cost of training and use, as well as its ability to learn. These two aspects are usually in competition: the larger a neural network is, the better it will perform the task assigned to it, but the more it will require memory and computing time resources for training. Within this context, neural networks with various structures are trained, which requires a new set of training hyperparameters for each new structure tested. The aim of the thesis is to address different aspects of this problem. The first contribution is a training method that operates within a large perimeter of network structures and tasks, without needing to adjust the learning rate. The second contribution is a network training and pruning technique, designed to be insensitive to the initial width of the network. The last contribution is mainly a theorem that makes possible to translate an empirical training penalty into a Bayesian prior, theoretically well founded. This work results from a search for properties that theoretically must be verified by training and pruning algorithms to be valid over a wide range of neural networks and objectives.
Through any location based services application (LBA) (i.e. m-tourism), users who request information while on the move, intentionally seek as well a quick and precise answer on any map. Typically, these datasets had been collected from many geographic databases worldwide. However, the increasing number of different GDBs covering the same area and the retrieval of accurate data/metadata for the requested service will imply lots of reasoning processes and databases' accesses in order to avoid nearly-duplicated records when displayed on the screen. In other words, my ultimate goal is to generate automatically a unique map from multiple providers' portrayals such as Google Maps, Bing and Yahoo Maps while homologous features should be integrated to avoid duplicate icons on the mobile screen. Our conceptual framework, based on some fusion algorithms, ontology reasoning for cartographic interoperability and geo-web services orchestration, had been implemented in some modular prototypes and tested for evaluation purpose.
Just-In-Time recommender systems involve all systems able to provide recommendations tailored to the preferences and needs of users in order to help them access useful and interesting resources within a large data space. Our work falls within this framework and focuses on developing a proactive context-aware recommendation approach for mobile devices that covers many domains. Indeed, the development of mobile devices equipped with persistent data connections, geolocation, cameras and wireless capabilities allows current context-aware recommender systems (CARS) to be highly contextualized and proactive. We also take into consideration to which degree the recommendation might disturb the user. It is about balancing the process of recommendation against intrusive interruptions. As a matter of fact, there are different factors and situations that make the user less open to recommendations. As we are working within the context of mobile devices, we consider that mobile applications functionalities such as the camera, the keyboard, the agenda, etc., are good representatives of the user's interaction with his device since they somehow stand for most of the activities that a user could use in a mobile device in a daily basis such as texting messages, chatting, tweeting, browsing or taking selfies and pictures.
In this thesis, we have investigated how to exploit user-generated-content for personalized news recommendation purpose. The intuition behind this line of research is that the opinions provided by users, on news websites, represent a strong indicator about their profiles. We have addressed this problem by proposing three main contributions. Firstly, we have proposed a profile model that accurately describes both users' interests and news article contents. Secondly, we have investigated the problem of noise on opinions and how we can retrieve only relevant opinions in response to a given query. Results show that our approach outperforms two recent proposed opinions ranking strategies, particularly for controversial topics. Results show that diverse opinions give the best performance over other leveraging strategies.
The discourse structure of a document is a key element to understand the content conveyed by a text. It affects, for instance, the temporal structure of a text, or the interpretation of anaphoric expressions. In this thesis, we will study the effects of the discourse structure on sentiment analysis. Sentiment analysis is an extremely active research domain in natural language processing. The last years have seen the multiplication of the available textual data conveying opinion on the web, and the automation of the summary of opinion documents became crucial for who wants to keep an overview of the opinion on a given subject. Most of the current research efforts describe an opinion extraction at the document level or at the sentence level, ignoring the discourse structure. In this thesis work, we address opinion extraction through the discourse framework of the SDRT (Segmented Discourse Representation Theory), and try to answer to the following questions: - Is there a link between the discourse structure of a document and the opinions contained in that document? - What is the role of discourse relations in the determination of whether a textual segment is objective or subjective? - What is the impact of the discourse structure in the determination of the overall opinion conveyed by a document? - Does a discourse based approach really bring additional value compared to a classical "bag of words" approach?
Complex Event Processing (CEP) consists of the analysis of data-streams in order to extract particular patterns and behaviours described, in general, in a logical formalism. In the classical approach, data of a stream – or events – are supposed to be the complete and perfect observation of the system producing these events. However, in many cases, the means for collecting such data, such as sensors, are not infallible and may miss the detection of a particular event or on the contrary produce. In this thesis, we have studied the possible models of representation of uncertainty and, thus, to offer the CEP a robustness to this uncertainty as well as the necessary tools to allow the recognition of complex behaviours based on the chronicle formalism. In this perspective, three approaches have been considered. The first one is based on Markov logical networks to represent the structure of the chronicles under a set of logical formulas of a confidence value. We show that this model, although widely applied in the literature, is inapplicable for a realistic application with regard to the dimensions of such a problem. The second approach is based on techniques from the SAT community to enumerate all possible solutions of a given problem and thus to produce a confidence value for the recognition of a chronicle expressed, again, under a logical structure. Finally, we propose a last approach based on the Markov chains to produce a set of samples explaining the evolution of the model in agreement with the observed data. These samples are then analysed by a recognition system to count the occurrences of a particular chronicle.
This thesis proposes theoretical and numerical contributions to use Entropy-regularized Optimal Transport (EOT) for machine learning. We introduce Sinkhorn Divergences (SD), a class of discrepancies between probability measures based on EOT which interpolates between two other well-known discrepancies: Optimal Transport (OT) and Maximum Mean Discrepancies (MMD). We develop an ecient numerical method to use SD for density fitting tasks, showing that a suitable choice of regularization can improve performance over existing methods.
Making semantic parsers robust to lexical and stylistic variations is a real challenge with many industrial applications. Nowadays, semantic parsing requires the usage of domain-specific training corpora to ensure acceptable performances on a given domain. Transfer learning techniques are widely studied and adopted when addressing this lack of robustness, and the most common strategy is the usage of pre-trained word representations. However, the best parsers still show significant performance degradation under domain shift, evidencing the need for supplementary transfer learning strategies to achieve robustness. This work proposes a new benchmark to study the domain dependence problem in semantic parsing. We use this bench to evaluate classical transfer learning techniques and to propose and evaluate new techniques based on adversarial learning. All these techniques are tested on state-of-the-art semantic parsers. We claim that adversarial learning approaches can improve the generalization capacities of models. We test this hypothesis on different semantic representation schemes, languages and corpora, providing experimental results to support our hypothesis.
This thesis aims to improve the intuitiveness of human-computer interfaces. In particular, machines should try to replicate human's ability to process streams of information continuously. However, the sub-domain of Machine Learning dedicated to recognition on time series remains barred by numerous challenges. Our studies use gesture recognition as an exemplar application, gestures intermix static body poses and movements in a complex manner using widely different modalities. To do so, we reimplemented the two within a shared test-bed which is more amenable to a fair comparative work. We propose adjustments to Neural Network training losses and the Hybrid NN-HMM expressions to accommodate for highly imbalanced data classes. Although recent publications tend to prefer BDRNNs, we demonstrate that Hybrid NN-HMM remain competitive. Finally, we show that input representations learned via both approaches are largely inter-compatible. The second part of our work studies one-shot learning, which has received relatively little attention so far, in particular for sequential inputs such as gestures. We propose a model built around a Bidirectional Recurrent Neural Network. Its effectiveness is demonstrated on the recognition of isolated gestures from a sign language lexicon. We propose several improvements over this baseline by drawing inspiration from related works and evaluate their performances, exhibiting different advantages and disadvantages for each
Developers are eager to create various web applications to meet people's increasing demands. To build a web application, developers need to know some basic programming technologies. Moreover, they prefer to use some third-party components (such as server-side libraries, client-side libraries, REST services) in the web applications. By including those components, they could benefit from maintainability, reusability, readability, and efficiency. In this thesis, we propose to help developers to use third-party components when they create web applications. We present three impediments when developers using the third-party components: What are the best JavaScript libraries to use? How to get the standard specifications of REST services? How to adapt to the data changes of REST services? As such, we present three approaches to solve these problems. Those approaches have been validated through several case studies and industrial data. We describe some future work to improve our solutions, and some research problems that our approaches can target.
Combinatorial Optimization (CO) is an area of research that is in a constant progress. Solving a Combinatorial Optimization Problem (COP) consists essentially in finding the best solution (s) in a set of feasible solutions called a search space that is usually exponential in cardinality in the size of the problem. To solve COPs, several methods have been proposed in the literature. A distinction is made mainly between exact methods and approximation methods. Since it is not possible to aim for an exact resolution of NP-Complete problems when the size of the problem exceeds a certain threshold, researchers have increasingly used Hybrid (HA) or parallel computing algorithms in recent decades. In this thesis we consider the COP class of Survivability Network Design Problems. We present an approximation parallel hybrid algorithm based on a greedy algorithm, a Lagrangian relaxation algorithm and a genetic algorithm which produces both lower and upper bounds for flow-based formulations. In order to validate the proposed approach, a series of experiments is carried out on several applications: the k-Edge-Connected Hop-Constrained Network Design Problem (kHNDP) when L = 2,3, The problem of the Steiner k-Edge-Connected Network Design Problem (SkESNDP) and then, two more general problems namely the kHNDP when L >= 2 and the k-Edge-Connected Network Design Problem (kESNDP). The experimental study of the parallelisation is presented after that. In the last part of this work, we present a two parallel exact algorithms: a distributed Branch-and-Bound and a distributed Branch-and-Cut. A series of experiments has been made on a cluster of 128 processors and interesting speedups has been reached in kHNDP resolution when k=3 and L=3.
Numerous chemicals are used as ingredients by the cosmetics industry and are included in cosmetics formula. Aside from the assessment of their efficacy, the cosmetics industry especially needs to assess the safety of their chemicals for human. Toxicological screening of chemicals is performed with the aim of revealing the potential toxic effect of the tested chemical. Among the potential effects we want to detect, the developmental toxicity of the chemical (teratogenicity), meaning its capability of provoking abnormalities during the embryonic development, is crucial. With respect to the international regulations that forbid the use of animal testing for the safety assessment of cosmetics, the toxicological assessment of chemicals must base on an ensemble of in silico assays, in vitro assays and alternative models based assays. For now, a few alternative methods have been validated in the field of developmental toxicology. The development of new alternative methods is thus required. In addition to the safety assessment, the environmental toxicity assessment is also required. The use of most of cosmetics and personal care products leads to their rejection in waterways after washing and rince. This results in the exposition of some aquatic environments (surface waters and coastal marine environments) to chemicals included in cosmetics and personal care products. Thus, the environmental assessment of cosmetics and of their ingredients requires the knowledge of their toxicity on organisms that are representative of aquatic food chains. In this context, the fish embryo model, which is ethically acceptable according to international regulations, presents a dual advantage for the cosmetics industry. Firstly, as a model representative of aquatic organisms, it is accurate for the environmental assessment of chemicals. Secondly, this model is promising for the assessment of the teratogenic effect of chemicals on human. For this reason, a teratogenicity assessment test is developed. This test is based on the analysis of medaka fish embryos (Oryzias Latipes) at 9 days post fertilization, after balneation in a predetermined concentration of the chemical under study. The analysis of functional and morphological parameters allows to calculate a teratogenicity index, that depends on both rates of dead and malformed embryos. This index allows to to draw a conclusion concerning the teratogenic effect of the chemical. The objective of this project is to automate the teratogenicity test, by automated image and video classification. We then focus on the detection of two common malformations: axial malformations and absence of a swim bladder, based on a machine learning classification. This analysis must be completed by the detection of other malformations so that we can measure the rate of malformed embryos and thus, calculate the teratogenicity index of the tested chemical
Our research focuses on two interrelated objectives. The first one aims at providing assistance to the evaluation of scientific writings because of; the numbers of publication, which keep on rising, the boundaries between the areas become and it's becoming more and more difficult to find relevant publications so as the practical need for assessments is appearing as unavoidable. We also have to find new ways to help the evaluation of the research work, through a wide range of indications different from those usually used for research assessment, notably, through the identification of the research problem. These indications that announce the formulation of the research problem in scientific articles can be identified as "speech formulas. " Our research does not extend to the formulation of the scientific problem because of the complexity of this concept and the difficulty of defining it from the point of view of information extraction. We propose a model of the speech forms that we have integrated into the parser Xerox Incremental Parser (XIP) in pattern recognition rules. We used a corpus of Educational Sciences Research articles from the Scientext corpus to detect these speeches formulas. The choice of field is motivated by my participation in the European project EERQI which aims to strengthen and enhance the worldwide visibility and competitiveness of European research in education. Different methodological approaches were adopted to perform a fine linguistic study of these formulas as: discourse analysis (Mr. Pecman, 2004, K. Hyland, 2005, Á, Sándor A. Kaplan, G. Randeau, 2006, D. Siepman, 2007, A. Tutin, 2007-2010), robust parsing (S. Aït-Mokhtar, J. Chanod, R. Roux, 2002). Thus, the goal is to implement an applicative approach aiming at helping expert reading through the identification, typology and functioning of lexical associations which convey the research problem.
This thesis that the issue is part of the controversy in the number of affixes in the Fon language, it led us to ask if we can make the affixal derivation on his own names? We solved this problem through the different parts that make up this thesis. We input, discussed the theory of "Switch", the application starts with the identification of phonological units, their definition and their classification according to their oppositional traits, and contrastive, and also how these units are combined them. We realize that in his language, composition and derivation are also factors training nouns and verbs. Our approach in this area has been to follow a logical order from the smallest units up to larger: phonemes, syllables, phonological words. A state of the art of lexicology and lexicography is done. However there is still embrionnaire, despite the fact that it was started with the European penetration, the works of missionaries, and the implementation of new projects in the advent of independence. Part deals with derivational morphology, it is one of the most important parts of the thesis. In an onomastic approach, it implements the derivation of the nomenclature of the names of the kings of Abomey, the organizational structure of power in the royal court to which is added the matrix method of creating new names. At the same orde idea, a syntactic and morphological study of the numbering system was made to facilitate counting in his language. Through an ethno-linguistic analysis, we processed through a varied typology, event anthroponyms: choice of personal names related to life, fate, the destiny, death, family, fertility, friendship and success, the names of days, months and names of people who were born on a certain day of the week, names that reflect local and ethnological realities. In a dynamic perspective, after taking stock of lexicology and lexicography for tabulating the period until the beginning of computerization, that is to say, lexicology and automation, dictionaric to lead to the creation bilingual etymological dictionary in fon and French languages. Inputs are both quantitative and qualitative, because our problem has been resolved, we opened a perspective towards the computerization of languages on the one hand and the other hand on the problem of the emergence of national languages as factors of development to meet the Millennium development Goals (MDGs).
Last years, e-recruitment expansion has led to the multiplication of web channels dedicated to job postings. In an economic context where cost control is fundamental, assessment and comparison of recruitment channel performances have become necessary. The purpose of this work is to develop a decision-making tool intended to guide recruiters while they are posting a job on the Internet. This tool provides to recruiters the expected performance on job boards for a given job offer. The job offer performance predictive algorithm is based on a hybrid recommender system, suitable to the cold-start problem. The hybrid system, based on a supervised similarity measure, outperforms standard multivariate models.
Myotonic dystrophy (DM) is considered one of the most complex neuromuscular diseases. Although research work over the past 30 years has permitted a better understanding of its underlying molecular mechanisms, the unusual nature of its genetic anomalies, its multisystemic expression and its broad clinical spectrum do not allow, at the moment, optimal patient management. The purpose of my work was to deepen our knowledge of this rare disease and to clarify its natural history. The first part of my manuscript is dedicated to the presentation of the DM-Scope Registry, on which all my thesis work is based. After the description of the concept, the functioning and the data collection platform, the manuscript features the characteristics of the DM1 cohort, from which our analyses were conducted: the clinical spectrum covered, multisystemic impairment, genotype/phenotype correlations, interrelations between symptoms and comparison to myotonic dystrophy type II (DM2). In the second part, we focus on the major progress achieved through the existence of DM-Scope and the analyses conducted during my thesis: (i) detailing the natural history of the disease, in particular proposing a new classification; (ii) highlighting the phenotype's determining factors such as gender, mutation size, interrelations between symptoms. This work has led to recommendations for care, in particular for the transition from child to adult, but also the validation of important inclusion criteria for clinical trials such as gender. DM-Scope provides access to available biological samples for basic research studies and validates new therapeutic approaches. DM-Scope is now a worldwide leader and an essential tool in translational research in DM. The DM-Scope concept can be transferred to any other population and can be used for care management in other rare diseases. Finally, we present the development of a survival model built from the DM-Scope cohort. This model has three specificities: (i) it is applicable to high dimensional data, in such cases as DM-Scope, where there is a large number of measurements; (ii) it takes into account competitive risks, when patients are simultaneously exposed to several events. In our registry, the study of respiratory-related deaths is biased if competing events such as heart disease deaths are not taken into account ; (iii) it models the heterogeneity between patient groups probably due to divergent care, called "centres effects". DM-Scope data analysis requires such specificity of frailty models due to its multicentric coverage (55 centres). This model can be transferred and applied to other data, considering the following: more and more large-scaled registries are being used ; a majority of survival analyses includes censorship caused by the occurrence of the event of interest ; multicentre studies have become increasingly common.
Our prototype implements all theabove mentioned artifacts and proposes a workflow enabling its users to evaluate andimprove CMs efficiently and effectively. We conducted a survey to validate the selection of the quality constraints through a first experience and also conducted a second experiment to evaluate the efficacy and efficiency of our overall approach and proposed improvements.
A Classification problem makes use of a training set consisting of data labeled by an oracle. The larger the training set, the best the performance. However, requesting the oracle may be costly. The goal of Active Learning is thus to minimize the number of requests to the oracle while achieving the best performance. To Increase the precision on the estimate, we need to label more data. Thus, there is a dilemma between labeling data in order to increase the performance of the classifier or to better know how to select data. This dilemma is well studied in the context of finite budget optimization under the name of exploration versus exploitation dilemma. The most famous solutions make use of the principle of Optimism in the Face of Uncertainty. In this thesis, we show that it is possible to adapt this principle to the active learning problem for classification. Several algorithms have been developed for classifiers of increasing complexity, each one of them using the principle of Optimism in the Face of Uncertainty, and their performances have been empirically evaluated
Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems. Their manual construction requires extensive expertise in both languages involved and it is a costly process. Several automatic methods were proposed as an alternative but they often rely of resources available in a limited number of languages and their performances are still far behind the quality of manual translations. Our work concerns bilingual lexicon extraction from multilingual parallel and comparable corpora, in other words, the process of finding translation pairs among the common multilingual vocabulary available in such corpora.
The advent and availability of machine learning and artificial intelligence grabbed the headlines in the past few years, opening doors to their application in various sectors such as banking, finance, medical, etc. Machine learning can help preventing fraud and improving risk management, investment predictions and decision making. Learning models also became more prominent in the healthcare sector with many potential applications offering high-quality and cost-effective care to patients. The rapid evolutions in the learning models are facilitating the development of Natural Language Processing (NLP) tools that can exploit and manipulate huge amount of data. Driven by the aforementioned motivations, the main purpose of this PhD thesis is to contribute to new learning methods for dynamic complex systems in the healthcare sector. The candidate which is employed by a medical center in Saudi Arabia has access to a large amount of anonymous clinical data. He has to explore a range of AI tools, techniques, and frameworks and apply them to the clinical data in order to extract valuable knowledge and generate accurate predictive results. This process includes many steps: Anonymizing the data Cleansing the data: removing noise and detecting outliers Creating learning, validation and testing data sets (balanced er imbalanced) Proposing and comparing different language models such as Bert, doc2vec, etc. Adding new rules to refine the existing language models Applying machine learning algorithms to train the models Comparing the accuracy and performance of the trained models The trained models can be used to tackle many classification or prediction problems. A primary outcome of the study is a clinical explainable auto-diagnosis tool that can assist medical doctors.
This thesis aims to analyze uniquely-referring names which refer to singular referents indicating a place, a person, an event, an institution or a product of human activity in a corpus of tourist guide specific to Algeria.
Information theory has influenced a large number of scientific fields since its first introduction in 1948. Apart from Fitts'law and Hick's law, which came out when experimental psychologists were still enthusiastic about applying information theory to various areas of psychology, the relation between information theory and human-computer interaction (HCI) has rarely been explored. This thesis demonstrates that information theory can be used as a unified tool to understand and design human-computer communication.
In reliance upon contrastive analysis, our study examines the standpoints which become manifest through the use of terminology as it relates to public policy in security matters. Our research rests upon a selection of non-binding instruments in French and in Spanish, published between 2001 and 2018 by the European Commission and by two Member States, namely France and Spain. Behind our decision to analyse documents issued by both European and domestic authorities, lie peculiarities in the way institutional discourse is produced. Recent studies in the area have shown that such discourse fosters a rhetoric of consensus, and tends to drown political debate. As it happens, these communication strategies themselves reflect clear-cut ideological standpoints. By examining how such terms circulate in an environment as diverse as the EU, one uncovers discrepancies that prove to be typical of the varied utterances produced in collective security matters. For our purpose here, we have adopted a theoretical approach that weaves the study of terminology into certain notions of French discourse analysis (ADF). Terminology places the term itself, i.e. the lexical unit used in a specialised field of knowledge, at its very core. Our own study will therefore focus on the value which the lexicon acquires when uttered by a legitimate authority – the institution – in the specific area of security policy. Terminological research has gradually shown that like lexical units, terms are bound both to the context in which they are used and to the circumstances in which their surrounding discourse is produced. Variants in the expressions used, as these emerge from each sub-corpus, will thus depend on the linguistic and extra-linguistic environment surrounding use of the term. Against that background, we have set out to ascertain whether the variants might be symptomatic of discrepant ideological standpoints. In that sense, ADF, which has traditionally dealt with the ideologies underpinning language, lends us the notions one requires to grasp the reasons that may lie behind shifts in a given term. The methodological approach has allowed us to combine lexicometric analysis of the corpus with detailed, in-context observation of a term's inter-textual recurrences and terminographical sources. We set out our results further to a course of analysis which in reliance on deductive method, begins with selecting certain terms: “prévention”, “détection”, “répression”, “combattant terroriste étranger” et “criminalité transfrontalière”. The terms have been selected based upon upstream research into the literature of international relations, and typify the tensions that feed into academic, political and legal debate. In the first section, we examine the terms which pertain to strategic action (“prévention”, “détection”, “répression”), and in the second, we reflect on how the notions of a threat and the enemy have been conceptualised (“combattant terroriste étranger” and “criminalité transfrontalière”). In conclusion, our work has been designed to examine the mismatches and interpretative loopholes that are generated, as terms circulate and are put to the purpose of legitimising rhetorical practices. Our thesis will demonstrate that at the end of the day, institutional discourse in security matters has obscured a debate which is nonetheless very much alive, and a worthwhile topic of concern to a far wider public
The massive increase in the availability of data generated everyday by individuals on the Internet has made it possible to address the predictability of financial markets from a different perspective. Without making the claim of offering a definitive answer to a debate that has persisted for forty years between partisans of the efficient market hypothesis and behavioral finance academics, this dissertation aims to improve our understanding of the price formation process in financial markets through the use of Big Data analytics. Examining users' self-reported trading characteristics, the essay provides empirical evidence of sentiment-driven noise trading at the intraday level, consistent with behavioral finance theories. The second essay proposes a methodology to measure investor attention to news in real-time by combining data from traditional newswires with the content published by experts on the social media platform Twitter. The essay demonstrates that news that garners high attention leads to large and persistent change in trading activity, volatility, and price jumps. The third essay provides new insights into the empirical literature on small capitalization stocks market manipulation by examining a novel dataset of messages published on the social media plat-form Twitter. The essay proposes a novel methodology to identify suspicious behaviors by analyzing interactions between users and provide empirical evidence of suspicious stock recommendations on social media that could be related to market manipulation.
Its aim is to model the knowledge of the field of the balance of payments and the international investment position, i.e. the concepts and the terms denoting them. Natural language processing (NLP) tools and an ontoterminology editor will be used, and the work will imply interactions with domain experts. The ontoterminologies built will be exported in machine readable exchange formats, e.g. W3C. Ultimate goal of this thesis is to elaborate an e-dictionary which will facilitate knowledge sharing and transfer to non-experts.
With the rise of the number of sensors and actuators in an aircraft and the development of reliable data links from the aircraft to the ground, it becomes possible to improve aircraft security and maintainability by applying real-time analysis techniques. However, given the limited availability of on-board computing and the high cost of the data links, current architectural solutions cannot fully leverage all the available resources limiting their accuracy. Our goal is to provide a distributed algorithm for failure prediction that could be executed both on-board of the aircraft and on a ground station and that would produce on-board failure predictions in near real-time under a communication budget. In this approach, the ground station would hold fast computation resources and historical data and the aircraft would hold limited computational resources and current flight's data. In this thesis, we will study the specificities of aeronautical data and what methods already exist to produce failure prediction from them and propose a solution to the problem stated. Our contribution will be detailed in three main parts. First, we will study the problem of rare event prediction created by the high reliability of aeronautical systems. Many learning methods for classifiers rely on balanced datasets. Several approaches exist to correct a dataset imbalance and we will study their efficiency on extremely imbalanced datasets. Second, we study the problem of log parsing as many aeronautical systems do not produce easy to classify labels or numerical values but log messages in full text. We will study existing methods based on a statistical approach and on Deep Learning to convert full text log messages into a form usable as an input by learning algorithms for classifiers. We will then propose our own method based on Natural Language Processing and show how it outperforms the other approaches on a public benchmark. Last, we offer a solution to the stated problem by proposing a new distributed learning algorithm that relies on two existing learning paradigms Active Learning and Federated Learning. We detail our algorithm, its implementation and provide a comparison of its performance with existing methods
Neuroblastoma is the most frequent solid extra-cranial cancer of childhood. This cancer displays a high heterogeneity both at clinical and molecular levels. In order to answer this question, identification of populations coexisting at diagnosis and/or relapse in the patients which have relapsed is a prerequisite. This would allow, between other things, to study the pathways differently altered in clones that are specific to each time point. On these data, our method identified differences in the functional mutation rate, i.e. the number of putative functional variants by total number of variants, between the ancestral clones, clones expanding at relapse, and clones shrinking at relapse.
The objective of this thesis is to formalize a methodology aiming to validate the architecture of system of assistive tools installation making for piloting of electric wheelchair. We want to give again a sufficient beginning of mobility to the onboard person, by using experiments of navigations in virtual environments. So we have to conceive a system able to find the relevant improvements to bring to the electric wheelchair. We worked out an architecture based on cycles "Experiment?Analyze and decision-making? Modification of the wheelchair ". This architecture contains three modules: a simulator, an evaluator, a configurator. This work is a part of the VAHM (developed in LASC) project using commercial electric wheelchairs enhanced by several sensors and a calculator. Our simulator replaces the physical part of VAHM. From the analysis of the data resulting from the experiments, we have calculated criteria representative of behaviours or/and particular situations which have occurred during navigation. We indexed as robotics engineers, the whole of the problematic situations according to several aspects. We have identified and characterized each aspect by a whole of criteria. The vector of criteria constitutes the input of a decision-making system which will indicate at exit the functionalities to be installed to improve the mobility of the patient in powered wheelchair. The base of knowledge of the decision-making system is supported by reflections, concerning the behaviours and particular situations that a handicapped person will meet during navigation. The originality of our work comes from the handicap directed application allowing to evaluate and to highlight the difficulties of a group of people with severe motor disabilities, not able to move in real worlds, by using settings in virtual environments situations. The architecture of the system let to provide solutions of assistance using an evolutionary decision-making system based on fuzzy logic.
Nowadays, online social media has transformed the way we create, share and access information. These platforms rely on gigantic networks that promote the free exchange of information between hundreds of millions of people around the world, and this instantly. Whether related to a global event or in connection with a local event, these messages may influence a society and may contain information useful for the detection or prediction of real-world phenomena. However, some broadcast messages can have a very negative impact in real life. These messages containing false information can have disastrous consequences. To avoid and anticipate these dramatic situations, follow rumors, avoid bad reputations, it is necessary to study and then model the propagation of information. However, most of the diffusion models introduced are based on axiomatic hypotheses represented by mathematical models. As a result, these models are far removed from the users'dissemination behaviors in that they do not incorporate observations made on concrete dissemination cases. In our work, we study the phenomenon of diffusion of information at two scales. On a microscopic scale, we observed diffusion behaviors based on the personality traits of users by analyzing the messages they post in terms of feelings and emotions. On a macroscopic scale, we analyzed the evolution of the diffusion phenomenon by taking into account the geographical dimension of the users.
Sharing information between users constitutes the cornerstone of the Web 2.0. This raises concerns as this data can be exploited by several entities to breach user privacy. Our first contribution consists in demonstrating the privacy threats behind releasing personal information publicly. Two attacks are constructed to show that a malicious attacker (i.e., any external attacker with access to the public profile) can breach user privacy and even threaten his online security. Our first attack shows how seemingly harmless interests (e.g., music interests) can leak privacy-sensitive information about users. In particular, we infer their undisclosed (private) attributes using the public attributes of other users sharing similar interests. Leveraging semantic knowledge from Wikipedia and a statistical learning method, we demonstrated through experiments---based on more than 104K Facebook profiles---that our inference technique efficiently predicts attributes that are very often hidden by users. Our second attack is at the intersection of computer security and privacy. In fact, we show the disastrous consequence of privacy breach on security by exploiting user personal information---gathered from his public profile---to improve the password cracking process. First, we propose a Markov chain password cracker and show through extensive experiments that it outperforms all probabilistic password crackers we compared against. We propose a practical, yet formally proved, method to estimate the uniqueness of each profile by studying the amount of information carried by public profile attributes. To achieve our goal, we leverage Ads Audience Estimation platform and an unbiased sample of more than 400K Facebook public profiles. Our measurement results show that the combination of gender, current city and age can identify close to 55% of users to within a group of 20 and uniquely identify around 18% of them. In the second part of this thesis, we investigate the privacy threats resulting from the interactions between the OSN platform and external entities. Our findings indicate that OSN tracking is diffused among almost all website categories which allows OSNs to reconstruct a significant portion of users'web profile and browsing history. Finally, we develop a measurement platform to study the interaction between OSN applications---of Facebook and RenRen---and fourth parties. We show that several third party applications are leaking user information to ``fourth''party entities such as trackers and advertisers. This behavior affects both Facebook and RenRen with varying severity.
Machine Learning is widely utilized and tested in several fields. Such approaches show obvious potential for application in design and manufacturing. Prospective needs of such methods imply decision-making for manufacturing technology, optimization of design and manufacturing parameters as well as dynamic control of the manufacturing process. However, several challenging issues remain open in order to apply these techniques in those specific fields. Indeed, Machine Learning and other popular approaches (deep learning, convolutional neural networks) do require an extensive number of data to train the models. In design and manufacturing, efficient data collection is often impossible, especially in the early design steps where both part and product do not exist. Furthermore, even if the data collection is feasible, such models are usually trained for specific technologies and geometries. Hence, the generalization capability is limited for such type of models. More specifically, how to develop Machine Learning methods approaching draw near the human capabilities by using small number of cases in the design and manufacturing field? Answering these two questions is fundamental to apply Machine Learning in design and manufacturing. Solving these two research issues would lead to machine learning models embedding generalization capability for the design and manufacturing fields. Prospective and innovant applications would emerge naturally and include decision-taking process, dynamic optimization of process parameters and dynamic control. During this PhD thesis, several potential solutions will be explored. All of them considering meta-learning as a basis, with the aim to approach human performances in learning. One pillar of the research strategy will be the development of specialized metrics, models and optimization methods. Domain-based metrics able to grasp the knowledge in design and manufacturing will be utilized and can use the DACM method, a metamodeling approach developed by Tampere University. This method creates causal graphs as graphical representations of cause-effects relationships between the variables of the problem. DACM has been previously developed in a joint PhD thesis between Grenoble Alpes University and Tampere University. Potential utilization is envisioned through Bayesian Networks which can be considered as the following step after a causal graph generation. They allow to integrate and discover new correlations and provide a statistical representation of the causal model. Signal processing, recommender systems and natural language processing make extensive use of these techniques. They already repeatedly proved their performances in the Netflix annual competition. Grenoble Alpes university and Tampere University have conjointly developed and submitted a proof of concept using this approach and submitted in journal. During the joint PhD thesis, the goal is to explore these different approaches for applications in decision-making process, optimal selection of design and process parameters and dynamic control in manufacturing process. An ambitious publication plan in journals and conferences in the fields of design and manufacturing as well in informatics is planned. The joint PhD thesis is in form of a compilation of journal articles (four minimum).
Modelling spatial relationships is critical in a variety of applications of VREs, such as human learning environments, virtual museums, or navigation-aids systems. However, spatial relationships have been considered as abstract information and thus, difficult to specify. Addressing this issue, this thesis proposes an approach to model spatial relationships among virtual objects in VREs. First, we formalise a formal model of spatial relationships dedicated to VREs. Second, we provide a language and a framework to specify spatial relationships at a conceptual level. We claim that the proposed language is a relevant basis to specify spatial constraints related to activities of agents and users within VREs.
This thesis is focused on Graph Matching (GM) problems and in particular the Graph Edit Distance (GED) problems. However, these problems are known to be complex and hard to solve, as the GED is a NP-hard problem. Operations Research (OR) field offers a wide range of exact and heuristic algorithms that have accomplished very good results when solving optimization problems. So, basically all the contributions presented in thesis are methods inspired from OR field. The proposed heuristic approaches are adapted versions of existing MILP-based heuristics (also known as matheuristics), by considering problem-dependent information to improve their performances and accuracy.
One of the difficulties of an unresourced language is the lack of technology services in the speech and text processing. In this thesis, we faced the problematic of an acoustical study of the isolated and continous speech in Fongbe as part of the speech recognition. Tonal complexity of the oral and the recent agreement of writing the Fongbe led us to study the Fongbe throughout the chain of an automatic speech recognition. In addition to the collected linguistic resources (vocabularies, large text and speech corpus, pronunciation dictionaries) for building the algorithms, we proposed a complete recipe of algorithms (including algorithms of classification and recognition of isolated phonemes and segmentation of continuous speech into syllable), based on an acoustic study of the different sounds, for Fongbe automatic processing. In this manuscript, we also presented a methodology for developing acoustic models and language models to facilitate speech recognition in Fongbe. In this study, it was proposed and evaluated an acoustic modeling based on grapheme (since the Fongbe don't have phonetic dictionary) and also the impact of tonal pronunciation on the performance of a Fongbe ASR system. Finally, the written and oral resources collected for Fongbe and experimental results obtained for each aspect of an ASR chain in Fongbe validate the potential of the methods and algorithms that we proposed.
With the enormous amount of electronically available information on the internet, the development of information extraction technology becomes and more important. It is the linguist's task to create terminological databases and thesauri for this application. The research presented in this thesis is situated in the domain of Natural Language Processing (NLP), in which the lexicon plays a central role. Although this is an appropriate approach it is to some extent insufficient because it does not take into account the semantic of words and their ambiguity. For this, one needs to build dictionaries that relate syntax and semantics...
User profiling is essential for personalization systems (e.g. personalized information retrieval systems, recommendation systems) to identify user information (preference, interests...), in order to propose relevant content based on his/her specific needs and requirements. Many works have shown that user's social neighbors can be a meaningful source to infer his/her interests. The user's profile built within this approach is called "social profile". As user behaviors evolve over time, it is necessary to take into consideration the evolution of user interests in user profiling process. In the case of social profile, user interests are extracted from the information shared by his/her social neighbors. Hence, the evolution of extracted interests is related to the evolution of information shared on user social network and to the evolution of relationships between the user and his/her social neighbors. To handle this, we propose to apply a time-aware method into existing social profile building process (individual based and community based approaches). This strategy aims at weighting user's interests in the social profile based on their temporal score. The temporal score of an interest is computed by combining the temporal score of information used to extract the interests (computed by considering their freshness) with the temporal of individuals who share the information in the network (computed by considering the freshness of the interaction with the user). The technique and temporal function used to compute the temporal score are customizable. Thus, we can find out the most appropriate technique or temporal function depending on the types or characteristics of the adopted social network. The experiments conducted on DBLP and Twitter showed that the so-called time-aware social profiling process applying our proposed time-aware method outperforms the existing time-agnostic social profiling process. We also found that the most appropriate technique, temporal function and social profiling approach vary depending on the network characteristics (size, density) and to the social network type. Our approach opens many opportunities for future studies in social information filtering and many application domains as well as on the Web (e.g. evolution of social profile in personalization of search engines, recommender systems in e-commerce,).
This thesis focuses on the automatic construction of linguistic tools and resources for analyzing texts of low-resource languages. We propose an approach using Recurrent Neural Networks (RNN) and requiring only a parallel or multi-parallel corpus between a well-resourced language and one or more low-resource languages. This parallel or multi-parallel corpus is used to construct a multilingual representation of words of the source and target languages. We used this multilingual representation to train our neural models and we investigated both uni and bidirectional RNN models. We also proposed a method to include external information (for instance, low-level information from Part-Of-Speech tags) in the RNN to train higher level taggers (for instance, SuperSenses taggers and Syntactic dependency parsers). We demonstrated the validity and genericity of our approach on several languages and we conducted experiments on various NLP tasks: Part-Of-Speech tagging, SuperSenses tagging and Dependency parsing. Our approach has the following characteristics and advantages: (a) it does not use word alignment information, (b) it does not assume any knowledge about target languages (one requirement is that the two languages (source and target) are not too syntactically divergent), which makes it applicable to a wide range of low-resource languages, (c) it provides authentic multilingual taggers (one tagger for N languages).
The aim of my PhD thesis is the study of nominal designations of events for automatic extraction. My work is part of natural language processing, or in a multidisciplinary approach that involves Linguistics and Computer Science. The aim of information extraction is to analyze natural language documents and extract information relevant to a particular application. In this general goal, many information extraction campaigns were conducted: for each event considered, the task of the campaign is to extract some information (participants, dates, numbers, etc..). All these information are set around the event and the work does not care about the words used to describe the event (especially when it comes to a name). The event is seen as an all-encompassing as the quantity and quality of information that compose it. Unlike work in general information retrieval, our main interest is focused only on the way are named events that occur particularly in the nominal designation used. For us, this is the event that happens that is worth talking about. The most important events are the subject of newspaper articles or appear in the history books. An event can be evoked by a verbal or nominal description. In this thesis, we reflected on the notion of event. We observed and compared the different aspects presented in the state of the art to construct a definition of the event and a typology of events generally agree that in the context of our work and designations nominal events. We also released our studies of different types of training corpus of the names of events, we show that each can be ambiguous in various ways. For these studies, the composition of an annotated corpus is an essential step, so we have the opportunity to develop an annotation guide dedicated to nominal designations events. We studied the importance and quality of existing lexicons for application in our extraction task automatically. We also focused on the context of appearance of names to determine the eventness, for this purpose, we used extraction rules. Following these studies, we extracted an eventive relative weighted lexicon (whose peculiarity is to be dedicated to the extraction of nominal events), which reflects the fact that some names are more likely than others to represent events. Used as a tip for the extraction of event names, this weight can extract names that are not present in the lexicons existing standards. Finally, using machine learning, we worked on learning contextual features based in part on the syntax to extract event names.
In order to facilitate the engineers task of evaluating the fire safety level, and to allow the specialists involved in the field to use their preferred languages and tools, we propose to create a language dedicated to the field of fire safety, which automatically generates a simulation, taking into account the specific languages used by the specialists involved in the field. This DSL requires the definition, the formalization, the composition and the integration of several models, regardig to the specific languages used by the specialists involved in the field. The specific language dedicated to the field of fire safety is designed by composing and integrating several other DSLs described by technical and natural languages (as well as natural languages referring to technical ones). These latter are modeled in a way that their components must be precise and based on mathematical foundations, in order to verify the consistency of the system (people and materials are safe) before it implementation. In this context, we propose to adopt a formal approach, based on algebraic specifications, to formalize the languages used by the specialists involved in the generation system, focusing on both syntaxes and semantics of the dedicated languages. In the algebraic approach, the concepts of the domain are abstracted by data types and the relationships between them. The semantics of specific languages is described by the relationships, the mappings between the defined data types and their properties. The simulation language is based on a composition of several specific DSLs previously described and formalized. The DSLs are implemented based on the concepts of functional programming and the Haskell functional language, well adapted to this approach. The result of this work is a software dedicated to the automatic generation of a simulation, in order to facilitate the evaluation of the fire safety level to the engineers.
This thesis presents a model for automated left-right grammar checking based on analysis of a corpus of typescript errors. Studies in cognitive psychology have shown that the revision process works by confronting expectations with results. For humans, detecting a grammatical error therefore relies on an unfulfilled expectation on the part of the revisor. The model presented here is based on this principle. In order to deal with expectations from the point of view of computational processing, two common concepts in NLP are called upon: the unification principle and chunk segmentation. The former is particularly adapted to checking agreements, while the latter provides an intermediate computational unit to delimit, and therefore simplify, detection of grammatical inconsistencies. Finally, the model's originality lies in the left-right analysis it provides, which is constructed as the text is produced/read.
This thesis is situated in the context of logic-based Information Retrieval (IR) models. However, a study of current logic-based IR models shows that these models generally have some shortcomings. First, logic-based IR models normally propose complex, and hard to obtain, representations for documents and queries. Second, the retrieval decision d-&gt;q, which represents the matching between a document d and a query q, could be difficult to verify or check. Finally, the uncertainty measure U(d-&gt;q) is either ad-hoc or hard to implement. In this thesis, we propose a new logic-based IR model to overcome most of the previous limits. We represent documents and queries as logical sentences written in Disjunctive Normal Form. We also argue that the retrieval decision d-&gt;q could be replaced by the validity of material implication. We then exploit the potential relation between PL and lattice theory to check if d-&gt;q is valid or not. We first propose an intermediate representation of logical sentences, where they become nodes in a lattice having a partial order relation that is equivalent to the validity of material implication. Accordingly, we transform the checking of the validity of d-&gt;q, which is a computationally intensive task, to a series of simple set-inclusion checking. In order to measure the uncertainty of the retrieval decision U(d-&gt;q), we use the degree of inclusion function Z that is capable of quantifying partial order relations defined on lattices. Finally, our model is capable of working efficiently on any logical sentence without any restrictions, and is applicable to large-scale data. Our model also has some theoretical conclusions, including, formalizing and showing the adequacy of van Rijsbergen assumption about estimating the logical uncertainty U(d-&gt;q) through the conditional probability P(q|d), redefining the two notions Exhaustivity and Specificity, and the possibility of reproducing most classical IR models as instances of our model. We build three operational instances of our model. An instance to study the importance of Exhaustivity and Specificity, and two others to show the inadequacy of the term-independence assumption. Our experimental results show worthy gain in performance when integrating Exhaustivity and Specificity into one concrete IR model. The work presented in this thesis can be developed either by doing more experiments, especially about using relations, or by more in-depth theoretical study, especially about the properties of the Z function.
This study on segmental reduction (i.e. deletion or temporal reduction) in spontaneous French allows us to propose two research methods for linguistic studies on large corpora, to investigate different factors of variation and to bring new insights on the propensity of segmental reduction. We applied the descendant method using forced alignment with variants when it concerns a specific reduction phenomena. Otherwise, we used the ascendant method using absent and short segments as indicators. Three reduction phenomena are studied: schwa elision, /ʁ/ deletion and the propensity of segmental reduction. The descendant method was used for analyzing schwa elision and /ʁ/ deletion. Common factors used for the two studies are post-lexical context, speech style, sex and profession. Schwas elision at initial syllable position in polysyllabic words and post-consonantal /ʁ/ deletion at word final position are not always conditioned by the same variation factors. Similarly, lexical schwa and epenthetic schwa are not under the influence of the same variation factors. The study on the propensity of segmental reduction allows us to apply the ascendant method and to investigate segmental reduction in general. Results suggest that liquids and glides resist less the reduction procedure than other consonants and nasal vowels resist better reduction procedure than oral vowels. Among oral vowels, high rounded vowels tend to be reduced more often than other oral vowels.
Automatic identification of multiword expressions (MWEs) is a pre-requisite for many natural language processing applications. This task is challenging because MWEs, especially verbal ones (VMWEs) like to kick the bucket (which means to die), exhibit surface variability (no buckets were kicked). We address here a subproblem of VMWE identification, namely the identification of occurrences of VMWEs previously seen in corpora, whatever their surface form, which requires to take ambiguity into account to avoidliteral (he kicked the old bucket) or coincidental occurrences (he kicked the ball and the bucket fell down). To this end, we considered two main approaches: The first one is based on a language independent measure of VMWE variability. The second one consists in modeling the problem as a classification task on the basis of features relevant to the VMWE morphosyntactic variability, which led to a system (VarIDE) that participated in the PARSEME shared task on automatic identification of VMWEs in 2018.
This thesis is set within the context of Accordys, a knowledge engineering project aiming at providing a case-based reasoning system for fetopathology, i.e. the medical domain studying rare diseases and dysmorphia of fetuses. The project is based on a corpus of french fetal exam reports. This material consists in raw text reports diplaying a very specific vocabulary (only partially formalized in french medical terminologies), a "note taking" style that makes difficult to use tools analysing the grammar in the text, and a layout and formatting that shows a latent common structuration (organisation in sections, sub-sections, observations). Mapping a case with the model (instanciating the case model) is done through a Monte Carlo tree matching method. We compare this with similarity measurements obtained by representing our reports (both without further processing and after semantic enrichment through a semantic annotator) in a vector model.
This thesis tackles the problem of the automatic recognition of similes in literary texts written in English or in French and proposes a framework to describe them from a stylistic perspective. For the purpose of this study, a simile has been defined as a syntactic structure that draws a parallel between at least two entities, lacks compositionality and is able to create an image in the receiver's mind. Three main points differentiate the proposed approach from existing ones: it is strongly influenced by cognitive and linguistic theories on similes and comparisons, it takes into consideration a wide range of markers and it can adapt to diverse syntactic scenarios. Concretely speaking, it relies on three interconnected modules: - a syntactic module, which extracts potential simile candidates and identifies their components using grammatical roles and a set of handcrafted rules, - a semantic module which separates creative similes from both idiomatic similes and literal comparisons based on the salience of the ground and semantic similarity computed from data automatically retrieved from machine-readable dictionaries;-and an annotation module which makes use of the XML format and gives among others information on the type of comparisons (idiomatic, perceptual…) and on the semantic categories used. Finally, the two annotation tasks we designed show that the automatic detection of figuration in similes must take into consideration.
The aim of this thesis is to carry out a syntactic analysis of time adverbials in Korean which correspond to a time, a date or a duration (e. G. Ohu du si-ei (at two o'clock in the afternoon), 5uel 6il (on May 6th), and 3sigan dongan (for 3 hours)). For formal linguistics as applied in natural language processing, the most exhaustive and explicit description possible is essential. The 'lexicon-grammar'methodology (M. Gross 1975, 1986b) has provided us with a model for the formal and systematic description of natural language. We have described the lexical combinations concerned using graphs of finite-state automata, which constitute 'local grammars'representing the various types of adverbial sequences in as much detail as possible. Our graphs can be integrated directly into an automatic parser for the purpose of locating the Korean adverbs of time, date and duration in large corpora of texts. We devote chapter 3 to the analysis of forms which can be interpreted as duration, and chapter 4 to the analysis of forms which can be interpreted as date or time. We investigate how the temporal sequences studied in the second chapter can enter into sentences which allow them to be interpreted as expressing duration or date
A guided wave-based structural health monitoring (SHM) system aims at determining the integrity of a wide variety of plate-like structures, including aircraft fuselages, pipes, tanks etc. It relies on a sparse array of piezoelectric transducers for guided waves (GWs) excitation and sensing. This thesis presents studies conducted with the purpose of developing such a GWs-based SHM system that is capable of efficient defect detection, localization and sizing aeronautical plate-like structures made of aluminum and composite materials. This work also provides a comprehensive overview of DAS, MV and Excitelet defect imaging algorithms, determines their performance using statistical analysis of an extensive dataset of simulated guided waves imaging (GWI) results and proposes a method for sparse defect imaging. While defect detection and localization are straightforward from the image analysis, the defect sizing is a more complex problem due to its high dimensionality and non-linearity. It is demonstrated that this problem can be solved by means of machine learning methods, relying on an extensive database of simulated GWI results. They are efficient under stationary operational conditions but vulnerable to environmental variations, especially to the temperature fluctuation. Finally, this work presents studies on the robustness of GWI methods against thermal effects, and a defect detection model capable of analyzing deteriorated GWI results is proposed. Different techniques for thermal effects compensation are reviewed, and improvements are proposed. Their effectiveness is validated for aluminum plates but further improvements are required to translate these techniques to composite plates.
This thesis investigates the joint modeling of visual and textual content of multimedia documents to address cross-modal problems. Such tasks require the ability to match information across modalities. A common representation space, obtained by eg Kernel Canonical Correlation Analysis, on which images and text can be both represented and directly compared is a generally adopted solution. Nevertheless, such a joint space still suffers from several deficiencies that may hinder the performance of cross-modal tasks. An important contribution of this thesis is therefore to identify two major limitations of such a space. The first limitation concerns information that is poorly represented on the common space yet very significant for a retrieval task. The second limitation consists in a separation between modalities on the common space, which leads to coarse cross-modal matching. To deal with the first limitation concerning poorly-represented data, we put forward a model which first identifies such information and then finds ways to combine it with data that is relatively well-represented on the joint space. Evaluations on emph{text illustration} tasks show that by appropriately identifying and taking such information into account, the results of cross-modal retrieval can be strongly improved. The major work in this thesis aims to cope with the separation between modalities on the joint space to enhance the performance of cross-modal tasks. We propose two representation methods for bi-modal or uni-modal documents that aggregate information from both the visual and textual modalities projected on the joint space. Evaluations show that our approaches achieve state-of-the-art results on several standard and challenging datasets for cross-modal retrieval or bi-modal and cross-modal classification.
The aim of this thesis is part of the broad issue of information retrieval in Electronic Health Records (EHRs). The aspects tackled in this topic are numerous: on the one hand clinomics information retrieval within EHRs and secondly information retrieval within unstructured data from EHRs. As a first step, one of the objectives is to integrate in EHRs information beyond the scope of medicine to integrate data, information and knowledge from molecular biology ; omic data from genomics, proteomics or metabolomics. The integration of this type of data improves health information systems, their interoperability and the processing and exploitation of data for clinical purposes. An important challenge is to ensure the integration of heterogeneous data, through research on conceptual models of data, ontology and terminology servers, and semantic data warehouses. The integration of this data and their interpretation into a conceptual data model is an important challenge. Finally, it is important to integrate clinical research and fundamental research in order to ensure continuity of knowledge between research and clinical practice and to understand personalized medicine challenges. This thesis thus leads to the design and development of a generic model of omics data exploited in a prototype application for information retrieval and visualization in omic and clinical data within a sample of 2,000 patients. The second objective of this thesis is the multi-terminological indexing of medical documents through the development of the Extracting Concepts with Multiple Terminologies tool (ECMT). It uses terminologies embedded in the Health Terminology/Ontology Portal (HeTOP) to identify concepts in unstructured documents. From a document written by a human, and therefore potentially showing typing errors, spelling or grammar mistakes, the challenge is to identify concepts and thus structure the information contained in the text. In health information retrieval, indexing is of great interest for information retrieval in unstructured documents, such as reports and medical notes.
It consists of autonomous mobile nodes that communicate over bandwidth-constrained wireless links. Nodes in a MANET are free to move randomly and organize themselves arbitrarily. They can join/quit the network in an unpredictable way; such rapid and untimely disconnections may cause network partitioning. In such cases, the network faces multiple difficulties. Data replication is a possible solution to increase data availability. However, implementing replication in MANET is not a trivial task due to two major issues: the resource-constrained environment and the dynamicity of the environment makes making replication decisions a very tough problem. In this thesis, we propose a fully decentralized replication model for MANETs. This model is called CReaM: “Community-Centric and Resource-Aware Replication Model”. When the consumption of one resource exceeds a predefined threshold, replication is initiated with the goal of balancing the load caused by requests over other nodes. The data item to replicate is selected depending on the type of resource that triggered the replication process. The best data item to replicate in case of high CPU consumption is the one that can better alleviate the load of the node, i.e. a highly requested data item. Oppositely, in case of low battery, rare data items are to be replicated (a data item is considered as rare when it is tagged as a hot topic (a topic with a large community of interested users) but has not been disseminated yet to other nodes). To this end, we introduce a data item classification based on multiple criteria e.g., data rarity, level of demand, semantics of the content. To select the replica holder, we propose a lightweight solution to collect information about the interests of participating users. Users interested in the same topic form a so-called “community of interest”. Through a tags analysis, a data item is assigned to one or more communities of interest. Based on this framework of analysis of the social usage of the data, replicas are placed close to the centers of the communities of interest, i.e. on the nodes with the highest connectivity with the members of the community. The results of evaluating CReaM show that CReaM has positive effects on its main objectives.
This dissertation examines whether the different categorization processes shaping audiences' valuations in markets bring stability or variability to audiences' valuations. While seminal research on categorization emphasized the stabilizing role of market categories, recent research suggests that audiences' valuations can vary substantially even in markets which are well-structured by pre-existing categories. This variability notably results from audiences' heterogeneous preferences for typical offerings, from shifts in categories' meanings or from audiences' reliance on multiple models of valuation. Taking stock of these new results, this dissertation asks why audiences' valuations are so variable and explores in more details the role that market categories play in this phenomenon. This dissertation proposes that i) ambiguous categories, ii) the influence of temporary attractions among audiences alongside more stable categories and iii) the co-existence of different types of evaluators all contribute to produce variability in audiences' valuations. The first two empirical essays use data from publicly listed firms in the U.S. In these essays, firms' similarity to existing category prototypes or audiences' temporary attractions toward certain features are measured using semantics extracted from large corpora of annual reports and IPO prospectuses. The third essay is a theoretical model. This dissertation contributes to the literature on market categories, to the burgeoning research on optimal distinctiveness and to computational approaches to the study of organizations.
Two types of genre criteria are determined and used to define articles: genre-oriented morphosyntactic descriptors, and textual components and thematics. The partition is methodological, as the two levels constantly interfere. After a description of the main corpus, a contrastive analysis enables us to highlight extrinsic variation principles: personal styles, genre and domain variations and language impact. This quantitative and qualitative study finally provides a linguistic description of the different sides of a complex and multidimensionnel object, as well as a methodology to observe and characterize genres.
The difficult nature of Una meditación has been highlighted by both scholars and Juan Benet himself. This dissertation characterizes such a text complexity and thereby the singularity of the reading experience of Benet's novel. Our work relies on the psycholinguistics of reading comprehension. This framework allows us to achieve a definition of standard readability to which Una meditación is implicitly compared when judged as “difficult”. We study the two features that revealed to be the main sources of reading difficulty in Benet's text: the narrative and sentence structures, and the particular system of reference to the characters. At the level of the text structure, the narration and—at its own scale—the sentence are characterized by a strong discontinuity, however concealed; by a spiral temporal development; and by the scrambling of the hierarchy of the fictional events. At the level of character reference, the notion of name of the character loses its traditional meaning. Names are barely used or these are ambiguous, multiple, or inexistent. However it is above all the omnipresence of the pronominal reference that disconcerts the reader, implicitly imposing memorizing every detail of the text. We also analyze the figure of the narrator, and criticize a common reading of Benet's novel in which the text is the produce of a recollection. We conclude that the “difficulty” of Una meditación is the result of a writing that, by means of the indiscernibility of the characters and their stories, goes beyond literary fiction and aims at a generic portrait of human nature.
In summer 2013, the term "Big Data" appeared and attracted a lot of interest from companies. This thesis examines the contribution of these methods to actuarial science. It addresses both theoretical and practical issues on high-potential themes such as textit{Optical Character Recognition} (OCR), text analysis, data anonymization and model interpretability. Starting with the application of machine learning methods in the calculation of economic capital, we then try to better illustrate the boundary that may exist between automatic learning and statistics. Highlighting certain advantages and different techniques, we then study the application of deep neural networks in the optical analysis of documents and text, once extracted. The use of complex methods and the implementation of the General Data Protection Regulation (GDPR) in 2018 led us to study its potential impacts on pricing models. By applying anonymization methods to pure premium calculation models in non-life insurance, we explored different generalization approaches based on unsupervised learning. Finally, as regulations also impose criteria in terms of model explanation, we conclude with a general study of methods that now allow a better understanding of complex methods such as neural networks
The goal of the thesis is to evaluate new forms of Human-Computer Interaction. Thus, behaviours and reactions of users are collected using ergonomic methods and eye-tracking technologies. An experimental approach was adopted in order to evaluate the contribution of each part. For that, participants (student of bachelor's degree) handled a software of animation's creation which was unknown for them (Flash) in order to carry out three scenarios. Throughout their exploration of the software, participants were accompanied by a help system. In the first experiment an ECA (provided by FT R &amp; D) enunciate help messages; in the second one a adaptive system (detection of intention and evolution according to knowledge) was used. The various studies carried out show that the two innovations employed were perceived positively by the majority of the participants. They showed in addition that a ECA has a reassuring effect and that it can probably be used in first experiences with a software. For the adaptive system, the fact that the system evolves in an autonomous way did not disturb the participants, but hardly improves the performances.
In this thesis multi-agent argumentation debates are studied. Our work is motivated by the issues which are raised when a large number of users interact and debate on the Web, by exchanging arguments on various topics. These issues are raised on the levels of representing the debating users'knowledge, representing the debate, computing the debate's conclusions, evaluating the debate's quality, defining specific protocols for user interaction, and studying debate strategies which users employ in order to achieve particular goals. This thesis'contribution consists in: a) proposing a way to model a multi-agent argumentation debate where the participants have different types of expertise, and proposing a way to aggregate their opinions; b) offering support to the agent who is arbitrating a debate, proposing a way to evaluate the quality of a debate on the basis of how confident we can be on its conclusions, and proposing solutions for improving the quality of a debate which lacks definite conclusions; c) offering support to the debating agents in order to determine which arguments they should put forward, studying dynamic argumentation systems, studying the potential ways in which an agent can influence a dynamic argumentation system in order to achieve his goal, studying the minimal change allowing an agent to achieve his goal, studying several argumentation strategies based on minimal change; d) defining, studying and evaluating multi-agent argumentation protocols, defining protocols of different types (1) based on numerical argument evaluation and (2) based on argument extensions, using different techniques to ensure a debate's coherence while ensuring some liberty of expression to the agents, and finally performing an important number of experiments (on debates) in order to test various strategies and evaluate them with respect to specific criteria.
Multichannel sound recording and processing is a research field at Orange Labs. An ongoing thesis work concerns multichannel filtering with machine learning which shows the added value of neural networks to estimate a solid spatial enhancement filter. This present PhD thesis aims at using machine learning to provide needed information for the filtering: phrase beginning and end, and sound sources position over time. The idea is to couple ambisonics data and deep neural networks. The thesis will thus focus on 3 aspects of source localisation: estimation of the number of sources, estimation of the arrival directions and sources tracking over time.
We present the creation of two resources for Hungarian NLP applications: a rule-based shallow parser and a database of verbal subcategorization frames. Hungarian, as a non-configurational language with a rich morphology, presents specific challenges for NLP at the level of morphological and syntactic processing. While efficient and precise morphological analyzers are already available, Hungarian is under-resourced with respect to syntactic analysis. Our work aimed at overcoming this problem by providing resources for syntactic processing. Hungarian language is characterized by a rich morphology and a non-configurational encoding of grammatical functions. These features imply that the syntactic processing of Hungarian has to rely on morphological features rather than on constituent order. More concretely, we attempt to adapt current results in argument realization and lexical semantics to the task of labeling sentence constituents according to their syntactic function and semantic role in Hungarian. Within the syntax-semantics interface, the field of argument realization deals with the (partial or complete) prediction of verbal subcategorization from semantic properties. We claim that contrary to the widely shared presumption, adjuncts are often not fully productive. We therefore propose a gradual notion of productivity, defined in relation to Levin-type lexical semantic verb classes (Levin, 1993; Levin and Rappaport-Hovav, 2005).
This research investigates the worlds of contemporary French knowledge production in order to understand the different meanings of the term "open" in sciences. Specific attention has been drawn to the qualifying adjective open in relation to the french translations (ouvert, libre gratuit) as well as associated terms (science, data, access) with this formula. This inquiry began in 2013 and focused mainly on a specific event, the consultation on the bill for a "Digital Republic" (September-October 2015), in particular Article 9 on "open access to scientific publications in public research". This online consultation has allowed for a national and public scope to the issue of access to knowledges. As an "equipped" reality test via a participative website, arose the opportunity to observe almost "live" the defense of different conceptions of "what should be" the contemporary regime of knowledges in France. Through a grounded theory approach around this particular crystallisation moment of the debates on open in sciences has led me to gradually constitute a corpus of documents, reflecting the deployment of the exchanges on different digital spaces/apparatus (consultation website, scientific blogs, academic notebooks, mainstream press, etc.). Within an iterative research process, I combined digital methods (digital mapping of the similarity of votes) and qualitative analysis of the corpus, as well as the theoretical concepts mobilized at the crossroads between information and communication sciences and pragmatic sociology of critique. Subsequently, by switching from modeling to transposable theorization into other fields of research, I show how the distinction between two logics (technoindustrial or processual), behind the discourses on open can be relevant to analyze the current reconfigurations of other societal arrangements. The consultation by itself illustrates this point with the coexistence of two "digital" conceptions of democracy (extended representative or contributive), embodied in the design of the consultative platform. In the last part, I propose to explain the dynamics between the reconfiguration of a spirit and its social arrangement, by considering the permanent coupling between cognition, technologically mediated actions and socio-technical environment. Finally, the PhD experience narrated throughout this inquiry is also an example of an enaction process on my own conceptions of open. In this sense, it opens further reflections on the situated and incarnated nature of any production of knowledges, which escapes neither the limits nor the potentialities of metacognition.
The study uses and informational practices of users in information retrieval process is a major focus of research in Information and Communication Sciences, judging by the work on this issue and on the Internet. If the North has a strong tradition in this area, to the south by cons, specifically in Francophone countries of Africa in south of the Sahara, few studies have been devoted to this theme. This study falls within this thematic and geographical context and aims as public target doctoral students from the University of Bamako. This will be first to define a typology of this information users community (specialization, doctoral schools and laboratories attachment, thematic research, etc.) and then evaluate its information needs and practices finally, to identify the existence of possible access to information problems and propose possible solutions. For data collection a survey of doctoral students and observing their behavior information search situation will be favored. The results of the study will enable a better understanding of community needs and the informational practices of doctoral students from the University of Bamako and could be used by the units of information to improve the supply of information services them.
This study focuses on discursive representations of Japanese animation. In France, Japanese cartoons, known as anime, are now embedded in the cultural habits of many generations, but faced controversial beginnings when they first reached the country in the mid-1970. In Japan, the country where it was born, anime has long been a part of Japanese popular culture. Through this study, we aim to find out how anime, which are depicted as well established both in Japan and France, are perceived by young people in these two countries. We also seek to know if the culture of those two countries can exert influence on how it is described by them. The study was carried out in the form of a survey conducted among French and Japanese students, through a questionnaire in their respective languages. The discursive and semantic analysis of the surveyed participants' answers has enabled us to extract their representations about anime, but also about French and Japanese cultures. This was followed by a comparative analysis of our results which allowed us to confront French students' perspective with that of their Japanese counterparts.
The building of syntactically informative Arabic linguistic resources is a major issue for the development of new machine processing tools. We propose in this thesis to create an Arabic treebank that integrates a new type of information, which is based on the Property Grammar formalism. A syntactic property is a relation between two units of a given syntactic structure. We have thus been able to construct, using this grammar, other Arabic linguistic resources. In the learning model, we integrated a probabilistic lexicalized property grammar that may positively affect the parsing result and describe its syntactic structures with its properties. Finally, we evaluated the parsing results of this approach by comparing them to those of the Stanford Parser.
This thesis work focuses on a class of unsupervised, probabilistic deep learning methods that use variational inference to create high capacity, scalable models for time series modelling and analysis. We present two classes of variational deep learning, then apply them to two specific problems related to the maritime domain. The first application is the identification of dynamical systems from noisy and partially observed data. We introduce a framework that merges classical data assimilation and modern deep learning to retrieve the differential equations that control the dynamics of the system. Using a state space formulation, the proposed framework embeds stochastic components to account for stochastic variabilities, model errors and reconstruction uncertainties. The second application is maritime traffic surveillance using AIS data. We propose a multitask probabilistic deep learning architecture can achieve state-of-the-art performance in different maritime traffic surveillance related tasks, such as trajectory reconstruction, vessel type identification and anomaly detection, while reducing significantly the amount data to be stored and the calculation time. For the most important task—anomaly detection, we introduce a geospatial detector that uses variational deep learning to builds a probabilistic representation of AIS trajectories, then detect anomalies by judging how likely this trajectory is.
The Chronicles of Narnia (1950-1956) is a well-known collection of seven novels, usually seen as belonging to the genre of Children's literature and Fantasy. One of the main characteristics of the novels lies in their symbolic dimension, which evokes the Christian tradition and is expressed in the text through a second layer of meaning. Our thesis involves the analysis of a corpus including the English originals of The Chronicles of Narnia and their respective French translations, entitled Le Monde de Narnia (2005). The main markers which make the object of discussion are: deictics, modality, transitivity, lexical choice and semantic prosody. The discourse features related to these markers are analysed with respect to the narrative instance of the narrator, which has a key role in conveying the ideology of the text and which controls the focalization process. Our analysis draws particular attention to the sacred dimension in the texts, as well as to the themes of violence, death and gender in children's literature. Children's literature is usually characterised by an educational goal, and the Narnia books prove to be a powerful means to convey values within society, at a given moment in time. Our research reveals that the French translations tend to weaken the religious message of the original texts, distancing the reader or blurring space boundaries. Moreover, the ideology in the target texts is characterised by a number of discrepancies by comparison with the source texts; different values are given prominence, among those already present in the Narnia books. Using a method of analysis of translated texts, the thesis brings a contribution to the understanding of the challenges a translator may face when confronted with the task of translating ideology and point of view in books for children.
In Mexico, one of the priority technological problems is the preservation of cultural heritage in its digital form. In this research, the main interest is the ordering, management and identification of intangible cultural heritage in images. In computer vision, the integration of the Human Visual System (HVS) into automatic learning methods and classifiers has become an intensive research field for object recognition and content mining. The so-called saliency maps, are defined as a topographic representation of visual attention on a scene, modeling attention instantaneously and assigning a degree of interest to each pixel value on the image. Saliency maps proved to be very efficient to point out regions of interest in several tasks of visual content and its understanding. In this context, we focus on the integration of visual attention models in the training pipeline of Deep Neural Networks (DNNs) for the recognition of Mexican architectural structures. We consider the main contributions of this research are in the following areas of interest: • Specific purpose dataset: gathering data related to the topic is a key task to solve the problem of architectural classification. • Data selection: we use saliency prediction methods to select and crop context-relevant regions on images. • Visual attention modeling: we annotate images through a real task of image observation, we record eye-fixations with an eye-tracker system to build subjective saliency maps. • Visual attention integration: we integrate visual attention in deep neural networks in two ways; i) to filter out features in a saliency-based pooling layer and ii) in attention mechanisms. In this research, different essential components for the training of a neural network are tackled down with the aim of recognizing Mexican cultural content and extrapolating these findings to large-scale databases in similar classification tasks, such as in ImageNet. Finally, we show that the integration of visual attention models generated through a psycho-visual experiment allows to reduce training time and improve performances in terms of accuracy.
Pharmacovigilance is a fundamental discipline for safety and confidence in the medicines. This discipline has evolved over time, and has been strengthened, but still suffers from imperfections. We proposed through this work to provide an original solution for its improvement. In the first part, we describe and analyze the evolution of pharmacovigilance and its current functioning from both the legal and the scientific point of view and at both national and European levels. We analyzed the legal and practical weaknesses and have formulated proposals to address them. Then we review a number of possibilities before developing in the second part an original approach: pedagogy. Having established that the field pharmacovigilance is based on health professionals, we studied the provision of training of these professionals and offering to provide more qualitative and enough quantitative academic training in pharmacovigilance and iatrogenic, keeping in mind that we must rely on teaching methods and tools that should be adapted to current students. The proposed pedagogical paradigm is based on a pedagogy of explanation, on research in pharmaceutical law and a hybrid innovative pedagogy, combining face to face courses and e-learning resources. The tools used include lectures supplemented by formative assessments as tests done with electronic voting boxes and clinical cases in e-learning available on a platform. This education should lead students to a better understanding of pharmacovigilance and practical management of iatrogenic drug events. Consequently, the work done, allows us to better train, health professionals for drug risk management with an ultimate goal of strengthening the pharmacovigilance system. This will be complemented in the future by simulation and role-playing.
Nowadays, huge amounts of data relative to predictive maintenance are generated by massive industrial systems instrumentation, and are under-exploited. Therefore, developing algorithms that can continuously analyze these complex and heterogeneous data in order to predict faults and relative maintenance operations, is a major issue. Current machine learning approaches, and particularly deep learning-based ones - that yielded impressive results in computer vision and natural language processing fields - offer thus strong potential, and some first approaches have already been tested on specific maintenance use cases. This thesis aims at suggesting new data analysis approaches, learning a lot from data richness and available knowledge on monitorig and fault diagnosis.
This thesis presents a human-robot interaction (HRI) framework to classify large vocabularies of static and dynamic hand gestures, captured with wearable sensors. Static and dynamic gestures are classified separately thanks to the segmentation process. Experimental tests on the UC2017 hand gesture dataset showed high accuracy. Online classification of dynamic gestures allows successful predictive classification. The proposed network achieved a high accuracy on the rejection of untrained patterns of the UC2018 DualMyo dataset.
This construction is done by describing some of the reasonings taking place in the knowledge acquisition process and particularly the ones that allow to resolve the "associative anaphora". We define a knowledge representation model, having a linguistics basis and cognitif elements. In order to support this model, we propose an object oriented formalism, whose theoretical foundations are lesniewski's logical system: ontology and mereology. The first system relies upon a primitif functor called "epsilon" meaning "is-a", the second one upon the "part-of" relation called "ingredience". These logical systems constitute a more appropriate theoretical foundation than the traditional predicate calculus.
The aim of this thesis is to examine the morphosyntactic and semantic properties of abstract nouns related to verbal and adjectival predicates. From the assumption that the stative feature common to these nouns allows a unified analysis, we propose a study relying on the idea that stative nouns are distinguished by their uses, and show that, in addition to a purely stative meaning, these nouns can also convey other information, in which they denote occurrences. The second part is dedicated to the syntactic behaviour of stative nouns, i.e. number and determination, but also adjectival modification. This enables us to identify two distinct morphosyntactic behaviours, that parallel the distinction between stative and occurrence understanding highlighted in the first part. On the one hand, in their property sense, these nouns have a behaviour similar to that of massive concrete nouns and qualify as relational nouns, i.e. they require an argument with which they enter into a predication relationship (at the syntactic level). On the other hand, in their occurrence sense, these nouns behave like concrete count nouns and are not inherently relational. To sum up, the analysis of stative nouns shows that they share semantic properties with certain types of verbal and adjectival predicates, as well as syntactic properties with various classes of concrete nouns.
This thesis explores properties of estimations procedures related to aggregation in the problem of high-dimensional regression in a sparse setting. It benefits from strong results in fixed and random designs with a PAC-Bayesian approach. Chapter 2 analyses the statistical behaviour of the prediction loss of the EWA with Laplace prior in the fixed design setting. Chapter 4 explores the statisctical behaviour of adjusted versions of the Lasso for the transductive and semi-supervised learning task in the random design setting.
Nowadays, the need for very up to date authoritative spatial data has significantly increased. Thus, to fulfill this need, a continuous update of authoritative spatial datasets is a necessity. This task has become highly demanding in both its technical and financial aspects. In terms of road network, there are three types of roads in particular which are particularly challenging for continuous update: footpath, tractor and bicycle road. They are challenging due to their intermittent nature (e.g. they appear and disappear very often) and various landscapes (e.g. forest, high mountains, seashore, etc.).Simultaneously, GPS data voluntarily collected by the crowd is widely available in a large quantity. The number of people recording GPS data, such as GPS traces, has been steadily increasing, especially during sport and spare time activities. The traces are made openly available and popularized on social networks, blogs, sport and touristic associations'websites. However, their current use is limited to very basic metric analysis like total time of a trace, average speed, average elevation, etc. Particular attention is paid on roads that exist in reality but are not represented in authoritative datasets (missing roads). The approach we propose consists of three phases. The first phase consists of evaluation and improvement of VGI traces quality. The quality of traces was improved by filtering outlying points (machine learning based approach) and points that are a result of secondary human behaviour (activities out of main itinerary). Remained points are then evaluated in terms of their accuracy by classifying into low or high accurate (accuracy) points using rule based machine learning classification. The second phase deals with detection of potential updates. For that purpose, a growing buffer data matching solution is proposed. The size of buffers is adapted to the results of GPS point's accuracy classification in order to handle the huge variations in VGI traces accuracy. As a result, parts of traces unmatched to authoritative road network are obtained and considered as candidates for missing roads. Finally, in the third phase we propose a decision method where the “missing road” candidates should be accepted as updates or not. This decision method was made in multi-criteria process where potential missing roads are qualified according to their degree of confidence. Missing roads in IGN authoritative database BDTopo® were successfully detected and proposed as potential updates
This PhD thesis is part of the research project PERDIDO, which aims at extracting and retrieving displacements from textual documents. The objective of this PhD is to propose a method for establishing a processing chain to support the geoparsing and geocoding of text documents describing events strongly linked with space. We propose an approach for the automatic geocoding of itineraries described in natural language. Our proposal is divided into two main tasks. The first task aims at identifying and extracting information describing the itinerary in texts such as spatial named entities and expressions of displacement or perception. The second task deal with the reconstruction of the itinerary. Our proposal combines local information extracted using natural language processing and physical features extracted from external geographical sources such as gazetteers or datasets providing digital elevation models. The geoparsing part is a Natural Language Processing approach which combines the use of part of speech and syntactico-semantic combined patterns (cascade of transducers) for the annotation of spatial named entities and expressions of displacement or perception. The main contribution in the first task of our approach is the toponym disambiguation which represents an important issue in Geographical Information Retrieval (GIR). We propose an unsupervised geocoding algorithm that takes profit of clustering techniques to provide a solution for disambiguating the toponyms found in gazetteers, and at the same time estimating the spatial footprint of those other fine-grain toponyms not found in gazetteers. We propose a generic graph-based model for the automatic reconstruction of itineraries from texts, where each vertex represents a location and each edge represents a path between locations. To build automatically this graph-based representation of the itinerary, our approach computes an informed spanning tree on a weighted graph. Each edge of the initial graph is weighted using a multi-criteria analysis approach combining qualitative and quantitative criteria. Criteria are based on information extracted from the text and information extracted from geographical sources. For instance, we compare information given in the text such as spatial relations describing orientation (e.g., going south) with the geographical coordinates of locations found in gazetteers. Finally, according to the definition of an itinerary and the information used in natural language to describe itineraries, we propose a markup langugage for encoding spatial and motion information based on the Text Encoding and Interchange guidelines (TEI) which defines a standard for the representation of texts in digital form. Additionally, the rationale of the proposed approach has been verified with a set of experiments on a corpus of multilingual hiking descriptions (French, Spanish and Italian).
The management of knowledge and innovation are the themes to strong importance today, especially because these two topics are linked and they influence the performance of firms. The objective of this research is to analyze the link between knowledge management and innovation from three industries of simple product in the industrial pole of Barcarena city, Pará State, Brazil. We want to evaluate both the relationship of Knowledge Management in these companies and their innovative capacity, to understand the influence of knowledge for the innovative capacity, especially incremental innovation. For this, we will use models of analysis which take into account factors such as culture, leadership, technology, human resources and process. Our methodological approach is qualitative. We choose as basic concepts and theoretical literature about the Knowledge Management and Innovation. The axis of the industry was chosen by its economic importance in the region where the research has been developed. In addition, according to the results of the companies that better manage the knowledge have more possibiliter to innovate.
Digitalized music production exploded in the past decade. Huge amount of data drives the development of effective and efficient methods for automatic music analysis and retrieval. This thesis focuses on performing semantic analysis of music, in particular mood and genre classification, with low level and mid level features since the mood and genre are among the most natural semantic concepts expressed by music perceivable by audiences. In order to delve semantics from low level features, feature modeling techniques like K-means and GMM based BoW and Gaussian super vector have to be applied. In this big data era, the time and accuracy efficiency becomes a main issue in the low level feature modeling. Our first contribution thus focuses on accelerating k-means, GMM and UBM-MAP frameworks, involving the acceleration on single machine and on cluster of workstations. To achieve the maximum speed on single machine, we show that dictionary learning procedures can elegantly be rewritten in matrix format that can be accelerated efficiently by high performance parallel computational infrastructures like multi-core CPU, GPU. In particular with GPU support and careful tuning, we have achieved two magnitudes speed up compared with single thread implementation. Regarding data set which cannot fit into the memory of individual computer, we show that the k-means and GMM training procedures can be divided into map-reduce pattern which can be executed on Hadoop and Spark cluster. Our matrix format version executes 5 to 10 times faster on Hadoop and Spark clusters than the state-of-the-art libraries. Beside signal level features, mid-level features like harmony of music, the most natural semantic given by the composer, are also important since it contains higher level of abstraction of meaning beyond physical oscillation. Our second contribution thus focuses on recovering note information from music signal with musical knowledge. This contribution relies on two levels of musical knowledge: instrument note sound and note co-occurrence/transition statistics. In the instrument note sound level, a note dictionary is firstly built i from Logic Pro 9. With the musical dictionary in hand, we propose a positive constraint matching pursuit (PCMP) algorithm to perform the decomposition.
The objective of this thesis is to evaluate the interest of a new control interface for powered wheelchairs, a force feedback joystick, intended for people with severe motor disabilities which have difficulties to pilot their wheelchair in an usual way. This joystick will have to be implemented on a "smart" wheelchair provided with telemetric sensors. The force feedback is calculated according to the proximity of the obstacles and assists the user, without forcing him, to move towards the free direction. The first chapter of the report is a state of the art on the smart wheelchairs, on the control modes in teleoperation, on the haptic interfaces in robotics and on the modelling of piloting tasks. The second chapter describes the design of a simulator of wheelchair piloting intended to test new functionalities. The third and final chapter relates to a set of experimental results aiming at concluding on the interest of the force feedback for wheelchair piloting and on the choice of its calculation algorithm. The parameters tested are in particular the configuration of the environment (corridor, doors passages, free space …) and the kinematics of the wheelchair (front-wheel drive, rear-wheel drive)
How are English, Irish, Scottish, Welsh Kale and American Travellers represented through a few old gadji myths which die hard.
In the last few years, deep learning has changed irrevocably the field of computer vision. Faster, giving better results, and requiring a lower degree of expertise to use than traditional computer vision methods, deep learning has become ubiquitous in every imaging application. Thus this thesis first focused on the topic of hyper-parameter optimization for deep neural networks, i.e. methods for automatically finding efficient neural networks on specific tasks. The thesis includes a comparison of different methods, a performance improvement of one of these methods, Bayesian optimization, and the proposal of a new method of hyper-parameter optimization by combining two existing methods: Bayesian optimization and Hyperband. From there, we used these methods for medical imaging applications such as the classification of field-of-view in MRI, and the segmentation of the kidney in 3D ultrasound images across two populations of patients. This last task required the development of a new transfer learning method based on the modification of the source network by adding new geometric and intensity transformation layers. Finally this thesis loops back to older computer vision methods, and we propose a new segmentation algorithm combining template deformation and deep learning. The method is validated on the task of kidney segmentation in 3D US images.
In various domains, graphs represent a useful representation for many types of data. Prominent examples entail behavioural analyses performed in cybersecurity or social network analysis. In the former, internet user behaviour can be observed by monitoring DNS requests, interpreted as successive steps of a random walker on a graph in which nodes represent domain names and edges represent population-level average behaviour. Therefore studying user behaviour can be done by analyzing the subgraph induced by specific user movements. In the latter, graph representations naturally emerge from user interactions. For example nodes can represent users, and any relation between two users (messages or common interests) can be interpreted as edges. Understanding and analyzing graph structures appear to be a key tool in many real-world applications. It is thus essential to find efficient and robust methods for tasks such as node or graph classification. Coincidentally with the fact that graph neural network approaches have shown superior results in various domains, their robustness has been investigated and attacks have been developed in the context of node classification and graph classification (Zügner, Akbarnejad, and Günnemann 2018; H. Dai et al. 2018; L. Sun et al. 2018). With respect to privacy, a number of works have started to investigate how techniques for computing over encrypted data such as homomorphic cryptography (FHE) can be applied to the inference phase of deep neural network models with encouraging results when a clear-domain network is evaluated over an encrypted-domain input (Bourse et al. 2018; Chabanne et al. 2019; Chabanne et al. 2017; Dowlin et al. 2016).
The aim of this research is to develop a generalised descriptive repertory of french direct interrogative structures. The domain is centered around computational linguistics and information retrieval. In the linguistics section, transformation and types of transformation used for processing interrogatives are formally defined. The notational features which are valid throughout the research are then presented, followed by the individual transformations for the ten french interrogative elements (qui, quoi, que, pourquoi, combien, ou, quand, comment, quel, lequel) and inverted forms (est-il venu? que fait pierre? qui est-ce qui est venu?...). The implementation as a repertory aims at incorporating the descriptive linguistic constraints. Risif - our automatic french interrogative structures repertory, is programmed in fx, and is designed to code and diagnose interro- gative phrases. A data-base query system (cbd) with a linguistics component is the final issue. Cbd is used independently or in direct relation with risif. One can view and compare structures extracted from the data with those previously analysed (via risif). Data base modification and creation within the fx lisp environment is also possible.
However, the construction of such resources is costly. Therefore, they are only available for a limited number of languages and domains. Moreover, the choice of semantic formalization can differ according to the needs (SQL queries, logical formulas,...). In this context, it is necessary to develop methods that are less dependent on this supervision data, for example by exploiting linguistic resources. In the framework of this thesis, we are interested in developing joint approaches for semantic analysis and controlled generation of texts based on 'auto-encoder'neural architectures: a sentence is encoded into a latent representation (by semantic analysis) and then regenerated from this latent representation (controlled generation). We will focus on weakly supervised approaches using knowledge of the language, of the domain and of the targeted formalism rather than on the accumulation of annotated data.
Machine translation aims at automatically translating documents from one language to another without human intervention. With the advent of deep neural networks (DNN), neural approaches to machine translation started to dominate the field, reaching state-ofthe-art performance in many languages. Combined with the architectural flexibility of DNNs, this framework paved the way for further research in multimodality with the objective of augmenting the latent representations with other modalities such as vision or speech, for example. This thesis focuses on a multimodal machine translation (MMT) framework that integrates a secondary visual modality to achieve better and visually grounded language understanding. I specifically worked with a dataset containing images and their translated descriptions, where visual context can be useful forword sense disambiguation, missing word imputation, or gender marking when translating from a language with gender-neutral nouns to one with grammatical gender system as is the case with English to French. I propose two main approaches to integrate the visual modality: (i) a multimodal attention mechanism that learns to take into account both sentence and convolutional visual representations, (ii) a method that uses global visual feature vectors to prime the sentence encoders and the decoders. Through automatic and human evaluation conducted on multiple language pairs, the proposed approaches were demonstrated to be beneficial. Finally, I further show that by systematically removing certain linguistic information from the input sentences, the true strength of both methods emerges as they successfully impute missing nouns, colors and can even translate when parts of the source sentences are completely removed.
This thesis is dedicated to the problem of training and integration strategies of several modalities (visual, textual), in order to perform an efficient Visual Concept Detection and Annotation (VCDA) task, which has become a very popular and important research topic in recent years because of its wide range of application such as image/video indexing and retrieval, security access control, video monitoring, etc.
The human brain is made of a large number of interconnected neural networks which are composed of neurons and synapses. With a low power consumption of only few Watts, the human brain is able to perform computational tasks that are out of reach for today's computers, which are based on the Von Neumann architecture. Neuromorphic hardware design, taking inspiration from the human brain, aims to implement the next generation, non-Von Neumann computing systems. In this thesis, emerging non-volatile memory devices, specifically Phase-Change Memory (PCM) and Oxide-based resistive memory (OxRAM) devices, are studied as artificial synapses in neuromorphic systems. The use of PCM devices as binary probabilistic synapses is studied for complex visual pattern extraction applications, evaluating the impact of the PCM programming conditions on the system-level power consumption. A programming strategy is proposed to mitigate the impact of PCM resistance drift. It is shown that, using scaled devices, it is possible to reduce the synaptic power consumption. The OxRAM resistance variability is evaluated experimentally through electrical characterization, gathering statistics on both single memory cells and at array level. A model that allows to reproduce OxRAM variability from low to high resistance state is developed. An OxRAM-based convolutional neural network architecture is then proposed on the basis of this experimental work.
This thesis focuses on the automatic recognition of human stress during stress-inducing interactions (public speaking, job interview and serious games), using audio and visual cues. Part of this work is dedicated to the study of information fusion form those various modalities. Stress expression and coping are influenced both by interpersonal differences (personality traits, past experiences, cultural background) and contextual differences (type of stressor, situation's stakes). Inter-individual and inter-corpora comparisons highlight the variability of stress expression. A possible application of this work could be the elaboration of therapeutic software to learn stress coping strategies, particularly for social phobics.
The GenderedNews project aims to propose new methods for measuring and explaining the level of gender bias in the news media in France. These can be defined as the fact that the news media tend on the one hand to overweight men compared to women in terms of mentions and quotes, and on the other hand to attribute to women a specific social role often involving, among other things, anonymity, a reduced capacity for action in society and the confusion between this action and their marital or family status. Numerous empirical studies have proven the existence of these biases and have made it possible to better understand them on an international scale. However, research on this issue is often based on data limited in volume and produced by NGOs, administrations or media regulatory bodies. They generally give rise to manual content analyses that do not allow systematic account of changes in sexist biases in the media over the long term and on a large number of sources, nor explain these biases in terms of variables such as media funding, size of newsrooms or other organizational variables. The GenderedNews project aims to provide and analyze large and stable data sources over time as well as explore new methods for documenting gender bias in the media. It is based on a collaborative research scheme between a media sociologist and a computer scientist with skills in media studies, gender studies, natural language processing and digital data collection. It also has an important partnership dimension, with leading media being associated and providing access to data. GenderedNews focuses on two kinds of biases and two different measures of those biases. Sampling biases occur through the selection of a biased sample of people mentioned in the media. They can be studied by merely counting how many men and women access to public visibility on one side and by studying the framing patterns of the pictured men and women on the other side. Sourcing biases occur through the selection of a biased sample of people who, in addition to be visible, are allowed to express their views in the media. They can also be studied using the two approaches: counting how many on one side and analysing how on the other. Within the project, the PhD student will contribute more specifically to the study of sourcing biases.
The management of uncertainty is also an integral part of decision-making processes in the medical field. In the case of a medical incident during an air travel, this uncertainty includes three additional sources: (1) variability of the aeronautical conditions, (2) individual variability of the patient's conditions, (3) individual variability of the intervener's skills. Presently, medical incidents in the plane are estimated worldwide at 350 per day and when they occur, they are handled in 95 \% of cases by health professionals who are passengers. It is often for them a first experience. Apart from telemedicine with remote assistance, the intervener, often alone in the face of his doubts and uncertainty, has no other decision aid tool on board. Civil aviation also has feedback systems to manage the complexity of such processes. Event collection and analysis policies are put in place internationally, for example ECCAIRS (European Co-ordination Center for Accident and Incident Reporting Systems) and ASRS (Aviation Safety Reporting System). Finally, we propose a Clinical Decision Support System (CDSS) architecture that integrates the management of the uncertainties present on both the collected data and the skill levels of the medical professionals involved.
To achieve this goal, an information retrieval system (IRS) must represent, store and organize information, then provide to the user the elements corresponding to the need for information expressed by his query. Most of information retrieval systems (IRS) use simple terms to index and retrieve documents. However, this representation is not precise enough to represent the contents of documents and queries, because of the ambiguity of terms isolated from their context. A solution to this problem is to use multi-word terms to replace simple term. This approach is based on the assumption that a multi-word term is less ambiguous than a simple term. Our thesis is part of the information retrieval in Arabic specific domain. The objective of our work was on the one hand, identifying a multi-word terms present in queries and documents. On the other hand, exploiting the richness of language by combining several linguistic knowledge belonging at the morphological and syntax level, and showing how the contribution of syntactic and morphological knowledge helps to improve access to information. In addition, we have defined linguistically a multi-word term in Arabic and we developed a system of identification of multi-word terms which is based on a mixed approach combining statistical model and linguistic data
Several phenomenas cause source code duplication like inter-project copying and adaptation or cloning inside a same project. Looking for code matches allows to factorize them inside a project or to highlight plagiarism cases. We study statical similarity retrieval methods on source code that may be transformed via edit operations like insertion, deletion, transposition, in- or out-lining of functions. Sequence similarity retrieval methods inspired from genomics are studied and adapted to find common chunks of tokenized source. After an explanation on alignment and n-grams lookup techniques, we present a factorization method that merge function call graphs of projects to a single graph with the creation of synthetic functions modeling nested matches. It relies on the use of suffix indexation structures to find repeated token factors. Syntax tree indexation is explored to handle huge code bases allowing to lookup similar sub-trees with their hash values computed via heterogeneous abstraction profiles. Before and after match retrieval, we define similarity metrics to preselect interesting code spots, refine the search process or enhance the human understanding of results
The aim of this project is to explore and exploit document classification and natural language processing techniques through an implementation of a new approach based on multi-agent system that would answer a need for websites analysis and classification tasks at Olfeo.
Time and space representation is an important task in many domains such as natural language processing, geographic information systems (GIS), computer vision, robot navigation. Many qualitative approaches have been proposed to represent the spatial or temporal entities and their relations. The majority of these formalisms use qualitative constraints networks (QCNs) to represent information about a system. In some application, e. g. multi-agent systems, spatial or temporal information come from different sources, i. e. each source provides a spatial or temporal QCN representing relative positions between objects. The multiplicity of sources providing spatial or temporal information makes that the underlying QCNs are generally conflicting. Merging multiple sources information has attracted much attention in the framework of propositional logic. We take an inspiration from these works in order to define some merging process specified to QCNs, and study their logic and computational properties.
This thesis focuses on the linguistic expression of feelings and emotions in a corpus until now little questioned in this type of studies, in this case the forms of instantaneous communication made possible by a new technology and that seems predisposed to many expressive markers. It moves the cursor studies on the linguistic expression of these emotional categories of the system to employment, interviewing a corpus formed by four forms of communication: blogs, discussion forums, Facebook and Twitter. The work anchors reflection cognitively seeking to show, in a dynamic perspective, how to build this type of discourse in mediated interaction. It studies the different linguistic and extra-linguistic manifestations that load these electronic writings an emotional dimension opening on a large interactive dimension. It allows reflection on the written / oral borders and the construction of a new expressive language specific to electronic writings.
With the growth in Internet of Things, the realization of environments composed of diverse connected resources (devices, sensors, services, data, etc.) becomes a tangible reality. Creating these applications however goes hand-in-hand with the design of tools supporting the nomadic users roaming in these spaces, in particular by enabling the efficient selection of resources. While such a selection calls for the design of theoretically grounded descriptions, it should also consider the profile and preferences of the users. Finally, the rise of (possibly mobile) connected resources calls for designing a scalable process underlying this selection. Progress in the field is however sluggish especially because of the ignorance of the stakeholders (and the interactions between them) composing this eco-system of “IoT-enabled smart environments”. Thus, the multiplicity of diverse connected resources entails interoperability and scalability problems. While the Semantic Web helped in solving the interoperability issue, it however emphasizes the scalability one. Revolving from our research works performed over the last 6 years, this dissertation identifies the interactions between the stakeholders of the nascent ecosystem to further propose formal representations. The dissertation further designs a framework providing search capabilities to support the selection of connected resources through a semantic analysis. In particular, the framework relies on a distributed architecture that we design in order to manage scalability issues.
This thesis is about the design of a complete processing chain dedicated to unconstrained handwriting recognition. Three main difficulties are adressed: pre-processing, optical modeling and language modeling. The pre-processing stage is related to extracting properly the text lines to be recognized from the document image. An iterative text line segmentation method using oriented steerable filters was developed for this purpose. The difficulty in the optical modeling stage lies in style diversity of the handwriting scripts. Statistical optical models are traditionally used to tackle this problem such as Hidden Markov models (HMM-GMM) and more recently recurrent neural networks (BLSTM-CTC). Using BLSTM we achieve state of the art performance on the RIMES (for French) and IAM (for English) datasets. The language modeling stage implies the integration of a lexicon and a statistical language model to the recognition processing chain in order to constrain the recognition hypotheses to the most probable sequence of words (sentence) from the language point of view. The difficulty at this stage is related to the finding the optimal vocabulary with minimum Out-Of-Vocabulary words rate (OOV). Enhanced language modeling approaches has been introduced by using sub-lexical units made of syllables or multigrams. The sub-lexical units cover an important portion of the OOV words. Otherwise equivalent performances are obtained with a compact sub-lexical language model. Thanks to the compact lexicon size of the sub-lexical units, a unified multilingual recognition system has been designed.
In recent years, Named Data Networking (NDN) has emerged as one of the most promising future networking architectures. To be adopted at Internet scale, NDN needs to resolve the inherent issues of the current Internet. Assuming that (i) a computer is located in the enterprise network that is based on an NDN architecture, (ii) the computer has already been compromised by suspicious media such as a malicious email, and (iii) the company installs a firewall connected to the NDN-based future Internet, this thesis focuses on a situation that the compromised computer (i.e., malware) attempts to send leaked data to the outside attacker. The contributions of this thesis are fivefold. Firstly, this thesis proposes an information leakage attack through a Data and through an Interest in NDN. Secondly, in order to address the information leakage attack, this thesis proposes an NDN firewall which monitors and processes the NDN traffic coming from the consumers with the whitelist and blacklist. Thirdly, this thesis proposes an NDN name filter to classify a name in the Interest as legitimate or not. To take traffic flow to the NDN firewall from the consumer into account, fourthly, this thesis proposes an NDN flow monitored at an NDN firewall. Fifthly, in order to deal with the drawbacks of the NDN name filter, this thesis proposes an NDN flow filter to classify a flow as legitimate or not. The performance evaluation shows that the flow filter complements the name filter and greatly chokes the information leakage throughput
The current dissertation presents an ERP-investigation of metrical stress processing in French. Indeed, while metrical stress is well known to play an invaluable role in speech comprehension, its functions in French speech processing are unclear. French is a language traditionally described as having no accent. This dissertation questions the traditional view and aligns to two metrical models on French accentuation, which propose stress to be encoded in cognitive templates underlying the abstract representation of the word. In our interdisciplinary investigation of metrical stress processing in French, we take a functional, yet metrically rooted, approach. We use the method of Event-Related Potentials (ERP), which provides us with a highly sensitive and temporally precise measure allowing us to determine whether there is metrical stress in French, and to what extent metrical stress aids the listener in speech comprehension.
Because of the rise of the social Web, both the research and the industry are interested in automatic processing of opinions in text. In this work, we assume a multilingual and multidomain environment and aim at automatic and adaptive polarity classification. We propose a method for automatic construction of multilingual affective lexicons from microblogging to cover the lack of lexical resources. We propose a text representation model based on dependency parse trees to replace a traditional n-grams model. In our model, we use dependency triples to form n-gram like features. We believe this representation covers the loss of information when assuming independence of words in the bag-of-words approach. Finally, we investigate the impact of entity-specific features on classification of minor opinions and propose normalization schemes for improving polarity classification. The effectiveness of our approach has been proved in experimental evaluations that we have performed across multiple domains (movies, product reviews, news, blog posts) and multiple languages (English, French, Russian, Spanish, Chinese) including official participation in several international evaluation campaigns (SemEval'10, ROMIP'11, I2B2'11).
This dissertation studies the French adverb autrement, through its three main uses: adverb of manner, connective denoting negative hypothesis, and topic shifter. The importance of anaphora resolution and discourse structure is stressed. After a review of the literature on discourse structure and on the adverb, the characteristics of the three uses are defined thanks to spoken and written corpora, showing how context is instrumental in retrieving the antecedent and how the adverb relies on discourse and builds it at the same time. Already in the adverb of manner, anaphor and right scope are crucial in the construction of meaning. With the connective, referential relations leave room for logical relations holding from proposition to proposition, whereas the topic shifter is a metalinguistic use handling abstract discourse entities. A core of meaning [anaphor and negation] is identified, common to the three uses and accounting for bridges between them. This synchronic study is then used to reconstruct the adverb's grammaticalization, detailed observation in the present counterbalancing sparse historical data. It is shown that the notion of a construction, i.e. the use of the adverb in some context, has made evolution possible: in particular, word order in Old French was crucial, allowing the adverb of manner to occupy the initial position where reanalysis could occur ; the use of conjunctions also favored the emergence of some of the adverb's meanings.
Augmented surgery uses medical apparatus (Augmented Surgery Devices or ASD) allowing the surgeon to improve their orientation, and thus enhance the surgical environment to facilitate carrying out their actions. The development of these devices, their proliferation and the media exposure they receive, has led the government to question the quality associated with interventions assisted by these apparatus. In this paper, we illustrate the issues of quality associated with ASD-assisted interventions through a chronological description of the first active medical robot used for fitting total hip replacements. We then discuss the notion of quality in medicine in general and finally the quality of ASD in particular. We will see that there are no specific provisions for these devices and it doesn't seem appropriate to speak of the quality of an ASD without taking into account the environment in which it is used. This is why it is crucial to structure the use of these devices as well as the environment in which they are used. One way to structure this environment is to use ontologies. Using the ontology editing function of the ISIS software, we modeled surgery, as well as the associated environment, for ligament insufficiency of the anterior cruciate ligament with and without ASD. This ontological representation consists of a set of 45 Ontological Diagrams (OD) having a total of 1072 concepts. We describe the materials and methods used to build all of these diagrams. To speak of the quality of ASD, a user can create their information system from our ontological model in order to have their own metrics. The validation of our structural model was carried out by an expert through a scenario of surgery, created from the ontological model. Finally we discuss the possible prospects for our work.
Determinantal point processes (DPPs) generate random configuration of points where the points tend to repel each other. The notion of repulsion is encoded by the sub-determinants of a kernel matrix, in the sense of kernel methods in machine learning. This special algebraic form makes DPPs attractive both in statistical and computational terms. This thesis focuses on sampling from such processes, that is on developing simulation methods for DPPs. Applications include numerical integration, recommender systems or the summarization of a large corpus of data. In the finite setting, we establish the correspondence between sampling from a specific type of DPPs, called projection DPPs, and solving a randomized linear program. In this light, we devise an efficient Markov-chain-based sampling method. In the continuous case, some classical DPPs can be sampled by computing the eigenvalues of carefully randomized tridiagonal matrices. We provide an elementary and unifying treatment of such models, from which we derive an approximate sampling method for more general models. In higher dimension, we consider a special class of DPPs used for numerical integration. We implement a tailored version of a known exact sampler, which allows us to compare the properties of Monte Carlo estimators in new regimes. In the context of reproducible research, we develop an open-source Python toolbox, named DPPy, which implements the state of the art sampling methods for DPPs
In this thesis, we propose Triangular Similarity Metric Learning (TSML) for automatically specifying a metric from data. A TSML system is loaded in a siamese architecture which consists of two identical sub-systems sharing the same set of parameters. Each sub-system processes a single data sample and thus the whole system receives a pair of data as the input. The TSML system includes a cost function parameterizing the pairwise relationship between data and a mapping function allowing the system to learn high-level features from the training data. In terms of the cost function, we first propose the Triangular Similarity, a novel similarity metric which is equivalent to the well-known Cosine Similarity in measuring a data pair. Based on a simplified version of the Triangular Similarity, we further develop the triangular loss function in order to perform metric learning, i.e. to increase the similarity between two vectors in the same class and to decrease the similarity between two vectors of different classes. Compared with other distance or similarity metrics, the triangular loss and its gradient naturally offer us an intuitive and interesting geometrical interpretation of the metric learning objective. In terms of the mapping function, we introduce three different options: a linear mapping realized by a simple transformation matrix, a nonlinear mapping realized by Multi-layer Perceptrons (MLP) and a deep nonlinear mapping realized by Convolutional Neural Networks (CNN). With these mapping functions, we present three different TSML systems for various applications, namely, pairwise verification, object identification, dimensionality reduction and data visualization. For each application, we carry out extensive experiments on popular benchmarks and datasets to demonstrate the effectiveness of the proposed systems.
The thesis presents a study of thirteen indefinite quantifiers in Mandarin Chinese noun phrases. Chapter 2 provides an overview of some saillant characteristics of Chinese noun phrases. In Chapter 3, the distributional properties of thirteen indefinite quantifiers. In Chapter 4, the quantifier function of yīdiǎnr 'a little/un peu' is opposed to that of the minimizer yīdiǎnr 'the least/le moindre'.
This research formulates some thoughts that are essential to initiate students, non native speakers, to academic writing and help in mastering it. Several questions have guided this study: What role might have the descriptive studies of scientific writing in a successful familiarization with academic writing? What is the point of an introduction to rhetorical functions based on transdisciplinary phraseology and on a so-called genre approach? Is it possible to submit support elements that would be beneficial to all students regardless of their disciplines? An exploratory study of a particular rhetorical function that is "positioning" has allowed us to understand the extent to which linguistic elements, namely transdisciplinary collocations could help students to less apprehend the requirement of an essentially polyphonic and argumentative writing or to further show their positioning in their academic writings.
Crowdsourcing has proved its ability to address large scale data collection tasks at a low cost and in a short time. However, due to the dependence on unknown workers, the quality of the crowdsourcing process is questionable and must be controlled. Indeed, maintaining the efficiency of crowdsourcing requires the time and cost overhead related to this quality control to stay low. Current quality control techniques suffer from high time and budget overheads and from their dependency on prior knowledge about individual workers. In this thesis, we address these limitation by proposing the CAWS (Context-Aware Worker Selection) method which operates in two phases: in an offline phase, the correlations between the worker declarative profiles and the task types are learned. Then, in an online phase, the learned profile models are used to select the most reliable online workers for the incoming tasks depending on their types. Using declarative profiles helps eliminate any probing process, which reduces the time and the budget while maintaining the crowdsourcing quality. In order to evaluate CAWS, we introduce an information-rich dataset called CrowdED (Crowdsourcing Evaluation Dataset). The generation of CrowdED relies on a constrained sampling approach that allows to produce a dataset which respects the requester budget and type constraints. Through its generality and richness, CrowdED helps also in plugging the benchmarking gap present in the crowdsourcing community. Using CrowdED, we evaluate the performance of CAWS in terms of the quality, the time and the budget gain. Results shows that automatic grouping is able to achieve a learning quality similar to job-based grouping, and that CAWS is able to outperform the state-of-the-art profile-based worker selection when it comes to quality, especially when strong budget ant time constraints exist. Finally, we propose CREX (CReate Enrich eXtend) which provides the tools to select and sample input tasks and to automatically generate custom crowdsourcing campaign sites in order to extend and enrich CrowdED.
Exploring unexploited but newly digitized resources to find relevant information is a complicated task due to the amount of available resources. Thanks to the ANR project CIRESFI, the most important resource for the Italian Comedy of the 18th century, is a set of accounting registers consisting of 28,000 pages. Information retrieval is a long and complex process that requires expertise at every step: detection and segmentation in paragraphs, lines or words, features extraction, handwriting recognition. Systems based on deep neural networks dominate these approaches. The major issue is the need of a large amount of data to achieve their learning. However, the registers of the Italian Comedy have no ground truth. To overcome this lack of data, we explore approaches that involving transfer learning. That means using heterogeneous labeled and available data, with at least one common feature with our data to drive the systems, and then applying them to our data. All of our experiments have shown us the difficulty of carrying out this task, each choice at each stage having a strong impact on the rest of the system. We converge on a solution separating the optical model from the language model in order to achieve independent learning with different available resources and joining together thanks to a projection of the information into a non-latent common space.
XML has become the de facto format for data exchange. We aim at establishing a multi-system environment where some local original systems work in harmony with a global integrated system, which is a conservative evolution of local ones. Data exchange is possible in both directions, allowing activities on both levels. We propose a set of tools to help dealing with XML database evolution. Experimental results are discussed, showing the efficiency of our methods in many situations.
This thesis is an evaluation of the possibilities of automatising a morphological analysis of the Russian words. This analysis is submitted to two major constraints: 1) it can rely only on the knowledge of the word itself and not on the context from which it is extracted, 2) no stem is known a priori, with the exception of the roots forming part of: - homonym derivatives, - derivatives whose segmentation cannot be controlled by simple rules. An important part of this work consists in defining factors leading to an appreciable reduction of this set of stems. This analysis is done with the aid of: three sets of morphemes (prefixes, suffixes and endings) and the two forms of the reflexive pronoun, rules of recognition of foreign words, morphological incompatibilities. The analysis produces grammatemes (sort of identification sheet of the word) of the analysed word, as reduced as possible. The reduction of this grammateme is the result of the intersection of the sets of information bound to each one of the morphemic elements forming the word to be analysed.
This thesis addresses the challenge of designing urban mobility systems. It aims at developing a traveler experience model to help diagnose travel problems in a design approach and improve the relevance of transportation models for travelers. By combining the views of user-experience design and transportation, it helps to deepen the understanding of how travelers experience their journey and especially the problems they face. The first axis of investigation is related to the modeling of the traveler experience to feed a relevant and rich diagnosis of travel problems. In the second axis, travelers are involved, through a grounded theory approach, to identify the problems they encounter when using urban mobility systems, using appropriate stimuli. The third axis introduces travel subjective attributes into transport models to improve their accuracy. This research used action research as a methodology. It combines literature review in design and transportation disciplines, four field observations, fifteen in-depth interviews with transport travelers and experts, five problem-solving workshops, and two experiments, in a cyclical improvement of results. The various uses of the model have led to an in-depth diagnosis of three urban mobility systems (suburban train, on-demand bus, dedicated shuttle) and the development of traveler-centric attributes for an optimization model and a multi-agent simulation that was tested by a survey of over 450 participants.
Machine translation (MT) has increasingly become an indispensable tool for decoding the meaning of a text from a source language into a target language in our current information and knowledge era. In particular, MT of proper names (PN) plays a crucial role in providing the specific and precise identification of persons, places, organizations, and artefacts through the languages. Despite a large number of studies and significant achievements of named entity recognition in the NLP community around the world, there has been almost no research on PNMT for Vietnamese language. Due to the different features of PN writing, transliteration or transcription and translation from a variety of languages including English, French, Russian, Chinese, etc. into Vietnamese, the PNMT from those languages into Vietnamese is still challenging and problematic issue. This study focuses on the problems of English-Vietnamese and French-Vietnamese PNMT arising from current MT engines. First, it proposes a corpus-based PN classification, then a detailed PNMT error analysis to conclude with some pre-processing solutions in order to improve the MT quality. Through the analysis and classification of PNMT errors from the two English-Vietnamese and French-Vietnamese parallel corpora of texts with PNs, we propose solutions concerning two major issues: (1)corpus annotation for preparing the pre-processing databases, and (2)design of the pre-processing program to be used on annotated corpora to reduce the PNMT errors and enhance the quality of MT systems, including Google, Vietgle, Bing and EVTran. The efficacy of different annotation methods of English and French corpora of PNs and the results of PNMT errors before and after using the pre-processing program on the two annotated corpora are compared and discussed in this study. They prove that the pre-processing solution reduces significantly PNMT errors and contributes to the improvement of the MT systems' for Vietnamese language.
The general goal of the work to be carried out is the delivery of flexible and robust methods for merging open-domain knowledge.
Organic electronics is a field of research dealing with the development of new technologies based on organic semiconductor materials (OSCs). In general, two approaches are used for the molecular design of OSCs. The first approach consists in assembling efficient molecular fragments, in order to synthesize functional materials for a specific application such as phosphorescent organic light-emitting diodes (PhOLEDs). The second approach is more risky as it aims to develop new molecular fragments which may have one or several desired properties for a given application. In this thesis work, both approaches have been developed. On the one hand, we have developed host materials for PhOLEDs by adjusting their properties (first approach), and, on the other hand, we have been interested in a new generation of OSCs: molecular nanorings (second approach). This work has enabled to reach the, green and blue PhOLEDs displaying the highest overall performances ever reported in literature. This work allowed us to incorporate for the first time molecular nanorings in organic field-effect transistors in order to study their transport properties.
This thesis is part of a study that explores automatic transcription potential for the instrumentation of educational situations. Our contribution covers several axes. First, we describe the enrichment and the annotation of COCo dataset that we produced as part of the ANR PASTEL project. In this multi-thematic framework, we are interested in the problem of the linguistic adaptation of automatic speech recognition systems (ASR). The proposed language model adaptation is based both on the lecture presentation supports provided by the teacher and in-domain data collected automatically from the web. Thus, we proposed two evaluation protocols. The first one deals with an intrinsic evaluation, making it possible to estimate performance only for domain words of each lecture (IWER_Average). The second protocol offers an extrinsic evaluation, which estimates the performance for two tasks exploiting transcription: information retrieval and indexability. As LM adaptation is based on a collection of data from the web, we study the reproducibility of language model adaptation results by comparing the performances obtained over a long period of time. Over a collection period of one year, we were able to show that, although the data on the Web changed in part from one month to the next, the performance of the adapted transcription systems remainedconstant (i.e. no significant performance changes), no matter the period considered. Finally, we are intersted on thematic segmentation of ASR output and alignment of slides with oral lectures. For thematic segmentation, the integration of slide's change information into the TextTiling algorithm provides a significant gain in terms of F-measure. For alignment of slides with oral lectures, we have calculated a cosine similarity between the TF-IDF representation of the transcription segments andthe TF-IDF representation of text slides and we have imposed a constraint torespect the sequential order of the slides and transcription segments.
Existing methods for rhythmic analysis typically focus on one of those levels, failing to exploit music's rich structure and compromising the musical consistency of automatic estimations. In this work, we propose novel approaches for leveraging multi-scale information for computational rhythm analysis. Our models account for interrelated dependencies that musical audio naturally conveys, allowing the interplay between different time scales and accounting for music coherence across them. Our methods are systematically evaluated on a diverse group of datasets, ranging from Western music to more culturally specific genres, and compared to state-of-the-art systems and simpler variations. The overall results show that our models for downbeat tracking perform on par with the state of the art, while being more musically consistent. Moreover, our model for the joint estimation of beats and microtiming takes further steps towards more interpretable systems. The methods presented here offer novel and more holistic alternatives for computational rhythm analysis, towards a more comprehensive automatic analysis of music.
In the recent years, deep learning has become the leading approach to modern artificial intelligence (AI). The important improvement in terms of processing time required for learning AI based models alongside with the growing amount of available data made of deep neural networks (DNN) the strongest solution to solve complex real-world problems. The natural multidimensionality of the data is elegantly embedded within complex and hypercomplex neurons composing the model. In particular, quaternion neural networks (QNN) have been proposed to deal with up to four dimensional features, based on the quaternion representation of rotations and orientations. Unfortunately, and conversely to complex-valued neural networks that are nowadays known as a strong alternative to real-valued neural networks, QNNs suffer from numerous limitations that are carrefuly addressed in the different parts detailled in this thesis. The thesis consists in three parts that gradually introduce the missing concepts of QNNs, to make them a strong alternative to real-valued NNs. The first part introduces and list previous findings on quaternion numbers and quaternion neural networks to define the context and strong basics for building elaborated QNNs. The second part introduces state-of-the-art quaternion neural networks for a fair comparison with real-valued neural architectures. More precisely, QNNs were limited by their simple architectures that were mostly composed of a single and shallow hidden layer. In this part, we propose to bridge the gap between quaternion and real-valued models by presenting different quaternion architectures. First, basic paradigms such as autoencoders and deep fully-connected neural networks are introduced. In a conventional QNN scenario, input features are manually segmented into three or four components, enabling further quaternion processing. Unfortunately, there is no evidence that such manual segmentation is the representation that suits the most to solve the considered task. Morevover, a manual segmentation drastically reduces the field of application of QNNs to four dimensional use-cases. Therefore the third part introduces a supervised and an unsupervised model to extract meaningful and disantengled quaternion input features, from any real-valued input signal, enabling the use of QNNs regardless of the dimensionality of the considered task. Conducted experiments on speech recognition and document classification show that the proposed approaches outperform traditional quaternion features.
Choosing appropriate database management systems (DBMS) and/or execution platforms for given database (DB) is complex and tends to be time-and effort-intensive since this choice has an important impact on the satisfaction of non-functional requirements (e.g., temporal performance or energy consumption). Indeed, a large number of tests have been performed for assessing the quality of developed DB. This assessment often involves metrics associated with non-functional requirement. That leads to a mine of tests covering all life-cycle phases of the DB's design. Tests and their environments are usually published in scientific articles or specific websites such as Transaction Processing Council (TPC). Therefore, this thesis bas taken a special interest to the capitalization and the reutilization of performed tests to reduce and mastery the complexity of the DBMS/platforms selection process. By analyzing the test accurately, we identify that tests concem: the data set, the execution platform, the addressed non-functional requirements, the used queries, etc. Thus, we propose an approach of conceptualization and persistence of all dimensions as well as the results of tests. Conseguently, this thesis leads to the following contributions. (1) The design model based on descriptive, prescriptive and ontological concepts to raise the different dimensions. (2) The development of a multidimensional repository to store the test environments and their results. (3) The development of a decision making methodology based on a recommender system for DBMS and platforms selection.
In Natural Language Processing, there is many works in Word Sense Disambiguation (WSD). It's explained in two points: first, this task is used in many application software, second, there is no agreement for the method. We present a new model based on theory of dynamic construction of meaning. We develop a new type of semantic classes of lexical units which depend of context.
Information retrieval and decision support systems need fast and accurate access to the content of documents and eﬃcient medical knowledge processing. Indexing (describing using keywords) enables access to knowledge and knowledge processing. In the medical domain, an increasing number of resources are available in electronic format, and there is a growing need for automatic solutions to facilitate knowledge access and indexing. The objectives of my PhD work are the implementation of an automatic multi-terminology multi-document and multi-task indexing help-system namely F-MTI (French Multi-terminology Indexer). It uses Natural Language processing methods to produce an indexing proposition for medical documents. We applied it to resources indexing in a French online health catalogue, namely CISMeF, to therapeutical data indexing for drug medication and to diagnosis and health procedures indexing for patient medical records.
In this thesis, I will mainly focus on variational inference and probabilistic models. In particular, I will cover several projects I have been working on during my PhD about improving the efficiency of AI/ML systems with variational techniques. The thesis consists of two parts. In the first part, the computational efficiency of probabilistic graphical models is studied. In the second part, several problems of learning deep neural networks are investigated, which are related to either energy efficiency or sample efficiency
This study deals with the usage of two French linguistic variables liaison and elision, which are traditionally described as phonological variables. They are studied during natural interactions between three children and their parents. More precisely, the aim of this thesis is to describe the specificities of the child directed speech (CDS) concerning the usage of liaison and elision to measure their impact on the emergence of these phonological variables in the speech of the children. After the presentation of the theoretical context of the study (Usage-Based Models and Construction Grammar) and the methodology used to collect, structure, and analyse the data, the research is divided into three analysis sections. The aim of the first corpus based study, a descriptive one, is twofold. The first objective is to describe the variation to which children are exposed at home. A second objective is to compare the results of previous studies on liaison acquisition, obtained mainly from experimental tasks, with data extracted from dense corpora collected during natural interactions between the children and their parents. In particular, this study shows that usage factors, including the frequency of items, influence the production of phonological variables. The second study focuses on the specificities of CDS. The results show that the usage of phonological variables is modulated in CDS, essentially at an early stage of language acquisition. Then, this modulation attenuates during the child's development. The aim of the third study is to connect parent's productions and children's productions. It appears that the results concerning the development of phonological variation are in step with the assumptions provided by the usage-based models: at an early stage, the variation is memorized into specific constructions, particularly salient and frequent in CDS.
Today's fast changing environment imposes new challenges for effective management of business processes. In such a highly dynamic environment, the business process design becomes time-consuming, error-prone, and costly. Therefore, seeking reuse and adaptability is a pressing need for a successful business process design. Configurable reference models recently introduced were a step toward enabling a process design by reuse while providing flexibility. A configurable process model is a generic model that integrates multiple process variants of a same business process in a given domain through variation points. These variation points are referred to as configurable elements and allow for multiple design options in the process model. A configurable process model needs to be configured according to a specific requirement by selecting one design option for each configurable element. On the one hand, as configurable process models tend to be very complex with a large number of configurable elements, many automated approaches have been proposed to assist their design. However, existing approaches propose to recommend entire configurable process models which are difficult to reuse, cost much computation time and may confuse the process designer. On the other hand, the research results on configurable process model design highlight the need for means of support to configure the process. Therefore, many approaches proposed to build a configuration support system for assisting end users selecting desirable configuration choices according to their requirements. Our objective is twofold: (i) assisting the configurable process design in a fin-grained way using configurable process fragments that are close to the designers interest and (ii) automating the creation of configuration support systems in order to release the process analysts from the burden of manually building them. In order to achieve the first objective, we propose to learn from the experience gained through past process modeling in order to assist the process designers with configurable process fragments. The proposed fragments inspire the process designer to complete the design of the ongoing process. To achieve the second objective, we realize that previously designed and configured process models contain implicit and useful knowledge for process configuration.
This thesis focuses on the formalisms that make it possible to mathematically represent not only the meaning of independent sentences, but also whole texts, including the meaning relations that link sentences together. Not only are we interested in meaning and its representation, but also on the algorithmic process of how this representation is computed using the sequence of words that constitute the text. We thus find ourselves at a point where three disciplines intersect: discourse analysis, formal semantics and computational linguistics. Most formal work on discourse pay little attention to reporting verbs (say, tell, etc.) and attitude verbs (think, believe, etc.). These verbs, henceforth 'AVs', all express the attitude or stance of one person on a given proposition. They are used frequently and introduce many subtleties that are not addressed in current theories. The main objective of this thesis is to shed light on the principles of a formal grammar that is compatible with discourse analysis that takes AVs into account. We therefore start by presenting a set of linguistic data illustrating the interactions between AVs and discourse relations. Adverbial connectives (then, for example, etc.) are usually considered anaphoric. One might wonder, however, whether, in practice, a computational linguistic system cannot deal with this particular category of anaphora as a kind of structural dependency, meaning that syntax is somehow extended above the sentence level. This is what we try to achieve using the D-STAG formalism. Consequently, we develop an anaphor based approach, in which the arguments of discourse relations are not determined solely by the grammatical structures of the utterances. This is made possible by continuation semantics, which we use in conjunction with event semantics. We suggest a number of potential answers and study the case of negation in more detail. We argue that these difficulties originate from the standard analysis of negation, which interprets positive and negative sentences is an essentially different fashion. Rejecting this view, we propose a novel formalisation of negative events that is relevant to the analysis of various linguistic phenomena.
These days with the increasing abundance of data with high dimensionality, high dimensional classification problems have been highlighted as a challenge in machine learning community and have attracted a great deal of attention from researchers in the field. In recent years, sparse and stochastic learning techniques have been proven to be useful for this kind of problem. In this thesis, we focus on developing optimization approaches for solving some classes of optimization problems in these two topics. Our methods are based on DC (Difference of Convex functions) programming and DCA (DC Algorithms) which are wellknown as one of the most powerful tools in optimization. The thesis is composed of three parts. The first part tackles the issue of variable selection. The second part studies the problem of group variable selection. The final part of the thesis concerns the stochastic learning. In the first part, we start with the variable selection in the Fisher's discriminant problem (Chapter 2) and the optimal scoring problem (Chapter 3), which are two different approaches for the supervised classification in the high dimensional setting, in which the number of features is much larger than the number of observations. Continuing this study, we study the structure of the sparse covariance matrix estimation problem and propose four appropriate DCA based algorithms (Chapter 4). Two applications in finance and classification are conducted to illustrate the efficiency of our methods. The second part studies the L_p,0regularization for the group variable selection (Chapter 5). Using a DC approximation of the L_p,0norm, we indicate that the approximate problem is equivalent to the original problem with suitable parameters. Considering two equivalent reformulations of the approximate problem we develop DCA based algorithms to solve them. Regarding applications, we implement the proposed algorithms for group feature selection in optimal scoring problem and estimation problem of multiple covariance matrices. In the third part of the thesis, we introduce a stochastic DCA for large scale parameter estimation problems (Chapter 6) in which the objective function is a large sum of nonconvex components. As an application, we propose a special stochastic DCA for the loglinear model incorporating latent variables
This cross-disciplinary research study covers controlled languages and French to Arabic machine translation, two intimately related concepts. In a situation of crisis where communication must play its full role, and in the context of increasing globalisation where many languages coexist, our research findings show that the combination of these two concepts is sorely needed. No one can deny today the predominant role played by security in people's daily life and the significant challenges it presents in modern societies. However, and contrary to an entrenched idea that tends to associate the risk of poor communication only with oral transmission, the use of written language can also be subject to risk. Indeed, a protocol or an alert which is badly formulated can provoke serious accidents due to misunderstanding, in particular during a crisis and under stress. It is in this context that our research has been undertaken. Indeed, new concepts are introduced by means of several normative methods involved not only in the controlling process but also in the machine translation process. It introduces new concepts including controlled mirror macrostructures, where the syntax and semantics of the source and target languages are represented at the same level.
Social interaction refers to any interaction between two or more individuals, in which information sharing is carried out without any mediating technology. In the context of testing and observational studies, multiple mechanisms are used to study these interactions such as questionnaires, direct observation and analysis of events by human operators, or a posteriori observation and analysis of recorded events by specialists (psychologists, sociologists, doctors, etc.). In order to face the aforementioned issues, the need to automatize the social interaction analysis process is highlighted. So, it is a question of bridging the gap between human-based and machine-based social interaction analysis processes. Therefore, we propose a holistic approach that integrates multimodal heterogeneous cues and contextual information (complementary "exogenous" data) dynamically and optionally according to their availability or not. Such an approach allows the analysis of multi "signals" in parallel (where humans are able only to focus on one). This analysis can be further enriched from data related to the context of the scene (location, date, type of music, event description, etc.) or related to individuals (name, age, gender, data extracted from their social networks, etc.). The contextual information enriches the modeling of extracted metadata and gives them a more "semantic" dimension. Managing this heterogeneity is an essential step for implementing a holistic approach.
In this dissertation, we propose methods and data driven machine learning solutions which address and benefit from the recent overwhelming growth of digital media content. First, we consider the problem of improving the efficiency of image retrieval. We propose a coordinated local metric learning (CLML) approach which learns local Mahalanobis metrics, and integrates them in a global representation where the l2 distance can be used. This allows for data visualization in a single view, and use of efficient ` 2-based retrieval methods. Our approach can be interpreted as learning a linear projection on top of an explicit high-dimensional embedding of a kernel. This interpretation allows for the use of existing frameworks for Mahalanobis metric learning for learning local metrics in a coordinated manner. Our experiments show that CLML improves over previous global and local metric learning approaches for the task of face retrieval. We explore different metric learning strategies over features from the intermediate layers of the networks, to reduce the discrepancies between the different modalities. In our experiments we found that the depth of the optimal features for a given modality, is positively correlated with the domain shift between the source domain (CNN training data) and the target domain. Experimental results show the that we can use CNNs trained on visible spectrum images to obtain results that improve over the state-of-the art for heterogeneous face recognition with near-infrared images and sketches. Third, we present convolutional neural fabrics for exploring the discrete andexponentially large CNN architecture space in an efficient and systematic manner. Instead of aiming to select a single optimal architecture, we propose a “fabric” that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyperparameters of the fabric (the number of channels and layers) are not critical for performance. The acyclic nature of the fabric allows us to use backpropagation for learning. Learning can thus efficiently configure the fabric to implement each one of exponentially many architectures and, more generally, ensembles of all of them. While scaling linearly in terms of computation and memory requirements, the fabric leverages exponentially many chain-structured architectures in parallel by massively sharing weights between them. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset
This problem is usually addressed with a two-stepped treatment: filtering the multidimensional, heterogeneous and imprecise measurements into symbolic events and then using efficient plan recognition techniques on those events. This allows, among other things, the possibility of describing high level symbolic plan steps without being overwhelmed by low level sensor specificities. However, the first step is information destructive and generates additional ambiguity in the recognition process. Furthermore, splitting the behavior recognition task leads to unnecessary computations and makes the building of the plan library tougher. Thus, we propose to tackle this problem without dividing the solution into two processes. We present a hierarchical model, inspired by the formal language theory, allowing us to describe behaviors in a continuous way, and build a bridge over the semantic gap between measurements and intents. Thanks to a set of algorithms using this model, we are able, from observations, to deduce the possible future developments of the monitored area while providing the appropriate explanations.
Usually, human beings are able to quickly distinguish between different places, solely from their visual appearance. Such a semantic category can thus be used as contextual information which fosters object detection and recognition. Recent works in semantic place recognition seek to endow the robot with similar capabilities. Contrary to classical localization and mapping works, this problem is usually addressed as a supervised learning problem. The question of semantic places recognition in robotics-the ability to recognize the semantic category of a place to which scene belongs to-is therefore a major requirement for the future of autonomous robotics. It is indeed required for an autonomous service robot to be able to recognize the environment in which it lives and to easily learn the organization of this environment in order to operate and interact successfully. If we make the hypothesis that objects are more easily recognized when the scene in which they appear is identified, the second approach seems more suitable. It is however strongly dependent on the nature of the image descriptors used, usually empirically derived from general considerations on image coding. Compared to these many proposals, another approach of image coding, based on a more theoretical point of view, has emerged the last few years. Energy-based models of feature extraction based on the principle of minimizing the energy of some function according to the quality of the reconstruction of the image has lead to the Restricted Boltzmann Machines (RBMs) able to code an image as the superposition of a limited number of features taken from a larger alphabet. It has also been shown that this process can be repeated in a deep architecture, leading to a sparse and efficient representation of the initial data in the feature space. A complex problem of classification in the input space is thus transformed into an easier one in the feature space. We show that after appropriate coding a softmax regression in the projection space is sufficient to achieve promising classification results. To our knowledge, this approach has not yet been investigated for scene recognition in autonomous robotics. We compare our methods with the state-of-the-art algorithms using a standard database of robot localization. We study the influence of system parameters and compare different conditions on the same dataset. These experiments show that our proposed model, while being very simple, leads to state-of-the-art results on a semantic place recognition task.
This thesis deals with the semantics of causative constructions. It develops a semantic typology for English periphrastic causative verbs CAUSE, MAKE, HAVE, GET, and LET, based on the force-dynamics model. The third chapter offers a discussion of some of the most commonly shared hypotheses about the semantics of English periphrastic causative verbs in literature. In the fourth chapter, we propose a corpus study of the lexico-semantic features of the verbs CAUSE, MAKE, HAVE, GET and LET. The last chapter presents a newly semantic typology for English periphrastic causative verbs, drawn upon the data we collected from our corpus study.
The abundance of biomedical information expressed in natural language has resulted in the need for methods to process this information automatically. In the field of Natural Language Processing (NLP), Information Extraction (IE) focuses on the extraction of relevant information from unstructured data in natural language. A great deal of IE methods today focus on Machine Learning (ML) approaches that rely on deep linguistic processing in order to capture the complex information contained in biomedical texts. In particular, syntactic analysis and parsing have played an important role in IE, by helping capture how words in a sentence are related. It focuses on a task-based approach to dependency parsing evaluation and parser selection, including a detailed error analysis. In order to achieve a high quality of syntax-based IE, different stages of linguistic processing are addressed, including both pre-processing steps (such as tokenization) and the use of complementary linguistic processing (such as the use of semantics and coreference analysis). This thesis also explores how the different levels of linguistics processing can be represented for use within an ML-based IE algorithm, and how the interface between these two is of great importance. The methods and approaches described are explored using two different biomedical corpora, demonstrating how the IE results are used in real-life tasks.
Industrial clean rooms or operating rooms are critical places often hosting dangerous or complex processes. Their design, building and use are thus difficult and constrained by a large amount of standards and rules. Qualifying these environments, in order to ensure their quality, consequently requires a high level of expertise and lacks assisting tools. This leads us to propose a unified approach aiming at easing the qualification process of standardized environments. It relies on a graph-based representation of the set of standards and rules that apply to a specific case, as well as on step-by-step modelling of the whole target environment. This approach, applied to medical environments for validation purposes, remains generic and can be applied to any kind of standardized environment.
This work presents novel techniques for parsing the structures of multi-party dialogue and argumentative texts. Finding the structure of extended texts and conversations is a critical step towards the extraction of their underlying meaning. The task is notoriously hard, as discourse is a high-level description of language, and multi-party dialogue involves many complex linguistic phenomena. Historically, representation of discourse moved from local relationships, forming unstructured collections, towards trees, then constrained graphs. Our work uses the latter framework, through Segmented Discourse Representation Theory. We base our research on a annotated corpus of English chats from the board game The Settlers of Catan. We discuss two corpus-related experiments. The first expands the definition of the Right Frontier Constraint, a formalization of discourse coherence principles, to adapt it to multi-party dialogue. The second demonstrates a data extraction process giving a strategic advantage to an artificial player of Settlers by inferring its opponents'assets from chat negotiations. We propose new methods to parse dialogue, using jointly machine learning, graph algorithms and linear optimization, to produce rich and expressive structures with greater accuracy than previous attempts. We describe our method of constrained discourse parsing, first on trees using the Maximum Spanning Tree algorithm, then on directed acyclic graphs using Integer Linear Programming with a number of original constraints. We finally apply these methods to argumentative structures, on a corpus of English and German texts, jointly annotated in two discourse representation frameworks and one argumentative. We compare the three annotation layers, and experiment on argumentative parsing, achieving better performance than similar works.
As part of ongoing work to computerize a large number of "poorly endowed" languages, especially those in the French-speaking world, we have created a French-Somali machine translation system dedicated to a journalistic sub-language, allowing to obtain quality translations from a bilingual body built by post-editing of GoogleTranslate results for the Somali and non-French speaking populations of the Horn of Africa. The latter is an aligned corpus of very good quality, because we built in by post-editions editing pre-translations of produced by GT, which uses with a combination of the its French-English and English-Somali MT language pairs. It That corpus was also evaluated by 9 bilingual annotators who gave assigned a quality note score to each segment of the corpus and corrected our post-editing. On the other hand, we have set up an iMAG (multilingual interactive access gateway) that allows non-French-speaking Somali surfers on the continent to access the online edition of the newspaper "La Nation de Djibouti" in Somali.
We developed an original approach to Arabic traditional morphology, involving new concepts in Semitic lexicology, morphology, and grammar for standard written Arabic. This new methodology for handling the rich and complex Semitic languages is based on good practices in Finite-State technologies (FSA/FST) by using Unitex, a lexicon-based corpus processing suite. For verbs (Neme, 2011), I proposed an inflectional taxonomy that increases the lexicon readability and makes it easier for Arabic speakers and linguists to encode, correct, and update it. Traditional grammar defines inflectional verbal classes by using verbal pattern-classes and root-classes. In our taxonomy, traditional pattern-classes are reused, and root-classes are redefined into a simpler system. The lexicon of verbs covered more than 99% of an evaluation corpus. For nouns and adjectives (Neme, 2013), we went one step further in the adaptation of traditional morphology. First, while this tradition is based on derivational rules, we found our description on inflectional ones. Next, we keep the concepts of root and pattern, which is the backbone of the traditional Semitic model. Still, our breakthrough lies in the reversal of the traditional root-and-pattern Semitic model into a pattern-and-root model, which keeps small and orderly the set of pattern classes and root sub-classes. I elaborated a taxonomy for broken plural containing 160 inflectional classes, which simplifies ten times the encoding of broken plural. Since then, I elaborated comprehensive resources for Arabic. These resources are described in Neme and Paumier (2019). To take into account all aspects of the rich morphology of Arabic, I have completed our taxonomy with suffixal inflexional classes for regular plurals, adverbs, and other parts of speech (POS) to cover all the lexicon. In all, I identified around 1000 Semitic and suffixal inflectional classes implemented with concatenative and non-concatenative FST devices. From scratch, I created 76000 fully vowelized lemmas, and each one is associated with an inflectional class. These lemmas are inflected by using these 1000 FSTs, producing a fully inflected lexicon with more than 6 million forms. I extended this fully inflected resource using agglutination grammars to identify words composed of up to 5 segments, agglutinated around a core inflected verb, noun, adjective, or particle. The agglutination grammars extend the recognition to more than 500 million valid delimited word forms, partially or fully vowelized. The flat file size of 6 million forms is 340 megabytes (UTF-16). It is compressed then into 11 Mbytes before loading to memory for fast retrieval. The generation, compression, and minimization of the full-form lexicon take less than one minute on a common Unix laptop. The lexical coverage rate is more than 99%. The tagger speed is 5000 words/second, and more than 200 000 words/s, if the resources are preloaded/resident in the RAM. The accuracy and speed of our tools result from our systematic linguistic approach and from our choice to embrace the best practices in mathematical and computational methods. The lookup procedure is fast because we use Minimal Acyclic Deterministic Finite Automaton (Revuz, 1992) to compress the full-form dictionary, and because it has only constant strings and no embedded rules.
The problem of automatic logical meaning representation for ambiguous natural language utterances has been the subject of interest among the researchers in the domain of computational and logical semantics. Ambiguity in natural language may be caused in lexical/syntactical/semantical level of the meaning construction or it may be caused by other factors such as ungrammaticality and lack of the context in which the sentence is actually uttered. The traditional Montagovian framework and the family of its modern extensions have tried to capture this phenomenon by providing some models that enable the automatic generation of logical formulas as the meaning representation. However, there is a line of research which is not profoundly investigated yet: to rank the interpretations of ambiguous utterances based on the real preferences of the language users. This gap suggests a new direction for study which is partially carried out in this dissertation by modeling meaning preferences in alignment with some of the well-studied human preferential performance theories available in the linguistics and psycholinguistics literature. In order to fulfill this goal, we suggest to use/extend Categorial Grammars for our syntactical analysis and Categorial Proof Nets as our syntactic parse. We also use Montagovian Generative Lexicon for deriving multi-sorted logical formula as our semantical meaning representation. We use a framework called Montagovian Generative Lexicon.
During crisis events such as disasters, the need of real-time information retrieval (IR) from microblogs remains inevitable. However, the huge amount and the variety of the shared information in real time during such events over-complicate this task. Unlike existing IR approaches based on content analysis, we propose to tackle this problem by using user-centricIR approaches with solving the wide spectrum of methodological and technological barriers inherent to: 1) the collection of the evaluated users data, 2) the modeling of user behavior, 3) the analysis of user behavior, and 4) the prediction and tracking of prominent users in real time. In this context, we detail the different proposed approaches in this dissertation leading to the prediction of prominent users who are susceptible to share the targeted relevant and exclusive information on one hand and enabling emergency responders to have a real-time access to the required information in all formats (i.e. text, image, video, links) on the other hand. Based on the selected features, we have designed several engineered features qualifying user activities by considering both their on-topic and off-topic shared information. Thirdly, based on this proposed user modeling approach, we train various prediction models to learn to differentiate between prominent and non-prominent users behavior during crisis event. The efficiency and efficacy of these prediction models have been validated thanks to the data collections extracted by our multi-agents system MASIR during two flooding events who have occured in France and the different ground-truths related to these collections.
Our study focuses on elliptic mechanisms within associative meronymic anaphora. We departed from the assumption that in this type of anaphora, there are two structures: a deep structure and a surface structure. The first consists in the presence of three elements: the whole, the partitive predicate and the part. The second, which shows the different types of ellipsis, is the topic of our work. We focused on three types of Ellipsis, which we considered typical meronymic anaphora: ellipsis of partitive predicate, ellipsis of the second element of the structure [N De N] and that of the anaphoric antecedent. Treated separately, nominal anaphora, verbal anaphora, and adverbial anaphora were initially submitted to a syntactic-semantic description, and then, to the three primary functions theory. This theory allowed us to explain the ability of certain items to be elided in the associative meronymic anaphora.
Mobile devices such as smartphones, smartwatches or smart glasses have revolutionized how we interact. We are interested in smart glasses because they have the advantage of providing a simultaneous view of both the physical and the digital worlds. However, the interaction for smart glasses is not well explored. More suitable interactions on this device can convince users to use it more in everyday life. The thesis subject is focused on the study and development of new interaction techniques with smart glasses. Indeed, it is not possible to interact so finely in mobility or in an emergency situation compared to a stable situation such as sitting in front of a desktop. The work context is in the health field and in particular a healthcare professional visiting his patient in a hospital. The professional must be able to access the patient's data already collected, obtain physiological data in real time, prepare his diagnosis and communicate with his colleagues. The scientific problem for the thesis here is to find solutions that allow him to perform these tasks in a more precise and less restrictive way. The goal is to make the diagnosis and information sharing more effective and to make a device still uncontrolled a functional system in a professional healthcare environment. In this perspective, the work of this thesis presents theoretical and applicative contributions. We first listed the various work performed in the context of smart glasses for the health field, while indicating potential, relevant results and limitations. We focused on smart glasses to visualize and manipulate patient records. From a conceptual point of view, we have proposed an eight-dimensional design space to identify gaps in existing systems and assist in the design of new systems. From an application point of view, for output interaction, we introduced a technique called K-Fisheye on a tile-based interface that allows the user to browse a large dataset like in the health record. We used the design space to adapt an existing system for the smart glasses. The prototype obtained called mCAREglass. Finally, we designed a new text entry technique called TEXTile by using a new interactive fabric communicating with glasses.
Many applications produce and receive continuous, unlimited, and high-speed data streams. This raises obvious problems of storage, treatment and analysis of data, which are only just beginning to be treated in the domain of data streams. The learning of this model is, contrary to that of its former self, driven not only by the novelty part in the input data but also by the data itself. Thereby, ILoNDF can continuously extract new knowledge relating to the relative frequencies of the data and their variables. Firstly, we focus on the study of ILoNDF's behavior for one-class classification when dealing with high-dimensional noisy data. This study enabled us to highlight the pure learning capacities of ILoNDF with respect to the key classification methods suggested until now. Next, we are particularly involved in the adaptation of ILoNDF to the specific context of information filtering. Our goal is to set up user-oriented filtering strategies rather than system-oriented in following two types of directions. The first direction concerns user modeling relying on the model ILoNDF. This provides a new way of looking at user's need in terms of specificity, exhaustivity and contradictory profile-contributing criteria. The second direction, complementary to the first one, concerns the refinement of ILoNDF's functionality in order to confer it the capacity of tracking drifting user's need over time. Finally, we consider the generalization of our previous work to the case where streaming data can be divided into multiple classes.
One of the main challenges of the synthesizer market and the research in sound synthesis nowadays lies in proposing new forms of synthesis allowing the creation of brand new sonorities while offering musicians more intuitive and perceptually meaningful controls to help them reach the perfect sound more easily. Indeed, today's synthesizers are very powerful tools that provide musicians with a considerable amount of possibilities for creating sonic textures, but the control of parameters still lacks user-friendliness and may require some expert knowledge about the underlying generative processes. In this thesis, we are interested in developing and evaluating new data-driven machine learning methods for music sound synthesis allowing the generation of brand new high-quality sounds while providing high-level perceptually meaningful control parameters. The first challenge of this thesis was thus to characterize the musical synthetic timbre by evidencing a set of perceptual verbal descriptors that are both frequently and consensually used by musicians. Two perceptual studies were then conducted: a free verbalization test enabling us to select eight different commonly used terms for describing synthesizer sounds, and a semantic scale analysis enabling us to quantitatively evaluate the use of these terms to characterize a subset of synthetic sounds, as well as analyze how consensual they were. In a second phase, we investigated the use of machine learning algorithms to extract a high-level representation space with interesting interpolation and extrapolation properties from a dataset of sounds, the goal being to relate this space with the perceptual dimensions evidenced earlier. Following previous studies interested in using deep learning for music sound synthesis, we focused on autoencoder models and realized an extensive comparative study of several kinds of autoencoders on two different datasets. These experiments, together with a qualitative analysis made with a non real-time prototype developed during the thesis, allowed us to validate the use of such models, and in particular the use of the variational autoencoder (VAE), as relevant tools for extracting a high-level latent space in which we can navigate smoothly and create new sounds. However, so far, no link between this latent space and the perceptual dimensions evidenced by the perceptual tests emerged naturally. As a final step, we thus tried to enforce perceptual supervision of the VAE by adding a regularization during the training phase. Using the subset of synthetic sounds used in the second perceptual test and the corresponding perceptual grades along the eight perceptual dimensions provided by the semantic scale analysis, it was possible to constraint, to a certain extent, some dimensions of the VAE high-level latent space so as to match these perceptual dimensions. A final comparative test was then conducted in order to evaluate the efficiency of this additional regularization for conditioning the model and (partially) leading to a perceptual control of music sound synthesis.
Drug non-compliance refers to situations where the patient does not follow instructions from medical authorities when taking medications. Such situations include taking too much (overuse) or too little (underuse) of medications, drinking contraindicated alcohol, or making a suicide attempt using medication. According to [HAYNES 2002] increasing drug compliance may have a bigger impact on public health than any other medical improvements. However non-compliance data are difficult to obtain since non-adherent patients are unlikely to report their behaviour to their healthcare providers. First we collect a corpus of messages written by users from medical forums. We build vocabularies of medication and disorder names such as used by patients. We use these vocabularies to index medications and disorders in the corpus. Then we use supervised learning and information retrieval methods to detect messages talking about non-compliance. We identify 3 main motivations: self-medication, seeking an effect besides the effect the medication was prescribed for, or being in addiction or habituation situation. Self-medication is an umbrella for several situations: avoiding an adverse effect, adjusting the medication's effect, underuse a medication seen as useless, taking decisions without a doctor's advice. Non-compliance can also happen thanks to errors or carelessness, without any particular motivation. Our work provides several kinds of result: annotated corpus with non-compliance messages, classifier for the detection of non-compliance messages, typology of non-compliance situations and analysis of the causes of non-compliance.
In this thesis, we study the segmentation of an audio stream in speech, music and speech on music (S/M). This is a fundamental step for all application based on automatic transcription of radiophonic stream and most commonly multimedia. The target application here is a keyword detection system in broadcast programs. The application performance depends on the quality of the signal segmentation given by the speech/music discrimination system. Indeed, bad signal classification can give miss-detections or false alarms. To improve the speech/music discrimination task, we propose a new signal parameterization method. We use the wavelet decomposition which allows an analysis of non-stationary signal like music for instance. We compute different energies on wavelet coefficients to construct our feature vectors. We chose a class/non-class architecture because it allows to find independently the best parameters for each S/NS and P/NP tasks. A fusion of the classifier ouputs is then performed to obtain the final decision: speech, music or speech on music. The obtained results on a real broadcast program corpus show that our wavelet-based parameterization gives a significant improvement in performance in both M/NM and S/M discrimination tasks compared to the baseline parameterization using cepstral coefficients.
The present thesis subject aims to contribute to the study of translation in Egypt between 1798 and 1873 and its role in the reforms undertaken by intellectuals under the aegis of the reformist Wali's policy (Mohamed Ali Pasha), but also the al-Alsun school and its contribution to translation, lexical expansion and the acquisition of Nahḍa. Our subject is particularly important because it deals not only with the beginnings of linguistic and cultural relations between France and Egypt which contributed to the emergence of modern legal Arabic with the school of al-Alsun in Cairo in 1834, but also represent, for some unknown reasons, an uncharted territory for researchers. Particular attention has been given to the region of Mašriq al 'arabī with a good reason, for the latter constitutes a hub where the bulk of translation activities took place, as opposed to other areas of the Arab world. As a matter of fact, while collecting material for this research project, no book dealing with this school has been found which, however, played a crucial role in the evolution of Arab society in general and that of Egypt in particular. However, as far as «translation» is concerned in the Arab world, the historical dimension brings to light such great skills as the translating the person into Arabic, or the Greek into Arabic in the Omeyyad era and Abbasside, but also translating in the contemporary sense of the word, which forms the subject of this research.
The aim of this thesis is to study sentiments in comparable documents. First, we collect English, French and Arabic comparable corpora from Wikipedia and Euronews, and we align each corpus at the document level. We further gather English-Arabic news documents from local and foreign news agencies. Second, we present a cross-lingual document similarity measure to automatically retrieve and align comparable documents. Then, we propose a cross-lingual sentiment annotation method to label source and target documents with sentiments. Finally, we use statistical measures to compare the agreement of sentiments in the source and the target pair of the comparable documents. The methods presented in this thesis are language independent and they can be applied on any language pair
This work deals with audio-visual speech synthesis. In the vast literature available in this direction, many of the approaches deal with it by dividing it into two synthesis problems. One of it is acoustic speech synthesis and the other being the generation of corresponding facial animation. But, this does not guarantee a perfectly synchronous and coherent audio-visual speech. To overcome the above drawback implicitly, we proposed a different approach of acoustic-visual speech synthesis by the selection of naturally synchronous bimodal units. The synthesis is based on the classical unit selection paradigm. The main idea behind this synthesis technique is to keep the natural association between the acoustic and visual modality intact. We describe the audio-visual corpus acquisition technique and database preparation for our system. We present an overview of our system and detail the various aspects of bimodal unit selection that need to be optimized for good synthesis. The main focus of this work is to synthesize the speech dynamics well rather than a comprehensive talking head. We describe the visual target features that we designed. We subsequently present an algorithm for target feature weighting. This algorithm that we developed performs target feature weighting and redundant feature elimination iteratively. This is based on the comparison of target cost based ranking and a distance calculated based on the acoustic and visual speech signals of units in the corpus. Finally, we present the perceptual and subjective evaluation of the final synthesis system. The results show that we have achieved the goal of synthesizing the speech dynamics reasonably well
Natural Language Processing (NLP) systems are continuously faced with the problem of generating concurrent hypotheses, of which some can be erroneous. In order to avoid the propagation of erroneous hypotheses, it appears to be essential to apply specific control strategies, which aim to distinguishing concurrent hypotheses based on their relevance. On most of observed indetermination cases, we have noticed that multiple heterogeneous knowledge sources have to be combined to determine the hypotheses relative relevance. According to this observation, we show that the control of the indetermination cases can be formalised as a decisional process based on multiple criteria. This approach differs from alternative methods by the importance granted to knowledge and preferences that an expert can express about a given problem. From this innovative intersection between NLP and MCDA, our work has been focalised on the development of a decisional module dedicated to multicriteria control. The integration of this module into a complete NLP system has allowed us to attest the feasibility of our approach and to perform experimentation on concrete indetermination cases.
This thesis is the interaction result of two disciplines that are the change detection in multitemporal images and the evidential reasoning using the Dempster-Shafer theory (DST). Addressing the problem of change detection and analyzing by the DST, requires the determination of an exhaustive and exclusive frame of discernment. This issue is complex when images lake prior information. The idea of this algorithm is the representation of each class by a varied number of centroids in order to guarantee a better characterization of classes. To ensure the frame of discernment exhaustiveness, we proposed a new cluster validity index able to identify the optimal number of semantic classes. The third contribution is to exploit the position of the pixel in relation to class centroids and its membership distribution in order to define the mass distribution that represents information. We have emphasized the capacity of evidential conflict to indicate multi-temporal transformations. We reasoned on the decomposition of the global conflict and the estimation of the partial conflicts between the couples of focal elements to measure the conflict caused by the change. This strategy allows to identify the couple of classes that participate in the change. To quantify this conflict, we proposed a new measure of change noted CM. Finally, we proposed an algorithm to deduce the binary map of changes from the partial conflicts map.
In the forensic speaker recognition field, voice disguise presents a specific interest. Most criminals try to disguise their voice before a miscellaneous calls or a terrorist claim. Their aim is to change the register of their voice quality in order to false their identity or to mimic the voice of another person. This thesis proposes to analyse two different kinds of disguise: the transformation of the voice by non electronic and deliberate means and the conversion of the voice based in electronic and deliberate technique, in order to take the identity of a target person. In one hand, the analysis of voice transformation is based on an acoustic point of view to measure specific changes in speech and on an automatic approach to detect disguise. Four kinds of disguises which are considered as the most common in forensics are studied. A constraint of audibility and intelligibility has been imposed to the speakers who have realized the database. In the automatic experiment, the best way to detect a voice disguise is to use SVM technique. The level of performance is an AUC (area under curve) at 0,79. In another hand, voice conversion techniques are proposed and applied on two forensic scenarios: the imitation of a politician from an Internet recording, and the application of voice disguise reversibility. Different kinds of tests are proposed to evaluate the relevance of the result, based on objective and subjective measurements.
Evidence-Based Medicine has been formalized as Clinical Practice Guidelines, which define workflows and recommendations to be followed for a given clinical domain. These documents were formalized aiming to standardize healthcare and seeking the best patient outcomes. Nevertheless, clinicians do not adhere as expected to these guidelines due to several clinical and implementation limitations. On one hand, clinicians do not feel familiar, agree with and or are unaware of guidelines, hence doubting their self-efficacy and outcome expectancy compared to previous or more common practices. On the other hand, maintaining these guidelines updated with the most recent evidence requires continuous versioning of these paper-based documents. Clinical Decision Support Systems are proposed to help during the clinical decision-making process with the computerized implementation of the guidelines to promote their easy consultation and increased compliance. Even if these systems help improving guideline compliance, there are still some barriers inherited from paper-based guidelines that are not solved, such as managing complex cases not defined within the guidelines or the lack of representation of other external factors that may influence the provided treatments, biasing from guidelines'recommendations (i.e. patient preferences). This thesis proposes an advanced Clinical Decision Support System for coping with the purely guideline-based support limitations and going beyond the formalized knowledge by analyzing the clinical data, outcomes, and performance of all the decisions made over time. To achieve these objectives, an approach for modeling the clinical knowledge and performance in a semantically validated and computerized way has been presented, leaning on an ontology and the formalization of the Decisional Event concept. Moreover, a domain-independent framework has been implemented for easing the process of computerizing, updating and implementing Clinical Practice Guidelines within a Clinical Decision Support System in order to provide clinical support for any queried patient. For addressing the reported guideline limitations, a methodology for augmenting the clinical knowledge using experience has been presented along with some clinical performance and quality evaluation over time, based on different studied clinical outcomes, such as the usability and the strength of the rules for evaluating the clinical reliability behind the formalized clinical knowledge. Finally, the accumulated Real World Data was explored to support future cases, promoting the study of new clinical hypotheses and helping in the detection of trends and patterns over the data using visual analytics tools. The presented modules had been developed and implemented in their majority within the European Horizon 2020 project DESIREE, in which the use case was focused on supporting Breast Units during the decision-making process for Primary Breast Cancer patients management, performing a technical and clinical validation over the presented architecture, whose results are presented in this thesis. Nevertheless, some of the modules have been also used in other medical domains such as Gestational Diabetes guidelines development, highlighting the interoperability and flexibility of the presented work.
With the explosive growth of digitization in cultural heritage, many cultural heritage institutions have been converting physical objects of cultural heritage into digital representation or descriptive representation. However, the conversion resulted in several issues such as: 1) the documents are descriptive in nature, 2) ambiguity and brevity of the documents, 3) dedicated vocabulary is used in the documents, and 4) there is also variation in the terms used in the document. Besides, the usage of inaccurate keywords also resulted in short query problem. Most of the time, the issues are caused by the aggregated fault in annotating the documents while the short query problem is caused by naive user who has little prior knowledge and experience in cultural heritage domain. In this research, the main aim is to model information access system to overcome partially the issues arising from the documentation process and the background of the users of digital cultural heritage. Therefore, three types of information access tool are introduced and established namely information retrieval system, context search, and mobile game on cultural heritage that allow the user to access, learn, and explore the information on cultural heritage. Basically, the main idea for information retrieval system and context search is to incorporate the link relationship between terms into the Language Model by extending of Dirichlet Smoothing to solve the problems arising from both the documentation process and background of the users. In addition, a Preference Model is introduced based on the Theory of Charging a Capacitor to quantify the cognitive context based on time and integrate into the extended Dirichlet Smoothing. Besides, a mobile game is introduced by integrating the elements of the games of monopoly and treasure hunt to mitigate the problems arising from the background of the users especially their casual behavior. The first and second approaches were tested on the Cultural Heritage in CLEF (CHiC) collection that consists of short queries and documents. The results show that the approach is effective and yields better accuracy during the retrieval. Finally, a survey was carried out to investigate the third approach, and the result suggests that the game is able to help the participants to explore and learn the information on cultural heritage. In addition, the participants also felt that an information seeking tool that is integrated with the game can provide more information to the user in a more convenient manner while playing the game and visiting the heritage sites in the game. In conclusion, the results show that the proposed solutions are able to solve the problems arising from the documentation process and the background of the users of digital cultural heritage.
This thesis presents a method of Micro-Systemic Linguistic Analysis of Thai compound words. Some notable points of view are discussed. The second chapter identifies some essential characteristics of the Thai language such as a non-space writing style resulted in ambiguity in machine translation. Different entities between Thai and French languages are underlined by means of the micro-systematic theory of the Centre Tesnière. The third chapter analyses Thai compound words using a hybrid method involving morpho-syntactic parsing and a rule-based system corresponding to our model of data analysis. The fourth chapter employs a technique of lexical-syntactic and semantic control enabling the definition of efficient algorithms. The final chapter concludes our work with some future perspectives. This study is presented as a reliable approach which enhances the elimination of word ambiguities in machine translation. This hybrid method allows us to reach our objective and to find an effective way to translate Thai to French automatically. The result could be an accessible tool for international research in the Thai and French languages
Detecting abusive language online is a classification problem with critical challenges that depend on machine understanding of natural language and the variety of rich and complex contexts in which natural language occurs. It has been shown that neural networks and standard Deep Learning (DL) techniques can detect explicit offensive content in conversations but it is more difficult to make machines detect more subtle, but also more common forms of so called 'passive toxicity'such as sarcasm, passive-aggressive behavior, and politely worded but incendiary hate speech. Traditional Machine Learning (ML) models and more recently word-based and character-based Convolution Neural Networks (CNN) and Recurrent Neural Networks (RNN) like Long Short-Term Memory (LSTM) models and Gated Recurrent Units (GRU), coupled with word embedding have been used to assist comment moderation and hate-speech detection. Furthermore, some work focuses on interpreting these neural networks detecting verbal aggression. As part of the Conversation AI research initiative at Google Jigsaw, our research will focus on exploring how new ground-breaking methods in deep learning can capture passive toxicity. Conversation AI has experience in developing both theoretical models and tools for users and websites. The proposed thesis will explore two orthogonal directions: labelled datasets and models. For supervised classification, we need large datasets of labeled comments from actual online conversations. We plan to start from the public datasets provided by the Jigsaw/Conversation-AI team. They can either be manually augmented with labels by crowdsourcing or automatically labeled from linguistic features such as tags. From initial experiments, these datasets contain significant number of passive toxic comments that get intermediate toxicity probabilities. We will follow the approach used by Perspective API of selecting a small number of likely categories for labelling different kinds of passive toxicity. We will leverage and improve the transformer and BERT models based on attention mechanisms for this application and see how they compare to other deep learning models. Because passive toxicity leverages ambiguity in natural language, we will focus on modeling the emotional impact of messages, which Conversation AI found resulted in higher levels of inter-annotator agreement (compared to more complex semantic distinctions that policy violation documents try to make). Methods of Reinforcement Learning (RL) may also be explored in this context. The other aspect of the research is to analyze and understand the origins and mechanisms of passive aggression. This can be done by measuring and visualizing the impact it has on conversations. Open Source code and publications in competitive venues will be produced during the PhD program. The models will be implemented in Python, with the TensorFlow framework. Metrics used for comparing architectures include accuracy, precision, recall, specificity, fall-out, F1 score, Receiver Operating Characteristic curve Area Under Curve (ROC-AUC). This research is expected to bring answers to various questions pertaining to conversations on social networks, such as: how to develop Deep Learning to identify passive toxicity in online conversations? Can it understand the relevance of posts in a debate? What triggers passive toxicity in conversation? What happens in a conversation right after passive aggression has been detected? Is it a one-time or an ongoing behavior from interlocutors? What are the replies to subtle toxicity that can forestall an *awry-turning* conversation? How does the early detection of it matter for keeping the conversation on track? We hope the resulting models will perform well enough that they end up being provided through the Perspective API so that they can be directly used by industry, and to support a wide range of other research initiatives. In order to bring people out of their online 'filter bubbles', they need to be able to have conversations on platforms safe from aggressiveness, violence and scapegoating. Machine Learning, Deep Learning and Natural Language Processing strategies can help us to 'disagree more constructively'(The Righteous Mind, Jonathan Haidt). Therefore, this research should allow to increase participation, quality, and empathy in online conversation at scale by developing new models applied to new datasets suggesting unbiased behaviors, guide and educate users about fairness. Using technology to enable more rational and informed debate enters into a goal of fighting against online harassment while defending public debates, freedom of speech, democracy and pluralism.
Nowadays, there is a growing demand for personalization and the “one-size-fits-all” approach for hypermedia systems is no longer applicable. Adaptive hypermedia (AH) systems adapt their behavior to the needs of individual users. However due to the complexity of their authoring process and the different skills required from authors, only few of them have been proposed. These last years, numerous efforts have been put to propose assistance for authors to create their own AH. However, as explained in this thesis some problems remain. In this thesis, we tackle two particular problems. A first problem concerns the integration of authors'materials (information and user profile) into models of existing systems. Thus, allowing authors to directly reuse existing reasoning and execute it on their materials. We propose a semi-automatic merging/specialization process to integrate an author's model into a model of an existing system. Our objectives are twofold: to create a support for defining mappings between elements in a model of existing models and elements in the author's model and to help creating consistent and relevant models integrating the two models and taking into account the mappings between them. A second problem concerns the adaptation specification, which is famously the hardest part of the authoring process of adaptive web-based systems. We propose an EAP framework with three main contributions: a set of elementary adaptation patterns for the adaptive navigation, a typology organizing the proposed elementary adaptation patterns and a semi-automatic process to generate adaptation strategies based on the use and the combination of patterns. Our objectives are to define easily adaptation strategies at a high level by combining simple ones. Furthermore, we have studied the expressivity of some existing solutions allowing the specification of adaptation versus the EAP framework, discussing thus, based on this study, the pros and cons of various decisions in terms of the ideal way of defining an adaptation language. We propose a unified vision of adaptation and adaptation languages, based on the analysis of these solutions and our framework, as well as a study of the adaptation expressivity and the interoperability between them, resulting in an adaptation typology. The unified vision and adaptation typology are not limited to the solutions analysed, and can be used to compare and extend other approaches in the future. Besides these theoretical qualitative studies, this thesis also describes implementations and experimental evaluations of our contributions in an e-learning application.
This dissertation concerns the description of possessive constructions in Tongugbe, one of the many dialects of the Ewe language, which is spoken in south-eastern Ghana, along the Volta River. It presents a detailed description of the constructions; and explores the relationship that exists between clausal possessive constructions and locative and existential constructions. In addition to this, the work presents a first outline grammar of Tongugbe. The grammar presents notably preliminary findings on the duration contrast in tones of Tongugbe and a rich demonstrative paradigm. The possessive constructions can be grouped into attributive, predicative and external possessor constructions. It is shown that the structural configurations of attributive possessive constructions are functionally motivated. It is also demonstrated that structural variations in predicative possessive and external possessor constructions correspond to differences in meaning. Finally, it is argued that, synchronically, clausal possessive constructions and locative and existential constructions are not reducible to a single structure. The view supported here then is that each construction is a form-meaning pair.
This thesis proposes a new approach to natural language processing. Rather than trying to estimate directly the probability distribution of a random sentence, we will detect syntactic structures in the language, which can be used to modify and create new sentences from an initial sample. The study of syntactic structures will be done using Markov substitute sets, sets of strings that can be freely substituted in any sentence without affecting the whole distribution. This point of view splits the issue of language analysis into two parts, a model selection stage where Markov substitute sets are selected, and a parameter estimation stage where the actual frequencies for each set are estimated. We show that these substitute processes form exponential families of distributions, when the language structure (the Markov substitute sets) is fixed. On the other hand, when the language structure is unknown, we propose methods to identify Markov substitute sets from a statistical sample, and to estimate the parameters of the distribution. Markov substitute sets show some connections with context-Free grammars, that can be used to help the analysis. We then proceed to build invariant dynamics for Markov substitute processes. They can among other things be used to effectively compute the maximum likelihood estimate. Indeed, Markov substitute models can be seen as the thermodynamical limit of the invariant measure of crossing-Over dynamics.
The purpose of this study was to consider the implementation of Problem Based Learning (PBL) as an epistemologically sound teaching methodology to teach English for Specific Purposes (ESP) and particularly English for Academic Medical Purposes (EAMP). The study examined whether PBL is compatible with language teaching and determined the benefits that this methodology can bring to ESP. The study also attempted to solve problems with English learning that were identified in the Preparatory Year Health Colleges (Female Branch) within Hail University, Saudi Arabia. A needs analysis was conducted in the institution to examine the English learning situation and better identify these learning problems. Then PBL was implemented to determine if it provided a possible solution to the issue. This entailed a change in the macro-methodological and micro-methodological levels, as Demaizière (1996) called 'le niveau macromethodologique' and 'le niveau micromethodologique' (p.66). In the empirical part of this study, a longitudinal study was conducted with 13 students who were observed through a period of 8 weeks and over five PBL tutorials, which took place over fifteen sessions. During these fifteen sessions, learners' behaviors or indicators of autonomy were observed at the group level for the first and third session of each PBL tutorial and at the individual level in session 2. In general, the results favored the implementation of this approach in teaching English for Academic Medical Purposes (EAMP).
To acquire their native language, babies have to learn both the forms of words (e.g., “dog” in English, “chien” in French) and their meanings (the category of dogs). These two aspects of language learning have typically been studied independently. However, recent findings from developmental psychology and machine learning have pointed out that this assumption is problematic, and have suggested that form and meaning may interact with one another throughout development. This dissertation explores this hypothesis through an interdisciplinary investigation that combines tools from speech recognition and experimental psychology. First, I developed a computational model of joint form and meaning acquisition, capable of learning from a corpus of natural speech. Second, I tested the cognitive plausibility of this model with adult subjects.
This analysis is tackled by training deep neural networks, which have reached competitive results in many domains. In this work, we focus on the understanding of daily life images, in particular on the interactions between objects and people that are visible in images, which we call visual relations. This provides a fuzzy set of relations that more accurately represents visible relations. Using the semantic similarities between relations, the model is able to learn to detect uncommon relations from similar and more common ones. However, the improved training does not always translate to improved detections, because the objective function does not capture the whole relation detection process. Thus during the inference phase, we combine knowledge to model predictions in order to predict more relevant relations, aiming to imitate the behaviour of human observers
Optimal Transport is a theory that allows to define geometrical notions of distance between probability distributions and to find correspondences, relationships, between sets of points. Many machine learning applications are derived from this theory, at the frontier between mathematics and optimization. How can it be adapted when the data are varied and not embedded in the same metric space? This thesis proposes a set of Optimal Transport tools for these different cases. More broadly, we analyze the mathematical properties of the various proposed tools, we establish algorithmic solutions to compute them and we study their applicability in numerous machine learning scenarii which cover, in particular, classification, simplification, partitioning of structured data, as well as heterogeneous domain adaptation.
Massive Open Online Courses (MOOCs) have seen their numbers increase significantly since the democratization of the Internet. Dashboards summarizing students'activites are regularly offered to instructors, but they do not allow them to understand collective activities in the forums. From a socio-constructivist point of view, the exchanges and interactions sought by instructors in forums are essential for learning (Stephens, 2014). So far, studies have analyzed interactions in two ways: semantically but on a small scale or statistically and on a large scale but ignoring the quality of the interactions. The scientific contribution of this thesis relates to the proposal of an interactive detection approach of collective activities which takes into account their temporal, semantic and social dimensions. We seek to answer the problem of detecting and observing the collective dynamics that take place in MOOC forums. By collective dynamic, we mean all the qualitative and quantitative interactions of learners in the forums and their temporal changes. But, unlike previous studies, our approach is not limited to global or individual-centered analysis.
This approach has a drawback: it requires long-term investment. It is then necessary to develop methods and computational tools to help the construction of such data that are required to be directly applicable to texts. This work focuses on a specific linguistic representation: local grammars that describe precise and local constraints in the form of graphs. Two issues arise: How to efficiently build precise, complete and text-applicable grammars? How to deal with their growing number and their dispersion? To handle the first problem, a set of simple and empirical methods have been exposed on the basis of M. Gross (1975)'s lexicon-grammar methodology. The whole process of linguistic analysis and formal representation has been described through the examples of two original phenomena: expressions of measurement (un immeuble d'une hauteur de 20 mètres) and locative prepositional phrases containing geographical proper names (à l'île de la Réunion). Each phenomenon has been narrowed to elementary sentences. This enables semantically classify them according to formal criteria. The syntactical behavior of these sentences has been systematically studied according to the lexical value of their elements. Then, the observed properties have been encoded either directly in the form of graphs with an editor or in the form of syntactical matrices then semi-automatically converted into graphs according to E. Roche (1993). These studies led to develop new conversion algorithms in the case of matrix systems where linguistic information is encoded in several matrices. For the second issue, a prototype on-line library of local grammars have been designed and implemented. The objective is to centralize and distribute local grammars constructed within the RELEX network of laboratories. We developed a set of tools allowing users to both store new graphs and search for graphs according to different criteria.
This dissertation addresses the questions of discourse modeling within a grammatical framework called Abstract Categorial Grammars (ACGs). ACGs provide a unified framework for both syntax and semantics. We focus on the discourse formalisms that make use of a grammatical approach to capture the discourse structure regularities. In particular, we propose ACG encodings of two discourse formalisms: G-TAG and D-STAG. Both G-TAG and D-STAG make use of an extra-grammatical processing to deal with discourse connectives that appear at clause-medial positions. In contrast, the ACG encodings of G-TAG and D-STAG offer a purely grammatical approach to clause-medial connectives. Each of these ACG encodings are second-order. Grammars of this class have reversibility properties that allow us to use the same polynomial algorithmes both for the discourse parsing and generation tasks.
A crucial issue in statistical natural language processing is the issue of sparsity, namely the fact that in a given learning corpus, most linguistic events have low occurrence frequencies, and that an infinite number of structures allowed by a language will not be observed in the corpus. Neural models have already contributed to solving this issue by inferring continuous word representations. These continuous representations allow to structure the lexicon by inducing semantic or syntactic similarity between words. However, current neural models only partially solve the sparsity issue, due to the fact that they require a vectorial representation for every word in the lexicon, but are unable to infer sensible representations for unseen words. This issue is especially present in morphologically rich languages, where word formation processes yield a proliferation of possible word forms, and little overlap between the lexicon observed during model training, and the lexicon encountered during its use. Today, several languages are used on the Web besides English, and engineering translation systems that can handle morphologies that are very different from western European languages has become a major stake. The goal of this thesis is to develop new statistical models that are able to infer in an unsupervised fashion the word formation processes underlying an observed lexicon, in order to produce morphological analyses of new unseen word forms.
Most statistical methods are not designed to directly work with incomplete data. The study of data incompleteness is not new and strong methods have been established to handle it prior to a statistical analysis. On the other hand, deep learning literature mainly works with unstructured data such as images, text or raw audio, but very few has been done on tabular data. Hence, modern machine learning literature tackling data incompleteness on tabular data is scarce. This thesis focuses on the use of machine learning models applied to incomplete tabular data, in an insurance context. We propose through our contributions some ways to model complex phenomena in presence of incompleteness schemes, and show that our approaches outperform the state-of-the-art models
Tensor factorization has been increasingly used to analyze high-dimensional low-rank data of massive scale in numerous application domains, including recommender systems, graph analytics, health-care data analysis, signal processing, chemometrics, and many others. In these applications, efficient computation of tensor decompositions is crucial to be able to handle such datasets of high volume. The main focus of this thesis is on efficient decomposition of high dimensional sparse tensors, with hundreds of millions to billions of nonzero entries,which arise in many emerging big data applications. We achieve this through three major approaches. In the first approach, we provide distributed memory parallel algorithms with efficient point-to-point communication scheme for reducing the communication cost. These algorithms are agnostic to the partitioning of tensor elements and low rank decomposition matrices, which allow us to investigate effective partitioning strategies for minimizing communication cost while establishing computational load balance. We use hypergraph-based techniques to analyze computational and communication requirements in these algorithms, and employ hypergraph partitioning tools to find suitable partitions that provide much better scalability. Second, we investigate effective shared memory parallelizations of these algorithms. Here, we carefully determine unit computational tasks and their dependencies, and express them using a proper data structure that exposes the parallelism underneath. Third, we introduce a tree-based computational scheme that carries out expensive operations(involving the multiplication of the tensor with a set of vectors or matrices, found at the core of these algorithms) faster by factoring out and storing common partial results and effectively re-using them. With this computational scheme, we asymptotically reduce the number of tensor-vector and -matrix multiplications for high dimensional tensors, and thereby render computing tensor decompositions significantly cheaper both for sequential and parallel algorithms. Finally, we diversify this main course of research with two extensions on similar themes. The first extension involves applying the tree-based computational framework to computingdense tensor decompositions, with an in-depth analysis of computational complexity and methods to find optimal tree structures minimizing the computational cost. The second work focuses on adapting effective communication and partitioning schemes of our parallel sparse tensor decomposition algorithms to the widely used non-negative matrix factorization problem,through which we obtain significantly better parallel scalability over the state of the art implementations. We point out that all theoretical results in the thesis are nicely corroborated by parallel experiments on both shared-memory and distributed-memory platforms. With these fast algorithms as well as their tuned implementations for modern HPC architectures, we render tensor and matrix decomposition algorithms amenable to use for analyzing massive scale datasets.
This PhD thesis deals with the pragmatic functions of 'you know', 'then' and 'ya'nĩ' (I mean) as autonomous syntactic structures and as operational and multifunctional linguistic units in conversation. Within a pragmatic framework, the research discusses the correlation between position and function, in which the pragmatic value of a marker in initial position is different from that conveyed in medial or final positions. The goal of this study is to contribute to the pragmatic analysis of conversations by analyzing political verbal interactions in Arabic and English television broadcasts. The results of this empirical study show that the multi-functionality of these linguistic items is related to their syntactic flexibility. These discourse markers imply a variety of contextual functions which evolve gradually on a pragmatic scale. The pragmatic evolution of 'you know', 'then' and 'ya'nĩ' generate complex semantic expressions. These pragmatic units in fact become increasingly complex; they go beyond their basic meaning to acquire progressively contextual implications. Thus, these markers refer to other interpretations and transcend their immediate semantic base. In this respect, they can be substituted according to the context of their occurrence. The results reveal that 'ya'nĩ' can be substituted with other markers from different grammatical categories in English. Pragmatic variation depends on the illocutionary perspective of the speaker, the relation with the hearer and the organization of the verbal interaction. In two distinct sociocultural situations, English and Arabic, it is confirmed that these linguistic items are contextual and multifunctional conversational units. Their role is relevant in a social situation where the interaction between the speaker and the hearer is a salient factor, as in the case of political verbal exchanges in television broadcasts.
Our doctoral research focuses on the influence of phonics instruction on first-grade students' progress. Its purpose is to identify effective teaching practices and to contribute to the training of teachers. This research is part of a larger study conducted by Roland Goigoux, which aimed to assess the influence of reading and writing on the quality of learning. The first part of our research examines causal relationships between the characteristics of phonics instruction and students' performances in decoding and spelling. First, we study the influence of the speed of teaching of grapheme-phoneme relationships (tempo) and of the decodable part of texts used to teach reading (rendement effectif). Our results reveal a significant influence of these two variables on the quality of learning, this influence being different according to students' initial levels. Besides, we propose a planning of the phonics instruction based on the theoretical frequency of the grapheme-phoneme correspondences in texts written in standard French which can serve as references for the teachers. We also study the effects of the teaching time allocated to encoding tasks on reading achievement, effects which appear to be significant and positive but which vary according to the nature of the tasks and to students' characteristics. In the second part of our dissertation, we attempt to analyze and document teaching practices of experienced first-grade teachers for training purposes. We analyze a reference situation of the teaching of reading and writing from the video recordings of thirty six collective sessions of reading. Then, we describe prototypical teaching scenarios and lay the foundations for a training intended to develop the professional skills of the teachers. Specifically, we raise the issue of the relationship between the resolution of decoding and understanding tasks and the autonomy that decoding success afforded the students. We finally present the digital platform we designed, which allows calculating the decodable part of texts used during reading instruction. This platform named Anagraph has been designed to help teachers plan the study of the grapheme-phoneme correspondences and to choose texts adapted to their teaching
Structured learning has become ubiquitous in Natural Language Processing; a multitude of applications, such as personal assistants, machine translation and speech recognition, to name just a few, rely on such techniques. Imitation learning aims to perform approximate learning and inference in order to better exploit richer dependency structures. In this thesis, we explore the use of this specific learning setting, in particular using the SEARN algorithm, both from a theoretical perspective and in terms of the practical applications to Natural Language Processing tasks, especially to complex tasks such as machine translation. Concerning the theoretical aspects, we introduce a unified framework for different imitation learning algorithm families, allowing us to review and simplify the convergence properties of the algorithms. With regards to the more practical application of our work, we use imitation learning first to experiment with free order sequence labelling and secondly to explore two step decoding strategies for machine translation.
The digital revolution has evolved the act of writing; its forms have changed. The knowledge gathered by this research introduces a new angle for setting up a new writing system. For that reason, sign languages offer the perfect opportunity for this kind of research. In this study we try to understand the link between gestures and meaning for the speaker and discover what features and how much of signing (gestures, body language) can be kept in the act of writing. Our objective is to maintain the integral meaning of gestures for the signer/writer. First, this multidisciplinary research focuses on what can be transferred from the former gestural act (signing) to the latter (writing). Then, we consider the tool that will enable this transfer. To do so, we follow a phenomenological approach, or in other terms, a descriptive methodology from the firstperson point of view. This methodology is built upon signers' feedback gathered of the experience lived during interviews. Shaping this method to fit the French SL offers precise gestural descriptions from signers themselves. This database is then compared with alinguistic and kinesiological analysis from the third-person point of view. These gestural meaning results enable us to reflect on how to create a guided experience tool enabling the assimilation of SL's gestural matter and the creation of scriptural forms. To do that, we follow a UX design, an enaction design, and a tool based approach in order to offer immersion and interaction. This kind of device offers a new perspective to signers on their own language and more generally, offers the possibility for any user to form a new relationship with her or his own gestures.
The Frank-Wolfe algorithms, a.k.a. conditional gradient algorithms, solve constrained optimization problems. They break down a non-linear problem into a series of linear minimization on the constraint set. This contributes to their recent revival in many applied domains, in particular those involving large-scale optimization problems. In this dissertation, we design or analyze versions of the Frank-Wolfe algorithms. We notably show that, contrary to other types of algorithms, this family is adaptive to a broad spectrum of structural assumptions, without the need to know and specify the parameters controlling these hypotheses.
Two independent subjects are studied in this thesis, the first of which consists of two distinct problems. In the first part, we begin with the Principal-Agent problem in degenerate systems, which appear naturally in partially observed random environment in which the Agent and the Principal can only observe one part of the system. Our approach is based on the stochastic maximum principle, the goal of which is to extend the existing results using dynamic programming principle to the degenerate case. Afterward, we use the sufficient condition of the Agent's problem to verify that the previously obtained optimal contract is indeed implementable. Meanwhile, a parallel study is devoted to the wellposedness of path-dependent FBSDEs in the chapter IV. Finally, we study the Principal-Agent problem with multiple Principals. The Agent can only work for one Principal at a time and therefore needs to solve an optimal switching problem. By using randomization, we show that the value function of the Agent's problem and his optimal control are given by an Itô process. This representation allows us to solve the Principal's problem in the mean-field case when there is an infinite number of Principals. We justify the mean-field formulation using an argument of backward propagation of chaos. The second part of the thesis consists of chapter V and VI. The motivation of this work is to give a rigorous theoretical underpinning for the convergence of gradient-descent type of algorithms frequently used in non-convex optimization problems like calibrating a deep neural network. We show that the corresponding energy function has a unique minimiser which can be characterized by some first order condition using derivatives in measure space. We present a probabilistic analysis of the long-time behavior of the mean-field Langevin dynamics, which have a gradient flow structure in 2-Wasserstein metric. By using a generalization of LaSalle's invariance principle, we show that the flow of marginal laws induced by the mean-field Langevin dynamics converges to the stationary distribution, which is exactly the minimiser of the energy function. As for deep neural networks, we model them as some continuous-time optimal control problems. Firstly, we find the first order condition by using Pontryagin maximum principle, which later helps us find the associated mean-field Langevin system, the invariant measure of which is again the minimiser of the optimal control problem. As last, by using the reflection coupling, we show that the marginal distribution of the mean-field Langevin system converges to the unique invariant measure exponentially.
Research has shown that non-literalness is pervasive in language and that it is not always an ornamental device (e.g. to invest time in something, to be in love, the leg of a table, etc.). As an attempt to bridge this gap, I propose a comparative study of figurative language development in first and second language acquisition. Then, I propose an L1/L2 comparative study where I analyze semi-guided interactions taking place between native English-speaking children (aged 7, 11 and 15), French learners of English (in their first year of high school, first year of B.A. in English studies and last year of M.A. in English studies), as well as native English-speaking adults. The results of this PhD project revealed many similar aspects in the figurative productions of native English-speaking children and French students. Lastly, I observed a large amount of deviant figurative forms in the leaner's productions, mainly resulting from L1 transfers and lexical overextensions. Taking into account these observations, implications for teaching are presented.
Real-time embedded systems are increasingly omnipresent in everyday life. The development cycle of critical systems can take months or even years. Therefore, modeling of these systems should be analyzed at an early stage in the development cycle to verify that all requirements are met, including temporal requirements (e.g., latencies, end-to-end delays). This thesis, which was funded as part of FUI project, offers three major contributions. The first contribution relates to mono-processor task system with deterministic multi-periodic communication relationships. A pattern based on Semaphore Precedence Constraint (SPC) has been extended to support cycles in the case of dynamic priority scheduling. An unfolding graph has also been proposed in order to present the cyclicity of SPC-based systems and guarantee deadlock free. The extended SPC pattern and the corresponding scheduling analysis tests have been implemented for the standard AADL language. The second contribution of this thesis consists in proposing an exact calculation of end-to-end response time using the time Petri net formalism for identical multiprocessor systems. It takes into account the dependence between the tasks: precedence and mutual exclusion. The third contribution concerns the capitalization of the efforts of the analysis process. Indeed, many analytical tests have been proposed, notably by academic researchers, based on scheduling theory and dedicated to the different software and hardware architectures. However, one of the main difficulties encountered by designers is to choose the most appropriate analysis test to validate and/or correctly dimension their designs. In industrial environment, there are few analytical tests used despite the multitude of the tests offered. This thesis therefore aims to facilitate the analysis process, reduce the semantic gap between the business model and the entries in the analysis tests as well as accelerate technology transfer and the adoption of academic tests. The thesis proposes an analysis repository playing the role of a dictionary of tests, their contexts for correct use, the tools implementing them, as well as a mechanism for choosing tests according to the input business model.
In a user-centered design process, artifacts evolve in iterative cycles until they meet user requirements and then become the final product. Every cycle gives the opportunity to revise the design and to introduce new requirements which might affect the artifacts that have been set in former development phases. Keeping the consistency of requirements in such artifacts along the development process is a cumbersome and time-consuming activity, especially if it is done manually. Nowadays, some software development frameworks implement Behavior-Driven Development (BDD) and User Stories as a means of automating the test of interactive systems under construction. Automated testing helps to simulate user's actions on the user interface and therefore check if the system behaves properly and in accordance with the user requirements. However, current tools supporting BDD requires that tests should be written using low-level events and components that only exist when the system is already implemented. As a consequence of such low-level of abstraction, BDD tests can hardly be reused with more abstract artifacts. In order to prevent that tests should be written to every type of artifact, we have investigated the use of ontologies for specifying both requirements and tests once, and then run tests on all artifacts sharing the ontological concepts. The resultant behavior-based ontology we propose herein is therefore aimed at raising the abstraction level while supporting test automation on multiple artifacts. This thesis presents this ontology and an approach based on BDD and User Stories to support the specification and the automated assessment of user requirements on software artifacts along the development process of interactive systems. Two case studies are also presented to validate our approach. The first case study evaluates the understandability of User Stories specifications by a team of Product Owners (POs) from the department in charge of business trips in our institute. With the help of this first case study, we designed a second one to demonstrate how User Stories written using our ontology can be used to assess functional requirements expressed in different artifacts, such as task models, user interface (UI) prototypes, and full-fledged UIs. The results have shown that our approach is able to identify even fine-grained inconsistencies in the mentioned artifacts, allowing establishing a reliable compatibility among different user interface design artifacts.
In the rise of the internet, user-generated content from social networking services is becoming a giant source of information that can be useful to businesses on the aspect where users are viewed as customers or potential customers for companies. Exploitation of user-generated texts can help identify their feelings, intentions, or reduce the effort of the agents who are responsible for collecting or receiving information on social networking services. As part of this thesis, the content of texts such as speeches, statements, conversations from interactive communication on social media platforms become the main data object of our study. We propose a method for extracting a CCG tree from the dependency structure of the sentence, and a general architecture to build a bridge of relationship between syntaxes and semantics of French sentences.
This dissertation explores how embedding small data-driven contextual visualizations can complement text documents. More specifically, I identify and define important aspects and relevant research directions for the integration of small data-driven contextual visualizations into text. This integration should eventually become as fluid as writing and as usable as reading a text. I define word-scale visualisations as small data-driven contextual visualizations embedded in text documents. These visualizations can use various visual encodings including geographical maps, heat maps, pie charts, and more complex visualizations. They can appear at a range of word scales, including sizes larger than a letter, but smaller than a sentence or paragraph. Word-scale visualisations can help support and be used in many forms of written discourse such as text books, notes, blog posts, reports, stories, or poems. As graphical supplements to text, word-scale visualisations can be used to emphasize certain elements of a document (e.g. a word or a sentence), or to provide additional information. For example, a small stock chart can be embedded next to the name of a company to provide additional information about the past trends of its stocks. In another example, game statistics can be embedded next to the names of soccer teams or players in daily reports from the UEFA European Championship. These word-scale visualisations can then for example allow readers to make comparison between number of passes of teams and players. The main benefit of word-scale visualisations is that the reader can remain focused on the text as the visualization are within the text rather than alongside it. I investigate placement options to embed word-scale visualisations and quantify their effects on the layout and flow of the text. I also explore how word-scale visualisations can be combined with interaction to support a more active reading by proposing interaction methods to collect, arrange and compare word-scale visualisations. Finally, I propose design considerations for the authoring of word-scale visualisations and conclude with application examples. In summary, this dissertation contributes to the understanding of small data-driven contextual visualizations embedded into text and their value for Information Visualization.
Geometric data analysis, beyond convolutionsTo model interactions between points, a simple option is to rely on weighted sums known as convolutions. Over the last decade, this operation has become a building block for deep learning architectures with an impact on many applied fields. We should not forget, however, that the convolution product is far from being the be-all and end-all of computational mathematics. To let researchers explore new directions, we present robust, efficient and principled implementations of three underrated operations: 1. Optimal transport, which generalizes sorting to spaces of dimension D &gt; 1.3. Hamiltonian geodesic shooting, which replaces linear interpolation when no relevant algebraic structure can be defined on a metric space of features. Our PyTorch/NumPy routines fully support automatic differentiation and scale up to millions of samples in seconds. They generally outperform baseline GPU implementations with x10 to x1,000 speed-ups and keep linear instead of quadratic memory footprints. These new tools are packaged in the KeOps (kernel methods) and GeomLoss (optimal transport) libraries, with applications that range from machine learning to medical imaging. Documentation is available at: www.kernel-operations.io/keops and /geomloss.
While our representation of the world is shaped by our perceptions, our languages, and our interactions, they have traditionally been distinct fields of study in machine learning. Fortunately, this partitioning started opening up with the recent advents of deep learning methods, which standardized raw feature extraction across communities. However, multimodal neural architectures are still at their beginning, and deep reinforcement learning is often limited to constrained environments. Yet, we ideally aim to develop large-scale multimodal and interactive models towards correctly apprehending the complexity of the world. As a first milestone, this thesis focuses on visually grounded language learning for three reasons (i) they are both well-studied modalities across different scientific fields (ii) it builds upon deep learning breakthroughs in natural language processing and computer vision (ii) the interplay between language and vision has been acknowledged in cognitive science. More precisely, we first designed the GuessWhat?! game for assessing visually grounded language understanding of the models: two players collaborate to locate a hidden object in an image by asking a sequence of questions. Finally, we investigate how reinforcement learning can support visually grounded language learning and cement the underlying multimodal representation. We show that such interactive learning leads to consistent language strategies but gives raise to new research issues.
This thesis defines a semantic model, non probabilist and predictive, for the decisional analysis of professional and institutional social networks. The presented multidisciplinary model, in parallel to the Galam sociophysics, integrates some semantic methods of natural language processing and knowledge engineering, some measures of statistic sociology and some electrodynamic laws, applied to the economic performance and social climate optimisation. It has been developped and experimented in line with the Socioprise project, funded by the French State Secretariat for the prospective and development of the digital economy.
Multiple Sclerosis (MS) is the most common progressive neurological disease of young adults worldwide and thus represents a major public health issue with about 90,000 patients in France and more than 500,000 people affected with MS in Europe. In order to optimize treatments, it is essential to be able to measure and track brain alterations in MS patients. In fact, MS is a multi-faceted disease which involves different types of alterations, such as myelin damage and repair. Under this observation, multimodal neuroimaging are needed to fully characterize the disease. Magnetic resonance imaging (MRI) has emerged as a fundamental imaging biomarker for multiple sclerosis because of its high sensitivity to reveal macroscopic tissue abnormalities in patients with MS. Conventional MR scanning provides a direct way to detect MS lesions and their changes, and plays a dominant role in the diagnostic criteria of MS. Moreover, positron emission tomography (PET) imaging, an alternative imaging modality, can provide functional information and detect target tissue changes at the cellular and molecular level by using various radiotracers. However, in clinical settings, not all the modalities are available because of various reasons. In this thesis, we therefore focus on learning and predicting missing-modality-derived brain alterations in MS from multimodal neuroimaging data.
This dissertation explores how misogyny may target language uses which may be perceived as feminine and centers on the "Valley Girl" stereotype. This term was popularized in the 1980s by Frank Zappa's eponymous single and originally referred to supposedly vain and unintelligent female teenagers who belonged to the Californian middle class. Though Valley Girls were ridiculed in the song, the impact it had launched a craze that manifested linguistically in Valspeak. This dialect comprises markers which are mainly phonetic (the California Vowel Shift), prosodic (the High Rising Terminal contour), lexical ("fer sure," "gag me with a spoon"), or that can be found at the discourse level (LIKE). Though some of these markers were not (solely) popularized by Valley Girls, they may nevertheless be perceived as such, and a speaker using them may trigger negative social evaluations. This research explores how the potential stigmatizing perception of Valspeak may be linked to misogyny, which is a phenomenon we refer to as the "linguistic misogyny" of Valspeak. To what extent may linguistic stigma be induced by the gender of the prototypical speakers of this dialect? Three main analyses are provided. First, a quantitative perceptual dialectology study of three Valspeak markers (the California Vowel Shift, the High Rising Terminal contour, and LIKE) is conducted with native American English speakers. Then, qualitative interviews are carried out in order to determine what ideologies are associated with Valspeak markers and the Valley Girl persona. The third part of the analysis focuses on three humorous representations of female characters in television programs: Parks and Recreation, Family Guy, and Ew! (a segment on The Tonight Show Starring Jimmy Fallon). It is suggested that Valspeak markers may be recruited in order to portray intellectually-challenged female characters without explicitly referring to the Valley Girl stereotype.
With the rapid growth of digital video content, automatic video understanding has become an increasingly important task. Video understanding spans several applications such as web-video content analysis, autonomous vehicles, human-machine interfaces (eg, Kinect). This thesis makes contributions addressing two major problems in video understanding: webly-supervised action detection and human action localization. Webly-supervised action recognition aims to learn actions from video content on the internet, with no additional supervision. We propose a novel approach in this context, which leverages the synergy between visual video data and the associated textual metadata, to learn event classifiers with no manual annotations. We show the importance of both the main steps of our method, ie,query generation and data pruning, with quantitative results. We evaluate this approach in the challenging setting where no manually annotated training set is available, i.e., EK0 in the TrecVid challenge, and show state-of-the-art results on MED 2011 and 2013 datasets. In the second part of the thesis, we focus on human action localization, which involves recognizing actions that occur in a video, such as "drinking" or "phoning", as well as their spatial and temporal extent. We propose a new person-centric framework for action localization that tracks people in videos and extracts full-body human tubes, i.e., spatio-temporal regions localizing actions, even in the case of occlusions or truncations. The motivation is two-fold. First, it allows us to handle occlusions and camera viewpoint changes when localizing people, as it infers full-body localization. Our tracking algorithm connects the image detections temporally to extract full-body human tubes. We evaluate our new tube extraction method on a recent challenging dataset, DALY, showing state-of-the-art results.
This study presents the characteristics, writing systems and structure of Uyghur language by doing a linguistic study. Our approach will consist of new trial models that facilitate the development and realization of Uyghur software tools, and contribute to the Uyghur information technology. More precisely, our study consists of four phases: Firstly, we are going to present the main issues of the study, characteristics of the language and its writing systems, especially the unification procedure of the Latin-Script Uyghur. Secondly, we briefly introduce some basic notions for the retrieval of information, and we will do a demonstration of named entities retrieval, using an extraction tool, in order to test concepts and theories that we are proposing. Then, we will discuss linguistic issues – mainly on the agglutinative aspect and morphological suffixation rules – which are applied during the implementation of prototype tools proposed in this study. Finally, we underline problems in natural language processing (NLP) created by Uyghur language and non-Uyghur supporting environments. We will discuss the existing difficulties and we will suggest innovative solutions to resolve such problems with the following fields: Standardization of Uyghur fonts and creation of a Unicode based Uyghur font, Implementation of system-level and browser-level input methods and-reation of multi-script converting tools, Realization of an online Uyghur – English dictionary, Implementation of a lexical generator based on the morphological suffixation rules of Uyghur, Design and creation of an suffix analyzer and explorer, Demonstration of Uyghur information retrieval, Implementation of a parser and spell checker
The benefit of performing Big data computations over individual's microdata is manifold, in the medical, energy or transportation fields to cite only a few, and this interest is growing with the emergence of smart-disclosure initiatives around the world. To regain indivuals' trust, it becomes essential to propose user empowerment solutions, that is to say allowing individuals to control the privacy parameter used to make computations over their microdata. This work proposes a novel concept of personalized anonymisation based on data generalization and user empowerment. Moreover, we propose a decentralized computing infrastructure based on secure hardware enforcing these personalized privacy guarantees all along the query execution process. Secondly, this manuscript studies the personalization of anonymity guarantees when publishing data. We propose the adaptation of existing heuristics and a new approach based on constraint programming. Experiments have been done to show the impact of such personalization on the data quality. Individuals' privacy constraints have been built and realistically using social statistic studies
This thesis offers a multidimensional (sociolinguistic, phonetic, and phonological) description and study of the variety of English spoken in Greater Manchester. We discuss the study of linguistic change and the use of corpora in linguistics from a methodological and epistemological point of view. Our work is conducted in the framework of the PAC programme (Phonology of Contemporary English: usage, varieties and structure) and within the LVTI project (Language, Urban Life, Work, Identity), and based on the PAC-LVTI Manchester corpus, which is composed of authentic and recent fieldwork data. Our analysis notably focuses on the phenomenon of regional dialect levelling, which has been largely documented in recent English sociolinguistic research. In particular, we are interested in the hypothesis of the expansion of a supralocal variety in the north of England. Our study deals mainly with the vowels of Greater Manchester English, and relies on a phonetic-acoustic analysis of our informants' realisations. We describe the major characteristics of the Mancunian variety based on the few studies published so far, and statistically evaluate their correlation with traditional sociolinguistic factors such as age, gender or socio-economic profile. We also explore the relevance of attitudinal factors for the study of our data. We discuss the evolutions of the system in the light of regional dialect levelling, and question the role played by internal and external factors in these linguistic changes.
This PhD thesis is the result of my research work in the machine learning, image processing and intelligent transportation field for solving the problem of multi-task pedestrian protection system (PPS) including not only pedestrian classification, detection and tracking, but also pedestrian action-unit classification and prediction, and finally pedestrian risk estimation. Moreover, our PPS system uses original cross-modality deep learning approaches. The goal of our research work is to develop an intelligent pedestrian protection component-based only on single stereo vision system using an optimal cross-modality deep learning architecture in order to classify the current pedestrian action, predict their next actions and finally to estimate the pedestrian risk by the time to cross for each pedestrian. First, we investigate the classification component where we analyzed how learning representations from one modality would enable recognition for other modalities within various deep learning, which one term as cross-modality learning. Second, we study how the cross-modality learning improves an end-to-end the pedestrian action.
Based on a Publish/Subscribe paradigm, Web Syndication formats such as RSS have emerged as a popular means for timely delivery of frequently updated Web content. According to these formats, information publishers provide brief summaries of the content they deliver on the Web, while information consumers subscribe to a number of RSS feeds and get informed about newly published items. We propose a keyword-based index for user subscriptions to match it on the fly with incoming items. We study three indexing techniques for user subscriptions. We present analytical models to estimate memory requirements and matching time. We also conduct a thorough experimental evaluation to exhibit the impact of critical workload parameters on these structures. For subscriptions which are never notified, we adapt the indexes to support a partial matching between subscriptions and items.
The rise of digital media technology over the last decades has transformed the way in which organizations are evaluated. Every day, on a plurality of platforms and websites, individuals disclose information about their interactions with organizations and their products or services. Compared to traditional media or professional critics, digital users and customers tend to share subjective and partial experiences, have lower concerns for accuracy and balance, and often put emphasis on the emotional content. As more customers rely on this information for their purchasing choices, firms in many industries find themselves in a position where it is hard to ignore the opinions expressed online by customers as inconsequential. In this thesis, I study how the strategies and behaviors of organizations are affected by this “democratization” of evaluation process. The empirical setting for my analyses is the fine-dining industry. In the first chapter, I study online reviews as a source of information for restaurants, which may learn about problems, errors, or improvement opportunities. I examine what features of customer feedback make it more likely to be considered by target restaurants. With an online experiment in the French restaurant industry, I find that decision makers allocate attention to feedback that is expected to have a stronger impact on the reputation and performance of the restaurant. In the second chapter, I analyze the effects of the interaction between amateur and expert evaluations. In particular, I study the entry of an expert evaluator (i.e., Michelin guide) in a market, and how it pushes some organizations to make strategic choices that signal their aspirations. Drawing on literature on organizational status, I find that restaurants better rated by Michelin make changes to their offer with the aim to self-identify with the élite group. These changes consist in the adoption or removal of certain features displayed in their menus. In addition, by using topic modeling techniques applied to Yelp reviews, I observe that customers' reactions to the entry of Michelin make restaurants more or less sensitive to the expert's evaluations. In the third chapter, I focus on how organizations use public responses to customers to address criticism in online settings. Recent studies are not conclusive on the reputational benefits of public responses to reviews. These responses may reduce the likelihood of future negative reviews while, at the same time, draw attention to problems. Building on existing literature on reputation and impression management, I propose that organizations may resolve this trade-off by making a strategic use of different types of verbal accounts (e.g., apology). Although public responses to customers may be counterproductive, adapting the style of public responses to the features of customer reviews might be an optimal strategy for organizations. For this study I analyze restaurant reviews in France and the United States using standard econometric models supported by supervised learning techniques.
This case study focuses on the integration of digital tools to the teaching and learning of English as a foreign language. It aims to describe and analyse the different processes that contribute to the emergence of the social dimension of autonomy. It is based on the idea that several processes come to the fore during the realization of a situated and mediated collective activity accomplished by small groups of learners. In order to study these processes, a digital storytelling task-project was proposed to groups of learners enrolled in the first year of university of a pre-service English language-training course. The materials designed by the learners were then tested as pedagogical resources by teachers of seven primary schools in France. A systemic modelling of the above activity makes it possible to identify three focus areas that contribute to the emergence of social autonomy. The first relates to the intra group operations of the small groups who accomplished the task-project. The second is the contribution of the community of primary school teachers who tested the teaching materials. The third constitutes the ways in which social digital tools were used by small groups to collaborate and work together. The results from the analyses related to the three focus areas allow for a modelling of certain variables that contribute to the understanding of the emergence of social autonomy.
Bilingual corpora are an essential resource used to cross the language barrier in multilingual Natural Language Processing (NLP) tasks. Most of the current work makes use of parallel corpora that are mainly available for major languages and constrained areas. Comparable corpora, text collections comprised of documents covering overlapping information, are however less expensive to obtain in high volume. Previous work has shown that using comparable corpora is beneficent for several NLP tasks. Apart from those studies, we will try in this thesis to improve the quality of comparable corpora so as to improve the performance of applications exploiting them. The idea is advantageous since it can work with any existing method making use of comparable corpora. We first discuss in the thesis the notion of comparability inspired from the usage experience of bilingual corpora. The notion motivates several implementations of the comparability measure under the probabilistic framework, as well as a methodology to evaluate the ability of comparability measures to capture gold-standard comparability levels. The comparability measures are also examined in terms of robustness to dictionary changes. The experiments show that a symmetric measure relying on vocabulary overlapping can correlate very well with gold-standard comparability levels and is robust to dictionary changes. Based on the comparability measure, two methods, namely the greedy approach and the clustering approach, are then developed to improve the quality of any given comparable corpus. The general idea of these two methods is to choose the high quality subpart from the original corpus and to enrich the low-quality subpart with external resources. The experiments show that one can improve the quality, in terms of comparability scores, of the given comparable corpus by these two methods, with the clustering approach being more efficient than the greedy approach. The enhanced comparable corpus further results in better bilingual lexicons extracted with the standard extraction algorithm. Lastly, we investigate the task of Cross-Language Information Retrieval (CLIR) and the application of comparable corpora in CLIR. We develop novel CLIR models extending the recently proposed information-based models in monolingual IR. The information-based CLIR model is shown to give the best performance overall. Bilingual lexicons extracted from comparable corpora are then combined with the existing bilingual dictionary and used in CLIR experiments, which results in significant improvement of the CLIR system.
Our main goal was to define a theoretical and computational framework allowing formal modeling and automatic exploration of various discursive structures involved in this textual organization. We notably propose to describe those structures using the three elementary categories of units, relations and schemas, and outline recurrent properties of discursive patterns and clues which signal their presence: variable granularity, fuzziness, possible non-linearity and non-sequentiality, local/global interactions... In order to give a formal description of the studied linguistic phenomena and to make their computational analysis possible, in a corpus-based approach, we propose the CDML formalism (Contraint-based Discourse Modeling Language). A CDML parser has been implemented and may be used to apply such a formal description to a corpus and automatically detect textual structures satisfying the given constraints. The second considers contrastive relations between various kind of textual objects, at different granularity levels.
The present dissertation is concerned with the use and interpretation of null and pronominal subjects in Brazilian Portuguese. This investigation examines these phenomena in an attempt to disentangle the semantic and discursive factors that can be relevant for choice between these anaphoric expressions in Brazilian Portuguese and the way in which this choice is articulated with the general theory of anaphora resolution. The starting point of this dissertation was the research looking into null and overt subjects from the perspective of Generative Grammar, specially the Parametric Theory. Throughout the present work, however, the analyses proposed in this perspective were shown not to account for the data at stake. The generalization that poor verbal morphology is directly related to the absence or reduced frequency of null subjects, for example, is challenged through experimental data and an investigation of the relative frequency of null subjects across discourse persons in corpora. An alternative explanation presented in the previous literature, namely the importance of the antecedents' features of Animacy and Specificity, seems to better account for the attested distribution. However, this explanation is not sufficient for understanding the choice between null and overt subjects in Brazilian Portuguese, since the number of animate and specific null subjects is still relatively higher than in languages with obligatory expression of subjects. Therefore, it is argued that discourse factors seem to play a crucial role in the use of null and overt subjects in Brazilian Portuguese. The first is a standard feature in the literature about anaphora resolution (expressed by a variety of terms, such as Salience, Familiarity, Accessibility, etc.), which is part of the reverse mapping hypothesis according to which the more obvious the subject is, the less explicit the co-referential form is allowed to be. The second factor, Contrast, is the main finding of the present dissertation: as is the case for other levels of linguistic analyses and other phenomena in language, the choice of anaphoric expression in Brazilian Portuguese seems to be driven by efficiency. In the present case, this means that, when the backgrounded information and the asserted (focused) information in an utterance contrast the most, it is more likely that a null subject will be used. The design of a grammar that deals with these multiple features is sketched, specifically, a multi-layered scalar probabilistic grammar is proposed, whose semantic and discourse constraints act in parallel through a probabilistic mapping. It is, thus, shown that null subjects are likely in discursive co-reference, since in these contexts their antecedents are more obvious and the focused information contrasts the most with the background. An apparent counter-example to the proposal sketched here is analyzed: the generic interpretation of null subjects. However, it is shown that the same semantic constraints cross-linguistically applied to other generic constructions can produce generic null subjects in Brazilian Portuguese, given the failure to be grounded predicted by the approach proposed here. Finally, on-line evidence for the analysis of the use and interpretation of null and pronominal subjects is provided. The results found in three eye-tracking while reading experiments provide striking evidence in favor of the proposal put forward here, according to which null and overt subjects and their interpretation can be accounted for in terms of constraints on interpretation rather than licensing.
Lexical frozeness is one of the main obstacles to automatic processing of natural language. The present work intends to be a contribution to improve automatic processing applied to fossilized verbal sequences (SVF) such as casser sa pipe. Notions of degrees of lexical frozeness and double structuration guided our study. In order to determinate the degree of lexical frozeness of SVF, we analysed their internal structure and their external structuration. Consequently, they can be included in predicates classes developped in L. L. I., based on G. GROSS theory of object classes. After a survey of publications about (verbal) lexical frozeness, we identify and collect criteria to measure SVF degrees of lexical frozeness through the general structure analysis of [V SN SP] sequences. Then, we propose formal tools for automatic recognition of metaphor through analysis of SVF coming from sports. Lastly, we described two syntactico-semantic classes of predicats: <deplacement>; and &lt;états humains&gt;.
The thesis begins with a short historical reminder of ethnomethodology, considered as a scientific field, since the whole beginners during the 30's until the 1967 explosion in US and Europe. The second part of the thesis is devoted to the concrete application of these theoretical concepts in the field of technological strategies which have been elaborated in France in the area of natural language processing. Three studies successively describe the ethnomethods and rational properties of practical activities which are used in an administrative team, the elaboration of a technology policy and indexical descriptions of the language industry field. The conclusion tries to show how the concepts and methods developped by ethnomethodology can increase, in this field, the efficacy of strategical analysis and the quality of research and development programs
With the advent and increasing popularity of Computer Supported Collaborative Learning (CSCL) and e-learning technologies, the need of automatic assessment and of teacher/tutor support for the two tightly intertwined activities of comprehension of reading materials and of collaboration among peers has grown significantly. Whereas a shallow or surface analysis is easily achievable, a deeper understanding of the discourse is required, extended by meta-cognitive information available from multiple sources as self-explanations. As specificity of the analysis, in terms of individual learning we have focused on the identification of reading strategies and on providing a multi-dimensional textual complexity model integrating surface, word specific, morphology, syntax and semantic factors. Complementarily, the collaborative learning dimension is centered on the evaluation of participants' involvement, as well as on collaboration assessment through the use of two computational models: a polyphonic model, defined in terms of voice inter-animation, and a specific social knowledge-building model, derived from the specially designed cohesion graph corroborated with a proposed utterance scoring mechanism. Our approach integrates advanced Natural Language Processing techniques and is focused on providing a qualitative estimation of the learning process. Various cognitive validations for all our automated evaluation systems have been conducted and scenarios including the use of ReaderBench, our most advanced system, in different educational contexts have been built. One of the most important goals of our model is to enhance understanding as a “mediator of learning” by providing automated feedback to both learners and teachers or tutors. The main benefits are its flexibility, extensibility and nevertheless specificity for covering multiple stages, starting from reading classroom materials, to discussing on specific topics in a collaborative manner, and finishing the feedback loop by verbalizing metacognitive thoughts in order to obtain a clear perspective over one's comprehension level and appropriate feedback about the collaborative learning processes.
The contribution of this PhD belongs to the domain of Natural Language Processing, Information Extraction, Information Retrieval and Cognitive Science in a context of Geomatics. In this context, our contribution aims at automatically processing texts telling travel stories to interpret the geographical information they contain and more particularly the itineraries they describe. The approach we propose is an incremental method to discover the semantics of the document: starting from semantics of a simple nominal syntagm level - through proposition level semantics- Our contribution lies in the proposition of a task driven conceptual model (for IR and the conception of educational activities), the proposition of an approach to characterise the itinerary in the text and the proposition of an operating process to link the characterization of the itinerary in the text with its conceptual model. Our approach is clearly an experimental one, that is why we rely our works on a prototype that we developed. This prototype (PIIR for Prototype pour l'Interprétation d'Itinéraires dans des Récits) allows us to confront our analyses and algorithms with automatic aimed treatment.
Since the 90s, Internet is at the heart of the labor market. First mobilized on specific expertise, its use spreads as increase the number of Internet users in the population. Seeking employment through "electronic employment bursary" has become a banality and e-recruitment something current. This information explosion poses various problems in their treatment with the large amount of information difficult to manage quickly and effectively for companies. We present in this PhD thesis, the work we have developed under the E-Gen project, which aims to create tools to automate the flow of information during a recruitment process. We interested first to the problems posed by the routing of emails. The ability of a companie to manage efficiently and at lower cost this information flows becomes today a major issue for customer satisfaction. After, we present work that was conducted as part of the analysis and integration of a job ads via Internet. We present a solution capable of integrating a job ad from an automatic or assisted in order to broadcast it quickly. Based on a combination of classifiers systems driven by a Markov automate, the system gets very good results. Thereafter, we present several strategies based on vectorial and probabilistic models to solve the problem of profiling candidates according to a specific job offer to assist recruiters. We have evaluated a range of measures of similarity to rank candidatures by using ROC curves. Relevance feedback approach allows to surpass our previous results on this task, difficult, diverse and higly subjective.
Our research described in this thesis is about the learning of a motif-based representation from time series to perform automatic classification. Meaningful information in time series can be encoded across time through trends, shapes or subsequences usually with distortions. Approaches have been developed to overcome these issues often paying the price of high computational complexity. Among these techniques, it is worth pointing out distance measures and time series representations. We focus on the representation of the information contained in the time series. This framework proposes to transform a set of time series into a feature space, using subsequences enumerated from the time series, distance measures and aggregation functions. One particular instance of this framework is the well-known shapelet approach. The potential drawback of such an approach is the large number of subsequences to enumerate, inducing a very large feature space and a very high computational complexity. We show that most subsequences in a time series dataset are redundant. Therefore, a random sampling can be used to generate a very small fraction of the exhaustive set of subsequences, preserving the necessary information for classification and thus generating a much smaller feature space compatible with common machine learning algorithms with tractable computations. We also demonstrate that the number of subsequences to draw is not linked to the number of instances in the training set, which guarantees the scalability of the approach. The combination of the latter in the context of our framework enables us to take advantage of advanced techniques (such as multivariate feature selection techniques) to discover richer motif-based time series representations for classification, for example by taking into account the relationships between the subsequences. These theoretical results have been extensively tested on more than one hundred classical benchmarks of the literature with univariate and multivariate time series. Moreover, since this research has been conducted in the context of an industrial research agreement (CIFRE) with Arcelormittal, our work has been applied to the detection of defective steel products based on production line's sensor measurements.
Nigerian learners of French as a Foreign Language are generally faced with difficulties while using French Past Tenses in producing written composition. In this thesis, we are particularly interested in the case of the Yoruba learners of French language. The analysis of their written composition copies reveals that most of the errors committed originate from the mother tongue, Yoruba which does not know the tense-markedness of French language with her conjugation and complicated verb endings. On the other hand, through our analysis of copies of two objective tests in which students were to produce the missing verb forms, we also found that the learners lack some theoretical linguistic knowledge which is important in understanding French past tenses: for instance, Benveniste's “Discours &amp; récit” and Weinrich's “Premier plan / Arrière-plan”. We believe that students would better understand the use of French past tenses if they have a good grasp of the “grammatical aspect” notion and if this linguistic notion is taken into account while teaching the topic. We brought the research to a close with different suggestions on how to improve the teaching / learning of the French tenses concerned here. On the whole, placing oneself on the didactic perspective, we are of the opinion that all these information put together can help develop a Methodology for the teaching and learning of French past tenses; and by so doing, advance the more the cause of the teaching and learning of French in Nigeria.
Many problems in machine learning are naturally cast as the minimization of a smooth function defined on a Euclidean space. For supervised learning, this includes least-squares regression and logistic regression. In this manuscript, we consider the particular case of the quadratic loss. In the first part, we are interestedin its minimization when its gradients are only accessible through a stochastic oracle. In the second part, we consider two applications of the quadratic loss in machine learning: clustering and estimation with shape constraints. This new framework suggests an alternative algorithm that exhibits the positive behavior of both averaging and acceleration. The second main contribution aims at obtaining the optimal prediction error rates for least-squares regression, both in terms of dependence on the noise of the problem and of forgetting the initial conditions. Our new algorithm rests upon averaged accelerated gradient descent. The third main contribution deals with minimization of composite objective functions composed of the expectation of quadratic functions and a convex function. Weextend earlier results on least-squares regression to any regularizer and any geometry represented by a Bregman divergence. As a fourth contribution, we consider the the discriminative clustering framework. We propose its first theoretical analysis, a novel sparse extension, a natural extension for the multi-label scenario and an efficient iterative algorithm with better running-time complexity than existing methods. The fifth main contribution deals with the seriation problem. We propose a statistical approach to this problem where the matrix is observed with noise and study the corresponding minimax rate of estimation.
The Artificial Intelligence (AI) field has been unprecedented growing in recent years with spectacular, high-profile successes. In fact, AI is applied in many fields ranging from computer vision to natural language processing. Among all the techniques of artificial intelligence, neural network based deep learning has shown very good learning capabilities and good performance in many areas. The design and development of such networks is an arduous task that requires advanced knowledge in modern parallel architectures in order to make the best use of the computing power of such machines. This issue is more important with the massive development of AI in both academic and industrial world. The objective of the thesis is to define a development environment for deep learning neural networks which is capable of taking the current developments in neural network architectures into account. This environment has to include a domain-specific language allowing the user to describe the architecture of his system without worrying about how it will be deployed. This high-level language based on the principles of implicit parallelism provides the user with patterns or skeletons allowing to describe the neural network elements and their assembly in a simple way while giving indications that allows to define an efficient parallelization strategy. The environment must be able to derive from the high-level description, an efficient program and deployment of the neural network on the target architecture. In order to achieve this, it's necessary to detect and describe the relevant programming skeletons for neural network designers and to find efficient parallelization strategies for each of these skeletons. In addition, deep learning network analysis tools to detect optimizations in terms of deployment and communications will also be essential to obtain efficient systems. The results aim to be integrated in Huawei's MindSpore environment and contribute to Huawei's AI products and solutions, in order to explore the full potential power of its Compute Architecture for Neural Networks (CANN) as well as the hardware equipped with Huawei's Ascend chips.
Over the last few years, there has been tremendous growth in digitizing collections of cultural heritage documents. Recently, an important need has emerged which consists in designing a computer-aided characterization and categorization tool, able to index or group historical digitized book pages according to several criteria, mainly the layout structure and/or typographic/graphical characteristics of the historical document image content. Thus, the work conducted in this thesis presents an automatic approach for characterization and categorization of historical book pages. The proposed approach is applicable to a large variety of ancient books. In addition, it does not assume a priori knowledge regarding document image layout and content. It is based on the use of texture and graph algorithms to provide a rich and holistic description of the layout and content of the analyzed book pages to characterize and categorize historical book pages. The categorization is based on the characterization of the digitized page content by texture, shape, geometric and topological descriptors. This characterization is represented by a structural signature. More precisely, the signature-based characterization approach consists of two main stages. The first stage is extracting homogeneous regions. Then, the second one is proposing a graph-based page signature which is based on the extracted homogeneous regions, reflecting its layout and content. Afterwards, by comparing the different obtained graph-based signatures using a graph-matching paradigm, the similarities of digitized historical book page layout and/or content can be deduced. Subsequently, book pages with similar layout and/or content can be categorized and grouped, and a table of contents/summary of the analyzed digitized historical book can be provided automatically. As a consequence, numerous signature-based applications (e.g. information retrieval in digital libraries according to several criteria, page categorization) can be implemented for managing effectively a corpus or collections of books. To illustrate the effectiveness of the proposed page signature, a detailed experimental evaluation has been conducted in this work for assessing two possible categorization applications, unsupervised page classification and page stream segmentation. In addition, the different steps of the proposed approach have been evaluated on a large variety of historical document images.
The research work presented in this thesis is a part of the general framework of work on digital humanities that seeks, among other things, to contribute to the improvement of human-machine interactions. The objective of the study is twofold. Firstly, it is about studying a corpus of court decisions contained in the database of the International Transport Law Institute (IDIT) in order to determine the linguistic constraints of the judicial kind. Secondly, it is a matter of proposing interpretative paths that can help users to access to the legal information they are looking for. The issue of the interpretation assistance is seen through the study of the modalities and modal scenarios. The bias of this research is to consider multidisciplinarity as a theoretical and methodological asset that contributes to a better understanding of the issue of the interpretation assistance. As a result, several approaches (semantics of modalities, textual semantics, rhetorical argumentation, textometry) are called and articulated to work together towards the objectives set. The analysis of the corpus was conducted at two levels and in two approaches. In the first part, the empirical analysis proposed is quantitative and contrasting. It is conducted at the micro and mesotexual level as it focuses on the study of the lexicon. Based on the TXM tool, this first investigation allowed a comprehensive linguistic characterization of the corpus and an initial overview of its modal profile. It also highlighted modal expressions, concessional constructions, patterns, etc. which focus on key moments in the argumentative process and can therefore be used in the context of the interpretation assistance. In the second part, the empirical study focuses on modal analyses realized on complete texts. It is therefore treated in a qualitative and macrotextual approach. This analysis leads to the formulation of a carefully described scenario model. It can be divided into several levels, depending on how it has been constructed (foreground modalities, background modalities) and whether it characterizes a full text or a specific area of this text. Furthermore, the proposed schematic presentation for the scenarios highlighted the role that each modal zone would play in providing the interpretation assistance.
The first part of this thesis presents a computer assisted transcription of speech method. Every time the user corrects a word in the automatic transcription, this correction is immediately taken into account to re-evaluate the transcription of the words following it. The latter is obtained from a reordering of the confusion networks hypothesis generated by the ASR. In order to decrease the proper nouns error rate, an acoustic-based phonetic transcription method is proposed in this manuscript. The use of SMT [Laurent 2009] associated with the proposed method allows a significant reduce in term of word error rate (WER) and in term of proper nouns error rate (PNER).
The goal of computational linguistics is to provide a formal account linguistical knowledge, and to produce algorithmic tools for natural language processing. Often, this is done in a so-called generative framework, where grammars describe sets of valid sentences by iteratively applying some set of rewrite rules. Another approach, based on model theory, describes instead grammaticality as a set of well-formedness logical constraints, relying on deep links between logic and automata in order to produce efficient parsers. This thesis favors the latter approach. Making use of several existing results in theoretical computer science, we propose a tool for linguistical description that is both expressive and designed to facilitate grammar engineering. It first tackles the abstract structure of sentences, providing a logical language based on lexical properties of words in order to concisely describe the set of grammaticaly valid sentences. It then draws the link between these abstract structures and their representations (both in syntax and semantics), through the use of linearization rules that rely on logic and lambda-calculus. Then in order to validate this proposal, we use it to model various linguistic phenomenas, ending with a specific focus on languages that include free word order phenomenas (that is, sentences which allow the free reordering of some of their words or syntagmas while keeping their meaning), and on their algorithmic complexity.
This thesis deals with lexical unit extraction in contemporary Chinese from a corpus of specialized texts. It addresses the task of Chinese lexicon extraction using techniques based on linguistic characteristics of the Chinese language. The thesis also discusses how to evaluate the extraction of a lexicon in an industrial environment. The first part of the thesis describes the context of the study. We focus on describing the linguistic concepts of vocabulary and lexical units, and we also give a description of the construction of lexical units in contemporary Chinese. We then make a inventory of the different techniques used by the scientific community to address the task of extracting a contemporary Chinese lexicon. We conclude this section by describing lexicon extraction practices in industry, and we propose a formalization of the criteria used by terminologists to select the relevant lexical units. The second part of this thesis deals with the description of a method for extracting Chinese contemporary lexicon and its evaluation. We introduce a new numerical unsupervised method based on structural features of the lexical unit in Chinese and syntactic features of Chinese. The method includes an optional module to interact with a user (i. E. Semi-automatic). In the section related to the evaluation, we first evaluate the potential of the method by comparing extraction results to a reference standard and a reference method. We then implement a more pragmatic evaluation of the method by measuring the gains using this method as opposed to manual lexicon extraction by terminologists. The results obtained by our method are better than those produced by the reference method on the reference standard. The pragmatic evaluation shows that the method does not significantly improve the productivity of terminologists but can extract different lexical units than those obtained manually.
Big Data for biomedicine domain deals with a major issue, the analyze of large volume of heterogeneous data (e.g. video, audio, text, image). Ontology, conceptual models of the reality, can play a crucial role in biomedical to automate data processing, querying, and matching heterogeneous data. Initially, ontologies were built manually. In recent years, few semi-automatic methodologies have been proposed. The semi-automatic construction/enrichment of ontologies are mostly induced from texts by using natural language processing (NLP) techniques. NLP methods have to take into account lexical and semantic complexity of biomedical data: (1) lexical refers to complex phrases to take into account, (2) semantic refers to sense and context induction of the terminology. In this thesis, we propose methodologies for enrichment/construction of biomedical ontologies based on two main contributions, in order to tackle the previously mentioned challenges. The first contribution is about the automatic extraction of specialized biomedical terms (lexical complexity) from corpora. New ranking measures for single- and multi- word term extraction methods have been proposed and evaluated. In addition, we present BioTex software that implements the proposed measures. The second contribution concerns the concept extraction and semantic linkage of the extracted terminology (semantic complexity). The experiments conducted on real data highlight the relevance of the contributions.
However, these new features multiply by a factor 8 the amount of data to be processed before transmission to the end user. In addition to this new format, broadcasters and Over-The-Top (OTT) content providers have to encode videos in different formats and at different bitrates due to the wide variety of devices with heterogeneous video format and network capacities used by consumers. The objective of this thesis is thus to investigate lightweight scalable encoding approaches based on the adaptation of the spatio-temporal resolution. The first part of this document proposes two pre-processing tools, respectively using polyphase and wavelet frame-based approaches, to achieve spatial scalability with a slight complexity overhead. A variable frame-rate algorithm is first proposed as pre-processing. This solution has been designed to locally and dynamically detect the lowest frame-rate that does not introduce visible motion artifacts. The proposed variable frame-rate and adaptive spatial resolution algorithms are then combined to offer a lightweight scalable coding of 4K HFR video contents.
The quality of Volunteered Geographic Information (VGI) is currently a topic that question spatial data users as well as authoritative data producers who are willing to exploit the benefits of crowdsourcing. This phenomenon is one the main downsides of crowsdsourcing, and despite the small amount of incidents, it may be a barrier to the use of collaborative spatial data. This thesis follows an approach based on VGI quality -- in particular, the objective of this work is to detect vandalism in spatial collaborative data. First, we formalize a definition of the concept of carto-vandalism. Finally, the experiments explore the ability of learning methods to detect carto-vandalism
This thesis focuses on the integration of lexical and syntactic resources of French in two fundamental tasks of Natural Language Processing [NLP], that are probabilistic part-of-speech tagging and probabilistic parsing. In this paper, we use these resources to give an answer to two problems that we describe briefly below: data sparseness and automatic segmentation of texts. Through more and more sophisticated parsing algorithms, parsing accuracy is becoming higher for many languages including French. However, there are several problems inherent in mathematical formalisms that statistically model the task (grammar, discriminant models,...). Moreover, it is proved that spars ness is partly a lexical problem, because the richer the morphology of a language is, the sparser the lexicons built from a Treebank will be for that language. Our first problem is therefore based on mitigating the negative impact of lexical data sparseness on parsing performance. To this end, we were interested in a method called word clustering that consists in grouping words of corpus and texts into clusters. These clusters reduce the number of unknown words, and therefore the number of rare or unknown syntactic phenomena, related to the lexicon, in texts to be analyzed. Our goal is to propose word clustering methods based on syntactic information from French lexicons, and observe their impact on parsers accuracy. Furthermore, most evaluations about probabilistic tagging and parsing were performed with a perfect segmentation of the text, as identical to the evaluated corpus. But in real cases of application, the segmentation of a text is rarely available and automatic segmentation tools fall short of proposing a high quality segmentation, because of the presence of many multi-word units (compound words, named entities,...). In this paper, we focus on continuous multi-word units, called compound words, that form lexical units which we can associate a part-of-speech tag. We may see the task of searching compound words as text segmentation. Our second issue will therefore focus on automatic segmentation of French texts and its impact on the performance of automatic processes. In order to do this, we focused on an approach of coupling, in a unique probabilistic model, the recognition of compound words and another task. Recognition of compound words is performed within the probabilistic process rather than in a preliminary phase.
Automated planning is a field of artificial intelligence that aims at proposing methods to chose and order sets of actions with the objective of reaching a given goal. One then searches for plans into each component and try to assemble these local plans into a global plan for the original planning problem. The main interest of this approach is that, for some classes of planning problems, the components considered can be planning problems much simpler to solve than the original one. The main difference between this approach and the previous one is that it is not only modular but also distributed.
In this dissertation, we discuss how to use the human gaze data to improve the performance of the weak supervised learning model in image classification. The background of this topic is in the era of rapidly growing information technology. As a consequence, the data to analyze is also growing dramatically. Since the amount of data that can be annotated by the human cannot keep up with the amount of data itself, current well-developed supervised learning approaches may confront bottlenecks in the future. In this context, the use of weak annotations for high-performance learning methods is worthy of study. Specifically, we try to solve the problem from two aspects: One is to propose a more time-saving annotation, human eye-tracking gaze, as an alternative annotation with respect to the traditional time-consuming annotation, e.g. bounding box. The other is to integrate gaze annotation into a weakly supervised learning scheme for image classification. This scheme benefits from the gaze annotation for inferring the regions containing the target object. A useful property of our model is that it only exploits gaze for training, while the test phase is gaze free. This property further reduces the demand of annotations. The two isolated aspects are connected together in our models, which further achieve competitive experimental results.
Sinusoidal Modeling is one of the most widely used parametric methods for speech and audio signal processing. The eaQHM is shown to outperform aQHM in analysis and resynthesis of voiced speech. Based on the eaQHM, a hybrid analysis/synthesis system of speech is presented (eaQHNM), along with a hybrid version of the aHM (aHNM). Moreover, we present motivation for a full-band representation of speech using the eaQHM, that is, representing all parts of speech as high resolution AM-FM sinusoids. Experiments show that adaptation and quasi-harmonicity is sufficient to provide transparent quality in unvoiced speech resynthesis. The full-band eaQHM analysis and synthesis system is presented next, which outperforms state-of-the-art systems, hybrid or full-band, in speech reconstruction, providing transparent quality confirmed by objective and subjective evaluations. Regarding applications, the eaQHM and the aHM are applied on speech modifications (time and pitch scaling). The resulting modifications are of high quality, and follow very simple rules, compared to other state-of-the-art modification systems. Results show that harmonicity is preferred over quasi-harmonicity in speech modifications due to the embedded simplicity of representation. Moreover, the full-band eaQHM is applied on the problem of modeling audio signals, and specifically of musical instrument sounds. The eaQHM is evaluated and compared to state-of-the-art systems, and is shown to outperform them in terms of resynthesis quality, successfully representing the attack, transient, and stationary part of a musical instrument sound. Finally, another application is suggested, namely the analysis and classification of emotional speech. The eaQHM is applied on the analysis of emotional speech, providing its instantaneous parameters as features that can be used in recognition and Vector-Quantization-based classification of the emotional content of speech. Although the sinusoidal models are not commonly used in such tasks, results are promising.
Thanks to the democratization of new communication technologies, there is a growing quantity of textual resources, making Automatic Natural Language Processing (NLP) a discipline of crucial importance both scientifically and industrially. Easily available, these data offer unprecedented opportunities and, from opinion analysis to information research and semantic text analysis, there are many applications. However, this textual data cannot be easily exploited in its raw state and, in order to carry out such tasks, it seems essential to have resources describing semantic knowledge, particularly in the form of lexico-semantic networks such as that of the JeuxDeMots project. However, the constitution and maintenance of such resources remain difficult operations, due to their large size but also because of problems of polysemy and semantic identification. Moreover, their use can be tricky because a significant part of the necessary information is not directly accessible in the resource but must be inferred from the data of the lexico-semantic network. Our work seeks to demonstrate that lexico-semantic networks are, by their connexionic nature, much more than a collection of raw facts and that more complex structures such as interpretation paths contain more information and allow multiple inference operations to be performed. In particular, we will show how to use a knowledge base to provide explanations to high-level facts. These explanations allow at least to validate and memorize new information. In doing so, we can assess the coverage and relevance of the database data and consolidate it. Similarly, the search for paths is useful for classification and disambiguation problems, as they are justifications for the calculated results. In the context of the recognition of named entities, they also make it possible to type entities and disambiguate them (is the occurrence of the term Paris a reference to the city, and which one, or to a starlet?) by highlighting the density of connections between ambiguous entities, their context and their possible type. Finally, we propose to turn the large size of the JeuxDeMots network to our advantage to enrich the database with new facts from a large number of comparable examples and by an abduction process on the types of semantic relationships that can connect two given terms. Each inference is accompanied by explanations that can be validated or invalidated, thus providing a learning process.
This thesis has a bi-directional approach to the domain of spoken dialog systems. On the one hand, parts of the work emphasize on increasing the reliability and the intuitiveness of such interfaces. On the other hand, it also focuses on the design and development side, providing a platform made of independent specialized modules and tools to support the implementation and the test of prototypical spoken dialog systems technologies. The topics covered by this thesis are centered around an open-source framework for supporting the design and implementation of natural-language spoken dialog systems. According to the two directions taken in this work, the natural language understanding subsystem of the platform has been thought to be intuitive to use, allowing a natural language interaction. Finally, on the dialog management side, this thesis argue in favor of the deterministic modeling of dialogs. However, such an approach requires intense human labor, is prone to error and does not ease the maintenance, the update or the modification of the models. A new paradigm, the linked-form filling language, offers to facilitate the design and the maintenance tasks by shifting the modeling to an application specification formalism.
The purpose of this thesis is to prepare a French-Romanian / Romanian-French dictionary of veterinary medicine. In order to achieve this purpose, a first step was to study the methods used over time by the masters of the language. Our work methods have been offered by our more experienced colleagues and the ISO terminology norms. Thus, our research consisted in an essay to modeling the terminology work, on the way to find the best methods. In a second step, we established a research corpus, based on the methods of corpus linguistics. We have assembled a corpus of 2193 veterinary theses from 11 years (1998-2008) of the four French veterinary schools - VeThèses, on the theme of domestic vertebrates, theses available in digitized format. The term extraction step includes an essay to systematize the terms. The results of this work on the corpus formed a nomenclature and the VeTerm database.
Four methods are planned to identify the underground pipelines but they have limits and depend on many factors. Our investigation aims to solve the problem of reliable detection of underground networks by aggregation of the existing methods and reasoning at different abstraction levels.
At a time when computer science has invaded all aspects of our daily life, it is natural to see the computer field participating in human and social sciences work, and more particularly in linguistics where the need to develop computer software is becoming more and more pressing with the growing volume of analyzed corpora. Hence our thesis which consists in elaborating a program EPL that studies the white Lebanese Arabic speech. Starting from a corpus elaborated from two TV programs recorded then transcribed in Arabic letters, the program EPL, developed with Access software, allowed us to extract words and collocations, and to carry out a linguistic analysis on the lexical, phonetic, syntactic and collocational levels. The EPL's functioning as well as its development code are described in the computer part. Important annexes conclude the thesis and gather the result of the work of a team of researchers coming from different specialties.
Computer vision is the computational science aiming at reproducing and improving the ability of human vision to understand its environment. In this thesis, we focus on two fields of computer vision, namely image segmentation and visual odometry and we show the positive impact that interactive Web applications provide on each. The first part of this thesis focuses on image annotation and segmentation. Many interactions have been explored in the literature to help segmentation algorithms. The most common consist in designating contours, bounding boxes around objects, or interior and exterior scribbles. When crowdsourcing, annotation tasks are delegated to a non-expert public, sometimes on cheaper devices such as tablets. In this context, we conducted a user study showing the advantages of the outlining interaction over scribbles and bounding boxes. Another challenge of crowdsourcing is the distribution medium. While evaluating an interaction in a small user study does not require complex setup, distributing an annotation campaign to thousands of potential users might differ. A highlights tour of its functionalities and architecture is provided, as well as a guide on how to deploy it to crowdsourcing services such as Amazon Mechanical Turk. The application is completely opensource and available online. In the second part of this thesis we present our open-source direct visual odometry library. In that endeavor, we provide an evaluation of other open-source RGB-D camera tracking algorithms and show that our approach performs as well as the currently available alternatives. The visual odometry problem relies on geometry tools and optimization techniques traditionally requiring much processing power to perform at realtime framerates. Since we aspire to run those algorithms directly in the browser, we review past and present technologies enabling high performance computations on the Web. In particular, we detail how to target a new standard called WebAssembly from the C++ and Rust programming languages. Our library has been started from scratch in the Rust programming language, which then allowed us to easily port it to WebAssembly. Thanks to this property, we are able to showcase a visual odometry Web application with multiple types of interactions available. A timeline enables one-dimensional navigation along the video sequence. Pairs of image points can be picked on two 2D thumbnails of the image sequence to realign cameras and correct drifts. Colors are also used to identify parts of the 3D point cloud, selectable to reinitialize camera positions. Combining those interactions enables improvements on the tracking and 3D point reconstruction results.
Our working hypothesis is: FSL is a language and, therefore, machine translation is relevant. We describe the language specifications for automatic processing, based on scientific knowledge and proposals of our native FSL speaker informants. We also expose our methodology, and do present the advancement of our work in the formalization of linguistic data based on the specificities of FSL which certain (verbs scheme, adjective and adverb modification, organization of nouns, agreement patterns) require further analysis. We do present the application framework in which we worked on: the machine translation system and virtual characters animation system of France Telecom R&amp;D.After a short avatar technology presentation, we explain our control modalities of the gesture synthesis engine through the exchange format that we developed. Finally, we conclude with an evaluation, researches and developments perspectives that could follow this thesis. Our approach has produced its first results since we have achieved our goal of running the full translation chain: from the input of a sentence in French to the realization of the corresponding sentence in FSL with a synthetic character.
There is much CALL software on the Internet, designed by using authoring systems such as (Course builder, Hot Potatoes or Netquizz). Such activities poses several problems as the rigidity of software (the data used are predetermined and can not be altered or enhanced) and the not adaptability of course to the language skills of learners (the path is independent of its response to each step, they can not evaluate). The use of the NLP for the design of software CALL, is currently one of the method that can solve the problems. But after more than two decades since the early work, the advanced research in the topic of CALL based on the NLP remains weak, due to two main factors: the lack of NLP from language didactic psychoanalysts or computer scientists, and the cost of resources and products of natural language processing. The CALL work based on NLP for the Arabic language is practically not existed, in despite of a rich literature on the automatic processing of Arabic. In addition to factors mentioned above, the deficiency of the Arabic language in this area is due to that Arabic is a language difficult to treat automatically. Under this situation, and willing to enrich the possibilities for creating educational activities for Arabic we have: As a first step, developed a labelled dictionary of Arabic (as complete as possible), a derivative, a Conjugator and a morphological analyzer for Arabic. In a second step, we used these tools to create a number of educational applications for Arabic learning as a foreign language for French learners by using our system SALA.
This thesis proposes linguistic and phonetic investigations of French-Algerian Arabic code-switching. A corpus of 7h30 of speech (5h of spontaneous speech and 2h30 of read speech) has been designed with 20 males and females French-Algerian Arabic speakers. This thesis also proposes code-switching speech data processing methods such as language segmentation, code-switching utterance segmentation and transcription of French and Algerian Arabic dialect. Automatic speech alignment methods of the code-switching data are proposed with combined alignment of two monolingual alignments. We conducted experiments based on language automatic identification and automatic alignment with variations that deals with the question of the influence of a phonological system of a language A on code-switching speech in phonetic productions of French and Algerian Arabic. We performed also a variation study on vowel variation, in both French and Arabic productions. Finally, we dealt with three types of consonant variation in the code-switching speech: gemination, emphatization and voicing consonant as variants in production. The results shown that the code-switching French-Algerian Arabic is characterized by very short language switches witch constitute a big challenge to the code-switching languages identification. The code-switching has an impact of the phonetic variation in both vowel and consonants. The code-switching allows the speakers to produce less vowel and consonant variation than the monolingual speech.
Automatically generated text has been used in numerous occasions with distinct intentions. It can simply go from generated comments in an online discussion to a much more mischievous task, such as manipulating bibliography information. So, this thesis first introduces different methods of generating free texts that resemble a certain topic and how those texts can be used. Therefore, we try to tackle with multiple research questions. The first question is how and what is the best method to detect a fully generated document. Then, we take it one step further to address the possibility of detecting a couple of sentences or a small paragraph of automatically generated text by proposing a new method to calculate sentences similarity using their grammatical structure. The last question is how to detect an automatically generated document without any samples, this is used to address the case of a new generator or a generator that it is impossible to collect samples from. This thesis also deals with the industrial aspect of development. A simple overview of a publishing workflow from a high-profile publisher is presented. From there, an analysis is carried out to be able to best incorporate our method of detection into the production workflow. In conclusion, this thesis has shed light on multiple important research questions about the possibility of detecting automatically generated texts in different setting. Besides the researching aspect, important engineering work in a real life industrial environment is also carried out to demonstrate that it is important to have real application along with hypothetical research.
Enterprise systems' interoperability has been identified as a significant issue faced by enterprises, which need to collaborate with other companies and participate within Networked Enterprises. To achieve a higher quality of interoperability and ensure an effective collaboration, a certain number of Interoperability Requirements (IRs) should be satisfied. Thus, interoperability should be verified and continuously improved. A manner for verifying the enterprise systems' interoperability is through the Interoperability Assessment (INAS). It also has been identified that the IEC interdependencies are not explicitly defined. Indeed, their interdependencies should be considered as they can support the identification of impacts on the overall system. Further, the majority of the INAS approaches are manual-conducted, which is a laborious and time-consuming process and in many times depends on the “subjective” knowledge of experts, which can be expensive in time and money when hiring external consultants. A Design Science Research (DSR) methodology has been adopted for conducting the work. To formally conceptualise the knowledge about the INAS (subsuming the set of IRs, interoperability problems and solutions), we proposed the Ontology of Interoperability Assessment (OIA). A Model-Based System Engineering approach has been applied for defining and organising the concepts of the proposed ontology. A prototype of the KBIAS using the OIA as its knowledge model has been developed in a Java platform. The contribution proposed in this research has been evaluated through a case study based on a real Networked Enterprise
Subjective estimation and perception, complexity of the environment under study, interaction amongst subsystems, lack of precise data, missing data, limited information processing capacity and ambiguity in natural languages are major forms of uncertainty facing Decision Makers in the process of delivering strategic decisions in economic intelligent systems. This study employs soft computing paradigm to capture and analyze uncertainty based on information risk factors via our proposed knowledge reconciliation model based on ontology and the FuzzyWatch model. We extended this operation with fuzzy that is – what ontology captures is interpreted by fuzzy techniques (FuzzOntology). FuzzyWatch assists in reducing missing data problems. Future decisional process will contend with lesser information retrieval risks in Economic Intelligence process using this model.
These conversations may be found on online channels such as forums, mailing lists or chat rooms. We want to use dialogue acts for the analysis of online written conversations. Well-defined methods and models allowing for the fine-grained analysis of these conversations would represent a solid framework to support different user-assistance and dialogue analysis systems. However, current conversations analysis techniques were not developed with written online conversations in mind. It is necessary to adapt existing resources for these conversations. Our goal is to build a dialogue act model for problem-solving online written conversations, and to offer tools for the automatic recognition of these acts.
In this thesis, we proposed a multilingual generic approach for the automatic information extraction. Particularly, events extraction of price variation and temporal information extraction linked to temporal referential. Our approach is based on the constitution of several semantic maps by textual analysis in order to formalize the linguistic traces expressed by categories. We created a database for an expert system to identify and annotate information (categories and their characteristics) based on the contextual rule groups. Two algorithms AnnotEC and AnnotEV have been applied in the SemanTAS platform to validate our assumptions. We have obtained a satisfactory result; Accuracy and recall are around 80%. We presented extracted knowledge by a summary file.
Automatic morphological analysis of Slovak language is the first level of an automatical analyser for Slovak's scientifical and technical texts. Such a system could be used for different applications: automatic text indexation, automatic research of terminology or translation systems. A rule-based description of language's regularities as well as the use of all the formal level elements of words allow to reduce considerably the volume of dictionaries. Notably in case of inflectionally rich languages such as Slovak. The results obtained by our morphological analyser justify such an approach and confirm the high reliability of morphological analysis based on form-recognition for all lexical categories
In recent years, hacking has become an industry unto itself, increasing the number and diversity of cyber attacks. Threats on computer networks range from malware to denial of service attacks, phishing and social engineering. An effective cyber security plan can no longer rely solely on antiviruses and firewalls to counter these threats: it must include several layers of defence. Network-based Intrusion Detection Systems (IDSs) are a complementary means of enhancing security, with the ability to monitor packets from OSI layer 2 (Data link) to layer 7 (Application). Intrusion detection techniques are traditionally divided into two categories: signatured-based (or misuse) detection and anomaly detection. Most IDSs in use today rely on signature-based detection; however, they can only detect known attacks. IDSs using anomaly detection are able to detect unknown attacks, but are unfortunately less accurate, which generates a large number of false alarms. In this context, the creation of precise anomaly-based IDS is of great value in order to be able to identify attacks that are still unknown. In this thesis, machine learning models are studied to create IDSs that can be deployed in real computer networks. Firstly, a three-step optimization method is proposed to improve the quality of detection: 1/ data augmentation to rebalance the dataset, 2/ parameters optimization to improve the model performance and 3/ ensemble learning to combine the results of the best models. Flows detected as attacks can be analyzed to generate signatures to feed signature-based IDS databases. However, this method has the disadvantage of requiring labelled datasets, which are rarely available in real-life situations. Transfer learning is therefore studied in order to train machine learning models on large labeled datasets, then finetune them on benign traffic of the network to be monitored. This method also has flaws since the models learn from already known attacks, and therefore do not actually perform anomaly detection. Thus, a new solution based on unsupervised learning is proposed. It uses network protocol header analysis to model normal traffic behavior. Anomalies detected are then aggregated into attacks or ignored when isolated. Finally, the detection of network congestion is studied. The bandwidth utilization between different links is predicted in order to correct issues before they occur.
A musical score is a complex semiotic object that excels at conveying musical information in a human readable format. Nowadays, a lot of music is available exclusively as recorded performances, so systems which can automatically transform those performances into musical scores would be extremely beneficial for performers and musicologists. This task, called automatic music transcription (AMT), comprises a large number of subtasks which generically transform the performance input into higher level representations, such as unquantized and quantized MIDI files. We believe that a clear model of the information contained in a musical score would help the development and the evaluation of AMT systems. In particular we advocate for a clear separation between the music which the score encodes and the set of notation symbols which is employed to represent it.
The Semantic Web extends the Web by publishing structured and interlinked data using RDF.An RDF data set is a graph where resources are nodes labelled in natural languages. One of the key challenges of linked data is to be able to discover links across RDF data sets. Given two data sets, equivalent resources should be identified and linked by owl:sameAs links. This problem is particularly difficult when resources are described in different natural languages. For this purpose, we introduce a general framework in which each RDF resource is represented as a virtual document containing text information of neighboring nodes. This can be achieved by using machine translation or multilingual lexical resources. Similarity between elements of this space is taken for similarity between RDF resources. In particular, two strategies are explored: applying machine translation or using references to multilingual resources. Overall, evaluation shows the effectiveness of cross-lingual string-based approaches for linking RDF resources expressed in different languages. The methods have been evaluated on resources in English, Chinese, French and German. The best performance (over 0.90 F-measure) was obtained by the machine translation approach. The best experimental results involving just a pair of languages demonstrated the usefulness of such techniques for interlinking RDF resources cross-lingually.
This thesis deals with the dynamic adaptation of context-aware applications using information related to the social environment of users to enrich the service rendered by the applications. To achieve this goal our contribution mobilizes multidimensional modeling of the different levels of social contexts, especially the weight of the relationship between the actors. Particularly, we synthesize not only social contexts related to familiarity but also social contexts reasoned from the similarity of static and dynamic communities. Two models based on respectively graphs and ontologies are proposed in order to satisfy the heterogeneity of the social networks in real life. We use the actual data gathered on online social networking services for conducting experiments and the results are analyzed by checking the effectiveness of the models. In parallel we consider the point of view of the application, and we present two algorithms using social contexts to improve the strategy of transmission of data in the opportunistic network, particularly countermeasure against selfish nodes. The simulations of real scenarios confirm the advantages of introducing social contexts in terms of success rate and delay of transmission. We carry out a comparison with other popular transmission algorithms in the literature
It is broadly accepted that 70 % of the total life cycle cost is committed during the specification phase. Thus, we propose to methodologically integrate data science techniques into a collaborative requirement mining framework, which enables decision-makers to gain insight and discover opportunities in a massive set of requirements. Initially, classification models extract requirements from prescriptive documents. Requirements are subsequently analysed with natural language processing techniques so as to identify quality defects. We conclude that the theoretical and empirical validation of our proposition corroborates the assumption that data science is an effective way to gain insight from hundreds or thousands of requirements.
Cardiovascular disease (CVD) and cancer are the leading cause of death and morbidity in men and women in France, and their annual cost is high. The prevention of these chronic diseases is, with their early detection and their rapid and effective management, a possible way to reduce this cost. Thus, the National Health Nutrition Program was set up in France, helping the French to have a better diet, to help reduce the incidence of these diseases. The objective of this thesis is the construction of a system of personalized suggestions based on the profile of the individual and his cardiovascular risk. This approach requires the establishment of an interdisciplinary approach involving researchers in the fields of computer science, epidemiology and nutrition. The importance of this collaboration is justified by the need to produce suggestions supported by proven research in these areas. The first contribution of this thesis is the integration of semantic web technologies into a new approach to cardiovascular risk assessment that takes into account the interactions between these factors. The creation of the visualization tool MCVGraphViz allowed to implement this strategy. The second contribution consists in proposing a solution to exploit the knowledge present in the health plans and the recommendations concerning the prevention of cardiovascular diseases in France. Thus, we opted for a modular approach integrated into the tool MCVGraphViz that allows to produce recommendations (diet, physical activity, etc.) based on the assessed cardiovascular risk and the profile of the individual (sensory preferences, allergic constraints, physical capacity, etc.). The third contribution concerns the nutritional qualification of cooking recipes for a better follow-up: the approach is based on techniques of automatic language processing and ontological reasoning to qualify a nutritional point of view of cooking recipes. Many perspectives are exposed. Most of them aim to improve referral systems and the expressivity of the cardiovascular disease knowledge base.
Our research is quantitative and qualitative, based on textometrics tools and discourse analysis. Our literature review surveyed linguistic studies of advertising, through different schools of thought: descriptive linguistics, discourse analysis, semiology, rhetorics/stylistics, starting with Leech's founding work (The Language of Advertising, 1966), which led to several highlights: 1. advertising is indeed a specialty discourse, or a language for special purpose (LSP) 2. advertising shares some common features with both poetry and the language we speak everyday 3. intertextuality and active participation of the targeted reader are two key factors of advertising communication 4. the language of advertising is emotional, ambiguous, indirect and implicit 5. in a simple and playful way, advertising depicts a perfect, happy world 6. simplicity and conventionalization of the advertising discourse, with lexical constants. To find answers to some of our questions on the advertising discourse, language, communication and translation, we selected a quantitative approach, i.e., the lexicometric methodology (statistical linguistics, more recently called textometry) developed by the Saint-Cloud (France) group, including André Salem (founder and developer of Lexico,which we used to analyze our corpus). Our lexicometric analysis is complemented by a classic factor analysis (FCA)and Viprey's geodesical projection method (with the Astadiag software). With our parallel corpus of roughly 800,000 tokens in English and one million in French (170 advertising brochures downloaded from 20 brands/10carmakers'Canadian websites, for a total of 229 vehicles), we found distinct lexical profiles for each of the seven carmarket segments: AB, CD, EF, MPV, PK (pickup trucks), SP (sports cars) and SUV. We also showed that overall,advertisements are translated rather literally.
Their behaviors must be adapted for each use case by experts. Enabling the general public to teach new behaviors to robots may lead to better adaptation at lesser cost. In open scenarios, a home social robot should learn about its environment. The purpose of such a robot is not restricted to learning new behaviors or about the environment: it should provide entertainment or utility, and therefore support rich scenarios. We demonstrate the teaching of behaviors in these unique conditions: the teaching is achieved by the spoken language on Pepper robots deployed in homes, with no extra device and using its standard system, in a rich and open scenario. Using automatic speech transcription and natural language processing, our system recognizes unpredicted teachings of new behaviors, and a explicit requests to perform them. The new behaviors may invoke existing behaviors parametrized with objects learned in other contexts, and may be defined as parametric. Through experiments of growing complexity, we show conflicts between behaviors in rich scenarios, and propose a solution based on symbolic task planning and priorization rules to resolve them.
The development of Natural Language Processing (NLP) systems needs to determine the quality of their results. Whether aiming to compare several systems to each other or to identify both the strong and weak points of an isolated system, evaluation implies defining precisely and for each particular context a methodology, a protocol, language ressources (data needed for both system training and testing) and even evaluation measures and metrics. It is following these conditions that system improvement is possible so as to obtain more reliable and easy-to-exploit results. The contribution of evaluation to NLP is important due to the creation of new language resources, the homogenisation of formats for those data used or the promotion of a technology or a system. We have tried to reduce and delimit those manual interventions. To do so, we have supported our work by either conducting or participating in evaluation campaigns where systems are compared to each other or where isolated systems are evaluated. The management of the evaluation procedure has been formalised in this work and its diﬀerent phases have been listed so as to define a common evaluation framework, understandable by all. The main point of those evaluation phases regards quality measurement through the usage of metrics. Three consecutive studies have been carried out on human measures, automatic measures and the automation of quality computation, and the meta-evaluation of the mesures so as to evaluate their reliability. In that context, the study of the similarities between the technologies and between their evaluations has allowed us to highlight their common features and class them. This has helped us to show that a small set of measures allows to cover a wide range of applications for diﬀerent technologies. Our final goal has been to define a generic evaluation architecture, which is adaptable to diﬀerent NLP technologies, and sustainable, namely allowing to reuse language resources, measures or methods over time. Our proposal has been built on the conclusions drawn fromprevious steps, with the objective of integrating the evaluation phases to our architecture and incorporating the evaluation measures, all of which bearing in mind the place of language resource usage. The definition of this architecture has been done with the aim of fully automating the evaluation management work, regardless of whether this concerns an evaluation campaign or the evaluation of an isolated system.
From the point of view of Natural Language Processing (NLP), the extraction of events from texts is the most complex form of Information Extraction (IE) techniques, which more generally encompasses the extraction of named entities and relationships that bind them in the texts. The event extraction task can be represented as a complex combination of relations linked to a set of empirical observations from texts. As a result, adaptation to a new domain is an additional challenge. This thesis presents several strategies for improving the performance of an Event Extraction (EE) system using neural-based approaches exploiting morphological, syntactic, and semantic properties of word embeddings. These have the advantage of not requiring a priori modeling domain knowledge and automatically generate a much larger set of features to learn a model. More specifically, we proposed different deep learning models for two sub-tasks related to EE: event detection and argument detection and classification. Event Detection (ED) is considered an important subtask of event extraction since the detection of arguments is very directly dependent on its outcome. As a preliminary to the introduction of our proposed models, we begin by presenting in detail a state-of-the-art model which constitutes the baseline. In-depth experiments are conducted on the use of different types of word embeddings and the influence of the different hyperparameters of the model using the ACE 2005 evaluation framework, a standard evaluation for this task. We then propose two new models to improve an event detection system. One allows increasing the context taken into account when predicting an event instance by using a sentential context, while the other exploits the internal structure of words by taking advantage of seemingly less obvious but essentially important morphological knowledge. We also reconsider the detection of arguments as a high-order relation extraction and we analyze the dependence of arguments on the ED task.
Our work concerns systems that help users during museum visits and access to cultural heritage. Our goal is to design recommender systems, implemented in mobile devices to improve the experience of the visitor, by recommending him the most relevant items and helping him to personalize the tour he makes. We consider two mainly domains of application: museum visits and tourism. We propose a context-aware hybrid recommender system which uses three different methods: demographic, semantic and collaborative. Every method is adapted to a specific step of the museum tour. First, the demographic approach is used to solve the problem of the cold start. The semantic approach is then activated to recommend to the user artworks that are semantically related to those that the user appreciated. Finally, the collaborative approach is used to recommend to the user artworks that users with similar preferences have appreciated. We used a contextual post filtering to generate personalized museum routes depending on artworks which were recommended and contextual information of the user namely: the physical environment, the location as well as the duration of the visit. In the tourism field, the items to be recommended can be of various types (monuments, parks, museums, etc.). Because of the heterogeneous nature of these points of interest, we proposed a composite recommender system. Every recommendation is a list of points of interest that are organized in a package, where each package may constitute a tour for the user. The objective is to recommend the Top-k packages among those who satisfy the constraints of the user (time, cost, etc.). The experimental evaluation of the system we proposed using a real world data set crawled from Tripadvisor demonstrates its quality and its ability to improve both the relevance and the diversity of recommendations.
Advances in information and communication technologies has enabled the development of collaborative work in almost all sectors of human activity. To ensure the performance of the group and minimize the risk of errors, it is crucial that the team members share a common understanding of the situation in which they are involved. Progress in the study of collective cognition, the heart of collaborative work, has a clear potential that must be translated into tangible applications to optimize the management and execution of collective tasks. Real-time evaluation of the cognition of individuals and teams allows to envisage adaptive tools and systems to improve efficiency, performance and agility. In light of these challenges, our objective, commissioned by the DGA, is to find appropriate measures that would enable an assessment of the dynamics of the sharing of situational awareness, in the very constraining context of command and control room operations, which require the lowest possible level of instrumentation of operators. Our contribution to the field has been dual. We have proposed the concept of situation awareness synchrony to support the theoretical development of the study of the dynamics of situation awareness sharing. This doctoral work is presented as a demonstration of the interest and applicability of shared cognition evaluation systems in realistic collaborative work environments, and is supported by proposals concerning the future of research on C2.
As part of the fight against global warming, several countries around the world, including Canada and some European countries, including France, have established measures to reduce greenhouse gas emissions. One of the major areas addressed by the states concerns the transport sector and more particularly the development of public transport to reduce the use of private cars. To this end, the local authorities concerned aim to establish more accessible, clean and sustainable urban transport systems. In order to improve transport operators'knowledge of user travel in urban areas, we are taking advantage of the development of data science (e.g., data collection, development of machine learning methods). This thesis thus focuses on three main parts: (i) long-term forecasting of passenger demand using event databases, (ii) short-term forecasting of passenger demand and (iii) visualization of passenger demand on public transport. The research is mainly based on the use of ticketing data provided by transport operators and was carried out on three real case study, the metro and bus network of the city of Rennes, the rail and tramway network of "La Défense" business district in Paris, France, and the metro network of Montreal, Quebec in Canada
Selection of relevant acoustic speech features is key to in the design of any system using speech processing. For some 40 years, speech was typically considered as a sequence of quasi-stable portions of signal (vowels) separated by transitions (consonants). Despite a wealth of studies that clearly document the importance of coarticulation, and reveal that articulatory and acoustic targets are not context-independent, the view that each vowel has an acoustic target that can be specified in a context-independent manner remains widespread. This point of view entails strong limitations. It is well known that formant frequencies are acoustic characteristics that bear a clear relationship with speech production, and that can distinguish among vowels. Therefore, vowels are generally described with static articulatory configurations represented by targets in the acoustic space, typically by formant frequencies in F1-F2 and F2-F3 planes. Plosive consonants can be described in terms of places of articulation, represented by locus or locus equations in an acoustic plane. But formant frequencies trajectories in fluent speech rarely display a steady state for each vowel. They vary with speaker, consonantal environment (co-articulation) and speaking rate (relating to continuum between hypo- and hyper-articulation).
User-generated content on social media, such as Twitter, provides in many cases, the latest news before traditional media, which allows having a retrospective summary of events and being updated in a timely fashion whenever a new development occurs. However, social media, while being a valuable source of information, can be also overwhelming given the volume and the velocity of published information. Our work falls within these frameworks and focuses on developing a tweet summarization approaches for the two aforementioned scenarios. Nevertheless, tweet summarization task faces many challenges that stem from, on one hand, the high volume, the velocity and the variety of the published information and, on the other hand, the quality of tweets, which can vary significantly. In the prospective notification, the core task is the relevancy and the novelty detection in real-time. For timeliness, a system may choose to push new updates in real-time or may choose to trade timeliness for higher notification quality. The intuition behind our proposition is that context-aware similarity measure in word2vec is able to consider different words with the same semantic meaning and hence allows offsetting the word mismatch issue when calculating the similarity between a tweet and a topic. Second, we propose to compute the novelty score of the incoming tweet regarding all words of tweets already pushed to the user instead of using the pairwise comparison. The proposed novelty detection method scales better and reduces the execution time, which fits real-time tweet filtering. Third, we propose an adaptive Learning to Filter approach that leverages social signals as well as query-dependent features. To overcome the issue of relevance threshold setting, we use a binary classifier that predicts the relevance of the incoming tweet. In addition, we show the gain that can be achieved by taking advantage of ongoing relevance feedback. Finally, we adopt a real-time push strategy and we show that the proposed approach achieves a promising performance in terms of quality (relevance and novelty) with low cost of latency whereas the state-of-the-art approaches tend to trade latency for higher quality. This thesis also explores a novel approach to generate a retrospective summary that follows a different paradigm than the majority of state-of-the-art methods. We consider the summary generation as an optimization problem that takes into account the topical and the temporal diversity. Tweets are filtered and are incrementally clustered in two cluster types, namely topical clusters based on content similarity and temporal clusters that depends on publication time. Summary generation is formulated as integer linear problem in which unknowns variables are binaries, the objective function is to be maximized and constraints ensure that at most one post per cluster is selected with respect to the defined summary length limit.
During the past decades 3D animation widely spread into our everyday life being for entertainment such as video games or movies, or for communication more generally. Despite its common use, creating animations is still dedicated to skilled animators and not within reach of non-experts. In addition, the creation process traditionally follow a strict pipeline: after the initial storyboarding and character design phases, articulated characters are modeled, rigged and roughly positioned in 3D, at which point the layout of their relative poses can be established. Then, their actions and movements are broken down into keyframes which are interpolated to produce the final animation. Keyframing animations is a hard process during which animators need to spend time and efforts carefully tuning animation curves for each character's degrees of freedom and for each specific action. They also need to consistently sequence these actions over time in order to convey the desired character's intentions and personality. Some methods such as motion capture, aim at easing the creation process by directly transferring real recorded human motions to virtual characters. However, output animations still lack expressiveness and must be fixed by animators. In this thesis, we propose a way of easing the process of creating animation sequences starting from a database of individual animations. We focus on animating virtual characters reproducing a story played with props such as figurines instrumented with sensors. To reach this goal, we propose a new animation pipeline analyzing and transcribing hands motion into a sequence of animations adapted to the user hand space-time trajectories and their motion qualities. We also introduce a new procedural animation model inferring expressiveness fitting the narrator's hand motion qualities in terms of Laban Time and Weight Effort. Finally, we extend our system such that it can process multiple characters at a time, detecting and transferring interactions as well as making characters act with respect to pre-defined behaviors, letting users express their narrative creativity. We conclude with a discussion of future research directions.
In recent years, social network sites exploded in popularity and become an important part of the online activities on the web. This success is related to the various services/functionalities provided by each site (ranging from media sharing, tagging, blogging, and mainly to online social networking) pushing users to subscribe to several sites and consequently to create several social networks for different purposes and contexts (professional, private, etc.). Nevertheless, current tools and sites provide limited functionalities to organize and identify relationship types within and across social networks which is required in several scenarios such as enforcing users' privacy, and enhancing targeted social content sharing, etc. Particularly, none of the existing social network sites provides a way to automatically identify relationship types while considering users' personal information and published data. In this work, we propose a new approach to identify relationship types among users within either a single or several social networks. We provide a user-oriented framework able to consider several features and shared data available in user profiles (e.g., name, age, interests, photos, etc.). This framework is built on a rule-based approach that operates at two levels of granularity: 1) within a single social network to discover social relationships (i.e., colleagues, relatives, friends, etc.) by exploiting mainly photos' features and their embedded metadata, and 2) across different social networks to discover co-referent relationships (same real-world persons) by considering all profiles' attributes weighted by the user profile and social network contents. At each level of granularity, we generate a set of basic and derived rules that are both used to discover relationship types. To generate basic rules, we propose two distinct methodologies. On one hand, social relationship basic rules are generated from a photo dataset constructed using crowdsourcing. On the other hand, using all weighted attributes, co-referent relationship basic rules are generated from the available pairs of profiles having the same unique identifier(s) attribute(s) values. To generate the derived rules, we use a mining technique that takes into account the context of users, namely by identifying frequently used valid basic rules for each user. We present here our prototype, called RelTypeFinder, implemented to validate our approach. It allows to discover appropriately different relationship types, generate synthetic datesets, collect web data and photo, and generate mining rules. We also describe here the sets of experiments conducted on real-world and synthetic datasets. The evaluation results demonstrate the efficiency of the proposed relationship discovery approach.
Large-scale collaborative systems wherein a large number of users collaborate to perform a shared task attract a lot of attention from both academic and industry. We study the trust assessment problem and aim to design a computational trust model for collaborative systems. We focused on three research questions. 1. What is the effect of deploying a trust model and showing trust scores of partners to users? We designed and organized a user-experiment based on trust game, a well-known money-exchange lab-control protocol, wherein we introduced user trust scores. Our comprehensive analysis on user behavior proved that: (i) showing trust score to users encourages collaboration between them significantly at a similar level with showing nick-name, and (ii) users follow the trust score in decision-making. The results suggest that a trust model can be deployed in collaborative systems to assist users. 2. How to calculate trust score between users that experienced a collaboration? We designed a trust model for repeated trust game that computes user trust scores based on their past behavior. We validated our trust model against: (i) simulated data, (ii) human opinion, and (iii) real-world experimental data. We extended our trust model to Wikipedia based on user contributions to the quality of the edited Wikipedia articles. We proposed three machine learning approaches to assess the quality of Wikipedia articles: the first one based on random forest with manually-designed features while the other two ones based on deep learning methods. 3. How to predict trust relation between users that did not interact in the past? Given a network in which the links represent the trust/distrust relations between users, we aim to predict future relations. We proposed an algorithm that takes into account the established time information of the links in the network to predict future user trust/distrust relationships. Our algorithm outperforms state-of-the-art approaches on real-world signed directed social network datasets
With the increasing number of novice users of computer applications, the need for efficient assistance has become critical. To supply the need, we suggest using an Assistant Conversational Agent (ACA), an interface allowing the use of naturallanguage (used spontaneously when a problem arises) and providing a reassuring presence to the users. A preliminary study details the constitution (combining collectÎon and the use of thesauri) of a corpus of requests, which need is justified. This corpus of 11,626 requests is compared with others, and we show that it covers the studied domain of assistance and moreover, con tains requests regarding controlling of the application and chatting with the agent. Ln output, requests are expressed in a formallanguage (DAFI) for which we provide the syntax and the semantics. The analyzer is evaluated by comparing a manual annotation and the requests automatically produced, and we consider the use of sorne supervised machine learning approaches in order to identify conversationaJ acti vities. The methodology followed is validated through the integration of an ACA into an existing Web application for cooperative music prototyping. Finally, we describe the required architecture for the rational agent in charge of defining the reactions based on the formal requests expressed in DAFT and on the model of the assisted application, emphasizing the need for a cognitive model.
This is an attempt to show that any modelisation brings about losses in meaning. This is demonstrated through the realisation of a factual and thorough database on a number of company agreements. The first part presents the notions of information sciences and of modelisation. The computer tools available for the realisation of documentary applications are then analysed (relation and object-oriented database management systems, markup languages, hypertext, computational linguistics). This lead of a presentation of up-to-date documentary applications. The second part of the study deals with the differents modelisations attempts. A description of the five models realised follows a presentation of company agreements and a study of their structural problems. The last part describes the objectives and implementation of the ACCORD database realised with PostGres and HTML. Finally, a methodological approach for the factual treatment of corpuses made up of legal and administrative documents is presented. The conclusion underlines the interests of collaboration in implementing such applications and the transformations induced in documentation pratices owing to computer net.
A growing number of newsrooms around the world have established fact-checking headings or rubrics. They are dedicated to assess the veracity of claims, especially by politicians. This practice revisits an older fact-checking practice, born in the United States in the 1920's and based on an exhaustive and systematic checking of magazines'contents before publishing. The 'modern'version of fact-checking embodies both the willingness of online newsrooms to restore verified contents —despite the structural and economic crisis of the press— and their ability to capitalize on digital tools which enhance access to information. Through some thirty semi-structured interviews with French fact-checkers and the study of a sample of 300 articles and chronicles from seven media, this PhD thesis examines the extent to which fact-checking, as a journalistic genre, certainly valorizes a credible method, but also —and indirectly— reveals shortcomings in professional practices. Finally, it discusses how the promotion of more qualitative content, as well as media literacy, could place fact-checking at the heart of editorial strategies —the latter aiming at retrieving trust from the audience.
For several years now, a new phenomenon related to digital data is emerging: data which are increasingly voluminous, varied and rapid, appears and becomes available, they are often referred to as complex data. In this dissertation, we focus on a particular type of data: complex sequence of events, by asking the following question: “how to predict as soon as possible and to influence the appearance of future events within a complex sequence of events?”. We propose DEER: an algorithm for mining episode rules, which has the originality of controlling the horizon of the appearance of future events by imposing a temporal distance within the extracted rules. In a second phase, we address the problem of emergence detection in an events stream. We propose EER: an algorithm for detecting new emergent rules as soon as possible. In order to increase the reliability of new rules, EER relies on the similarity between theses rules and previously extracted rules. At last, we study the impact carried by events on other events within a sequence of events. We propose IE: an algorithm that introduces the concept of “influencer events” and studies the influence on the support, on the confidence and on the distance through three proposed measures. Our work is evaluated and validated through an experimental study carried on a real data set of blogs messages
Human beings naturally organize their space as composed of discrete units. Those units, called "semantic places", are characterized by their spatial extend and their functional unity. Recent works in semantic place recognition seek to endow the robot with similar capabilities. Contrary to classical localization and mapping work, this problem is usually tackled as a supervised learning problem. First, we combine global image characterization, which captures the global organization of the image, and visual words methods which are usually based unsupervised classification of local signatures. Our second but closely related, contribution is to use several images for recognition by using Bayesian methods for temporal integration. Our first model don't use the natural temporal ordering of images. Temporal integration is very simple but has difficulties when the robot moves from one place to another. A second model augment the classical Bayesian filtering approach by using the local order among images. We compare our methods to state-of-the-art algorithms on place recognition and place categorization tasks. We study the influence of system parameters and compare the different global characterization methods on the same dataset. These experiments show that our approach while being simple leads to better results especially on the place categorization task.
Variability in Big Data refers to data whose meaning changes continuously. For instance, data derived from social platforms and from monitoring applications, exhibits great variability. To achieve that goal, data scientists need: (a) measures to compare data in various dimensions such as age for users or topic for network traffic, and (b) efficient algorithms to detect changes in massive data. We propose appropriate measures for comparing user opinions in the form of rating distributions, and efficient algorithms that, given an opinion of interest in the form of a rating histogram, discover agreeing and disargreeing populations. Difference Explanation tackles the question of providing a succinct explanation of differences between two datasets of interest (e.g., buying habits of two sets of customers). We propose scoring functions designed to rank explanations, and algorithms that guarantee explanation conciseness and informativeness. Finally, Difference Evolution tracks change in an input dataset over time and summarizes change at multiple time granularities. We propose a query-based approach that uses similarity measures to compare consecutive clusters over time. Our indexes and algorithms for Difference Evolution are designed to capture different data arrival rates (e.g., low, high) and different types of change (e.g., sudden, incremental). The utility and scalability of all our algorithms relies on hierarchies inherent in data (e.g., time, demographic).We run extensive experiments on real and synthetic datasets to validate the usefulness of the three analytical tasks and the scalability of our algorithms. We show that Difference Exploration guides end-users and data scientists in uncovering the opinion of different user segments in a scalable way. Difference Explanation reveals the need to parsimoniously summarize differences between two datasets and shows that parsimony can be achieved by exploiting hierarchy in data. Finally, our study on Difference Evolution provides strong evidence that a query-based approach is well-suited to tracking change in datasets with varying arrival rates and at multiple time granularities. Similarly, we show that different clustering approaches can be used to capture different types of change.
Humanities challenges computer sciences since 60 years. The 90's marks a break, announcing qualitative analysis and interpretation of interoperable data, which became «knowledge». Since 2010, a disillusionment tarnishes the prospects, Digital Hmanities diversity increases. This method aims at co-creating historical knowledge. Facing the utopian modeling of qualitative knowledge in history, we designed a pragmatic process: the historian analyses quantitative data of a known corpus, this generates new hypothesis and certainties.
Most natural language processing tasks are modeled as prediction problems where one aims at finding the best scoring hypothesis from a very large pool of possible outputs. This work aims at understanding the importance of the search space and the possible use of constraints to reduce it in size and complexity. When information about the possible outputs of a sequence labeling task is available, it may seem appropriate to include this knowledge into the system, so as to facilitate and speed-up learning and inference. A case study on type constraints for CRFs however shows that using such constraints at training time is likely to drastically reduce performance, even when these constraints are both correct and useful at decoding. On the other side, we also consider possible relaxations of the supervision space, as in the case of learning with latent variables, or when only partial supervision is available, which we cast as ambiguous learning. Word order differences between languages pose several combinatorial challenges to machine translation and the constraints on word reorderings have a great impact on the set of potential translations that is explored during search. We study reordering constraints that allow to restrict the factorial space of permutations and explore the impact of the reordering search space design on machine translation performance. However, we show that even though it might be desirable to design better reordering spaces, model and search errors seem yet to be the most important issues.
This study aims at the implementation and evaluation of techniques for extracting semantic relations from a multilingual aligned corpus. Firstly, our observations will focus on the semantic comparison of translational equivalents in multilingual aligned corpus. From these equivalences, we will try to extract "cliques", which ara maximum complete related sub-graphs, where all units are interrelated because of a probable semantic intersection. These cliques have the advantage of giving information on both the synonymy and polysemy of units, and providing a form of semantic disambiguation. Secondly, we attempt to link these cliques with a semantic lexicon (like WordNet) in order to assess the possibility of recovering, for the Arabic units, a semantic relationships already defined for English, French or Spanish units. These relations would automatically build a semantic resource which would be useful for different applications of NLP, such as Question Answering systems, machine translation, alignment systems, Information Retrieval…etc.
The various stakeholders who describe study and implement a complex system require viewpoints that are dedicated to their concerns. However, in the context of Model-Driven Engineering, approaches to define and implement those viewpoints are either too rigid and inappropriate or completely ad hoc. In addition, those various viewpoints are rarely independent from each other. Therefore, we must strive to identify and describe the relationships/correspondences between the viewpoints in order to be able to verify that the parts of the solution given by the various stakeholders form a consistent whole. The work presented in this thesis provides a way to define dedicated languages based on UML for the viewpoints. For this, a method that analyzes the semantics of the textual descriptions of the concepts of the domain we want to map to UML has been implemented to facilitate the definition of UML profiles. To define a viewpoint based on some UML profiles, this thesis provides a method that lets the methodologist make explicit the viewpoint he/she wants. A tool can then generate the tooling that implements this viewpoint in a modeling environment together with the corresponding dedicated language while current practice is based on an implementation essentially manual. To assist the identification of relationships between the viewpoints, this thesis proposes again to analyze the semantics of textual descriptions of concepts of the languages used by the viewpoints. Used in combination with existing syntactic heuristics, the proposed approach provides good results when the terminologies of the languages that are analyzed are far apart. A theoretical framework based on category theory is provided to make explicit formally correspondences. To use this framework, a category for languages based on UML has been proposed. To be able to make explicit the correspondences between the models of those languages as well, the category of OWL ontologies is used. A solution is proposed to characterize correspondences that are more complex than the simple equivalence relationship. This theoretical framework provides a way to define formally complex relationships that can be used to verify the consistency of the architectural description. Once the description of the architecture has been integrated according to the formal correspondences, the issue of consistency is considered. The experiments carried out on a concrete test case to verify consistency on a syntactic perspective give satisfactory practical results. The experiments carried on the same test case to verify consistency on a semantic perspective don't give satisfactory practical results.
Our research examines the role of online offers credibility in the customer judgment process and its determinants. 294 persons were requested to express their perception of online book offers credibility which were composed varying on their source and also on a number of reputation signals. This quasi-experiment aims for measuring impact of these factors on offer credibility but also for investigating influence of this one on consumer interest in offer and consequently on his purchase intention. Statistical analysis bases on a SEM model. The most important research finding shows that credibility mediates effects of reputation signals on consumer interest in offer. This element of interest, in turn, executes mediating effect on the causal relationship between credibility and purchase intention. We found otherwise that credibility is also affected by consumer trust on the source and his interest manifested on the product category. Effect of trust is positive and remarkable while interest in the category effect is negative, which reveals that people with better knowledge level may be more skeptic than others in judgment process. According to these results, our proposal on the importance of credibility element in e-commerce activities is clear: look after the offer credibility is the nexus of merchants selling performance.
Artificial Intelligence is the field of research aiming at mimicking or replacing human cognitive abilities. As such, one of its subfields is focused on the progressive automation of the programming process. In other words, the goal is to transfer cognitive load from the human to the system, whether it be autonomous or guided by the user. In this thesis, we investigate the conditions for making a user-guided system autonomous using another subfield of Artificial Intelligence: Machine Learning. In our work, the requests are in written French, and the associated actions are represented by corresponding instructions in a programming language (here R and bash). The learning is performed using a set of examples composed by the users themselves while interacting. Thus they progressively define the most relevant actions for each request, making the system more autonomous. We collected several example sets for evaluation of the learning methods, analyzing and reducing the inherent collection biases. The proposed protocol is based on incremental bootstrapping of the system, starting from an empty or limited knowledge base. As a result of this choice, the obtained knowledge base reflects the user needs, the downside being that the overall number of examples is limited. To avoid this problem, after assessing a baseline method, we apply a case base reasoning approach to the request to command transfer problem: formal analogical reasoning. We show that this method yields answers with a very high precision, but also a relatively low coverage. We explore the analogical extension of the example base in order to increase the coverage of the provided answers. The running delay of the simple analogical approach is already around 1 second, and is badly influenced by both the automatic extension of the base and the relaxation of the constraints. Finally, the incremental operational assistant based on analogical reasoning was tested in simulated incremental condition in order to assess the learning behavior over time. The system reaches a stable correct answer rate after a dozen examples given in average for each command type. Although the effective performance depends on the total number of accounted commands, this observation opens interesting applicative tracks for the considered task of transferring from a rich source domain (natural language) to a less rich target domain (programming language).
This thesis presents a formal framework for the representation of Signed Languages (SLs), the languages of Deaf communities, in semi-automatic recognition tasks. SLs are complex visio-gestural communication systems; by using corporal gestures, signers achieve the same level of expressivity held by sound-based languages like English or French. However, unlike these, SL morphemes correspond to complex sequences of highly specific body postures, interleaved with postural changes: during signing, signers use several parts of their body simultaneously in order to combinatorially build phonemes. This situation, paired with an extensive use of the three-dimensional space, make them difficult to represent with tools already existent in Natural Language Processing (NLP) of vocal languages. A multi-modal logic was chosen as the basis of the formal language: the Propositional Dynamic Logic (PDL). This logic was originally created to specify and prove properties on computer programs. In particular, PDL uses the modal operators [a] and "a"; to denote necessity and possibility, respectively. For SLs, a particular variant based on the original formalism was developed: the PDL for Sign Language (PDLSL). With the PDLSL, body articulators (like the hands or head) are interpreted as independent agents; each articulator has its own set of valid actions and propositions, and executes them without influence from the others. Together, the use of PDLSL and the proposed specialized data structures could help curb some of the current problems in SL study; notably the heterogeneity of corpora and the lack of automatic annotation aids. On the same vein, this may not only increase the size of the available datasets, but even extend previous results to new corpora; the framework inserts an intermediate representation layer which can serve to model any corpus, regardless of its technical limitations. Afterwards, a formal verification algorithm may be able to find those features in corpora, as long as they are represented as consistent LTSs. Finally, the development of the formal framework led to the creation of a semi-automatic annotator based on the presented theoretical principles. The final product, is an automatically generated sub-lexical annotation, which can be later corrected by human annotators for their use in other areas such as linguistics.
Lexicon is widely acknowledged as a very important component of any Natural Language Processing system, and the use of lexical resources is growing rapidly. Resolving Prepositional Phrase Attachment Ambiguity is known as a bottleneck in automatic parsing, and nowadays most work use corpus-based lexical resources while using existing intuition-based dictionaries is not so common. Assessing how well a lexical resource resolves Prepositional Phrase Attachment Ambiguity is mainly performed on a single corpus; therefore, very little work has been done on adapting lexical resources to the type of corpus. In our study, we build two types of corpus: one is based on an existing dictionary (Lexicon-Grammar), the other is corpus-based (a 200 million word newspaper corpus). We put forward some linguistic characteristics for each of the five corpora which help to understand why the performance of each lexicon varies according to the corpus. Adapting the type of lexicon resource to be used on a given corpus is made more obvious as we assess how the corpus-based lexicon performs compared with a specialised lexicon acquired from each of the five test corpora.
This thesis addresses different aspects around the market microstructure modelling and market making problems, with a special accent from the practitioner's viewpoint. We wish to improve the knowledge of LOB for the research community, propose new modelling ideas and develop concrete applications to the interest of Market Makers. We would like to specifically thank the Automated Market Making team for providing a large high frequency database of very high quality as well as a powerful computational grid, without whom these researches would not have been possible. The first chapter introduces the incentive of this research and resumes the main results of the different works. Chapter 2 fully focuses on the LOB and aims to propose a new model that better reproduces some stylized facts. Through this research, not only do we confirm the influence of historical order flows to the arrival of new ones, but a new model is also provided that captures much better the LOB dynamic, notably the realized volatility in high and low frequency. In chapter 3, the objective is to study Market Making strategies in a more realistic context. High-frequency prediction with deep learning method is studied in chapter 4. Many results of the 1-step and multi-step prediction have found the non-linearity, stationarity and universality of the relationship between microstructural indicators and price change, as well as the limitation of this approach in practice.
The recognition of French Sign Language (LSF) as a natural language in 2005 has created an important need for the development of tools to make information accessible to the deaf public. With this prospect, this thesis aims at linguistic modeling for a system of generation of LSF. We first present the different linguistic approaches aimed at describing the sign language (SL). We then present the models proposed in computer science. In a second step, we propose an approach allowing to take into account the linguistic properties of the SL while respecting the constraints of a formalisation process. By studying the links between semantic functions and their observed forms in LSF Corpora, we have identified several production rules. We finally present the rule functioning as a system capable of modeling an entire utterance in LSF.
This PhD thesis takes place within the framework of the speech recognition in audio contents. The purpose of this work is to adapt the principles of audio identification to speech recognition as well as to design and to develop robust identification techniques. Audio identification systems by audio fingerprinting are designed to music track indexation but do not handle the specificities of the speech signal. At first, various methods of audio identification by fingerprint are studied as well as a first work of adaptation to speech recognition. New types of subfingerprint based on usual speech parameters are then proposed. Secondly, the various types of variability of the speech signal are described as well as the main parameters of acoustic representation of the speech signal. The robustness of various types of subfingerprint in extrincic variability and in intrinsic variability is estimated. In the presence of disturbances related to the environment and to the conditions of transmission of the speech signal (CTIMIT), a type of subfingerprint stemming from the audio identification turns out then the most robust.
Ambient assisted living aims to support the aging population. This is particularly the case with smart homes, equipped with multiple connected sensors, which enables to extend home care for the elderly. The manuscript begins by introducing the general problem of smart homes, after presenting further the three sub-themes that are the subject of the thesis, namely the activity recognition, privacy and dialogue systems. Activity recognition is the process of determining the day-to-day activities of a person or a group of people from the (raw) sensor data that the home is equipped with. An example of this is the detection of a person's fall. A smart home is typically based on the Internet of Things (IoT). Many data are produced, which may contain private or sensitive information. Some of this data must be shared externally, which may pose privacy issues. Finally, a natural way of communication for the user is to use the dialogue to interact with the smart home via dialogue manager. This thesis proposes contributions on these three sides, most of them based on deep learning.
The concept of complex IT systems includes all systems consisting of a large number of inter-connected and computer-managed components. The configuration and management of these systems involves a multitude of tasks that are critical to their proper functioning and their evolution. We also propose an extension to the «Planning Domain Definition Language» (PDDL) in order to modelize the knowledge of domain experts in the form of tasks decomposition methods that will be used to guide the HTN planning algorithm. Finally, we propose a panel of evaluation criteria of mixed-initiative systems that serve as a basis for the discussion about the performances and contributions of the MIP system.
This study focuses on Wolof medical terminology under the lexical semantics context. We talk about terminology units, particularly collocations. This choice has a direct link with our theoretical framework of analysis, the Meaning-Text Theory (MTT), which currently offers one of the best tools for describing collocation through Lexical Functions. Collocations constitute indices of specialization and have a singular lexico-syntactic functioning. We analyze them, on the basis of compiled scientific corpus, in order to have a holistic perception of lexical co-occurrence. Languages in Africa are often poorly endowed from a terminology view. This research is based on the lexical analysis model which takes into account three key parameters: meaning, form and combinatorics, to make a description of the Wolof lexicon which, in the long run, gives principles of terminology. The translational scope of the work lies in the interlinguistic approach we adopt to develop our list of terms. The operative side of the study is the constitution of a beginning of trilingual medical corpus (English-French-Wolof). The translation perspective of the terminology has revealed different processes to create and restitute medical terms (English and French) into Wolof.
Over the past 10 years, mobile phones have evolved considerably: the advent of touch screens and the disappearance of physical keyboards have changed the way we interact with these devices in our daily lives. However, text input is still an important task though activities such as taking notes, sending text messages or communicating on social networks. However, while using touch screen has great advantages in terms of dynamical interfaces and customization, such devices are not necessarily accessible to all. Indeed, for 39 million people worldwide affected by blindness, there are many difficulties related to the devices that will make such interactions difficult due to the lack of tactile reference: interactions with the device are possible but they are often laborious and repetitive, which then implies a lot of cognitive load, accuracy, memory and fatigue related problems. In this PhD thesis, we focused on the accessibility of text input interactions in the context of visual impairment. First, we studied the currently existing solutions designed for impaired users. The main issue of this research was to improve text input so that users have a better typing experience. As such, we designed a deductive solution called DUCK. This solution allows visually impaired to quickly enter text without worrying about how accurate their input is. A basic language-based modelling system allows at the each end of a word what the user wanted to type. This device was then tested with a sample of visually impaired people to assess the effectiveness of our solution. Further work was subsequently focused on two main optimizations. The first work focused on word lists. We studied and compared different interactions to allow the user to navigate and choose words effectively and easily when faced with a list of words proposed by a predictive or deductive system. The second optimization focuses on entering commonly used words. We also conducted a comparative study of different interactions models to type a short word efficiently without using the deduction system, which would be too time consuming for such words. Finally, we finish this PhD project by a longitudinal study that shows the DUCK keyboard with the integration of these optimizations. This new system has been used by visually impaired over a period of two weeks to study the effectiveness of the keyboard over the long term.
This thesis attempts to explore the relationship between language disorders and the way they are named by speech and language therapists. The French labels dysphasie, troubles spécifiques du langage écrit, aphasie, difficultés à l'apprentissage du langage écrit, dyslexie, retard de langage, relating to Specific Language Impairment, disorders of the written language, aphasia, dyslexia... belong to speech and language therapy (SLT) terminology and have always evolved following theoretical movements. In order to describe the inconsistency of the link between diagnostic labels and the reality of the labelled pathology, some epistemological, lexicological and terminological issues have been explored. Assessments enable SLT practitioners to make a diagnosis, following which they write a report (CRBO). These reports can be considered as the reflection of the representations speech and language therapists (SLTs) have of the disorders, and show how they use their own terminology. 435 authentic reports have been semi-automatically analysed descriptively using lexicological and terminological tools. The XML encoding captured the current use of the terms describing any pathology encountered by SLTs. The results show two terminological groups (one relating to the nature of the disorder, the other relating to its form), which illustrate the necessary nuances the SLT uses with these collocational phrases. The final phase of the analyses helped to produce a framework for a new speech and language therapy classification based on clinical practice (COFOP).
This thesis contributes to the understanding of an emergent and complex phenomenon: transformation/smartization of the urban public service system. This involves the cross combination of different services sectors such as transport, tourism, taxes, etc. We define three pillars of service science: Service System (SS), Service Innovation (IS) and Institutional Service Logic (LIS). We propose a method based on Latent Semantic Analysis (LSA), Factor Analysis (FA), text mining and grounded theory to inductively reveal 30 years of evolution of interdisciplinary SS, IS and LIS intellectual structure from 1986 to 2015. Then, we mobilize complex thinking as an integrating framework of components of the urban public service system that becomes smart, as a whole and its parts. Our research design is based on grounded theory ; observations, longitudinal case study with a multi-level approach (i.e. local and national) ; the dialogic model (Parmentier-Cajaiba &amp; Avenier, 2013) and the pragmatic constructivism epistemological paradigm (PECP). We define two working ontological hypotheses: the relational ontology and becoming ontology. From the theoretical point of view, our research contributes to the theoretical refinement of the literature on SS, IS and LIS. We propose three heuristic models from a process and content analysis (Baines et al. 2017) and grounded theory (Gioia &amp; Chittipeddi, 1991 ;Gioia et al., 2013). The first heuristic model contributes to the understanding of the institutional work process for the creation of institutional arrangements (Standard, boundary resources, APIs) between two antagonistic and complementary collective logics (Morin, 2005 ; Smets &amp; Jarzabkowski, 2013 ; Greenwood et al., 2017): the service-dominant logic of the market (Lusch &amp; Nambisan, 2015 ; Vargo &amp; Lusch, 2016) and the public service logic (Osborne et al., 2015 ; Osborne, 2017). The second heuristic model highlights components of a smart public service system. The third heuristic model highlights the drivers and barriers of institutional and structural transformation.
For a long time, polysemy used to be considered as a marginal or accidental phenomenon in language. Where as today, it is well known that polysemy is being part of linguistic systems. From the distinction established by G. Kleiber (1999), we consider two major trends in accordance with the way they conceive the link between meaning, reference and polysemy. On one hand, polysemy is described in terms of one basic referential sense from which secondary senses derive (objectivism). On the other hand, polysemy is analyzed as an areferential semantic potential from which senses emerge by contextual mechanisms (constructivism). On the basis of D. Tuggy's works (1993), we propose to organize the conceptual modelings of multiple meanings words along a continuum homonymy-polysemy-multifaciality-vagueness, in function of various parameters: entrenchment, cognitive salience, possibility of accessibility and of activation of the network components (schematic or elaborated values). So, we can highlight some organizational regularities specific to the semantic representation of polysems as well as a typology of polysemous senses. In Cognitive Grammar, it is a non modulary, compositional and dynamic process. The analysis of Adj-N and N-Adj noun phrases puts to the fore some regularities governing the activation of polysemic senses. These regularities are linked to the linguistic context (position and function of the adjective towards the qualified substantive) and to the extra-linguistic context.
In the field of machine learning, deep neural networks have become the inescapable reference for a very large number of problems. These systems are made of an assembly of layers,performing elementary operations, and using a large number of tunable variables. Using data available during a learning phase, these variables are adjusted such that the neural network addresses the given task. It is then possible to process new data. To achieve state-of-the-art performance, in many cases these methods rely on a very large number of parameters, and thus large memory and computational costs. Therefore, they are often not very adapted to a hardware implementation on constrained resources systems. Moreover, the learning process requires to reuse the training data several times, making it difficult to adapt to scenarios where new information appears on the fly. In this thesis, we are first interested in methods allowing to reduce the impact of computations and memory required by deep neural networks. Secondly, we propose techniques for learning on the fly, in an embedded context.
As we enter the era of digitalization, it is important to transform data into knowledge and use it to provide avenues for industrial improvement. The transformation of data into knowledge to optimize production is today a major industrial challenge. We will address the issues of scheduling, planning and load balancing on and between production lines. From a system point of view, these aspects are now considered as supported by ERP (Enterprise Resource Planning). ERPs are tools that are highly rigid in their structure and operation, imposing this rigidity on organizations. Our partner, iFAKT, an expert in load balancing, will support us in this thesis project. This thesis addresses two main areas of work: one on the integration of natural language processing, machine learning and a load balancing tool and the other on the actions to be taken in relation to this integration. For the implementation of these two major activities, we will contribute to the creation of methodologies combining the techniques and tools mentioned above within the framework of industry 4.0
Large industrial players have incorporated them into their production lines, but smaller companies hesitate due to high initial costs and the lack of programming expertise. In this thesis we introduce a framework that combines two disciplines, Programming by Demonstration and Automated Planning, to allow users without programming knowledge to program a robot. The user constructs the robot's knowledge base by teaching it new actions by demonstration, and associates their semantic meaning to enable the robot to reason about them.
My dissertation aims to observe and describe the acquisition of reference marks in written production among students from CE2 to CM2 (age 9 to 11) in narrative texts. This study makes it establish a map of students' competencies in terms of referential continuity according to their class level. Much research in linguistics have focused on the question of reference from the point of view of analysis and reception, but very little from the production point of view. In the didactics of French as a first language, studies that analyse referential expression in student texts mainly examine a limited sample of texts. Furthermore, psycholinguistic studies focusing on reference from a developmental point of view are more numerous for oral than for written studies and generally concern the youngest children (ages 0 to 3). In my dissertation, I lead the study from a progressive perspective. Indeed, I consider the development of students' writing skills by studying the processes they use to introduce and maintain referents in a narrative text. In particular, I explore whether students from CE2 to CM2 (9 to 11 years old) favour anaphoric relations (Reichler-Béguelin, 1988) or reference chains (Schnedecker, 1997 and if age and referent characteristics influence their choice. The pupil's linguistics marks analysis shows that class level and referent characteristics influence the number and nature of the coreferential expressions used.
XML has become a standard for representation and exchange of data across the web. Replication of data within different sites is used to increase the availability of data by minimizing the access's time to the shared data. However, the safety of the shared data remains an important issue. The aim of the thesis is to propose some models of XML access control that take into account both read and update rights and that overcome limitations of existing models. We consider the XPath language and the XQuery Update Facility to formalize respectively user access queries and user update operations. The last part of this thesis studies the practicality of our proposals. Firstly, we present our system, called SVMAX, that implements our solutions and we conduct an extensive experimental study, based on real-life DTD, to show that it scales well. Many native XML databases systems (NXD systems) have been proposed recently that are aware of the XML data structure and provide efficient manipulation of XML data by the use of most of W3C standards. Finally, we show that our system can be integrated easily and efficiently within a large set of NXD systems, namely BaseX, Sedna and eXist-db. To the best of our knowledge, SVMAX is the first system for securing XML data in the presence of arbitrary DTDs (recursive or not), a significant fragment of XPath and a rich class of XML update operations
During the first millennium BC, the already existing libraries needed to organize texts preservation, and were thus immediately confronted with the difficulties of indexation. The use of a title occurred then as a first solution, enabling a quick identification of every work, and in most of the cases, helping to discern works thematically close to a given one. While in Ancient Greece, titles have had a little informative function, although still performing an identification function, the invention of the printing office with mobile characters (Gutenberg, XVth century AD) dramatically increased the number of documents, which are today spread on a large-scale. But how some words can have a so big influence? What functions do the titles have to perform at this beginning of the XXIth century? How can one automatically generate titles respecting these functions? The automatic titling of textual documents is one of the key domains of Web pages accessibility (W3C standards) such as defined in a standard given by associations about the disabled. For a given reader, the goal is to increase the readability of pages obtained from a search, since usual searches are often disheartening readers who must supply big cognitive efforts. For a Website designer, the aim is to improve the indexation of pages for a more relevant search. Other interests motivate this study (titling of commercial Web pages, titling in order to automatically generate contents, titling to bring elements to enhance automatic summarization). In this study, we use NLP (Natural Language Processing) methods and systems. While numerous works were published about indexation and automatic summarization, automatic titling remained discreet and knew some difficulties as for its positioning in NLP. We support in this study that the automatic titling must be nevertheless considered as a full task. Having defined problems connected to automatic titling, and having positioned this task among the already existing tasks, we provide a series of methods enabling syntactically correct titles production, according to several objectives. In particular, we are interested in the generation of informative titles, and, for the first time in the history of automatic titling, we introduce the concept of catchiness. Our TIT' system consists of three methods (POSTIT, NOMIT, and CATIT), that enables to produce sets of informative titles in 81% of the cases and catchy titles in 78% of the cases.
We live in a world where a vast amount of data is being continuously generated. Data is different than simple numerical information, it now comes in a variety of forms. However, isolated data is valueless. But when this huge amount of data is connected, it is very valuable to look for new insights. At the same time, data is time sensitive. The most accurate and effective way of describing data is to express it as a data stream. If the latest data is not promptly processed, the opportunity of having the most useful results will be missed. So a parallel and distributed system for processing large amount of data streams in real time has an important research value and a good application prospect. This thesis focuses on the study of parallel and continuous data stream Joins. We divide this problem into two categories.
Local grammars constitute a descriptive formalism of linguistic phenomena and are commonly represented using directed graphs. Local grammars are used to recognize and extract patterns in a text, but they had some inherent limits in dealing with unexpected variations as well as in their capacity to access exogenous knowledge, in other words information to extract, during the analysis, from external resources and which may be useful to normalize, enhance validate or link the recognized patterns. The means are twofold: on the one hand, it is achieved by adding arbitrary conditional-functions, called extended functions, which are not predefined in advance and are evaluated from outside of the grammar. In the first part, we study the principles regarding the construction of the extended local grammars. Then, we present a proof-of-concept of a corpus-processing tool which implements the proposed formalism. Finally, we study some techniques to extract information from both well-formed and noisy texts. We focus on the coupling of external resources and non-symbolic methods in the construction of our grammars and we highlight the suitability of this approach in order to overcome the inherent limitations of classical local grammars
Automatic video understanding is expected to impact our lives through many applications such as autonomous driving, domestic robots, content search and filtering, gaming, defense or security. Video content is growing faster each year, for example on platforms such as YouTube, Twitter or Facebook. Automatic analysis of this data is required to enable future applications. Video analysis, especially in uncontrolled environments, presents several difficulties such as intraclass variability (samples from the same concept appear very differently) or inter-class confusion (examples from two different activities look similar). While these problems can be addressed with the supervised learning algorithms, fully-supervised methods are often associated with high annotation cost. Depending on both the task and the level of required supervision, the annotation can be prohibitive. For example, in action localization, a fully-supervised approach demands person bounding boxes to be annotated at every frames where an activity is performed. The cost of getting such annotation prohibits scalability and limits the number of training samples. This thesis addresses above problems in the context of two tasks, namely human action classification and localization. The former aims at recognizing the type of activity performed in a short video clip trimmed to the temporal extent of the action. The latter additionally extracts the space-time locations of potentially multiple activities in much longer videos. Our approach to action classification leverages information from human pose and integrates it with appearance and motion descriptors for improved performance. Our approach to action localization models the temporal evolution of actions in the video with a recurrent network trained on the level of person tracks. Finally, the third method in this thesis aims to avoid a prohibitive cost of video annotation and adopts discriminative clustering to analyze and combine different levels of supervision.
The Natural Language Processing (NLP) has technically improved regarding human speech vocabulary extension, morphosyntax scope, style and aesthetic. Affective Computing also tends to integrate an “emotional” dimension with a common goal shared with NLP which is to disambiguate the natural language and increase the human-machine interaction naturalness. Within social robotics, the interaction is modelled in dialogue systems trying to reach out an attachment dimension which effects need to an ethical and collective control. However, the situated natural language dynamics is undermining the automated system's efficiency, which is trying to respond with useful and suitable feedbacks. This thesis hypothesis supposes the existence of a “socio-affective glue” in every interaction, set up in between two individuals, each with a social role depending on a communication context. This glue is so the consequence of dynamics generated by a process which mechanisms rely on an altruistic dimension, but independent of dominance dimension as seen in emotions studies. This glue would allow the exchange of the language events between interlocutors, by regularly modifying their relation and their role, which is changing themselves this glue, to ensure the communication continuity. A wizard of oz approach – EmOz – is used to control the vocal primitives proposed as the only language tools of a Smart Home butler robot interacting with relationally isolated elderly. The relational isolation allows the dimensions the socio-affective glue in a contrastive situation where it is damaged. We could thus observe the primitives' effects through multimodal language cues. If the proposed primitives could have a real effect on the glue, the automated system will be able to train the persons to regain some unfit mechanisms underlying their relational construction, and so possibly increase their desire to communicate with their human social surroundings. The results from the collected EEE corpus show the relation changes through various interactional cues, temporally organised. These denoted parameters tend to build an incremental dialogue system in perspectives – SASI. The first steps moving towards this system reside on a speech recognition prototype which robustness is not based on the accuracy of the recognised language content but on the possibility to identify the glue degree (i.e. the relational state) between the interlocutors. Thus, the recognition errors avoid the system to be rejected by the user, by tempting to be balanced by this system's adaptive socio-affective intelligence.
Prostate cancer is the most frequent and the fourth leading cause of mortality in France. Actual diagnosis methods are often insufficient in order to detect and precisely locate cancer. Multiparametrics MRI is now one of the most promising method for accurate follow-up of the disease. However, the visual interpretation of MRI is not easy and it is shown that there is strongvariability among expert radiologists to perform diagnosis, especially when MR sequences are contradictory. Under these circumstances, a strong interest is for Computer-aided diagnosis systems (CAD) aiming at assisting expert radiologist in their final decision. This thesis presents our work toward the conception of a CADe which final goal is to provide a cancer probability map to expertradiologist. This thesis focuses both for cancer detection and characterization in order to provide a cancer probability map correlated to cancer aggressiveness (Gleason score). To that end we used a dictionary learning method to extract new features to better characterize cancer aggressiveness signatures as well as image features. Those features are then used as an input to Support Vector Machines (SVM) and Logistic Regression (LR) classifiers to produce a cancer probability map. We then focused on discriminating agressive cancers (Gleason score &gt;6) from other tissues and provided an analysis of the correlation between cancer aggressiveness and probabilities. Our work conclude on a strong capability to distinguish agressive cancer from other tissues but fails to precisely distinguish different grades of cancers
With the increasing awareness about the problem of climate change and the high level of energy consumption, a need for energy efficiency has emerged especially for electric power consumptions in buildings. Monitoring and understanding the electrical consumption of appliances can also be useful for predictive maintenance, power quality analyses, demand forecasting or occupancy detection. Thirty years ago, a method called Non Intrusive Load Monitoring (NILM) has been introduced. It consists of estimating individual appliance energy consumptions from the measurement of the total consumption of the building. Its main advantage over traditional sub-metering methods is to use a single electric power meter at the main breaker of the building and then use a disaggregation algorithm to separate the contributions of each appliance. The goal of this thesis is to address the algorithmic challenge offered by NILM. Its main difficulties are: (i) the standardization of the formulation, (ii) the ill-posedness of the problem, (iii) the lack of knowledge and (iv) the machine learning algorithm design. All our contributions follow from the principal objective that is to solve the NILM problem for huge systems such as commercial or industrial buildings using high frequency current and voltage measurements. However, houses and the specific equipment found inside these buildings are not excluded of the study. This thesis is split into two parts. In the first part, we tackle the lack of knowledge and datasets for NILM in commercial buildings. First of all, the NILM community has mostly focused on both residential NILM application and using low frequency data provided by power meter installed by utility providers. Our study on the rank of current matrix conducted for individual devices will serve as the base of a new device taxonomy and to prior assumptions on the rest of this thesis. Secondly, we address the lack of datasets especially for commercial buildings by developping an algorithm for generating synthetic current data based on a modelization of the current flowing through an electrical device. To encourage research on commercial buildings we release a synthesized dataset called SHED that can be used to evaluate NILM algorithms. In the second part, we deal with the NILM software challenges by exploring unsupervised source separation techniques. Motivated by the nature of the current signals, it uses a regularization term on the temporal variations of the activation matrix and a positivity constraint, and the columns of the signature matrix are constrained to lie in a specific set. To solve the resulting optimization problem, we rely on an alternating minimization strategy involving dual optimization and quasi-Newton algorithms. IVMF is the first proposed algorithm especially designed for high frequency NILM in huge buildings. We finally show that IVMF outperforms competing methods (Independent Component Analysis, Semi Non-negative Matrix Factorization) on NILM datasets.
Since 2006, deep learning algorithms which rely on deep architectures with several layers of increasingly complex representations have been able to outperform state-of-the-art methods in several settings. Deep architectures can be very efficient in terms of the number of parameters required to represent complex operations which makes them very appealing to achieve good generalization with small amounts of data. Although training deep architectures has traditionally been considered a difficult problem, a successful approach has been to employ an unsupervised layer-wise pre-training step to initialize deep supervised models. First, unsupervised learning has many benefits w.r.t. generalization because it only relies on unlabeled data which is easily found. Second, the possibility to learn representations layer by layer instead of all layers at once improves generalization further and reduces computational time. However, deep learning is a very recent approach and still poses a lot of theoretical and practical questions concerning the consistency of layer-wise learning with many layers and difficulties such as evaluating performance, performing model selection and optimizing layers. In this thesis we first discuss the limitations of the current variational justification for layer-wise learning which does not generalize well to many layers. We ask if a layer-wise method can ever be truly consistent, i.e. capable of finding an optimal deep model by training one layer at a time without knowledge of the upper layers. We find that layer-wise learning can in fact be consistent and can lead to optimal deep generative models. We prove that maximizing this criterion for each layer leads to an optimal deep architecture, provided the rest of the training goes well. Although this criterion cannot be computed exactly, we show that it can be maximized effectively by auto-encoders when the encoder part of the model is allowed to be as rich as possible. This gives a new justification for stacking models trained to reproduce their input and yields better results than the state-of-the-art variational approach. Additionally, we give a tractable approximation of the BLM upper-bound and show that it can accurately estimate the final log-likelihood of models. Taking advantage of these theoretical advances, we propose a new method for performing layer-wise model selection in deep architectures, and a new criterion to assess whether adding more layers is warranted. As for the difficulty of training layers, we also study the impact of metrics and parametrization on the commonly used gradient descent procedure for log-likelihood maximization. We show that gradient descent is implicitly linked with the metric of the underlying space and that the Euclidean metric may often be an unsuitable choice as it introduces a dependence on parametrization and can lead to a breach of symmetry. To mitigate this problem, we study the benefits of the natural gradient and show that it can restore symmetry, regrettably at a high computational cost. We thus propose that a centered parametrization may alleviate the problem with almost no computational overhead.
This thesis focuses on the synthesis of motion capture data with statistical models. Our starting point lies in two main problems one encounters when dealing with motion capture data synthesis, ensuring realism of postures and motion, and handling the large variability in the synthesized motion. We first describe an attempt to extend contextual Hidden Markov Models for handling variability in the data by conditioning the parameters of the models to an additional contextual information such as the emotion which which a motion was performed. We then propose a variant of a traditional method for performing a specific motion synthesis task called Inverse Kinematics, where we exploit Gaussian Processes to enforce realism of each of the postures of a generated motion. These preliminary results show some potential of statistical models for designing human motion synthesis systems. Yet none of these technologies offers the flexibility brought by neural networks and the recent deep learning revolution. The second part of the thesis describes the works we realized with neural networks and deep architectures. It builds on recurrent neural networks for dealing with sequences and on adversarial learning which was introduced very recently in the deep learning community for designing accurate generative models for complex data. We propose a simple system as a basis synthesis architecture, which combines adversarial learning with sequence autoencoders, and that allows randomly generating realistic motion capture sequences. Starting from this architecture we design few conditional neural models that allow to design synthesis systems that one can control up to some extent by either providing a high level information that the generated sequence should match (e.g. the emotion) or by providing a sequence in the style of which a sequence should be generated.
The automated treatment of familiar objects, either natural or artifacts, always relies on a translation into entities manageable by computer programs. The choice of these abstract representations is always crucial for the efficiency of the treatments and receives the utmost attention from computer scientists and developers. However, another problem rises: the correspondence between the object to be treated and "its" representation is not necessarily one-to-one! Therefore, the ambiguous nature of certain discrete structures is problematic for their modeling as well as their processing and analysis with a program. Natural language, and in particular its textual representation, is an example. The subject of this thesis is to explore this question, which we approach using combinatorial and geometric methods. These methods allow us to address the problem of extracting information from large networks of entities and to construct representations useful for natural language processing. Firstly, we start by showing combinatorial properties of a family of graphs implicitly involved in sequential models. These properties essentially concern the inverse problem of finding a sequence representing a given graph. The resulting algorithms allow us to carry out an experimental comparison of different sequential models used in language modeling. Secondly, we consider an application for the problem of identifying named entities. Following a review of recent solutions, we propose a competitive method based on the comparison of knowledge graph structures which is less costly in annotating examples dedicated to the problem. We also establish an experimental analysis of the influence of entities from capital relations. This analysis suggests to broaden the framework for applying the identification of entities to knowledge bases of different natures. These solutions are used today in a software library in the banking sector. Then, we perform a geometric study of recently proposed representations of words, during which we discuss a geometric conjecture theoretically and experimentally. This study suggests that language analogies are difficult to transpose into geometric properties, and leads us to consider the paradigm of distance geometry in order to construct new representations. Finally, we propose a methodology based on the paradigm of distance geometry in order to build new representations of words or entities. We propose algorithms for solving this problem on some large scale instances, which allow us to build interpretable and competitive representations in performance for extrinsic tasks. More generally, we propose through this paradigm a new framework and research leadsfor the construction of representations in machine learning.
This PhD thesis consists in jointly analyzing eye-tracking signals and multi-channel electroencephalograms (EEGs) acquired concomitantly on participants doing an information collection reading task in order to take a binary decision-is the text related to some topic or not? Textual information search is not a homogeneous process in time-neither on a cognitive point of view, nor in terms of eye-movement. On the contrary, this process involves several steps or phases, such as normal reading, scanning, careful reading-in terms of oculometry-and creation and rejection of hypotheses, confirmation and decision-in cognitive terms. In a first contribution, we discuss an analysis method based on hidden semi-Markov chains on the eye-tracking signals in order to highlight four interpretable phases in terms of information acquisition strategy: normal reading, fast reading, careful reading, and decision making. In a second contribution, we link these phases with characteristic changes of both EEGs signals and textual information. By using a wavelet representation of EEGs, this analysis reveals variance and correlation changes of the inter-channels coefficients, according to the phases and the bandwidth. And by using word embedding methods, we link the evolution of semantic similarity to the topic throughout the text with strategy changes. In a third contribution, we present a new model where EEGs are directly integrated as output variables in order to reduce the state uncertainty. This novel approach also takes into consideration the asynchronous and heterogeneous aspects of the data.
The teaching of prosody in French as a second language (FSL) classes and the use of speech software, which makes it possible to visualize the melody of learners and that of a French-speaking model, in order to compare them, are the subject of this research. Two attempts to experiment with the correction of speech prosody in the 1960s, which took visualization into account, are mentioned as a reminder. I was interested in the current situation of the teaching of the prosody of the FSL and the possibilities offered by digital tools, in order to understand it. Among other things, I wanted to see how learners reacted to such tools, and especially if their use brought convincing results. Learners worked from a manual, received independent training from a computer, were given group explanations from the teacher, sometimes with individual explanations. The first two experiments (pilot and general) were carried out with a migrant audience, the third was carried out in a language laboratory where the students worked by themselves, and the last one, with explanations, in small groups. The productions of the Francophone model and those of the learners were analyzed. Surveys on the use of a digital tool and visualization with WinPitch (WP) and WinPitch Language Teaching and Learning (WP LTL) showed that most learners enjoyed working with these software. For the last two experiments, conducted in a university context, I set up two working groups: one experimental with WP and the other control. It turned out that the students in the experimental group had better results than those in the control group. For the last experiment, explanations concerning "prosodic grammar", in this case the one based on Ph. The results obtained made it possible to validate the working hypotheses. They also show that the use of WP visualization, accompanied by the teacher's explanations, will be beneficial and justified in language classes in order to consciously improve oral expression in French.
The common thread of this thesis is the study of Hawkes processes. These point processes decrypt the cross-causality that occurs across several event series. Namely, they retrieve the influence that the events of one series have on the future events of all series. For example, in the context of social networks, they describe how likely an action of a certain user (such as a Tweet) will trigger reactions from the others. The first chapter consists in a general introduction on point processes followed by a focus on Hawkes processes and more specifically on the properties of the widely used exponential kernels parametrization. In the following chapter, we introduce an adaptive penalization technique to model, with Hawkes processes, the information propagation on social networks. This penalization is able to take into account the prior knowledge on the social network characteristics, such as the sparse interactions between users or the community structure, to reflect them on the estimated model. Our technique uses data-driven weighted penalties induced by a careful analysis of the generalization error. Next, we focus on convex optimization and recall the recent progresses made with stochastic first order methods using variance reduction techniques. The fourth chapter is dedicated to an adaptation of these techniques to optimize the most commonly used goodness-of-fit of Hawkes processes. Besides, such objectives include many linear constraints that are easily violated by classic first order algorithms, but in the Fenchel-dual problem these constraints are easier to deal with. Hence, our algorithm's robustness is comparable to second order methods that are very expensive in high dimensions. Finally, the last chapter introduces a new statistical learning library for Python 3 with a particular emphasis on time-dependent models, tools for generalized linear models and survival analysis. Called tick, this library relies on a C++ implementation and state-of-the-art optimization algorithms to provide very fast computations in a single node multi-core setting. Open-sourced and published on Github, this library has been used all along this thesis to perform benchmarks and experiments.
Coherence is a property that characterizes the text as a unified and interpretative whole. Chinese learners of French as a foreign language (FLE) may have difficulty producing a coherent text in French. This thesis therefore aims to analyze the management of the textual coherence of these learners, and more precisely, the uses of connectors as well as the resolution of anaphors and reference chains in the written productions of Chinese students who are at an intermediate and advanced level of FLE in France. The two cohorts receive the same instructions and carry out two writing tasks (narration and argumentation) using the word processing software GenoGraphiX which records and reconstructs the writing process.
In the field of neology, different methodological approaches for the detection and extraction of semantic neologisms have been developed using strategies such as word sense disambiguation and topic modeling, but there is still not a proposal for a system for the detection of these units. Beginning from a detailed study on the necessary theoretical assumptions required to delimit and describe semantic neologisms, in this thesis, we propose the development of an application to identify and extract said units using statistical,data mining and machine learning strategies. The proposed methodology is based on treating the process of detection and extraction as a classification task, which consists on analyzing the concordance of topics between the semantic field from the main meaning of a word and the text where it is found. To build the architecture of the proposed system,we analyzed five automatic classification methods and three deep learning based word embedding models. Our analysis corpus is composed of the semantic neologisms of the computer science field belonging to the database of the Observatory of Neology of the Pompeu Fabra University, which have been registered from 1989 to 2015. We used this corpus to evaluate the different methods that our system implements: automatic classification,keyword extraction from short contexts, and similarity list generation. This first methodological approach aims to establish a framework of reference in terms of detection and extraction of semantic neologisms.
Stochastic systems with partial information allow one to represent numerous systems whose parameters are unknown and whose operation may depend on out-of-control factors. In this thesis, we study several problems linked to these systems. First one is diagnosability, that is the capacity to decide if a particular event occurred. Second one is classification which is the capacity to decide from a trace of an execution which system produced it. Finally, we are interested in the guarantees one can obtain by learning the probability transitions of a stochastic system.
Throughout the centuries, financial institutions have shaped the financial landscape and influenced economic activity. The goal of this dissertation is to highlight, from a theoretical point of view, fundamental limitations of modern institutions and eventually derive implications regarding the future role of those institutions. The first chapter provides an analysis of Central Clearing Platforms (CCP). In the aftermath of the financial crisis of 2008, financial authorities around the world implemented regulations imposing central clearing on most derivative products. It is shown that central clearing often requires a larger liquidity buffer than bilateral clearing. The second chapter presents a continuous-time learning model meant to represent the learning process of an institution such as a central bank regarding a hidden information held by the market. The last chapter introduces a specific blockchain-based payment network called the Lightning Network. It allows users to transfer value instantly without relying on any trusted third party. We discuss the implications regarding the structure of this network as well as its ability to become an important part of the financial landscape.
Marine traffic is the main contributor to anthropogenic underwater noise: since the 1970s, the increase in deep-sea shipping has increased the ambient noise by more than 10 dB in some areas. In response to this concern, the Marine Strategy Framework Directive (MSFD) recommends acoustic monitoring. Few studies are concerned with coastal activity and the noises radiated by small craft while these coastal environments are the purveyors of 41.7% of the ecosystem services produced by the oceans. Between the academic and the industrial world, this PhD was to answer the different scientific and industrial questions on the topic of the coastal traffic in terms of the influence in the soundscape and the detection and classification of the coastal craft. Without information on the coastal maritime traffic, a visual identification protocol is proposed using GoPro® images processing and produced the same data as the AIS (position, speed, size and type of craft); It allows to create maritime traffic maps on a disk of 1.6km radius. The traffic is characterized by two acoustic descriptors: the SPL linked to the distance of the nearest boat and the ANL linked to the number of boats present in a 500 m radius disc. The spatiotemporal monitoring of these descriptors allows to identify the impact on the maritime traffic on the coastal acoustic landscape. The acoustic detection and the classification are performed after individual characterization of the noise by a set of acoustic parameters and using of supervised machine learning algorithm. A specific protocol for the creation of the classification tree is proposed by comparing the acoustic data with the physical and contextual characteristics of each boat. The methods are applied on the flotilla of coastal boats present in the Bay of Calvi (Corsica) during summer.
Today, a huge amount of data is made available to the research community through several web-based libraries. Enhancing data collected from scientific documents is a major challenge in order to analyze and reuse efficiently domain knowledge. To be enhanced, data need to be extracted from documents and structured in a common representation using a controlled vocabulary as in ontologies. Our research deals with knowledge engineering issues of experimental data, extracted from scientific articles, in order to reuse them in decision support systems. Experimental data can be represented by n-ary relations which link a studied object (e.g. food packaging, transformation process) with its features (e.g. oxygen permeability in packaging, biomass grinding) and capitalized in an Ontological and Terminological Ressource (OTR). An OTR associates an ontology with a terminological and/or a linguistic part in order to establish a clear distinction between the term and the notion it denotes (the concept). Our work focuses on n-ary relation extraction from scientific documents in order to populate a domain OTR with new instances. More precisely, firstly, we propose to focus on unit of measure extraction which are known to be difficult to identify because of their typographic variations. We propose to rely on automatic classification of texts, using supervised learning methods, to reduce the search space of variants of units, and then, we propose a new similarity measure that identifies them, taking into account their syntactic properties.
Hyperlinking is a way to connect texts, sounds, images or videos published on the World Wide Web. It is now the standard way through which Internet users, content editors or search engines create and communicate information. Prima facie, the relation between hyperlinking and European Union law does not appear to be self-evident. Yet, hyperlinking raises a range of legal issues that challenge its apprehension by EU law. As such, it is an important means to exercise the freedom of expression and information online. Thus it can be expected from EU law to promote or protect hyperlinking. Hyperlinking also contributes to the dissemination of illegal or harmful content online, in opposition with EU law. Regarding these issues, the purpose of this study is to assess how and in what extent EU law delimits in a coherent way the freedom to link and its limits. In order to answer this question, we will first demonstrate the emergence of a freedom to link regarding european intellectual property law. The application of EU law to hyperlinking is however facing problems regarding the coherence of the law. To solve this problem, proposals aimed at restoring the stability of EU law will punctuate this study. This study thus provides a basis for understanding and thinking, common to the European Union, regarding the application of the law to hyperlinking.
Developing lexico-semantic resources is a major issue in the Natural Language Processing field. These resources, by making explicit inter alia some knowledge possessed only by humans, aim at providing the ability of a precise and complete text understanding to NLP tasks. Popular resources-building strategies involving crowdsourcing are flowering in NLP and are proved to be successful. However, the resulted resources are not free of errors and lack some important semantic relations. We designed an endogenous consolidation system for this type of networks based on inferring and annotating new semantic relations using the already existing ones, as well as extracting and proposing inference rules able to (re)generate a considerable part of the network.
The goal of this thesis, conducted within an industrial framework, is to pair textual media content. Specifically, the aim is to pair on-line news articles to relevant videos for which we have a textual description. The main issue is then a matter of textual analysis, no image or spoken language analysis was undertaken in the present study. The question that arises is how to compare these particular objects, the texts, and also what criteria to use in order to estimate their degree of similarity. We consider that one of these criteria is the topic similarity of their content, in other words, the fact that two documents have to deal with the same topic to form a relevant pair. This problem fall within the field of information retrieval (ir) which is the main strategy called upon in this research. The pairing system developed in this thesis distinguishes different steps which complement one another. In the first step, the system uses natural language processing (nlp) methods to index both articles and videos, in order to overcome the traditionnal bag-of-words representation of texts. In the second step, two scores are calculated for an article-video pair: the first one reflects their topical similarity and is based on a vector space model; the second one expresses their proximity in time, based on an empirical function. The constraints imposed both by the data and the specific need of the partner company led us to adapt the evaluation protocol traditionnal used in ir, namely the cranfield paradigm. We therefore propose an alternative solution for evaluating the system that takes all our constraints into account.
In recent years, there has been an actual effort to constitute and promote children's writings corpora especially in French. The first research works on writing acquisition relied on small corpora that were not widely distributed. Longitudinal corpora, monitoring a cohort of children's productions from similar collection conditions from one year to the next, do not exist in French yet. Moreover, although natural language processing (NLP) has provided tools for a wide variety of corpora, few studies have been conducted on children's writings corpora. This new scope represents a challenge for the NLP field because of children's writings specificities, and particularly their deviation from the written norm. Hence, tools currently available are not suitable for the exploitation of these corpora. There is therefore a challenge for NLP to develop specific methods for these written productions. This thesis provides two main contributions. On the one hand, this work has led to the creation of a large and digitized longitudinal corpus of children's writings (from 6 to 11 years old) named the Scoledit corpus. Its constitution implies the collection, the digitization and the transcription of productions, the annotation of linguistic data and the dissemination of the resource thus constituted. On the other hand, this work enables the development of a method exploiting this corpus, called the comparison approach, which is based on the comparison between the transcription of children's productions and their standardized version. In order to create a first level of alignment, this method compared transcribed forms to their normalized counterparts, using the aligner AliScol. It also made possible the exploration of various linguistic analyses (lexical, morphographic, graphical). And finally, in order to analyse graphemes, an aligner of transcribed and normalized graphemes, called AliScol_Graph was created.
Working memory can be defined as the ability to temporarily store and manipulate information of any kind. For example, imagine that you are asked to mentally add a series of numbers. In order to accomplish this task, you need to keep track of the partial sum that needs to be updated every time a new number is given. The working memory is precisely what would make it possible to maintain (i.e. temporarily store) the partial sum and to update it (i.e. manipulate). In this thesis, we propose to explore the neuronal implementations of this working memory using a limited number of hypotheses. To do this, we place ourselves in the general context of recurrent neural networks and we propose to use in particular the reservoir computing paradigm. This type of very simple model nevertheless makes it possible to produce dynamics that learning can take advantage of to solve a given task. In this job, the task to be performed is a gated working memory task. The model receives as input a signal which controls the update of the memory. When the door is closed, the model should maintain its current memory state, while when open, it should update it based on an input. In our approach, this additional input is present at all times, even when there is no update to do. In other words, we require our model to be an open system, i.e. a system which is always disturbed by its inputs but which must nevertheless learn to keep a stable memory. In the first part of this work, we present the architecture of the model and its properties, then we show its robustness through a parameter sensitivity study. This shows that the model is extremely robust for a wide range of parameters. More or less, any random population of neurons can be used to perform gating. Furthermore, after learning, we highlight an interesting property of the model, namely that information can be maintained in a fully distributed manner, i.e. without being correlated to any of the neurons but only to the dynamics of the group. More precisely, working memory is not correlated with the sustained activity of neurons, which has nevertheless been observed for a long time in the literature and recently questioned experimentally. This model confirms these results at the theoretical level. In the second part of this work, we show how these models obtained by learning can be extended in order to manipulate the information which is in the latent space. We therefore propose to consider conceptors which can be conceptualized as a set of synaptic weights which constrain the dynamics of the reservoir and direct it towards particular subspaces; for example subspaces corresponding to the maintenance of a particular value. More generally, we show that these conceptors can not only maintain information, they can also maintain functions. In the case of mental arithmetic mentioned previously, these conceptors then make it possible to remember and apply the operation to be carried out on the various inputs given to the system. These conceptors therefore make it possible to instantiate a procedural working memory in addition to the declarative working memory. We conclude this work by putting this theoretical model into perspective with respect to biology and neurosciences.
Speaker diarization (SD) involves the detection of speakers within an audio stream and the intervals during which each speaker is active, i.e. the determination of 'who spoken when'. The first part of the work presented in this thesis exploits an approach to speaker modelling involving binary keys (BKs) as a solution to SD. BK modelling is efficient and operates without external training data, as it operates using test data alone. The presented contributions include the extraction of BKs based on multi-resolution spectral analysis, the explicit detection of speaker changes using BKs, as well as SD fusion techniques that combine the benefits of both BK and deep learning based solutions. The SD task is closely linked to that of speaker recognition or detection, which involves the comparison of two speech segments and the determination of whether or not they were uttered by the same speaker. Even if many practical applications require their combination, the two tasks are traditionally tackled independently from each other. The second part of this thesis considers an application where SD and speaker recognition solutions are brought together. The new task, coined low latency speaker spotting (LLSS), involves the rapid detection of known speakers within multi-speaker audio streams. It involves the re-thinking of online diarization and the manner by which diarization and detection sub-systems should best be combined.
The internet of things has now entered every home and, with a society more and more focused towards wellness, these sensors measure and offer henceforth a wide variety of physiological data. Virtual reality technologies reaching maturity, coupled with the advent of the internet of things, allow consequently new opportunities to propose improved immersive experiences. In an attempt to overcome these limitations, this study therefore focuses on the original usage of smart wearables as substitutes for traditional sensors in immersive applications. More precisely, the impact of biofeedback, via off-the-shelf smart wearables, on user engagement and the sense of agency. We have thus carried out two experiments allowing us to study the impacts of the different biofeedback modalities on user experience. Our first experiment implements a biofeedback based on heart rate in a virtual reality horror game, allowing to enhance the feeling of fear. The results of this experiment confirm the interest of using smart wearables to capture physiological data for immersive virtual reality experiences. They also highlight the positive impact of this biofeedback on user engagement. The second experiment focuses on the use of cardiac activity as a mandatory interaction mechanism. The results of this experiment demonstrate the possibility of using the said interaction mechanic for virtual reality experiences and indicate a positive impact on the sense of agency, linked with the level of competency of the participants. On a theoretical level, this thesis proposes a synthesis of user experience models in virtual environment and submit the foundations of a model that we call "physiological immersion".
Classifiers ga (cl.16), ku (cl.17) and mu (cl.18) express respectively “contact”, “distance” and “interiority”: a) when they are prefixed by ‑úma /place/; b) when they appear in conjugated verbs or are prefixed by themes of determinants; c) when they are followed by a name in the form of a free morpheme; d) or when they are followed by a verb. Bu (cl.14) expresses, in addition, “abstract” value when it is combined to lexical bases; when it is prefixed to the themes of determinants, it carries temporal, comparative and causal values; it serves thirdly to mark the hypothesis in the form of a free morpheme.
Since 2000, a significant progress has been recorded in research work which has proposed to learn object detectors using large manually labeled and publicly available databases. However, when a generic object detector is applied on images of a specific scene, the detection performances will decrease considerably. This decrease may be explained by the differences between the test samples and the learning ones at viewpoints taken by camera(s), resolution, illumination and background images. In addition, the storage capacity evolution of computer systems, the "video surveillance" democratization and the development of automatic video-data analysis tools have encouraged research into the road-traffic domain. The ultimate aims are the management evaluation of current and future trafic requests, the road infrastructures development based on real necessities, the intervention of maintenance task in time and the continuous road surveillance. Moreover, traffic analysis is a problematicness where several scientific locks should be lifted. These latter are due to a great variety of traffic fluidity, various types of users, as well multiple weather and lighting conditions. Thus, developing automatic and real-time tools to analyse road-traffic videos has become an indispensable task. These tools should allow retrieving rich data concerning the traffic from the video sequence and they must be precise and easy to use. This is the context of our thesis work which proposes to use previous knowledges and to combine it with information extracted from the new scene to specialize an object detector to the new situations of the target scene. In this thesis, we propose to automatically specialize a generic object classifier/detector to a road traffic scene surveilled by a fixed camera. We mainly present two contributions. This formalization approximates iteratively the previously unknown target distribution as a set of samples composing the specialized dataset of the target scene. The samples of this dataset are selected from both source dataset and target scene further to a weighting step using some prior information on the scene. The obtained specialized dataset allows training a specialized classifier to the target scene without human intervention. The second contribution consists in proposing two observation strategies to be used in the SMC filter's update step. They are used to weight the target samples. The different experiments carried out have shown that the proposed specialization approach is efficient and generic. We have been able to integrate multiple observation strategies. It can also be applied to any classifier / detector. In addition, we have implemented into the Logiroad OD SOFT software the loading and utilizing possibilities of a detector provided by our approach. We have also shown the advantages of the specialized detectors by comparing their results to the result of Logiroad's Vu-meter method.
The discourse produced in a guided tour stems from different communicative modalities which include the visit assisted by a socio-technical device and the visit guided by an education and visitor service officer. Several issues arise such as the genre taxonomy of the discourse linked to the specific field studied, the unit of the text segmentation which has to free itself from the scriptural or oral feature of the text production, the textual categorisation and indexation of the studied genre. Indeed, the characterisation parameter value must introduce an essential prototype in order to categorise and index the texts of the studied genres. Furthermore, the quantitative analysis of a selected text compilation is rooted in the discourse analysis and corpus linguistic approaches. The method followed here, which introduces textual segmentation rules such as qualitative manual annotation and quantitative analysis suggests structural patterns of each considered genre.
In this thesis, we focus on data integration of raw data coming from heterogeneous and multi-origin data sources on the Web. The global objective is to provide a generic and adaptive architecture able to analyze and combine this heterogeneous, informal, and sometimes meaningless data into a coherent smart data set. In this report, we propose new models and techniques to adapt the combination and integration process to the diversity of data sources. We focus on transparency and dynamicity in data source management, scalability and responsivity according to the number of data sources, adaptability to data source characteristics, and finally consistency of produced data (coherent data, without errors and duplicates). In order to address these challenges, we first propose a meta-models in order to represent the variety of data source characteristics, related to access (URI, authentication) extraction (request format), or physical characteristics (volume, latency). By relying on this coherent formalization of data sources, we define different data access strategies in order to adapt access and processing to data source capabilities. With help form these models and strategies, we propose a distributed resource oriented software architecture, where each component is freely accessible through REST via its URI. In order to improve the data quality of our approach, we then focus on the data uncertainty that could appear in a Web context, and propose a model to represent uncertainty in a Web context. We introduce the concept of Web resource, based on a probabilistic model where each resource can have different possible representations, each with a probability. This approach will be the basis of a new architecture optimization allowing to take uncertainty into account during our combination process
French population is rapidly aging. Senior citizens ratio is increasing and our society needs to rethink its organization, taking into account this change, better knowing this fast growing population group. Even if numerous cohorts of elderly people already exist worldly with four in France and, even as they live in growing numbers in nursing homes and out-patient treatment clinics, knowledge of this population segment is still missing. Today several health and medico-social structures groups as Korian and Orpéa invest in big relational data bases enabling them to get real-time information about their patients/residents. Since 2010 all Korian residents' files are dematerialized and accessible by requests. They contain at the same time, structured medico-social data describing the residents as well as their treatments and pathologies, but also free-textual data detailing their daily care by the medical staff. Through time and as the computerized resident file (DRI) was mainly conceived as a data base management application, it appeared essential to mine these data and build a decision-making tool intended to improve the care efficiency. The Ageing Well Institute becoming meanwhile the Korian Ageing Well Foundation chose then, working in a private/public partnership, to finance a research work intented to better understand these datas' informative potential, to assess their reliability and response to public health threats. This research work and this thesis were then designed in several steps: - First, a content analysis of the data warehouse DRI, the objective being to build a research data base, with a social side and a health side. This was the first paper subject. - Then, by direct extraction of the residents' socio-demographic information at nursing home (NH) entry, adding hospitalizations and deaths, and finally, by an iterative textual extraction process of the transmissions data and by using the Delphi method, we created twenty-four syndromes, added hospitalizations and deaths and built a syndromic data base, the Ageing Well data base. This information system of a new kind, allowed the constitution of a public health cohort for elderly people from the BBV residents' population and its syndromic longitudinal follow-up. The BBV was also scientifically assessed for surveillance and public health research through present situation analysis: content, periodicity and data quality. This cohort then gave us the opportunity to build a surveillance tool and follow the residents' population in real-time by watching their 26 daily frequency syndromic distributions. The methodology for that assessment, Atlanta CDCs' health surveillance systems method, was used for flu and acute gastro enteritis syndroms and was the second paper subject. - Finally, the building of a new public health tool: each syndrom's distribution through time (transmissions dates) and space (transmissions NH ids) opened the research field to new data exploration methods. I used these to study different health problems afflicting senior citizens: frequent falls, cancer, vaccinations and the end of life.
Dynamical systems exhibit temporal behaviors that can be expressed in various sequential forms such as signals, waveforms, time series, and event sequences. Detecting patterns over such temporal behaviors is a fundamental task for understanding and assessing these systems. Since many system behaviors involve certain timing characteristics, the need to specify and detect patterns of behaviors that involves timing requirements, called timed patterns, is evident. However, this is a non-trivial task due to a number of reasons including the concurrency of subsystems and density of time. The key contribution of this thesis is in introducing and developing emph{timed pattern matching}, that is, the act of identifying segments of a given behavior that satisfy a timed pattern. We propose timed regular expressions (TREs) and metric compass logic (MCL) as timed pattern specification languages. We first develop a novel framework that abstracts the computation of time-related aspects called the algebra of timed relations. Then we provide offline matching algorithms for TRE and MCL over discrete-valued dense-time behaviors using this framework and study some practical extensions. It is necessary for some application areas such as reactive control that pattern matching needs to be performed during the actual execution of the system. For that, we provide an online matching algorithm for TREs based on the classical technique of derivatives of regular expressions. We believe the underlying technique that combines derivatives and timed relations constitutes another major conceptual contribution for timed systems research. Furthermore, we present an open-source tool Montre that implements our ideas and algorithms. We explore diverse applications of timed pattern matching over several case studies using Montre. Finally we discuss future directions and several open questions emerged as a result of this thesis.
The present dissertation focuses on the factors implied in relative clause processing, mainly subject/object relative clauses and relative clause attachment. I propose to go beyond the notion of subject/object languages or high/low attachment languages. This description is necessary to grasp their influence in processing, especially the differences observed across languages in experiments. In line with the multifactorial approach, I analyse the influence of semantics and pragmatics in relative clause processing, especially in French and English. Finally, to broaden the perspective beyond the linguistic domain, I show the influence of other non-linguistic factors on relative clause processing, by presenting visual world eye-tracking and forced choice experiments about the influence of mathematical priming on relative clause attachment in French.
In the modern economy, creating agile business processes is one of the conditions to obtain/ maintain competitive advantage on the market. Actually, the management of business processes is supported by the BPM (Business Process Management) approach. It addresses the management, transformation and improvement of organizational operations. Yet, actual BPM does not feature the means to have a continuous adaptation of their business processes and quick adjustment of their models and resources allocation to meet changing environmental conditions. To support agility at IT level, we use the SOA (Service Oriented Architecture) approach. Indeed, the SOA can provide numerous benefits to the organization, enabling it to reduce complexity and increase flexibility through their reutilization and modularity features. Moreover, resources which are important assets in successful process's implementation are widely supported with agile organization regarded as primordial factor for successful agility implementation. For this reason, we propose an approach that combines management of processes with the required skills to their execution and to better enhance the process flexibility we combine BPM with SOA in a social environment.
The use of equivalent terms or semantically close is necessary to increase the coverageand sensitivity of applications such as information retrieval and extraction or semanticannotation of documents. In the context of the adverse drug reactions identification, sensitivityis also sought to detect more exhaustively spontaneous reports and better monitordrug risk. This is the reason that motivates our work. In our work, we thus seek to detectsemantically close terms and the together using several methods: unsupervised algorithms, terminological resources exploited with terminological reasoning and methodsof Natural Language Processing, such as terminology structuring, where we aim to detecthierarchical and synonymous relations. We conducted many experiments and evaluations of generated, which show that the proposed methods can efficiently contribute tothe task in question.
The aim of this dissertation is to explore the syntactic-semantic interface of 'constructions' which contain secondary predicates – either depictive or resultative. The main problems will be to deal (i) with the selection of the subjects (or hosts) of these types of predicates and (ii) with the aspectual class of the verbs used in these sentences. In the first part, the various patterns implied in resultative clauses will be examined, leading to the conclusion that the basic principle that governs the syntax of these clauses can be identified with the 'Restriction on direct objects' – or RDO. After invalidating many would-be counter-examples to the RDO, its validity will be reasserted, notably as a diagnostic of unaccusativity in English. The second part of the dissertation is devoted to depictive predicates – in particular (i) to the constraints that determine the choice of the Controller in this type of secondary predication, and (ii) to the relevant properties of depictive adjectives in contra-distinction to other types of adjuncts, often identified as 'participant-oriented'. Finally, the distribution of those Russian adjectives which possess long and short forms, which is conditioned by specific agreement or concord properties, is examined, leading to a tentative reconstruction of a diachronic process which has led to their distribution in today's Russian.
Within the field of brain mapping, we identified the need for a tool which is grounded in the detailed knowledge of individual variability of sulci. In this thesis, we develop a new brain mapping tool called NeuroLang, which utilises the spatial geometry of the brain. We approached this challenge with two perspectives: firstly, we grounded our theory firmly in classical neuroanatomy. Secondly, we designed and implemented methods for sulcus-specific queries in the domain-specific language, NeuroLang. We tested our method on 52 subjects and evaluated the performance of NeuroLang for population and subject-specific representations of neuroanatomy. Then, we present our novel, data-driven hierarchical organisation of sulcal stability. To conclude, we summarise the implication of our method within the current field, as well as our overall contribution to the field of brain mapping.
The acquisition of early lexicon is very important for the development of language considering that it is the early lexicon that builds infants'first significant utterances and that it prefigures to a certain extent infants'future language skills. It is well established that lexical acquisition presents common developmental trends and milestones, nevertheless a great amount of individual variation exists. This variation comes from linguistic, social and/or idiosyncratic factors. Further research should be done to investigate the possible influence of evaluation procedures on the results. Although the use of a complementary approach could limit this bias, it has rarely been used in lexical acquisition research. This work aims at describing not only the common developmental trajectories of early lexicon in French monolingual children, but also the inter-individual differences. More specifically, we want to show the importance of applying a complementary approach and of exploring word production during spontaneous interactions in real-life settings to better interpret inter-individual differences. Overall, the development and the composition of individual lexicon, evaluated through the IFDC, follow the trends already reported in the literature. As for the spontaneous vocabulary, we focused our study on 4 children at the 15-52; 50; 70-120 word linguistic stages (corpus CIBLÉ). The integration of two complementary approaches, i.e. parental questionnaires and spontaneous observations, proved to be efficient and allowed us to reliably evaluate the lexical development and to avoid the bias linked to the use of a single method. To better understand the results variations between the two methods, we explored the situational and interactional context on the corpus CIBLÉ. We defined and categorized the different situations in the corpus TOTAL, then we focused on the corpus CIBLÉ to calculate their duration and we found variations between situations. For example, the two children with the smallest lexicon had the longest duration of solitary play. During this activity, the number of produced words was generally very low. Next, we describe the interactional context, and more particularly, the rate and the nature of the children exchanges. The analyses revealed an important variation between measures and differences in the exchange rate among children. To a certain degree, for some children the interactional measures provide a richer interpretation of lexical measures. Our work clearly shows the advantages of combining several types of data to evaluate the early lexical development and the differences between individuals and encourages this approach. The analysis of situational and interactional contexts shows that these are crucial for understanding children lexical measures and better interpreting intra-and inter-individual differences.
Domain analysis is the process of analyzing a family of products to identify their common and variable features. In this thesis, our general contribution is to address mining and modeling variability from informal documentation. We investigate the applicability of this idea by instantiating it in two different contexts: (1) reverse engineering Feature Models (FMs) from regulatory requirements in nuclear domain and (2) synthesizing Product Comparison Matrices (PCMs) from informal product descriptions. In the first case study, we adopt NLP and data mining techniques based on semantic analysis, requirements clustering and association rules to assist experts when constructing feature models from these regulations. The evaluation shows that our approach is able to retrieve 69% of correct clusters without any user intervention. Moreover, features dependencies show a high predictive capacity: 95% of the mandatory relationships and 60% of optional relationships are found, and the totality of requires and exclude relationships are extracted. In the second case study, our proposed approach relies on contrastive analysis technology to mine domain specific terms from text, information extraction, terms clustering and information clustering. Overall, our empirical study shows that the resulting PCMs are compact and exhibit numerous quantitative and comparable information. The user study shows that our automatic approach retrieves 43% of correct features and 68% of correct values in one step and without any user intervention. We show that there is a potential to complement or even refine technical information of products. The main lesson learnt from the two case studies is that the exploitability and the extraction of variability knowledge depend on the context, the nature of variability and the nature of text.
With the development and the expansion of connected devices in every domain, several projects on stream processing have been developed. This thesis has been realized as part of the FUI Waves, a reasoning stream processing engine distributed. The use case for the development was the processing of data streamed from a potable water distribution network, more specifically the detection of anomalies in the quality measures and their contextualisation using external data. Several contributions have been realized and integrated in different stages of the project, with evaluations and publications witnessing their relevance. These contributions use an ontology that has been designed thanks to collaboration with domain experts working for our water data management project partner. The use of geographical data allowed to realize a profiling system aiming at improving the anomaly contextualisation process. An ontology encoding approach, adapted to RDF stream processing, has been developed to support RDFS inferences enriched with owl: sameAs. Conjointly, a compressed formalism (PatBin) has been designed to represent streams. PatBin is based on the regularity of patterns found in incoming streams. Moreover, a query language has been conceived from PatBin, namely PatBinQL. It integrates a reasoning strategy that combines both materialization and query rewritting. Finally, given deductions coming from a Waves machine learning component, a query generation tool has been developed. These different contributions have been evaluated on both real-world and synthetic datasets.
This thesis investigates new clustering paradigms and algorithms based on the principle of the shared nearest-neighbors (SNN. As most other graph-based clustering approaches, SNN methods are actually well suited to overcome data complexity, heterogeneity and high-dimensionality. The first contribution of the thesis is to revisit existing shared neighbors methods in two points. We first introduce a new SNN formalism based on the theory of a contrario decision. This allows us to derive more reliable connectivity scores of candidate clusters and a more intuitive interpretation of locally optimum neighborhoods. We also propose a new factorization algorithm for speeding-up the intensive computation of the required sharedneighbors matrices. The second contribution of the thesis is a generalization of the SNN clustering approach to the multi-source case. Whereas SNN methods appear to be ideally suited to sets of heterogeneous information sources, this multi-source problem was surprisingly not addressed in the literature beforehand. The main originality of our approach is that we introduce an information source selection step in the computation of candidate cluster scores. As shown in the experiments, this source selection step makes our approach widely robust to the presence of locally outlier sources. This new method is applied to a wide range of problems including multimodal structuring of image collections and subspace-based clustering based on random projections. The third contribution of the thesis is an attempt to extend SNN methods to the context of bipartite k-nn graphs. We introduce new SNN relevance measures revisited for this asymmetric context and show that they can be used to select locally optimal bi-partite clusters. Accordingly, we propose a new bipartite SNN clustering algorithm that is applied to visual object's discovery based on a randomly precomputed matching graph. Experiments show that this new method outperformed state-of-the-art object mining results on OxfordBuilding dataset. Based on the discovered objects, we also introduce a new visual search paradigm, i.e. object-based visual query suggestion.
Over the past few years, the Big Data concept has been widely developed. In order to analyse and explore all this data, it was necessary to develop new methods and technologies. Today, Big Data also exists in the health sector. Hospitals in particular are involved in data production through the adoption of electronic health records. The objective of this thesis was to develop statistical methods reusing these data in order to participate in syndromic surveillance and to provide decision-making support. This study has 4 major axes. First, we showed that hospital Big Data were highly correlated with signals from traditional surveillance networks. Secondly, we showed that hospital data allowed to obtain more accurate estimates in real time than web data, and SVM and Elastic Net models had similar performances. Then, we applied methods developed in United States reusing hospital data, web data (Google and Twitter) and climatic data to predict influenza incidence rates for all French regions up to 2 weeks. Finally, methods developed were applied to the 3-week forecast for cases of gastroenteritis at the national, regional and hospital levels.
We introduce the task of multiclass classification and the challenge of classifying with a large number of classes. In this domain, we introduce an asynchronous framework for performing distributed optimization. We present application of the proposed asynchronous framework on two popular domains: matrix factorization for large-scale recommender systems and large-scale binary classification. Whereas, in the case of large-scale binary classification we use a variant of SGD which uses variance reduction technique, SVRG as our optimization algorithm.
My research takes part in the field of Berber linguistics and precisely in phraseology addressing lexical fixation including the study of frozen sequences. In fact this research presents a continum of my works of the two years of master.In the first place, my research of Master 1 entitled "thematic study of Berber idiomatic expressions related to the human body, variety of Ayt Ḥmad Uɛisa, Middle Atlas", and of Master 2 devoted to the study of the semantico-syntactic typology of idiomatic expressions related to the human body, and in the second place, of other works carried out in Berber studies in general. It focuses on three different aspects of idiomatic expressions by conducting a thematic and semantico-syntactic study, and stressing the syntactic and semantic typology - study of the syntactic and semantic behaviors of each syntagm in the expression, the processes of construction of meaning - as well as on the relation between the semantism of the constituents and their syntactic fixation. The analysis will focus on an oral corpus that il will collect using semi-direct interviews conducted with native speakers of the variety of Ayt Ḥmad Uɛisa. I will then analyze the units of the corpus by referring to the general theoretical framework of international phraseology as well as the works done in the field of Berber phraseology in particular. The importance of this project lies in the fact that it will provide linguistic elements explaining the syntactic and semantic functioning of Berber idiomatic expressions, and help to understand them. This will undoubtedly contribute to preserve these linguistic forms which constitute a part of Berber language through the collection of the corpus and the constitution of a database that can be used in other perspectives, such as lexicography, phraseology didactic, translation and Language Processing.
The objective is to develop machine learning and deep learning methods to extract automatically key features and information from health data, especially from diabetes patient data records in order to improve health monitoring, treatment and prediction about the pathology and the efficiency of the diagnosis and health plans efficiency. The developed methods for diabetes will be explored for applications to other data sets, domains and sectors. The thesis will focus on defining a methodology that can be tuned and reused for various use cases as opposed to developing specific solutions per domain. The objective is to define a common underlying framework from which one can derive domain specific solutions.
The present thesis proposes an automatic morphological analysis of the kanji sequences in Japanese texts. This analysis is based on the graphemic, morphological and syntactic characteristics of the Japanese language. The first part of the thesis describes the Japanese writing system and its encoding methods. The second part deals with the Japanese parts of speech, in particular verbs, adjectives, particles and flexional suffixes which morphosyntaxic characteristics are essential for the morphological analysis. The third part describes the module of analysis: identification and formalization of the data necessary to the analysis, algorithm of the analysis and the related treatments, formalization of models of objects necessary to the data-processing handling of Japanese.
This thesis presents a modelisation of the main syntactical aspects of coordination using Guy Perrier's Interaction Grammars as the target formalism. Interaction Grammars make it possible to explicitly define conjuncts'valencies. This is precisely what our modelisation is based upon. We also present work around this modelisation that enabled us to provide a realistic implementation: lexicalized grammar development (using our tool XMG), lexical disambiguation based on automata intersection and parsing.
We aim to obtain better computational efficiency than pure metrical mapping techniques, better accuracy as well as usability for robot guidance compared to the topological mapping. A crucial step of any mapping system is the loop closure detection which is the ability of knowing if the robot is revisiting a previously mapped area. Therefore, we first propose a hierarchical loop closure detection framework which also constructs the global topological structure of our hybrid map. Using this loop closure detection module, a hybrid mapping framework is proposed in two step. These maps are further augmented with metric information at those nodes which correspond to image sub-sequences acquired while the robot is revisiting the previously mapped area. The second step augments this model by using road semantics. A Conditional Random Field based classification on the metric reconstruction is used to semantically label the local robot path (road in our case) as straight, curved or junctions. Metric information of regions with curved roads and junctions is retained while that of other regions is discarded in the final map. Loop closure is performed only on junctions thereby increasing the efficiency and also accuracy of the map. By incorporating all of these new algorithms, the hybrid framework presented can perform as a robust, scalable SLAM approach, or act as a main part of a navigation tool which could be used on a mobile robot or an autonomous car in outdoor urban environments. Experimental results obtained on public datasets acquired in challenging urban environments are provided to demonstrate our approach.
This thesis, which is organized in two independent parts, presents work on distributional semantics and on variable selection. In the first part, we introduce a new method for learning good word representations using large quantities of unlabeled sentences. The method is based on a probabilistic model of sentence, using a hidden Markov model and a syntactic dependency tree. We then evaluate our models on intrinsic tasks such as predicting human similarity judgements or word categorization, and on two extrinsic tasks: named entity recognition and supersense tagging. In the second part, we introduce, in the context of linear models, a new penalty function to perform variable selection in the case of highly correlated predictors. This penalty, called the trace Lasso, uses the trace norm of the selected predictors, which is a convex surrogate of their rank, as the criterion of model complexity. In particular, it is equal to the ℓ 1-norm if all predictors are orthogonal and to the ℓ 2-norm if all predictors are equal. We propose two algorithms to compute the solution of least-squares regression regularized by the trace Lasso, and perform experiments on synthetic datasets to illustrate the behavior of the trace Lasso.
Classical computer architectures are optimized to process pre-formatted information in a deterministic way and therefore struggle to treat unorganized natural data (images, sounds, etc.). As these become more and more important, the brain inspires new, neuromorphic computer circuits such as neural networks. In this work, we concentrate on memristors based on ferroelectric tunnel junctions that are composed of an ultrathin ferroelectric film between two metallic electrodes. We show that the polarization reversal in BiFeO3 films can induce resistance contrasts as high as 10^4 and how mixed domain states are connected to intermediate resistance levels. Changing the electrode materials provides insights into their influence on the electrostatic barrier and dynamic properties of these memristors. Their analysis in combination with piezoresponse force microscopy finally allows us to establish a model describing the memristor dynamics under arbitrary voltage signals. After the demonstration of an important learning rule for neural networks, called spike-timing-dependent plasticity, we successfully predict new, previously unexplored learning curves. This constitutes an important step towards the realization of unsupervised self-learning hardware neural networks.
Geohistorical data extracted from various and heterogeneous sources are highly inaccurate, uncertain or inexact according to the existing terminology. In particular, we focus on Paris historical street networks and its evolution between the end of the XVIIIth and the end of the XIXth centuries. Our proposal is based on a merged structure of multiple representations of data capable of modelling spatial networks at different times, providing tools such as pattern detection in order to criticize, qualify and eventually correct data and sources without using ground truth data but the comparison of data with each other through the merging process.
Generally, a network reflects the interactions between many entities of a system. These interactions have different sources, a social link or a friendship link in a social network, a cable in a router network, a chemical reaction in a protein-protein interaction network, a hyperlink in a webpage network. Nowaday, network science is a research area in its own right focusing on describing and modeling these networks in order to reveal their main features and improve our understanding of their mecanisms. Most of the works in this area use graphs formalism which provides a set of mathematical tools well suited for analyzing the topology of these networks. It exists many applications, for instance applications in spread of epidemy or computer viruses, weakness of networks in case of a breakdown, attack resilience, study for link prediction, recommandation, etc. The large majority of real-world networks depicts several levels of organization in their structure. Because of there is a weak global density coupled with a strong local density, we observe that nodes are usually organized into groups, called communities, which are more internally connected than they are to the rest of the network.
Important information for public health is contained within Electronic Health Records (EHRs). The vast majority of clinical data available in these records takes the form of narratives written in natural language. Although free text is convenient to describe complex medical concepts, it is difficult to use for medical decision support, clinical research or statistical analysis. Among all the clinical aspects that are of interest in these records, the patient timeline is one of the most important. Being able to retrieve clinical timelines would allow for a better understanding of some clinical phenomena such as disease progression and longitudinal effects of medications. Accessing the clinical timeline is needed to evaluate the quality of the healthcare pathway by comparing it to clinical guidelines, and to highlight the steps of the pathway where specific care should be provided. In this thesis, we focus on building such timelines by addressing two related natural language processing topics which are temporal information extraction and clinical event coreference resolution. Our main contributions include a generic feature-based approach for temporal relation extraction that can be applied to documents written in English and in French. We devise a neural based approach for temporal information extraction which includes categorical features. We present a neural entity-based approach for coreference resolution in clinical narratives. We perform an empirical study to evaluate how categorical features and neural network components such as attention mechanisms and token character-level representations influence the performance of our coreference resolution approach.
Recognizing surrounding objects is an important skill for the autonomy of robots performing in daily-life. Nowadays robots are equipped with sophisticated sensors imitating the human sense of touch. In this thesis, we exploit haptic data to perform haptic recognition of daily life objects using machine learning techniques. The main challenge faced in our work is the difficulty of collecting a fair amount of haptic training data for all daily-life objects. This is due to the continuously growing number of objects and to the effort and time needed by the robot to physically interact with each object for data collection. We solve this problem by developing a haptic recognition framework capable of performing Zero-shot, One-shot and Multi-shot Learning. We also extend our framework by integrating vision to enhance the robot's recognition performance, whenever such sense is available.
The current research aims to provide a syntactic and semantic analysis of Modern Greek transitive non-locative constructions with one direct object: N0 V N1. Our study is based on the syntactic framework of the Transformational Grammar defined by Zellig S. Harris. Based on 16 560 morphological verbal entries, we proceeded to the classification of transitive non-locative constructions. On the basis of formal criteria we divided them into 24 distinct classes that formed an inventory of 2 934 transitive non-locative verbal uses with one direct object. Likewise, the passive transformation is largely blocked in the 32GNM table, while the 32GCV and 32GRA tables regroup verbs accepting a support verb transformation.
In order to hope to develop a political philosophy that immediately recognizes our interdependence, we work in a first part to establish assumptions about what we mean by reality and our access to it. An event-based ontology seems compatible with the narrative ontogenesis which constitutes us individually by constituting a "we". This requires imagining everyone imagining the world and learning through stories, in an inductive logic that can reconcile hermeneutic phenomenology on the one hand and statistical learning on the other. From these stories each identifies universals, interpretable as principal components of a factorial statistical analysis of these stories that constitute us. Time plays a key role in the dynamics of this constitution as well as in the events gathered in these stories. The stakes are ultimately to share these universals in a common story, or, conversely, in a temporal break that may allow better access to a common world. We then work in a second part on the question of living together with republican ideas of freedom, equality and fraternity, and with those of plurality and boundaries. The political ecology that we see then is as republican as libertarian. In this context, justice is expressed by rightness, fidelity, sensitivity and a “fair excess”. The categorical imperative lies in the need to make others beautiful, free, and powerful, and to learn together. Law appears to develop dynamically in the very time that the City is developed. The possibility of the radically “new” worked in the first part allows articulating freedom and institutions. The logic of a code of honor ultimately allows not to surrender to the Almighty Reason without giving up the Enlightenment.
The instrumentation and observation of learners activities by exploiting interaction traces in the EHL and the development of indicators can help tutors to monitor activities of learners and support them in their collaborative learning process. Indeed, in a learning situation, the teacher needs to observe the behavior of learners in order to build an idea about their involvement, preferences and learning styles so that he can adapt the proposed activities. This estimation is based on automatic analysis of students textual asynchronous conversations.
Bluecime has designed a camera-based system to monitor the boarding station of chairlifts in ski resorts, which aims at increasing the safety of all passengers. This already successful system does not use any machine learning component and requires an expensive configuration step. Machine learning is a subfield of artificial intelligence which deals with studying and designing algorithms that can learn and acquire knowledge from examples for a given task. Such a task could be classifying safe or unsafe situations on chairlifts from examples of images already labeled with these two categories, called the training examples. The machine learning algorithm learns a model able to predict one of these two categories on unseen cases. Since 2012, it has been shown that deep learning models are the best suited machine learning models to deal with image classification problems when many training data are available. In this context, this PhD thesis, funded by Bluecime, aims at improving both the cost and the effectiveness of Bluecime's current system using deep learning.
Febrile seizures (FS) affect 2% to 5% of children aged 6 months to 5 years of age. Although FS are usually benign, they are associated with serious treatable neurological emergencies. Nowadays, three factors are used to evaluate this risk: the age of the child, whether the FS is simple or complex and the features of the clinical exam. The objective of this thesis was to investigate the hypothesis that among children experiencing a FS, only those with an abnormal clinical exam are at risk of serious, treatable neurological emergencies. We first created an informatics tool in order to exhaustively search for cases among more than one million electronic medical records from seven pediatric emergency departments (PED) between 2007 and 2011. Then, we identified visits of children with a FS. Finally, we evaluated the proportion of serious, treatable neurological emergencies associated with these visits, and more specifically with visits of children with a normal clinical exam. We found no serious treatable neurological emergencies among children visiting the ED for a FS with a normal clinical exam, whatever the age and the features of the seizure were. The studies described in this thesis associated with the available data in the literature support our hypothesis and highlight the need of guidelines regarding the management of these children. Finally, this thesis gives us the opportunity to discuss some considerations on the use of electronic medical records for clinical research.
The concept of “Business Rule Management System” (BRMS) has been introduced in order to facilitate the design, the management and the execution of company-specific business policies. Based on a symbolic approach, the main idea behind these tools is to enable the business users to manage the business rule changes in the system without requiring programming skills. It is therefore a question of providing them with tools that enable to formulate their business policies in a near natural language form and automate their processing. Nowadays, with the expansion of intelligent systems, we have to cope with more and more complex decision logic and large volumes of data. It is not straightforward to identify the causes leading to a decision. There is a growing need to justify and optimize automated decisions in a short time frame, which motivates the integration of advanced explanatory component into its systems. Thus, the main challenge of this research is to provide an industrializable approach for explaining the decision-making processes of business rules applications and more broadly rule-based systems. This approach should be able to provide the necessary information for enabling a general understanding of the decision, to serve as a justification for internal and external entities as well as to enable the improvement of existing rule engines. To this end, the focus will be on the generation of the explanations in themselves as well as on the manner and the form in which they will be delivered.
In this thesis, we focus on the problem of link prediction in binary tensors of order three and four containing positive observations only. Tensors of this type appear in web recommender systems, in bio-informatics for the completion of protein interaction databases, or more generally for the completion of knowledge bases. We benchmark our completion methods on knowledge bases which represent a variety of relationnal data and scales. Our approach is parallel to that of matrix completion. We optimize a non-convex regularised empirical risk objective over low-rank tensors. Our method is empirically validated on several databases, performing better than the state of the art. These performances however can only be reached for ranks that would not scale to full modern knowledge bases such as Wikidata. We focus on the Tucker decomposition which is more expressive than the Canonical decomposition but also harder to optimize. With these method, we obtain improved performances on several benchmarks for limited parameters per entities. Finally, we study the case of temporal knowledge bases, in which the predicates are only valid over certain time intervals. We propose a low-rank formulation and regularizer adapted to the temporal structure of the problem and obtain better performances than the state of the art.
Systems are becoming more and more complex, because to stay competitive, companies whichdesign systems search to add more and more functionalities to them. Additionally, this competition impliesthat the design of systems needs to be reactive, so that the system is able to evolve during its conception andfollow the needs of the market. Additionally, natural language is hard to process automatically: for example, it is hard to determine, usingonly a computer program, that two natural language requirements contradict each other. However, naturallanguage is currently unavoidable in the specifications we studied, because it remains very practical, and itis the most common way to communicate. We aim to complete these natural language requirements with elements which allow to make them lessambiguous and facilitate automatic processing. These elements can be parts of models (architectural modelsfor example) and allow to define the vocabulary and the syntax of the requirements. We experimented theproposed principles on real industrial specifications and we developped a software prototype allowing totest a specification enhanced with these vocabulary and syntax elements.
The objective of this project is to develop tools to automatically analyze the recorded speech of patients with the Huntington's disease, a serious neurodegenerative genetic disease, to obtain a marker allowing their clinical follow-up. This project will focus on the joint modeling of cognitive and emotional deficits, using a variety of representations of speech signals. We will examine three levels of representations on the different protocols: Intelligibility and articulatory measurements / Acoustic and prosodic representations of the vocal signal / Linguistic representations. In particular, we will use unsupervised machine learning techniques to detect repeated patterns in the signal, as well as quantify the individual evolutionary trajectories of patients in the different dimensions of the disease. The diversity of skills tested in patients'speech will enable us to report on the heterogeneity of their symptoms observed in the clinic (cognitive, behavioral, emotional and motor disorders). This project will offer clinicians and researchers composite markers that faithfully reflect the evolution of the disease specific to each patient.
This research relates to the field of Spanish didactics and focuses on students who study in the St. Denis area near Paris and their social representations of and attitudes towards this language. The effectiveness of the blended learning program on language acquisition was measured through pre-tests and post-tests to evaluate the language skills in Learning Comprehension, Written Comprehension and Short Written Expression following the Cefrl, and secondly on an analysis of a sample of students' productions. Students uses and appropriation modes of the long distance learning platform are measured by a tracking through comments and semiannual validations and also attitude survey. Our results highlighted a positive evolution concerning language competence and a change of attitude for two thirds of the students and a progression in fluency and accuracy according to the starting levels and targeting B1-B2 levels towards Gies 1 and 2. While the majority of students surveyed perceived both the blended language learning program and their workgroup as a positive learning experience, one third, said that they had difficulties in taking on the imposed workload and would rather work autonomously. The coordinated investment of the different actors required by the learning program is one of the sharing solutions asked for by the L2 teachers in the blended learning program, without which initiatives in field computer based learning are likely to remain individual and isolated.
The construction show a lack of efficiency to compare to other industries. In answer, the world of architecture set up a new process called BIM (Building Information Modeling). This process is based on a3D virtual mock up containing every information needed for the construction. During the implementation of this process, difficulties of interaction has been noted by the BIM users. BIM models are hard to observe and manage, explained by the fact that these models contain a large amount of information. BIM process proposes the same scheme for all the construction profile. The purpose is to offer an adapted environment, considering the profile of each BIM user, while keeping the actual design method. Firstly, this document describes the creation of virtual reality rooms dedicated to the construction. Secondly, it deals with the development of algorithms allowing the classification of components from BIM model, an adaptive system of visualization and a process to handle the model. These development are based on the consideration of the profile of the user, the trade of the user.
This work presents an experimental evaluation of various voice transformation techniques based on GMM models. These linear transforms, despite their quality obtained, they fail to some defects specially the oversmoothing effect, the problem of spectral distortion and the overfitting. In a first part, we proposed taking these issues into account to adapt the learning strategy of the conversion functions. The first main idea is to reduce the number of parameters describing the conversion function. The second idea considers the solutions based on linear transform are unstable face to the lack of the training data, hence the recourse to non-linear transform model like RBF. In a second part in some situations, we need to align non-parallel data from the source and target speakers, one solution consists to use a recursive representation of binary tree, whose depth depends on the learning data size.
Mobile devices offer measuring capabilities using embedded or connected sensors. They are critical because the performed measurements must be reliable because possibly used in rigorous context. Despite a real demand, there are relatively few applications assisting users with their measuring processes that use those sensors. Such assistant should propose methods to visualise and to compute measuring procedures while using communication functions to handle connected sensors or to generate reports. Such rarity of applications arises because of the knowledges required to define correct measuring procedures. Those knowledges are brought by metrology and measurement theory and are rarely found in software development teams. These premises bring the research question the presented works answer: What approach enables the conception of applications suitable to specific measurement procedures considering that the measurement procedures could be configured by the final user. The presented works propose a platform for the development of measuring assistant applications. The platform ensure the conformity of measuring processes without involving metrology experts. A study of metrology enables to show the need of applications measuring process expert evaluation. The verification is performed by confronting measuring processes to the knowledge scheme in the form of requests. Those requests are described with a request language proposed by the scheme. Measuring assistant applications require to propose to the user a measuring process that sequences measuring activities. That editor uses a domain specific language dedicated to the description of measuring assistant applications. The language is built upon concepts, formalisms and tools proposed by the metamodeling environment: Diagrammatic Predicat Framework (DPF). Then, mobile platforms need to execute a behaviour conforming to the editor described one. This eases their computation and the use of sensors. The application is able to consider an implementation model and to build the corresponding agent network in order to propose a behaviour matching the end users needs.
The influence of English is evident on languages worldwide. English is considered a global language of communication and is used by a large number of speakers worldwide for their interactions. It is clear that English dominates many aspects of daily life, such as technology, science, the media and the Internet. All the influences observed on the languages of the world that are due to the influence of English fall under the notion of Anglicisation, that covers all levels of linguistic analysis. In my dissertation I study the influence of English on Modern Greek (MG), which has been particularly strong during the last two to three decades. I aim to examine the phenomenon of Anglicisation in MG taking into account the English influence at all levels of linguistic analysis, focusing particularly on the lexical, phraseological and morphosyntactic level. In order to analyze my data, I use dictionaries and grammars for MG, as well as MG text corpora, the Hellenic National Corpus, and the Corpus of Greek Texts, the text corpora available through the Sketch Engine platform, but also the customized text corpus that I built exclusively for my data through Sketch Engine. Moreover, I investigate the factors responsible for the appearance and use of non-transliterated forms of the loanwords by examining their appearance in specialized vocabularies of MG, such as the vocabulary of sports and technology. Regarding the phraseological patterns and morphosyntactic structures that calque the equivalent English ones, I compare the frequency of appearance of the calqued structure in MG to the frequency of appearance of the equivalent MG structure. Furthermore, I try to determine the chronology of the insertion of English loanwords in MG, and finally, I draw some general conclusions, regarding Anglicisation in MG, based on the results of the research.
The prevalent deployment and usage of sensors in a wide range of sectors generate an abundance of multivariate data which has proven to be instrumental for researches, businesses and policies. More specifically, multivariate data which integrates temporal evolution, i.e. Multivariate Time Series (MTS), has received significant interests in recent years, driven by high resolution monitoring applications (e.g. healthcare, mobility) and machine learning. However, the explanations from the surrogate models cannot be perfectly faithful with respect to the original model, which is a prerequisite for numerous applications. Faithfulness is critical as it corresponds to the level of trust an end-user can have in the explanations of model predictions, i.e. the level of relatedness of the explanations to what the model actually computes. This thesis introduces new approaches to enhance both performance and explainability of MTS machine learning methods, and derive insights from the new methods about two real-world applications.
Today, learning computer assisted language is increasingly widespread in public and private institutions. However, it is still far from expectations teachers and learners, and still does not meet their needs. Computer-assisted language learning (CALL) today are rather test environments of learner knowledge and more like a support traditional learning. In addition, the feedback provided by these systems remains basic and can not be adapted for independent learning, because it should be able to diagnose problems a learner with spelling, grammar, conjugation, etc., and intelligently generate adequate feedback according to the situation of learning. This research exposes the capabilities of NLP tools to provide solutions to limitations CALL systems in order to develop a comprehensive system and CALL autonomous. We present a complete architecture of a multilingual system learning the computer assisted language for language learners Foreign, French and Arabic. This system could be used for learning languages by learners of the language as a second or foreign language. The first part of our work focuses on the adaptation of tools and resources from NLP for them to be used in a language learning environment computer assisted. These tools and resources, there are stemmers for Arabic and French corpora, electronic dictionaries, etc. Then, in the second section presents the handwriting recognition online. In this optical, we present a statistical approach based on neural network, then we present the design of the architecture of the recognition system as well the implementation of the recognition algorithm. The second part of the presentation focuses on the development, integration and exploitation of NLP tools (morphological analyzers recognition system writing, dictionaries, etc.) in our learning system assisted language computer. We also present the modules added to the platform to have a the complete architecture of a CALL system. These modules, figure generator feedback that corrects the mistakes of learners and generate a relevant educational feedback which allows the learner to identify and faults. Finally, we describe the tool automatic generation and automated various educational activities.
The second chapter (co-authored with Xavier Lambin) studies the impact of online reputation on ethnic discrimination. The third chapter (co-authored with Rossi Abi Rafeh) develops and estimates a model of industry dynamics. Faced with multiple entrants, the incumbent exploits these externalities by offering licensing deals to some entrants or by pursuing litigation in order to decrease the cost of delaying contracts offered to others. The number of delayed entrants increases with patent strength. The second chapter shows that reputation systems can mitigate ethnic discrimination by enabling ethnic minority sellers to accrue a high reputation quickly, leading buyers to update their beliefs. Using data from a ridesharing platform, we find that minority drivers with no reviews make 12% less revenue relative to similar nonminority drivers. To understand the mechanism behind this process, we construct a model of career concerns' of discriminated sellers in the presence of a reputation system. The model's estimates show that minority drivers, who just entered the platform, face overly pessimistic beliefs about the quality of their service. To alter these beliefs, they exert high effort and offer low introductory prices, swiftly boosting their reputation. Counterfactual simulations reveal that the cost of incorrect prior beliefs is high and that the reputation system strictly benefits minority drivers. The final chapter studies the entry and pricing decisions of sellers in a market with a reputation system. We provide a model of dynamic oligopoly with heterogeneity in marginal and opportunity costs and individual reputation as a state variable. We show that new sellers are generally less likely to reenter the platform than incumbents and sellers who have a lower chance of entering in subsequent periods set on average higher prices. The mechanism behind these findings is selection on marginal costs. We apply our model to a dataset on sellers on a large ridesharing marketplace. We showcase a negative correlation of tenure on the platform, measured by the number of reviews, and prices set by drivers. However, after accounting for drivers' unobserved characteristics, which we interpret as marginal costs, we find a positive relationship. We provide, further, evidence of selection on unobservable by studying reentry decisions. Finally, we calibrate our dynamic model to uncover the distribution of opportunity costs.
The main objective of the project is to investigate novel unsupervised or weakly supervised machine learning techniques for the automatic structuring of spoken conversations, given its audio recording and (manual or automatic) speech transcription. First, one will focus on named speaker diarization. Without any prior knowledge on the speakers involved in the conversation, this task can be divided into two subtasks: gathering the names of all speakers (thanks to named entity recognition and entity linking, for instance) followed by the attribution of each speech turns to the corresponding speaker (using natural language understanding and speaker recognition, for instance). Then, one will address the problem of classifying conversations according to the content of the exchanges (are speakers arguing, fighting, small-talking?). To that end, one could enrich natural language processing techniques with acoustic cues (prosody, rythm, etc.). The proposed approaches will be applied to movies or TV series, for which a significant amount of annotations are already available.
The aim of this thesis is to develop a tool that brings together all the research launched at IDHN within the framework of the analysis through platforms, in order to combine them into a single interface and to make them work together, with a view to analyzing the phenomenon of influence. Our choice is viky.ai, an open source platform developed by the company Pertimm to create and share linguistic agents, which analyze texts in all languages. Creating modifiable, reusable analytics agents that can be shared collaboratively by the community, can greatly simplify the handling of textual analysis on problems that are similarly found in many areas. viky.ai provides a unique codeless user interface. This is a new way to create semantic modules that are easy to integrate into any system through API. The semantic bricks, named viky.ai's agents, are multilingual assistants to find relevant data. These bricks consist of entities and interpretations and can be combined endlessly to build models in the areas of semantic analysis. Many NLP experts agree that NLP technologies combining rules and learning will be the best combination. More specifically, this thesis aims to develop agents in two research fiels already outlined: detection, characterization of ideologies and links to emotional speech; detection of topic changes in political discourses.
This thesis in computer science take place in the ILE domain (Interactive Learning Environment) and was realized within the AGATE project (an Approach for Genericity in Assistance To complEx tasks) that aims at proposing generic models and unified tools to make possible the setup of assistance systems in various existing applications. In this project, an assistance editor allows assistance designers to define assistance systems and a generic assistance engine executes these assistance systems on the various target-applications without disturbing them to help final users. In the educational context, teachers can want to set up assistance system to complete the pedagogical or non-pedagogical software used by learners. Pedagogical engineers therefore have the role of assistance designers and learners are end-users of such assisted applications. In order to answer this research question, we realized this thesis in two steps: first, the study of existing assistances within applications used in the educational context, then the exploitation and enrichment of models and tools of the AGATE project to adapt them to the educational context. In the first step, we studied the applications used by teachers in their courses as well as existing works proposing assistance system. In the second step, we confronted the models and tools proposed previously in the AGATE project to the characteristics of the assistance identified in educational context. The limitations of the previous models and tools led us to propose two contributions to the aLDEAS language and the SEPIA system in order to adapt them to the educational context. Whether in an educational context or not, it is important to be able to define easily and explicitly several modes of articulation between the different elements of an assistance system. We therefore proposed a model of articulation between aLDEAS rules in five modes: successive, interactive, simultaneous, progressive, independent. This model and this process have been implemented in SEPIA-edu. Then, a pedagogical guidance model allows to define different types of pedagogical guidance (free, sequential, contextualized, temporal, personalized). This model of activity, this pedagogical guidance pattern and this process have been implemented in SEPIA-edu
Context and goal:It is almost a truism to affirm that one of the main features of speech is its variability: variability inter-gender, inter-speaker, but also variability from one context to another, or from one repetition to another for a given subject. Variability underlies at the same time the beauty of speech, the complexity of its treatment by speech technologies, and the difficulty for understanding its mechanism. In this thesis we study certain aspects of speech variability, our starting point being the variability characterizing the repetitions of a given utterance by a given subject, in a given condition, which we call intrinsic variability. Models of speech motor control have mainly focused on the contextual aspects of speech variability, and have rarely considered its intrinsic component, even though it is this fundamental component of variability that gives speech it naturalness. The main goal of this thesis is to address the contextual and intrinsic component of speech variability in an integrative computational framework. To this aim, we postulate that the main component of the intrinsic variability of speech is not just execution noise, but that it results from a control strategy where intrinsic variability characterizes the abundance of possible productions of the intended speech item. Methodology: We formalize this idea in a probabilistic computational framework, Bayesian modeling, where the abundance of possible realizations of a given speech item is naturally represented as uncertainty, and where variability is thus formally manipulated. We illustrate the pertinence of this approach with three main contributions. Results: Firstly, we reformulate in Bayesian terms an existing model of speech motor control, the GEPPETO model, and demonstrate that this Bayesian reformulation, which we call B-GEPPETO, contains GEPPETO as a particular case. The somatosensory characterization of speech motor goals involved a certain number of hypotheses that we intended to evaluate with two experimental studies. This original analysis is made possible thanks to the unified representation of knowledge in the model, which enables to account for production and perception processes in a single computational framework. Taken together, these contributions illustrate how the Bayesian framework offers a structured and systematic approach for the construction of models in cognitive sciences. The framework facilitates the development of models and their progressive complexification by specifying and clarifying underlying assumptions.
The thesis project is at the junction of computational linguistics and field linguistics. In a context where linguistic diversity is threatened by the disappearance of almost half of the languages spoken on earth today, it is becoming crucial to provide field linguistics specialists with automatic or semi-automatic tools to collect linguistic data, annotate, enrich and archive them, and thereby try to preserve part of the cultural heritage of humanity. These issues are of growing interest within the automatic language processing community, in the context of sustained collaboration with teams of linguists. This thesis will thus take place within the framework of an international collaboration involving teams of linguists in France and Germany, with the support of the French National Research Agency. From a methodological point of view, the first task can be approached with statistical modelling tools, such as non-parametric Bayesian models, whose applicability will be studied here in a context where the data to be annotated are small, but where there are additional resources that can potentially be mobilised (lexicons, elements of morphological description, etc.). The development of these various types of statistical models will be validated in the languages of the project through experiments involving a close collaboration with users of these tools.
Global competitiveness has challenged manufacturing industry to rationalise different ways of bringing to the market new products in a short lead-time with competitive prices while ensuring higher quality levels. Modern PDP has required simultaneously collaborations of multiple groups, producing and exchanging information from multi-perspectives within and across institutional boundaries. However, it has been identified semantic interoperability issues in view of the information heterogeneity from multiple perspectives and their relationships across product development. This research proposes a conceptual framework of an Interoperable Product Design and Manufacturing based on a set of core ontological foundations and semantic mapping approaches. An experimental system has been performed using the Protégé tool to model the core ontologies and the Java platform integrated with the Jena to develop the interface with the user. The conceptual framework proposed in this research has been tested through experiments using rotational plastic products. Therefore, this research has shown that information rigorously-defined and their well-defined relationships can ensure the effectiveness of product design and manufacturing in a modern and collaborative PDP
This thesis includes three experiments and two pre-tests (N = 1135) in which three fundamental aspects of static message design on the Internet are studied: its format (infographics, audio or text), its colour and typography, on the theme of electronic waste recycling (studies 1 and 2) and then on human migration (study 3).The study of graphic aspects is relevant if we want to increase the persuasive power of a message. The format plays a major role (study 1a), making it possible to change attitudes and to anchor this change over time. Colours, on the other hand, do not seem to vary the persuasive force of the message or to lead readers to act in favour of recycling (study 1b). Nor does typography seem to play a role in the persuasive dynamic, whether it is considered legible or difficult to read (study 2). Theoretical approaches regarding the personality of typographies and their coherence with the context are developed. The analysis of the components of the ELM revealed, in each study, the strong link between the attitude of individuals and their sense of personal responsibility towards the theme addressed as well as their a priori knowledge. We have also seen that the levers of persuasion are not systematically the same according to the need for cognition. We suggest that persuasive messages should adopt a format that allows for central analysis at low cognitive cost, using a main colour and typography that are both readable and consistent with the theme developed, with arguments that reinforce readers'sense of responsibility.
Development of collaborative environment is a complex process. The complexity lies in the fact that collaborative environment development involves a lot of decision making. Several tradeoffs need to be made to satisfy current and future requirements from a potentially various set of user profiles. The handling of these complexities poses challenges for researcher, developers and companies. The knowledge required to make suitable design decisions and to rigorously evaluate those design decisions is usually broad, complex, and evolving. The results generate three models: SyCoW (synchronous collaborative work), SyCoE (synchronous collaborative environment) and SyCoEE (synchronous collaborative environment evaluation). In Part-II of this thesis we proposed a process for selection/development of collaborative environment, where we demonstrate how SyCoW, SyCoE and SyCoEE support this process in different ways. The results of evaluation confirmed the usability of MT-DT and provide arguments for our choices which we made during development of MT-DT.
Transfer learning with deep convolutional neural networks significantly reduces the computation and data overhead of the training process and boosts the performance on the target task, compared to training from scratch. However, transfer learning with a deep network may cause the model to forget the knowledge acquired when learning the source task, leading to the so-called catastrophic forgetting. Since the efficiency of transfer learning derives from the knowledge acquired on the source task, this knowledge should be preserved during transfer. This thesis solves this problem of forgetting by proposing two regularization schemes that preserve the knowledge during transfer. First we investigate several forms of parameter regularization, all of which explicitly promote the similarity of the final solution with the initial model, based on the L1, L2, and Group-Lasso penalties. We also propose the variants that use Fisher information as a metric for measuring the importance of parameters. The second regularization scheme is based on the theory of optimal transport, which enables to estimate the dissimilarity between two distributions. We benefit from optimal transport to penalize the deviations of high-level representations between the source and target task, with the same objective of preserving knowledge during transfer learning. With a mild increase in computation time during training, this novel regularization approach improves the performance of the target tasks, and yields higher accuracy on image classification tasks compared to parameter regularization approaches.
Nowadays, multiple actors of Internet technology are producing very large amounts of data. Sensors, social media or e-commerce, all generate real-time extending information based on the 3 Vs of Gartner: Volume, Velocity and Variety. The primary goal of this study is to establish, based on these approaches, an integrative vision of data life cycle set on 3 steps, (1) data synthesis by selecting key-values of micro-data acquired by different data source operators, (2) data fusion by sorting and duplicating the selected key-values based on a de-normalization aspect in order to get a faster processing of data and (3) the data transformation into a specific format of map of maps of maps, via Hadoop in the standard MapReduce process, in order to define the related graph in applicative layer. In addition, this study is supported by a software prototype using the already described modeling tools, as a toolbox compared to an automatic programming software and allowing to create a customized processing chain of BigData
With the introduction of clinical data warehouses, more and more health data are available for research purposes. While a significant part of these data exist in structured form, much of the information contained in electronic health records is available in free text form that can be used for many tasks. In this manuscript, two tasks are explored: the multi-label classification of clinical texts and the detection of negation and uncertainty. The first is studied in cooperation with the Rennes University Hospital, owner of the clinical texts that we use, while, for the second, we use publicly available biomedical texts that we annotate and release free of charge. In order to solve these tasks, we propose several approaches based mainly on deep learning algorithms, used in supervised and unsupervised learning situations.
Our research focuses on the recommendation of new job offers that have just been posted and have no interaction history (cold start). To this objective, we adapt well-knowns recommendations systems in the field of e-commerce by exploiting the record of use of all job seekers on previous offers. One of the specificities of the work presented is to have considered real data, and to have tackled the challenges of heterogeneity and noise of textual documents. The presented contribution integrates the information of the collaborative data to learn a new representation of text documents, which is required to make the so-called cold start recommendation of a new offer. The new representation essentially aims to build a good metric. The search space considered is that of neural networks. Neural networks are trained by defining two loss functions. The first seeks to preserve the local structure of collaborative information, drawing on non-linear dimension reduction approaches. The second is inspired by Siamese networks to reproduce the similarities from the collaborative matrix. The scaling up of the approach and its performance are based on the sampling of pairs of offers considered similar. The interest of the proposed approach is demonstrated empirically on the real and proprietary data as well as on the CiteULike public benchmark. Finally, the interest of the approach followed is attested by our participation in a good rank in the international challenge RecSys 2017 (15/100, with millions of users and millions of offers).
Over the past few years, neural network (NN) architectures have been successfully applied to many Natural Language Processing (NLP) applications, such as Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT). For the language modeling task, these models consider linguistic units (i.e words and phrases) through their projections into a continuous (multi-dimensional) space, and the estimated distribution is a function of these projections. Also qualified continuous-space models (CSMs), their peculiarity hence lies in this exploitation of a continuous representation that can be seen as an attempt to address the sparsity issue of the conventional discrete models. In the context of SMT, these echniques have been applied on neural network-based language models (NNLMs) included in SMT systems, and oncontinuous-space translation models (CSTMs). These models have led to significant and consistent gains in the SMT performance, but are also considered as very expensive in training and inference, especially for systems involving large vocabularies. To overcome this issue, Structured Output Layer (SOUL) and Noise Contrastive Estimation (NCE) have been proposed; the former modifies the standard structure on vocabulary words, while the latter approximates the maximum-likelihood estimation (MLE) by a sampling method. All these approaches share the same estimation criterion which is the MLE ; however using this procedure results in an inconsistency between the objective function defined for parameter estimation and the way models are used in the SMT application. The work presented in this dissertation aims to design new performance-oriented and global training procedures for CSMs to overcome these issues. The main contributions lie in the investigation and evaluation of efficient training methods for (large-vocabulary) CSMs which aim:(a) to reduce the total training cost, and (b) to improve the efficiency of these models when used within the SMT application. On the one hand, the training and inference cost can be reduced (using the SOUL structure or the NCE algorithm), or by reducing the number of iterations via a faster convergence. This thesis provides an empirical analysis of these solutions on different large-scale SMT tasks. On the other hand, we propose a discriminative training framework which optimizes the performance of the whole system containing the CSM as a component model. The experimental results show that this framework is efficient to both train and adapt CSM within SMT systems, opening promising research perspectives.
The concept Web of things (WOT) is gradually becoming a reality as the result of development of network and hardware technologies. Nowadays, there is an increasing number of objects that can be used in predesigned applications. The world is thus more tightly connected, various objects can share their information as well as being triggered through a Web-like structure. However, even if the heterogeneous objects have the ability to be connected to the Web, they cannot be used in different applications unless there is a common model so that their heterogeneity can be described and understood. In this thesis, we want to provide a common model to describe those heterogeneous objects and use them to solve user's problems. Users can have various requests, either to find a particular object, or to fulfill some tasks. We highlight thus two research directions. Thus, we first study the existing technologies, applications and domains where the WOT can be applied. We compare the existing description models in this domain and find their insufficiency to be applied in the WOT...
This PhD thesis deals with supervized machine learning and statistics. Under assumptions, such as the Bernstein condition, such rates are attainable. A robust estimator is an estimator with good theoretical guarantees, under as few assumptions as possible. This question is getting more and more popular in the current era of big data. Large dataset are very likely to be corrupted and one would like to build reliable estimators in such a setting. We show that the well-known regularized empirical risk minimizer (RERM) with Lipschitz-loss function is robust with respect to heavy-tailed noise and outliers in the label. When the class of predictor is heavy-tailed, RERM is not reliable. In this setting, we show that minmax Median of Means estimators can be a solution. Surprisingly, in large dimension, interpolating the data does not necessarily implies over-fitting. We study a high dimensional Gaussian linear model and show that sometimes the over-fitting may be benign.
Chemical flavor analysis provides a list of the odorants contained in a food product but is not sufficient to predict the odor resulting from their mixture. Indeed, odor perception relies on the processing by the olfactory system of many odorants embedded in complex mixtures and several perceptual interactions can occur. Thus, the prediction of the perceptual outcome of a complex odor mixture remains challenging and two main approaches emerge from the literature review. On the other hand, methodologies relying on recombination strategies after the chemical analyses of flavor have been successfully applied to identify those odorants that are key to the food odor. However, the choices of odorants to be recombined are mostly based on empirical approaches. Thus, two questions arise: How can we predict the odor quality of a mixture on the basis of the molecular structure of its odorants? How can we improve food flavor analysis in order to predict the odor of a food containing several tens of odorants? These two questions are at the basis of the thesis and of this manuscript which is divided in two main axes. The first axis describes the development of a model based on the concept of angle distances computed from the molecular structure of odorants in order to predict the odor similarity between mixtures. The results highlight the importance of taking into account the odor intensity dimension to reach a good prediction level. Moreover, several perspectives are proposed to extend the model prediction beyond the similarity dimension and to predict more qualitative dimensions of odors. The second axis presents an innovative strategy which allows integrating experts' knowledge in the flavor analysis procedure. Three different types of heterogeneous data are embedded in a mathematical model: chemical data, sensory data and knowledge from expert flavorists. Experts' knowledge is integrated owing to the development of an ontology, which is further used to define fuzzy rules optimized by evolutionary algorithms. The final output of the model is the prediction of red wines' odor profile on the basis of their odorants' composition. Overall, the thesis work brings original results allowing a better understanding of food odor construction and gives insights on the underlying relationships within the odor perceptual space for complex mixtures.
The crowd counting task is an important research problem. Now more and more people are concerned about safety issues. Considering the scenario of a crowded scene: a population density system analyzes the crowds and triggers a warning to divert the crowds when their population density exceeds a normal range. With such a system, the incident of the Shanghai New Year's stampede will not happen again. The most critical aspect, in some public places, is that we can not install an intelligent video surveillance system. So how do we estimate the high-density crowd area to avoid crowd trampling accidents? Facing these challenges, we propose implementation of real time reconfigurable embedded architecture for people counting in a crowd area. First, our work integrates the features of HOG and LBP, which not only combines the effective identification information of multiple features, but also eliminates most of the redundant information, thereby realizing effective compression of information, saving information storage space. Then, in terms of crowd counting, we use multiple sources of information, namely HOG, LBP and CANNY based filtering. These sources provide separate estimates of the number of counts and other statistical measures, through the support vector Machine SVM, classification. At the same time, in order to effectively solve the problem of extracting scale-related features in crowd counting. We propose a new framework M-MCNN based on MCNN for crowd counting on any single image. M-MCNN not only contains the original three columns of convolutional neural networks with different filter sizes, but replaces the fully connected layers with a convolutional layer of 1*1 filters, so the input image of the model can be of any size. Moreover, in a single individual sample, we greatly improve the learning of sample features by extracting the texture features of a single human head, and better use it for datasets. Finally, we implement our new framework M-MCNN through FPGA, and transplant it on the drone to estimate and predict the high-density crowd area in real time. Our model achieved good results in crowd counting.
This work is part of the "TEL Environments customization" project studying the use of activity traces in the evaluation and customization of mediated learning situations. Analyzing traces is generally performed using analysis tools developed by researchers specifically for their needs. Published research results can generally not be verified or compared, due to difficulties of sharing corpora and analysis tools. The aim of this research work is to provide researchers using TEL environments in their researches with a platform to share corpora of contextualized interaction traces and analyses performed on them, and to analyse those corpora using shared analysis and visualization tools. Heterogeneity of traces produced by TEL environments, due to the diversity of learning domains and to analysis needs makes the proposition of a common representation cannot satisfy the various needs of multidisciplinary researchers. We propose the “proxy” approach, a participative and incremental solution based on an ontology which defines three models: a corpus model defining the structure and description metadata of the corpus and its contents, a semantic model defining generic concepts which can be retrieved in shared corpora, and an operational model defining a set of operations ensuring interoperability between shared corpora and analysis tools. Based on this approach, we propose a platform architecture for sharing traces corpora and analysis tools allowing researchers to share their own corpora, to access to shared corpora, and to analyze them.
Since the early days of the DARPA challenge, the design of self-driving cars is catching increasing interest. This interest is growing even more with the recent successes of Machine Learning algorithms in perception tasks. While the accuracy of these algorithms is irreplaceable, it is very challenging to harness their potential. Real time constraints as well as reliability issues heighten the burden of designing efficient platforms. We discuss the different implementations and optimization techniques in this work. We tackle the problem of these accelerators from two perspectives: performance and reliability. We propose two acceleration techniques that optimize time and resource usage. On reliability, we study the resilience of Machine Learning algorithms. We propose a tool that gives insights whether these algorithms are reliable enough for safety critical systems or not. The Resistive Associative Processor accelerator achieves high performance due to its in-memory design which remedies the memory bottleneck present in most Machine Learning algorithms. As for the constant multiplication approach, we opened the door for a new category of optimizations by designing instance specific accelerators. The obtained results outperforms the most recent techniques in terms of execution time and resource usage. Combined with the reliability study we conducted, safety-critical systems can profit from these accelerators without compromising its security.
In recent years, deep learning has lead to groundbreaking developments in the image and natural language processing fields. However, in many domains, input data consists in neither images nor text documents, but in time series that describe the temporal evolution of observed or computed quantities. In this thesis, we study and introduce different representations for time series, based on deep learning models. Firstly, in the autonomous driving domain, we show that, the analysis of a temporal window by a neural network can lead to better vehicle control results than classical approaches that do not use neural networks, especially in highly-coupled situations. Thirdly, in the human pose motion generation domain, we introduce 2D convolutional generative adversarial neural networks where the spatial and temporal dimensions are convolved in a joint manner. Finally, we introduce an embedding where spatial representations of human poses are sorted in a latent space based on their temporal relationships.
Once devices are connected to the local network, applications deployed for example on a smart phone, a PC or a home gateway, discover the plug-n-play devices and act as control points. The aim of such applications is to orchestrate the interactions between the devices such as lights, TVs and printers, and their corresponding hosted services to accomplish a specific human daily task like printing a document or dimming a light. Even similar devices supporting the same services represent their capabilities in a different representation format and content. Additionally, the deployed application must use multiple protocols stacks to interact with the device. To accomplish interoperability between plug-n-play devices and applications, we propose a generic approach which consists in automatically generating proxies based on an ontology alignment. The alignment contains the correspondences between two equivalent devices descriptions. Consequently, the UPnP printing application will interact transparently with the generated proxy which adapts and transfers the invocations to the real DPWS printer.
Despite the typological differences between French and Turkish, we make an assumption that comparison between these two languages can be made in terms of meaning and meaning construction processes. This approach, based on the establishment of a repertoire of meanings, could be possibly extended to other languages that do not have such kinds of tools.
Computer vision is a field that includes methods for the acquisition, processing, analysis and understanding of images to produce numerical or symbolic information. A research contributing to the development of this area is to replicate the capabilities of human vision to perceive and understand images. Our thesis is part of this research axis. We propose several original contributions belonging to the context of graphics recognition and spotting context. The originality of the proposed approaches is the proposal of an interesting alliance between the Formal Concept Analysis and the Computer Vision fields. We face the study of the FCA field and more precisely the adaptation of the structure of concept lattice and its use as the main tool of our work. The main feature of our work lies in its generic aspect because the proposed methods can be combined with various other tools keeping the same strategies and following a similar procedure. It is a concise, accurate and flexible search space thus facilitating decision making. Our contributions are recorded as part of the recognition and localization of symbols in graphic documents. We propose to recognize and spot symbols in graphical documents (technical drawings for example) using the alliance between the bag of words representation and the Galois lattice formalism. We opt for various methods belonging to the computer vision field
Resources like terminologies or ontologies are used in a number of applications, including documentary description and information retrieval. Different methodologies have been proposed to build such resources, which range from experts'interviews to the use of textual corpora. This thesis focuses on the use of natural language processing methodologies, meant to help the building of ontologies from textual corpora, to build the particular type of resource, namely differential ontologies.
Extracting reliable source catalogs from images is crucial for a broad range of astronomical research topics. However, the efficiency of current source detection methods becomes severely limited in crowded fields, or when images are contaminated by optical, electronic and environmental defects. Performance in terms of reliability and completeness is now often insufficient with regard to the scientific requirements of large imaging surveys. In this thesis, we develop new methods to produce more robust and reliable source catalogs. We leverage recent advances in deep supervised learning to design generic and reliable models based on convolutional neural networks (CNNs).We present MaxiMask and MaxiTrack, two convolutional neural networks that we trained to automatically identify 13 different types of image defects in astronomical exposures. We also introduce a prototype of a multi-scale CNN-based source detector robust to image defects, which we show to significantly outperform existing algorithms. We discuss the current limitations and potential improvements of our approach in the scope of forthcoming large scale surveys such as Euclid.
Associated with preconceived images and full of prejudices, romance is a consumption product which embedies a social and cultural mirroring. In France for several decades, romance has gained an heterogeneous but loyal readership/audience. What are the motives of those novels which are said to be all similar? What do they represent for the readership? How are they perceived by those who read them? How are they read, apprehended and interpreted? These are some of the questions we try to give an answer to in this multidisciplinary work. This research enabled to unveil the mecanisms involved in the evolution of romance since World War II, and to observe the different modes of reading and reception of these novels
Since twenty years ago, access and use of medical data become major issues for health professional and lay people. In this context, multiphe health terminologies were be developped. Theses terminologies have mostly different format and purpose: SNOMED International for clinical coding, CCAM and ISD10 used for epidemiological and medico-economic purposes, MeSH thesaurus for bibliographic databases. Hence, and given the growing need for cooperation between health practitioners, a new "intreoprable" terminoloy base is now required. We use also UMLS to provide a large coverage of relations between terminologies. We benefit in the case of this PhD of an extensive experience in the Natural Language Processing field from a multiple CISMEF and LERTIM research projects.
Social sciences and Humanities research is often based on large textual corpora, that it would be unfeasible to read in detail. Natural Language Processing (NLP) can identify important concepts and actors mentioned in a corpus, as well as the relations between them. Such information can provide an overview of the corpus useful for domain-experts, and help identify corpus areas relevant for a given research question. To automatically annotate corpora relevant for Digital Humanities (DH), the NLP technologies we applied are, first, Entity Linking, to identify corpus actors and concepts. Second, the relations between actors and concepts were determined based on an NLP pipeline which provides semantic role labeling and syntactic dependencies among other information. Part I outlines the state of the art, paying attention to how the technologies have been applied in DH. Generic NLP tools were used. As the efficacy of NLP methods depends on the corpus, some technological development was undertaken, described in Part II, in order to better adapt to the corpora in our case studies. Part II also shows an intrinsic evaluation of the technology developed, with satisfactory results. The technologies were applied to three very different corpora, as described in Part III. Second, the PoliInformatics corpus, with heterogeneous materials about the American financial crisis of 2007-2008. Finally, the Earth Negotiations Bulletin (ENB), which covers international climate summits since 1995, where treaties like the Kyoto Protocol or the Paris Agreements get negotiated. For each corpus, navigation interfaces were developed. These user interfaces (UI) combine networks, full-text search and structured search based on NLP annotations. As an example, in the ENB corpus interface, which covers climate policy negotiations, searches can be performed based on relational information identified in the corpus: the negotiation actors having discussed a given issue using verbs indicating support or opposition can be searched, as well as all statements where a given actor has expressed support or opposition. Relation information is employed, beyond simple co-occurrence between corpus terms. The UIs were evaluated qualitatively with domain-experts, to assess their potential usefulness for research in the experts'domains. First, we payed attention to whether the corpus representations we created correspond to experts'knowledge of the corpus, as an indication of the sanity of the outputs we produced. Second, we tried to determine whether experts could gain new insight on the corpus by using the applications, e.g. if they found evidence unknown to them or new research ideas. Examples of insight gain were attested with the ENB interface; this constitutes a good validation of the work carried out in the thesis. Overall, the applications'strengths and weaknesses were pointed out, outlining possible improvements as future work.
This thesis describes the creation of the French FrameNet (FFN), a French language FrameNet type resource made using both the Berkeley FrameNet (Baker et al., 1998) and two morphosyntactic treebanks: the French Treebank (Abeillé et al., 2003) and the Sequoia Treebank (Candito et Seddah, 2012). The Berkeley FrameNet allows for semantic annotation of prototypical situations and their participants. It consists of: a) a structured set of prototypical situations, called frames. These frames incorporate semantic characterizations of the situations'participants (Frame Elements, or FEs); b) a lexicon of lexical units (LUs) which can evoke those frames; c) a set of English language frame annotations. In order to create the FFN, we designed a “domain by domain” methodology: we defined four “domains”, each centered on a specific notion (cause, verbal communication, cognitive stance, or commercial transaction). We then sought to obtain full frame and lexical coverage for these domains, and annotated the first 100 corpus occurrences of each LU in our domains. This strategy guarantees a greater consistency in terms of frame structuring than other approaches and is conducive to work on both intra-domain and inter-domains frame polysemy. Our annotating frames on continuous text without selecting particular LU occurrences preserves the natural distribution of lexical and syntactic characteristics of frame-evoking elements in our corpus. At the present time, the FFN includes 105 distinct frames and 873 distinct LUs, which combine into 1,109 LU-frame pairs (i.e. 1,109 senses). 16,167 frame occurrences, as well as their FEs, have been annotated in our corpus. In this thesis, I first situate the FrameNet model in a larger theoretical background. I then justify our using the Berkeley FrameNet as our resource base and explain why we used a domain-by-domain methodology. I next try to clarify some specific BFN notions that we found too vague to be coherently used to make the FFN. Specifically, I introduce more directly syntactic criteria both for defining a frame's lexical perimeter and for differentiating core FEs from non-core ones. Then, I describe the FFN creation itself first by delimitating a structure of frames that will be used in the resource and by creating a lexicon for these frames. I then introduce in detail the Cognitive Stances notional domain, which includes frames having to do with a cognizer's degree of certainty about some particular content. Next, I describe our methodology for annotating a corpus with frames and FEs, and analyze our treatment of several specific linguistic phenomena that required additional consideration (such as object complement constructions). Finally, I give quantified information about the current status of the FFN and its evaluation. I conclude with some perspectives on improving and exploiting the FFN.
This thesis bends over the analysis and the constraints of elaboration of components for open domain questions answering systems. The questions answering systems (QAs) is adapted to this task, because they associate a factual question with a precise answer. It is suggested adapting the QAs to a task of interaction to define to them a frame of generic adaptation to the systems of dialogue. The existing techniques for the QAs in opened domain, the various models of man-machine dialog as well as systems in the limit between the dialogue and the search for information are studied. Then, is presented the construction of a model of structure allowing to describe the interactions between questions. Of the formal frame, is deduct a method of calculation which is estimated. To discuss this evaluation and see how using our model, is realized the analysis of search engines for the systems of questions answers. In a last part, is presented a use of the data calculated for the improvement of the results of answers to the questions.
The Abjad is essential to translations, or high quality information research. It was evident that the whole Abjad was not taken into account.
Laughter is a social vocalization universal across cultures and languages. It is ubiquitous in our dialogues and able to serve a wide range of functions. Laughter has been studied from several perspectives, but the classifications proposed are hard to integrate. Despite being crucial in our daily interaction, relatively little attention has been devoted to the study of laughter in conversation, attempting to model its sophisticated pragmatic use, neuro-correlates in perception and development in children. In the current thesis a new comprehensive framework for laughter analysis is proposed, crucially grounded in the assumption that laughter has propositional content, arguing for the need to distinguish different layers of analysis, similarly to the study of speech: form, positioning, semantics and pragmatics. Preliminary investigations are conducted on the viability of a laughter form-function mapping based on acoustic features and on the neuro-correlates involved in the perception of laughter serving different functions in natural dialogue. Our results give rise to novel generalizations about the placement, alignment, semantics and function of laughter, stressing the high pragmatic skills involved in its production and perception. The development of the semantic and pragmatic use of laughter is observed in a longitudinal corpus study of 4 American-English child-mother pairs from 12 to 36 months of age. Results show that laughter use undergoes important development at each level analysed, which complies with what could be hypothesised on the base of phylogenetic data, and that laughter can be an effective means to track cognitive/communicative development, and potential difficulties or delays at a very early stage.
We have done several studies to investigate the phenomenon between pairs of unknown people, good friends, and between people coming from the same family. We expect that the amplitude of convergence is proportional to the social distance between the two speakers. We found this result. Then, we have studied the knowledge of the linguistic target impact on adaptation. To characterize the phonetic convergence, we have developed two methods: the first one is based on a linear discriminant analysis between the MFCC coefficients of each speaker and the second one used speech recognition techniques. Finally, we characterized the phonetic convergence with a subjective measurement using a new perceptual test called speaker switching. The test was performed using signals coming from real interactions but also with synthetic data obtained with the harmonic plus
The actors of a company must collaborate efficiently to achieve the common purposes of the company in an environment permanently evolving, while most current information processing systems are based on the analysis of the company's organization and on the collection of the user's needs at a given time. Whereas mediations, which permit actors'coordination, information exchanges, tasks'specifications and activity evaluation, would be real objects to conceive tools to support collaborative conception activities. This fits undergoes almost exclusively by communication and often following nonprescriptive circuits. The methods and tools of work organization can benefit from new technologies of communication to return these more effective adaptations. A new type of software will be then necessary to disambiguate, to facilitate, to structure and to allow complex searches in the exchanges arising among the actors (human beings and software agents) of a cooperative socio-technical system. Multi-Agents Systems, dynamically adaptable, can follow this evolutions and take into account each actors'specificities.
This thesis aims to develop a natural language processing tool able to analyze and reformulate arithmetic problems in French 3rd primary school cycle. More precisely, this tool should allow: To detect linguistic difficulties and challenges in these mathematical problems. To classify problems according to the linguistic difficulties they present. To generate a reformulation of these problems in order to facilitate their understanding. To evaluate the output of the generation, or compare 2 versions of a same problem.
This thesis focuses on Arabic embedded text detection and recognition in videos. Different approaches robust to Arabic text variability (fonts, scales, sizes, etc.) as well as to environmental and acquisition condition challenges (contrasts, degradation, complex background, etc.) are proposed. We introduce different machine learning-based solutions for robust text detection without relying on any pre-processing. The first method is based on Convolutional Neural Networks (ConvNet) while the others use a specific boosting cascade to select relevant hand-crafted text features. Standing out from the dominant methodology of hand-crafted features, we propose to learn relevant text representations from data using different deep learning methods, namely Deep Auto-Encoders, ConvNets and unsupervised learning models. Each one leads to a specific OCR (Optical Character Recognition) solution. Sequence labeling is performed without any prior segmentation using a recurrent connectionist learning model. Proposed solutions are compared to other methods based on non-connectionist and hand-crafted features. Both OCR and language model probabilities are incorporated in a joint decoding scheme where additional hyper-parameters are introduced to boost recognition results and reduce the response time. Given the lack of public multimedia Arabic datasets, we propose novel annotated datasets issued from Arabic videos. The OCR dataset, called ALIF, is publicly available for research purposes. Our proposed solutions were extensively evaluated. Obtained results highlight the genericity and the efficiency of our approaches, reaching a word recognition rate of 88.63% on the ALIF dataset and outperforming well-known commercial OCR engine by more than 36%.
This doctoral thesis addresses the process of developing a subtitling software adapted to the context of higher education from the point of view of a professional subtitler. First, it reviews the theoretical foundations of Audiovisual Translation and subtitling with the aim of defining the linguistic and semiotic characteristics of the audiovisual text and its translation. Second, it studies the explicit extratextual norms and identifies the functionalities required for their implementation allowing subtitlers to adapt the text to the target culture's expectations. In addition, this work deals with the professional aspect of this practice, analysing the impact that technological advances have on the subtitling process and on the tools available to translators. It also presents the use of subtitled audiovisual material in higher education environments through the specific case of a multilingual and multicultural distance-learning environment. This analysis of subtitling from different perspectives concludes with the creation of Miro Translate, a hybrid cloud platform specially designed for subtitling video lectures. Finally, the quality of this tool is studied through a usability test that measures users'perceived satisfaction, efficiency and effectiveness in order to identify the necessary actions for the improvement of the platform.
1. Econometric analysis of the impact of social media movements 2. Development of new methods to analyze social media data 3. Development of new natural language processing techniques for applied economics
In the first part of the thesis, we propose a generalized optimization approach for the graph-based semi-supervised learning which implies as particular cases the Standard Laplacian, Normalized Laplacian and PageRank based methods. Using random walk theory, we provide insights about the differences among the graph-based semi-supervised learning methods and give recommendations for the choice of the kernel parameters and labelled points. This application demonstrates that the proposed family of methods scales very well with the volume of data. Specifically, we propose random walk based algorithms for quick detection of large degree nodes and nodes with large values of Personalized PageRank. Finally, in the end of the thesis we suggest new centrality measure, which generalizes both the current flow betweenness centrality and PageRank. This new measure is particularly well suited for detection of network vulnerability.
Considered a historian who sacrifices his rigor and accuracy for the sake of rhetoric, Curtius Rufus enjoys, and with him his “fictionalized” history as well, a halftone reputation. The historian attempts to debunk the very nature of this wonderful East, the providential fortune claimed by Macedonian, and even language. The unbridled quest for glory pursued by the king and his dream of deification are here condemned: the East stands for a general inversion of norms and values, and fortune becomes an illusion leading to a feeling of impunity. Implicitly, it also proposes an ideal of power mainly based on balance and on the responsibility of the prince. The historian questions the relevance of a central myth of the Roman political imagination in the political context of the times, whose shadow looms over all ambitious men, starting with emperors or candidates for the Empire. His well crafted narrative is a call for a reflection on the actual exercise of power, its challenges and limitations.
The extraction of images semantic is a process that requires deep analysis of the image content. It refers to their interpretation from a human point of view. It consists in extracting single or multiple images semantic in order to facilitate its retrieval. These objectives clearly show that the extraction of semantic is not a new research field. This thesis deals with the semantic collaborative annotation of images and their retrieval. Firstly, it discusses how annotators could describe and represent images content based on visual information, and secondly how images retrieval could be greatly improved thank to latest techniques, such as clustering and recommendation. To achieve these purposes, the use of implicit image content description tools, interactions of annotators that describe the semantics of images and those of users that use generated semantics to retrieve the images, would be essential. In this thesis, we focus our research on the use of Semantic Web tools, in particular ontologies to produce structured descriptions of images. Ontology is used to represent image objects and the relationships between these objects. In other words, it allows to formally represent the different types of objects and their relationships. Ontology encodes the relational structure of concepts that can be used to describe and reason. This makes them eminently adapted to many problems such as semantic description of images that requires prior knowledge as well as descriptive and normative capacity. The contribution of this thesis is focused on three main points: semantic representation, collaborative semantic annotation and semantic retrieval of images. Semantic representation allows to offer a tool for the capturing semantics of images. To capture the semantics of images, we propose an application ontology derived from a generic ontology. Semantic retrieval allows to look for images with semantics provided by collaborative semantic annotation. It is based on clustering and recommendation. Clustering is used to group similar images corresponding to the user's query and recommendation aims to propose semantics to users based on their profiles. It consists of three steps: creation of users community, acquiring of user profiles and classification of user profiles with Galois algebra. Experiments were conducted to validate the approaches proposed in this work.
Cloud services offer reduced costs, elasticity and a promised unlimited managed storage space that attract many end-users. File sharing, collaborative platforms, email platforms, back-up servers and file storage are some of the services that set the cloud as an essential tool for everyday use. Currently, most operating systems offer built-in outsourced cloud storage applications, by design, such as One Drive and iCloud, as natural substitutes succeeding to the local storage. However, many users, even those willing to use the aforementioned cloud services, remain reluctant towards fully adopting cloud outsourced storage and services. Concerns related to data confidentiality rise uncertainty for users maintaining sensitive information. There are many, recurrent, worldwide data breaches that led to the disclosure of users' sensitive information. To name a few: a breach of Yahoo late 2014 and publicly announced on September 2016, known as the largest data breach of Internet history, led to the disclosure of more than 500 million user accounts; a breach of health insurers, Anthem in February 2015 and Premera BlueCross BlueShield in March 2015, that led to the disclosure of credit card information, bank account information, social security numbers, data income and more information for more than millions of customers and users. A traditional countermeasure for such devastating attacks consists of encrypting users' data so that even if a security breach occurs, the attackers cannot get any information from the data. Unfortunately, this solution impedes most of cloud services, and in particular, searching on outsourced data. Researchers therefore got interested in the following question: how to search on outsourced encrypted data while preserving efficient communication, computation and storage overhead? This question had several solutions, mostly based on cryptographic primitives, offering numerous security and efficiency guarantees. While this problem has been explicitly identified for more than a decade, many research dimensions remain unsolved. The main goal of this thesis is to come up with practical constructions that are (1) suitable for real life deployments verifying necessary efficiency requirements, but also, (2) providing good security insurances. Throughout our research investigation, we identified symmetric searchable encryption (SSE) and oblivious RAM (ORAM) as the two potential and main cryptographic primitives' candidate for real life settings. We have recognized several challenges and issues inherent to these constructions and provided a number of contributions that improve upon the state of the art. First, we contributed to make SSE schemes more expressive by enabling Boolean, semantic, and substring queries. Practitioners, however, need to be very careful about the provided balance between the security leakage and the degree of desired expressiveness. Second, we improve ORAM's bandwidth by introducing a novel recursive data structure and a new eviction procedure for the tree-based class of ORAM constructions, but also, we introduce the concept of resizability in ORAM which is a required feature for cloud storage elasticity.
In this dissertation we address the problem of weakly supervised object detection, wherein the goal is to recognize and localize objects in weakly-labeled images where object-level annotations are incomplete during training. To this end, we propose two methods which learn two different models for the objects of interest. In our first method, we propose a model enhancing the weakly supervised Deformable Part-based Models (DPMs) by emphasizing the importance of location and size of the initial class-specific root filter. We first compute a candidate pool that represents the potential locations of the object as this root filter estimate, by exploring the generic objectness measurement (region proposals) to combine the most salient regions and “good” region proposals. We then propose learning of the latent class label of each candidate window as a binary classification problem, by training category-specific classifiers used to coarsely classify a candidate window into either a target object or a non-target class. Furthermore, we improve detection by incorporating the contextual information from image classification scores. Finally, we design a flexible enlarging-and-shrinking post-processing procedure to modify the DPMs outputs, which can effectively match the approximate object aspect ratios and further improve final accuracy. Second, we investigate how knowledge about object similarities from both visual and semantic domains can be transferred to adapt an image classifier to an object detector in a semi-supervised setting on a large-scale database, where a subset of object categories are annotated with bounding boxes. We propose to transform deep Convolutional Neural Networks (CNN)-based image-level classifiers into object detectors by modeling the differences between the two on categories with both image-level and bounding box annotations, and transferring this information to convert classifiers to detectors for categories without bounding box annotations. We have evaluated both our approaches extensively on several challenging detection benchmarks, e.g., PASCAL VOC, ImageNet ILSVRC and Microsoft COCO. Both our approaches compare favorably to the state-of-the-art and show significant improvement over several other recent weakly supervised detection methods.
Bayesian statistics offer a theoretically well founded framework to reason about uncertainty, and it is one of the cornerstones of modern machine learning. Although seemingly quite remote from deep neural networks which are theoretically poorly understood, there is a great potential of cross-fertilization between deep learning and Bayesian statistics. The first is the design of energy and memory efficient neural network models that are suitable for deployment in devices such as mobile phones or drones. Current solutions only consider using sparsity inducing priors to suppress redundant network parameters in a given architecture, or posterior concentration to sparse deep networks. Another line of work on neural architecture search is based on reinforcement learning and requires training of thousands of deep neural nets across hundreds of GPUs. The second problem we want to address is the reliance of deep learning on large quantities of training data, which are typically time consuming and expensive to acquire. Vice-versa deep learning provides a rich set of techniques that can be used to address problems in probabilistic graphical models, such as Bayesian networks and Markov random fields. For many models of interest, the posterior is however intractable to compute exactly.
This dissertation examines the universal phenomenon of poly-divergence in archaic Chinese with five subtypes: the polygrammaticalization with the case of huò 或; the polylexicalization and the hybridization of these two with the case of rán 然; the poly-divergence under the influence of cult-culture-philosophical with the case of yī 一; the poly-divergence of the multifunctional structure with the case of "Reference + Comment"; the poly-divergence of the phrase with the case of "numeral +month". These divergences have the effect of reducing and avoiding the opacity caused by the non-obligatory usage of function word or the lack of exclusive function word. And these types of poly-divergence in Chinese are realized through mechanisms that are distinct from those of Western languages. Finally, a conclusion is drawn and it is argued that the poly-divergence is the essential model of evolution in archaic Chinese
Our work demonstrated the performance of the epidemic intelligence systems used for the early detection of infectious diseases in the world, the specific added value of each system, the greater intrinsic sensitivity of moderated systems and the variability of the type information source's used. The creation of a combined virtual system incorporating the best result of the seven systems showed gains in terms of sensitivity and timeliness that would result from the integration of these individual systems into a supra-system. They have shown the limits of these tools and in particular: the low positive predictive value of the raw signals detected, the variability of the detection capacities for the same disease, but also the significant influence played by the type of pathology, the language and the region of occurrence on the detection of infectious events. They established the wide variety of epidemic intelligence strategies used by public health institutions to meet their specific needs and the impact of these strategies on the nature, the geographic origin and the number of events reported. As well, they illustrated that under conditions close to the routine, epidemic intelligence permitted the detection of infectious events on average one to two weeks before their official notification, hence allowing to alert health authorities and therefore the anticipating the implementation of eventual control measures. Our work opens new fields of investigation which applications could be important for both users systems.