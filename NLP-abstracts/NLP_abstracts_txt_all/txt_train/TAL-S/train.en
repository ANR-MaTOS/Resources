With the unprecedented growth of user-generated content produced on microblogging platforms, finding interesting content for a given user has become a major issue.
However due to the intrinsic properties of microblogging systems, such as the volumetry, the short lifetime of posts and the sparsity of interactions between users and content, recommender systems cannot rely on traditional methods, such as collaborative filtering matrix factorization.
After a thorough study of a large Twitter dataset, we present a propagation model which relies on homophily to propose post recommendations.
Our approach relies on the construction of a similarity graph based on retweet behaviors on top of the Twitter graph.
We then conduct experiments on our real dataset to demonstrate the quality and scalability of our method.
We find that, counter-intuitively, in most cases recommender systems tend to open users perspectives.
However, for some specific users, the bubble effect is noticeable and we propose a model relying on communities to provide a list of recommendations closer to the user's usage of the platform.
Pattern recognition is a fundamental task for living beings and is perform very efficiently by the brain.
Artificial deep neural networks are making quick progress in reproducing these performance and have many applications such as image recognition or natural language processing.
However, they require extensive training on large datasets and heavy computations.
A promising alternative are spiking neural networks, which closely mimic what happens in the brain, with spiking neurons and spike-timing dependent plasticity (STDP).
They are able to perform unsupervised learning and have been used for visual or auditory pattern recognition. However, for now applications using STDP networks lag far behind classical deep learning.
Developing new applications for this kind of networks is all the more at stake that they could be implemented in low power neuromorphic hardware that currently undergoes important developments, in particular with analog miniaturized memristive devices able to mimic synaptic plasticity.
In this work, we chose to develop an STDP neural network to perform a specific task: spike-sorting, which is a crucial problem in neuroscience.
Brain implants based on microelectrode arrays are able to record the activity of individual neurons, appearing in the recorded signal as peak potential variations called action potentials.
Several neurons can be recorded by the same electrode.
The goal of spike-sorting is to extract and separate the activity of different neural cells from a common extracellular recording taking advantage of the fact that the shape of an action potential on an electrode depends on the neuron it stems from.
Most classical spike-sorting approaches use three separated steps: detecting all action potentials in the signal, extract features characterizing their shapes, and separating these features into clusters that should correspond to different neural cells.
Though online methods exists, most widespread spike-sorting methods are offline or require an offline preprocessing step, which is not compatible with online application such as Brain-computer interfaces (BCI).
Using an STDP network brings a new approach to meet these requirements.
We designed a network that take the electrode signal as an input, and output spikes that correspond to the spiking activity of the recorded neural cells.
It is organized into several layers, designed to achieve different processing steps, connected in feedforward way.
The first layer, composed of neurons acting as sensory neurons, convert the input signal into spike train.
The following layers are able to learn patterns from the previous layer thanks to STDP rules.
Each layer implement different mechanisms that improve their performance, such as resource-dependent STDP, intrinsic plasticity, plasticity triggered by inhibition, or neuron models having rebound spiking properties.
An attention mechanism has been implemented to make the network sensitive only to part of the signal containing action potentials.
This network was first designed to process data from a single electrode, and then adapted to process data from multiple electrodes.
It has been tested on simulated data, which allowed to compare the network output to the known ground truth, and also on real extracellular recordings associated with intracellular recordings that give an incomplete ground truth.
Different versions of the network were evaluated and compared to other spike-sorting algorithms, and found to give very satisfying results.
Following these software simulations, we initiated an FPGA implementation of the method, which constitutes a first step toward embedded neuromorphic implementation.
This study is intented to provide a first framework for the definition of an electronic dictionary of the Malagasy language to be used in the context of Natural Language Processing components.
Our work is based, on the one hand, on a descriptive study of the Malagasy language, and, on the other hand, on the proposition of a computer representation of the corresponding phenomena.
Following a preliminary chapter dedicated to a general presentation of the geographical and sociological context, the first part of the thesis presents a thorough analysis of Malagasian terms from the point of view of their morphology and syntactic behaviour.
In particular, we describe the various voices and modalities of Malagasian utterances: tense, aspect and mood.
The second part of the thesis comprises the formel representation that we associate to the linguistic data that have been previously described, so that a possible implementation can be derived in the form of a parser / generator of Malagasian predicative terms (AGTM).
Our system can also provide all the possible forms that can be derived from a root, given a set of restrictions provided as input.
The thesis comprises several appendixes which present the whole software implemented in Prolog, as well as sample results.
Emphasizing on the key role of polysemy in forming the lexicon is the main goal to be achieved in this dissertation paper.
The paper suggests a qualitative evaluation of polysemy in comparing it with other relations that form the lexicon.
The research confirms that the polysemic links must not be modeled independently from derivative links or conversational links.
This evaluation leads us to reveal that the boundary between polysemy and conversation is porous.
The properties of analogy has been used to compare the relations, which is well adapted in characterizing the links between the relations.
They are the links that connect lexis which form the objects of a comparison.
A polysemic link is a link by which two lexis are connected to each other in a polysemic relation.
This link can be compared to a link that connects two other lexis in a conversional relation.
In this paper, Wolof, an Atlantic language in West-Africa, is studied.
This language provides a fertile breeding ground for our explorations.
Modeling natural language is among fundamental challenges of artificial intelligence and the design of interactive machines, with applications spanning across various domains, such as dialogue systems, text generation and machine translation.
We propose a discriminatively trained log-linear model to learn the distribution of words following a given context.
The outcome is an efficient model that suitably captures long dependencies in language without a significant increase in time or space requirements.
In a log-linear model, both training and testing become increasingly expensive with growing number of classes.
The number of classes in a language model is the size of the vocabulary which is typically very large.
A common trick is to cluster classes and apply the model in two-steps; the first step picks the most probable cluster and the second picks the most probable word from the chosen cluster.
This idea can be generalized to a hierarchy of larger depth with multiple levels of clustering.
However, the performance of the resulting hierarchical classifier depends on the suitability of the clustering to the problem.
We study different strategies to build the hierarchy of categories from their observations.
This thesis develops a theoretical formalism that takes into account semantical discourse dynamics.
It focuses on the extension of Montague semantic with the notion of continuation and an exception handling and raising mechanism.
The formalism allows to handle dynamic phenomena such as cross-sentential anaphora, presuppositions triggered by referring expressions and presupposition projection.
Starting from the postulate that this French political party is now anchored in the system and, paradoxically, claims to be an 'anti-system'party, we study the speech construction of this opposition.
To carry out this research, we have used the Hyperbase software, text data analysis platform, to create a vast corpus over three million occurrences structured in five databases.
We have thus applied our tools and our analysis to more than 300 Lepenian speeches and three French presidential campaigns.
They progress from an infra-textual analysis, centred on the study of the lexical and syntactic specificities of the FNF discourse, to a textual analysis devoted to the inter- and supra-phrastic cohesion of the Lepenian textuality, in order to arrive at the discursive organization and the relations that the FNF discourse locally and globally poses to other discourses.
The aim of this thesis is to carry out lexical analysis of written texts in Lithuanian by automatic means, according to a heuristics from form to content based on symbolic methods.
This study attempts to make an expanded use of marks given by linguistic forms, drawing on graphemic and morphological aspects.
This formal starting point in conjunction with automation of linguistic tasks required a revision of the traditional grammatical point of view, concerning mainly parts of speech, lexical structure and suffixation.
This linguistic model, which needs further expansion, served as a basis for ALeksas, an analyzer of lexical forms.
This software implements a hybrid structure expanding a system of finite state automata.
The prototype computes the analysis of word forms, giving grammatical interpretations according to a set of formal criteria, instead of making use of a lexical database.
The results of the analysis of a corpus complied from various texts allowed us to delineate more precisely the advantages and shortcomings of Aleksas, as compared with other similar tools, and to also suggest possible enhancements.
Historically, Probabilistic Graphical Models (PGMs) are a solution for learning from uncertain and flat data, also called propositional data or attributevalue representations.
In the early 2000s, great interest was addressed to the processing of relational data which includes a large number of objects participating in different relations.
Probabilistic Relational Models (PRMs) present an extension of PGMs to the relational context.
With the rise of the internet, numerous technological innovations and web applications are driving the dramatic increase of various and complex data.
Consequently, Big Data has emerged.
However, all PRMs structure learning use wellstructured data that are stored in relational databases.
Graph databases are unstructured and schema-free data stores.
Edges between nodes can have various signatures.
Since, relationships that do not correspond to an ER model could be depicted in the database instance.
These relationships are considered as exceptions.
In this thesis, we are interested by this type of data stores.
Also, we study two kinds of PRMs namely, Direct Acyclic Probabilistic Entity Relationship (DAPER) and Markov Logic Networks (MLNs).
We propose two significant contributions.
First, an approach to learn DAPERs from partially structured graph databases.
A second approach consists to benefit from first-order logic to learn DAPERs using MLN framework to take into account the exceptions that are dropped during DAPER learning.
We are conducting experimental studies to compare our proposed methods with existing approaches.
This dissertation is composed of three distinct studies that empirically examine the role of the information disclosed on goodwill impairment for key firm stakeholders (i.e., financial analysts, peer firms, and external auditors).
In the first study, I examine the effect of disclosure transparency on disagreement among analysts, and disagreement between analysts and managers, in the context of goodwill impairment.
The second study examines whether the reporting of significant goodwill impairment by a firm (impairment firm) affects the corporate investment behavior of other firms in the same industry (peer firms).
The third study investigates the impact of the expanded audit report disclosure on firms' financial disclosure decisions.
Specifically, I examine whether firms adjust the levels of disclosure on goodwill impairment when auditors flag goodwill impairment as a risk of material misstatements in the expanded audit report.
This study contributes to the debate about the usefulness of the expanded audit report by identifying the mechanism through which expanded audit report impacts financial reporting and corporate decisions.
Dysarthria is a speech disorder resulting from neurological impairments of the speech motor control.
It can be caused by different pathologies (Parkinson's disease, A myotrophic Lateral Sclerosis - ALS, etc.) and affects different levels of speech production (respiratory,laryngeal and supra-laryngeal).
The majority of research work dedicated to the study of dysarthric speech relies on perceptual analyses.
The most known study, by F. L. Darley in 1969, led to the organization and the classification of dysarthria within 6 classes (completed with 2 additional classes in 2005).
Nowadays, perceptual evaluation is still the most used method in clinical practice for the diagnosis and the therapeutic monitoring of patients.
However, this method is known to be subjective, non reproductive and time-consuming.
These limitations make it inadequate for the evaluation of large corpora (in case of phonetic studies) or for the follow-up of the progression of the condition of dysarthric patients.
The work presented in this document falls within this framework and studies the contributions that these tools can have in the evaluation of dysarthric, and more generally pathological speech.
In this work, an automatic approach for the detection of abnormal phones in dysarthric speech is proposed and its behavior is analyzed on different speech corpora containing different pathologies, dysarthric classes, dysarthria severity levels and speech styles (read and spontaneous speech).
Unlike the majority of the automatic methods proposed in the literature that provide a global evaluation of the speech on general items such as dysarthria severity, intelligibility, etc., our proposed method focuses on the phone level aiming to achieve a better characterization of dysarthria effects and to provide a precise and useful feedback to the potential users (clinicians, phoneticians,patients).
This method consists on two essential phases: (1) an automatic phone alignment of the speech (2) an automatic classification of the resulting phones in two classes:normal and abnormal phones.
When compared to an annotation of phone anomalies provided by a human expert considered to be the "gold standard", the approach showed encouraging results and proved to be able to detect anomalies on the phone level.
The approach was also able to capture the evolution of the severity of the dysarthria suggesting a potential relevance and use in the longitudinal follow-up of dysarthric patients or for the automatic prediction of their intelligibility or the severity of their dysarthria.
Also, the automatic phone alignment precision was found to be dependent on the severity,the pathology, the class of the dysarthria and the phonetic category of each phone.
Furthermore, the speech style was found to have an interesting effect on the behaviors of both automatic phone alignment and anomaly detection.
Finally, the results of an evaluation campaign conducted by a jury of experts on the annotations provided by the proposed approach are presented and discussed in order to draw a panel of the strengths and limitations of the system.
Initially created for a reimbursement purpose, non-clinical claim databases are exhaustive Electronic Health Records (EHRs) which are particularly valuable for evidence-based studies.
New process models and an adapted process discovery algorithm are introduced, with the objective of accurately model characteristic transitions and time hidden in non-clinical claims data.
The second contribution is a preprocessing solution to handle one complexity of such data, which is the representation of medical events by multiple codes belonging to different standard coding systems, organized in hierarchical structures.
The proposed method uses auto-encoders and clustering in an adequate latent space to automatically produce relevant and explainable labels.
From these contributions, an optimization-based predictive method is introduced, which uses a process model to perform binary classification from event logs and highlight distinctive patterns as a global explanation.
A second predictive method is also proposed, which uses images to represent patient pathways and a modified Variational Auto-Encoders (VAE) to predict.
This method globally explains predictions by showing an image of identified predictive factors which can be both frequent and infrequent.
With the rise of Big Data, the processing of Volume, Velocity (growth and evolution) and data Variety concentrates the efforts of communities to exploit these new resources.
These new resources have become so important that they are considered the new "black gold".
In recent years, volume and velocity have been aspects of the data that are controlled, unlike variety, which remains a major challenge.
This thesis presents two contributions in the field of heterogeneous data matching, with a focus on the spatial dimension.
The first contribution is based on a two-step process for matching heterogeneous textual data: georepresentation and geomatching.
In the first phase, we propose to represent the spatial dimension of each document in a corpus through a dedicated structure, the Spatial Textual Representation (STR).
This graph representation is composed of the spatial entities identified in the document, as well as the spatial relationships they maintain.
To identify the spatial entities of a document and their spatial relationships, we propose a dedicated resource, called Geodict.
The second phase, geomatching, computes the similarity between the generated representations (STR).
To assess the relevance of a match, we propose a set of 6 criteria based on a definition of the spatial similarity between two documents.
The second contribution is based on the thematic dimension of textual data and its participation in the spatial matching process.
We propose to identify the themes that appear in the same contextual window as certain spatial entities.
The objective is to induce some of the implicit spatial similarities between the documents.
To do this, we propose to extend the structure of STR using two concepts: the thematic entity and the thematic relationship.
The thematic entity represents a concept specific to a particular field (agronomic, medical) and represented according to different spellings present in a terminology resource, in this case a vocabulary.
A thematic relationship links a spatial entity to a thematic entity if they appear in the same window.
The selected vocabularies and the new form of STR integrating the thematic dimension are evaluated according to their coverage on the studied corpora, as well as their contributions to the heterogeneous textual matching process on the spatial dimension.
This thesis explores the use of structured losses in two different domains.
In the first contribution, we focus on multi-agent reinforcement learning (MARL), in environments that can be separated into several loosely coupled tasks.
We set out to find policies that can generalize well to more agents and tasks than seen during training, effectively scaling up the size of problems that can be tackled.
Our solution assigns agents to tasks by approximately solving a centralized optimization problem whose objective function is parameterized by a neural network.
We study how the expressivity of the optimization problem and that of the neural network influence the generalization capabilities of the model, and show that with the right choices, the policy can generalize to more than 5 times more agents than seen during training.
In the second contribution we formulate object detection as a set prediction problem, and design a model that can effectively tackle this formulation.
Our solution leverages a deep convolutional network, as is customary in computer vision, and a transformer encoder-decoder network, an architecture that has enabled significant progress in natural language processing.
Crucially, our solution incorporates minimal inductive bias, thereby alleviating the need for hand-designed detection-specific components such as anchors or non-maximal suppression.
With a comparable parameter budget, our model matches the performance of well-established and highly-optimized baselines such as Retinanet and Faster R-CNN on the challenging COCO detection dataset.
Finally, we show that the method can be naturally extended to perform panoptic segmentation, where it outperforms competing approaches, thus showing the versatility of the model.
The diarization task tries to determine the number of speakers as well as their interventions in an audio file.
It is an interesting task for any enterprise willing to index its audiovisual contents.
Especially, the French National Audiovisual Institute (INA) desires to apply this task on its archives so as to improve its accessibility and its annotation.
However, the uses of the institute need a minimal quality which, most of the time, is not reached by the state-of-the-art automatic diarization systems yet.
In order to reach the wanted effectiveness, a human can correct the output of a diarization system.
Nevertheless, a human intervention is generally time-consuming and expensive.
In order to reduce these costs, a possible solution is to use a computer-assisted system: a human gives some information to a system in order that it can improve its predictions so as to decrease its intervention cost.
The present manuscript revolves around the computer-assisted diarization.
It proposes a metric so as to assess the human intervention cost to correct a diarization, a framework to evaluate the human corrections of a speaker diarization, an automaton simulating the human corrections to do for a diarization and some computer-assisted diarization systems decreasing the total human intervention cost.
More precisely, the proposed computer-assisted diarization systems reassess either only the speaker clustering or the segmentation and the speaker clustering.
They have given rise to numerous studies in Natural Language Processing.
Indeed, their study and precise identification are essential, both from a theoretical and applicative perspective.
However, most of the researches about the subject relate to everyday uses of language: "small talk" dialogs, requests for schedule, speeches, etc.
But what about spontaneous speech production made in a restrained framework?
To our knowledge, no study has ever been carried out in this context.
However, we know that using a "language specialty" in the framework of a given task leads to specific behaviours.
Our thesis work is devoted to the linguistic and computational study of disfluencies within such a framework.
These dialogs concern air traffic control, which entails both pragmatic and linguistic constraints.
We carry out an exhaustive study of disfluencies phenomena in this context.
At first we conduct a subtle analysis of these phenomena.
Then we model them to a level of abstraction, which allows us to obtain the patterns corresponding to the different configurations observed.
Finally we propose a methodology for automatic processing.
It consists of several algorithms to identify the different phenomena, even in the absence of explicit markers.
It is integrated into a system of automatic processing of speech.
Eventually, the methodology is validated on a corpus of 400 sentences.
Driven with the objective of rendering robots as socio-communicative, there has been a heightened interest towards researching techniques to endow robots with social skills and "commonsense" to render them acceptable.
This social intelligence or ``commonsense'' of the robot is what eventually determines its social acceptability in the long run.
Commonsense, however, is not that common.
Robots can, thus, only learn to be acceptable with experience.
However, teaching a humanoid the subtleties of a social interaction is not evident.
Even a standard dialogue exchange integrates the widest possible panel of signs which intervene in the communication and are difficult to codify (synchronization between the expression of the body, the face, the tone of the voice, etc.).
In such a scenario, learning the behavioral model of the robot is a promising approach.
This learning can be performed with the help of AI techniques.
This study tries to solve the problem of learning robot behavioral models in the Automated Planning and Scheduling (APS) paradigm of AI.
In the domain of Automated Planning and Scheduling (APS), intelligent agents by virtue require an action model (blueprints of actions whose interleaved executions effectuates transitions of the system state) in order to plan and solve real world problems.
During the course of this thesis, we introduce two new learning systems which facilitate the learning of action models, and extend the scope of these new systems to learn robot behavioral models.
These techniques can be classified into the categories of non-optimal and optimal.
Non-optimal techniques are more classical in the domain, have been worked upon for years, and are symbolic in nature.
However, they have their share of quirks, resulting in a less-than-desired learning rate.
The optimal techniques are pivoted on the recent advances in deep learning, in particular the Long Short Term Memory (LSTM) family of recurrent neural networks.
These techniques are more cutting edge by virtue, and produce higher learning rates as well.
This study brings into the limelight these two aforementioned techniques which are tested on AI benchmarks to evaluate their prowess.
They are then applied to HRI traces to estimate the quality of the learnt robot behavioral model.
This is in the interest of a long term objective to introduce behavioral autonomy in robots, such that they can communicate autonomously with humans without the need of ``wizard'' intervention.
A corpus as vast and rich as that of the tale in the oral tradition of the province of Valladolid justifies the creation of a lexicostatistic tool.
Computer analysis makes it possible to visualize precise and exhaustive data and to bring to the fore the various lexical units used in tales taken from the works of Joaquin DIAZ and from those of Aurelio M. ESPINOSA (hijo).
This study first dwells on the particular regional context and defines the terminological apparatus.
It then consists in the creation of a lexical data base -made possible by the processing of the whole corpus-derived from a large terminological body organized into several lexicosemantic categories; in the counting and representing by graphs of the data obtained; and finally in the interpretation of the results, registered and measured according to a sociolinguistic and sociocultural approach.
In the end, this study allows the classification of tales by a new method of textual processing.
In formal semantics, researchers assign meanings to sentences of a natural language.
This work is guided by the principle of compositionality: the meaning of an expression is a function of the meanings of its parts.
These functions are often formalized using the [lambda]-calculus.
However, there are areas of language which challenge the notion of compositionality, e.g. anaphoric pronouns or presupposition triggers.
These force researchers to either abandon compositionality or adjust the structure of meanings.
In the first case, meanings are derived by processes that no longer correspond to pure mathematical functions but rather to context-sensitive procedures, much like the functions of a programming language that manipulate their context with side effects.
In the second case, when the structure of meanings is adjusted, the new meanings tend to be instances of the same mathematical structure, the monad.
Monads themselves being widely used in functional programming to encode side effects, the common theme that emerges in both approaches is the introduction of side effects.
Furthermore, different problems in semantics lead to different theories which are challenging to unite.
Our thesis claims that by looking at these theories as theories of side effects, we can reuse results from programming language research to combine them.
In the first part of the thesis, we prove some of the fundamental properties of this calculus: subject reduction, confluence and termination.
Then in the second part, we demonstrate how to use the calculus to implement treatments of several linguistic phenomena: deixis, quantification, conventional implicature, anaphora and presupposition.
In the end, we build a grammar that features all of these phenomena and their interactions.
The advent of online platforms such as weblogs and social networking sites provided Internet users with an unprecedented means to express their opinions on a wide range of topics, including policy and commercial products.
This large volume of opinionated data can be explored and exploited through text mining techniques known as opinion mining or sentiment analysis.
Contrarily to traditional opinion mining work which mostly focuses on positive and negative opinions (or an intermediate in-between), we study a more challenging type of opinions: viewpoints.
Viewpoint mining reaches beyond polarity-based opinions (positive/negative) and enables the analysis of more subtle opinions such as political opinions.
In our first contribution, we explored the idea of separating opinion words (specific to both viewpoints and topics) from topical, neutral words based on parts of speech, inspired by similar practices in the litterature of non viewpoint-related opinion mining.
Our second contribution tackles viewpoints expressed by social network users.
We aimed to study to what extent social interactions between users – in addition to text content – can be beneficial to identify users'viewpoints.
Our different contributions were evaluated and benchmarked against state-of-the-art baselines on real-world datasets
Action recognition in videos is one of the key problems in visual data interpretation.
Despite intensive research, differencing and recognizing similar actions remains a challenge.
This thesis deals with fine-grained classification of sport gestures from videos, with an application to table tennis.
In this manuscript, we propose a method based on deep learning for automatically segmenting and classifying table tennis strokes in videos.
For developing such a system with fine-grained classification, a very specific dataset is needed to supervise the learning process.
To that aim, we built the “TTStroke-21” dataset, which is composed of 20 stroke classes plus a rejection class.
These recorded sessions were annotated by professional players or teachers using a crowdsourced annotation platform.
The annotations consist in a description of the handedness of the player and information for each stroke performed (starting and ending frames, class of the stroke).Fine-grained action recognition has some notable differences with coarse-grained action recognition.
In general, datasets used for coarse-grained action recognition, the background context often provides discriminative information that methods can use to classify the action, rather than focusing on the action itself.
In this thesis, we introduce a Twin Spatio-Temporal Convolutional Neural Network.
This deep learning network takes as inputs an RGB image sequence and its computed Optical Flow.
The RGB image sequence allows our model to capture appearance features while the optical flow captures motion features.
Those two streams are processed in parallel using 3D convolutions, and fused at the last stage of the network.
Our method gets an average classification performance of 87.3% with a best run of 93.2% accuracy on the test set.
When applied on joint detection and classification task, the proposed method reaches an accuracy of 82.6%.A systematic study of the influence of each stream and fusion types on classification accuracy has been performed, giving clues on how to obtain the best performances.
A comparison of different optical flow methods and the role of their normalization on the classification score is also done.
The extracted features are also analyzed by back-tracing strong features from the last convolutional layer to understand the decision path of the trained model.
Finally, we introduce an attention mechanism to help the model focusing on particular characteristic features and also to speed up the training process.
For comparison purposes, we provide performances of other methods on TTStroke-21 and test our model on other datasets.
We notice that models performing well on coarse-grained action datasets do not always perform well on our fine-grained action dataset.
The research presented in this manuscript was validated with publications in one international journal, five international conference papers, two international workshop papers and a reconductible task in MediaEval workshop in which participants can apply their action recognition methods to TTStroke-21.
Two additional international workshop papers are in process along with one book chapter.
With our thesis, we intend to lay out the linguistic and didactic foundations necessary for the future elaboration of a program or a method in Slavic intercomprehension by taking the example of the Western and the South-Western Slavic languages and in providing a linguistic analysis of three languages: Czech, Slovene and Croatian.
In our work, we seek mainly to provide two elements: - A series of linguistic hypotheses aimed at determining the points to be taught in an intercomprehension method concerning Czech, Slovene and Croatian; - A presentation of programs and support in intercomprehension didactics realized and tested as part of our curriculum.
In our work, we find that the didactics of Slavic intercomprehension differs in many ways from classical learning.
In the case of intercomprehension, many points that are normally heavy and complex to master may be only passed through quickly.
Thanks to our linguistical and didactical analyzes, we have been able to provide a reflection on one of the forms that Slavic intercomprehension formation can take in the future.
We particularly recommend the use of online resources, for example via the website www.rozrazum.eu, developed as a part of this thesis to test activities following the methodology made for Eurom 5 (Bonvino et al., 2001).
This website can initially be used as a test and development platform for didactical approaches, while being functional, and therefore available to a public of learners.
Epidemic intelligence aims to detect, investigate and monitor potential health threats while relying on formal (e.g. official health authorities) and informal (e.g. media) information sources.
Monitoring of unofficial sources, or so-called event-based surveillance (EBS), requires the development of systems designed to retrieve and process unstructured textual data published online.
The first objective of this thesis is to propose and compare approaches to enhance the identification and extraction of relevant epidemiological information from the content of online news.
This manuscript proposes new textual representation approaches by selecting, expanding, and combining relevant epidemiological features.
We show that adapting and extending text mining and classification methods improves the added value of online news sources for event-based surveillance.
We stress the role of domain expert knowledge regarding the relevance and the interpretability of methods proposed in this thesis.
While our researches are conducted in the context of animal disease surveillance, we discuss the generic aspects of our approaches regarding unknown threats and One Health surveillance.
Machine Translation is one of the most difficult tasks in natural language and speech processing.
The linguistic peculiarities of some languages makes the machine translation task more difficult.
In this thesis, we present a detailed study of machine translation systems from arabic to french and to english.
Our principle researches carry on building parallel corpora, arabic preprocessing and adapting translation and language models.
We propose a method for automatic extraction of parallel news corpora from a comparable corpora.
Two approaches for translation model adaptation are explored using whether parallel corpora extracted automatically or parallel corpora constructed automatically.
We demonstrate that adapting data used to build machine translation system improves translation.
Arabic texts have to be preprocessed before machine translation and this because of the agglutinative character of arabic language.
A prepocessing tool for arabic, SAPA (Segmentor and Part-of-speech tagger for Arabic), much faster than the state of the art tools and totally independant of any other external resource was developed.
This tool predicts simultaneously morphosyntactic tags and proclitics (conjunctions, prepositions, etc.) for every word, then splits off words into lemma and proclitics.
We describe also in this thesis, our named entity recognition tool for arabic, NERAr, and we focus on the impact of integrating named entity recognition in the preprocessing task. We used bilingual dictionaries to propose translations of the detected named entities.
We present then many approaches to adapt thematically translation and language models using a corpora consists of a set of multicategoric sentences.
These experiments open important research perspectives such as combining many systems when translating.
It would be interesting also to focus on a temporal adaptation of translation and language models.
Finally, improved machine translation systems from arabic to french and english are integrated in a multimedia platform analysis and shows improvements compared to basic machine translation systems.
The understanding of Vietnamese consumer behaviors toward brands is crucial for not only local but also foreign marketers to be prepared for the competition in the Vietnamese market.
The investigation is based on the theories of brand personality, antecedents (self-congruence and partner quality), consequences (WTP, consideration set size, and WOM), hot and cold BRQ, and brand purchase intention.
The aim behind of this research can be summed up in the following objective: firstly, to determine the effects of brand personality on antecedents of two components of BRQ in the context of Vietnam; secondly, to investigate the impacts of antecedents and consequences of two components of BRQ on brand purchase intention in the context of Vietnam.
A structure model was developed illustrating the relationships (assumed) between brand personality on antecedents and consequences of brand relationship quality (BRQ).
This resulted in the developed of twenty hypotheses.
To address the research aims, data were collected which focused on six product classes and 634 questionnaires were collected in final.
First, the results of our findings reveal that brand personality has a positive influence on two variables self-congruence and partner quality, but it is clearly seen that there is a different level of influence and importance.
Secondly, given that self-congruence is a more significant effect on hot than cold BRQ, on the other hand, partner quality is a more significant effect on cold than hot BRQ.
However, based on the path coefficient of self-congruence and partner quality, the results reveal that self-congruence has a positive significant effect on both hot and cold BRQ compared to partner quality.
We found that consideration set size and WOM have no relationship with brand purchase intention, while WTP has a positive significant effect on brand purchase intention.
The key contributions of this research provide a better understanding consumer behavior in the Vietnamese market.
The findings of our study show that hot BRQ has been shown to have a stronger and significant influence on consumer's WTP.
Cold BRQ, however, was found to strongly impact the consumer's WOM.
Therefore, hot BRQ, which is the emotional relationship quality, mainly increases the loyalty behavior of customers; in contrast, cold BRQ helps to attract new customers by positively word-of-mouth communication of customers.
Both the retention of current customers and the attraction of news customers are crucial drivers for the sustainable future of a brand or a product.
Managers need, therefore, try to positively impact both hot and cold BRQ of their customers.
Furthermore, based on the research results, they should focus on a willingness to pay price premium in order to increase their brand purchasing intention.
The genome of bacteria is classically separated into essential, stable and slow evolving replicons (chromosomes) and accessory, mobile and rapidly evolving replicons (plasmids).
This paradigm is being questioned since the discovery of extra-chromosomal essential replicons (ECERs), be they called ”megaplasmids”, ”secondary chromosomes” or ”chromids”, which possess both chromosomal and plasmidic features.
However, their true nature and the mechanisms permitting their integration within the sable genome are yet to be formally determined.
The relationships between replicons, with reference to their genetic information inheritance systems (GIIS), were explored under the assumption that the inheritance of ECERs is integrated to the cell cycle and highly constrained in contrast to that of standard plasmids.
The recent advances of Information and Communication Technology (ICT) have resulted in the development of several industries.
Adopting semantic technologies has proven several benefits for enabling a better representation of the data and empowering reasoning capabilities over it, especially within an Information Retrieval (IR) application.
This has, however, few applications in the industries as there are still unresolved issues, such as the shift from heterogeneous interdependent documents to semantic data models and the representation of the search results while considering relevant contextual information.
In this thesis, we address two main challenges.
The second one focuses on providing users with innovative search results, from the heterogeneous document corpus, helping the users in interpreting the information that is relevant to their inquiries and tracking cross document dependencies.
To cope with these challenges, we first propose a semantic representation of a heterogeneous document corpus that generates a semantic graph covering both the structural and the domain-specific dimensions of the corpus.
Then, we introduce a novel data structure for query answers, extracted from this graph, which embeds core information together with structural-based and domain-specific context.
In order to provide such query answers, we propose an innovative query processing pipeline, which involves query interpretation, search, ranking, and presentation modules, with a focus on the search and ranking modules.
However, in this thesis, it has been experimented in the Architecture, Engineering and Construction (AEC) industry using real-world construction projects.
The presented thesis deals with the adaptation of the conversion of a written text into speech using a parametric approach to the Arabic language.
Different methods have been developed in order to set up synthesis systems.
These methods are based on a description of the speech signal by a set of parameters.
Besides, each sound is represented by a set of contextual features containing all the information affecting the pronunciation of this sound.
Part of these features depend on the language and its peculiarities, so in order to adapt the parametric synthesis approach to Arabic, a study of its phonological peculiarities wasneeded.
Two phenomena were identified: the gemination and the vowels quantity (short/ long).
Two features associated to these phenomena have been added to the contextual features set.
Four combinations of modeling are possible: alternating the differentiation or fusion of simple and geminated consonants on the one hand and short and long vowels on the other hand.
A set of perceptual and objective tests was conducted to evaluate the effect of the fourunit modelling approaches on the quality of the generated speech.
The evaluations were made in the case of parametric synthesis by HMM then in the case of parametric synthesisby DNN.
The subjective results showed that when the HMM approach is used, the four approaches produce signals with a similar quality, this result that was confirmed by the objective measures calculated to evaluate the prediction of the durations of the speech units.
However, the results of objective evaluations in the case of the DNN approach have shown that the differentiation of simple consonants (respectively short vowels) geminated consonants (respectively long vowels) leads to a slightly better prediction of the durations than the other modelling approaches.
The last part of this thesis was devoted to the comparison of the synthesis approach by the HMMs to that by the DNNs.
All the tests conducted have shown that the use of DNNs has improved the perceived quality of the generated signals.
This "syntactic memory" is built from experience and particularly from the observation of sequences of objects whose organization obeys syntactic rules.
It must have the capability to aid recognizing as well as generating valid sequences in the future, i.e., sequences respecting the learnt rules.
This production of valid sequences can be done either in an explicit way, that is, by evoking the underlying rules, or implicitly, when the learning phase has made it possible to capture the principle of organization of the sequences without explicit recourse to the rules.
Although the latter is faster, more robust and less expensive in terms of cognitive load as compared to explicit reasoning, the implicit process has the disadvantage of not giving access to the rules and thus becoming less flexible and less explicable.
At first, the expert makes a choice to explicitly follow the rules of the trade.
But then, by dint of repetition, the choice is made automatically, without explicit evocation of the underlying rules.
This change in encoding rules in an individual in general and particularly in a business expert can be problematic when it is necessary to explain or transmit his or her knowledge.
Indeed, if the business concepts can be formalized, it is usually in any other way for the expertise which is more difficult to extract and transmit.
In our work, we endeavor to observe sequences of electrical components and in particular the problem of extracting rules hidden in these sequences, which are an important aspect of the extraction of business expertise from technical drawings.
We place ourselves in the connectionist domain, and we have particularly considered neuronal models capable of processing sequences.
We have evaluated these two models on different artificial grammars (Reber's grammar and its variations) in terms of learning, their generalization abilities and their management of sequential dependencies.
Finally, we have also shown that it is possible to extract the encoded rules (from the sequences) in the recurrent network with LSTM units, in the form of an automaton.
These systems are trained on large amount of data, and big data is now considered the « New Oil of the 21st century ».
AI systems can recognize patterns and regularities in data that humans can't, due to their memory limits.
We therefore propose to study the links between training data and performances of automatic speech recognition technology, with a focus on the gender distribution.
We choose to study speech as it now the new interface for human-machine interaction.
We aim at creating a continuum instead of class in order to describe corpus no longer in terms of gender, but in terms of vocal diversity.
This representation will allow us to highlight the vocal variability as well as questioning the pertinence of gender as a categorical distinction, alongside a sociophonetic study of speaker roles and interactions.
Even if the concept of decentralization is embedded to some extent at the very core of the Internet, today's “network of networks” integrates this principle only partially.
In this quest, a number of developers look back to the evergreen qualities of a relatively old technology, peer-to-peer (P2P), that leverages the socio-technical resources of the network's "dwarfs"-its periphery or "edge"-in a way that is, in fact, closer to the pre-commercial Internet.
This dissertation explores the distributed and decentralized approach to the technical architecture of Internet-based services.
It illustrates the co-shaping of a decentralized network architecture and of several different dynamics: the articulation between actors and contents, the allocation of responsibilities and the capacity to exert control, the organization of the market, the forms of existence and role of entities such as the nodes of a network, its users, its central or coordinating units.
This work analyses the conditions under which a network that structures itself according to a non-hierarchical or hybrid model, and delegates the responsibility of its functioning to its edge, can develop and thrive in today's Internet.
The dissertation follows the developers of three Internet services-a search engine, a storage service and a video streaming application-built on primarily decentralized network models; it also follows the collectives of pioneer users developing with these services, and selectively, the political venues where the Internet's medium-and long-term organization and governance are discussed.
This thesis contributes to the study of reliability and safety of computer and software systems which are modeled as discrete event systems.
The major contributions include the theory of Control Systems (C Systems) and the model monitoring approach.
In the first part of the thesis, we study the theory of control systems which combines and significantly extends regulated rewriting in formal languages theory and supervisory control.
The control system is a generic framework, and contains two components: the controlled component and the controlling component that restricts the behavior of the controlled component.
The two components are expressed using the same formalism, e.g., automata or grammars.
We consider various classes of control systems based on different formalisms, for example, automaton control systems, grammar control systems, and their infinite versions and concurrent variants.
After that, an application of the theory is presented.
The Büchi automata based control system is used to model and check correctness properties on execution traces specified by nevertrace claims.
In the second part of the thesis, we investigate the model monitoring approach whose theoretical foundation is the theory of control systems.
The key principle of the approach is “property specifications as controllers”.
In other words, the functional requirements and property specification of a system are separately modeled and implemented, and the latter one controls the behavior of the former one.
The model monitoring approach contains two alternative techniques, namely model monitoring and model generating.
The approach can be applied in several ways to improve reliability and safety of various classes of systems.
We present some typical applications to show its strong power.
First, the approach provides better support for the change and evolution of property specifications.
Second, it provides the theoretical foundation of safety-related systems in the standard IEC 61508 for ensuring the functional validity.
Third, it is used to formalize and check guidelines and consistency rules of UML.These results lay out the foundations for further study of more advanced control mechanisms, and provide a new way for ensuring reliability and safety
Interactions on the Internet require trust between each involved party.
Internet entities assume, at the same time, several roles, each having their own interests and motivations; leading to conflicts that must be addressed to enable security and trust.
In this thesis, we use, and focus on, Keystroke Dynamics (the way a user type on its keyboard) in an attempt to solve some of these conflicts.
Keystroke Dynamics is a a costless and transparent biometric modality as it does not require neither additional sensors nor additional actions from the user.
Unfortunately, Keystroke Dynamics also enables users profiling (s.a. identification, gender, age), against their knowledge and consent.
In order to protect users privacy, we propose to anonymize Keystroke Dynamics.
Still, such information can be legitimately needed by services in order to straighten user authentication.
We then propose a Personal Identity Code Respecting Privacy, enabling biometric users authentication without threatening users privacy.
We also propose a Social Proof of Identity enabling to verify claimed identities while respecting user privacy, as well as ensuring users past behaviors through a system of accountability.
Our research is centered on the analysis of the role of morphological derivation in the elaboration of the lexical aspectual meaning.
The prefix RE- is particularly interesting because of the complexity of its processual structure.
RE- implies a relation between a presupposed process and a posited process, and this relation is established by a third - intermediate - process, which can signify continuity, resumption or interruption.
The activation of this relation depends in particular on the meaning of the lexical base of the derived term and on the aspectual meaning of this base.
One aspect of a successful human-machine interface (e.g. human-robot interaction, chatbots, speech, handwriting…,etc) is the ability to have a personalized interaction.
This affects the overall human experience, and allow for a more fluent interaction.
At the moment, there is a lot of work that uses machine learning in order to model such interactions.
However, these models do not address the issue of personalized behavior: they try to average over the different examples from different people in the training set.
Identifying the human styles (persona) opens the possibility of biasing the models output to take into account the human preference.
In this thesis, we focused on the problem of styles in the context of handwriting.
The objective of my thesis is to study these problems of styles, in the domain of handwriting.
Available to us is IRONOFF dataset, an online handwriting datasets, with 410 writers, with ~25K examples of uppercase, lowercase letters and digits drawings.
For transfer learning, we used an extra dataset, QuickDraw!, a sketch drawing dataset containing ~50 million drawing over 345 categories.
Major contributions of my thesis are:
1) Propose a work pipeline to study the problem of styles in handwriting.
This involves proposing methodology, benchmarks and evaluation metrics.
We choose temporal generative models paradigm in deep learning in order to generate drawings, and evaluate their proximity/relevance to the intended/ground truth drawings.
We proposed two metrics, to evaluate the curvature and the length of the generated drawings.
In order to ground those metics, we proposed multiple benchmarks - which we know their relative power in advance -, and then verified that the metrics actually respect the relative power relationship.
2) Propose a framework to study and extract styles, and verify its advantage against the previously proposed benchmarks.
We settled on the idea of using a deep conditioned-autoencoder in order to summarize and extract the style information, without the need to focus on the task identity (since it is given as a condition).
We validate this framework to the previously proposed benchmark using our evaluation metrics.
We also to visualize on the extracted styles, leading to some exciting outcomes!
3) Using the proposed framework, propose a way to transfer the information about styles between different tasks, and a protocol in order to evaluate the quality of transfer.
We leveraged the deep conditioned-autoencoder used earlier, by extract the encoder part in it - which we believe had the relevant information about the styles - and use it to in new models trained on new tasks.
We extensively test this paradigm over a different range of tasks, on both IRONOFF and QuickDraw! datasets.
We show that we can successfully transfer style information between different tasks.
Study 1 determined text input speed in persons with cervical spinal cord injury and the influence of personal characteristics and type of computer device on text input speed.
Study 2 evaluated the effect of a dynamic virtual keyboard coupled with word prediction software on text input speed in persons with functional tetraplegia.
Study 3 analysed the word prediction software settings commonly prescribed by health-related professionals for people with cervical spinal cord injury.
Studies 4 and 5 evaluated the influence of the number of words displayed in the prediction list and the frequency of use setting on text input speed.
Finally, study 6 evaluated the influence of a training program on the use of word prediction software for persons with cervical spinal cord injury on text input speed.
The influence of the different word prediction software settings (number of words displayed in the prediction list and the frequency of use) on text input speed, the number of errors or comfort of use, differed depending on the level of injury.
We also found differences between the perception of the importance of some settings by health professionals and data in the literature regarding the optimization of settings.
Finally, training persons with cervical spinal cord injury in the use of word prediction software increased text input speed.
The results of this work highlighted that word prediction software settings influence text input speed in persons with cervical spinal cord injury, however not all professionals are aware of this.
Persons with cervical spinal cord injury training programs in the use of word prediction software need to be developed and validated.
Our logometric practice is a statistical application of François Rastier's semantic theories on a corpus of Pierre Mendès France's discurses.
Our statistical way determine particularity by generality: words by paragraph, paragraphs by text, texts by corpus.
We want to describe how situation – historical or social practice – influe on textual structure.
To study the distribution high frequency words in the corpus, our works use two innovent tools: the cotextual environnement and the asymmetric cooccurrence.
The former produces "smallworlds": description of lexical semantic networks in texts ; the latter describes rythms of lexical variations.
In our corpus, these rythms are associated to distinct argumentative values: persuasive-explanatory vs informative.
Let us define a specific preference as a preference that is not shared by any group of user.
A user with several specific preferences will likely be poorly served by a classic CF approach.
This is the problem of Grey Sheep Users (GSU).
In this thesis, I focus on three separate questions.
1) What is a specific preference?
I give an answer by proposing associated hypotheses that I validate experimentally.
2) How to identify GSU in preference data?
This identification is important to anticipate the low quality recommendations that will be provided to these users.
I propose numerical indicators to identify GSU in a social recommendation dataset.
These indicators outperform those of the state of the art and allow to isolate users whose quality of recommendations is very low.
3) How can I model GSU to improve the quality of the recommendations they receive?
I propose new recommendation approaches to allow GSU to benefit from the opinions of other users.
This thesis lies in the difficult context of linguistics and computer science.
More precisely, we aim to demonstrate the value of the simultaneous consideration of the document structure and linguistic knowledge for the classification of documents according to their style.
For this, we defined new descriptors, which, combined with linguistic descriptors exploiting hierarchy of text, are relevant to characterize the types of documents.
Then, we proposed a classification method based on non-presence of patterns in the documents.
One of originalities of our work is to combine linguistic and machine learning methods with techniques search for local patterns.
This hierarchy represents the logical structure of the document based on the principle that different windows of observation correspond to different types of information.
These are interconnected through the concept of inheritance of context in order to preserve the global coherence of the document.
On the other hand, assumptions related to the task of categorization have emerged, such as exploitation of the total or partial absence of patterns under certain constraints, which can be used to build new analogies for the categorization of documents.
Then, by analyzing by evidence pattrens with low or zero frequencies, a new approach of categorization by exclusion-inclusion was proposed by introducing a new concept such as exclusive patterns
For the past thirty years, scientific images have been an object of growing interest in the fields of human and social sciences, art history, history of science and more generally epistemology.
This favored the birth and rise, in the Anglo-Saxon world, of visual studies and more particularly of a branch which specifically concerns epistemocritics: the visual studies of science.
Indeed, in their production and dissemination of knowledge, sciences have an intimate link with visual representations diagrams, pictures, geometric figures, equations, etc.
This articulation between "seeing" and "knowing" has opened up new fields for research in epistemocriticism, which now establishes fruitful relationships with visual studies.
The latter insisted at the same time on the crucial role of the image in the process of knowledge acquisition (modeling, force of paradigmatic proposals, visual support for didactic vocation, document-witness of scientific practices, etc.).
At the heart of our project, there is a particular type of image: diagrams which are heuristic tools allowing groping knowledge to rise and take a more clearly-defined shape.
As hybrid forms, made of image and writing, the diagrams ask in their own way the question of the relationships between knowledge and visualization in the scientific process, a question which took on particular acuity with the crisis of language and representation, sparked by the scientific revolution at the turn of the 19th and 20th centuries.
Traditionally, we oppose the concept, associated with discursive argumentation, to the diagram which corresponds to intuition and imagination.
The beginning of the 20th century is also marked by formal innovations in literature, in particular by the use of visualization and spatialization procedures which aimed at renewing the modes of representation.
If we consider that these innovations are the product of diagrammatic imagination, then the latter can constitute a bridge between the scientific approach and the literary one.
The diagram is a visual representation of concepts or ideas and of the links between them.
It can also show the constitutive relationships of an object or the relationships between heterogeneous objects.
As diagram theorists have shown, the diagram is not an illustration: it is not a spatialized form of an idea that preexists it, but it brings the idea to life through visual representation.
The analogical relation with the object (iconic relation) on which the diagram is based on concerns the relations between the parts with each other and with the whole.
Furthermore, diagrams are characterized by their hybrid aspect which mixes text and image.
By taking as its object diagrammatic thinking, our project will analyse all configurations where text and image combine in graphic figures which maintain an iconic relationship with their object.
More broadly, we will make the hypothesis that diagrammatic imagination brings into play a series of tensions which partly cover the tensions between literature and science: tension between image and concept, between imagination and reason, between the intelligible and the sensitive, between the textual and the visual, between the visible and the readable, etc.
Our texts will therefore draw a synchronic corpus within the late XIXth-XXth century period which corresponds to the moment when the sciences, and mathematics in particular, were confronted with a crisis of representation which once again questioned the relationships between language and the world.
We will wonder how literary texts record and reflect this crisis, how they stage it with their own means, and how mathematical thinking can become the very origin of the creative process.
In the context of climate change, as droughts intensify and as more areas are subject to high water stress, this dissertation focuses on how to manage the imbalance between water resource availability and growing demand in two metropolises of the arid West of the United States.
Using an urban political ecology framework, the goal is to observe and analyze the power struggles between stakeholders involved in water resource management in a context where the system of large hydraulic infrastructures underpinning urban growth is increasingly called into question.
This mixed-methods survey brings together critical discourse analysis to deconstruct the dominant arguments and position-takings on water conservation, semi-structured interviews with water sector actors (institutions and environmental activists) and participant observation to question the tensions between discourses and changes in urban practices at the local level for adapting the urban metabolism to a world of less water.
This thesis shows, on the one hand, that adaptation strategies are implemented by dominant actors within the framework of socio-ecological fixes in order to maintain the growth trajectory of particularly attractive cities.
In this thesis, we introduce conversational implicatures intuitively using Jerry Hobbs's broad concept of granularity.
Then, we study conversational implicatures from an interdisciplinary perspective starting from its Gricean origins and moving into: sociology through politeness theory, inference through abduction and dialogue systems through speech act theory.
Finally, we develop the two lines of attack used in this thesis to study conversational implicatures: empirical analysis of a corpus of situated task-oriented conversation, and analysis by synthesis in the setup of a text-adventure game.
The increased availability of large amounts of data, from images in social networks, speech waveforms from mobile devices, and large text corpuses, to genomic and medical data, has led to a surge of machine learning techniques.
Such methods exploit statistical patterns in these large datasets for making accurate predictions on new data.
In recent years, deep learning systems have emerged as a remarkably successful class of machine learning algorithms, which rely on gradient-based methods for training multi-layer models that process data in a hierarchical manner.
These methods have been particularly successful in tasks where the data consists of natural signals such as images or audio; this includes visual recognition, object detection or segmentation, and speech recognition.
For such tasks, deep learning methods often yield the best known empirical performance; yet, the high dimensionality of the data and large number of parameters of these models make them challenging to understand theoretically.
Their success is often attributed in part to their ability to exploit useful structure in natural signals, such as local stationarity or invariance, for instance through choices of network architectures with convolution and pooling operations.
However, such properties are still poorly understood from a theoretical standpoint, leading to a growing gap between the theory and practice of machine learning.
This thesis is aimed towards bridging this gap, by studying spaces of functions which arise from given network architectures, with a focus on the convolutional case.
Our study relies on kernel methods, by considering reproducing kernel Hilbert spaces (RKHSs) associated to certain kernels that are constructed hierarchically based on a given architecture.
This allows us to precisely study smoothness, invariance, stability to deformations, and approximation properties of functions in the RKHS.
These representation properties are also linked with optimization questions when training deep networks with gradient methods in some over-parameterized regimes where such kernels arise.
They also suggest new practical regularization strategies for obtaining better generalization performance on small datasets, and state-of-the-art performance for adversarial robustness on image tasks.
A Software Product Line (SPL) manages commonalities and variability of a related software products family.
This approach is characterized by a systematic reuse that reduces development cost and time to market and increases software quality.
However, building an SPL requires an initial expensive investment.
However, the efficiency of this practice degrades proportionally to the growth of the family of products in concern, that becomes difficult to manage.
In this dissertation, we propose a hybrid approach that utilizes both SPL and C&amp;O to develop and evolve a family of software products.
The developer can then reduce these possibilities by expressing her preferences (e.g. products, artifacts) and using the proposed cost estimations on the operations.
We realized our approach by developing SUCCEED, a framework for SUpporting Clone-and-own with Cost-EstimatEd Derivation. We validate our works on a case study of families of web portals.
The area of multimodal interaction has expanded rapidly since the seminal "Put that there" demonstrator of R. Bolt that combines speech and gesture.
In parallel with the development of the Graphical User Interface technology, natural language processing and gesture recognition have made significant progress. In addition, recent interaction paradigms such as tangible and embodied user interfaces as well as augmented reality open a vast world of possibilities for interaction modalities.
In this thesis we address this problem of design and development for input multimodal interfaces (from the user to the system).
We describe a conceptual model of multimodality as a unified framework for modalities and combinations of modalities.
Based on this conceptual model, we define a generic component-based approach called ICARE which allows the easy and rapid design, development and maintenance of multimodal interfaces.
We have developed the ICARE tool to prove the usefulness of our component-based approach.
The ICARE tool is a graphical platform that enables the designer/developer to graphically manipulate and assemble ICARE software components in order to specify the multimodal interaction.
Routing delivery vehicles in dynamic and uncertain environments like dense city centers is a challenging task, which requires robustness and flexibility.
Their application to more complex problems has been facilitated by recent progresses in Deep Neural Networks, which can learn to represent a large class of functions in high dimensional spaces to approximate solutions with high performances.
To address DS-VRPs, we then introduce a new sequential multi-agent model we call sMMDP.
This fully observable model is designed to capture the fact that consequences of decisions can be predicted in isolation.
However, the implementation of this technology in real applications is hampered by the great degradation of performances in presence of acoustic nuisances.
A lot of effort has been invested by the research community in the design of nuisance compensation techniques in the past years.
These algorithms operate at different levels: signal, acoustic parameters, models or scores.
In order to implement this methodology, pairs of clean and corrupted data are artificially generated then used to develop nuisance compensation algorithms.
This method avoids making complex derivations and approximations.
The second class of techniques does not use any distortion model in the i-vectors domain.
Experiments are carried-out on noisy data and short utterances ; artificially corrupted NIST SRE 2008 data and natural SITW (short / noisy segments).
Even with the recent switch to 280 characters, Twitter messages considered in their singularity, without any additional exogenous information, can confront their readers with difficulties of interpretation.
The integration of contextualization on these messages is therefore a promising avenue of research to facilitate access to their information content.
In the last decade, most works have focused on building summaries from complementary sources of information such as Wikipedia.
In this thesis, we choose a different complementary path that relies on the analysis of conversations on Twitter in order to extract useful information for the contextualization of a tweet.
These information were integrated in a prototype which, for a given tweet, offers a visualization of a subgraph of the conversation graph associated with the tweet.
This subgraph, automatically extracted from the analysis of structural indicators distributions, allows to highlight particular individuals who play a major role in the conversation and tweets that have contributed to the dynamics of exchanges.
This prototype was tested on a panel of users to validate its efficiency and open up prospects for improvement.
The motivating theme behind corrupt feedback is that the feedback the learner receives is a corrupted form of the corresponding reward of the selected arm.
We consider two goals for the MAB problem with corrupt feedback: best arm identification and exploration-exploitation.
This thesis aims to detect figurative language devices in social networks.
We focus in particular on irony and sarcasm in Twitter and propose an approach based on supervised learning to predict if a tweet is ironic or not.
The obtained results for this extremely complex task are very encouraging and will allow a significant improvement of polarity detection in sentiments analysis.
Academic publishing in specialized journals and conference proceedings is the main way to communicate progress in science.
The underlying editorial and program committees represent the cornerstone of the evaluation process.
With the development of journals and the increasing number of scientific conferences held annually, searching for experts who would serve in these committees is a time-consuming and yet critical activity.
This PhD thesis focuses on the task of suggesting program committee (PC) members for scientific conferences.
It is organized into three parts.
First, we propose a modelling of the multifaceted scientific expertise of researchers based on a weighted heterogeneous graph.
Second, we define scientometric indicators to quantify the criteria involved in the composition of CPs.
Third, we design a CP member suggestion approach for a given conference, combining the results of the aforementioned scientometric indicators.
Our approach is experimented in the context of leading conferences of our research community: SIGIR, considering its editions from 1971 to 2015, and topically close conferences.
In human-agent interaction the engagement of the user is an essential aspect to complete the goal of the interaction.
In this thesis we study how the user's engagement could be favoured by the agent's behaviour.
Based on the outcomes of the latter study we propose an engagement-driven Topic Manager (computational model) that personalises the topics of an interaction in human-agent information-giving chat.
The Topic Selection component of the Topic Manager decides what the agent should talk about and when.
For this it takes into account the agent's dynamically updated perception of the user as well as the agent's own mental state.
The Topic Transition component of the Topic Manager, based upon an empirical study, computes how the agent should introduce the topics in the ongoing interaction without loosing the coherence of the interaction.
We implemented and evaluated the Topic Manager in a conversational virtual agent that plays the role of a visitor in amuseum.
Since the first sequencing of the human genome in the early 2000s, large endeavours have set out to map the genetic variability among individuals, or DNA alterations in cancer cells.
They have laid foundations for the emergence of precision medicine, which aims at integrating the genetic specificities of an individual with its conventional medical record to adapt treatment, or prevention strategies.
Translating DNA variations and alterations into phenotypic predictions is however a difficult problem.
DNA sequencers and microarrays measure more variables than there are samples, which poses statistical issues.
The data is also subject to technical biases and noise inherent in these technologies.
Finally, the vast and intricate networks of interactions among proteins obscure the impact of DNA variations on the cell behaviour, prompting the need for predictive models that are able to capture a certain degree of complexity.
This thesis presents novel methodological contributions to address these challenges.
First, we define a novel representation for tumour mutation profiles that exploits prior knowledge on protein-protein interaction networks.
For certain cancers, this representation allows improving survival predictions from mutation data as well as stratifying patients into meaningful subgroups.
Second, we present a new learning framework to jointly handle data normalisation with the estimation of a linear model.
Our experiments show that it improves prediction performances compared to handling these tasks sequentially.
Finally, we propose a new algorithm to scale up sparse linear models estimation with two-way interactions.
The obtained speed-up makes this estimation possible and efficient for datasets with hundreds of thousands of main effects, thereby extending the scope of such models to the data from genome-wide association studies.
The thesis deams with "pronominal" verbs in french, english and german.
In part i an evaluation of previous studies in done from classical studies related to terminology, grammar and the typology of pronominal verbs, to works in the field of natural language processing and computer-aided translation.
Tests and transformations are applied to these contexts to identity syntactic similarities.
In part 3 the resulting typology is described, providing a basis for the formalization phase prior to an implementation on sygmart, a tree-transfer system.
Finally a synchronic investigation leads to the conclusion that dual interpretation (reflexive and reciprocal) of some pronominal contexts can not be disambiguated unless a comprehensive knowledge basis can be integrated to the chain
This thesis aims to analyse the effects that a plurilingual course on line may have on the acquisition of language skills, especially the acquisition of a target language (French) by school students in Barcelona.
In order to test the hypothesis that such a course may enhance students'skills both in the languages of the training course and in French, the chosen methodology implied the drafting of an (almost) experimental protocol designed for the students of a school in Barcelona.
This protocol involved having a group of students participate in one of the training sessions set up on the Galanet platform, integrating it to their school teachings and including the drawing-up of different tests carried out before and after the course.
Moreover, each student had to describe their language profile at the beginning of the training.
Each test contains the same type of exercises, focusing on the same skills, and for each situation (either initial or final), there is one test on documents written in French and another on documents (written) in other romance languages (Italian, Portuguese, Romanian and Occitan - from the Vivarois region-).
The skills'targets are as follows: textual coherence/cohesion, global and accurate comprehension, the identification of the degree of perception of interculturality, translation/reformulation, segmentation and the identification/ implementation of a speech model.
All the data collected during the tests have been subject to both quantitative and qualitative analyses, and complemented by statistic processing, especially when correlations had to be made.
At the same time the semi-automatic processing of the corpus of messages that was carried out using the Calico platform helped determine the degree of intercomprehensive interactivity (taking into account the code-switching in a bi/plurilingual context) of the messages posted on the forum during the plurilingual training course.
This twofold analysis, based on concrete experiential data made it possible to conduct a case study and to determine, through the longitudinal study of their different results, if undergoing a training course in intercomprehension in a satisfactory way in terms of participation has a positive influence on the acquisition of the skills assessed by the tests, on which particular skill and how.
Students'questions are useful for their learning and for teachers'pedagogical adaptation.
We address this issue mainly in the context of a hybrid training program in which students ask questions online each week, using a flipped classroom approach, to help teachers prepare their on-site Q&amp;A session.
Our objective is to support the teacher to determine the types of questions asked by different groups of learners.
To conduct this work, we developed a question coding scheme guided by student's intention and teacher's pedagogical reaction.
Several automatic classification tools have been designed, evaluated and combined to categorize the questions.
We have shown how a clustering-based model built on data from previous sessions can be used to predict students'online profiles using exclusively the nature of the questions they ask.
These results allowed us to propose three alternative questions' organizations to teachers (based on questions' categories and learners' profiles), opening up perspectives for different pedagogical approaches during Q&amp;A sessions.
We have tested and demonstrated the possibility of adapting our coding scheme and associated tools to the very different context of a MOOC, which suggests a form of genericity in our approach.
Autonomous vehicles represent highly complex systems, where multiple types of failures could occur leading to a wrong execution on the road.
Therefore, each component should be thoroughly tested to anticipate potential failures and mitigate them.
Simulation testing methods are used to complement real test driving in the validation process.
The main contributions of this PhD thesis are threefold: detecting a maximum number of failures of the command law, detecting scenarios as close as possible to the border separating zones of failed and safe scenarios, and building explainable border models to identify the border as accurately as possible.
The algorithms of the first two objectives use machine learning (Random Forest) and optimization (CMA-ES) techniques to abide with the industrial requirement of reducing the computing power needed, and three approaches are considered to build border models while comparing their performances and explainabilities: Neural Networks, Mixed-Integer Linear Programming (MILP), and Genetic Programming (GP) applied to symbolic regression.
The work developed in this PhD thesis is focused on video sequence analysis.
The latter consists of object detection, categorization and tracking.
The development of reliable solutions for the analysis of video sequences opens new horizons for several applications such as intelligent transport systems, video surveillance and robotics.
In this thesis, we put forward several contributions to deal with the problems of detecting and tracking multi-objects on video sequences.
The proposed frameworks are based on deep learning networks and transfer learning approaches.
In a first contribution, we tackle the problem of multi-object detection by putting forward a new transfer learning framework based on the formalism and the theory of a Sequential Monte Carlo (SMC) filter to automatically specialize a Deep Convolutional Neural Network (DCNN) detector towards a target scene.
In a second contribution, we propose an original multi-object tracking framework based on spatio-temporal strategies (interlacing/inverse interlacing) and an interlaced deep detector, which improves the performances of tracking-by-detection algorithms and helps to track objects in complex videos (occlusion, intersection,strong motion).
In a third contribution, we provide an embedded system for traffic surveillance, which integrates an extension of the SMC framework so as to improve the detection accuracy in both day and night conditions and to specialize any DCNN detector forboth mobile and stationary cameras.
Throughout this report, we provide both quantitative and qualitative results.
On several aspects related to video sequence analysis, this work outperformsthe state-of-the-art detection and tracking frameworks.
In addition, we havesuccessfully implemented our frameworks in an embedded hardware platform forroad traffic safety and monitoring.
Although a considerable body of research work has addressed the problem of ontology matching, few studies have tackled the large ontologies used in the biomedical domain.
This approach integrates a novel partitioning algorithm as well as a set of matching learning techniques.
The partitioning method is based on hierarchical clustering and does not generate isolated partitions.
In addition, focusing on context-aware local training sets based on local feature selection and resampling techniques significantly enhances the obtained results.
Given the ambiguities created by the anaphora at the natural languages, many researchers in the field of natural language processing (NLP) have implemented various approaches to solving the problem.
We propose to adapt these approaches to the automatic pronominal anaphora resolution in RESUMAN.
In the first part presents the cognitive models, linguistic and textual that reflect the operation and interpretation of pronouns.
In the second part, the procedures for interpretation of pronominal anaphora are on display: the minimum distance of procedure, the procedure parallel functions, the procedure of the subject or the theme proceedings and procedures morphological analysis, semantics and pragmatics.
In the third part, a new version of algorithm, based on a statistical approach, is presented.
The RESUMAN software improves the performance of a TAL in case of dense texts anaphora.
The performance of this software are evaluated and limits are discussed.
Comparative genomics is as a fundamental discipline to unravel evolutionary biology.
To overcome a mere descriptive knowledge of it the first challenge is to develop a higher-level description of the content of a genome.
Therefore we used the modular representation of genomes to explore quantitative laws that regulate how genomes are built from elementary functional and evolutionary ingredients.
The first part sets off from the observation that the number of domains sharing the same function increases as a power law of the genome size.
Since functional categories are aggregates of domain families, we asked how the abundance of domains performing a specific function emerges from evolutionary moves at the family level.
We found that domain families are also characterized by family-dependent scaling laws.
The second chapter provides a theoretical framework for the emergence of shared components from dependency in empirical component systems with non-binary abundances.
The ensemble of resulting realizations reproduces both the distribution of shared components and the law for the growth of the number of distinct families with genome size.
The last chapter extends the component systems approach to microbial ecosystems.
Using our findings about families scaling laws, we analyzed how the abundance of domain families in a metagenome is affected by the constraint of power-law scaling of family abundance in individual genomes.
The result is the definition of an observable, whose functional form contains quantitative information on the original composition of the metagenome.
In the last years, the amount of available data on the social Web has exploded.
For the average user, it became hard to find quality content without being overwhelmed with publications.
For service providers, the scalability of such services became a challenging task.
The aim of this thesis is to achieve a better user experience by offering the filtering and recommendation features.
Filtering consists to provide for a given user, the ability of receiving only a subset of the publications from the direct network.
Where recommendation allows content discovery by suggesting relevant content producers on given topics.
We developed MicroFilter, a scalable filtering system able to handle Web-like data flows and RecLand, a recommender system that takes advantage of the network topology as well as the content in order to provide relevant recommendations.
This thesis focuses on acoustic and prosodic (fundamental frequency (F0), duration, intensity) analyses of French from large-scale audio corpora portraying different speaking styles: prepared and spontaneous speech.
We are interested in particularities of segmental phonetics and prosody that may characterize pronunciation.
Automatic classification (AC) was conducted to discriminate homophones by only acoustic and prosodic properties depending on their part-of-speech function or their position within prosodic words.
Results from AC of two homophone pairs, et/est (and/is) and à/a (ton/has), revealed that the et/est pair was more discriminable.
A selection of prosodic and inter-phoneme attributes, that is 15 attributes, performed as good results as with 62 attributes.
Then corresponding perceptual tests have been conducted to verify if humans also use acoustico-prosodic parameters for the discrimination.
Results suggested that acoustic and prosodic information might help in operating the correct choice in similar ambiguous syntactic structures.
From the hypothesis that pronunciation variants were due to varying prosodic constraints, we examined overall prosodic properties of French on a lexical and phrase level.
The comparison between lexical and grammatical words revealed F0 rise and lengthening at the end of final syllable on lexical words, while these phenomena were not observed for grammatical words.
Analyses also revealed that the mean profile of a n length noun phrase could be different from that of a n length noun with a low F0 at the beginning of a noun phrase.
The prosodic profiles can be helpful to locate word boundaries.
Findings in this thesis will lead to localize focus and named-entity using discriminative classifiers, and to improve word boundary locations by an ASR post-processing step.
Recently, malware, short for malicious software has greatly evolved and became a major threat to the home users, enterprises, and even to the governments.
To mitigate this problem, malware researchers have proposed various data mining and machine learning approaches for detecting and classifying malware samples according to the their static or dynamic feature set.
Although the proposed methods are effective over small sample set, the scalability of these methods for large data-set are in question.
Moreover, it is well-known fact that the majority of the malware is the variant of the previously known samples.
Consequently, the volume of new variant created far outpaces the current capacity of malware analysis.
Thus developing malware classification to cope with increasing number of malware is essential for security community.
The key challenge in identifying the family of malware is to achieve a balance between increasing number of samples and classification accuracy.
To achieve our goal, firstly we developed a portable, scalable and transparent malware analysis system called VirMon for dynamic analysis of malware targeting Windows OS.
Secondly we set up a cluster of five machines for our online learning framework module (i.e. Jubatus), which allows to handle large scale of data.
This configuration allows each analysis machine to perform its tasks and delivers the obtained results to the cluster manager.
Essentially, the proposed framework consists of three major stages.
The first stage consists in extracting the behavior of the sample file under scrutiny and observing its interactions with the OS resources.
At this stage, the sample file is run in a sandboxed environment.
Our framework supports two sandbox environments: VirMon and Cuckoo.
During the second stage, we apply feature extraction to the analysis report.
The label of each sample is determined by using Virustotal, an online multiple anti-virus scanner framework consisting of 46 engines.
Then at the final stage, the malware dataset is partitioned into training and testing sets.
The thesis objective is to design and build a high quality Hidden Markov Model (HMM-)based Text-To-Speech (TTS) system for Vietnamese – a tonal language.
The system is called VTED (Vietnamese TExt-tospeech Development system).
In view of the great importance of lexical tones, a “tonophone” – an allophone in tonal context – was proposed as a new speech unit in our TTS system.
A new training corpus, VDTS (Vietnamese Di-Tonophone Speech corpus), was designed for 100% coverage of di-phones in tonal contexts (i.e. di-tonophones) using the greedy algorithm from a huge raw text.
A total of about 4,000 sentences of VDTS were recorded and pre-processed as a training corpus of VTED.
In the HMM-based speech synthesis, although pause duration can be modeled as a phoneme, the appearanceof pauses cannot be predicted by HMMs.
Lower phrasing levels above words may not be completely modeled with basic features.
This research aimed at automatic prosodic phrasing for Vietnamese TTS using durational clues alone as it appeared too difficult to disentangle intonation from lexical tones.
Syntactic blocks, i.e. syntactic phrases with a bounded number of syllables (n), were proposed for predicting final lengthening (n = 6) and pause appearance (n = 10).
Improvements for final lengthening were done by some strategies of grouping single syntactic blocks.
The quality of the predictive J48-decision-tree model for pause appearance using syntactic blocks combining with syntactic link and POS (Part-Of-Speech) features reached F-score of 81.4% Precision=87.6%, Recall=75.9%), much better than that of the model with only POS (F-score=43.6%)or syntactic link (F-score=52.6%) alone.
The architecture of the system was proposed on the basis of the core architecture of HTS with an extension of a Natural Language Processing part for Vietnamese.
Pause appearance was predicted by the proposed model.
Contextual feature set included phone identity features, locational features, tone-related features, and prosodic features (i.e. POS, final lengthening, break levels).
Mary TTS was chosen as a platform for implementing VTED.
In the MOS (Mean Opinion Score) test, the first VTED, trained with the old corpus and basic features, was rather good, 0.81 (on a 5 point MOS scale) higher than the previous system – HoaSung (using the non-uniform unit selection with the same training corpus); but still 1.2-1.5 point lower than the natural speech.
The quality of the final VTED, trained with the new corpus and prosodic phrasing model, progressed by about 1.04 compared to the first VTED, and its gap with the natural speech was much lessened.
In the tone intelligibility test, the final VTED received a high correct rate of 95.4%, only 2.6% lower than the natural speech, and 18% higher than the initial one.
The error rate of the first VTED in the intelligibility test with the Latin square design was about 6-12% higher than the natural speech depending on syllable, tone or phone levels.
The final one diverged about only 0.4-1.4% from the natural speech.
The interest in hyperspectral image data has been constantly increasing during the last years.
Indeed, hyperspectral images provide more detailed information about the spectral properties of a scene and allow a more precise discrimination of objects than traditional color images or even multispectral images.
In this thesis, we are mainly interested in the reduction and partitioning of hyperspectral images of high spatial dimension.
The proposed approach consists essentially of two steps: features extraction and classification of pixels of an image.
A new approach for features extraction based on spatial and spectral tri-occurrences matrices defined on cubic neighborhoods is proposed.
A comparative study shows the discrimination power of these new features over conventional ones as well as spectral signatures.
Concerning the classification step, we are mainly interested in this thesis to the unsupervised and non-parametric classification approach because it has several advantages: no a priori knowledge, image partitioning for any application domain, and adaptability to the image information content.
A comparative study of the most well-known semi-supervised (knowledge of number of classes) and unsupervised non-parametric methods (K-means, FCM, ISODATA, AP) showed the superiority of affinity propagation (AP).
Secondly, the partitioning of large size hyperspectral images is hampered by its quadratic computational complexity.
To overcome these two drawbacks, we propose an approach which consists of reducing the number of pixels to be classified before the application of AP by automatically grouping data points with high similarity.
We also introduce a step to optimize the preference parameter value by maximizing a criterion related to the interclass variance, in order to correctly estimate the number of classes.
The proposed approach was successfully applied on synthetic images, mono-component and multi-component and showed a consistent discrimination of obtained classes.
It was also successfully applied and compared on hyperspectral images of high spatial dimension (1000 × 1000 pixels × 62 bands) in the context of a real application for the detection of invasive and non-invasive vegetation species.
In French, accentuation is said to be post-lexical, marking the phrase rather than the word.
That is, the primary final accent (FA) is considered to be perceptively weakened when co-occurring with a major prosodic boundary, while the Initial Accent (IA), regarded as a secondary and optional accent, is thought to hold merely a rhythmic function in balancing longer constituents.
The existence of a third level, the Intermediate Phrase (ip), while advanced by some authors, remains controversial.
The aim of our study is to investigate the organization of prosodic phrasing in French.
We propose a perception study on a corpus in which syntactically ambiguous structures were manipulated, and asked 80 participants to perform 3 distinct perception tasks: a prominence, boundary and grouping task.
The perceived prosodic events were then related to their acoustic realization.
Taken together, our results indicate that listeners are able to distinguish finer-grained grouping levels than those predicted in traditional French descriptions.
Moreover, lexical words are systematically realized by an accentual bipolarization (IA+FA), with each accent carrying the same metrical weight.
The function of IA is shown to be more one of structuration than rhythmic balancing, with IA even marking structure more readily than FA.
Finally, our results indicate that FA is not perceptively weakened when co-occurring with major prosodic boundaries, but instead remains a metrical mark at the level of the lexical word, in a manner independent from the level of constituency.
In order to optimize the existing language, we set out to assess the appropriate levels of simplification that would achieve more accurate and faster comprehension with minimum pilot training.
We first delved into the controlled language domain to form an overview of the existing controlled languages, their context, and rules.
From this research we attempted to find solutions for optimization, but at the same time we strove to offer an original contribution to the field through this work.
Conversational virtual agents with social behavior are often based on at least two different disciplines: computer science and psychology.
In most cases, psychological findings are converted into computational mechanisms in order to make agents look and behave in a believable manner.
More precisely, we are interested in task-oriented conversational agents, which are used as a custumer-relationship channel to respond to users request.
We propose an affective model of emotional responses' generation and control during a task-oriented interaction.
Our proposed model is based, on one hand, on the theory of Action Tendencies (AT) in psychology to generate emotional responses during the interaction.
This model has been implemented in an agent architecture endowed with a natural language processing engine developed by the company DAVI.
In order to confirm the relevance of our approach, we realized several experimental studies.
In the second, we studied the impact of different emotional regulation strategies on the agent perception by the user.
Finally, the third study focuses on the evaluation of emotional agents in real-time interactions.
Our results show that the regulation process contributes in increasing the credibility and perceived competence of agents as well as in improving the interaction.
Our results highlight the need to take into consideration of the two complementary emotional mechanisms: the generation and regulation of emotional responses.
They open perspectives on different ways of managing emotions and their impact on the perception of the agent.
In this thesis I investigate both in a theoretical and historical perspective the occurrence of macroeconomic crisis.
Building on narrative economics, text mining techniques and complex system theories I provide evidence of the mechanisms driving systemic crisis and show that rather than the nature of the shocks it is the architecture of the economic system that creates the vulnerability of economies and the ingredients for large scale crisis.
This chapter use a combination of Natural Language Processing(NPL) techniques and narrative approach to create a structured data, reflecting the macroeconomic outlook of IMF members since the early 1950s.
Using a combination of supervised and unsupervised learning techniques we create a large training sample of around 30,000 observations from IMF reports.
This training sample characterized each documents in around 20economically relevant crisis categories, from Sovereign default, currency crisis or banking crisis to epidemic outbreaks or violent conflicts.
The database and quantitative tools will be available in a R package (shortly available in github).
Building on narrative economics, complex system theory and ecological models of resilience I show that macroeconomic systems are systems in perpetual disequilibrium with recurrent crisis outbreaks (the triggers) that heterogeneously affects the core of the system, (the spreaders) and eventually the critical nodes (the hotspots) in a dynamic similar to wildfire forest.
In a third chapter I take a narrower perspective by considering the episodes looking at the experiences when the IMF intervened with a lending arrangements.
In this Chapter entitle "The Instruments of the Lender of Last Resort: curing the fundamentals or calming the panic?" I provide an historical overview of 70 years of intervention of the IMF to provide assistance to distress sovereign.
In a fourth chapter I provide evidence of the Taylor rule of the IMF when designing the lending arrangements to provide assistance to distress countries.
This work provide new lights on the occurrence of systemic crisis and on the dynamic of the global perturbations that some unexpected events can generate.
This work try to bring at the center of the analysis the narrative nature of economic crisis and the importance of the architecture of the system of crisis and intend to drag the attention of the importance of the finding the right balance between economic development, complexity of economic systems and severity of crisis.
In a context of a very fragile and vulnerable international economic system combined with the rising emergence of non economic- and ecologically critical crisis, particular attention should be given to the development of new standards of crisis manage-ment targeted at reducing the intensity of the crisis at the same time of mitigating the vulnerability due to the interconnection of economic sectors, increasing the self-resilience of the different economic nodes, and isolating the more critical elements of the system to the avoid systemic wide spread economic collapses
Stories are a communication tool that allow people to make sense of the world around them.
It represents a platform to understand and share their culture, knowledge and identity.
Stories carry a series of real or imaginary events, causing a feeling, a reaction or even trigger an action.
For this reason, it has become a subject of interest for different fields beyond Literature (Education, Marketing, Psychology, etc.) that seek to achieve a particular goal through it (Persuade, Reflect, Learn, etc.).
However, stories remain underdeveloped in Computer Science.
There are works that focus on its analysis and automatic production.
However, those algorithms and implementations remain constrained to imitate the creative process behind literary texts from textual sources.
Thus, there are no approaches that produce automatically stories whose 1) the source consists of raw material that passed in real life and 2) and the content projects a perspective that seeks to convey a particular message.
Working with raw data becomes relevant today as it increase exponentially each day through the use of connected devices.
Given the context of Big Data, we present an approach to automatically generate stories from ambient data.
The objective of this work is to bring out the lived experience of a person from the data produced during a human activity.
Any areas that use such raw data could benefit from this work, for example, Education or Health.
It is an interdisciplinary effort that includes Automatic Language Processing, Narratology, Cognitive Science and Human-Computer Interaction.
This approach is based on corpora and models and includes the formalization of what we call the activity récit as well as an adapted generation approach.
It consists of 4 stages: the formalization of the activity récit, corpus constitution, construction of models of activity and the récit, and the generation of text.
Each one has been designed to overcome constraints related to the scientific questions asked in view of the nature of the objective: manipulation of uncertain and incomplete data, valid abstraction according to the activity, construction of models from which it is possible the Transposition of the reality collected though the data to a subjective perspective and rendered in natural language.
We used the activity narrative as a case study, as practitioners use connected devices, so they need to share their experience.
The results obtained are encouraging and give leads that open up many prospects for research.
In this thesis we present a general framework, based on the object-oriented paradigm, for modeling and designing a model of speech data representation, and we propose a particular use of it for cineradiographic data, including sagittal views of the vocal tract, frontal pictures of the lips, and acoustic signals.
Indeed, the notion of a derived data model has been useful for users, to manage raw data and the results of data analysis in the same way.
Our investigation consisted in developing a data model to represent basic speech data entities used most often in the speech research community.
This model has been useful in managing all speech databases available at icp.
Data abstraction techniques such as inheritance were found to be essential for describing either primary data (x ray realizations and video realizations) or derived data and their components.
The tokens were pronounced by a female french speaker at the institute of phonetics in strasbourg.
We focused our study on anticipatory coarticulation, our aim is to contribute to understanding the programming unit in speech production.
We observed situations where vcv sequences were produced in accordance with the khozhevnikov &amp; chistovich model; other situations confirm the look-ahead model hypothesis, or the m. E. M (abry &amp; lallouache) model hypothesis, or the ohman's theory.
Of these four hypotheses, only the m. E. M model appears to be consistent these data.
Assuming that workplace significantly affects information seeking and information management patterns,this study explores accessibility and management of information sources among a group of research engineers.
The study explores how these engineers, who belong to the RandD entity of a major energy group,require, search and manage information sources in given professional contexts.
The study provides an analytic cartography of the various components of the organizational and informational environments whereby the activities and tasks of the above mentioned actors take place.
A wide range of practices has been identified via interviews but also through the activities and work rhythms observed.
Knowledge Base Population (KBP) is an important and challenging task specially when it has to be done automatically.
A Knowledge Base (KB) contains different entities, relationships among them and various properties of the entities.
Generally, relations are extracted based on the lexical and syntactical information at the sentence level.
However, global information about known entities has not been explored yet for RE task.
We propose to extract a graph of entities from the overall corpus and to compute features on this graph that are able to capture some evidence of holding relationships between a pair of entities.
In order to evaluate the relevance of the proposed features, we tested them on a task of relation validation which examines the correctness of relations that are extracted by different RE systems.
Experimental results show that the proposed features lead to outperforming the state-of-the-art system.
This thesis focuses on building semantic analysis models and tools for exploiting corpora of interviews carried out by a company specialized innovation projects support.
The interviews are analyzed to determine whether an innovation project meets various criteria of consumer acceptance.
Beyond the general difficulties of ambiguity in semantic analysis of opinions, the automatized analysis will have to be able to process contents that belong to a multitude of languages for specific purposes.
The models and tools will have to include general NLP processes (syntaxic and semantic analysis) and ontologies allowing the integration of various terminologies.
High-Performance Computing (HPC) platforms are growing in size and complexity.
In an adversarial manner, the power demand of such platforms has rapidly grown as well, and current top supercomputers require power at the scale of an entire power plant.
In an effort to make a more responsible usage of such power, researchers are devoting a great amount of effort to devise algorithms and techniques to improve different aspects of performance such as scheduling and resource management.
But HPC platform maintainers are still reluctant to deploy state of the art scheduling methods and most of them revert to simple heuristics such as EASY Backfilling, which is based in a naive First-Come-First-Served (FCFS) ordering.
Newer methods are often complex and obscure, and the simplicity and transparency of EASY Backfilling are too important to sacrifice.
At a first moment we explored Machine Learning (ML) techniques to learn on-line parallel job scheduling heuristics.
Using simulations and a workload generation model, we could determine the characteristics of HPC applications (jobs) that lead to a reduction in the mean slowdown of jobs in an execution queue.
Modeling these characteristics using a nonlinear function and applying this function to select the next job to execute in a queue improved the mean task slowdown in synthetic workloads.
When applied to real workload traces from highly different machines, these functions still resulted in performance improvements, attesting the generalization capability of the obtained heuristics.
At a second moment, using simulations and workload traces from several real HPC platforms, we performed a thorough analysis of the cumulative results of four simple scheduling heuristics (including EASY Backfilling).
We also evaluated effects such as the relationship between job size and slowdown, the distribution of slowdown values, and the number of backfilled jobs, for each HPC platform and scheduling policy.
We show experimental evidence that one can only gain by replacing EASY Backfilling with the Smallest estimated Area First (SAF) policy with backfilling, as it offers improvements in performance by up to 80% in the slowdown metric while maintaining the simplicity and the transparency of EASY.
SAF reduces the number of jobs with large slowdowns and the inclusion of a simple thresholding mechanism guarantees that no starvation occurs.
Overall we achieved the following remarks:
(i) simple and efficient scheduling heuristics in the form of a nonlinear function of the jobs characteristics can be learned automatically, though whether the reasoning behind their scheduling decisions is clear or not can be up to argument.
(ii) The area (processing time estimate multiplied by the number of processors) of the jobs seems to be a quite important property for good parallel job scheduling heuristics, since many of the heuristics (notably SAF) that achieved good performances have the job's area as input.
(iii) The backfilling mechanism seems to always help in increasing performance, though it does not outperform a better sorting of the jobs waiting queue, such as the sorting performed by SAF.
This thesis addresses the extraction of relational information from scientific documents in Life Sciences, i.e. transforming unstructured text into machine-readable structured information.
The extraction of specialized semantic relationships between entities detected in text makes explicit and formalizes the underlying structures.
Current state-of-the art methods rely on supervised machine learning.
Supervised learning, and even more so recent deep learning methods, require many training examples that are costly to produce, all the more in specific domains such as Life Sciences.
We hypothesize that combining information and knowledge available in specific domains with the latest deep learning word embedding models can offset the absence or limited amount of annotated training data.
For this purpose, the thesis will design a rich representation of texts that draws both from linguistic information obtained from syntactic parsing and domain knowledge obtained from knowledge graphs such as ontologies.
Integrating ontologies in the information extraction process will additionally facilitate information integration with other data, such as experimental or analytical data.
Information and communication technologies largely affect many branches of the law.
The contract law is not an exception and many contracts are now concluded online, regardless of the device used.
The use of this means of communication is not without influence on the perfection of the contract, in particular on the means of expression of the intention of the parties in the digital environment.
Indeed, the latter offers vast prospects in terms of instantaneousness, immateriality and automation of the expression of its contractual agreement, raising questions about the validity of contracts formed by electronic means.
Observing the practices that have taken place on the Internet, it is now possible to note the influence of digital means of communication on the expression of contractual agreement, that is to say on the individual intentions of the parties, as well as the meeting of these.
Individual intentions have thus been subjected to a process consisting of a series of obligatory steps, supposed to limit the cases in which the conclusion of the agreement would be an error.
However, this process opens the way to the automation of the expression of individual intentions and their meeting, announcing then the age of contracts concluded or even executed nearly instantaneously thanks to recent development in artificial intelligence applied to the legal field.
Localization is the process of determining the position of an entity in a local or global coordinate system.
The applications of localization are widely spread across different contexts.
For instance, in events, tracking the participants can save lives during crises.
In health-care, elderly people can be tracked to respond to their needs in critical situations like falling.
In warehouses, robots transferring products from one place to another require accurate knowledge of products'positions as well as other robots.
In industrial context of the factory of the future, localization is invaluable to achieve automated processes that are flexible enough to be reconfigured for various purposes.
Localization is considered a topic of high interest both in the academia and industry especially with the advent of 5G. The requirements of 5G pave the way for revolutionizing localization capabilities; Enhanced Mobile Broadband (eMBB) that is expected to reach 10 Gbits/s, Ultra-Reliable Low-Latency Communication (URLLC) which is less than 1 ms and massive Machine-Type Communication (mMTC) allowing to connect around 1 million devices per km.
In this work, we focus on two main types of localization; range-based localization and fingerprinting based localization.
In range-based localization, a network of devices with a maximum communication range estimate inter-distance values to their first-hop neighbors.
These distances along with knowledge of positions of few anchor nodes are used to localize other nodes in the network using a triangulation based solution.
The proposed method is capable of localizing ≈ 90% of nodes in a network with an average degree of ≈ 10.
In the second contribution, wireless channel responses, aka. Channel State Information (CSI) is used to estimate the position of a transmitter communicating with a MIMO antenna.
In this work, we apply classical learning techniques (K-nearest neighbors) and deep learning techniques (Multi-Layer Perceptron Neural Network and Convolutional Neural Networks) to localize a transmitter in indoor and outdoor contexts.
Our work achieved the first place in the indoor positioning competition prepared by IEEE's Communication Theory Workshop among 8 teams from highly reputable universities worldwide by achieving a Mean Square Error of 2.3 cm.
This thesis addresses the task of establishing adense correspondence between an image and a 3D object template.
We aim to bring vision systemscloser to a surface-based 3D understanding of objects by extracting information that is complementary to existing landmark- or partbased representations.
We use convolutional neural networks (CNNs) to densely associate pixels with intrinsic coordinates of 3D object templates.
Through the established correspondences we effortlessly solve a multitude of visual tasks, such as appearance transfer, landmark localization and semantic segmentation by transferring solutions from the template to an image.
We show that geometric correspondence between an imageand a 3D model can be effectively inferred forboth the human face and the human body.
This work represents one attempt to develop a computer aided diagnosis system for epilepsy lesion detection based on neuroimaging data, in particular T1-weighted and FLAIR MR sequences.
Given the complexity of the task and the lack of a representative voxel-level labeled data set, the adopted approach, first introduced in Azami et al., 2016, consists in casting the lesion detection task as a per-voxel outlier detection problem.
Manual features, designed to mimic the characteristics of certain epilepsy lesions, such as focal cortical dysplasia (FCD), on neuroimaging data, are tailored to individual pathologies and cannot discriminate a large range of epilepsy lesions.
Such features reflect the known characteristics of lesion appearance; however, they might not be the most optimal ones for the task at hand.
Our first contribution consists in proposing various unsupervised neural architectures as potential feature extracting mechanisms and, eventually, introducing a novel configuration of siamese networks, to be plugged into the outlier detection context.
The proposed system, evaluated on a set of T1-weighted MRIs of epilepsy patients, showed a promising performance but a room for improvement as well.
To this end, we considered extending the CAD system so as to accommodate multimodality data which offers complementary information on the problem at hand.
Our second contribution, therefore, consists in proposing strategies to combine representations of different imaging modalities into a single framework for anomaly detection.
The extended system showed a significant improvement on the task of epilepsy lesion detection on T1-weighted and FLAIR MR images.
Our last contribution focuses on the integration of PET data into the system.
Given the small number of available PET images, we make an attempt to synthesize PET data from the corresponding MRI acquisitions.
Eventually we show an improved performance of the system when trained on the mixture of synthesized and real images.
Analogical reasoning is recognized as a core component of human intelligence.
It has been extensively studied from philosophical and psychological viewpoints, but recent works also address the modeling of analogical reasoning for computational purposes, particularly focused on analogical proportions.
We are interested here in the use of analogical proportions for making predictions, in a machine learning context.
In recent works, analogy-based classifiers have achieved noteworthy performances, in particular by performing well on some artificial problems where other traditional methods tend to fail.
Starting from this empirical observation, the goal of this thesis is twofold.
The first topic of research is to assess the relevance of analogical learners on real-world, practical application problems.
The second topic is to exhibit meaningful theoretical properties of analogical classifiers, which were yet only empirically studied.
The field of application that was chosen for assessing the suitability of analogical classifiers in real-world setting is the topic of recommender systems.
A common reproach addressed towards recommender systems is that they often lack of novelty and diversity in their recommendations.
As a way of establishing links between seemingly unrelated objects, analogy was thought as a way to overcome this issue.
Experiments here show that while offering sometimes similar accuracy performances to those of basic classical approaches, analogical classifiers still suffer from their algorithmic complexity.
On the theoretical side, a key contribution of this thesis is to provide a functional definition of analogical classifiers, that unifies the various pre-existing approaches.
So far, only algorithmic definitions were known, making it difficult to lead a thorough theoretical study.
We were also able to identify a criterion that ensures a safe application of our analogical inference principle, which allows us to characterize analogical reasoning as some sort of linear process.
Our objective was to develop numerical quantitative biomarkers to characterize the geometrical organization of the four FN variants (that differ by the inclusion/exclusion of EDA/EDB) from 2D confocal microscopy images, and to compare sane and cancerous tissues.
First, we showed through two classification pipelines, based on curvelet features and deep learning framework, that the FN variants can be distinguished with a similar performance to that of a human annotator.
We constructed a graph-based representation of the fibers, which were detected using Gabor filters.
Graphspecific attributes were employed to classify the variants, proving that the graph representation embeds relevant information from the confocal images.
Furthermore, we identified various techniques capable to differentiate the graphs, allowing us to compare the FN variants quantitatively and qualitatively.
Performance analysis using toy graphs showed that the methods, which are based on graph matching and optimal transport, can meaningfully compare graphs.
Using the graph-matching framework, we proposed different methodologies for defining the prototype graph, representative of a certain FN class.
Additionally, the graph matching served as a tool to compute parameter deformation maps between the variants.
These deformation maps were analyzed in a statistical framework showing whether or not the variation of the parameters can be explained by the variance within the same class.
In our work, we investigate both aspects in proposing machine learning-based algorithms adapted to the different information sources that can be collected.
In terms of outdoor mobility, we use the collected GPS coordinate data to discover the daily mobility patterns of the users.
This clustering method is based on estimating probability densities of the trajectories, which alleviate the problems caused by the data noise.
By contrast, we utilize the collected WiFi fingerprint data to study indoor human mobility.
Moreover, as for accurate indoor location recognition, we presume that there exists a latent distribution governing the input and output at the same time.
Based on this assumption, we develop a variational auto-encoder (VAE)-based semi-supervised learning model.
In the unsupervised learning procedure, we employ a VAE model to learn a latent distribution of the input, the WiFi fingerprint data.
In the supervised learning procedure, we use a neural network to compute the target, the user coordinates.
Furthermore, based on the same assumption used in the VAE-based semi-supervised learning model, we leverage the information bottleneck theory to devise a variational information bottleneck (VIB)-based model.
This is an end-to-end deep learning model which is easier to train and has better performance.
The aim of contextual MT is to overcome this limitation by providing ways of integrating extra-sentential context into the translation process.
Context, concerning the other sentences in the text (linguistic context) and the scenario in which the text is produced (extra-linguistic context), is important for a variety of cases, such as discourse-level and other referential phenomena.
Successfully taking context into account in translation is challenging.
Evaluating such strategies on their capacity to exploit context is also a challenge, standard evaluation metrics being inadequate and even misleading when it comes to assessing such improvement in contextual MT.
In this thesis, we propose a range of strategies to integrate both extra-linguistic and linguistic context into the translation process.
We accompany our experiments with specifically designed evaluation methods, including new test sets and corpora.
Our contextual strategies include pre-processing strategies designed to disambiguate the data on which MT models are trained, post-processing strategies to integrate context by post-editing MT outputs and strategies in which context is exploited during translation proper.
We cover a range of different context-dependent phenomena, including anaphoric pronoun translation, lexical disambiguation, lexical cohesion and adaptation to properties of the scenario such as speaker gender and age.
Our experiments for both phrase-based statistical MT and neural MT are applied in particular to the translation of English to French and focus specifically on the translation of informal written dialogues.
In this work a framework to interface efficient probabilistic modeling for both the SLU and the DM modules is described and investigated.
Tractability is ensured by the use of an intermediate summary space.
Also to reduce the development cost of SDS an approach based on clustering is proposed to automatically derive the master-summary mapping function.
A implementation is presented in the Media corpus domain (touristic information and hotel booking) and tested with a simulated user.
The relatively recent presence of transnational migrants calls for a renewed look at the dynamics of identification between "indigenous" and "non-indigeneous" and, more particularly, its implications for education.
By giving disposable cameras to secondary school students in the town of Bozen-Bolzano, a city that seems to embody a certain hybridism, with the mission of photographing "the languages of the neighborhood" to create an exhibition, the goal is to investigate the ways young people interpret the plurality that surrounds them.
Providing the computational infrastucture needed to solve complex problems arising in modern society is a strategic challenge.
Organisations usually address this problem by building extreme-scale parallel and distributed platforms.
High Performance Computing (HPC) vendors race for more computing power and storage capacity, leading to sophisticated specific Petascale platforms, soon to be Exascale platforms.
These systems are centrally managed using dedicated software solutions called Resource and Job Management Systems(RJMS).
A crucial problem addressed by this software layer is the job scheduling problem, where the RJMS chooses when and on which resources computational tasks will be executed.
This manuscript provides ways to address this scheduling problem.
No two platforms are identical.
Indeed, the infrastructure, user behavior and organization's goals all change from one system to the other.
We therefore argue that scheduling policies should be adaptative to the system's behavior.
In this manuscript, we provide multiple ways to achieve this adaptativity.
Through an experimental approach, we study various trade-offs between the complexity of the approach, the potential gain, and the risks taken.
Our problematic concerns the links between logic and topoï, semantics and topoï and finally between logic and semantics.
The work therefore consists in highlighting the durability of the notion of topos, its evolution and its integration in logic, in semantics and even in computer science.
From then on, topoi, logic and semantics are the three facets of this problematic.
It also shows that this notion is present in all linguistic approaches and that semantics is the unifying axis of all linguistic research.
It is not our intention to give a new definition, but we have tried to summarize the main characteristics of the topoi and to draw up a table of past and current research developed on this notion.
The approach we have adopted is constructive and diachronic.
She highlighted the evolution of topoi, their forms, their senses and their uses.
The notion of topoi continues to exist from antiquity to the present day, since it has semantic foundations, which are inherent in all natural languages.
The first presents the terminological conception of the concept of topoi and its evolutionary aspect.
It highlights the different jobs, the different definitions and the evolutionary aspect of the topoï.
In this first part, we tried to sketch out the history of the notion of topoi, the use that has been made by logicians, rhetoricians, linguists, pragmatists and computer scientists.
The second is devoted to the presentation of the semantic foundations of logic and topoi and to the complementary relationship of these three notions, namely logic, semantics and topoi.
We have specified that the theory of topoi is a theory of meaning.
And the third part deals with the lexical foundations of the topoi and the modern exploitation of this notion, namely the automatic processing, the theory of the frames and the ontology, while presenting the different semantic argumentative theories.
We presented a small example, a sample extraction of information using the NooJ platform.
Natural language understanding often relies on common-sense reasoning, for which knowledge about semantic relations, especially between verbal predicates, may be required.
This thesis addresses the challenge of using a distibutional method to automatically extract the necessary semantic information for common-sense inference.
Typical associations between pairs of predicates and a targeted set of semantic relations (causal, temporal, similarity, opposition, part/whole) are extracted from large corpora, by exploiting the presence of discourse connectives which typically signal these semantic relations.
In order to appraise these associations, we provide several significance measures inspired from the literature as well as a novel measure specifically designed to evaluate the strength of the link between the two predicates and the relation.
The relevance of these measures is evaluated by computing their correlations with human judgments, based on a sample of verb pairs annotated in context.
We assess the potential of these representations for several applications.
Regarding discourse analysis, the tasks of predicting attachment of discourse units, as well as predicting the specific discourse relation linking them, are investigated.
Using only features from our resource, we obtain significant improvements for both tasks in comparison to several baselines, including ones using other representations of the pairs of predicates.
We also propose to define optimal sets of connectives better suited for large corpus applications by performing a dimension reduction in the space of the connectives, instead of using manually composed groups of connectives corresponding to predefined relations.
These diverse applications aim to demonstrate the promising contributions provided by our approach, namely allowing the unsupervised extraction of typed semantic relations.
This research deals with the study of a subset of expressive conversational formulas (FEC) (you speak, what good is it, it surprises me) apprehended from the angle of their analysis and description syntactic, semantic, pragmatic and discursive.
This study examines complex questions such as the terminology chosen, semantic values, syntactic behavior, the role of context or pragmatic workings.
We also propose to reflect on the status of FECs in lexicography, which allows them to better understand their description in relation to what general and specialized dictionaries offer.
We conducted this study in four stages.
First, on a theoretical level, we have presented a synthesis of existing work by showing the difficulties of a terminological and definitional nature linked to this linguistic phenomenon for which a delimitation is required.
For our part, we have chosen the term expressive formula and we have justified our choice through the selection of criteria allowing to define this notion.
Subsequently, from a methodological point of view, the study takes the approach of corpus linguistics in a qualitative and quantitative approach.
It is based on corpora considered in several settings: written (literature - tweets) and and oral (Orféo).
After presenting the theoretical and methodological framework, we proceeded to the concrete study through the discursive analysis of FECs in two different sub-corpora.
The primary objective of this study is to arrive at well-defined criteria limiting this subclass of pragmatic phraseologisms compared to the other subtypes proposed by researchers in this linguistic field.
Finally, we proposed the lexicographic treatment of certain formulas selected within the framework of the Polonium project.
The challenge here is to arrive at a functional lexicographic model for the lexicographic processing of FEC.We believe that we have succeeded in achieving the challenges that we set ourselves at the start.
Through a discursive analysis of the FECs in the text, we wanted to refine the defining features and opt for an exhaustive definition of this subset.
By examining their status in the field of lexicography, we have retained a detailed and promising descriptive method of FECs.
The population in cities is slated to double by mid-century according to estimates prepared by the World Health Organization.
This rapid increase in population will impact transportation and economic growth, and will increase responsibilities of local managing authorities and different stakeholders.
It is a need of the hour to convert cities into smart cities in order to provide new service to the public, by using available resources in an optimum manner.
From crowd-sourced data and open governmental data to other online sources, a variety of data sources can provide users with smart tools to efficiently manage their daily activities.
Moreover, with the advancement in Internet and mobile technologies, social networking platforms such as Facebook and Twitter have become popular modes of communication.
They allow users to share a spectrum of information, including spatio-temporal data, both publicly and within their community of interest in real-time.
Scrutinizing knowledge from different types of available, rich, geo-tagged, and crowd-sourced data and incorporating it on a map has become more feasible.
In this thesis, we first propose a constraint-aware route recommendation system in lack of physical infrastructure environment that leverages geo-tagged data in social media and user-generated content to identify upcoming traffic constraints and, thus, recommend an optimized path.
We have designed and developed a system using a spatial grid index to inform users about upcoming constraints and calculate a new, optimized path in minimal response time.
Later, the concept of “smart maps” will be introduced by collecting, managing, and integrating heterogeneous data sources in order to infer relevant knowledge-based layers.
Unlike conventional maps, smart maps extract information about live events (e.g., concert, competition, incidents, etc.), online offers, and statistical analysis (e.g., dangerous areas) by encapsulating incoming semi- and un-structured data into structured generic packets.
This methodology sets the ground for providing different intelligent services and applications.
Moreover, developing smart maps requires an efficient and scalable processing and the visualization of knowledge-based layers at multiple map scales, thus allowing a smooth and clutter-free browsing experience.
Finally, we introduce Hadath, a scalable and efficient system that extracts social events from a variety of unstructured data streams.
The system comprises a data wrapping component which digests different types of data sources, and prepossesses data to generate structured data packets out of unstructured streams.
As a result, live events can be displayed at different spatio-temporal resolutions, thus allowing a smooth and unique browsing experience.
Finally, to validate our proposed system, we conducted experiments on real-world data streams.
The final output of our system named Hadath creates a unique and dynamic map browsing experience
This dissertation addresses text-independent Automatic Speaker Verification (ASV) using features issued from Maximum Likelihood Linear Regression (MLLR) adaptation of Markov models with Gaussian mixture observation densities.
MLLR transform coefficients obtained by adaptation of a speaker-independent model to speech data capture relevant cues characterizing a speaker.
We focus on the MLLR-SVM paradigm classifying these features using Support Vector Machines (SVM).
We propose a purely acoustic approach which avoids the need for transcripts and structural language constraints of previous systems by using Constrained MLLR (CMLLR) transforms together with Speaker Adaptive Training (SAT) of a Universal Background Model (UBM).
We focus on multi-class (C)MLLR-SVM systems using LVCSR acoustic models.
We perform a comprehensive experimental study of adaptation schemes exploring multiple axes such as front-end type, transform type, number of transforms, model type or training method.
Why and how do large companies deal with customer complaints?
What effects does this treatment have on the internal regulation of firms?
What can the customer expect?
This thesis proposes to deal with this set of questions by an ethnographic survey conducted in two large French companies.
Based on the analytical tools developed by Albert O. Hirschman, it provides a historical and sociological description of the complaint-handling practices.
Thus, it wishes to contribute to the question of the influence of the client of a commodity on the companies that produce and sell it.
A case law is a corpus of judicial decisions representing the way in which laws are interpreted to resolve a dispute.
It is essential for lawyers who analyze it to understand and anticipate the decision-making of judges.
Its exhaustive analysis is difficult manually because of its immense volume and the unstructured nature of the documents.
The estimation of the judicial risk by individuals is thus impossible because they are also confronted with the complexity of the judicial system and language.
The automation of decision analysis enable an exhaustive extraction of relevant knowledge for structuring case law for descriptive and predictive analyses.
In order to make the comprehension of a case law exhaustive and more accessible, this thesis deals with the automation of some important tasks for the expert analysis of court decisions.
First, we study the application of probabilistic sequence labeling models for the detection of the sections that structure court decisions, legal entities, and legal rules citations.
Then, the identification of the demands of the parties is studied.
The proposed approach for the recognition of the requested and granted quanta exploits the proximity between sums of money and automatically learned key-phrases.
We also show that the meaning of the judges'result is identifiable either from predefined keywords or by a classification of decisions.
Finally, for a given category of demands, the situations or factual circumstances in which those demands are made, are discovered by clustering the decisions.
For this purpose, a method of learning a similarity distance is proposed and compared with established distances.
This thesis discusses the experimental results obtained on manually annotated real data.
Finally, the thesis proposes a demonstration of applications to the descriptive analysis of a large corpus of French court decisions.
Current mobile devices, and mobile phones in particular, are equipped with different wireless technologies that increase and diversify their communication capabilities.
The combined and effective use of these technologies offers various opportunities in terms of services and applications.
However, it requires detailed analysis in terms of security and choice of the communication mean to use according to context-dependent criteria: energy costs, financial costs, preferences of the involved entities, privacy issues, etc.
Our contribution to this project is the creation of collaborative applications adequately using the available wireless technologies on the considered equipments.
In other words, we try to use the most appropriate communication mean (according to the criteria listed above) that two or more mobile devices can use to perform exchanges (without considering their respective positions).
Then, the transparency of targets localization becomes a rule.
We can synthesize the central question that we have chosen to study in the following manner: how to allow a set of mobile terminals (mobile phones in particular) to securely communicate using the most appropriate technology depending on the context?
Our goal is to answer this question by defining a multilevel platform taking into account the different technologies available on the considered equipments.
It is necessaty to identify the elements to consider in the design of the platform, to model them, to develop reference applications and to validate the relevance of the proposed solutions with qualitative and quantitative evaluations.
This thesis proposes a new approach to scientific writings which takes discourse markers as starting point.
It is part of the framework of French for Academic Purposes.
In this work, we are particularly interested in multi-word discourse markers and we integrate them into a broader concept of phraseology.
The particularity of this work lies in linking linguistic descriptions of discourse markers and didactic transposition of these tokens with a corpus, which is still little discussed in the didactic francophone field.
We aim to meet two main objectives of linguistic and didactic nature.
The linguistic objectives are to set up a model for analyzing multi-word discourse markers that combines both syntactic and semantic properties and is totally reconfigurable to other discourse markers.
Linguistic analyses will then be used for the teaching/learning of these units.
For didactic purposes, this research aims to develop a methodology for teaching/learning discourse markers from the observation of the corpus.
Methodological considerations proposed in the framework of the thesis provide attractive ways for teaching/learning these language elements and for making access to the academic writings easier to non-native students.
People are at the center of many computer vision tasks, such as surveillance systems or self-driving cars.
They are also at the center of most visual contents, potentially providing very large datasets for training models and algorithms.
While stereoscopic data has been studied for long, it is only recently that feature-length stereoscopic ("3D") movies became widely available.
In this thesis, we study how we can exploit the additional information provided by 3D movies for person analysis.
We first explore how to extract a notion of depth from stereo movies in the form of disparity maps.
Leveraging the relative ease of the person detection task in 3D movies, we develop a method to automatically harvest examples of persons in 3D movies and train a person detector for standard color movies.
We formulate the segmentation problem as a multi-label Conditional Random Field problem, and our method integrates an occlusion model to produce a layered, multi-instance segmentation.
After showing the effectiveness of this approach as well as its limitations, we propose a second model which only relies on tracks of person detections and not on pose estimates.
We formulate our problem as a convex optimization one, with the minimization of a quadratic cost under linear equality or inequality constraints.
These constraints weakly encode the localization information provided by person detections.
This method does not explicitly require pose estimates or disparity maps but can integrate these additional cues.
Our method can also be used for segmenting instances of other object classes from videos.
We evaluate all these aspects and demonstrate the superior performance of this new method.
Equivalence relations between queries allow to group the participants into communities.
Those communities are then used as an abstraction to split the general organization problem into several easier and smaller subproblems.
In order to stay language-independent, the organization is based on a simple and modular API, that rely on a query answering using views mechanism, well known in databases.
Choice between the different rewritten queries is done using an adjustable cost model.
Relations between communities are thus materialized by a spreading mechanism, a participant from one community joining the other(s) to contribute.
This allows to avoid the capacities problem on the organization's abstract level, while efficiently taking care of it on the concrete one.
Inside the communities, all the participants receive the common results they need using a spanning tree.
The QTor approach, incrementally built, allows an efficient reduce of the processing and diffusion costs (processing cost being optimal in some cases, e.g. containment) with a reasonable latency, for a limited organization cost.
Experiments have shown that the organization is flexible, regarding both the expressed queries and the participants'capacities.
A demonstrator was built, allowing to both perform (automatic or interactive) simulations, and deploy the system over a real network, with a single.
Unsupervised information extraction in open domain gains more and more importance recently by loosening the constraints on the strict definition of the extracted information and allowing to design more open information extraction systems.
In this new domain of unsupervised information extraction, this thesis focuses on the tasks of extraction and clustering of relations between entities at a large scale.
The objective of relation extraction is to discover unknown relations from texts.
A relation prototype is first defined, with which candidates of relation instances are initially extracted with a minimal criterion.
To guarantee the validity of the extracted relation instances, a two-step filtering procedures is applied: the first step with filtering heuristics to remove efficiently large amount of false relations and the second step with statistical models to refine the relation candidate selection.
A multi-level clustering procedure is design, which allows to take into account the massive data and diverse linguistic phenomena at the same time.
First, the basic clustering groups similar relation instances by their linguistic expressions using only simple similarity measures on a bag-of-word representation for relation instances to form high-homogeneous basic clusters.
Second, the semantic clustering aims at grouping basic clusters whose relation instances share the same semantic meaning, dealing with more particularly phenomena such as synonymy or more complex paraphrase.
Different similarities measures, either based on resources such as WordNet or distributional thesaurus, at the level of words, relation instances and basic clusters are analyzed.
Moreover, a topic-based relation clustering is proposed to consider thematic information in relation clustering so that more precise semantic clusters can be formed.
Finally, the thesis also tackles the problem of clustering evaluation in the context of unsupervised information extraction, using both internal and external measures.
For the evaluations with external measures, an interactive and efficient way of building reference of relation clusters proposed.
The application of this method on a newspaper corpus results in a large reference, based on which different clustering methods are evaluated.
Linguistic analysis is a fundamental and essential step for natural language processing.
It often includes part-of-speech tagging and named entity identification in order to realize higher level applications, such as information retrieval, automatic translation, question answers, etc.
Since the word is the elementary unit for automated language processing, it is indispensable to segment sentences into words for Chinese language processing.
In most existing system described in the literature, segmentation, part-of-speech tagging and named entity recognition are often presented as three sequential, independent steps.
With these combinations of steps, segmentation can be improved by complementary information supplied by part-of-speech tagging and named entity recognition, and global analysis of Chinese improved.
Consequently, this approach is not suitable for creating multilingual automatic analysis systems.
This dissertation studies the integration Chinese automatic analysis into an existing multilingual analysis system LIMA.
Firstly, the treatment for Chinese should be compatible and follow the same flow as other languages.
And secondly, in order to keep the system coherent, it is preferable to employ common modules for all the languages treated by the system, including a new language like Chinese.
To respect these constraints, we chose to realize the phases of segmentation, part-of-speech tagging and named entity recognition separately.
Our modular treatment includes a specific module for Chinese analysis that should be reusable for other languages with similar linguistic features.
After error analysis of this purely modular approach, we were able to improve our segmentation with enriched information supplied by part-ofspeech tagging, named entity recognition and some linguistic knowledge.
In our final results, three specific treatments have been added into the LIMA system: a pretreatment based on a co-occurrence model applied before segmentation, a term tokenization relative to numbers written in Chinese characters, and a complementary treatment after segmentation that identifies certain named entities before subsequent part-of-speech tagging.
The system of determination of the French language differs from the system of determination in the Polish language.
The suggested descriptions are formalized in databases which are designed for automatic processing of natural languages.
In the thesis, the functioning and properties of determiners in the Polish language are systematically described.
Additionally their syntactic-semantic properties which results in the division into predicative determination and argumentative determination are considered.
The work contributes to the development of LDI electronic dictionaries.
To support health professionals in their clinical processes, several monitoring and medical care systems have been built and deployed in the hospital setting.
These systems are mainly used to collect medical data on patients,analyze and present the outcomes in different ways.
They represent support and assistance to health professionals in their decision making regarding the evolution in the health status of the patients followed.
The use of such systems always requires an adaptation to both the medical field and the mode of intervention.
It is necessary, in a hospital setting, to adapt and evolve these systems in a simple manner, limiting any corrective or evolutionary maintenance.
Moreover, these systems should be able to consider dynamically the domain knowledge from medical experts.
This approach allows especially the organization of the medical data collection by taking into account the patient¿s context, the ontology-based knowledge representation of the domain and permits the exploitation of the medical guidelines and the clinical experience.
In continuity of our research team¿s previous work, we chose to expand with our approach, the E-care platform which is dedicated to monitoring and early detection of any abnormality of the health status of patients with chronic diseases.
We were able to adapt easily the E-care platform for the various experiments that have been conducted,including EPHAD of the Mutualité Française in Anjou-Mayenne, Hautepierre hospital and Lausanne hospital (CHUV).
The outcomes of these experiments have shown the effectiveness of the proposed approach.
Where, the adaptation of the platform regarding to the domain and mode of intervention of each of these experiments is limited to the simple configuration.
Furthermore, the proposed approach has attracted the interest of the medical staff regarding the organization of the medical data collection, and the exploitation of the medical knowledge which brings assistance to the health professionals for better decision making.
At first, we detail a method based on a particle filter to estimate at any time, the position of the signer's head, hands, elbows and shoulders in a monoview video.
This method has been adapted to the French Sign Language in order to make it more robust to occlusion, inversion of the signer's hands or disappearance of hands from the video frame.
Then, we propose a classification of the motion patterns that are frequently involved in the sign of production, thanks to the analysis of motion capture data.
The parametric models associated to each sign pattern are used in the frame of automatic signe retrieval in a video from a filmed sign example.
Dependency parsing is an essential component of several NLP applications owing its ability to capture complex relational information in a sentence.
These systems require a significant amount of annotated data and are thus targeted toward specific languages for which this type of data are available.
Unfortunately, producing sufficient annotated data for low-resource languages is time- and resource-consuming.
To address the aforementioned issue, the present study investigates three bootstrapping methods, namely, (1) multi-lingual transfer learning, (2) deep contextualized embedding, and (3) Co-training.
Multi-lingual transfer learning is a typical supervised learning approach that can transfer dependency knowledge using multi-lingual training data based on multi-lingual lexical representations.
Deep contextualized embedding maximizes the use of lexical features during supervised learning based on enhanced sub-word representations and language model (LM).
Our approaches have the advantage of requiring only a small bilingual dictionary or easily obtainable unlabeled resources (e.g., Wikipedia) to improve parsing accuracy in low-resource conditions.
We evaluated our parser on 57 official CoNLL shared task languages as well as on Komi, which is a language we developed as a training and evaluation corpora for low-resource scenarios.
The evaluation results demonstrated outstanding performances of our approaches in both low- and high-resource dependency parsing in the 2017 and 2018 CoNLL shared tasks.
A survey of both model transfer learning and semi-supervised methods for low-resource dependency parsing was conducted, where the effect of each method under different conditions was extensively investigated.
Where they have to adapt to volatile customers who want to find cheaper products and services and that are more corresponding to their needs.
The SME is then confronted with problems of responsiveness and flexibility in responding to these customers.
As an effect, it seeks to reduce the costs and time to market and to provide high quality and innovative goods and services.
The SME's information system is an asset on which it can rely to implement this strategy and so to maximize its responsiveness and flexibility but also to reach the sought profitability and quality.
These are key qualities that guarantee autonomy and recognition, qualities that are highly needed by any SME.
Part of this information system is computerized.
It stores and processes the information needed by the different decision-making, business and support processes that serve the enterprise's strategy.
It is crucial to understand the features, interfaces and data that make up this computerized system and develop them according to the needs of SME.
The SME is therefore tempted to embark, alone or accompanied, in so-called computerization projects i.e. projects for the development or improvement of its computerized system.
We are interested in projects aimed at developing management applications of SMEs.
The SME – then assuming the role of project owner – along with the development team – supporting the role of project management – have to share a common vision of the computerization needs.
They are then called upon to carry out jointly requirements engineering (RE) activities.
RE guides the SMEs to be able to describe and formalize its needs.
It then allows the development team to specify more formally these needs as requirements which then define the required development work.
RE is often carried out with the assistance of project owner support.
This crucial step remains difficult for SMEs.
It is most often performed by the development team itself to address the lack of resources, time and skills of SMEs.
However, the involvement of the SME's members is vital to the success of any computerization project, especially if it permanently affects the functioning of the enterprise.
This work, developed through a collaborative with the company RESULIS, consisted in developing a requirements engineering method which offers SMEs concepts, simple languages, modeling and verification means that are easily and intuitively manipulated and provide sufficient and relevant formalization of the SME's requirements.
This method is based on principles derived from both enterprise modeling and systems engineering fields for requirements elicitation.
Semi-formal verification and validation means are applied to guarantee some expected qualities of the resulting requirements.
The method is also integrated in the model driven development cycle to enable a posteriori the production of prototypes and make interoperable the languages and tools used by both the SME and the development team.
Citizen science, in particular voluntary crowdsourcing, represents a little experimented solution to produce language resources for some languages which are still little resourced despite the presence of sufficient speakers online.
We present in this work the experiments we have led to enable the crowdsourcing of linguistic resources for the development of automatic part-of-speech annotation tools.
We have applied the methodology to three non-standardised languages, namely Alsatian, Guadeloupean Creole and Mauritian Creole.
For different historical reasons, multiple (ortho)-graphic practices coexist for these three languages.
The difficulties encountered by the presence of this variation phenomenon led us to propose various crowdsourcing tasks that allow the collection of raw corpora, part-of-speech annotations, and graphic variants.
The intrinsic and extrinsic analysis of these resources, used for the development of automatic annotation tools, show the interest of using crowdsourcing in a non-standardized linguistic framework: the participants are not seen in this context a uniform set of contributors whose cumulative efforts allow the completion of a particular task, but rather as a set of holders of complementary knowledge.
The resources they collectively produce make possible the development of tools that embrace the variation.
The platforms developed, the language resources, as well as the models of trained taggers are freely available.
Since its emergence in the early 1990s, the notion of ontology has been quickly distributed in many areas of research.
Given the promise of this concept, many studies focus on the use of ontologies in many areas like information retrieval, electronic commerce, semantic Web, data integration, etc..
The effectiveness of all this work is based on the assumption of the existence of a domain ontology that is already built an that can be used.
However, the design of such ontology is particularly difficult if you want it to be built in a consensual way.
If there are tools for editing ontologies that are supposed to be already designed, and if there are also several platforms for natural language processing able to automatically analyze corpus of texts and annotate them syntactically and statistically, it is difficult to find a globally accepted procedure useful to develop a domain ontology in a progressive, explicit and traceable manner using a set of information resources within this area.
The goal of ANR DaFOE4App (Differential and Formal Ontology Editor for Application) project, within which our work belongs to, was to promote the emergence of such a set of tools.
Unlike other tools for ontologies development, the platform DaFOE presented in this thesis does not propose a methodology based on a fixed number of steps with a fixed representation of theses steps.
Indeed, in this thesis we generalize the process of ontologies development for any number of steps.
The interest of such a generalization is, for example, to offer the possibility to refine the development process by inserting or modifying steps.
We may also wish to remove some steps in order to simplify the development process.
The aim of this generalization is for instance, for the overall process of ontologies development, to minimize the impact of adding, deleting, or modifying a step while maintaining the overall consistency of the development process.
To achieve this, our approach is to use Model Driven Engineering to characterize each step through a model and then reduce the problem of switching from one step to another to a problem of models transformation.
Established mappings between models are then used to semi-automate the process of ontologies development.
The originality of the MQL language lies in its ability, through queries syntactically compact, to explore the graph of mappings using the transitivity property of mappings when retrieving informations.
Language Recognition is the problem of discovering the language of a spoken definition utterance.
This thesis achieves this goal by using short term acoustic information within a GMM-UBM approach.
The main problem of many pattern recognition applications is the variability of problem in the observed data.
In the context of Language Recognition (LR), this troublesome variability is due to the speaker characteristics, speech evolution, acquisition and transmission channels.
In the context of Speaker Recognition, the variability problem is solved by solution the Joint Factor Analysis (JFA) technique.
Here, we introduce this paradigm to Language Recognition.
The second, more technical assumption consists in the unwanted variability part to be thought to live in a low-dimensional, globally defined subspace.
In this work, we analyze how JFA behaves in the context of a GMM-UBM LR framework.
We also introduce and analyze its combination with Support Vector
Machines(SVMs).The first JFA publications put all unwanted information (hence the variability) improvement into one and the same component, which is thought to follow a Gaussian distribution.
This handles diverse kinds of variability in a unique manner.
But in practice,we observe that this hypothesis is not always verified.
We have for example the case, where the data can be divided into two clearly separate subsets, namely data from telephony and from broadcast sources.
In this case, our detailed investigations show that there is some benefit of handling the two kinds of data with two separate systems and then to elect the output score of the system, which corresponds to the source of the testing utterance.
For selecting the score of one or the other system, we need a channel source related analyses detector.
We propose here different novel designs for such automatic detectors.
In this framework, we show that JFA's variability factors (of the subspace) can be used with success for detecting the source.
This opens the interesting perspective of partitioning the data into automatically determined channel source categories,avoiding the need of source-labeled training data, which is not always available.
The JFA approach results in up to 72% relative cost reduction, compared to the overall results GMM-UBM baseline system.
Using source specific systems followed by a score selector, we achieve 81% relative improvement.
The web explosion has led Information Retrieval (IR) to be extended and web search engines emergence.
The conventional IR methods, usually intended for simple textual searches, faced new documents types and rich and scalable contents.
The users, facing these evolutions, ask more for IR systems search results quality.
In this context, the personalization main objective is improving results returned to the end user based sing on its perception and its interests and preferences.
This thesis context is concerned with these different aspects.
Its main objective is to propose new and effective solutions to the personalization problem.
To achieve this goal, a spatial and semantic web personalization system integrating implicit user modeling is proposed.
This system has two components: 1/ user modeling; /2 implicit users'collaboration through the construction of a users'models network.
A system prototype was developed for the evaluation purpose that contains: a) user model quality evaluation; b) information retrieval quality evaluation; c) information retrieval quality evaluation with the spatial user model data; d) information retrieval quality evaluation with the whole user model data and the users'models network.
Experiments showed amelioration in the personalized search results compared to a baseline web search.
Semantic knowledge is mandatory for Natural Language Processing.
Unfortunately, classifications that have universal goals are an utopia.
There exists systems that extracts semantic knowledge from specialized texts but it is well known that it is not possible to do such an extraction from texts said to be of "general" language.
The goal of this doctoral dissertation is to show that this idea is false.
We show that a thematic analysis of non-specialized texts (newspapers, newswires or HTML pages gathered from the Web) usually allows to reduce the problem to a classical one where the analysis of a technical corpus is done, but where the human interventions are limited.
With our approach, the theme of text segments is detected by the statistical analysis of word distributions, designed notions of similarity and aggregation.
They allow to aggregate the words of similar segments to build thematic domains where higher weighted words describe the theme.
We then group the words that appear as the same argument of the same verb in the various text segments belonging to a theme.
We have implemented our model in a system called SVETLAN'which has been tested on several French and English million words corpus.
The empirical analysis of the results shows that, as anticipated, words are usually in a strong mutual semantic relation in the classes that are obtained, in the context determined by the theme.
This project aims modelling the acoustic-prosodic variations of short informational units in spontaneous speech, so to allow their classification in large semantic categories linked to their pragmatic use.
The approach is based on the Language into Act theory (henceforth L-AcT: Cresti 2000 ; Moneglia and Raso, 2014 ; Cavalcante 2016, 2020), in which the informational functions (inclusive illocutionary ones) of spontaneous speech interactions are essentially organized by prosody.
Speech is segmented into intonation units encapsulating sets of words into the same prosodic envelope.
This creates a contrast between the words of different prosodic envelopes: segmentation thus leads to a functional organization of speech (Barth-Weingarten, 2016; Barbosa and Raso, 2018; Izre'el et al., forthcoming-a; Izre'el et al., forthcoming-b).
The L-AcT proposes that there is an isomorphic relationship between intonation unit and informational unit: each intonation unit composing the utterance acquires informational value, with the exception of scanning units, produced voluntarily (to underline a point) or involuntarily (performance problems).
Syntactic compositionality is a property of informational units, which establish functional relationships guided by intonation contours, without syntactic compositionality playing a necessary role in this (Cresti 2014).
In the discourse flow, we distinguish between prosodic borders with terminal or non-terminal value, which can be automatically detected with good levels of recall and precision, based on their prosodic correlates (Teixeira, 2018; Teixeira, Barbosa and Raso, 2018; Raso, Teixeira and Barbosa, forthcoming).
The terminal boundaries mark the end of a sequence composed of intonative non-terminal units, marked by the non-terminal boundaries and delineating the informational units.
This thesis deals with the analysis of short informational units, i.e. those performed on a single phonological word and encapsulated in an intonative unit (preceded or followed by at least one non-terminal prosodic boundary).
Short units can theoretically have as a function all informational values, covering all discourse markers as well as most textual units: this will allow the observation of a wide range of informational functions.
By applying the classification process to these short units alone, prosodic variations due to other linguistic levels (hierarchical, etc.) are avoided.
Moreover, words and expressions appearing on short units are often frequent in oral corpora, which allows analyses to be based on larger amounts of data.
Data will be extracted from C-ORAL corpora in Brazilian Portuguese, Italian and American English (Cresti and Moneglia, 2005; Raso and Mello, 2012; Cavalcante and Ramos, 2016), with a possible application to French (the corpus exists but needs to be resegmented and informationally annotated).
These corpora are prosodically and informationally segmented, in accordance with the premises of the L-AcT.
L-AcT methods have already been applied for the analysis of different informational units, notably in this theoretical framework (Raso and Vieira, 2016; Moneglia and Cresti, 2018; Gobbo, 2019; Cavalcante, 2020), and have demonstrated the feasibility of this task.
The project aims at characterizing the different prosodic structures observed on the target units, in the framework of an acoustical-prosodic analysis, which will aim at a clustering of prosodic shapes and an estimation of the best means of prosodic representation (the phonetic and phonological levels proposing variable description formats).
On the basis of these descriptions of prosodic units, a learning process will have to model the links between prosodic forms and linguistic functions of the targeted units.
These two processes will aim at testing the role played by prosody in the attribution of informational meaning for spontaneous speech, and also at evaluating the cross-linguistic similarity of signifier shapes, and if their use, distribution and frequency can vary from one language to another.
In the first part of this thesis, we present a new pronunciation variant generation method which works by adapting standard i.e., dictionary-based, pronunciations to a spontaneous style.
Its strength and originality lie in exploiting a wide range of linguistic, articulatory and acoustic features and to use a probabilistic machine learning framework, namely conditional random fields (CRFs) and language models.
Extensive experiments on the Buckeye corpus demonstrate the effectiveness of this approach through objective and subjective evaluations.
Listening tests on synthetic speech show that adapted pronunciations are judged as more spontaneous than standard ones, as well as those realized by real speakers.
Speech disfluencies are one of the most pervasive phenomena in spontaneous speech, therefore being able to automatically generate them is crucial to have more expressive synthetic speech.
The proposed approach provides the advantage of generating several types of disfluencies: pauses, repetitions and revisions.
To achieve this task, we formalize the problem as a theoretical process, where transformation functions are iteratively composed.
We present a first implementation of the proposed process using CRFs and language models, before conducting objective and perceptual evaluations.
These experiments lead to the conclusion that our proposition is effective to generate disfluencies, and highlights perspectives for future improvements.
Museums are considering the personalization trend to accommodate the diversity of visitors and their visiting practices.
In order to support the spread of personalized visits, we question the contribution of tangible interactions, not only for visitors but also for museum professionals.
How to help cultural mediators design personalized visits that reflect the diversity of visitor profiles?
How to help visitors choose and follow the visit that suits their wishes and needs?
We applied a user-centered design process with partner museums to design, implement and evaluate tangible tools to answer these questions.
The user needs analysis (cultural mediators and visitors) allowed us to define six main characteristics to consider for the personalization of visits and to identify the concept of multi-criteria personalization.
For the cultural mediators, we propose an interface concept combining the choice of visitor characteristics to constitute a profile and the dynamic monitoring of the personalized visits creation progress for each combination.
We instantiated this concept using two interaction modalities, tangible and tactile, which we compared through an experimental study with museum mediators.
For the visitors, we iteratively designed prototypes to help them choose personalized visits and conducted a pilot study in a partner museum.
The prototypes designed during this thesis implement the token+constraint interaction paradigm.
We propose a systematic literature review referencing the token+constraint interaction paradigm, as well as a heuristic grid of 24 properties divided into five categories that resume, synthesize and illustrate the concepts of the seminal article.
The adaptation may take different forms (evolution, alignment, merging, etc.), and represents several scientific challenges.
One of the most important is to preserve the consistency of the ontology during the changes.
To address this issue, we are interested in this thesis to study the ontology changes and we propose a formal framework that can evolve and merge ontologies without affecting their consistency.
First we propose TGGOnto (Typed Graph Grammars for Ontologies), a new formalism for the representation of ontologies and their changes using typed graph grammars (TGG).
Second, we propose EvOGG (Evolving Ontologies with Graph Grammars), an ontology evolution approach that is based on the TGGOnto formalism that avoids inconsistencies using an a priori approach.
We focus on OWL ontologies and we address both: (1) ontology enrichment by studying their structural level and (2) ontology population by studying the changes affecting individuals and their assertions.
The proposed approach consists of three steps: (1) the similarity search between concepts based on syntactic, structural and semantic techniques; (2) the ontologies merging by the algebraic approach SPO; (3) the global ontology adaptation with graph rewriting rules.
To validate our proposals, we have developed several open source tools based on AGG (Attributed Graph Grammar) tool.
These tools were applied to a set of ontologies, mainly on those developed in the frame of the CCAlps (Creatives Companies in Alpine Space) European project, which funded this thesis work.
This thesis is part of the RAPSODIE project which aims at proposing a speech recognition device specialized on the needs of deaf and hearing impaired people.
Two aspects are studied: optimizing the lexical models and extracting para-lexical information.
This detection aims to inform the deaf and hearing impaired people when a question is addressed to them
Analysing and formalising the emotional aspect of the Human-Machine Interaction is the key to a successful relation.
Beyond and isolated paralinguistic detection (emotion, disfluences…), our aim consists in providing the system with a dynamic emotional and interactional profile of the user, which can evolve throughout the interaction.
This profile allows for an adaptation of the machine's response strategy, and can deal with long term relationships.
A multi-level processing of the emotional and interactional cues extracted from speech (LIMSI emotion detection tools) leads to the constitution of the profile.
Low level cues (F0, energy, etc.), are then interpreted in terms of expressed emotion, strength, or talkativeness of the speaker.
These mid-level cues are processed in the system so as to determine, over the interaction sessions, the emotional and interactional profile of the user.
The profile is made up of six dimensions: optimism, extroversion, emotional stability, self-confidence, affinity and dominance (based on the OCEAN personality model and the interpersonal circumplex theories).
The social behaviour of the system is adapted according to the profile, and the current task state and robot behaviour.
Fuzzy logic rules drive the constitution of the profile and the automatic selection of the robotic behaviour. These determinist rules are implemented on a decision engine designed by a partner in the project ROMEO.
We implemented the system on the humanoid robot NAO.
Using these systems allowed us to collect emotional data in robotic interaction contexts, by controlling several emotion elicitation parameters. This thesis presents the results of these data collections, and offers an evaluation protocol for Human-Robot Interaction through systems with various degrees of autonomy.
Thanks to recent advances in artificial intelligence and natural language processing, the goal of the multidisciplinary project IA4Allergie is to convert vast amounts of textual data stored in healthcare data warehouses to extract and infer new information in order to adapt healthcare for patients suffering from allergic respiratory diseases.
This thesis studies the role of intrinsic motivation in the emergence and development of communicative systems in populations of artificial agents.
To be more specific, our goal is to explore how populations of agents can use a particular motivation system called autotelic principle to regulate their language development and the resulting dynamics at the population level.
To achieve this, we first propose a concrete implementation of the autotelic principle.
The relation between the two elements is not steady but regularly becomes destabilised when new skills are learned, which allows the system to attempt challenges of increasing complexity.
Then, we test the usefulness of the autotelic principle in a series of language evolution experiments.
In the first set of experiments, a population of artificial agents should develop a language to refer to objects with discrete values.
These experiments focus on how unambiguous communicative systems can emerge when the autotelic principle is employed to scaffold language development into stages of increasing difficulty.
In the second set of experiments, agents should agree on a language to communicate with about colour samples.
In this part, we explore how the motivation system can regulate the linguistic complexity of interactions for a continuous domain and examine the value of the autotelic principle as a mechanism to control several language strategies simultaneously.
To summarise, we have shown through our work that the autotelic principle can be used as a general mechanism to regulate complexity in language emergence in an autonomous way for discrete and continuous domains.
Gastronomy and onomastics have never previously been the subject of a joint study from a linguistic point of view.
The objective of this thesis is to provide a starting point for research on the structure, nature and place of proper names in French gastronomy.
After considering the various problems related to the definition of such concepts as gastronomy and proper name, we achieved a synthesis of the main theoretical elements that form the basis for research on the names of dishes from a linguistic, artistic, historic or legislative perspective.
First, the evolution of the proper name in gastronomy over the past 70 years is studied from a normative perspective through the lexical comparison of the first and the latest editions of the Larousse Gastronomique (1938 and 2007) and the establishment of a categorization of proper names.
Second, proper names in a corpus reflecting the use was examined using Parisian restaurant menus and flyers advertising for food delivery.
The comparison of the results for the two types of corpus will shed light on the differences, both quantitative and classificatory, in the use of proper names in a normative or in a creative context.
We present an automatic semantic annotation system for Korean on the EXCOM (EXploration COntextual for Multilingual) platform.
The purpose of natural language processing is enabling computers to understand human language, so that they can perform more sophisticated tasks.
EXCOM identifies semantic information in Korean text using our new method, the Contextual Exploration Method.
Our system properly annotates approximately 90% of standard Korean sentences, and this annotation rate holds across text domains.
This thesis focuses on the modelisation of syntax and syntax-semantics interface of sentences, and investigate how the control of the surgeneration caused by the treatment of linguistics movements with higher order types can take place at the level of derivation structures.
For this purpose, we look at the possibility to extend the type system of Abstract Categorial Grammars with the constructions of disjoint sum, cartesian product and dependent product, which enable syntactic categories to be labeled by feature structures.
At first, we demonstrate that the calculus associated with this extension enjoy the properties of confluence and normalization, by which beta-equivalence can be computed in the grammatical formalism.
We also reduce the same problem for beta-eta-equivalence to a few hypothesis.
Then, we show how this feature structures can be used to control linguistics movements, through the examples of case constraints, extraction islands for overt and covert movements and multiples interrogative extractions, and we discuss the relevancy of operating these controls on the derivation structures
This thesis focuses on the identification of multi-word expressions, addressed through a transition-based system.
A multi-word expression (MWE) is a linguistic construct composed of several elements whose combination shows irregularity at one or more linguistic levels.
Identifying MWEs in context amounts to annotating the occurrences of MWEs in texts, i.e. to detecting sets of tokens forming such occurrences.
Transition-based analysis is a famous NLP technique to build a structured output from a sequence of elements, applying a sequence of actions (called «transitions») chosen from a predefined set, to incrementally build the output structure.
In this thesis, we propose a transition system dedicated to MWE identification within sentences represented as token sequences, and we study various architectures for the classifier which selects the transitions to apply to build the sentence analysis.
The first variant of our system uses a linear support vector machine (SVM) classifier.
The following variants use neural models: a simple multilayer perceptron (MLP), followed by variants integrating one or more recurrent layers.
The preferred scenario is an identification of MWEs without the use of syntactic information, even though we know the two related tasks.
We further study a multitasking approach, which jointly performs and take mutual advantage of morphosyntactic tagging, transition-based MWE identification and dependency parsing.
The thesis comprises an important experimental part.
Firstly, we studied which resampling techniques allow good learning stability despite random initializations.
Secondly, we proposed a method for tuning the hyperparameters of our models by trend analysis within a random search for a hyperparameter combination.
Our variants produce very good results, including state-of-the-art scores for many languages in the PARSEME 1.0 and 1.1 datasets.
One of the variants ranked first for most languages in the PARSEME 1.0 shared task.
By the way, our models have poor performance on MWEs that are were not seen at learning time.
The Autonomous Vehicle is meant to drive itself, without any driver intervention, whatever the driving situation.
This vehicle includes a new function, called AD, for Autonomous Driving, function.
This function can be in different states (Available, Active for example) according to environmental conditions evolution.
This states change is managed by a supervision function, named AD Supervision.
The main goal of my works consists in guaranteeing that AD function remains always in a safe state.
In other words, the AD Supervision must always respect all the functional and safety requirements that specify its behavior.
These two fields contribute to the design of the same function but distinguish at several aspects: objectives, constraints, planning, tools…
In our case study, these differences are illustrated by considered requirements: the functional requirements are allocated to global AD function, while the safety requirements specify the behavior of local redundant sub-functions ensuring a continuous service in case of failure.
The consistency of the two perspectives as early as possible in the design phase and in an industrial context, is the central problematic addressed.
The safety issues due to Autonomous Vehicle make this topic essential for the automotive manufacturers.
To meet these concerns, we proposed a tooled and collaborative approach for safe design of AD Supervision.
This approach is integrated in the normative processes (standards ISO 26262 and ISO 15288) as well as in the internal design processes at Renault.
It is based on formal verification by model checking, parallel composition of finite sate automata and technical expertise.
This approach advocates the utilization of a same formalism (state automata) by the two professions to perform activities sharing a common goal: behavior requirements verification in preliminary design phase.
A method to translate requirements into formal properties and to build state models has been deployed.
The result is a progressive consolidation of treated requirements, initially expressed in free natural language.
The potential ambiguities, inconsistencies and incompleteness are exhibited and treated.
Business process management approaches are now an integral part of the life and of the performance quest in organizations.
Nevertheless, this field remains historically compartmentalized, particularly concerning the role of the information system in the company.
The results of this work were applied to a real industrial case, which demonstrated the relevance and necessity of a convergence between an information system and a company's business process approach.
Contemporary lexicography provide ressources offering many opportunities for natural language processing tasks.
It begins with a selective overview of formalisation and computerisation for study of lexicon, wich defines the principle of exploration : the nodes are similar to objects, which have some attributes and edges represent relations.
Then two sets of exploratory experiments are conducted.
The first one shows that the resource formalisation makes it possible to detect automatically analogies consistent with intuition, that several kind of analogical explorations are possible and that the approach allows to check the consistency of the resource and to bring out lexical rules.
The second one is focused around the concept of lexical derivation configurations.
It shows how grouping of analogous subgraphs reveals recurrent connections.
The progress status of the resource doesn't enable us to obtain successfully completed rules and models, but results are nontheless encouraging.
Such knowledge can be used to identify linguitic phenomena and to design instruments to support lexicographic activity.
This thesis explains and presents our approach of rule-based system of arabic named entity recognition and classification.
This work involves two disciplines: linguistics and computer science.
Computer tools and linguistic rules are merged to give birth to a new discipline: Natural Languge Processsing, which operates in different levels (morphosyntactic, syntactic, semantic, syntactico-semantic…).
This work of thesis is incorporated within the general domain of natural language processing, but it particularly falls within the scope of the continuity of the accomplished work in terms of morphosyntactic analysis and the realisation of lexical data bases of SAMIA and then DIINAR as well as the accompanying scientific recearch.
To understand what it is about, it was important to start with named entity definition.
To carry out this task, we distinguished between two main named entity types: pur proper name and descriptive named entities.
We have also established a referential classification on the basis of different classes and sub-classes which constitue the reference for our semantic annotations.
Nevertheless, we are confronted with two major difficulties: lexical ambiguity and the frontiers of complex named entities.
Our system adoptes a syntactico-semantic rule-based approach.
After Level 0 of morpho-syntactic analysis, the system is made up of five levels of syntactic and syntactico-semantic patterns based on tne necessary linguisic information (i.e. morphosyntactic, syntactic, semantic and syntactico-semantic information).
This work has obtained very good results in termes of precision, recall and F-measure.
The output of our system has an interesting contribution in different applications of the natural language processing especially in both tasks of information retrieval and information extraction.
In fact, we have concretely exploited our system output in both applications (information retrieval and information extraction).
In addition to this unique experience, we envisage in the future work to extend our system into the sentence extraction and classification, in which classified entities, mainly named entities and verbs, play respectively the role of arguments and predicates.
The second objective consists in the enrichment of different types of lexical resources such as ontologies.
The repurposing of clinical data for research has become widespread with the development of clinical data warehouses.
These data warehouses are modeled to integrate and explore structured data related to thesauri.
These data come mainly from machine (biology, genetics, cardiology, etc.) but also from manual data input forms.
The production of care is also largely providing textual data from hospital reports (hospitalization, surgery, imaging, anatomopathologic etc.), free text areas in electronic forms.
This mass of data, little used by conventional warehouses, is an indispensable source of information in the context of rare diseases.
Indeed, the free text makes it possible to describe the clinical picture of a patient with more precision and expressing the absence of signs and uncertainty.
Particularly for patients still undiagnosed, the doctor describes the patient's medical history outside any nosological framework.
This wealth of information makes clinical text a valuable source for translational research.
However, this requires appropriate algorithms and tools to enable optimized re-use by doctors and researchers.
We present in this thesis the data warehouse centered on the clinical document, which we have modeled, implemented and evaluated.
In three cases of use for translational research in the context of rare diseases, we attempted to address the problems inherent in textual data: (i) recruitment of patients through a search engine adapted to textual (data negation and family history detection), (ii) automated phenotyping from textual data, and (iii) diagnosis by similarity between patients based on phenotyping.
These methods and algorithms were integrated into the software Dr Warehouse developed during the thesis and distributed in Open source since September 2017.
In the latest years, the Web has shifted from a read-only medium where most users could only consume information to an interactive medium allowing every user to create, share and comment information.
The downside of social media as an information source is that often the texts are short, informal and lack contextual information.
On the other hand, the Web also contains structured Knowledge Bases (KBs) that could be used to enrich the user-generated content.
This dissertation investigates the potential of exploiting information from the Linked Open Data KBs to detect, classify and track events on social media, in particular Twitter.
More specifically, we address 3 research questions: i) How to extract and classify messages related to events? ii) How to cluster events into fine-grained categories? and 3) Given an event, to what extent user-generated contents on social medias can contribute in the creation of a timeline of sub-events?
We provide methods that rely on Linked Open Data KBs to enrich the context of social media content; we show that supervised models can achieve good generalisation capabilities through semantic linking, thus mitigating overfitting; we rely on graph theory to model the relationships between NEs and the other terms in tweets in order to cluster fine-grained events.
Finally, we use in-domain ontologies and local gazetteers to identify relationships between actors involved in the same event, to create a timeline of sub-events.
We show that enriching the NEs in the text with information provided by LOD KBs improves the performance of both supervised and unsupervised machine learning models.
Recently, Convolutional Neural Networks have become the state-of-the-art soluion(SOA) to most computer vision problems.
In order to achieve high accuracy rates, CNNs require a high parameter count, as well as a high number of operations.
This greatly complicates the deployment of such solutions in embedded systems, which strive to reduce memory size.
Indeed, while most embedded systems are typically in the range of a few KBytes of memory, CNN models from the SOA usually account for multiple MBytes, or even GBytes in model size.
In this manuscript, the main levers allowing to tailor computational complexity of a generic CNN-based object detector are identified and studied.
In order to perform object detection in an efficient way, the detection process is divided into two stages.
The first stage involves a region proposal network which allows to trade-off recall for the number of operations required to perform the search, as well as the number of regions passed on to the next stage.
Furthermore, CNNs also exhibit properties that confirm their over-dimensionment.
This over-dimensionement is one of the key success factors of CNNs in practice, since it eases the optimization process by allowing a large set of equivalent solutions.
However, this also greatly increases computational complexity, and therefore complicates deploying the inference stage of these algorithms on embedded systems.
In order to ease this problem, we propose a CNN compression method which is based on Principal Component Analysis (PCA).
PCA allows to find, for each layer of the network independently, a new representation of the set of learned filters by expressing them in a more appropriate PCA basis.
This PCA basis is hierarchical, meaning that basis terms are ordered by importance, and by removing the least important basis terms, it is possible to optimally trade-off approximation error for parameter count.
Through this method, it is possible to compress, for example, a ResNet-32 network by a factor of ×2 both in the number of parameters and operations with a loss of accuracy &lt;2%.
It is also shown that the proposed method is compatible with other SOA methods which exploit other CNN properties in order to reduce computational complexity, mainly pruning, winograd and quantization.
Furthermore, parallelizing the PCA compressed network over 8 PEs achieves a x11.68 speed-up with respect to the original network running on a single PE.
We are interested, in this thesis, to the study of mixture models and generalized linear models, with an application to co-infection data between arboviruses and malaria parasites.
After a first part dedicated to the study of co-infection using a multinomial logistic model, we propose in a second part to study the mixtures of generalized linear models.
The proposed method to estimate the parameters of the mixture is a combination of a moment method and a spectral method.
Finally, we propose a final section for studing extreme value mixtures under random censoring.
The estimation method proposed in this section is done in two steps based on the maximization of a likelihood.
With the emergence of the Web of Data, most notably Linked Open Data (LOD), an abundance of data has become available on the web.
However, LOD datasets and their inherent subgraphs vary heavily with respect to their size, topic and domain coverage, the schemas and their data dynamicity (respectively schemas and metadata) over the time.
To this extent, identifying suitable datasets, which meet specific criteria, has become an increasingly important, yet challenging task to support issues such as entity retrieval or semantic search and data linking.
Particularly with respect to the interlinking issue, the current topology of the LOD cloud underlines the need for practical and efficient means to recommend suitable datasets: currently, only well-known reference graphs such as DBpedia (the most obvious target), YAGO or Freebase show a high amount of in-links, while there exists a long tail of potentially suitable yet under-recognized datasets.
This problem is due to the semantic web tradition in dealing with "finding candidate datasets to link to", where data publishers are used to identify target datasets for interlinking.
While an understanding of the nature of the content of specific datasets is a crucial prerequisite for the mentioned issues, we adopt in this dissertation the notion of "dataset profile" - a set of features that describe a dataset and allow the comparison of different datasets with regard to their represented characteristics.
Our first research direction was to implement a collaborative filtering-like dataset recommendation approach, which exploits both existing dataset topic proles, as well as traditional dataset connectivity measures, in order to link LOD datasets into a global dataset-topic-graph.
This approach relies on the LOD graph in order to learn the connectivity behaviour between LOD datasets.
However, experiments have shown that the current topology of the LOD cloud group is far from being complete to be considered as a ground truth and consequently as learning data.
Facing the limits the current topology of LOD (as learning data), our research has led to break away from the topic proles representation of "learn to rank" approach and to adopt a new approach for candidate datasets identication where the recommendation is based on the intensional profiles overlap between different datasets.
By intensional profile, we understand the formal representation of a set of schema concept labels that best describe a dataset and can be potentially enriched by retrieving the corresponding textual descriptions.
This representation provides richer contextual and semantic information and allows to compute efficiently and inexpensively similarities between proles.
We identify schema overlap by the help of a semantico-frequential concept similarity measure and a ranking criterion based on the tf*idf cosine similarity.
The experiments, conducted over all available linked datasets on the LOD cloud, show that our method achieves an average precision of up to 53% for a recall of 100%.
Furthermore, our method returns the mappings between the schema concepts across datasets, a particularly useful input for the data linking step.
In order to ensure a high quality representative datasets schema profiles, we introduce Datavore| a tool oriented towards metadata designers that provides ranked lists of vocabulary terms to reuse in data modeling process, together with additional metadata and cross-terms relations.
The tool relies on the Linked Open Vocabulary (LOV) ecosystem for acquiring vocabularies and metadata and is made available for the community.
Within the context of collaborative enterprise information systems, these works aim to propose an approach for assessing the interoperability and the non-interoperability.
The majority of these costs are attributable to the time and resources spent to put in place interfaces for exchanging information.
This mainly affects enterprise global performance by increasing the cost and the delay to obtain the expected services.
We suggest to address enterprise interoperability measurement in order to allow to any enterprise to fully evaluate, a priori, its own capacity to interoperate, and therefore to anticipate possible problems before a partnership.
Our works consist in defining indicators and metrics to quantify and then to qualify the interoperability between the enterprise systems and to propose some improvement strategies when the evaluated interoperability level is not sufficient
This thesis has two principal aims.
In the first place, we would like to offer an overview of the current academic knowledge, both theoretical and empirical, of the processes of linguistic accommodation between interlocutors, in a general sense, and of the rhythmic characteristics of the Spanish language, in particular.
In the second place, we present two empirical studies designed to analyze the influence of sentence-level rhythmic regularity and phonological phrasing on the processes of linguistic accommodation.
Taken together, the data gathered in this thesis indicate that regular rhythmic sentences, arranged in accentual groups, generate a greater amount of resemblance between Spanish speakers in terms of rhythm and F0 range, with respect to irregular rhythmic sentences and sentences arranged in accentual feet.
Moreover, a lower value of F0 mean and a narrower F0 range were observed during the use of both regular rhythmic sentences and sentences arranged in accentual groups compared to the opposite conditions.
In addition, some known facts related to women having a higher F0 mean, a wider F0 range, and speaking slower regarding men were also found during the first experiment.
Companies, administrations, and sometimes individuals, have to face many frauds on documents they receive from outside or process internally.
Invoices, expense reports, receipts...any document used as proof can be falsified in order to earn more money or not to lose it.
In France, losses due to fraud are estimated at several billion euros per year.
Since the flow of documents exchanged, whether digital or paper, is very important, it would be extremely costly and time-consuming to have them all checked by fraud detection experts.
That's why we propose in our thesis a system for automatic detection of false documents.
While most of the work in automatic document detection focuses on graphic clues, we seek to verify the textual information in the document in order to detect inconsistencies or implausibilities.
To do this, we first compiled a corpus of documents that we digitized.
After correcting the characters recognition outputs and falsifying part of the documents, we extracted the information and modelled them in an ontology, in order to keep the semantic links between them.
The information thus extracted, and increased by its possible disambiguation, can be verified against each other within the document and through the knowledge base established.
The semantic links of ontology also make it possible to search for information in other sources of knowledge, particularly on the Internet.
In this thesis, we present two computer models to structure textual information for large databases of medieval charters.
The two models, one applied to the recognition of named entities, the other to the detection of parts of the diplomatics discourse, are supervised Conditional random fields (CRF) models trained on a hand-annotated corpus of medieval charters (Corpus Burgundiae Medii Aevi or CBMA).
The main Named Entity Recognition model has proven to be robust in its application to widely varying corpora in size, chronology and origin.
The secondary model detecting parts of the diplomatic discourse, although less efficient, remains valid as a structuring tool.
At the moment both can be used for indexing and studying a wide variety of diplomatics sources, thus saving huge human efforts.
We have developed different solutions to overcome the gap between model's dependence on its original training-set and its ability to be applied to other corpora.
Similarly, various corrections and additions were made to the golden-corpus from several historical and linguistic analysis concerning writing phenomena in charters, which greatly helped to improve the initial performance.
In a later step we applied our automatic tools in the recognition of names of people, places and parts of the diplomatics discourse on thousands of charters from the CBMA corpus in order to study different questions concerning historical science and diplomatics.
These studies concern the semi-automatic dating of a non-dated cartulary; the evolution of the spatial vocabulary in the charters of the central Middle Ages and the indexing of charters from their scriptural modules, in particular formulae of the charter protocols.
This studies has a twofold purpose: on the one hand have shown different strategies for abstracting and adapting to the automatic processing well-known methods of research in history; on the other hand, seek to provide us tools with an applicative framework to obtain relevant knowledge to the historical science using massive processing.
The continuous increasing needs in medicine and healthcare, accentuate the need of well-adapted medical alert systems.
Such alert systems may be used by a variety of patients and medical actors.
These systems should allow monitoring a wide range of medical variables.
Detected alerts have two quality indices.
The applicability index which indicates how well a patient is affected by the alert, and the confidence index, which expresses the reliability of the alert concerning the freshness of the data used in its detection.
Quality indices associated with a detected alert are calculated using information related to an alert situation configured by the user.
An alert situation is defined from multiple activation conditions.
An activation condition is constructed from a linguistic value expressing the state (e.g. high temperature) or trend (e.g. systolic blood pressure rising) of an observable entity (temperature, systolic blood pressure, etc.).
When the alert condition is evaluated, the system uses knowledge previously prepared by users regarding linguistic values.
That is, what linguistic value best represents a quantitative value in a specific context.
Since many alerts can be detected, we define a notification policy to notify only the relevant alerts in order to keep the users'interest.
First alerts are filtered from the quality indices.
Of the remaining alerts, the system filters by expressiveness: to keep more sustainable trends and the most expressive linguistic values.
Then, in the case of consecutive alerts, the system keeps only those that fulfill the user preferences, such as those whose applicability index increases.
The ultimate goal is to keep the user loyalty to the alert system by providing quality service.
The user appropriates the system while he defines the alert situation.
Thus, he is able to adapt the alert situation by himself to the context to obtain better alerts.
The adaptation is guided by quality indices used to reduce false positives and false negatives as well as to control the over-alerting.
We propose to leverage existing systems by providing dynamism and evolution features, as well as facilities for setup and real-time adaptation to the context of use in order to fully exploit the observations.
The Information Extraction from clinical notes provides relevant information to identify adverse side effects in post-marketing surveillance of medications (Pharmacovigilance), which is more difficult to discover by traditional medical studies since patients are taking several treatments at the same time.
In recent years, data mining techniques have allowed to discover knowledge stored in big datasets, such as the clinical records collected by hospitals throughout patient's life.
The goal of this work is identify adverse side effects caused by treatments.
This problem is divided Named Entity Recognition (NER) and Relation Extraction tasks.
Nowadays, supervised approaches based on Deep Learning and Machine Learning algorithms solve this problem in the state of the art.
These supervised systems require rich features in order to learn efficient models during training, therefore, we focus on building comprehensive word representations (the input of the neural network), using character-based word representations and word representations.
The proposed representation improves the performance of the baseline model, and the final model reached the performances of state of the art methods.
Then we have extracted contextual information through Deep Learning models and other different features obtained from the relations, in order to identify the Adverse Drug Reaction relations.
The proposed model improved the overall accuracy and the extraction of Adverse Drug Reaction compared to the baseline, indicating the effectiveness of combining Deep Learning models and extensive feature engineering.
This thesis focuses on text Automatic Summarization and particularly on Update Summarization.
This research problem aims to produce a differential summary of a set of new documents with regard to a set of old documents assumed to be known.
It thus adds two issues to the task of generic automatic summarization: the temporal dimension of the information and the history of the user.
In this context, the work presented here is based on an extractive approach using integer linear programming (ILP) and is organized around two main axes: the redundancy detection between the selected information and the user history and the maximization of their saliency.
For the first axis,we were particularly interested in the exploitation of inter-sentence similarities to detect the redundancies between the information of the new documents and those present in the already known ones, by defining a method of semantic clustering of sentences.
Concerning our second axis, we studied the impact of taking into account the discursive structure of documents, in the context of the Rhetorical Structure Theory (RST), to favor the selection of information considered as the most important.
The benefit of the methods thus defined has been demonstrated in the context of evaluations carried out on the data of TAC and DUC campaigns.
Finally, the integration of these semantic and discursive criteria through a delayed fusion mechanism has proved the complementarity of these two axes and the benefit of their combination.
With the growing web, a number of applications seek to meet the needs of users or machines having diverse cultural backgrounds.
From this context of cultural diversity arises conflicts linked to different world conceptions.
Offering adaptated services requires the integration of a form of cultural awareness in the system.
An artificial cultural awareness is composed of formal cultural representations and mediations providing the system with the means to interpret the represented cultures and to determine their differences.
Those coarse-grained models, even though they are adapted, limit the possible understanding of the represented cultures.
As a consequence they constitute a bottleneck for the development of culturally-aware systems.
I study the construction, the formalisation and the mediation of these emic cultural representations.
My main contributions are the design and validation of, in one hand, a new semi-automatic ethnographic process for building emic models through text-mining, in another hand, an emic artificial cultural awareness based on the mapping of cultural ontologies coming from those models.
Consider an Erdős-Rényi (ER) graph with edge probability q and size n containing a planted subgraph of size m and probability p.
We derive a statistical test based on the eigenvalue and eigenvector properties of a suitably defined matrix to detect the planted subgraph.
We analyse the effect of side-information on the detectability threshold of Belief Propagation (BP) applied to the above problem.
We show that BP correctly recovers the subgraph even with noisy side-information for any positive value of an effective SNR parameter.
This is in contrast to BP without side-information which requires the SNR to be above a certain threshold.
Finally, we study the asymptotic behaviour of PageRank on a class of undirected random graphs called fast expanders, using Random Matrix Theoretic techniques.
We show that PageRank can be approximated for large graph sizes as a convex combination of the normalized degree vector and the personalization vector of the PageRank, when the personalization vector is sufficiently delocalized.
Subsequently, we characterize asymptotic PageRank on Stochastic Block Model (SBM) graphs, and show that it contains a correction term that is a function of the community structure.
Learning stochastic models generating sequences has many applications in natural language processing, speech recognitions or bioinformatics.
Traditional learning algorithms such as the one of Baum-Welch are iterative, slow and may converge to local optima.
A recent alternative is to use the Method of Moments (MoM) to design consistent and fast algorithms with pseudo-PAC guarantees.
However, MoM-based algorithms have two main disadvantages.
First, the PAC guarantees hold only if the size of the learned model corresponds to the size of the target model.
Second, although these algorithms learn a function close to the target distribution, most do not ensure it will be a distribution.
Thus, a model learned from a finite number of examples may return negative values or values that do not sum to one.
This thesis addresses both problems.
First, we extend the theoretical guarantees for compressed models, and propose a regularized spectral algorithm that adjusts the size of the model to the data.
Then, an application in electronic warfare is proposed to sequence of the dwells of a super-heterodyne receiver.
The present dissertation investigates the argument structure of two groups of Italian parasyntheticverbs: denominal verbs paraphrased as "make X become N", where N is the base noun (henceforth BN); adjectival verbs paraphrased as "make X more A", where A is the base adjective.
The dissertation starts with three chapters of general interest.
The first one describes new experimental methods that can be employed in generative linguistics.
The second and third one describe useful frameworks and the morphological process of parasynthesis.
The first part of the dissertation analyses BNs.
It is shown by means of several experiments that Italian native speakers accept the pseudo-resultative construction.
Results of a comparative study with French are reported and show that French behaves differently to Italian in this respect.4The second part analyses stativity diagnostics and apply them in the study of DPVs.
The last chapter applies stativity diagnostics in the natural language processing domain.
Hierarchical image representations have been widely used in the image classification context.
Such representations are capable of modeling the content of an image through a tree structure.
In this thesis, we investigate kernel-based strategies that make possible taking input data in a structured form and capturing the topological patterns inside each structure through designing structured kernels.
We develop a structured kernel dedicated to unordered tree and path (sequence of nodes) structures equipped with numerical features, called Bag of Subpaths Kernel (BoSK).
It is formed by summing up kernels computed on subpaths (a bag of all paths and single nodes) between two bags.
We also propose a scalable version of BoSK (SBoSK for short), using Random Fourier Features technique to map the structured data in a randomized finite-dimensional Euclidean space, where inner product of the transformed feature vector approximates BoSK.
This strategy allows tile/sub-image classification.
Further relying on (S)BoSK, we introduce a novel multi-source classification approach that performs classification directly from a hierarchical image representation built from two images of the same scene taken at different resolutions, possibly with different modalities.
Evaluations on several publicly available remote sensing datasets illustrate the superiority of (S)BoSK compared to state-of-the-art methods in terms of classification accuracy, and experiments on an urban classification task show the effectiveness of proposed multi-source classification approach.
The main objective of our thesis paper is to examine the intelligibility of erroneous prepositional uses produced by French learners of English.
We also propose certain effective pedagogical approaches to teaching English prepositions/particles.
The results of our corpus analysis allow us to observe the extent to which erroneous spatial prepositions may affect the intelligibility of the transferred message.
In many fields, novel technologies employed in information acquisition and measurement (e.g. phenotyping automated greenhouses) are at the basis of a phenomenal creation of data.
In particular, we focus on two real use cases: plants observations in botany and phenotyping data in biology.
Our contributions can be, however, generalized to Web data.
In addition to their huge volume, data are also distributed.
Indeed, each user stores their data in many heterogeneous sites (e.g. personal computers, servers, cloud); yet he wants to be able to share them.
Thus, the global objective of this work is to define a set of techniques enabling sharing and discovery of data in heterogeneous distributed environment, through the use of search and recommendation approaches.
Diversification techniques allow users to receive results with better novelty while avoiding redundant and repetitive content.
By introducing a distance between each result presented to the user, diversity enables to return a broader set of relevant items.
However, few works exploit profile diversity, which takes into account the users that share each item.
In this work, we show that in some scenarios, considering profile diversity enables a consequent increase in results quality: surveys show that in more than 75% of the cases, users would prefer profile diversity to content diversity.
Additionally, in order to address the problems related to data distribution among heterogeneous sites, two approaches are possible.
First, P2P networks aim at establishing links between peers (nodes of the network): creating in this way an overlay network, where peers directly connected to a given peer p are known as his neighbors.
This overlay is used to process queries submitted by each peer.
However, in state of the art solutions, the redundancy of the peers in the various neighborhoods limits the capacity of the system to retrieve relevant items on the network, given the queries submitted by the users.
In this work, we show that introducing diversity in the computation of the neighborhood, by increasing the coverage, enables a huge gain in terms of quality.
The second category of approaches is called multi-site.
Generally, in state of the art multi-sites solutions, the sites are homogeneous and consist in big data centers.
A prototype regrouping all contributions have been developed, with two versions addressing each of the use cases considered in this thesis.
In this thesis, we were interested in the impact of the quantity and quality of information exchanged between individuals in a group on their collective performance in two very specific types of tasks.
In a first series of experiments, subjects had to estimate quantities sequentially, and could revise their estimates after receiving the average estimate of other subjects as social information.
We controlled this social information through virtual participants (which number we controlled) giving information (which value we controlled), unknowingly to the subjects.
We showed that when subjects have little prior knowledge about a quantity to estimate, (the logarithms of) their estimates follow a Laplace distribution.
Since the median is a good estimator of the center of a Laplace distribution, we defined collective performance as the proximity of the median (log) estimate to the true value.
We found that after social influence, and when the information provided by the virtual agents is correct, the collective performance increases with the amount of information provided (fraction of virtual agents).
We also analysed subjects'sensitivity to social influence, and found that it increases with the distance between personal estimate and social information.
These analyses made it possible to define five behavioral traits: to keep one's opinion, to adopt that of others, to compromise, to amplify social information or to contradict it.
Our results showed that the subjects who adopt the opinion of others are the ones who best improve their performance because they are able to benefit from the information provided by the virtual agents.
We then used these analyses to construct and calibrate a model of collective estimation, which quantitatively reproduced the experimental results and predicted that a limited amount of incorrect information can counterbalance a cognitive bias that makes subjects underestimate quantities, and thus improve collective performance.
Further experiments have validated this prediction.
In a second series of experiments, groups of 22 pedestrians had to segregate into clusters of the same "color", without visual cue (the colors were unknown), after a short period of random walk.
To help them accomplish their task, we used an information filtering system (analogous to a sensory device such as the retina), taking all the positions and colors of individuals in input, and returning an acoustic signal to the subjects (emitted by tags attached to their shoulders) when the majority of their k nearest neighbors was of a different color from theirs.
This PhD thesis, conducted in cooperation with ONERA, focuses on active 3D object recognition by an autonomous visual agent.
Whereas in passive recognition, acquisition modalities of observations are fixed and may generate ambiguities, active recognition exploits the possibility of controling these modalities online in a sequential inference process in order to remove these ambiguities.
The aim of this work is to design, in a statistical learning framework, planning strategies in the acquisition of information while achieving a realistic implementation of active recognition.
The first part of the work is dedicated to learning to plan.
The second part of this work focuses on maximally exploiting observations acquired during recognition.
The possibility of an active multi-scale recognition is investigated to allow an interpretation as soon as the sequential acquisition process begins.
Observations are also used to robustly estimate the pose of the object to ensure consistency between the planned and actual modality of the visual agent.
This thesis focuses on zero-shot visual recognition, which aims to recognize images from unseen categories, i.e. categories not seen by the model during training.
After categorizing existing methods into three main families, we argue that ranking methods habitually make several detrimental implicit assumptions.
We propose to adapt the usual formulation of the hinge rank loss so that such methods may take inter and intra-class relations into account.
We also propose a simple process to address the gap between accuracies on seen and unseen classes, from which these methods frequently suffer in a generalized zero-shot learning setting.
In our experimental evaluation, the combination of these contributions enables our proposed model to equal or surpass the performance of generative methods, while being arguably less restrictive.
In a second part, we focus on the semantic representations used in a large-scale zero-shot learning setting.
In this setting, semantic information customarily comes from word embeddings of the class names.
We argue that usual embeddings suffer from a lack of visual content in training corpora.
We thus propose new visually oriented text corpora as well as a method to adapt word embedding models to these corpora.
We further propose to complete unsupervised representations with short descriptions in natural language, whose generation requires minimal effort when compared to extensive attributes.
Construction of ontologies is a tedious task which still requires a great amount of manual work.
Texts, as knowledge sources, can help, but TALN tools stop at linguistic level.
Manual conceptualization fill the gap between a linguistic model and a conceptual model.
In this thesis we study how a symbolic clustering method, Formal Concept Analysis, can be combined with a linguistic model to help the knowledge engineer.
We have experimented on three different domains represented by same-sized corpora.
We propose solutions that combine FCA and terminological analysis, to let the computer suggest usefull clusters and faithful representation of texts.
Recent deep learning architectures and algorithms have shown impressive results for several Natural Language Processing (NLP) tasks such as Named entity recognition, Part-of-Speech tagging, Dependency parsing and Semantic role labelling.
The actual performance of certain NLP tools for English evaluated on in-domain data is close to human level, thanks to deep learning models trained on huge annotated datasets.
Contrariwise, approaching human-level accuracy on more complex domains and low-resource languages is still a hard issue.
This thesis falls within the scope of incorporating expert knowledge and linguistic resources in Deep Neural Networks (DNNs) in order to improve the performance of NLP tools for specialty areas and low-resource languages.
The proposed subject aims to explore and experiment new approaches for incorporating expert knowledge and linguistic resources in deep neural networks.
We propose to tackle this issue along the following key aspects, as an extension of the research work already carried out at LASTI (Laboratoire Analyse Sémantique Texte et Image) laboratory:
- Taking into account heterogeneous expert knowledge and linguistic resources: Ontologies, Terminology databases, Lexicons, Named entity recognition rules, Dependency parsing recognition rules, etc.
- Implementing a formalism to describe expert knowledge and linguistic resources in a multi-level representation.
The objective is to define a structure in which the different types of expert knowledge and linguistic resources will be represented separately but the whole representation would be described in the same format (model).
- Exploring new strategies for incorporating expert knowledge and linguistic resources in deep neural networks.
The underlying idea is to propose an integration mechanism which can be adapted to each expert knowledge and linguistic resource.
My thesis proposes an analysis of the Italian parasynthetic verbs in within the framework of the Construction Morphology.
The widespread definition of parasynthesis in literature corresponds to 'double simultaneous affixation on a derivational base' ([pref+[X]N/A+suff]V, cf. for example IMBARCARE 'to board').
Such definition is motivated by the impossibility of attesting the intermediate derivational stage between the base and the derived verb (cf. BARCA 'boat', *IMBARCA, *BARCARE) and it derives from a morpheme-based, incremental and concatenative approach to morphology that assumes that derivational processes are oriented rules.
In my thesis I propose an alternative analysis which defines parasynthetic verbs as verbs built by prefixation.
I consider as parasynthetic verb each verb belonging to the schema [préf[X]N/A]V (note that the suffix is analyzed as inflectional).
This definition is purely based on the membership parameter in schema [préf[X]N/A]V (note that the suffix is analyzed as inflectional).
According to this approach, the fact that a word is not attested not only represents an unreliable criterion from an empirical point of view but it also seems to be negligible within a theoretical framework considering morphological processes as non-oriented.
The corpus includes 1674 lexemes automatically extracted from ItWaC.
The structural variables for these verbs are (i) the prefix selected (a-, in-, s-, de-, dis-), (ii) the inflectional class (-are,-ire), (iii) the category of the base (N, A).
Each lexeme is defined as a construction, i.e. a form-meaning pair (the form corresponding to a possible combination of variables and the meaning to a holistic sense).
The possible semantic values are: (i) the change of state, (ii) the change of locative relation and (iii) the intensive/iterative value.
For (i) and (ii) I propose a unified analysis in terms of a general semantic component expressing a change (formalized by the predicate BECOME), while the class of verbs expressing the value (iii) are not included in this generalization.
Our research work presented in this thesis aims the optimization of the performance of formant tracking algorithms.
We began by analyzing different existing techniques used in the automatic formant tracking.
This analysis showed that the automatic formant estimation remains difficult despite the use of complex techniques.
For the non-availability of database as reference in Arabic, we have developed a phonetically balanced corpus in Arabic while developing a manual phonetic and formant tracking labeling.
Then we presented our two new automatic formant tracking approaches which are based on the estimation of Fourier ridges (local maxima of spectrogram) or wavelet ridges (local maxima of scalogram) using as a tracking constraint the calculation of center of gravity of a set of candidate frequencies for each formant, while the second tracking approach is based on dynamic programming combined with Kalman filtering.
Finally, we made an exploratory study using manually labeled corpus as a reference to quantify our two new approaches compared to other automatic formant tracking methods.
We tested the first approach based on wavelet ridges detection, using the calculation of the center of gravity on synthetic signals and then on real signals issued from our database by testing three types of complex wavelets (CMOR, SHAN and FBSP).
Following these tests, it appears that formant tracking and scalogram resolution given by CMOR and FBSP wavelets are better than the SHAN wavelet.
To quantitatively evaluate our two approaches, we calculated the absolute difference average and standard deviation.
We made several tests with different speakers (male and female) on various long and short vowels and continuous speech signals issued from our database using it as a reference.
The formant tracking results are compared to those of Fourier ridges method calculating the center of gravity, LPC analysis combined with filter banks method of Kamran.
Therefore, this method provides a correct formant tracking (F1, F2 and F3) and closer to the reference.
They are also very close to the Fourier ridges method using the calculation of center of gravity.
The results obtained in the case of female speakers confirm the trend observed over the male speakers
This thesis deals with the study of random methods for learning large-scale data.
Firstly, we propose an unsupervised approach consisting in the estimation of the principal components, when the sample size and the observation dimension tend towards infinity.
This approach is based on random matrices and uses consistent estimators of eigenvalues and eigenvectors of the covariance matrix.
Then, in the case of supervised learning, we propose an approach which consists in reducing the dimension by an approximation of the original data matrix and then realizing LDA in the reduced space.
Dimension reduction is based on low–rank approximation matrices by the use of random matrices.
A fast approximation algorithm of the SVD and a modified version as fast approximation by spectral gap are developed.
Experiments are done with real images and text data.
Compared to other methods, the proposed approaches provide an error rate that is often optimal, with a small computation time.
Finally, our contribution in transfer learning consists in the use of the subspace alignment and the low-rank approximation of matrices by random projections.
This thesis in contrast linguistics aims to study the syntactic-semantic specificities of inchoative verbs in Arabic and compare them with those of start to (commencer (à/par/de)) and to begin (se mettre à) in French.
We relied on a corpus of French and Arabic literary texts from the following authors (Najube Mahfouz, Marcel Pagnol and Albert Camus).
According to our corpus, inchoative verbs can appear in the following simple constructions: (subject + inchoative verb + direct object complement, indirect object complement, adverb or without complement), as well as in compound constructions (subject + V1 accomplished inchoative verb + V2 uncompleted) in Arabic, and (S + conjugated V1 + V2 infinitive) in French.
Our analysis is based on the syntactic frameworks proposed by Peeters (1993).
Indeed, the application of this theory and the syntactic variation of the grammatical elements of the Arabic sentence lead us, first, to propose different syntactic frameworks of certain inchoative Arabic verbs found in our corpus by determining their syntactic and semantic specificities.
Concerning the simple form of inchoative verbs in Arabic, بدأ (bada'a) is the most frequently used verb.
It is compatible with certain prepositions and accepts any type of subject and complement.
As a result, it is distinguished by six grammatical constructions and four syntactic frames.
On the other hand, the verbs هم (hamma), شرع (šara῾a) and أخذ ('aẖaḏa), in their simple use, each of them is governed only by a preposition, therefore, their simple construction is mainly characterized by a syntactic framework.
In addition, the verb راح (rāḫa) is combined with two syntactic frames.
Regarding their compound form, it should be noted that all inchoative verbs in Arabic are shared by a compound construction (S + V1 completed + V2 uncompleted).
Semantically, most of these verbs are characterized by an imperfective value.
It should be noted that the verb بدأ (bada'a) conveys a determinative as well as a non-determinative value.
On the other hand, the verb شرع (šara῾a) is defined only by a perfective value, whereas هم (hamma) by a semi-perfective value.
Therefore, from the results of Peeters as well as ours, we find that, on the one hand, the verb بدأ (bada'a) and commencer (à/par/de) are almost syntactically and semantically equivalent.
On the other hand, the verbs هم (hamma) and se mettre à always serve to indicate speed and suddenness, but the action of the former is always semi-perfective.
We can say that there are always points of divergence and convergence between inchoative verbs in both languages (Arabic and French).
Finally, based on the written productions of FLE Libyan learners, we have tried to identify pedagogical approaches in order to identify the origin of the difficulties in the use of verbs, commencer (à/par/de) et se mettre à.
Typical Internet users today have their data scattered over several devices, applications, and services.
Managing and controlling one's data is increasingly difficult.
In this thesis, we adopt the viewpoint that the user should be given the means to gather and integrate her data, under her full control.
In that direction, we designed a system that integrates and enriches the data of a user from multiple heterogeneous sources of personal information into an RDF knowledge base.
The system is open-source and implements a novel, extensible framework that facilitates the integration of new data sources and the development of new modules for deriving knowledge.
We introduce a time-based clustering algorithm to extract stay points from location history data.
Using data from additional mobile phone sensors, geographic information from OpenStreetMap, and public transportation schedules, we introduce a transportation mode recognition algorithm to derive the different modes and routes taken by the user when traveling.
The algorithm derives the itinerary followed by the user by finding the most likely sequence in a linear-chain conditional random field whose feature functions are based on the output of a neural network.
We also show how the system can integrate information from the user's email messages, calendars, address books, social network services, and location history into a coherent whole.
To do so, it uses entity resolution to find the set of avatars used by each real-world contact and performs spatiotemporal alignment to connect each stay point with the event it corresponds to in the user's calendar.
Finally, we show that such a system can also be used for multi-device and multi-system synchronization and allow knowledge to be pushed to the sources.
The incursion of Islam in Sub-Saharan Africa from the 19th Century was operated through trans-Saharan commerce between the peoples of North Africa and those of the Sahel.
This contact, maintained by the commercial caravans of these two peoples engendered the progressive islamisation of the Hausa-speaking populations.
Under the influence of Arabic, several terms were introduced into Hausa lexicon.
This Islamic effect comes with a revolution in the production of Arabic-Ajami literature.
On the basis of these observations, this thesis proposes to analyse the borrowed Arabic lexicon in the poetic works of the author, and their integration into the Hausa language.
Our corpus is made up of poetic works that we have first of all lemmatized using statistical calculations with the help of Excel software.
The principal results, obtained in the form graphs, indicate a frequency of very high usage of words borrowed from Arabic.
The association of the linguistic and computer analyses enabled us to confirm, in a formal and impartial manner, that most of the frequent borrowings fall under religious domains, and as such linked to situational vocabulary.
The use of electronic medical records (EMRs) and electronic prescribing are priorities in the various European action plans on connected health.
The development of the EMR is a tremendous source of data; it captures all symptomatic episodes in a patient's life and should lead to improved medical and care practices, as long as automatic treatment procedures are set up.
As such, we are working on hospitalization prediction based on EMRs and after having represented them in vector form, we enrich these models in order to benefit from the knowledge resulting from referentials, whether generalist or specific in the medical field, in order to improve the predictive power of automatic classification algorithms.
Determining the knowledge to be extracted with the objective of integrating it into vector representations is both a subjective task and intended for experts, we will see a semi-supervised procedure to partially automate this process.
As a result of our research, we designed a product for general practitioners to prevent their patients from being hospitalized or at least improve their health.
Thus, through a simulation, it will be possible for the doctor to evaluate the factors involved on the risk of hospitalization of his patient and to define the preventive actions to be planned to avoid the occurrence of this event.
Past searches provide a useful source of information for new users (new queries).
Due to the lack of ad-hoc IR collections, to this date there is a weak interest of the IR community on the use of past search results.
Indeed, most of the existing IR collections are composed of independent queries.
These collections are not appropriate to evaluate approaches rooted in past queries because they do not gather similar queries due to the lack of relevance judgments.
Therefore, there is no easy way to evaluate the convenience of these approaches.
In addition, elaborating such collections is difficult due to the cost and time needed.
Thus a feasible alternative is to simulate such collections.
Besides, relevant documents from similar past queries could be used to answer the new query.
This principle could benefit from clustering of past searches according to their similarities.
Thus, in this thesis a framework to simulate ad-hoc approaches based on past search results is implemented and evaluated.
Four randomized algorithms to improve precision are proposed and evaluated, finally a new measure in the clustering context is proposed.
THE TRÉSOR DE LA LANGUE FRANÇAISE ANDTHE OXFORD ENGLISH DICTIONARY: A DIACHRONICAL ANALYSIS OF LOAN-WORDSABSTRACT
There is no language that does not expand thanks to loan-words: they permit the lexical stock to get richer and refreshed as are developed the relationships between cultures and countries.
English and French languages, since they have been spreading over all continents, have acquired a lot of words from other horizons, that, moreover, they often shared.
Actually, we can but notice that their geographic proximity and the richness of their history have aroused an important interpenetration during more than ten centuries.
That is why we wanted to show, in this study, the impact of loan-words on both languages, and to analyse the way the most extensive dictionaries on either side of the Channel — the Trésor de la Langue Française and the Oxford English Dictionary — dealt with them.
In the first part of this work, we study how French and English lexicons were built up over the course of time according to foreign contributions, and we define the very notion of loan-word in order to show how complex it is.
Afterwards, we present the corpus on which rests this study.
The second part is dedicated to an exhaustive presentation of the Trésor de la Langue Française and of the Oxford English Dictionary.
After a recounting of language dictionaries and of the creation of those two dictionaries, their main features are highlighted and their constitution accurately examined, as well macrostructurally as microstructurally.
We also point out the advantages of their informatisation.
In this research, we address the problem of retrieving services which fulfil users'need expressed in query in free text.
Our goal is to cope the term mismatch problems which affect the effectiveness of service retrieval models applied in prior re- search on text descriptions-based service retrieval models.
These problems are caused due to service descriptions are brief.
We have applied a family of Information Retrieval (IR) models for the purpose of contributing to increase the effectiveness acquired with the models applied in prior research on service retrieval.
Besides, we have conducted systematic experiments to compare our family of IR models with those used in the state-of-the-art in service discovery.
From the outcomes of the experiments, we conclude that our model based on query expansion via a co-occurrence thesaurus outperforms the effectiveness of all the models studied in this research.
Therefore, we have implemented this model in S3niffer, which is a text description-based service search engine.
Knowledge bases are deductive databases where the machinery of logic is used to represent domain-specific and general-purpose knowledge over existing data.
In the existential rules framework a knowledge base is composed of two layers: the data layer which represents the factual knowledge, and the ontological layer that incorporates rules of deduction and negative constraints.
The main reasoning service in such framework is answering queries over the data layer by means of the ontological layer.
As in classical logic, contradictions trivialize query answering since everything follows from a contradiction (ex falso quodlibet).
Recently, inconsistency-tolerant approaches have been proposed to cope with such problem in the existential rules framework.
They deploy repairing strategies on the knowledge base to restore consistency and overcome the problem of trivialization.
However, these approaches are sometimes unintelligible and not straightforward for the end-user as they implement complex repairing strategies.
This would jeopardize the trust relation between the user and the knowledge-based system.
In this thesis we answer the research question: ``How do we make query answering intelligible to the end-user in presence of inconsistency?''.
The answer that the thesis is built around is ``We use explanations to facilitate the understanding of query answering''.
We propose meta-level and object-level dialectical explanations that take the form of a dialogue between the user and the reasoner about the entailment of a given query.
We study these explanations in the framework of logic-based argumentation and dialectics and we study their properties and their impact on users.
These disorders are common during the course of Parkinson's disease, decrease the quality of life of subjects, and increase caregiver burden.
Being able to predict which individuals are at higher risk of developing these disorders and when is of high importance.
The objective of this thesis is to study impulse control disorders in Parkinson's disease from the statistical and machine learning points of view, and can be divided into two parts.
The first part consists in investigating the predictive performance of the altogether factors associated with these disorders in the literature.
The second part consists in studying the association and the usefulness of other factors, in particular genetic data, to improve the predictive performance.
For several years, the deployment of information and communication technology into the management of chronical pathologies is taking a considerable place, more particularly in the evolution of health's practices and in the improvement of the well-being of the patient.
Chronical pathologies are of long duration and they need to be under a regular monitoring of the healthcare professional, composed of multidisciplinary or different actors in charge with the patients.
On the other side the patients are also charged of following a healthcare protocol at home previously defined by the health care team.
Nevertheless, the different forms of representing the contests of this protocol, it is not always complete and comprehensible for the patients.
Furthermore, each one of the patients is unique and a proper definition of the health care protocol must be personalised and conform to his individual treatment and even to his personal wishes or constraints.
But this is not the case of information guides or medical references that are supplied in general.
The research developed in this thesis introduces a qualitative approach for representing and reasoning on moving entities in a two-dimensional geographical space.
Movement patterns of moving entities are categorized based on a series of qualitative spatial models of topological relations between a directed line and a region, and orientation relations between two directed lines, respectively.
Qualitative movements are derived from the spatio-temporal relations that characterize moving entities conceptualized as either points or regions in a two-dimensional space.
Such a spatio-temporal framework supports the derivation of the basic movement configurations inferred from moving and static entities.
The approach is complemented by a tentative qualification of the possible natural language expressions of the primitive movements identified.
The notion of conceptual transition that favors the exploration of possible trajectories in the case of incomplete knowledge configurations is introduced and explored.
Composition tables are also studied and provide additional reasoning capabilities.
The whole approach is applied to the analysis of flight patterns and maritime trajectories.
Computational models for automatic text understanding have gained a lot of interest due to unusual performance gains over the last few years, some of them leading to super-human scores.
This success reignited some grandeur claims about artificial intelligence, such as universal sentence representation.
In this thesis, we question these claims through two complementary angles.
Firstly, are neural networks and vector representations expressive enough to process text and perform a wide array of complex tasks?
In this thesis, we will present currently used computational neural models and their training techniques.
Secondly, we will discuss the question of universality in sentence representation: what actually lies behind these universality claims?
We delineate a few theories of meaning, and in a subsequent part of this thesis, we argue that semantics (unsituated, literal content) as opposed to pragmatics (meaning as use) is preponderant in the current training and evaluation data of natural language understanding models.
To alleviate that problem, we show that discourse marker prediction (classification of hidden discourse markers between sentences) can be seen as a pragmatics-centered training signal for text understanding.
We build a new discourse marker prediction dataset that yields significantly better results than previous work.
In addition, we propose a new discourse-based evaluation suite that could incentivize researchers to take into account pragmatic considerations when evaluating text understanding models.
This study focuses on the analysis of identity discourse of Darwich's collection: "Why did you leave the horse alone?" in order to know the nature of relationship that the me of Darwich maintains with the me of the Other.
In other words, our collection looks like a “dialogue” where the me of Darwich and the me of the Other plunge into an argumentative discourse.
To answer our question about the relationship between the two parts of dialogue, we have firstly chosen a theoretical approach: the presentation of the emblematic figure of Darwich perceived as one, having a personal, cultural, social and national identity, different from the identity of the other one.
Secondly, through a practical point of view, the type of relationship between the two sides has been shown through the analysis of the corpus consisting of six groups.
The efficient communication tends to follow the principle of the least effort.
According to this principle, using a given language interlocutors do not want to work any harder than necessary to reach understanding.
This fact leads to the extreme compression of texts especially in electronic communication, e.g. microblogs, SMS, search queries.
However, sometimes these texts are not self-contained and need to be explained since understanding them requires knowledge of terminology, named entities or related facts.
The first aim of this work is to help a user to better understand a short message by extracting a context from an external source like a text collection, the Web or the Wikipedia by means of text summarization.
To this end we developed an approach for automatic multi-document summarization and we applied it to short message contextualization, in particular to tweet contextualization.
The proposed method is based on named entity recognition, part-of-speech weighting and sentence quality measuring.
In contrast to previous research, we introduced an algorithm for smoothing from the local context.
Moreover, we developed a graph-based algorithm for sentence reordering.
The method was also adapted to snippet retrieval.
The evaluation results indicate good performance of the approach.
Trauma, whether accidental or intentional, is a major public health problem.
In France, until recently, only road safety was the subject of attention claiming national exhaustiveness of epidemiological surveillance, coordinated by the Ministry of the Interior and with no strong link with the health system.
The current system for centralizing summaries of emergency department visits (OSCOUR® network) is a powerful real-time surveillance tool that, through the coding of primary and secondary diagnoses (ICD-10 codes), provides indicators on certain seasonal pathologies suchh as influenza or gastroenteritis.
However, this type of information is not sufficient to establish surveillance indicators related to trauma because the mechanism that led to the trauma (motor vehicle accident, suicide, violence, fall, etc.) is not known.
The addition of mechanisms related to trauma would allow for national epidemiological surveillance of trauma, evaluation of prevention strategies, etc., as well as the development of indicators for the prevention of trauma and improve predictive patient flow management.
The detailed reason for coming to the emergency department is not available in a standardized database but is described in detail in free-text clinical notes that are stored in electronic medical records.
The overall objective of the project is therefore to develop a tool that would allow the creation of standardized information on trauma mechanisms from these clinical texts.
To this end, the latest NLP (Natural Language Processing) techniques as part of Artifiial Intelligence will be tested, compared and applied.
This research is set within the fields of third language acquisition and multilingualism with a focus on developing our understanding of how multilingual competence functions.
The research attempts to determine if the languages of a multilingual play distinctive roles in the acquisition of a new language specifically regarding the lexical and syntactic components in crosslinguistic interactions, and whether there is convergence between these phenomena.
The research also examines the metalinguistic and crosslinguistic activities which allow learners to manage and comprehend the target language.
The data for this research was collected from eleven case studies of speakers of Spanish and English with a variety of levels of proficiency in French.
Participants were required to complete three speaking tasks.
Approaches sourced from research from the fields of third language acquisition and language contact were utilised and developed to capture the complexity of the interactions, the typological relationship among the languages in contact, and the varying levels of the languages under examination.
Using quantitative and qualitative analysis this research demonstrates how multilingual competence operates.
The data indicates that participants activate all their languages to different degrees relating to different types of crosslinguistic interactions, syntactic properties and level of language.
Participants also typically resort to their metalinguistic and crosslinguistic awareness to help them comprehend the target language and manage their output by the use of cognates, crosslinguistic consultations and inferences.
This thesis is located at the crossroads of speech processing, pattern recognition and multimedia information retrieval: the indexing of emotions for searching by content.
In this context, our work is directed towards speaker independent emotional recognition and indexing.
Second, the best parameters were sorted and chosen by the method of Forced Sequential Forward Selection (FSFS) with the conclusion of the effectiveness of this method in combination with the proposed approach Gb Symbolic Standardization to face the problem of robustness in our speaker independent recognition system.
The other part of the thesis is based on the study of classification techniques used in the emotion recognition.
Some experience on the interlanguage environment are also studied.
Finally, on the basis of these results, an indexing engine was built on a reel corpus.
In this work, we explore how Natural Language Generation (NLG) techniques can be used to address the task of (semi-)automatically generating language learning material and activities in Camputer-Assisted Language Learning (CALL).
In particular, we show how a grammar-based Surface Realiser (SR) can be usefully exploited for the automatic creation of grammar exercises.
Our surface realiser uses a wide-coverage reversible grammar namely SemTAG, which is a Feature-Based Tree Adjoining Grammar (FB-TAG) equipped with a unification-based compositional semantics.
More precisely, the FB-TAG grammar integrates a flat and underspecified representation of First Order Logic (FOL) formulae.
In the first part of the thesis, we study the task of surface realisation from flat semantic formulae and we propose an optimised FB-TAG-based realisation algorithm that supports the generation of longer sentences given a large scale grammar and lexicon.
The approach followed to optimise TAG-based surface realisation from flat semantics draws on the fact that an FB-TAG can be translated into a Feature-Based Regular Tree Grammar (FB-RTG) describing its derivation trees.
The derivation tree language of TAG constitutes a simpler language than the derived tree language, and thus, generation approaches based on derivation trees have been already proposed.
Our approach departs from previous ones in that our FB-RTG encoding accounts for feature structures present in the original FB-TAG having thus important consequences regarding over-generation and preservation of the syntax-semantics interface.
The concrete derivation tree generation algorithm that we propose is an Earley-style algorithm integrating a set of well-known optimisation techniques: tabulation, sharing-packing, and semantic-based indexing.
In the second part of the thesis, we explore how our SemTAG-based surface realiser can be put to work for the (semi-)automatic generation of grammar exercises.
Usually, teachers manually edit exercises and their solutions, and classify them according to the degree of dificulty or expected learner level.
A strand of research in (Natural Language Processing (NLP) for CALL addresses the (semi-)automatic generation of exercises.
Mostly, this work draws on texts extracted from the Web, use machine learning and text analysis techniques (e.g. parsing, POS tagging, etc.).
These approaches expose the learner to sentences that have a potentially complex syntax and diverse vocabulary.
In contrast, the approach we propose in this thesis addresses the (semi-)automatic generation of grammar exercises of the type found in grammar textbooks.
In other words, it deals with the generation of exercises whose syntax and vocabulary are tailored to specific pedagogical goals and topics.
Because the grammar-based generation approach associates natural language sentences with a rich linguistic description, it permits defining a syntactic and morpho-syntactic constraints specification language for the selection of stem sentences in compliance with a given pedagogical goal.
"Sentiment", "opinion" and "emotion" are words really vaguely defined; not even the dictionary seems to be of any help, being it the first to define each of the three by using the remaining two.
And yet, the civilised world is heavily affected by opinions: companies need them to understand how to sell their products; people use them to buy the most fitting product and, more generally, to weigh their decisions; researchers exploit them in Artificial Intelligence studies to understand the nature of the human being.
Today we can count on a humongous amount of available information, though it's hard to use it.
In fact, the so-called “Big data” are not always structured – especially for certain languages.
French research suffers from a lack of readily available resources for tests.
In the context of Natural Language Processing, this thesis aims to explore the nature of sentiment and emotion.
Some of our contributions to the NLP research community are: creation of new resources for sentiment and emotion analysis, tests and comparisons of several machine learning methods to study the problem from different points of view-classification of online reviews using sentiment polarity, classification of product characteristics using Aspect-Based Sentiment Analysis.
Finally, a psycholinguistic study-supported by a machine learning and lexical approaches – on the relation between who judges, the reviewer, and the object that has been judged, the product.
If present search engines allow to find documents corresponding to a large information need, they are not adapted to a precise information need, such as "Who was the president of the United States in 1978?".
Question answering system aim at fulfilling such needs.
The keyword request of a classical search engine is replaced by a question, thus in natural language, and the output consists in the precise answer to the question, instead of a list of documents to read.
The question answering domain benefits from works in Information Retrieval (IR), but gives the user a facilitated interaction, thanks to the manipulation and comprehension of natural language of the Natural Language Processing domain (NLP).
The goal of this work is to study how to access to a precise information, and in particular what characterizes an answer to a question.
The level of linguistic knowledge needed for the system to recognize the link between a question and a potential answer is examined.
The issue of the link between question and answer is studied on a syntactic level, in order to determine the impact of this type of knowledge in a precise information retrieval process.
Information retrieval in semi-structured (practically written in XML) mixes aspects of traditional information retrieval and of database querying.
The structure is very important, but the information need is vague.
The retrieval unit can have different sizes (a paragraph, a figure, an entire article…).
Furthermore, XML flexibility may create some breaks in the natural flow of the text.
Problems raised at this level are many, notably for document content analysis and querying.
We studied the specific solutions that could bring the natural language processing (NLP) techniques.
We proposed a theoretical frame and a practical approach to allow the use of traditional textual analysis techniques in XML documents, disregarding the structure.
We also conceived an interface for querying XML documents in natural language, and proposed methods using the structure in order to improve the retrieval of relevant elements.
This thesis deals with an approach, guided by an ontology, designed to annotate documents from a corpus where each document describes an entity of the same type.
In our context, all documents have to be annotated with concepts that are usually too specific to be explicitly mentioned in the texts.
In addition, the annotation concepts are represented initially only by their name, without any semantic information connected to them.
Finally, the characteristics of the entities described in the documents are incomplete.
The population step (1) adds to the ontology information from the documents in the corpus but also from the Web of Data (Linked Open Data or LOD).
The LOD represents today a promising source for many applications of the Semantic Web, provided that appropriate techniques of data acquisition are developed.
In the settings of SAUPODOC, the ontology population has to take into account the diversity of the data in the LOD: multiple, equivalent, multi-valued or absent properties.
The correspondences to be established, between the vocabulary of the ontology to be populated and that of the LOD, are complex, thus we propose a model to facilitate their specification.
Then, we show how this model is used to automatically generate SPARQL queries and facilitate the interrogation of the LOD and the population of the ontology.
The latter, once populated, is then enriched (2) with the annotation concepts and definitions that are learned through examples of annotated documents.
Reasoning on these definitions finally provides the desired annotations.
Experiments have been conducted in two areas of application, and the results, compared with the annotations obtained with classifiers, show the interest of the approach.
This new era of small UAVs currently populating the airspace introduces many safety concerns, due to the absence of a pilot onboard and the less accurate nature of the sensors.
This necessitates intelligent approaches to address the emergency situations that will inevitably arise for all classes of UAV operations as defined by EASA (European Aviation Safety Agency).
Hardware limitations for these small vehicles point to the utilization of analytical redundancy, rather than to the usual practice of hardware redundancy in manned aviation.
In the course of this study, machine learning practices are implemented in order to diagnose faults on a small fixed-wing UAV to avoid the burden of accurate modeling needed in model-based fault diagnosis.
A supervised classification method, SVM (Support Vector Machines) is used to classify the faults.
The data used to diagnose the faults are gyro and accelerometer measurements.
The idea to restrict the data set to accelerometer and gyro measurements is to check the method's classification ability, with a small and inexpensive chip set and without the need to access the data from the autopilot, such as the control input information.
This work addresses the faults in the control surfaces of a UAV.
More specifically, the faults considered are the control surface stuck at an angle and the loss of effectiveness.
First, a model of an aircraft is simulated.
This model is not used for the design of Fault Detection and Diagnosis (FDD) algorithms, but is instead utilized to generate data.
Simulated data are used instead of flight data in order to isolate the probable effects of the controller on the diagnosis, which may complicate a preliminary study on FDD for drones.
The results show that for simulated measurements, SVM gives very accurate results on the classification of the loss of effectiveness faults on the control surfaces.
These promising results call for further investigation so as to assess SVM performance on fault classification with flight data.
Real flights were arranged to generate faulty flight data by manipulating the open source autopilot, Paparazzi.
All data and the code are available in the code sharing and versioning system, Github.
Training is held offline due to the need for labeled data and the computational burden of the tuning phase of the classifiers.
Results show that from the flight data, SVM yields an F1 score of 0.98 for the classification of control surface stuck faults.
For the loss of efficiency faults, some feature engineering, involving the addition of past measurements is needed in order to attain the same classification performance.
A promising result is discovered when spinors are used as features instead of angular velocities.
Results show that by using spinors for classification, there is a vast improvement in classification accuracy, especially when the classifiers are untuned.
Using spinors and a Gaussian Kernel, an untuned classifier gives an F1 score of 0.9555, which was 0.2712 when gyro measurements were used as features.
In summary, this work shows that SVM gives a satisfactory performance for the classification of faults on the control surfaces of a drone using flight data.
This work addresses the issue of establishing and maintaining the coordination in a mixed human-agent teamwork in the context of CVET.
The objective of this research is to provide human-like conversational behavior of the virtual agents in order to cooperate with a user and other agents to achieve shared goals.
We propose a belief-desire-intention (BDI) like Collaborative Conversational agent architecture(C2BDI) that treats both deliberative and conversational behaviors uniformly as guided by the goal-directed shared activity.
We put forward an integrated model of coordination which is founded on the shared mental model based approaches to establish coordination in a human-agent teamwork.
We argue that natural language interaction between team members can affect and modify the individual and shared mental models of the participants.
Finally, we describe the cultivation of coordination in a mixed human-agent teamwork through natural language conversation.
In order to establish the strong coupling between decision making and the collaborative conversational behavior of the agent, we propose first, the Mascaret based semantic modeling of human activities and the VE, and second, the information state based context model.
This representation allows the treatment of semantic knowledge of the collaborative activity and virtual environment, and information exchanged during the dialogue conversation in a unified manner.
To endow the communicative capabilities to C2BDI agent, we put forward the information state based approach for the natural language processing of the utterances.
We define collaborative conversation protocols that ensure the coordination between team members.
Finally, in this thesis, we propose a decision making mechanism, which is inspired by the BDI based approach and provides the interleaving between deliberation and conversational behavior of the agent.
We have applied the proposed architecture to three different scenarios in the CVET.
We found that the multiparty collaborative conversational behavior of C2BDI agent is more constructive and facilitates the user to effectively coordinate with other team members to perform a shared task.
Numerous domains have interests in studying the viewpoints expressed online, be it for marketing, cybersecurity, or research purposes with the rise of computational social sciences.
We propose in this manuscript two contributions to the field of stance detection, focused around the difficulty of obtaining annotated data of quality on social medias.
Our first contribution is a large and complex dataset of 22853 Twitter profiles active during the French presidential campaign of 2017.
This is one of the rare datasets that considers a non-binary stance classification and, to our knowledge, the first one with a large number of profiles, and the first one proposing overlapping political communities.
This dataset can be used as-is to study the campaign mechanisms on Twitter, or used to test stance detection models or network analysis tools.
We then propose two semi-supervised generic stance detection models using a handful of seed profiles for which we know the stance to classify the rest of the profiles by exploiting various proximities.
Indeed, current stance detection models are usually grounded on the specificities of some social platforms, which is unfortunate since it does not allow the integration of the multitude of available signals.
By infering proximities from differents types of elements available on social medias, we can detect profiles close enough to assume they share a similar stance on a given subject.
Our first model is a sequential ensemble algorithm which propagates stances thanks to a multi-layer graph representing proximities between profiles.
Using datasets from two platforms, we show that, by combining several types of proximities, we can achieve excellent results.
Our second model allows us to observe the evolution of profiles'stances during an event with as little as one seed profile by stance.
This model confirms that a large majority of profiles do not change their stance on social medias, or do not express their change of heart.
With the growing mass of textual data on the Web, automatic summarization of topic-oriented collections of documents has become an important research field of Natural Language Processing.
The experiments described in this thesis were framed within this context.
We proposed several similarity measures which were evaluated and compared on different data sets: the SemEval 2014 challenge corpus for the English language and own built datasets for French.
The good performance showed by our measures led us to use them in a multi-document summary task, which implements a pagerank-type algorithm.
The system was evaluated on the DUC 2007 datasets for English and RPM2 corpus for French.
This simple approach, based on a resource readily available in many languages, proved efficient, robust and the encouraging outcomes open up real prospects of improvement.
The importance of collaborative systems in real-world applications has grown significantly over the recent years.
The majority of new applications are designed in a distributed fashion to meet collaborative work requirements.
Although such applications are more and more used into many fields, the lack of an adequate access control concept is still limiting their full potential.
We propose a optimistic approach to enforce access control in existing collaborative editing solutions in the sense that a user can temporarily violate the access control policy.
To enforce the policy, we resort to the selective undo approach in order to eliminate the effect of illegal document updates.
We investigate a theoretical study of the undo problem and propose a generic solution for selectively undoing operations.
Finally, we apply our framework on a collaboration prototype and measure its performance in the distributed grid GRID?5000 to highlight the scalability of our solution
This thesis deals with the field of information retrieval and the recommendation of reading.
It has for objects: — The creation of new approach of document retrieval and recommendation using techniques of combination of results, aggregation of social data and reformulation of queries; — The creation of an approach of recommendation using methods of information retrieval and graph theories.
Two collections of documents were used.
First one is a collection which is provided by CLEF (Social Book Search-SBS) and the second from the platforms of electronic sources in Humanities and Social Sciences OpenEdition.org (Revues.org).
The modelling of the documents of every collection is based on two types of relations: — For the first collection (SBS), documents are connected with similarity calculated by Amazon which is based on several factors (purchases of the users, the comments, the votes, products bought together, etc.); — For the second collection (OpenEdition), documents are connected with relations of citations, extracted from bibliographical references.
The manuscript is structured in two parts.
The first part "state of the art" includes a general introduction, a state of the art of informationretrieval and recommender systems.
The second part "contributions" includes a chapter on the detection of reviews of books in Revues.org; a chapter on the methods of IR used on complex queries written in natural language and last chapter which handles the proposed approach of recommendation which is based on graph.
Question Answering is a discipline which lies in between natural language processing and information retrieval domains.
Emergence of deep learning approaches in several fields of research such as computer vision, natural language processing, speech recognition etc. has led to the rise of end-to-end models.
In the context of GoASQ project, we investigate, compare and combine different approaches for answering questions formulated in natural language over textual data on open domain and biomedical domain data.
The thesis work mainly focuses on 1) Building models for small scale and large scale datasets, and 2) Leveraging structured and semantic information into question answering models.
Hybrid data in our research context is fusion of knowledge from free text, ontologies, entity information etc. applied towards free text question answering.
The current state-of-the-art models for question answering use deep learning based models.
In order to facilitate using them on small scale datasets on closed domain data, we propose to use domain adaptation.
We model the BIOASQ biomedical question answering task dataset into two different QA task models and show how the Open Domain Question Answering task suits better than the Reading Comprehension task by comparing experimental results.
We pre-train the Reading Comprehension model with different datasets to show the variability in performance when these models are adapted to biomedical domain.
We find that using one particular dataset (SQUAD v2.0 dataset) for pre-training performs the best on single dataset pre-training and a combination of four Reading Comprehension datasets performed the best towards the biomedical domain adaptation.
We perform some of the above experiments using large scale pre-trained language models like BERT which are fine-tuned to the question answering task.
The performance varies based on the type of data used to pre-train BERT.
For BERT pre-training on the language modelling task, we find the biomedical data trained BIOBERT to be the best choice for biomedical QA.
We highlight the necessity for using Lexical and Expected Answer Types in open domain and biomedical domain question answering by performing several verification experiments.
These types are used to highlight entities in two QA tasks which shows improvements while using entity embeddings based on the answer type annotations.
We manually annotated an answer variant dataset for BIOASQ and show the importance of learning a QA model with answer variants present in the paragraphs.
Our hypothesis is that the results obtained from deep learning models can further be improved using semantic features and collective features from different paragraphs for a question.
We propose to use ranking models based on binary classification methods to better rank Top-1 prediction among Top-K predictions using these features, leading to an hybrid model that outperforms state-of-art-results on several datasets.
We experiment with several overall Open Domain Question Answering models on QA sub-task datasets built for Reading Comprehension and Answer Sentence Selection tasks.
We show the difference in performance when these are modelled as overall QA task and highlight the wide gap in building end-to-end models for overall question answering task.
This thesis presents: 1) the development of a novel approach to find direct associations between pairs of elements linked indirectly through various common features, 2) the use of this approach to directly associate biological functions to protein domains (ECDomainMiner and GODomainMiner), and to discover domain-domain interactions, and finally 3) the extension of this approach to comprehensively annotate protein structures and sequences.
Using inferred function-domain associations and considering taxonomy information, thousands of annotation rules have automatically been generated.
Then, these rules have been utilized to annotate millions of protein sequences in the TrEMBL database
The work includes the definition of a model for opinion predicates and their arguments (source, topic and message), the creation of a lexicon of opinion predicates which have information from the model associated, and the implementation of three systems.
Indeed these works had scores that fall between 63% and 89.5%.Moreover, in addition to the systems made for the identification of opinions, our work has led to the construction of several resources for Spanish: a lexicon of opinion predicates, a 13,000 words corpus with opinions annotated and a 40,000 words corpus with opinion predicates end sources annotated.
Producing a discourse to share an emotion or express a feeling requires the interlocutor to frame situated discursive strategies as well as in a specific communicative situation as in a particular language-culture.
This study on specialized language in Ecuadorian context focus on speakers and the strategies they build to express their emotions while tasting specific products.
Indeed, various interrogations arise such as: "Does a specific textual genre exist to describe the procedure and the discursive prototype of a tasting praxis?";
the one regarding the link between a discursive production and the speaker identity who uses different lexicons and grammars to talk about a specific product: "Do experts and consumers follow the same discursive strategies?";
the one regarding a situated terminology: "How do social representations and discursive praxis affect the involved speaker contributions?".
Indeed, these interrogations should help to characterise the expression of the sensoriality situated in a specific professional context, tasting in Spanish language within an Ecuadorian culture, while offering a scientific interpretation of terminological choices used as sensory and hedonic descriptors which include an emotive dimension.
Therefore, this study is grounded in the cognitive semantics, corpus linguistics, and textometric analysis theoretical frameworks where discursive practices are representative of sociocultural praxis.
The suggested methodology uses computer-based and data-mining tools proper to natural language processing applied to the three followed corpora:
1. The corpus of diachronic texts built on the compilation of the last ten years specialised media written production which allows to identify the current descriptors used to describe the perceived feelings and emotions while tasting a chocolate bar.
2. The corpus built on speakers' texts they produce when they share their own descriptors conceptualisation and meaning.
This corpus will be compiled without any contact with the product, the chocolate, in order to focus on the cognitive dimension of the selected descriptor social representation.
3. And the corpus built on spoken productions where these descriptors are in discourse to express the perceived feelings and emotions while tasting a chocolate bar.
The analysis of these different corpora offers an authentic research topic which is representative of speakers' perceptions when they set up their discursive strategies to share an emotion or express a feeling.
This method allows a qualitative interpretation on the basis of a quantitative data analysis in order to suggest a description of different expression strategies of sensory experience.
Beside these corpus compilations, filling a questionnaire before the tasting session allows the speakers' characterisation and the data collection about the social representation they could mobilise in their discursive choices.
Beyond the compilation of an unseen corpus and its discursive analysis, this study is of interest to provide a study on the strategies in use to express emotions and feelings in a tasting context, hence it offers a description of a specialized discourse genre which could improve the produced discourse to address potential consumers.
The main ambitious objective of the project is also its main originality.
It consists in automatically extracting the narrative structure of TV series.
Current TV series are based on complex structures involving several intertwined story arcs within the same episode.
The first scientific barrier is therefore related to the so-called semantic gap between the actual storyline conveyed by TV series and the type of information that can be automatically extracted and processed by computer programs.
A narrative approach could be based only on searching information such as the collection temporal structure, characters (who are they?)
But, this difficult problem cannot be solved using only one of its sides (who?
The thesis, conducted as part of a CIFRE grant, and extending one of the aspects of the ANR project Traouiero, first addresses the production, extension and improvement of multilingual corpora by machine translation (MT) and contributory post-editing (PE).
Functional and technical improvements have been made to the SECTra and iMAG software produced in previous PhD theses (P.C. Huynh, H.T. Nguyen), and progress has ben made toward a generic definition of the structure of a multilingual, annotated and multi-media corpus that may contain usual documents as well as pseudo-documents (such as Web pages) and meta-segments.
As part of an internal project on the LIG website and of a project (TABE-FC) in cooperation with Xiamen University, it has been possible to demonstrate the value of incremental learning in statistical MT, under certain conditions, through an experiment that spread over the whole thesis.
The third part of the thesis is devoted to contributing and making available computer tools and resources.
The main ones are related to the COST project MUMIA of the EU and result from the exploitation of the CLEF-2011 collection of 1.5 million partially multilingual patents.
Large translation memories have been extracted from it (17.5 million segments), 3 MT systems have been produced (de-fr, en-fr, fr-de), and a website of support for multilingual IR on patents has been constructed.
One also describes the on-going implementation of JianDan-eval, a platform for building, deploying and evaluating MT systems.
Modeling complex processes often involve a high number of variables with anintricate correlation structure.
For example, many spatially-localized processes display spatial regularity, as variables corresponding to neighboring regions are more correlated than distant ones.
The formalism of weighted graphs allows us to capture relationships between interacting variables in a compact manner, permitting the mathematical formulation of many spatial analysis tasks.
We introduce a new preconditioning scheme for the existing generalized forward-backward proximal splitting algorithm, specifically designed for graphs with high variability in neighbourhood configurations and edge weights.
We show that our proposed approaches reach or outperform state-of-the-art for geostatistical aggregation as well as image recovery problems.
The second part focuses on the development of a new model, expanding continuous-time Markov chain models to general undirected weighted graphs.
This allows us to take into account the interactions between neighbouring nodes in structured classification, as demonstrated for a supervised land-use classification task from cadastral data.
This thesis presents a non-standardized text analysis approach which consists a chain process modeling allowing the automatic annotation of texts: grammar annotation using a morphosyntactic tagging method and semantic annotation by putting in operates a system of named-entity recognition.
In this context, we present a system analysis of the Middle French which is a language in the course of evolution including: spelling, the flexional system and the syntax are not stable.
The texts in Middle French are mainly distinguished by the absence of normalized orthography and the geographical and chronological variability of medieval lexicons.
The main objective is to highlight a system dedicated to the construction of linguistic resources, in particular the construction of electronic dictionaries, based on rules of morphology.
Then, we will present the instructions that we have carried out to construct a morphosyntactic tagging which aims at automatically producing contextual analyzes using the disambiguation grammars.
Finally, we will retrace the path that led us to set up local grammars to find the named entities.
Hence, we were asked to create a MEDITEXT corpus of texts in Middle French between the end of the thirteenth and fifteenth centuries.
This thesis deals with neural network based coarticulation modeling, and aims to synchronize facial animation of a 3D talking head with speech.
We propose in this work a coarticulation model, i.e. a model able to predict spatial trajectories of articulators from speech.
We rely on a sequential model, the recurrent neural networks, and more specifically the Gated Recurrent Units, which are able to consider the articulation dynamic as a central component of its modeling.
Unfortunately, the typical amount of data in articulatory and audiovisual databases seems to be quite low for a deep learning approach.
To overcome this difficulty, we propose to integrate articulatory knowledge into the networks during its initialization.
The RNNs robustness allow uw to apply our coarticulation model to predict both face and tongue movements, in french and german for the face, and in english and german for the tongue.
Evaluation has been conducted through objective measures of the trajectories, and through experiments to ensure a complete reach of critical articulatory targets.
We also conducted a subjective evaluation to attest the perceptual quality of the predicted articulation once applied to our facial animation system.
Finally, we analyzed the model after training to explore phonetic knowledges learned.
Nowadays, machine translation has reached good results when applied to several language pairs such as English – French, English – Chinese, English – Spanish, etc.
However, research on machine translation for under-resourced language pairs always faces to the lack of training data.
Thus, we have addressed the problem of retrieving a large parallel bilingual text corpus to build a statistical machine translation system.
The originality of our work lies in the fact that we focus on under-resourced languages for which parallel bilingual corpora do not exist in most cases.
This manuscript presents our methodology for extracting a parallel corpus from a comparable corpus, a richer and more diverse data resource over the Web.
We propose three methods of extraction.
The first method follows the classical approach using general characteristics of documents as well as lexical information of the document to retrieve both parallel documents and parallel sentence pairs.
However, this method requires additional data of the language pair.
The second method is a completely unsupervised method that does not require additional data and it can be applied to any language pairs, even under resourced language pairs.
The last method deals with the extension of the second method using a third language to improve the extraction process (triangulation).
The proposed methods are validated by a number of experiments applied on the under resourced Vietnamese language and the English and French languages.
This thesis describes the applications of natural language processing (NLP) to industrial risk management.
We focus on the domain of civil aviation, where incident reporting and accident investigations produce vast amounts of information, mostly in the form of textual accounts of abnormal events, and where efficient access to the information contained in the reports is required.
We start by drawing a panorama of the different types of data produced in this particular domain.
We analyse the documents themselves, how they are stored and organised as well as how they are used within the community.
We show that the current storage and organisation paradigms are not well adapted to the data analysis requirements, and we identify the problematic areas, for which NLP technologies are part of the solution.
Specifically addressing the needs of aviation safety professionals, two initial solutions are implemented: automatic classification for assisting in the coding of reports within existing taxonomies and a system based on textual similarity for exploring collections of reports.
Based on the observation of real-world tool usage and on user feedback, we propose different methods and approaches for processing incident and accident reports and comprehensively discuss how NLP can be applied within the safety information processing framework of a high-risk sector.
By deploying and evaluating certain approaches, we show how elusive aspects related to the variability and multidimensionality of language can be addressed in a practical manner and we propose bottom-up methods for managing the overabundance of textual feedback data
This thesis addresses the analysis and generation of expressive movements for virtual human characters.
Based on previous results from three different research areas (perception of emotions and biological motion, automatic recognition of affect and computer character animation), a low-dimensional motion representation is proposed.
This representation consists of the spatio-temporal trajectories of end-effectors (i.e., head, hands and feet) and pelvis.
We have argued that this representation is both suitable and sufficient for characterizing the underlying expressive content in human motion and for controlling the generation of expressive whole-body movements.
In order to prove these claims, this thesis proposes:
i.) A new motion capture database inspired by physical theater theory.
This database contains examples from different motion classes (i.e., periodic movements, functional behaviors, spontaneous motions, and theater-inspired motion sequences) and distinct emotional states (happiness, sadness, relaxedness, stress and neutral) performed by several actors.
ii.) A user study and automatic classification framework de-signed to qualitatively and quantitatively assess the amount of emotion-related information conveyed and encoded in the proposed representation.
We have observed that although slight differences in performance were found with respect to the cases in which the entire body was used, our proposed representation preserves most of the motion cues salient to the expression of affect and emotions.
iii.) A simple motion synthesis system able to capable of: a) reconstructing whole-body movements from the proposed low-dimensional representation, and b) producing novel end-effector (and pelvis) expressive trajectories.
A quantitative and qualitative evaluation of the generated whole body motions shows that these motions are as expressive as the movements recorded from human actors
The extraction of spatial information from textual data has become an important research topic in the field of Natural Language Processing (NLP).
It meets a crucial need in the information society, in particular, to improve the efficiency of Information Retrieval (IR) systems for different applications (tourism, spatial planning, opinion analysis, etc.).
Such systems require a detailed analysis of the spatial information contained in the available textual data (web pages, e-mails, tweets, SMS, etc.).
However, the multitude and the variety of these data, as well as the regular emergence of new forms of writing, make difficult the automatic extraction of information from such corpora.
To meet these challenges, we propose, in this thesis, new text mining approaches allowing the automatic identification of variants of spatial entities and relations from textual data of the mediated communication.
These approaches are based on three main contributions that provide intelligent navigation methods.
Our first contribution focuses on the problem of recognition and identification of spatial entities from short messages corpora (SMS, tweets) characterized by weakly standardized modes of writing.
The second contribution is dedicated to the identification of new forms/variants of spatial relations from these specific corpora.
Finally, the third contribution concerns the identification of the semantic relations associated withthe textual spatial information.
In this thesis, we study several models of collaboration between Software Engineering and Semantic Web.
The main objective of our work is to provide the developer with the tools to design, in the declarative manner, a business "executable" layer of an application in order to simulate its operation and thus show the compliance of the application with the customer requirements defined at the beginning of the software life cycle.
On the other hand, another advantage of this approach is to allow the developer to share and reuse the business layer description of a typical application in a domain using ontology.
This typical application description is called "Application Template".
The reuse of the business layer description of an application is an interesting aspect of software engineering.
That is the key point we want to consider in this thesis.
In the first part of this thesis, we deal with the modeling of the business layer.
We first present an ontology-based approach to represent business process and the business rules and show how to verify the consistency of business process and the set of business rules.
Then, we present an automatic check mechanism of compliance of business process with a set of business rules.
The second part of this thesis is devoted to define a methodology, called personalization, of creating of an application from an "Application Template".
This methodology will allow the user to use an Application Template to create his own application by avoiding deadlock and semantic errors.
We introduce at the end of this part the description of an experimental platform to illustrate the feasibility of the mechanisms proposed in the thesis.
Through this study we introduce a dynamic perspective of semantic annotation.
This perspective considers the passage of time and the permanent ﬂow of documents that makes the collections grow and their annotation systems to extend and evolve.
We also bring a vision of the quality of annotations systems based on the notion of information access.
In our vision, the quality of annotation vocabulary depends on the amount and complexity of information to be navigated by a user while searching for a certain topic.
To address the problem of the dynamics in semantic annotation, this work proposes a modular architecture for dynamic semantic annotation.
This architecture models the activities involved in the semantic annotation process in abstract modules dedicated to the diﬀerent tasks that users have to perform.
As a case of study we took blogging annotation.
We gathered a corpus containing up to 10 years of annotated blog posts with categories and tags and we analyzed the annotation habits.
By testing automatic tag and category strategies, we measure the impact of the dynamics in the annotation system.
We propose some strategies to control this impact, which helps to evaluate the obsolescence of examples.
Finally we propose a framework relying on three quality metrics and an interactive method to recover the quality of an indexing system based on semantic annotation.
The metrics are evaluated over time to observe the degradation in indexing quality.
A series of studied examples are presented to observe the performance of the measures to guide the restructuring of the indexing annotation system.
While vaccines represent a great achievement for public health, the risk of adverse effects is a real threat for vaccine acceptability by both the population and healthcare professionals.
France still ranks as the country having the highest vaccine defiance.
This often turned into poor vaccination coverages.
This origin of this mistrust in vaccines is probably related to the intense polemic around anti-hepatitis B (HB) vaccination and the risk of multiple sclerosis in the 1990's.
The main aim of this thesis was to assess the putative link between vaccination and demyelinating disorders by considering two examples: anti-HB and anti-papillomavirus (HPV) vaccines.
For both vaccines, methods adopted a stepwise evidence-based approach.
Hypothesis generation was based on evidence regarding the biological plausibility, the published case reports, the disproportionality analyses conducted in the US Vaccine Adverse Event Reporting System (VAERS) and the analysis of signals detected by spontaneous reporting systems, if any.
For the research question centered on the anti-HB vaccination, observed-to-expected analyses based on all confirmed cases reported to the French pharmacovigilance in the 1990's were also conducted.
Results were non-conclusive for both vaccines.
For anti-HB vaccination, several elements could give credence to an association with central demyelination: a weak and indirect biological plausibility, the analysis of the French signal detected in the 1990's which revealed a complete disjunction between the target and the joint populations, and the results of the disproportionality analyses in VAERS.
Nevertheless, neither the meta-analysis nor the observed-to-expected analyses (although might be easily reversed by a moderate degree of underreporting), provided statistically significant findings.
If the excess risk actually existed, it would be weak and would be a concern for adults only.
The current recommendations which are minimizing the probability of the French population to be exposed at an adult age, are therefore more than justified.
For the anti-HPV vaccination, after reviewing all materials available, the risk of central demyelination seems, at this date, unlikely.
Nevertheless, a doubt remains regarding a possible excess risk of Guillain Barré Syndrome (GBS) in the follow of an anti-HPV immunization.
To conclude, a strong association with a risk of central demyelination can be ruled out for both vaccines, making the benefit and risk balances still largely positive for both products if used in their current target populations.
In that context, an independent, clear and scientifically-based communication is the key element to promote vaccination programmes and to generate the confidence and adherence of the general population.
The future of vaccine pharmacovigilance could rely on the implementation of a collaborative GP-patient network-based solution using SMS and smartphones, as already experimented in Australia.
While collecting potential adverse effects of vaccines, it would also be a unique opportunity to place the patients at the heart of the surveillance system, giving them a voice and potentially contributing to restore their confidence in vaccines and even, in the decision-makers in the field of public health.
From a general point of view this thesis addresses an automatic path to build a solution choosing a compatible set of building blocks to provide such a solution to solve a given problem.
To create the solution it is considered the compatibility of each available building block with the problem and also the compatibility between each building block to be employed within a solution all together.
In the particular perspective of this thesis the building blocks are meta-models and the given problem is a description of a problem that can be solved using software using a multi-agent system paradigm.
Nevertheless if no solution is found it also indicates that the problem can not be solved through this paradigm using the available meta-models.
The process addressed by the thesis consists of the following main steps:
(1) Through a process of characterization the problem description is analyzed in order to locate the solution domain and therefore employ it to choose a list of most domain compatible meta-models as candidates.
(3) The matching step is built over a multi-agent system where each agent represents a candidate meta-model.
Within this multi-agent system each agent interact with each other in order to find a group of suitable meta-models to represent a solution.
Each agent use as criteria the compatibility between their represented candidate meta-model with the other represented meta-models.
This thesis focuses on providing a process and a prototype tool to solve the last step.
Therefore the proposed path has been created using several concepts from meta-analysis, cooperative artificial intelligence, Bayesian cognition, uncertainty, probability and statistics.
The aim of the dissertation is to analyze Polish and French manner of motion verbs and the properties that they display as predicates.
In both languages, the argument structures of such predicates determine the verb grammatical behavior.
Isc, jechac, plynac, biec, leciec, frunac, pelznac are known as determinate verbs; they lexicalize as manner and path of motion.
The verbs taken under analysis in chapter 3 form the indetreminate sub-group: chodzic, jezdzic, plywac, biegac, latac, fruwac, pelzac conflate motion and its manner, but unlike their determinate correpondents, their lexical meaning does not contain any information relative to path.
Chapter 5 summarizes the main differences between Polish and French manner of motion verbs.
In other words, they are able to describe as well telic and atelic motion events.
Automatic verification has nowadays become a central domain of investigation in computer science.
Over 25 years, a rich theory has been developed leading to numerous tools, both in academics and industry, allowing the verification of Boolean properties-those that can be either true or false.
Current needs evolve to a finer analysis, a more quantitative one.
Extension of verification techniques to quantitative domains has begun 15 years ago with probabilistic systems.
However, many other quantitative properties are of interest, such as the lifespan of an equipment, energy consumption of an application, the reliability of a program, or the number of results matching a database query.
Expressing these properties requires new specification languages, as well as algorithms checking these properties over a given structure.
This thesis aims at investigating several formalisms, equipped with weights, able to specify such properties: denotational ones-like regular expressions, first-order logic with transitive closure, or temporal logics-or more operational ones, like navigating automata, possibly extended with pebbles.
A first objective of this thesis is to study expressiveness results comparing these formalisms.
In particular, we give efficient translations from denotational formalisms to the operational one.
These objects, and the associated results, are presented in a unified framework of graph structures.
This permits to handle finite words and trees, nested words, pictures or Mazurkiewicz traces, as special cases.
Therefore, possible applications are the verification of quantitative properties of traces of programs (possibly recursive, or concurrent), querying of XML documents (modeling databases for example), or natural language processing.
Second, we tackle some of the algorithmic questions that naturally arise in this context, like evaluation, satisfiability and model checking.
In particular, we study some decidability and complexity results of these problems depending on the underlying semiring and the structures under consideration (words, trees...).
Finally, we consider some interesting restrictions of the previous formalisms.
Some permit to extend the class of semirings on which we may specify quantitative properties.
Another is dedicated to the special case of probabilistic specifications: in particular, we study syntactic fragments of our generic specification formalisms generating only probabilistic behaviors.
It is well known in the enterprises that each new project to be carried out is usually similar to a certain previous projects. Those projects can be structured according to a common reference process depending on their type.
The main difficulty lies in the formalization of the expertise business process.
The traditional knowledge capitalization approaches based on the experts'debriefings showed their limits: the experts often leave out details which may be of relevance because the debriefings are habitually realizedexternally to the activities.
Our thesis relies on the idea that it is possible to construct the operational process, implemented during the collaborative activities in a product development study, from the traces recorded by the used IT tools.
The constructed operational process allows the business actors and experts to step back on their work and formalize the new deducted experience to enhance the expertise business processes of the firm.
Our work had taken place in the ERPI (Equipe de Recherche sur les Processus Innovatifs) laboratory of the “Université de Lorraine” under a partnership with TDC Software society and through a CIFRE Convention.
This dissertation offers five key contributions:
• A double cycle to capitalize over the instrumented activities.
• A global approach for the management of expertise business processes.
• An ontology “OntoProcess” to conceive the generic organizational aspects, separating distinctly the concepts related to traces from those related to the business process, and providing extensions in function of the used tools.
• A multi-agents system based on the ontology “OntoProcess” to support the presented global approach of the expertise business processes management.
• A trace based system that allows the construction of the operational process from the traces registered over the study
The popularization of social networks and digital documents increased quickly the information available on the Internet.
However, this huge amount of data cannot be analyzed manually.
We also analyzed other NLP tasks (word encoding representation,semantic similarity, sentence and multi-sentence compression) to generate more stable and informative cross-lingual summaries.
Most of NLP applications (including all types of text summarization) use a kind of similarity measure to analyze and to compare the meaning of words, chunks, sentences and texts in their approaches.
A way to analyze this similarity is to generate a representation for these sentences that contains the meaning of them.
The meaning of sentences is defined by several elements, such as the context of words and expressions, the order of words and the previous information.
Analyzing these problems,we propose a neural network model that combines recurrent and convolutional neural networks to estimate the semantic similarity of a pair of sentences (or texts) based on the local and general contexts of words.
Our model predicted better similarity scores than baselines by analyzing better the local and the general meanings of words and multi-word expressions.
In order to remove redundancies and non-relevant information of similar sentences,we propose a multi-sentence compression method that compresses similar sentences by fusing them in correct and short compressions that contain the main information of these similar sentences.
We model clusters of similar sentences as word graphs.
Our approach outperformed baselines by generating more informative and correct compressions for French, Portuguese and Spanish languages.
Finally, we combine these previous methods to build a cross-language text summarization system.
Our system is an {English, French, Portuguese, Spanish}-to-{English,French} cross-language text summarization framework that analyzes the information in both languages to identify the most relevant sentences.
Inspired by the compressive text summarization methods in monolingual analysis, we adapt our multi-sentence compression method for this problem to just keep the main information.
Analyzing {English,French, Portuguese, Spanish}-to-{English, French} cross-lingual summaries, our system significantly outperforms extractive baselines in the state of the art for all these languages.
The popularity of OSM is mainly conditioned by the integrity and the quality of UGC as well as the protection of users'privacy.
Based on the definition of information quality as fitness for use, the high usability and accessibility of OSM have exposed many information quality (IQ) problems which consequently decrease the performance of OSM dependent applications.
Such problems are caused by ill-intentioned individuals who misuse OSM services to spread different kinds of noisy information, including fake information, illegal commercial content, drug sales, mal-ware downloads, and phishing links.
The propagation and spreading of noisy information cause enormous drawbacks related to resources consumptions, decreasing quality of service of OSM-based applications, and spending human efforts.
The majority of popular social networks (e.g., Facebook, Twitter, etc) over the Web 2.0 is daily attacked by an enormous number of ill-intentioned users.
However, those popular social networks are ineffective in handling the noisy information, requiring several weeks or months to detect them.
Moreover, different challenges stand in front of building a complete OSM-based noisy information filtering methods that can overcome the shortcomings of OSM information filters.
These challenges are summarized in: (i) big data; (ii) privacy and security; (iii) structure heterogeneity; (iv) UGC format diversity; (v) subjectivity and objectivity; (vi) and service limitations.
In this thesis, we focus on increasing the quality of social UGC that are published and publicly accessible in forms of posts and profiles over OSNs through addressing in-depth the stated serious challenges.
As the social spam is the most common IQ problem appearing over the OSM, we introduce a design of two generic approaches for detecting and filtering out the spam content.
The first approach is for detecting the spam posts (e.g., spam tweets) in a real-time stream, while the other approach is dedicated for handling a big data collection of social profiles (e.g., Twitter accounts).
The increasing need of human assistance pushed researchers to develop automatic, smart and tireless dialogue systems that can converse with humans in natural language to be either their virtual assistant or their chat companion.
The industry of dialogue systems has been very popular in the last decade and many systems from industry and academia have been developed.
In this thesis, we study retrieval-based dialogue systems which aim to find the most appropriate response to the conversation among a set of predefined responses.
The main challenge of these systems is to understand the conversation and identify the elements that describe the problem and the solution which are usually implicit.
Most of the recent approaches are based on deep learning techniques which can automatically capture implicit information.
However these approaches are either complex or domain dependent.
We propose a simple, end-to-end and efficient retrieval-based dialogue system that first matches the response with the history of the conversation on the sequence-level and then we extend the system to multiple levels while keeping the architecture simple and domain independent.
We perform several analyzes to determine possible improvements.
The aim of this work is to improve the clarity and precision of the technical specifications written in French by the engineers at CNES (Centre National d'Études Spatiales / National Centre for Space Studies) prior to the realization of space systems.
The importance of specifications (and particularly of the requirements that are part of them) for the success of large-scale projects is indeed widely acknowledged; similarly, the main risks associated with the use of natural language (ambiguity, vagueness, incompleteness) are relatively well identified.
A Controlled Natural Language (CNL) – i.e. a set of linguistic rules constraining the lexicon, the syntax and the semantics – seems to be an interesting option, provided that it remains close enough to natural language.
Unfortunately, the CNLs for technical writing that we have examined are not always relevant from a linguistic point of view.
Our methodology for developping a CNL for requirements writing in French at CNES relies on the hypothesis of the existence of a textual genre; besides, we make use of existing Natural Language Processing tools and methods to validate the relevance of the rules on a corpus of genuine requirements written for former projects.
To evaluate the accuracy of the analyser, we submitted Malaysian texts and one Indonesian text to the system.
This analyser uses: a set of rules, a few list of exceptions, a restricted list of bases and formal identification criteria.
The algorithm is non deterministic.
Analysed words are treated without taking account of their contexts.
The evaluation of the analyser gave around 97% of correct analysis and 2% of incorrect analysis.
Very few affixed words were not analysed (rate less than 0,5%)
Text has been the dominant way of storing data in computer systems and sending information around the Web.
Extracting meaningful representations out of text has been a key element for modelling language in order to tackle NLP tasks like text classification.
These representations can then form groups that one can use for supervised learning problems.
More specifically, one can utilize these linguistic groups for regularization purposes.
The main goal of this thesis is to study the aforementioned problems; first, by examining new graph-based representations of text.
Next, we studied how groups of these representations can help regularization in machine learning models for text classification.
Last, we dealt with sets and measuring distances between documents, utilizing our proposed linguistic groups, as well as graph-based approaches.
In the first part of the thesis, we have studied graph-based representations of text.
Turning text to graphs is not trivial and has been around even before word embeddings were introduced to the NLP community.
In our work, we show that graph-based representations of text can capture effectively relationships like order, semantic or syntactic structure.
Moreover, they can be created fast while offering great versatility for multiple tasks.
In the second part, we focused on structured regularization for text.
Textual data suffer from the dimensionality problem, creating huge feature spaces.
Regularization is critical for any machine learning model, as it can address overfitting.
In our work we present novel approaches for text regularization, by introducing new groups of linguistic structures and designing new algorithms.
In the last part of the thesis, we study new methods to measure distance in the word embedding space.
First, we introduce diverse methods to boost comparison between documents that consist of word vectors.
Next, representing the comparison of the documents as a weighted bipartite matching, we show how we can learn hidden representations and improve results for the text classification task.
Finally, we conclude by summarizing the main points of the total contribution and discuss future directions.
How are the social and semantic structures of a scientific community driving future research dynamics?
In this thesis we combine natural language processing techniques and network theory methods to analyze a very large dataset of scientific publications in the field of computational linguistics,i.e.the ACL Anthology.
Ultimately, our goal is to understand the role of collaborations among researchers in building and shaping the landscape of scientific knowledge, and, symmetrically, to understand how the configuration of this landscape influences individual trajectories of researchers and their interactions.
We use natural language processing tools to extract the terms corresponding to scientific concepts from the texts of the publications.
Then we reconstruct a socio-semantic network connecting researchers and scientific concepts, and model the dynamics of its evolution at different scales.
To achieve this, we first build a statistical model, based on multivariate logistic regression, that quantifies the role that social and semantic features play in the evolution of the socio-semantic network, namely in the emergence of new links.
Then, were construct the evolution of the field through different visualizations of the knowledge produced therein, and of the flow of researchers across the different subfields of the domain.
To summarize, we have shown through our work that the combination of natural language processing techniques with complex network analysis makes it possible to investigate in a novel way the evolution of scientific fields.
Prosodic highlighting refers to the distinction of a constituent through various prosodic means, especially accentuation and intonation.
It is taken to fulfill several functions: marking the different types of focus, as well as emphatic functions (named here “insisting” and “expressiveness”).
The main goal of this thesis is to determine whether prosodic highlighting and its functions display specific features in interpreted speech, a speaking style that can be defined as the oralization of a written text previously memorized by the speaker (typically an actor).
This question is relevant for linguistics and phonetics on several counts.
First, little is still known about prosodic differences between functions of prosodic highlighting.
Moreover, few studies have analyzed the prosodic characteristics of interpreted speech.
Finally, through their innovative protocols, the two experiments described in this thesis present a methodological contribution.
A production experiment consisted in having speakers replicate spontaneous conversations in read and interpreted speech.
A group of experts then annotated the occurrences of prosodic highlighting in the corpus, and assigned a function to each occurrence.
A perception experiment was also led in order to compare the realization of each function independently of speaking style.
Despite a relatively low agreement rate between experts (which raises several methodological and theoretical questions), our analyses reveal several important results.
The frequency of occurrence of prosodic highlighting is highest in interpreted speech, followed by read speech.
This confirms our prediction and suggests that interpreted speech is more suited to the study of prosodic highlighting than other speaking styles.
A strong association is observed between insisting and initial secondary accent, which confirms many previous studies.
However, there is almost no influence of speaking style on the realization of prosodic highlighting and its functions.
We attribute this result to a lack of data and to the fact that some prosodic features were not taken into account in the analysis.
This thesis provides a corpus‐based description of Kakabe, a Mande language spoken in Guinea, with a focus on phonology.
It consists of a short grammatical sketch and two parts dedicated to the analysis of the segmental and the suprasegmental phonology.
I also describe various strategies of loanword adaptation used in Kakabe, such as vowel epenthesis and consonant cluster simplification.
Kakabe is a terraced‐level tone language (H vs. L), featuring downdrift, downstep, H raising, floating L, and a number of tonal processes, such as OCP style H‐insertion between two L domains, tone spread and leveling of HLH contour.
As a result, the distance between the underlying lexical tones and their surface realization can be rather important.
Each tonal process is applied within one particular prosodic unit.
Therefore, tonal processes participate in phrasing the speech into prosodic units.
Kakabe uses a number of boundary tones to signal illocutionary force of the utterance.
Lexical tones and boundary tones coexists with intonational operations on the F0 curve.
The appendices include a Kakabe‐French dictionary, comprising 3400 entries, and an oral corpus of 12 hours of various genres, transcribed, glossed and time‐aligned with audio and video.
Within the framework of the international development of open access to scientific publications, this thesis analyses more precisely the French situation in the European context.
This analysis was carried out through a process of action research within a group of actors of the French Professional Group for B to B Information and Knowledge (GFII) who are concerned by open access.
At first, we seek to highlight the driving forces behind the development of open access by relying on a method of prospective developed by LIPSOR/ CNAM.The results led us to contribute to the design of an information website whose purpose is to display the national publisher policies on self-archiving practices in order to support the development of new deposit practices at the national level.
The prospective analysis has indeed revealed the importance of embargo periods for the financial balances of the publishers.
By adopting a more distanced point of view, we initiate a new reflection about the real impact of open access on two important driving forces which seem to play an increasing role in the knowledge economy, namely creativity and interdisciplinarity.
To learn is to extract and distill pertinent information from a set of evidence.
Yet, the evidence a learner has, or could have, at hand may seem insufficient for what she is aiming to acquire.
The insufficiency of the evidence is often evoked in respect to language acquisition: the word meanings and grammar that individuals know appear to require more evidence than that to which they have access in their environments.
While the brunt of research has focused on identifying overlooked sources of evidence and widening the evidence set, here we switch gears and probe what exactly it is that the evidence set needs to support.
We propose that the evidence a learner has may be insufficient to provide her with the knowledge competent language users think they have; however, the very same evidence may be sufficient to provide a learner with the information needed for cognitive processing of language.
Very broadly, there may be a gap between what we think we acquire and what we really acquire.
In the introductory section of this dissertation, we begin by presenting evidence of a gap between what we feel words mean and how meanings are processed in the mind.
We frame this gap in the broader context of how minds, with finite access to evidence, make sense the external world.
Then, in the first chapter, we investigate how comparably little evidence can fuel acquisition of grammar in an ecologically valid setting.
Our results reveal that just a handful of words can spur a virtuous cycle of grammar and vocabulary acquisition in infants.
Next, in the second chapter, we examine whether the same set of evidence gives rise to productive knowledge or generalization (i.e., the capacity to use prior knowledge to interpret novel situations) across development, from infancy into childhood and through to adulthood.
Our data show that infants and adults generalize a novel grammatical context to new words, but pre-school children do not.
We interpret our results within the extant literature, pointing to the live possibility that what counts as knowledge may depend on where an individual places her 'knowledge threshold'rather than an immutable ideal.
Finally, in the third chapter, we probe whether evidence that reflects a knowledge state (e.g., explicitly hearing a direct translation: 'Bamoule'means 'cat') is inherently more informative, or merely more appealing, for a learner.
Our results demonstrate that pre-packaged knowledge-state evidence boosts confidence, but has variable effects on performance.
Across three chapters and ten experiments, we build up a set of fundamental features about what it is 'to know': (1) a little evidence can go a long way, (2) how much evidence is considered to be enough may depend on a modifiable threshold, and (3) the mind may crave certainty.
We advance the conclusion that the set of evidence available to an individual can be sufficient to foster knowledge. It may just not be sufficient to foster the kind of knowledge we think we have.
Therefore, to better understand the way we learn, we need to investigate what is 'to know'from the point of view of the learner.
Deep learning is a major advance in Artificial Intelligence (A.I) in recent years.
It has quickly established itself as a standard in many fields by beating other machine learning records in its primary domains of application: computer vision and natural language processing.
Deep learning algorithms are promising in many other domains of science and especially in precision medicine.
In this thesis, we will focus on the prediction of phenotypes (diagnosis, prognosis, response to treatment) from gene expression profiles.
Most of these articles appeared in the past two years and only a small part deals with phenotype prediction.
Contrary to images or natural language where several hundred thousand or million examples are available, GE sets contain very few patients (&lt;5000).
Due to this small number of samples, the model is overfitting.
We will use the state-of-the-art methods on images such as adversarial autoencoders, ladder networks or generative adversarial networks and adapt them to the specific problem of expression data.
Another solution is to integrate datasets coming from different production platforms.
Until now, any satisfying solution has been found to resolve this problem because of the presence of complex links among the measures of the different platforms.
Indeed, a GE measure from a platform A can be spread over several expressions and mixed with other expressions from a platform B. Our idea is to build a neural network able to learn the correspondence between two platforms.
To do this, we will use domain transfer methods such as CycleGAN.
Another major challenge concerns the interpretation of neural networks and their predictions.
The General Data Protection Regulation (GDPR), adopted recently by the European Union, imposes that the user of machine learning algorithms must be able to explain the decision of a given model.
There is a real need to make neural networks more interpretable and this is particularly true in the medical field for two reasons.
First, it is important to ensure that the neural network bases its predictions on a reliable representation of the data and does not focus on irrelevant artifacts.
Without explanation, doctors cannot trust the decision of a neural network regardless of its performances.
The interpretation of neural networks will be carried out using perturbation and back-propagation of the output signal methods.
In this thesis, we will collaborate with the Institute of Cardiometabolism and Nutrition (ICAN), which will provide us with the GE profile of patients suffering from cardiometabolic diseases.
They will also give us access to the data of the European Metacardis project, which contains clinical and GE data from more than 2000 patients.
Expected result: We will propose appropriate deep learning approaches for the prediction from GE data and the biological interpretation of neural networks.
The networks that will be trained on large datasets will be available to the scientific community.
Thus, by transfer learning, researchers will be able to use our networks on smaller datasets and improve their results.
The objective is to provide an equivalent of the VGG or ResNet for GE data.
In this research, we are interested in investigating issues related to query evaluation and optimization in the framework of aggregated search.
Aggregated search is a new paradigm to access massively distributed information.
It aims to produce answers to queries by combining fragments of information from different sources.
The queries search for objects (documents) that do not exist as such in the targeted sources, but are built from fragments extracted from the different sources.
The sources might not be specified in the query expression, they are dynamically discovered at runtime.
In our work, we consider data dependencies to propose a framework for optimizing query evaluation over distributed graph-oriented data sources.
For this purpose, we propose an approach for the document indexing/orgranizing process of aggregated search systems.
We consider information retrieval systems that are graph oriented (RDF graphs).
Using graph relationships, our work is within relational aggregated search where relationships are used to aggregate fragments of information.
Our goal is to optimize the access to source of information in a aggregated search system.
These sources contain fragments of information that are relevant partially for the query.
We aim at minimizing the number of sources to ask, also at maximizing the aggregation operations within a same source.
For this, we propose to reorganize the graph database(s) in partitions, dedicated to aggregated queries.
We use a semantic or strucutral clustering of RDF predicates.
For structural clustering, we propose to use frequent subgraph mining algorithms, we performed for this, a comparative study of their performances.
For semantic clustering, we use the descriptive metadata of RDF predicates and apply semantic textual similarity methods to calculate their relatedness.
The increase of available data in almost every domain raises the necessity of employing algorithms for automated data analysis.
This necessity is highlighted in predictive maintenance, where the ultimate objective is to predict failures of hardware components by continuously observing their status, in order to plan maintenance actions well in advance.
These observations are generated by monitoring systems usually in the form of time series and event logs and cover the lifespan of the corresponding components.
Analyzing this history of observation in order to develop predictive models is the main challenge of data driven predictive maintenance.
Towards this direction, Machine Learning has become ubiquitous since it provides the means of extracting knowledge from a variety of data sources with the minimum human intervention.
The goal of this dissertation is to study and address challenging problems in aviation related to predicting failures of components on-board.
The amount of data related to the operation of aircraft is enormous and therefore, scalability is a key requirement in every proposed approach.
This dissertation is divided in three main parts that correspond to the different data sources that we encountered during our work.
In the first part, we targeted the problem of predicting system failures, given the history of Post Flight Reports.
We proposed a regression-based approach preceded by a meticulous formulation and data pre-processing/transformation.
Our method approximates the risk of failure with a scalable solution, deployed in a cluster environment both in training and testing.
To our knowledge, there is no available method for tackling this problem until the time this thesis was written.
The second part consists analyzing logbook data, which consist of text describing aircraft issues and the corresponding maintenance actions and it is written by maintenance engineers.
The logbook contains information that is not reflected in the post-flight reports and it is very essential in several applications, including failure prediction.
However, since the logbook contains text written by humans, it contains a lot of noise that needs to be removed in order to extract useful information.
We tackled this problem by proposing an approach based on vector representations of words (or word embeddings).
Our approach exploits semantic similarities of words, learned by neural networks that generated the vector representations, in order to identify and correct spelling mistakes and abbreviations.
Finally, important keywords are extracted using Part of Speech Tagging.
In the third part, we tackled the problem of assessing the health of components on-board using sensor measurements.
In the cases under consideration, the condition of the component is assessed by the magnitude of the sensor's fluctuation and a monotonically increasing trend.
In our approach, we formulated a time series decomposition problem in order to separate the fluctuation from the trend by solving a convex program.
This thesis deals with the capture, annotation, synthesis and evaluation of arm and hand motions for the animation of avatars communicating in Sign Languages (SL).
Currently, the production and dissemination of SL messages often depend on video recordings which lack depth information and for which editing and analysis are complex issues.
Signing avatars constitute a powerful alternative to video.
They are generally animated using either procedural or data-driven techniques.
Procedural animation often results in robotic and unrealistic motions, but any sign can be precisely produced.
With data-driven animation, the avatar's motions are realistic but the variety of the signs that can be synthesized is limited and/or biased by the initial database.
As we considered the acceptance of the avatar to be a prime issue, we selected the data-driven approach but, to address its main limitation, we propose to use annotated motions present in an SL Motion Capture database to synthesize novel SL signs and utterances absent from this initial database.
To achieve this goal, our first contribution is the design, recording and perceptual evaluation of a French Sign Language (LSF) Motion Capture database composed of signs and utterances performed by deaf LSF teachers.
Our second contribution is the development of automatic annotation techniques for different tracks based on the analysis of the kinematic properties of specific joints and existing machine learning algorithms.
Our last contribution is the implementation of different motion synthesis techniques based on motion retrieval per phonological component and on the modular reconstruction of new SL content with the additional use of motion generation techniques such as inverse kinematics, parameterized to comply to the properties of real motions.
Specialists in didactics aim to create an efficient method, whose teaching / learning content and tools improve phonetic skills in foreign languages.
As for the educational content, research studies have proved that sounds and phonemes of a foreign language are processed according to the structure of the phonetic and phonological space of the native language.
Other works point out that it is particularly relevant to compare linguistic systems in order to predict future difficulties and abilities language learners will be confronted with.
As for transmission tools, studies have shown the beneficial effects of interdisciplinarity and the pertinent role music plays on cognitive and learning development.
Our research objective falls within this scientific context.
Our purpose has been two-fold.
First, we tried to identify which parameter, related to the production of the singing voice whilst separate from the speaking voice, may facilitate the perception of non-native vowels.
Secondly, we aimed at comparing the effects on the ability to produce non-native vowels of two corrective phonetic methods, one of which used the “singing voice” tool.
Through the results of these studies, we tried to understand how Italian as a native language interacts with the perception and the production of French as a target language.
Our studies have shown that vowel pitch and duration do not impact the discrimination of /y/ and /ø/, and that the consonant sharpness plays a role on the discrimination of /y/ in a CV type syllable.
We found a positive effect of the method, which uses singing-voice as a tool, on the production of the sound spectrum of French closed vowels, but not on the evolution of the sounds and phonemes into the acoustic space.
Our results support the theory that phonetic teaching and learning is relevant in language classes and suggest that singing-voice may be a useful tool to ease the perception and the production of non-native vowels.
This thesis addresses the problem of multichannel audio source separation by exploiting deep neural networks (DNNs).
We build upon the classical expectation-maximization (EM) based source separation framework employing a multichannel Gaussian model, in which the sources are characterized by their power spectral densities and their source spatial covariance matrices.
We explore and optimize the use of DNNs for estimating these spectral and spatial parameters.
Employing the estimated source parameters, we then derive a time-varying multichannel Wiener filter for the separation of each source.
We extensively study the impact of various design choices for the spectral and spatial DNNs.
We consider different cost functions, time-frequency representations, architectures, and training data sizes.
Those cost functions notably include a newly proposed task-oriented signal-to-distortion ratio cost function for spectral DNNs.
Furthermore, we present a weighted spatial parameter estimation formula, which generalizes the corresponding exact EM formulation.
On a singing-voice separation task, our systems perform remarkably close to the current state-of-the-art method and provide up to 2 dB improvement of the source-to-interference ratio.
On a speech enhancement task, our systems outperforms the state-of-the-art GEV-BAN beamformer by 14%, 7%, and 1% relative word error rate improvement on 6-channel, 4-channel, and 2-channel data, respectively
The aim of this study is propose a typology of predicates of motion in Hungarian.
The typology reflects a simple objective perception of motion and space.
The analysis uses the theory of object classes, which we applied to Hungarian.
Our classification is based on semantic properties such as directionality, mood destination, goal, place and the aspectual properties.
These semantic properties are completed by morpho-syntactic properties needed for natural language processing.
The contrastive component of our study has made it possible to propose a better description of the classes of predicates in Hungarian and to bring out the morpho-syntactic and combinatory differences specific to both languages in the expression of motion, such as the role of verb prefixes, locative complements, and underline the importance of noun predicate.
The mass of information in the legal field, which is constantly increasing, has generated a capital need to organize and structure the contents of the documents available, and thus transform them into an intelligent guide, capable of provide complete and immediate answers to queries in natural language.
As a result, question-answering systems (QASs) respond perfectly to this need by offering the different mechanisms to provide adequate and precise answers to questions expressed in natural language.
Indeed, this type of system allows the user to ask a question in natural language and to receive a precise answer to his request instead of a set of documents deemed relevant, as is the case for search engines.
Our objective is to set up a question-answering system operating in the legal field in Morocco which mostly uses the French and Arabic languages, see the English language.
Its purpose is to give a relevant and concise answer to questions in the legal field, stated in natural language by a user, without the latter having to go through legal documents to find an answer to his question, which can be expensive in terms of time.
In this work, we study analogy-based models for Machine Learning of Natural Language.
The analogical approach offers an alternative to both deductive methods (in which specific knowledge is infered from general knowledge) and inductive methods (in which general knowledge is infered from specific knowledge).
In this setting, the analysis of a new entity is performed by comparison with available data; inference is directly achieved from specific knowledge to specific knowledge.
In this approach, abstraction, which is involved in both deductive and inductive models is no longer required.
Moreover, this approach correctly account for the paradigmatic organization of linguistic data, which easily relates one linguistic entity with others through specific schemes; the linguistic knowledge is thus implicitly represented within the corpus.
In particular, this paradigmatic organization suggests to consider analogical proportions.
A learning model is presented, which relies on the exploitation of analogical proportions.
We introduce the notion of analogical extension, which allows for the expression of its learning bias.
We also propose a formal algebraic framework which gives a meaning to the notion of analogical proportion between structured objects.
Our perception is by nature multimodal, i.e. it appeals to many of our senses.
To solve certain tasks, it is therefore relevant to use different modalities, such as sound or image.
This thesis focuses on this notion in the context of deep learning.
For this, it seeks to answer a particular problem: how to merge the different modalities within a deep neural network?
We first propose to study a problem of concrete application: the automatic recognition of emotion in audio-visual contents.
This leads us to different considerations concerning the modeling of emotions and more particularly of facial expressions.
We thus propose an analysis of representations of facial expression learned by a deep neural network.
In addition, we observe that each multimodal problem appears to require the use of a different merge strategy.
Finally, we are interested in a multimodal view of knowledge transfer.
Indeed, we detail a non-traditional method to transfer knowledge from several sources, i.e. from several pre-trained models.
For that, a more general neural representation is obtained from a single model, which brings together the knowledge contained in the pre-trained models and leads to state-of-the-art performances on a variety of facial analysis tasks.
At STMicroelectronics, the Business Intelligence team is daily confronted to exploit data and information to create reports about manufacturing activities in order to supervise it.
In such an industrial organization, products change regularly and data can quickly become obsolete.
Consequently, over time, the number of created reports is highly growing, while knowledge about their creation is lost.
This is shown in a qualitative and quantitative evaluation of the main part of the STMicroelectronics'knowledge system.
As a result, problems related to knowledge obsolescence, duplication, non-centralization and proliferation continuously arise.
Its objective is to effectively and continuously capitalize expert knowledge while targeting business needs and providing an evolving solution.
It is based on a Business Intelligence for Business Intelligence system (BI4BI).
Since knowledge is embedded not only in systems and tools, but also in human minds and practices, our proposed knowledge capitalization solution also involves people and organizations: it proposes to collect users'feedbacks and insights to integrate them in knowledge representation and in our BI4BI tool.
Automata theory bas been developed to overcome both theoretical and practical problems.
Nowadays, automata are considered as basic knowledge by all computer scientists, and they are used in most softwares.
In 1974, Samuel Eilenberg gave a new machine model unifying the most common automata such as transducers, pushdown automata, and even Turing machines.
We propose an effective design of Eilenberg machines and study simulation techniques.
Our Simulator is defined by a functional program that progressively enumerates solutions, exploring a research space according to various of strategies.
We introduce the notion of finite Eilenberg machines, and formally prove the correction of the underlying simulation engine.
In recent years, the concept of Brzozowski's derivatives has led to many novel and efficient algorithms that compile regular expressions into non-deterministic automata.
We review these state of the art algorithms, give an efficient OCaml implémentation, and compare their efficiency in a common framework.
Natural Language Processing (NLP) is a research field which grew up dramatically the last few years.
Large scale communication ways made appear huge data amounts with a lot of added value, but impossible to process by hand.
Meaning extraction of these exchanges is a crucial commercial stake for a lot of commercial companies, like the GAFAMI (social networks, clients feeds on web distributed goods,), but also for public entities.
Academic and private research so did naturally turn to develop tools and techniques allowing to pinpoint the meaning of a text for the lowest price and the fewest functional constraints.
These technologies are especially interesting for they allow setting up a social network listening system about a target place, like a railway station, a museum or an airport.
It is to say that the information running about it can be exploited to answer different needs.
In example, it is possible to predict peak days and to locate where the crowd will be the densest on the site to enhance staff management or to detect targets for potential malicious actions.
Flow scheme building according to users spoken language can also help optimizing the customers-oriented information display to make it more accessible.
Nevertheless, meaning extraction from text data can face a linguistic barrier in particular if those are written in different language.
Their acquisition and their analysis must so be operable independently from the used language so the information behind the text can be exploited.
Regarding all of this, building a tool capable of listening to different social networks in order to identify elements, from various languages, we want to quantify and synthetize may help to make services and security better in a place such as an airport.
Events with an emergency trait are pinpointed because they may degrade the airport activity itself.
For example, a storm can prevent planes to land or lift-off, and you must foresee it in order to deal with the impacted passengers, staff, connections and so on.
A traffic jam on the other hand prevent people from leaving the airport, or pilots and personnel from getting on board.
This is in these problematics that this research project takes its place.
We address the problem of model-independent searches for New Physics (NP), at the Large Hadron Collider (LHC) using the ATLAS detector.
Particular attention is paid to the development and testing of novel Machine Learning techniques for that purpose.
The present work presents three main results.
Firstly, we put in place a system for automatic generic signature monitoring within TADA, a software tool from ATLAS.
We explored over 30 signatures in the data taking period of 2017 and no particular discrepancy was observed with respect to the Standard Model processes simulations.
Secondly, we propose a collective anomaly detection method for model-independent searches for NP at the LHC.
We propose the parametric approach that uses a semi-supervised learning algorithm.
This approach uses penalized likelihood and is able to simultaneously perform appropriate variable selection and detect possible collective anomalous behavior in data with respect to a given background sample.
Thirdly, we present preliminary studies on modeling background and detecting generic signals in invariant mass spectra using Gaussian processes (GPs) with no mean prior information.
Two methods were tested in two datasets: a two-step procedure in a dataset taken from Standard Model simulations used for ATLAS General Search, in the channel containing two jets in the final state, and a three-step procedure from a simulated dataset for signal (Z′) and background (Standard Model) in the search for resonances in the top pair invariant mass spectrum case.
Our study is a first step towards a method that takes advantage of GPs as a modeling tool that can be applied to several signatures in a more model independent setup.
Because of their multiple ecological roles, macrophytes are an important component of hydrosystems, and are thus essential to conserve.
However, during the summer season, high densities of submerged species cause recurrent problems in certain rivers, especially in urban areas for users and managers, and can have negative consequences for ecosystem health.
In a context of global change, the overgrowth of submerged macrophytes calls for new tools to better understand the dynamics of macrophytes meadows and to predict their dynamics according to different environmental scenarios.
In this context, this thesis aims to develop a toolbox accompanying a multispecific mechanistic model for the production of submerged aquatic plants, the DEMETHER model.
To do this, the model requires determining certain ecophysiological parameters and having spatialized biomass data for its calibration, as well as bathymetric and substrate data.
The first phase of this work then consisted in field surveys to characterize the study site and in developing numerical or experimental tools for the acquisition of these data.
The first tool developed aims to monitor submerged macrophytes by remote sensing.
The method explored here confirmed the potential of high spatial resolution (50 cm) multispectral imagery of Pléiades satellites, processed by machine learning algorithms, to map the distribution of macrophyte beds and quantify their biomass in situ.
This approach has also led us to propose an optimized sampling strategy for macrophytes in large rivers for future investigations.
This work opens up interesting perspectives for applying the method to drone imagery, and continuing its development for automated monthly monitoring.
In parallel, a tool was developed for measuring physiological parameters via oximetry and applied to the two species of interest.
The data obtained provide information in particular on the photosynthetic and respiratory capacities of each species in response to limiting factors (light, temperature).
The second phase of this work consisted in applying the DEMETHER model to explore different climate change scenarios.
Simulations of the macrophyte dynamics in terms of biomass were carried out for current thermal conditions and for a foreseeable rise in temperatures by 2041-2070.
The results showed the importance of the temperature sensitivity of certain physiological processes to explain the distribution patterns of the two species studied, highlighting the interest of mechanistic modelling to understand the structuring of macrophyte communities.
The first results obtained with this toolbox confirmed its functionality.
However, in order to extend its application range, each of the tools developed during the thesis will need to be further improved, in particular to refine the calibration of the DEMETHER model.
Specific suggestions have been made to this aim.
This PhD thesis deals with the automatic continuous Cued Speech (CS) recognition based on the images of subjects without marking any artificial landmark.
In order to realize this objective, we extract high level features of three information flows (lips, hand positions and shapes), and find an optimal approach to merging them for a robust CS recognition system.
We first introduce a novel and powerful deep learning method based on the Convolutional Neural Networks (CNNs) for extracting the hand shape/lips features from raw images.
Theadaptive background mixture models (ABMMs) are also applied to obtain the hand position features for the first time.
All these methods make significant contributions to the feature extraction in CS.
Then, due to the asynchrony problem of three feature flows (i.e., lips, hand shape and hand position) in CS,the fusion of them is a challenging issue.
In order to resolve it, we propose several approaches including feature-level and model-level fusion strategies combined with the context-dependent HMM.
To achieve the CS recognition, we propose three tandem CNNs-HMM architectures with different fusion types.
All these architectures are evaluated on the corpus without any artifice, and the CS recognition performance confirms the efficiency of our proposed methods.
The result is comparable with the state of the art using the corpus with artifices.
In parallel,we investigate a specific study about the temporal organization of hand movements in CS,especially about its temporal segmentation, and the evaluations confirm the superior performance of our methods.
Nonetheless, hitherto there has been no systematic theory that tries to propose a formal account regarding the make-up and range of actual and possible conversational types.
In this thesis, we take a topological approach to classifying conversations and develop a formal theory of conversational types in the framework of Type Theory with Records (TTR).
Conversely, we investigate whether the variation in the distribution of NSUs can serve as a means of structuring the space of conversational types.
This study is concerned with the linguistic realisation of explanation in contemporary English.
After a definitional work on the notion, two types of explanation are identified: clarifying explanation and causal explanation.
The study adopts an utterer-centered approach while also considering pragmatic aspects.
The linguistic analysis is based on a corpus of didactic texts and it focuses on the role played by connectives in explanatory discourse.
Finally, the study also aims at formalising some discourse relations involved in explanations and designing linguistic rules in order to automatically identify the discourse relations at stake.
This dissertation is part of a larger movement, both national and international, acknowledging the growing importance and inquiring about the democratic legitimacy of judicial institutions.
In looking at the judicial office and its practice, it investigates the role of public opinion, largely considered an element of democratic legitimacy.
To obtain a more complete perspective on judicial institutions and public opinion, a comparative approach is adopted and the United States Supreme Court, and the European Court of Human Rights are examined.
This study adopts the following reasoning.
At a theoretical level, it attempts to clarify The multifaceted concept of “public opinion” and to establish the different sources of judicial legitimacy, in order to determine whether public opinion can be considered such a source.
It then studies the substance of judicial decisions, which reveal judges' conception of the role of public opinion in democracy and in the judicial evolution of rights and liberties.
The content-study of judicial decisions focuses on first on the relationship between public opinion and democracy in the protection of freedom of expression and second on the rote of public opinion in the evolution of the rights of homosexual persons.
This dissertation offers a study of parce que constructions in oral speech, at the interface between corpus and theoretical linguistics.
The corpus under study comprises conversations recorded from four surveys from the PFC (Phonologie du Français Contemporain – Phonology of Contemporary French) project database.
The first stage of the annotation process allows the syntactic characterization of said constructions.
To this end, we put forward a new Explanation-type relation, which allows the annotation of parce que in relation to utterance modality.
We provide elements in order to identify the terms connected by parce que which confirm the benefit of a twofold analysis of the corpus, from both a syntactic and a discursive point of view.
Despite all the design efforts, there are always uses and situations for which the user interface is not perfect.
This thesis investigates self-explanatory user interfaces for improving the quality perceived by end users.
The approach follows the principles of model-driven engineering.
It consists in keeping the design models at runtime so that to dynamically enrich the user interface with a set of possible questions and answers.
The questions are related to usage (for instance, "What's the purpose of this button? ", "Why is this action not possible"?) as well as to design rationale (for instance, "Why are the items not alphabetically ordered?").
This thesis proposes a software infrastructure UsiExplain based on the UsiXML metamodels.
An evaluation conducted on a case study related to a car shopping webiste confirms that the approach is relevant especially for usage questions.
The constant increase of available documents and tools to access them has led to a change of research practices.
For a few years now, more and more information retrieval platforms are made available online to the scientific community or the public.
Formerly, the main issue for researchers was to identify if a particular resource existed.
Today, the challenge is more about finding how to access pertinent information.
We have identified two distinct levers to limit the impact of this new search paradigm.
First, we believe that it is necessary to analyze how the different search platforms are used.
To be able to understand and read into users behavior is a necessary step to comprehend what users understand, and to identify what they need to get an in-depth understanding of the operation of such platforms.
Indeed, most systems act as black boxes which conceal the underlying transformations applied on data.
Why is the search engine returning those particular results?
Why is this document more pertinent than another?
Such seemingly naive questions are nonetheless essential to undertake an analytical approach of the information search and retrieval task.
We think that users have a right and a duty to question themselves about the relevance of such and such tool at their disposal.
To help them cope with these issues, we developped a dual-use information search platform.
On the one hand, it can be used to observe and understand user behavior.
On the other hand, it can be used as a pedagogical medium to highlight research biases users can be exposed to.
At the same time, we believe that the tools themselves must be improved.
In the second part of this thesis, we study the impact that the quality of documents can have on their accessibility.
Because of the increase of documents available online, human operators are less and less able to insure their quality.
Thus, there is a need to set up new strategies to improve the way search platform operate and process documents.
We propose a new method to automatically identify and correct errors generated by information extraction process such as OCR.
The present work introduces in phonetics, the atomic decomposition of the signal also known as the Matching Pursuit and treats a group of atoms by compression without losses and finally measures the distance of the list of atoms compressed using the Kolmogorov's algorithms.
The calibration is based on an initial classical analysis of the co-articulation of sound sequences of VCV and CV, or V ∈ {[i] [u] [a]} and C ∈ {[t] [d] [s] [δ]}∪ [tʕ] [dʕ] [sʕ [δʕ]} the excerpts culled from a corpus made up of four arabic speaking areas.
The locus equation of CV vs CʕV, makes it possible to differentiate the varieties of the language.
In the second analysis, an algorithm of atomic adaptative decomposition or Matching Pursuit is applied to the sequences VCV and VCʕV still on the same corpus.
The atomic sequences representing VCV et VCʕV are then compressed without losses and the distances between them are searched for by Kolmogorov's algorithms.
The classification of phonetic recordings obtained from these arabic speaking areas is equivalent to that of the first method.
The findings of the study show how the introduction of Matching Pursuit's in phonetics works, the great robustness of the use of algorithms and suggesting important possibilities of automation of processes put in place, while opening new grounds for further investigations
This work has been jointly supervised by U. Jean Monnet Saint Etienne, in the Hubert Curien Lab (Frederique Laforest, Christophe Gravier, Julien Subercaze) and U. Mohamed V Rabat, LeRMA ENSIAS (Rachida Ahjoun, Mounia Abik).
Knowledge, education and learning are major concerns in today's society.
The technologies for human learning aim to promote, stimulate, support and validate the learning process.
Our approach explores the opportunities raised by mixing the Social Web and the Semantic Web technologies for e-learning.
More precisely, we work on discovering learners profiles from their activities on the social web.
The Social Web can be a source of information, as it involves users in the information world and gives them the ability to participate in the construction and dissemination of knowledge.
We focused our attention on tracking the different types of contributions, activities and conversations in learners spontaneous collaborative activities on social networks.
The learner profile is not only based on the knowledge extracted from his/her activities on the e-learning system, but also from his/her many activities on social networks.
We propose a methodology for exploiting hashtags contained in users' writings for the automatic generation of learner's semantic profiles.
Hashtags require some processing before being source of knowledge on the user interests.
We have defined a method to identify semantics of hashtags and semantic relationships between the meanings of different hashtags.
By the way, we have defined the concept of Folksionary, as a hashtags dictionary that for each hashtag clusters its definitions into meanings.
Semantized hashtags are thus used to feed the learner's profile so as to personalize recommendations on learning material.
The goal is to build a semantic representation of the activities and interests of learners on social networks in order to enrich their profiles.
We also discuss our recommendation approach based on three types of filtering (personalized, social, and statistical interactions with the system).
We focus on personalized recommendation of pedagogical resources to the learner according to his/her expectations and profile
The contributions of this thesis are numerical and theoretical tools for the resolution of blind inverse problems in imaging.
A very popular approach consists in estimating this operator from an image containing point sources (microbeads or fluorescent proteins in microscopy, stars in astronomy).
Such an observation provides a measure of the impulse response of the degradation operator at several points in the field of view.
Processing this observation requires robust tools that can rapidly use the data.
We propose a toolbox that estimates a degradation operator from an image containing point sources.
The estimated operator has the property that at any location in the field of view, its impulse response is expressed as a linear combination of elementary estimated functions.
This makes it possible to estimate spatially invariant (convolution) and variant (product-convolution expansion) operators.
An important specificity of this toolbox is its high level of automation: only a small number of easily accessible parameters allows to cover a large majority of practical cases.
The size of the point source (e.g. bead), the background and the noise are also taken in consideration in the estimation.
This tool, coined PSF-estimator, comes in the form of a module for the Fiji software, and is based on a parallelized implementation in C++.
The operators generated by an optical system are usually changing for each experiment, which ideally requires a calibration of the system before each acquisition.
To overcome this, we propose to represent an optical system not by a single operator (e.g. convolution blur with a fixed kernel for different experiments), but by subspace of operators.
This set allows to represent all the possible states of a microscope.
We introduce a method for estimating such a subspace from a collection of low rank operators (such as those estimated by the toolbox PSF-Estimator).
We show that under reasonable assumptions, this subspace is low-dimensional and consists of low rank elements.
In a second step, we apply this process in microscopy on large fields of view and with spatially varying operators.
This implementation is possible thanks to the use of additional methods to process real images (e.g. background, noise, discretization of the observation).The construction of an operator subspace is only one step in the resolution of blind inverse problems.
In this thesis, we present a multilevel scheme consisting of both hardware and software solutions to improve the daily operational life of firefighters.
As a core part of this scheme, we design and develop a smart system of wearable IoT devices used for state assessment and localization of firefighters during interventions.
To ensure a maximum lifetime for this system, we propose multiple data-driven energy management techniques for resource constraint IoT devices.
The first one is an algorithm that reduces the amount of data transmitted between the sensor and the destination (Sink).
This latter exploits the temporal correlation of collected sensor measurements to build a simple yet robust model that can forecast future observations.
Then, we coupled this approach with a mechanism that can identify lost packets, force synchronization, and reconstruct missing data.
Furthermore, knowing that the sensing activity does also require a significant amount of energy, we extended the previous algorithm and added an additional adaptive sampling layer.
Finally, we also proposed a decentralized data reduction approach for cluster-based sensor networks.
All the previous algorithms have been tested and validated in terms of energy efficiency using custom-built simulators and through implementation on real sensor devices.
The results were promising as we were able to demonstrate that our proposals can significantly improve the lifetime of the network.
The last part of this thesis focusses on building data-centric decision-making tools to improve the efficiency of interventions.
Since sensor data clustering is an important pre-processing phase and a stepstone towards knowledge extraction, we review recent clustering techniques for massive data management in IoT and compared them using real data for a gas leak detection sensor network.
Furthermore, with our hands on a large dataset containing information on 200,000 interventions that happened during a period of 6 years in the region of Doubs, France.
We study the possibility of using Machine Learning to predict the number of future interventions and help firefighters better manage their mobile resources according to the frequency of events.
The topic of the thesis is the extraction and segmentation of clothing items from still images using techniques from computer vision, machine learning and image description, in view of suggesting non intrusively to the users similar items from a database of retail products.
We firstly propose a dedicated object extractor for dress segmentation by combining local information with a prior learning.
A person detector is applied to localize sites in the image that are likely to contain the object.
Then, an intra-image two-stage learning process is developed to roughly separate foreground pixels from the background.
Finally, the object is finely segmented by employing an active contour algorithm that takes into account the previous segmentation and injects specific knowledge about local curvature in the energy function.
We then propose a new framework for extracting general deformable clothing items by using a three stage global-local fitting procedure.
A set of template initiates an object extraction process by a global alignment of the model, followed by a local search minimizing a measure of the misfit with respect to the potential boundaries in the neighborhood.
The results provided by each template are aggregated, with a global fitting criterion, to obtain the final segmentation.
In addition, we introduce a novel dataset called RichPicture, consisting of 1000 images for clothing extraction from fashion images.
The methods are validated on the public database and compares favorably to the other methods according to all the performance measures considered.
Ranking data, i.e., ordered list of items, naturally appears in a wide variety of situations, especially when the data comes from human activities (ballots in political elections, survey answers, competition results) or in modern applications of data processing (search engines, recommendation systems).
The design of machine-learning algorithms, tailored for these data, is thus crucial.
However, due to the absence of any vectorial structure of the space of rankings, and its explosive cardinality when the number of items increases, most of the classical methods from statistics and multivariate analysis cannot be applied in a direct manner.
Hence, a vast majority of the literature rely on parametric models.
In this thesis, we propose a non-parametric theory and methods for ranking data.
Our analysis heavily relies on two main tricks.
The first one is the extensive use of the Kendall's tau distance, which decomposes rankings into pairwise comparisons.
This enables us to analyze distributions over rankings through their pairwise marginals and through a specific assumption called transitivity, which prevents cycles in the preferences from happening.
The second one is the extensive use of embeddings tailored to ranking data, mapping rankings to a vector space.
Three different problems, unsupervised and supervised, have been addressed in this context: ranking aggregation, dimensionality reduction and predicting rankings with features.
The first part of this thesis focuses on the ranking aggregation problem, where the goal is to summarize a dataset of rankings by a consensus ranking.
In this work, we have investigated the hardness of this problem in two ways.
Firstly, we proposed a method to upper bound the Kendall's tau distance between any consensus candidate (typically the output of a tractable procedure) and a Kemeny consensus, on any dataset.
Then, we have casted the ranking aggregation problem in a rigorous statistical framework, reformulating it in terms of ranking distributions, and assessed the generalization ability of empirical Kemeny consensus.
The second part of this thesis is dedicated to machine learning problems which are shown to be closely related to ranking aggregation.
The first one is dimensionality reduction for ranking data, for which we propose a mass-transportation approach to approximate any distribution on rankings by a distribution exhibiting a specific type of sparsity.
The second one is the problem of predicting rankings with features, for which we investigated several methods.
Our first proposal is to adapt piecewise constant methods to this problem, partitioning the feature space into regions and locally assigning as final label (a consensus ranking) to each region.
Our second proposal is a structured prediction approach, relying on embedding maps for ranking data enjoying theoretical and computational advantages.
There is no single technique that will allow all relevant behaviour of the speech articulators (lips, tongue, palate...) to be spatially ant temporally acquired.
This includes: 2D Ultrasound (US) data to recover the dynamic of the tongue, stereovision data to recover the 3D dynamic of the lips, electromagnetic sensors that provide 3D position of points on the face and the tongue, and 3D Magnetic Resonance Imaging (MRI) that depict the vocal tract for various sustained articulations.
We investigate the problems of the temporal synchronization and the spatial registration between all these modalities, and also the extraction of the shape articulators from the data (tongue tracking in US images).
Finally, the fused data are evaluated on an existing articulatory model to assess their quality for an application in speech production.
Machine learning proposes numerous algorithms to solve the different tasks that can be extracted from real world prediction problems.
To solve the different concerned tasks, most Machine learning algorithms somehow rely on relationships between instances.
Pairwise instances relationships can be obtained by computing a distance between the vectorial representations of the instances.
Considering the available vectorial representation of the data, none of the commonly used distances is ensured to be representative of the task that aims at being solved.
In this work, we investigate the gain of tuning the vectorial representation of the data to the distance to more optimally solve the task.
We more particularly focus on an existing graph-based algorithm for classification task.
An algorithm to learn a mapping of the data in a representation space which allows an optimal graph-based classification is first introduced.
By projecting the data in a representation space in which the predefined distance is representative of the task, we aim at outperforming the initial vectorial representation of the data when solving the task.
A theoretical analysis of the introduced algorithm is performed to define the conditions ensuring an optimal classification.
A set of empirical experiments allows us to evaluate the gain of the introduced approach and to temper the theoretical analysis.
Digital Scholarly Editions are critically annotated patrimonial literary resources, in a digital form.
Such editions roughly take the shape of a transcription of the original resources, augmented with critical information, that is, of structured data.
In a collaborative setting, the structure of the data is explicitly defined in a schema, an interpretable document that governs the way editors annotate the original resources and guarantees they follow a common editorial policy.
Digital editorial projects classically face two technical problems.
The first has to do with the expressiveness of the annotation languages, that prevents from expressing some kinds of information.
The second relies in the fact that, historically, schemas of long-running digital edition projects have to evolve during the lifespan of the project.
However, amending a schema implies to update the structured data that has been produced, which is done either by hand, by means of ad-hoc scripts, or abandoned by lack of technical skills or human resources.
In this work, we define the theoretical ground for an annotation system dedicated to scholarly edition.
We define eAG, a stand-off annotation model based on a cyclic graph model, enabling the widest range of annotation.
We define a novel schema language, SeAG, that permits to validate eAG documents on-the-fly, while they are being manufactured.
We also define an inline markup syntax for eAG, reminiscent of the classic annotation languages like XML, but retaining the expressivity of eAG.
Eventually, we propose a bidirectional algebra for eAG documents so that, when a SeAG S is amended, giving S', an eAG I validated by S is semi-automatically translated into an eAG I'validated by S', and so that any modification applied to I (resp. I') is semi-automatically propagated to I'(resp. I) – hence working as an assistance tool for the evolution of SeAG schemas and eAG annotations.
The design of control-command systems often suffers from problems of communication and interpretation of specifications between the various designers, frequently coming from a wide range of technical fields.
In order to address the design of these systems, several methods have been proposed in the literature.
Among them, the so-called mixed method (bottom-up/top-down), which sees the design realized in two steps.
In the first step (bottom-up), a model of the system is defined from a set of standardized components.
This model undergoes, in the second (top-down) step, several refinements and transformations to obtain more concrete models (codes, applications, etc.).
To guarantee the quality of the systems designed according to this method, we propose two formal verification approaches,based on Model-Checking, in this thesis.
The first approach concerns the verification of standardized components and allows the verification of a complete elementary control-command chain.
The second one consists in verifying the model of architecture (PandID) used for the generation of control programs.
The latter is based on the definition of an architectural style in Alloy for the ANSI/ISA-5.1 standard.
To support both approaches, two formal semi-automated verification flows based on Model-Driven Engineering have been proposed.
This integration of formal methods in an industrial context is facilitated by the automatic generation of formal models from design models carried out by business designers.
Our two approaches have been validated on a concrete industrial case of a fluid management system embedded in a ship.
In this context, user's queries often require the composition of multiple Cloud DaaS services to be answered.
Defining the semantics of DaaS services is the first step towards automating their composition.
An interesting approach to define the semantics of DaaS services is by describing them as semantic views over a domain ontology.
However, defining such semantic views cannot always be done with certainty, especially when the service's returned data are too complex.
In this dissertation, we propose a probabilistic approach to model the semantic uncertainty of data services.
In our approach, a DaaS service with an uncertain semantics is described by several possible semantic views, each one is associated with a probability.
Based on our modeling, we study the problem of interpreting an existing composition involving services with uncertain semantics.
We also study the problem of compositing uncertain DaaS services to answer a user query, and propose efficient methods to compute the different possible compositions and their probabilities.
We conduct a series of experiments to evaluate the performance of our composition algorithms.
The obtained results show the efficiency and the scalability of our proposed solutions
This PhD thesis belongs to the Natural Language Processing (NLP) field, and relates to the automated, semantic analysis of discourse structure.
More precisely, we address the issue of thematic analysis, which aims at studying the structure of texts with respect to the organisation of their informational content.
This task is of particular importance for Information Retrieval, which constitutes the primary application of our work.
The concept of "theme" being particularly complex but scarcely studied for itself in the information retrieval literature, the first part of our dissertation is devoted to a large bibliographical study about the notions of theme, topic, subject, and aboutness, within the linguistics, information science and NLP fields.
We draw from this study a definition of the theme as a discursive, semantic and structured object.
We propose several models and processes, devoted firstly to the semantic analysis of geographical documents, and secondly to the automatic analysis of temporal discourse frames in the sense of Michel Charolles.
We generalise this work introducing the notions of composite topic and semantic axis.
The last part is devoted to the LinguaStream platform, an integrated experimentation environment that we designed to ease the elaboration of operational linguistic models, and that lead us to propose some original methodological principles.
The aim of this project is to understand how organizations are able to adapt to their environment and to make it evolve in making use of the wealth of market information.
We also show the importance of the objective of the intra-organizational transfer between market analysts and product managers and of the level of control over the receiver on the coordination of the two activities of customer agility.
The proliferation of social networks and all the personal data that people share brings many opportunities for developing exciting new applications.
At the same time, however, the availability of vast amounts of personal data raises privacy and security concerns.
In this thesis, we develop methods to identify the social networks accounts of a given user.
We first study how we can exploit the public profiles users maintain in different social networks to match their accounts.
We identify four important properties – Availability, Consistency, non-Impersonability, and Discriminability (ACID) – to evaluate the quality of different profile attributes to match accounts.
Exploiting public profiles has a good potential to match accounts because a large number of users have the same names and other personal infor-mation across different social networks.
Yet, it remains challenging to achieve practically useful accuracy of matching due to the scale of real social networks.
To demonstrate that matching accounts in real social networks is feasible and reliable enough to be used in practice, we focus on designing matching schemes that achieve low error rates even when applied in large-scale networks with hundreds of millions of users.
Then, we show that we can still match accounts across social networks even if we only exploit what users post, i.e., their activity on a social networks.
This demonstrates that, even if users are privacy conscious and maintain distinct profiles on different social networks, we can still potentially match their accounts.
Finally, we show that, by identifying accounts that correspond to the same person inside a social network, we can detect impersonators.
The analysis of the academic literature on the influence of color leads to the proposition of a conceptual framework of the possible impacts of background color on the context of online consumer reviews (chapter 2).
The empirical phase of this research is based on three experimental studies.
A first study examines the influence of a negative valence background color (red) for a negative online consumer review on viewer evaluations (chapter 3).
A second experimental study investigates the role of the perceived valence of color as an influencing mechanism in online consumer reviews (chapter 4).
The research for greater ecological validity leads to a third experimental study.
Participants are exposed to several reviews simultaneously on a model page of online consumer reviews (chapter 5).
This research presents a series of contributions for use by academics as well as practitioners (conclusion).
The focus of this PhD thesis is to design an optimal Energy Management System (EMS) for a Hybrid Electric Vehicle (HEV) following traffic constraints.
In the current state of the art, EMS are typically divided between real-time designs relying on local optimization methods, and global optimization that is only suitable for off-line use due to computational constraints.
The starting point of the thesis is that in terms of energy consumption, the stochastic aspect of the traffic conditions can be accurately modelled thanks to (speed,acceleration) probability distributions.
In order to reduce the data size of the model, we use clustering techniques based on the Wasserstein distance, the corresponding barycenters being computed by either a Sinkhorn or Stochastic Alternate Gradient method.
Thanks to this stochastic traffic model, an off-line optimization can be performed to determine the optimal control (electric motor torque) that minimizes the fuel consumption of the HEV over a certain road segment.
In the past years, deep neural networks such as convolutional or recurrent ones have become highly popular for solving various prediction problems, notably in computer vision and natural language processing.
Yet, regularizing these networks when the amount of labeled data is scarce is still an open problem.
The main popular approaches to regularize deep neural networks are based on early stopping the optimization procedure, model averaging, and data augmentation.
The goal of this PhD is to study and develop optimization algorithms, regularization strategies, and architecture designs that are adapted to deep neural networks, with a particular focus on the low-sample-size regime.
Spectral norms appear indeed as natural quantities for controlling the model complexity of deep networks.
Other stratgies based on regularizing directly with the RKHS norm introduced in will also be investigated.
In particular, we expect these architectures to play a central role in the complexity of the network training problem.
In a similar vein, recent results related to the Shapley-Folkman theorem show that they also have intrinsically low duality gap in some settings.
Many resources published on the Web of data are related to spatial references that describe their location.
These spatial references are a valuable asset for interlinking and visualizing data over the Web.
However, these spatial references may be presented with different levels of detail and different geometric modelling from one data source to another.
These differences are a major challenge for using geometries comparison as a criterion for interlinking georeferenced resources.
This challenge is even amplified more due to the open and often volunteered nature of the data that causes geometric heterogeneities between the resources of a same data source.
In this PhD thesis, we propose a vocabulary for formalizing the knowledge about the characteristics of every single geometry in a dataset.
We propose a semi-automatic approach for acquiring this knowledge by using geographic reference data.
Then, we propose to use this knowledge in approach for adapting dynamically the setting of the comparison of each pair of geometries during an interlinking process.
We propose an additional interlinking approach based on geographic reference data for detecting n:m links between data sources.
Finally, we propose Web mapping applications for georeferenced resources that remain readable at different map scales
This thesis focuses on using controlled language for Thai software requirements specifications.
The study describes the ambiguities and problems encountered in Thai software requirements specifications; both syntactic ambiguity and semantic ambiguity.
The study also describes the nature of the Thai language.
The model of controlled language for Thai software requirements specifications is composed of three main components: lexical analysis,syntactic analysis, and semantic analysis.
For syntactic analysis, a controlled syntax is created using Backus-Naur Form (BNF).
In the lexical analysis stage, an XML format lexical resource is built to store words according to their domain.
The words received from the XML resource are conceptually correct but may be semantically irrelevant.
To solve this issue, the model applies Boolean Matrices to align sentences semantically.
As a result, the sentences produced from the model are guaranteed to be syntactically and semantically correct.
After having created this model, a program for testing the efficiency of the model is developed.
The model is evaluated using four testing methods as follows: 1. functional testing for the correctness of the sentence's syntax, 2.functional testing for the semantic correctness of the sentences produced by the model, 3. acceptance testing in terms of user satisfaction with the program, and 4. acceptance testing in terms of the validity of the outputs.
The positive results signify that: 1. the sentences produced by the proposed model are syntactically correct, 2. the sentences produced by the proposed model are semantically correct, 3. the users are satisfied and accept the software created, and 4. the users approve and understand the sentences produced from this model.
In this thesis, we report on our work on developing Natural Language Processing (NLP) algorithms to aid readers and authors of scientific (biomedical) articles in detecting spin (distorted presentation of research results).
Our algorithm focuses on spin in abstracts of articles reporting Randomized Controlled Trials (RCTs).
We studied the phenomenon of spin from the linguistic point of view to create a description of its textual features.
We annotated a set of corpora for the key tasks of our spin detection pipeline: extraction of declared (primary) and reported outcomes, assessment of semantic similarity of pairs of trial outcomes, and extraction of relations between reported outcomes and their statistical significance levels.
Besides, we annotated two smaller corpora for identification of statements of similarity of treatments and of within-group comparisons.
We developed and tested a number of rule-based and machine learning algorithms for the key tasks of spin detection(outcome extraction,outcome similarity assessment, and outcome-significance relation extraction).
The best performance was shown by a deep learning approach that consists in fine-tuning deep pre-trained domain-specific language representations(BioBERT and SciBERT models) for our downstream tasks.
This approach was implemented in our spin detection prototype system, called De-Spin, released as open source code.
Our prototype includes some other important algorithms, such as text structure analysis (identification of the abstract of an article, identification of sections within the abstract), detection of statements of similarity of treatments and of within-group comparisons, extraction of data from trial registries.
Identification of abstract sections is performed with a deep learning approach using the fine-tuned BioBERT model, while other tasks are performed using a rule-based approach.
Our prototype system includes a simple annotation and visualization interface
With the widespread use of Artificial Intelligence (AI) systems, understanding the behavior of intelligent agents and robots is crucial to guarantee smooth human-agent collaboration since it is not straightforward for humans to understand the agent's state of mind.
Recent studies in the goal-driven Explainable AI (XAI) domain have confirmed that explaining the agent's behavior to humans fosters the latter's understandability of the agent and increases its acceptability.
However, providing overwhelming or unnecessary information may also confuse human users and cause misunderstandings.
For these reasons, the parsimony of explanations has been outlined as one of the key features facilitating successful human-agent interaction with a parsimonious explanation defined as the simplest explanation that describes the situation adequately.
While the parsimony of explanations is receiving growing attention in the literature, most of the works are carried out only conceptually.
This thesis proposes, using a rigorous research methodology, a mechanism for parsimonious XAI that strikes a balance between simplicity and adequacy.
To provide parsimonious explanations, HAExA relies first on generating normal and contrastive explanations and second on updating and filtering them before communicating them to the human.
To evaluate the proposed architecture, we design and conduct empirical human-computer interaction studies employing agent-based simulation.
The studies rely on well-established XAI metrics to estimate how understood and satisfactory the explanations provided by HAExA are.
The results are properly analyzed and validated using parametric and non-parametric statistical testing.
To accomplish this task, we propose in Chapter 3, three new models for modeling topic and word-topic dependencies between consecutive documents in document streams.
The second extension makes use of copulas, which constitute a generic tool to model dependencies between random variables.
We rely here on Archimedean copulas, and more precisely on Franck copula, as they are symmetric and associative and are thus appropriate for exchangeable random variables.
Our experiments, conducted on five standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones, as dynamic topic models, temporal LDA and the Evolving Hierarchical Processes,both in terms of perplexity and for tracking similar topics in document streams.
Compared to previous proposals, our models have extra flexibility and can adapt to situations where there are no dependencies between the documents.
On the other hand, the "Exchangeability" assumption in topic models like LDA oftenresults in inferring inconsistent topics for the words of text spans like noun-phrases, which are usually expected to be topically coherent.
In Chapter 4, we propose copulaLDA (copLDA), that extends LDA by integrating part of the text structure to the model and relaxes the conditional independence assumption between the word-specific latent topics given the per-document topic distributions.
We demonstrate empirically the effectiveness of copLDA on both intrinsic and extrinsic evaluation tasks on several publicly available corpora.
To complete the previous model (copLDA), Chapter 5 presents an LDA-based model that generates topically coherent segments within documents by jointly segmenting documents and assigning topics to their words.
The coherence between topics is ensured through a copula, binding the topics associated to the words of a segment.
In addition, this model relies on both document and segment specific topic distributions so as to capture fine-grained differences in topic assignments.
We show that the proposed model naturally encompasses other state-of-the-art LDA-based models designed for similar tasks.
Furthermore, our experiments, conducted on six different publicly available datasets, show the effectiveness of our model in terms of perplexity, Normalized Pointwise Mutual Information, which captures the coherence between the generated topics, and the Micro F1 measure for text classification.
In this thesis, we propose a new deep-learning-based approach for online classification on streams of high-dimensional data.
In recent years, Neural Networks (NN) have become the primary building block of state-of-the-art methods in various machine learning problems.
Most of these methods, however, are designed to solve the static learning problem, when all data are available at once at training time.
Performing Online Deep Learning is exceptionally challenging.
The main difficulty is that NN-based classifiers usually rely on the assumption that the sequence of data batches used during training is stationary, or in other words, that the distribution of data classes is the same for all batches (i.i.d. assumption).
When this assumption does not hold Neural Networks tend to forget the concepts that are temporarily not available in the stream.
In the literature, this phenomenon is known as catastrophic forgetting.
The approaches we propose in this thesis aim to guarantee the i.i.d. nature of each batch that comes from the stream and compensates for the lack of historical data.
To do this, we train generative models and pseudo-generative models capable of producing synthetic samples from classes that are absent or misrepresented in the stream and complete the stream's batches with these samples.
We test our approaches in an incremental learning scenario and a specific type of continuous learning.
Our approaches perform classification on dynamic data streams with the accuracy close to the results obtained in the static classification configuration where all data are available for the duration of the learning.
Besides, we demonstrate the ability of our methods to adapt to invisible data classes and new instances of already known data categories, while avoiding forgetting the previously acquired knowledge.
Autonomous Vehicles (AV) are emerging systems and considered cornerstones of the future of mobility.
Their design is a source of many academic and industrial research efforts.
The industrialization of AV is the mean for mobility stakeholders to strengthen their future position.
AVs function by interacting with their operational environment and must be fit for their Operational Context (OC).
This research work aims to support the architecting activities of Autonomous Vehicles to result in architectures fit for their Operational Context.
An OC ontology for AV is proposed to support scenario identification and definition in the early design phase, for a scenario-based design approach.
Using this ontology, a method to design AV logical architecture based on the OC is proposed.
The consideration of the OC in the architecting activities of AV is strengthened with a second method aiming at assessing the impact of OC change on the AV's architecture during the design phase.
The proposed contributions are validated with industrial case studies on the design of AV architectures given the OC and its evolution
Establishing the similarity of time series is at the core of many data mining tasks such as time series classification, time series clustering, time series retrieval, among others.
Metrics to establish similarities between time series are specific in the sense that they must be able to take into account the differences in the values making the series as well as distortions along the timelines.
The most popular similarity metric is the Dynamic Time Warping (DTW) measure.
However, it is costly to compute, and using it against numerous and/or very long time series is difficult in practice.
It shows how shapelets that preserve DTW measures can be used in the specific context of large scale time series retrieval.
This manuscript is making major contributions: (1) it explains how DTW-preserving shapelets can be used in the specific context of time series retrieval; (2) it proposes some shapelet selection strategies in order to cope with scale, that is, in order to deal with extremely large collection of time series; (3) it details how to handle both univariate and multivariate time series, hence covering the whole spectrum of time series retrieval problems.
The core of the contribution presented in this manuscript allows to easily trade-off the complexity of the transformation against the accuracy of the retrieval.
Experiments using the UCR and the UEA datasets demonstrate the vast performance improvements compared to state of the art techniques.
Establishing an Environment to Manage Linguistic Resources for a Text Analysis Platform Systems integrating natural language processing often use lexicons and grammars, sometimes indirectly corpora.
Because of the quantity and the complexity of the information in these linguistic resources, they are likely to become a source of inconsistency.
In this thesis we explore how to improve the management of linguistic resources for an industrial search engine in nineteen languages that performs an elaborate textual analysis.
We propose a method to formalize the linguistic architecture of the linguistic processing and its resources.
This formalization shows how the knowledge contained in the resources is exploited and gives us the possibility to build management tools compliant with the system's architecture.
The environment implemented in this way focuses on updating and acquiring the linguistic resources, while their exploitation is defined by the industrial constraints.
Keywords: linguistic architecture, linguistic resource, linguistic resource management, NLP system, NLP tool, natural language processing.
We propose an empirical contribution to recent attempts to unify cognitive science and social science.
We focus on Cultural Attraction Theory (CAT), a framework that proposes a common ontology made of representations for cognitive and social science to address interdisciplinary questions.
CAT hypothesizes that in spite of important transformations at the micro-level, the overall distribution of representations remains stable due to dynamical attractors.
We develop two case studies to show this with short written utterances.
The first examines transformations that quotations undergo as they are propagated online.
By connecting data mining tools with psycholinguistics, we show that word substitutions in quotations are consistent with the hypothesis of cultural attractors and with known effects of lexical features.
The second case study expands these results, and makes use of a purposefully developed web experiment to gather quality transmission chain data sets.
By extending a bioinformatics alignment algorithm, we decompose transformations into simpler operations, and propose a first descriptive model which relates psycholinguistic knowledge of sentence transformation to evolutionary trends elicited in the cultural evolution literature.
Finally, we show that further understanding the evolution of such representations requires an account of meaning in context, a task for which we flesh out possible empirical approaches.
Learning from data streams is emerging as an important application area.
When the environment changes, it is necessary to rely on on-line learning with the capability to adapt to changing conditions a.k.a. concept drifts.
Adapting to concept drifts entails forgetting some or all of the old acquired knowledge when the concept changes while accumulating knowledge regarding the supposedly stationary underlying concept.
Ensemble methods have been among the most successful approaches.
However, the management of the ensemble which ultimately controls how past data is forgotten has not been thoroughly investigated so far.
Our work shows the importance of the forgetting strategy by comparing several approaches.
The results thus obtained lead us to propose a new ensemble method with an enhanced forgetting strategy to adapt to concept drifts.
Experimental comparisons show that our method compares favorably with the well-known state-of-the-art systems.
In our work, we go one step further by introducing a meta-learning mechanism that is able to detect relevant states of the environment, to recognize recurring contexts and to anticipate likely concepts changes.
Hence, the method we suggest, deals with both the challenge of optimizing the stability-plasticity dilemma and with the anticipation and recognition of incoming concepts.
This is accomplished through an ensemble method that controls a ensemble of incremental learners.
This thesis studies the integration of phonetic landmarks into standard statistical large vocabulary continuous speech recognition (LVCSR).
Landmarks are discrete time instances that indicate the presence of phonetic events in the speech signal.
The goal is to develop landmark detectors that are motivated by phonetic knowledge in order to model selected phonetic classes more precisely than it is possible with standard acoustic models.
The thesis presents two landmark detection approaches, which make use of segment-based information and studies two different methods to integrate landmarks into the decoding, which are landmark-based pruning and a weighted combination approach.
While both approaches improve speech recognition performance compared to the baseline using weighted combination of landmarks and acoustic scores during decoding, they do not outperform standard frame-based phonetic predictions.
Since these results indicate that landmark-driven LVCSR requires the integration of very heterogeneous information, the thesis presents a third integration framework that is designed to integrate an arbitrary number of heterogeneous and asynchronous landmark streams into LVCSR.
The results indicate that this framework is indeed ale to improve the baseline system, as soon as landmarks provide complementary information to the regular acoustic models.
The manufacturers and the operators of the fleets of cyber-physical systems (CPSs) are subjected to huge expectations expressed in terms of the availability and reliability of the provided products and services during the exploitation of these fleets in dynamic environments.
These expectations foster the fleet manufacturers, particularly in the transportation sector, to develop effective mechanisms as far as the reactive planning of the maintenance operations at the fleet level is concerned.
In this research work, a multi-agent system (MAS) for the reactive maintenance planning of a fleet of CPSs is proposed.
The proposed MAS is conceived by using the ANEMONA design methodology and it aims at optimizing the fleet maintenance planning decisions to meet the specified objectives.
The experiments carried out in the course of this work demonstrate the ability of the proposed MAS in planning the fleet maintenance effectively (i.e. satisfying the fleet's availability and reliability requirements in a static environment) and reactively (i.e. being able to adapt/modify the fleet maintenance planning decisions following perturbations).
The effectiveness of the MAS model is validated by a mathematical programming model and its reactivity is tested by using simulated perturbations.
An application in rail transport industry to the fleet of trains at Bombardier Transportation France is proposed.
The proposed MAS is integrated in a decision support system called "MainFleet".
The development of Main-Fleet at Bombardier is ongoing.
This work concerns the joint segmentation of a set images in a Bayesian framework.
The proposed model combines the hierarchical Dirichlet process (HDP) and the Potts random field.
Hence, for a set of images, each is divided into homogeneous regions and similar regions between images are grouped into classes.
On the one hand, thanks to the HDP, it is not necessary to define a priori the number of regions per image and the number of classes, common or not.
On the other hand, the Potts field ensures a spatial consistency.
The arising a priori and a posteriori distributions are complex and makes it impossible to compute analytically estimators.
A Gibbs algorithm is then proposed to generate samples of the distribution a posteriori.
Moreover,a generalized Swendsen-Wang algorithm is developed for a better exploration of the a posteriori distribution.
Finally, a sequential Monte Carlo sampler is defined for the estimation of the hyperparameters of the model.
The choice of the best partition is done by minimization of a numbering free criterion.
These methods have been evaluated on toy examples and natural images.
The performance are assessed by metrics well-known in statistics but unused in image segmentation.
How words are recognized?
How do we process word meaning?
These questions have been pursued in lexical access and word recognition studies in the last half century of research in psycho-, neuro-, and linguistics.
Morphological processing is an essential level of processing for information extraction during word recognition.
In one extreme, full-entry models propose whole word storage in memory and post-lexical morphological processing based on paradigms;
in the other extreme, decompositional models posit pre-lexical decomposition and morphemic activation based on rules;
between then, dual-mechanism models consider two routes for word recognition, a whole-word associative route and a combinatorial rule-based route.
In the present thesis, it was investigated the morphological processing of French inflected verbs in visual modality in five studies.
Study 1 researched the mental lexicon organization in function of surface and cumulative frequencies;
Study 2 explored different stem formation processes;
Study 3 investigated morphological operations in the inflectional suffixes;
Study 4 tested the verbal morphological processing in L2 French speakers;
and Study 5 tested verbal violations coupled with electroencephalography acquisition.
The results suggest that all inflected French verbs are processed by a single-mechanism model with pre-lexical morphological decomposition for lexical activation and word recognition.
It is proposed different processing for the lexical and functional morphemes.
Words are decomposed in atomic morphemes, morphemic representations are activated in the mental lexicon, and word constituents are recombined for word verification
Inline quality control of the product is an important objective for industries growth.
Controlling a product quality requires measurements of its quality characteristics.
One hundred percent control is an important objective to overcome the limits of the control by sampling, in the case of defects related to exceptional causes.
However, industrial constraints have limited the deployment of measurement of product characteristics directly within production lines.
Human visual control is limited by its duration incompatible with the production cycle at high speed productions, by its cost and its variability.
Computer vision systems present a cost that reserves them for productions with high added value.
In addition, the automatic control of the quality of the appearance of the products remains an open research topic.
Our work aims to meet these constraints, as part of the injection-molding process of thermoplastics.
We propose a control system that is non-invasive for the production process.
Parts are checked right out of the injection molding machine.
We will study the contribution of non-conventional imaging.
Thermography of a hot molded part provides information on its geometry, which is complementary to conventional imaging.
Specifications include complex geometric features, as well as appearance features, which are difficult to formalize.
However, the appearance characteristics are difficult to formalize.
To automate aspect control, it is necessary to model the notion of quality of a part.
In order to exploit the measurements made on the hot parts, our approach uses statistical learning methods.
Thus, the human expert who knows the notion of quality of a piece transmits his knowledge to the system, by the annotation of a set of learning data.
Our control system then learns a metric of the quality of a part, from raw data from sensors.
We favor a deep convolutional network approach (Deep Learning) in order to obtain the best performances in fairness of discrimination of the compliant parts.
The small amount of annotated samples available in our industrial context has led us to use domain transfer learning methods.
Finally, in order to meet all the constraints and validate our propositions, we realized the vertical integration of a prototype of device of measure of the parts and the software solution of treatment by statistical learning.
The device integrates thermal imaging, polarimetric imaging, lighting and the on-board processing system necessary for sending data to a remote analysis server.
Two application cases make it possible to evaluate the performance and viability of the proposed solution
This thesis comprises three separate but interconnected essays that focus on the determinants and economic implications of corporate narrative disclosure.
Our study provides evidence that difficult-to-read annual reports can act as a non-trivial impediment to investors' ability to process information into useful trading signals.
The findings are robust to a battery of sensitivity tests, including endogeneity, use of alternative regression techniques, and use of alternative liquidity and readability proxies.
Using a large panel of U.S. public firms, the second essay presents the first evidence highlighting the relation between annual report readability and cost of equity capital.
We hypothesize that complex textual reporting deters investors' ability to process and interpret annual reports, leading to higher information risk, and thus higher cost of equity financing.
Consistent with our prediction, we find that greater textual complexity is associated with higher cost of equity capital.
The third essay investigates the effect of firms' tax avoidance practices on the textual properties of their annual filings.
Using a large sample of U.S.-listed firms, we document a positive and statistically significant relation between corporate tax avoidance and annual report textual complexity.
The findings are also robust to a number of checks, including, using additional control variables, employing alternative regression methodologies, and addressing endogeneity concerns.
Based on a corpus of five hundred research articles and counting more than six million words, the objective is twofold.
First, it is necessary to understand the mechanisms and processes by which HSS build their terminological apparatus and then to identify the combinatorial profiles of terms as well as their contextual interactive functioning.
The approach being part of corpus linguistics and natural language processing (NLP), the approach is therefore semasiological, combining socioterminology and lexicometry.
The analysis of the behaviour of these items in context allows us to identify semantic shifts and their relationship.
We assume that the term, although often presented as monosemic and stable in its biunivocal relationship with the concept it denominates, seems to be subject to denominational variations and is, therefore, indicative of enunciation strategies that manifest themselves in its lexicogenesis.
The availability of very large electronic text corpora makes it possible today to design methods that build word representations by an automatic, exhaustive exploration of their uses in these texts.
These representations are at the heart of a number of natural language processing applications, from information extraction to machine translation through question-answering.
In specialized domains however, two factors hinder the applicability of these methods.
On the one hand, domain-specific corpora are necessarily smaller than corpora with no domain restriction, whereas the size of the input corpora is a key factor in the quality of the created representations.
On the other hand, complex terms (made of multiple words) are of particular importance in specialized domains, whereas standard methods are designed to handle simple terms (made of one word).
The ADDICTE project aims to overcome these difficulties by providing a better representation of complex terms, by supplementing the analysis of domain-specific texts with additional resources (domain terminologies and out-of-domain texts), and by designing representations that can take advantage of such additional information.
In this context, the proposed thesis specifically addresses the design and test of methods that aim to take advantage of external resources to build distributional word representations.
This thesis falls within the framework of Natural Language Processing.
The problems of automatic summarization of Arabic documents which was approached, in this thesis, are based on two points.
The first point relates to the criteria used to determine the essential content to extract.
The second point focuses on the means to express the essential content extracted in the form of a text targeting the user potential needs.
In order to show the feasibility of our approach, we developed the "L.A.E" system, based on a hybrid approach which combines a symbolic analysis with a numerical processing.
The evaluation results are encouraging and prove the performance of the proposed hybrid approach.
These results showed, initially, the applicability of the approach in the context of mono documents without restriction as for their topics (Education, Sport, Science, Politics, Interaction, etc), their content and their volume.
They also showed the importance of the machine learning in the phase of classification and selection of the sentences forming the final extract.
This thesis is part of the problematics of the extraction of meaning from texts and textual flows, produced in our case during collaborative processes.
More specifically, we are interested in work-related emails and collaborative textual documents, with a first application to educational documents.
The motivation for this interest is to help users gain access to useful information more quickly; we hence seek to locate them in the texts.
Thus, we are interested in the tasks referred to in the emails, and to the fragments of educational documents which concern the themes of their interests.
Two corpora, one of e-mails and one of educational documents, mainly in French, have been created.
This was essential because there is virtually no previous work on this type of data in French.
Our first theoretical contribution is a generic modeling of the structure of these data.
We use it to specify the formal processing of documents, a prerequisite for semantic processing.
We demonstrate the difficulty of the problem of segmentation, standardization and structuring of documents in different source formats, and present the SEGNORM tool, the first software contribution of this thesis.
SEGNORM segments and normalizes documents (in plain or tagged text), recursively and in units of configurable size.
In the case of emails, it segments the messages containing quotations of messages into individual messages, thereby keeping the information about the chaining between the intertwined fragments.
It also analyzes the metadata of the messages to reconstruct the threads of discussions, and retrieves in the quotations the messages of which one does not have the source file.
We then discuss the semantic processing of these documents.
We propose an (ontological) modeling of the notion of task, then describe the annotation of a corpus of several hundred messages originating from the professional context of VISEO and GETALP.
We then present the second software contribution of this thesis: the tool for locating tasks and extracting their attributes (temporal constraints, assignees, etc.).
This tool, based on a combination of an expert approach and machine learning, is evaluated according to classic criteria of accuracy, recall and F-measure, as well as according to the quality of use.
Finally, we present our work on the MACAU-CHAMILO platform, third software contribution, which helps learning by (1) structuring of educational documents according to two ontologies (form and content), (2) multilingual access to content initially monolingual.
This is therefore again about structuring along the two axes, form and meaning.
(1) The ontology of forms makes it possible to annotate the fragments of documents by concepts such as theorem, proof, example, by levels of difficulty and abstraction, and by relations such as elaboration_of, illustration_of…
The domain ontology models the formal objects of informatics, and more precisely the notions of computational complexity.
This makes it possible to suggest to the users fragments useful for understanding notions of informatics perceived as abstract or difficult.
(2) The aspect related to multilingual access has been motivated by the observation that our universities welcome a large number of foreign students, who often have difficulty understanding our courses because of the language barrier.
We proposed an approach to multilingualize educational content with the help of foreign students, by online post-editing of automatic pre-translations, and, if necessary, incremental improvement of these post-editions.
(Our experiments have shown that multilingual versions of documents can be produced quickly and without cost.)
This work resulted in a corpus of more than 500 standard pages (250 words/page) of post-edited educational content into Chinese.
This thesis presents a computer device aimed at helping future FFL teacher training in Colombian universities.
It is grounded in text linguistics and aims to contribute to improving the linguistic level of university students currently in training.
To do so, this device is based on a textual corpus specifically annotated and labeled thanks to natural language processing (NLP) tools and to manual annotations in XML format.
This should allow the development of activities with a formative aim, while also taking into account the needs expressed by the target public (teachers/trainers and their students, the trainees).
As explained throughout this thesis, the elaboration of such a system is based on knowledge and skills stemming from several disciplines and/or fields: language didactics, educational engineering, general linguistics, textual linguistics, corpus linguistics, NLP and CALL.
The ambition is to provide trainees and trainers in higher education in Colombia with a tool designed according to their needs and their learning aims and objectives.
Finally, the originality of this system consists in the choice of target users, the didactic training model implemented and the specificity of the corpus annotated for the activities.
It is one of the first CALL systems based on textual linguistics specifically targeted at training future FFL teachers in a non-native language context.
The increasing mass of User-Generated Content (UGC) on the Internet means that people are now willing to comment, edit or share their opinions on different topics.
This content is now the main ressource for sentiment analysis on the Internet.
Due to abbreviations, noise, spelling errors and all other problems with UGC, traditional Natural Language Processing (NLP) tools, including Named Entity Recognizers and part-of-speech (POS) taggers, perform poorly when compared to their usual results on canonical text (Ritter et al., 2011).
This thesis deals with Named Entity Recognition (NER) on some User-Generated Content (UGC).
We have created an evaluation dataset including multi-domain and multi-sources texts.
We then developed a Conditional Random Fields (CRFs) model trained on User-Generated Content (UGC).
In order to improve NER results in this context, we first developed a POS tagger on UGC and used the predicted POS tags as a feature in the CRFs model.
To turn UGC into canonical text, we also developed a normalization model using neural networks to propose a correct form for Non-Standard Words (NSW) in the UGC.
Automatically extract information from text documents has become a major challenge during these last years.
One of the main proxy task that has been proposed has a way to evaluate a reading model is question-answering.
A model is expected to produce an answer regarding a question and an associated text document.
For the moment, models that have been proposed require lots of training data and tend to suffer from a lack of robustness against noise in the dataset.
The objective of this thesis is to investigate how adversarial learning can improve the performance of reading models.
Different components of statistical machine translation systems are considered as optimization problems.
Indeed, the learning of the translation model, the decoding and the optimization of the weights of the log-linear function are three important optimization problems.
Knowing how to define the right algorithms to solve them is one of the most important tasks in order to build an efficient translation system.
Several optimization algorithms are proposed to deal with decoder optimization problems.
They are combined to solve, on the one hand, the decoding problem that produces a translation in the target language for each source sentence, on the other hand, to solve the problem of optimizing the weights of the combined scores in the log-linear function to fix the translation evaluation function during the decoding.
The reference system in statistical translation is based on a beam-search algorithm for the decoding, and a line search algorithm for optimizing the weights associated to the scores.
We propose a new statistical translation system with a decoder entirely based on genetic algorithms.
Genetic algorithms are bio-inspired optimization algorithms that simulate the natural process of evolution of species.
They allow to handle a set of solutions through several iterations to converge towards optimal solutions.
This work allows us to study the efficiency of the genetic algorithms for machine translation.
This work allowed us to propose a new machine translation system with a decoder entirely based on genetic algorithms
Information security relies on the correct interaction of several abstraction layers: hardware, operating systems, algorithms, and networks.
However, protecting each component of the technological stack has a cost; for this reason, many devices are left unprotected or under-protected.
This thesis addresses several of these aspects, from a security and cryptography viewpoint.
To that effect we introduce new cryptographic algorithms (such as extensions of the Naccache–Stern encryption scheme), new protocols (including a distributed zero-knowledge identification protocol), improved algorithms (including a new error-correcting code, and an efficient integer multiplication algorithm), as well as several contributions relevant to information security and network intrusion.
Furthermore, several of these contributions address the performance of existing and newly-introduced constructions.
It consists of identifying some textual objects such as person, location and organization names.
The work of this thesis focuses on the named entity recognition task for the oral modality.
In the first part, we study the characteristics of the named entity recognition downstream of the automatic speech recognition system.
We present a methodology which allows named entity recognition following a hierarchical and compositional taxonomy.
We measure the impact of the different phenomena specific to speech on the quality of named entity recognition.
In the second part, we propose to study the tight pairing between the speech recognition task and the named entity recognition task.
For that purpose, we take away the basic functionnalities of a speech recognition system to turn it into a named entity recognition system.
Therefore, by mobilising the inherent knowledge of the speech processing to the named entity recognition task, we ensure a better synergy between the two tasks.
We carry out different types of experiments to optimize and evaluate our approach.
In this thesis we used MRI (Magnetic Resonance Imaging) data of the vocal tract to study speech production.
The first part consist of the study of the impact that the velum, the epiglottis and the head position has on the phonation of five french vowels.
Acoustic simulations were used to compare the formants of the studied cases with the reference in order to measure their impact.
Therefore the second part presents some algorithms that one can use in order to enhance speech production data.
Several image transformations were combined in order to generate estimations of vocal tract shapes which are more informative than the original ones.
At this point, we envisaged apart from enhancing speech production data, to create a generic speaker model that could provide enhanced information not for a specific subject, but globally for speech.
Finally, the last part of the thesis, refers to a selection of open questions of the field that are still left unanswered, some interesting directions that one can expand this thesis and some potential approaches that could help someone move forward towards these directions.
This thesis aims to extract a sample of texts used as a training population for a markov process.
We adopted a stratification sampling.
We set up a software called "multivariate sampling" which extracts a sample from a stratified and ambiguous corpus minimizing "the loss of information" in a certain sense, the results obtained are very satisfying since we improved the number of parts of speech tagged correctly.
This probability is not unique whatever topologies used.
The li isometries show us that it is impossible to obtain a unique solution to this problem.
A unique solution constraints the "true" and "false" representation to be the same.
It appeared that one has to distinguish the "true" associated to a logical formula from "the certain event" known in the probability theory
finally, we proposed a new markov model capable to take into account he context associated to a pos.
This thesis presents the implementation of a french grammar using a formalism called tree adjoining grammar (tag).
This formalism belongs to the family of unification based formalisms which have proved fruitful for natural language processing.
We have added some constraints to the original formalism, especially a "lexicalization" constraint, which captures the philosophy of M. Gross 'lexicon-grammars.
We have defined the elementary structures needed for describing a large range of syntactic french phenomena such as: subcategorization, agreement, complement clauses and unbounded dependencies, support verb constructions and idioms, different kind of relative clauses and cleft extractions, passive and extraposition.
We have defined the necessary features associated to these structures.
We present for each phenomenon a detailed comparison with other formalisms such as gpsg, lfg and string grammars.
We then present the parsing strategy suitable for "lexicalized" grammars and the current state of implementation of our french parser.
The majority of recent embedded systems are based on massively parallel MPSoC architectures, hence the necessity of developing embedded parallel applications.
Embedded parallel application design becomes more challenging: It becomes a parallel programming for non-trivial heterogeneous multiprocessors with diverse communication architectures and design constraints such as hardware cost, power, and timeliness.
This is especially critical for embedded systems powered by MPSoC, where ever demanding applications have to run smoothly on numerous cores, each with modest power budget.
Moreover, application performance does not necessarily improve as more cores are added.
Application performance can be limited due to multiple bottlenecks including contention for shared resources such as caches and memory.
It becomes time consuming for a developer to pinpoint in the source code the bottlenecks decreasing the performance.
To overcome these issues, in this thesis, we propose a fully three automatic methods which detect the instructions of the code which lead to a lack of performance due to contention and scalability of processors on a chip.
The methods are based on data mining techniques exploiting gigabytes of low level execution traces produced by MPSoC platforms.
Our profiling approaches allow to quantify and pinpoint, automatically the bottlenecks in source code in order to aid the developers to optimize its embedded parallel application.
We performed several experiments on several parallel application benchmarks.
Our experiments show the accuracy of the proposed techniques, by quantifying and pinpointing the hotspot in the source code.
In this dissertation, the thesis that deep neural networks are suited for analysis of visual, textual and fused visual and textual content is discussed.
Recurrent neural networks for spoken language understanding (slot filling): different architectures are compared for this task with the aim of modeling both the input context and output label dependencies.2) Action prediction from single images: we propose an architecture that allow us to predict human actions from a single image.
The architecture was extensively studied an evaluated in international benchmarks within the task of video hyperlinking where it defined the state of the art today.4)
Social networks and online communities have become major players in the field of tourism.
The information exchanged on these communities is increasingly influencing consumer behavior.
At the same time, the number of mobile phone users is also growing.
Mobile offers their users different services.
Social networks associated with mobile technology can thus be a source of information for the tourist and thus be considered as a key factor that influences his behavior during his trip.
Our purpose would be to study the behavior of the tourist in the context of the use of a mobile application that is based on the sharing of opinions and feedback from other travelers.
The present research examined the problems in adapting Thai toponyms or place names in a corpus of four French guidebooks on Thailand.
The linguistic and translation analysis showed that Thai toponyms were well integrated in French at different levels.
Firstly, they were romanised by various systems including using French graphemes.
In the referential semantic perspective, their fundamental value is locative, but in some contexts they could be metonymically amd metaphorically interpreted.
Moreover, despite the problem of meaning and non-translation of the proper names, the semantic transfer of Thai toponyms into French was possible by using various translation procedures.
The author could modify the literal translation of Thai toponyms or create a new one to better present the dominant characterisation of the place with free translation technique.
These strategies revealed the pragmatic nature of the guidebooks which is to make the reader discover or know the unknown place and arouse the reader's interest.
This research aims to understand how the visual electronic word of mouth emitted by a youtubeur influences the affect and the purchase intention of the consumers.
We study the impact of word of mouth characteristics and the effect of social contagion.
The main results are:1.
The professional status of the youtubeur, the existence of commercial links between brands and youtubers and the valence of the magazine have an impact on the feeling expressed and therefore the intention to purchase.2.
There is a social contagion, both emotional and behavioral within the audience of electronic and visual word of mouth that impacts its effectiveness.3.
The number of subscribers of an individual (individual characteristic of the commentator) impacts his sensitivity to social contagion.
The explosion of data on the Internet leads to challenges in working with them.
Semantic Web and ontology are required to address those problems.
Nowadays, ontology plays more and more an important role as a means in domain knowledge representation.
It requires access to a more concrete level of description where each NPI can be evaluated by science, monitored by professionals and explained to the patient.
To do this, an international and evolutionary classification based on the results of science is necessary.
Constructing this ontology manually is time consuming and thus an expensive process.
Particularly, the step of collecting the NPI terminology requires much more time than the rest, because of heterogeneous and big resources in the context of NPIs.
An automatic or semi-automatic method is thus essential to support NPI experts in this task.
Therefore, a simple and friendly visualization of the ontology for NPI experts needs to be considered.
The first contribution concerns the semi-automatic process for collecting NPI terms.
Two approaches, knowledge-based and corpus-based, are presented to retrieve candidate NPI terms.
A new similarity measure for NPI is proposed and evaluated.
The second contribution is a new method for ontology visualization based on MindMap.
A web-based tool is then implemented to convert OWL ontologies to FreeMind documents which can be imported by existing Mind-Mapping applications to make visualizations.
Considering the industrial context, we focused our research on some issues, in informatics and lexicography.
We also show how to extend it to meet new needs such as those of the INNOVALANGUES project.
Finally, we have created a "lemmatisation middleware", LEXTOH, which allows calling several morphological analyzers or lemmatizers and then to merge and filter their results.
Combined with a new dictionary creation tool, CREATDICO, LEXTOH allows to build on the fly a "mini-dictionary" corresponding to a sentence or a paragraph of a text being "post-edited" online under IMAG/SECTRA, which performs the lexical proactive support functionality foreseen in [Huynh, C.-P., 2010].
With the impressive progress that has been made in transcribing spoken language, it is becoming increasingly possible to exploit transcribed data for tasks that require comprehension of what is said in a conversation.
The work in this dissertation, carried out in the context of a project devoted to the development of a meeting assistant, contributes to ongoing efforts to teach machines to understand multi-party meeting speech.
We have focused on the challenge of automatically generating abstractive meeting summaries.
We first present our results on Abstractive Meeting Summarization (AMS), which aims to take a meeting transcription as input and produce an abstractive summary as output.
We introduce a fully unsupervised framework for this task based on multi-sentence compression and budgeted submodular maximization.
We also leverage recent advances in word embeddings and graph degeneracy applied to NLP, to take exterior semantic knowledge into account and to design custom diversity and informativeness measures.
Next, we discuss our work on Dialogue Act Classification (DAC), whose goal is to assign each utterance in a discourse a label that represents its communicative intention.
DAC yields annotations that are useful for a wide variety of tasks, including AMS.
We propose a modified neural Conditional Random Field (CRF) layer that takes into account not only the sequence of utterances in a discourse, but also speaker information and in particular, whether there has been a change of speaker from one utterance to the next.
We provide a novel approach to ACD in which we first introduce a neural contextual utterance encoder featuring three types of self-attention mechanisms and then train it using the siamese and triplet energy-based meta-architectures.
We further propose a general sampling scheme that enables the triplet architecture to capture subtle patterns (e.g., overlapping and nested clusters).
The object of this thesis is to study causality through its discursive expression in French texts.
This linguistic study has been made in the perspective of automatic language processing.
This work takes place within a project of semantic filtering of texts (named SAFIR: automatic information filtering for summarizing texts) which is dedicated to the production of syntheses and summaries.
But the present work ranges over knowledge acquisition through texts.
Our first objective is to index the various linguistic processes that are used by authors to convey causal relations.
We use an original contextual exploration method that is not based upon a "deep representation" of the text under consideration, rather upon an automatic identification of markers that are considered as relevant.
We propose a map made of 1500 markers (verbs, phrases, adverbs,...)
We adopt the formalism of the classes of objects to describe metonymy in French.
The objective of this description is twofold: to provide a description of the regularities in the way this mechanism works and to constitute lexical bases dedicated to the automatic treatment of languages.
It is postulated that the minimal frame of analysis of the lexical items is the simple sentence defined in terms of predicates and arguments.
According to this principle, the metonymy is analyzed as a semantic mechanism indicated by a transfer of predicates between classes of words correlated to appropriate predicates.
When it concerns elementary nouns, the transfer operates a semantic re-categorization and creates new uses.
The categorization of the words into arguments and predicates allows us to propose a tripartite classification of metonymy.
The metonymy of the meronymic type and those who are of the argumental type, put in relation elementary nouns, whereas the metonymy of the predicative type operates a structural re-categorization.
In this last instance, we distinguish at first a double actualization (predicative and argumental) of polysemous lexical items, before witnessing a transfer of predicates which corresponds to the structural re-categorization.
Transfer and actualization are both organizing principles which bring to light the systematic character of metonymy.
The main aim of this thesis is to investigate the automatic quality assessment of spoken language translation (SLT), called Confidence Estimation (CE) for SLT.
Due to several factors, SLT output having unsatisfactory quality might cause various issues for the target users.
Therefore, it is useful to know how we are confident in the tokens of the hypothesis.
Our first contribution of this thesis is a toolkit LIG-WCE which is a customizable, flexible framework and portable platform for Word-level Confidence Estimation (WCE) of SLT.
WCE for SLT is a relatively new task defined and formalized as a sequence labelling problem where each word in the SLT hypothesis is tagged as good or bad accordingto a large feature set.
We propose several word confidence estimators (WCE) based on our automatic evaluation of transcription (ASR) quality, translation (MT) quality,or both (combined/joint ASR+MT).
This research work is possible because we built a specific corpus, which contains 6.7k utterances for which a quintuplet containing: ASRoutput, verbatim transcript, text translation, speech translation and post-edition of the translation is built.
The conclusion of our multiple experiments using joint ASR and MT features for WCE is that MT features remain the most influent while ASR features can bring interesting complementary information.
As another contribution, we propose two methods to disentangle ASR errors and MT errors, where each word in the SLT hypothesis is tagged as good, asr_error or mt_error.
We thus explore the contributions of WCE for SLT in finding out the source of SLT errors.
Furthermore, we propose a simple extension of WER metric in order to penalize differently substitution errors according to their context using word embeddings.
Our experiments show that the correlation of the new proposed metric with SLT performance is better than the one of WER.
To conclude, we have proposed several prominent strategies for CE of SLT that could have a positive impact on several applications for SLT.
Robust quality estimators for SLT can be used for re-scoring speech translation graphs or for providing feedback to the user in interactive speech translation or computer-assisted speech-to-text scenarios.
Keywords: Quality estimation, Word confidence estimation (WCE), Spoken Language Translation (SLT), Joint Features, Feature Selection.
The present publications cover a relatively vast area of applied linguistics from teaching foreign languages and german as a foreign language to machine translation ; they deal with questions of syntax for computational linguistics, problems in automatic indexation etc.
It does not only include a treatment of a variety of issues but also a formalization of rules.
A series of articles in Brazilian Portuguese written during a teaching assignment in Brazil, deal with the question of automatic text analysis of portuguese.
These articles contain various aspects: a detailed formalization of morphology and syntax, application problems in automatic indexation, information systems for parliament and machine translation.
Since 1985 emphasis has been on machine translation.
This includes strategic considerations for determining the kind of skills needed in the development of such a system ; also, certain specific problems have been analyzed and the systems have been tested.
This, in tur, has led to conclusions concerning the software part of the system.
Events are central in many Natural Language Processing tasks, despite the lack of a unified definition for the concept.
Creating these templates is an expert task and therefore costly, painstaking and hard to extend to new domains.
Meanwhile, the amount of data produced by individuals and organizations has grown exponentially, opening unprecedented perspectives of applications.
To this end, we propose a bottom-up approach composed of three main steps.
The first step clusters several textual mentions of a same particular event (i.e tied to a time and place) to identify distinct instances.
The second step groups these instances together based on more abstract features to infer event types.
Finally, the third and last step extracts the most salient elements of each type to produce the synthetic, template-like structure we are looking for.
The WebLab platform is an application used to define and execute media-mining workflows.
A designer can create complex media-mining workflows using components, whose operation is not always known (black-boxes services).
These complex workflows can lead to a problem of data quality, however, and before this work, no tool existed to analyse and improve the quality of WebLab workflows.
To deal with black-box services, we choose to tackle this quality problem with a non-intrusive approach: we enhance the definition of the WebLab workflow with provenance and quality propagation rules.
Provenance rules generate fine-grained data dependency links between data and services after the execution of a WebLab workflow.
Then the quality propagation rules use these links to reason on the influence that the quality of the data used by a component has on the quality of the output data…
In this thesis we propose several methods for unsupervised person identification in TV broadcast using the names written on the screen.
As the use of biometric models to recognize people in large video collections is not a viable option without a priori knowledge of people present in this videos, several methods of the state-of-the-art proposes to use other sources of information to get the names of those present.
These methods mainly use the names pronounced as source of names.
However, we can not have a good confidence in this source due to transcription or detection names errors and also due to the difficulty of knowing to who refers a pronounced name.
The names written on the screen in TV broadcast have not be used in the past due to the difficulty of extracting these names in low quality videos.
However, recent years have seen improvements in the video quality and overlay text integration.
We therefore re-evaluated in this thesis, the use of this source of names.
We first developed LOOV (for LIG Overlaid OCR in Video), this tool extract overlaid texts written in video.
With this tool we obtained a very low character error rate.
This allows us to have an important confidence in this source of names.
We then compared the written names and pronounced names in their ability to provide the names of person present in TV broadcast.
We found that twice persons are nameable by written names than by pronounced names with an automatic extraction of them.
Another important point to note is that the association between a name and a person is inherently easier for written names than for pronounced names.
With this excellent source of names we were able to develop several unsupervised naming methods of people in TV broadcast.
We started with late naming methods where names are propagated onto speaker clusters.
These methods question differently the choices made during the diarization process.
We then proposed two methods (integrated naming and early naming) that incorporate more information from written names during the diarization process.
To identify people appear on screen, we adapted the early naming method for faces clusters.
Finally, we have also shown that this method also works for multi-modal speakers-faces clusters.
With the latter method, that named speech turn and face during a single process, we obtain comparable score to the best systems that contribute during the first evaluation REPERE
Metabolomics allows large-scale studies of the metabolic profile of an individual, which is representative of its physiological state.
Metabolic markers characterising a given condition can be obtained through the comparison of those profiles.
Therefore, metabolomics reveals a great potential for the diagnosis as well as the comprehension of mechanisms behind metabolic dysregulations, and to a certain extent the identification of therapeutic targets.
However, in order to raise new hypotheses, those applications need to put metabolomics results in the light of global metabolism knowledge.
This contextualisation of the results can rely on metabolic networks, which gather all biochemical transformations that can be performed by an organism.
The major bottleneck preventing this interpretation stems from the fact that, currently, no single metabolomic approach allows monitoring all metabolites, thus leading to a partial representation of the metabolome.
This thesis proposes a new approach to overcome those limitations, through the suggestion of relevant metabolites, which could fill the gaps in a metabolomics signature.
This method is inspired by recommender systems used for several on-line activities, and more specifically the recommendation of users to follow on social networks.
This approach has been used for the interpretation of the metabolic signature of the hepatic encephalopathy.
It allows highlighting some relevant metabolites, closely related to the disease according to the literature, and led to a better comprehension of the impaired mechanisms and as a result the proposition of new hypothetical scenario.
Ln th is Natural Language Processing Ph. D. Thesis, we aim to perform semantic role labeling on French domain-specific texts.
This task first disambiguates the sense of predicates in a given text and annotates its child chunks with semantic roles such as Agent, Patient or Destination.
The task helps many applications in domains where annotated corpora exist, but is difficult to use otherwise.
We first evaluate on the FrameNet corpus an existing method based on VerbNet, which explains why the method is domain-independant.
To apply this method to French, we first translate lexical resources.
We first translate the WordNet lexical database.
Next, we translate the VerbNet lexicon which is organized semantically using syntactic information.
We obtains its translation, VerbuNet, by reusing two French verb lexicons (the Lexique-Grammaire and Les Verbes Français) and by manually modifying and reorganizing the resulting lexicon.
Finally, once those building blocks are in place, we evaluate the feasibilty of semantic role labeling of French and English in three specific domains.
We study the pros and cons of using VerbNet and VerbnNet to annotate those domains before explaining our future work.
With the dramatic growth of digital information, finding precise answers to natural language questions is more and more essential for retrieving domain knowledge in real time.
Many research works tackled answer retrieval for factual questions in open domain.
Less works were performed for domain-specific question answering such as the medical domain.
Compared to the open domain, several different conditions are met in the medical domain such as specialized vocabularies, specific types of questions, different kinds of domain entities and relations.
Document characteristics are also a matter of importance, as, for example, clinical texts may tend to use a lot of technical abbreviations while forum pages may use long “approximate” terms.
A key process for this task is to analyze the questions and the source documents semantically and to use standard formalisms to represent the obtained annotations.
We propose a medical question-answering approach based on: (i) NLP methods combing domain knowledge, rule-based methods and statistical ones to extract relevant information from questions and documents and (ii) Semantic Web technologies to represent and interrogate the extracted information.
In the same way as TV channels, data streams are represented as a sequence of successive events that can exhibit chronological relations (e.g. a series of programs, scenes, etc.).
For a targeted channel, broadcast programming follows the rules defined by the channel itself, but can also be affected by the programming of competing ones.
In such conditions, event sequences of parallel streams could provide additional knowledge about the events of a particular stream.
In the sphere of machine learning, various methods that are suited for processing sequential data have been proposed.
Long Short-Term Memory (LSTM) Recurrent Neural Networks have proven its worth in many applications dealing with this type of data.
Nevertheless, these approaches are designed to handle only a single input sequence at a time.
The main contribution of this thesis is about developing approaches that jointly process sequential data derived from multiple parallel streams.
The application task of our work, carried out in collaboration with the computer science laboratory of Avignon (LIA) and the EDD company, seeks to predict the genre of a telecast.
This prediction can be based on the histories of previous telecast genres in the same channel but also on those belonging to other parallel channels.
We propose a telecast genre taxonomy adapted to such automatic processes as well as a dataset containing the parallel history sequences of 4 French TV channels.
Two original methods are proposed in this work in order to take into account parallel stream sequences.
The first one, namely the Parallel LSTM (PLSTM) architecture, is an extension of the LSTM model.
PLSTM simultaneously processes each sequence in a separate recurrent layer and sums the outputs of each of these layers to produce the final output.
The second approach, called MSE-SVM, takes advantage of both LSTM and Support Vector Machines (SVM) methods.
Firstly, latent feature vectors are independently generated for each input stream, using the output event of the main one.
These new representations are then merged and fed to an SVM algorithm.
The PLSTM and MSE-SVM approaches proved their ability to integrate parallel sequences by outperforming, respectively, the LSTM and SVM models that only take into account the sequences of the main stream.
The two proposed approaches take profit of the information contained in long sequences.
However, they have difficulties to deal with short ones.
Though MSE-SVM generally outperforms the PLSTM approach, the problem experienced with short sequences is more pronounced for MSE-SVM.
Finally, we propose to extend this approach by feeding additional information related to each event in the input sequences (e.g. the weekday of a telecast).
This extension, named AMSE-SVM, has a remarkably better behavior with short sequences without affecting the performance when processing long ones.
In this work, we consider data from functional Magnetic Resonance Imaging (fMRI), that we study in a machine learning setting: we learn a model of brain activity that should generalize on unseen data.
We first focus on unsupervised analysis of terabyte-scale fMRI data acquired on subjects at rest (resting-state fMRI).
We present new methods for running sparse matrix factorization/dictionary learning on hundreds of fMRI records in reasonable time.
Our leading approach relies on introducing randomness in stochastic optimization loops and provides speed-up of an order of magnitude on a variety of settings and datasets.
We provide an extended empirical validation of our stochastic subsampling approach, for datasets from fMRI, hyperspectral imaging and collaborative filtering.
We derive convergence properties for our algorithm, in a theoretical analysis that reaches beyond the matrix factorization problem.
We then turn to work with fMRI data acquired on subject undergoing behavioral protocols (task fMRI).
We investigate how to aggregate data from many source studies, acquired with many different protocols, in order to learn more accurate and interpretable decoding models, that predicts stimuli or tasks from brain maps.
As a consequence, our multi-study model performs better than single-study decoding.
Our approach identifies universally relevant representation of brain activity, supported by a few task-optimized networks learned during model fitting.
Finally, on a related topic, we show how to use dynamic programming within end-to-end trained deep networks, with applications in natural language processing.
Controlled Natural Languages (CNL) are artificial languages that use a subset of the vocabulary, morphological forms and syntactical constructions of a natural language while eliminating its polysemy.
In a way, they constitute the bridge between formal languages and natural languages.
Therefore, they perform the communicative function of the textual mode while being precise and computable by the machine without any ambiguity.
In particular, they can be used to facilitate the population or update of knowledge bases within the framework of a human-machine interface.
Since 1971, the French Marine Hydrographic and Oceanographic Service (SHOM) issues the French Coast Pilot Books Instructions nautiques, collections of general, nautical and statutory information, intended for use by sailors.
They are mandatory for fishing and commercial ships.
Among these standards, one of a particular interest is the universal model of hydrographic data (S-100 standard, January, 2010).This thesis analyses the use of a CNL to represent knowledge contained in the Instructions nautiques.
This CNL purpose is to act as a pivot between the writing of the text by the dedicated operator, the production of the printed or online publication, and the interaction with knowledge bases and navigational aid tools.
We will focus especially on the interaction between the Instructions nautiques Controlled Natural Language and the corresponding Electronic Navigational Charts (ENC).More generally, this thesis asks the question of the evolution of a CNL and the underlying ontologies involved in the Instructions nautiques project.
Instructions nautiques have the particularity of combining both strictness (numerical data, electronic charts, legislation) and a certain amount of flexibility (text writing by human operators, unpredictability of the knowledge to be included due to the evolution of sailors¿ practices and needs).
We define in this thesis a dynamic CNL in the same way that dynamic ontologies are defined in particular domains.
The mechanisms of the CNL presented in this thesis, although developed for the domain of the maritime navigation, have the potential to be adapted to other domains using multimodal corpuses.
Finally, the benefits in the future of a controlled hybrid language are undeniable: the use of the different modalities in their full potential can be used in many different applications (for example, the exploitation of the visual modality for a 3D extension).
Graph-Based Semi-Supervised Learning (G-SSL) techniques learn from both labelled and unlabelled data to build better classifiers.
Despite successes, its performance can still be improved, particularly in cases of graphs with unclear clusters or unbalanced labelled datasets.
To address such limitations, the main contribution of this dissertation is a novel method for G-SSL referred to as the Lγ - PageRank method.
It consists of a generalization of the PageRank algorithm based on the positive γ-th powers of the graph Laplacian matrix.
The theoretical study of Lγ - PageRank shows that (i) for γ &lt; 1, it corresponds to an extension of the PageRank algorithm to Levy processes: where random walkers can now perform far-distant jumps in a single step; and (ii) for γ &gt; 1, it operates on signed graphs: where nodes belonging to one same class are more likely to share positive edges while nodes from different classes are more likely to be connected with negative edges.
We show the existence of an optimal γ-th power that maximizes performance, for which a method for its automatic estimation is devised and assessed.
Experiments on several datasets demonstrate that the L´evy flight random walkers can enhance the detection of classes with complex local structures and that the signed graphs can significantly improve the separability of data and also override the issue of unbalanced labelled data.
In addition, we study efficient implementations of Lγ -PageRank.
Extensions of Power Iteration and Gauss-Southwell, successful algorithms to efficiently compute the solution of the standard PageRank algorithm, are derived for Lγ - PageRank.
Moreover, the dynamic versions of Power Iteration and Gauss-Southwell, which can update the solution of standard PageRank in sublinear complexity when the graph evolves or new data arrive, are also extended to Lγ - PageRank.
Lastly, we apply Lγ - PageRank in the context of Internet routing.
We address the problem of identifying the Autonomous Systems (AS) of inter-AS links from the network of IP addresses and AS public registers.
Experiments on tracerout measurements collected from the Internet show that Lγ - PageRank can solve this inference task with no errors, even when the expert does not provide labelled examples of all classes.
Researchers in the field of ECAs try to create agents that can be more natural, believable and easy to use.
Designing an ECA requires understanding that manner, personality, emotion, and appearance are very important issues to be considered.
In this thesis, we are interested in increasing believability of ECAs by placing personality at the heart of the human-agent verbal interaction.
We propose a model relating personality facets and hidden communication goals that can influence ECA behaviors.
Life sciences produce a huge amount of data (e.g., clinical trials, scientific articles) so that integrating and analyzing all the datasets related to a given research question like the correlation between phenotypes and genotypes, is a key element for knowledge discovery.
Indeed, ontologies provide a common domain vocabulary for humans, and formal entity definitions for machines.
A large number of biomedical ontologies and terminologies has been developed to represent and annotate various datasets.
However, datasets represented with different overlapping ontologies are not interoperable.
It is therefore crucial to establish correspondences between the ontologies used; an active area of research known as ontology matching.
Original ontology matching methods usually exploit the lexical and structural content of the ontologies to align.
These methods are less effective when the ontologies to align are lexically heterogeneous i.e., when equivalent concepts are described with different labels.
To overcome this issue, the ontology matching community has turned to the use of external knowledge resources as a semantic bridge between the ontologies to align.
This approach arises several new issues mainly: (1) the selection of these background resources, (2) the exploitation of the selected resources to enhance the matching results.
Several works have dealt with these issues jointly or separately.
In our thesis, we made a systematic review and historical evaluation comparison of state-of-the-art approaches.
Ontologies, others than the ones to align, are the most used background knowledge resources.
Related works often select a set of complete ontologies as background knowledge, even if, only fragments of the selected ontologies are actually effective for discovering new mappings.
We propose a novel BK-based ontology matching approach that selects and builds a knowledge resource with just the right concepts chosen from a set of ontologies.
We propose two methods to select the most relevant mappings from the candidate ones: (1) based on a set of rules and (2) with Supervised Machine Learning.
We experiment and evaluate our approach in the biomedical domain, thanks to the profusion of knowledge resources in biomedicine (ontologies, terminologies and existing alignments).We evaluated our approach with extensive experiments on two Ontology Alignment Evaluation Initiative (OAEI) benchmarks.
Our results confirm the effectiveness and efficiency of our approach and overcome or compete with state-of-the-art matchers exploiting background knowledge resources.
The renovation of housing buildings is today a major focus of the fight to reduce the energy consumption and, more generally, global warming.
However, if the situation seems technically simple, there are still many obstacles (economic, social, cultural) that slow down the renovation of the existing park.
These brakes require to look at the renovation in a global perspective, with the work of a facilitator, able to establish a relationship of trust with the inhabitants.
This global approach is even more important when it comes to renovating condominiums.
Indeed, in these buildings, decision making is complicated by the number of actors and their specific organization.
To allow condominiums to vote for renovation works, a long and expensive accompaniment work is necessary.
To make this work of accompaniment as effective as possible, we have designed a decision tool, usable by the facilitator, which allows him to carry out a multidisciplinary diagnosis of the co-ownership and to recommend solutions of accompaniments ajusted.
In particular, we defined a list of influential criteria for the decision of a condominium to undertake renovations, and we compiled them into an evaluation tool.
The co-ownership is studied according to all its characteristics (quality of use, technical possibilities, economic potential, sociological profile of the owners, quality of the neighborhood, collective dynamics) and specific prescriptions are made according to the evaluation of each criterion.
Although the first results are encouraging, there are still many opportunities for improvement to massively use this tool and make the diagnosis even more precise.
Many methods exist for solving multicriteria optimization problems, and it is not easy to choose the right method well adapted to a given multicriteria problem.
Even after choosing a multicriteria method, various parameters (e.g. weight, utility functions, etc.) must be carefully determined either to find the optimal solution (best compromise) or to classify all feasible solutions (the set of alternatives).
To overcome this potential difficulty, elicitation methods are used in order to help the decision maker to fix safely the parameters.
Additionally, we assume that we have a set of feasible solutions, and we also make the assumption that we have prior information about the preferences of the decision maker, and we focus on how to use this information, rather than how to get them.
In the first contribution of this work, we take advantage of a simple and quickly computable statistical measure, namely, the Spearman ρ correlation coefficient, to develop an gready approche, and two exact approaches based on constraint programming (CP) and linear integer programming (MIP).
These methods are then used to automatically elicit the appropriate parameter of the lexicographic ordering method.
We also propose some elicitation models for most commonly used multicriteria methods, such as MinLeximax method used to ensure fairness and efficiency requirements, the weighted sum method, and OWA operators.
Characterizing the quality of smells is a complex process that consists in identifying a set of descriptors best summarizing the olfactory sensation.
Generally, this characterization results in a limited set of descriptors provided by sensorial analysis experts.
These sensorial analysis sessions are however very costly for industrials.
Indeed, such oriented approaches based on vocabulary learning limit, in a restrictive manner, the possible descriptors available for any uninitiated public, and therefore require a costly vocabulary-learning phase.
If we could entrust this characterization to neophytes, the number of participants of a sensorial analysis session would be significantly enlarged while reducing costs.
However, in that setting, each individual description is not related to a set of non-ambiguous descriptors anymore, but to a bag of terms expressed in natural language (NL).
Two issues are then related to smell characterization implementing this approach.
Hence, this work focuses first on the definition and evaluation of models that can be used to summarize a set of terms into unambiguous entity identifiers selected from a given ontology.
Among the several strategies explored in this contribution, we propose to compare hybrid approaches taking advantages of knowledge bases (symbolic representations) and word embeddings defined from large text corpora analysis.
The results we obtain highlight the relative benefits of mixing symbolic representations with classic word embeddings for this task.
We then formally define the problem of summarizing sets of concepts and we propose a model mimicking Human-like Intelligence for scoring alternative summaries with regard to a specific objective function.
Interestingly, this non-oriented approach for identifying the quality of odors appears to be an actual cognitive automation of the task today performed by expert operators in sensorial analysis.
It therefore opens interesting perspectives for developing scalable sensorial analyses based on large sets of evaluators when assessing, for instance, olfactory pollution around industrial sites.
In recent years, deep learning has enabled impressive achievements in Machine Translation.
Neural Machine Translation (NMT) relies on training deep neural networks with large number of parameters on vast amounts of parallel data to learn how to translate from one language to another.
One crucial factor to the success of NMT is the design of new powerful and efficient architectures.
For this purpose, we introduce Pervasive Attention, a model based on two-dimensional convolutions that jointly encode the source and target sequences with interactions that are pervasive throughout the network.
To improve the efficiency of NMT systems, we explore online machine translation where the source is read incrementally and the decoder is fed partial contexts so that the model can alternate between reading and writing.
We also address the resource-efficiency of encoder-decoder models and posit that going deeper in a neural network is not required for all instances.
We design depth-adaptive Transformer decoders that allow for anytime prediction and sample-adaptive halting mechanisms to favor low cost predictions for low complexity instances and save deeper predictions for complex scenarios.
The automatic processing of speech is an area that encompasses a large number of works: speaker recognition, named entities detection or transcription of the audio signal into words.
All this information can be exploited by automatic indexing techniques which will allow indexing of large document collections.
The work presented in this thesis are interested in the automatic indexing of speakers in french audio documents.
Specifically we try to identify the various contributions of a speaker and nominate them by their first and last name.
This process is known as named identification of the speaker.
The particularity of this work lies in the joint use of audio and its transcript to name the speakers of a document.
The first and last name of each speaker is extracted from the document itself (from its rich transcription more accurately), before being assigned to one of the speakers of the document.
We begin by describing the context and previous work on the speaker named identification process before submitting Milesin, the system developed during this thesis.
The contribution of this work lies firstly in the use of an automatic detector of named entities (LIA_NE) to extract the first name / last name of the transcript.
Afterwards, they rely on the theory of belief functions to perform the assignment to the speakers of the document and thus take into account the various conflicts that may arise.
Finally, an optimal assignment algorithm is proposed.
This system gives an error rate of between 12 and 20 % on reference transcripts (done manually) based on the corpus used.
We then present the advances and limitations highlighted by this work.
We propose an initial study of the impact of the use of fully automatic transcriptions on Milesin.
This is why many elderly people care projects: technical, academic and commercial have seen the light of day in recent years.
This thesis work wasc arried out under Cifre agreement, jointly between the company KRG Corporate and the BMBI laboratory (Biomechanics and Bioengineering) of the UTC (Université of Technologie of Compiègne).
Its purpose is to oﬀer a sensor for sound recognition and everyday activities, with the aim of expanding and improving the tele-assistance system already marketed by the company.
Several speech recognition or speaker recognition methods have already been proven in the field of sound recognition, including GMM (Modèle de mélange gaussien – Gaussian Mixture Model), SVM-GSL (Machine à vecteurs de support, GMM-super-vecteur à noyau linéaire – Support vector machine GMM Supervector Linear kernel) and HMM (Modèle de Markov caché – Hidden Markov Model).
In the same way, we proposed to use i-vectors for sound recognition.
Then we broadened our spectrum, and used Deep Learning, which currently gives very good results in classification across all domains.
The methods mentioned above were also tested under noisy and then real conditions.
At the beginning of this PhD, no treebank for Serbian was available.
However, manually annotated treebanks are an essential resource for developing (training and evaluating) statistical tools for syntactic analysis (parsers).
Efficient parsers, in turn, facilitate the annotation of large corpora, which can be used as a basis for research in theoretical linguistics.
In order to address this issue, we created a suite of NLP resources for Serbian.
Firstly, we created the ParCoTrain-Synt treebank, a 101 000 token corpus, complete with morphosyntactic annotation, lemmatisation and syntactic dependency annotation.
We also built the ParCoLex lexicon, containing 7 million entries for 157 000 different lemmas.
Using these two resources, we trained models for parsing, morphosyntactic tagging and lemmatisation.
All of the above resources are available at the following address: https: //github.com/aleksandra-miletic/serbian-nlp-resources.
We also used these resources in two experiments in Serbian linguistics, demonstrating that the ParCoTrain-Synt treebank is well suited to empirical studies based on quantitative data analysis.
Significant advances have been achieved in bilingual word-level alignment from comparable corpora, yet the challenge remains for phrase-level alignment.
Traditional methods to phrase alignment can only handle phrase of equal length, while word embedding based approaches learn phrase embeddings as individual vocabulary entries suffer from the data sparsity and cannot handle out of vocabulary phrases.
Since bilingual alignment is a vector comparison task, phrase representation plays a key role.
In this thesis, we study the approaches for unified phrase modeling and cross-lingual phrase alignment, ranging from co-occurrence models to most recent neural state-of-the-art approaches.
We review supervised and unsupervised frameworks for modeling cross-lingual phrase representations.
Two contributions are proposed in this work.
Our proposition improves significantly bilingual alignment of different length phrases.
Based on a video recording of conversational British English and within the framework of Multimodal Discourse Analysis, this study tests whether three different syntactic types of subordinate structures are evenly integrated to their environment.
Subordinate constructions have been described in syntax as dependent forms elaborating on primary elements of discourse.
Beyond showing that subordinate constructions are not evenly dependent on their environment depending on how speakers use the prosodic and kinetic modalities to express greater (in)dependency, our results in production as in perception suggest on the one hand that appositive clauses show more break than the other syntactic types, and on the other hand that the creation of a break mainly relies on prosodic cues.
This thesis describes the implementation of a speech understanding system on a microprocessor.
The system is designed to accept continuous speech from one speaker and to work within the context of a limited task situation and small vocabularies.
The system utilizes phonetic recognition at the phonetic level and an optimal one-pass dynamic programming algorithm at the lexical and syntactic levels.
The system has an interactive program for the definition of grammars for a given specific task language and a program of orthographic-phonetic translation that takes into account some phonological variations of words.
Graphs are ubiquitous in many fields of research ranging from sociology to biology.
A graph is a very simple mathematical structure that consists of a set of elements, called nodes, connected to each other by edges.
Graph clustering is a central problem in the analysis of graphs whose objective is to identify dense groups of nodes that are sparsely connected to the rest of the graph.
These groups of nodes, called clusters, are fundamental to an in-depth understanding of graph structures.
There is no universal definition of what a good cluster is, and different approaches might be best suited for different applications.
Whereas most of classic methods focus on finding node partitions, i.e. on coloring graph nodes so that each node has one and only one color, more elaborate approaches are often necessary to model the complex structure of real-life graphs and to address sophisticated applications.
In particular, in many cases, we must consider that a given node can belong to more than one cluster.
Besides, many real-world systems exhibit multi-scale structures and one much seek for hierarchies of clusters rather than flat clusterings.
Furthermore, graphs often evolve over time and are too massive to be handled in one batch so that one must be able to process stream of edges.
In this work, we study alternative approaches and design novel algorithms to tackle these different problems.
The novel methods that we propose to address these different problems are mostly inspired by variants of modularity, a classic measure that accesses the quality of a node partition, and by random walks, stochastic processes whose properties are closely related to the graph structure.
We provide analyses that give theoretical guarantees for the different proposed techniques, and endeavour to evaluate these algorithms on real-world datasets and use cases.
This thesis is devoted to the study of the preposition iz in modern Russian, through the constructions that contain it and form a complex semantic network.
While prepositions have always been a central part of linguistic research, iz has not been described in much detail in terms of its semantics and function, even though some works have been devoted to it.
Chapter II presents a detailed analysis of the uses of iz within a construction, considered as a semantic-syntactic unit.
This detailed analysis highlights the parameters that show similarities between the various uses of iz by establishing links between them and by organising them into a well-structured network of meanings.
This network of meanings forms the semantic profile of iz itself.
Chapter III provides a summary including the main ideas developed in the chapter analysing iz, and compares them through a contrastive study on the uses of the prepositions ot and s, which appear in similar contexts and, as a result, enter into competition with iz.
This contrastive analysis makes it possible to identify the specificities of the uses of each of the three prepositions and to determine the way in which they separate areas of use.
Elsewhere, particular consideration is given to the quantitative analysis of combinations of the three prepositions iz, ot and s with prefix verbs.
The results obtained clearly confirm the hypothesis of a correlation between the prepositions and the homonymous prefixes ot and ot-, s and s-, and the synonymous prefixes iz and vy-.
Lastly, the arguments developed throughout the work are used to explain erroneous use cases involving the preposition iz in works for learners of Russian (mainly French and English-speaking).
Thanks to this comparative dimension between languages with a different structure, the analysis carried out highlights the fact that certain constructions with iz (in particular, 'belonging', 'identification of an entity within a group of entities') have specific parameters that are not present in “equivalent” constructions in French and English.
The general conclusion of the thesis that concludes this study presents all of the results and data obtained, as does the Annex, which brings together the main quantitative data taken from research on prepositions studied mainly in the Russian National Corpus (ruscorpora.ru).
The results obtained can contribute to linguistic studies of prepositions and to lexicographical entries, as well as to research related to acquisition of Russian by non-Russian-speaking adults, and didactics of Russian as a foreign language for example.
In this thesis, we study information diffusion in online social networks.
Websites like Facebook or Twitter have indeed become information medias, on which users create and share a lot of data.
Most existing models of the information diffusion phenomenon relies on strong hypothesis about the structure and dynamics of diffusion.
In this document, we study the problem of diffusion prediction in the context where the social graph is unknown and only user actions are observed.
- We propose a learning algorithm for the independant cascades model that does not take time into account.
Experimental results show that this approach obtains better results than time-based learning schemes.
- We then propose several representations learning methods for this task of diffusion prediction.
This let us define more compact and faster models.
- Finally, we apply our representation learning approach to the source detection task, where it obtains much better results than graph-based approaches.
This computer science thesis presents a computational cognitive modeling work using eye movements of people faced to different information search tasks on textual material.
People should judge whether a piece of text is semantically related or not to a goal expressed by a few words.
More specifically, we analyzed eye movements during two information search tasks: reading a paragraph with the task of quickly deciding i) if it is related or not to a given goal and ii) whether it is better related to a given goal than another paragraph processed previously.
One model is proposed for each of these situations.
Our simulations are done at the level of eye fixations and saccades.
In particular, we predicted the time at which participants would decide to stop reading a paragraph because they have enough information to make their decision.
The models make predictions at the level of words that are likely to be fixated before a paragraph is abandoned.
We followed a statistical parametric approach in the construction of our models.
The models are based on a Bayesian classifier.
We proposed a two-variable linear threshold to account for the decision to stop reading a paragraph, based on the Rank of the fixation and i) the semantic similarity (Cos) between the paragraph and the goal and ii) the difference of semantic similarities (Gap) between each paragraph and the goal.
For both models, the performance results showed that we are able to replicate in average people's behavior faced to the information search tasks studied along the thesis.
The thesis includes two main parts: 1) designing and carrying out psychophysical experiments in order to acquire eye movement data and 2) developing and testing the computational cognitive models.
As multimedia sources have become massively available online, helping users to understand the large amount of information they generate has become a major issue.
One way to approach this is by summarizing multimedia content, thus generating abridged and informative versions of the original sources.
This PhD thesis addresses the subject of text and audio-based multimedia summarization in a multilingual context.
Text-based multimedia summarization uses transcripts to produce summaries that may be presented either as text or in their original format.
The transcription of multimedia sources can be done manually or automatically by an Automatic Speech Recognition(ASR) system.
The transcripts produced using either method differ from well formed written language given their source is mostly spoken language.
In addition, ASR transcripts lack syntactic information.
For example, capital letters and punctuation marks are unavailable, which means sentences are nonexistent.
Finally, we extend ARTEX, astate-of-the-art extractive text summarization method, to process documents in MSA by adapting preprocessing modules.
The resulting summaries can be presented as plain text or in their original multimedia format by aligning the selected SUs.
Concerning audio-based summarization, we introduce an extractive method which represents the informativeness of the source based on its audio features to select the segments that are most pertinent to the summary.
During the training phase, our method uses available transcripts of the audio documents to create an informativeness model which maps a set of audio features with a divergence value.
Subsequently, when summarizing new audio documents, transcripts are not needed anymore.
Results over amulti-evaluator scheme show that our approach provides understandable and informative summaries.
Evaluation measures is also a field which we deal with.
We also explore the possibility of measuring the quality of an automatic transcript based on its informativeness.
In addition, we study to what extent automatic summarization may compensate for the problems raised during the transcription phase.
Lastly, we study how text informativeness evaluation measures may be extended to passage interestingness evaluation.
With tremendous generation of data, we have data collected from different information sources having heterogeneous properties, thus it is important to consider these representations or views of the data.
This problem of machine learning is referred as multiview learning.
It has many applications for e.g. in medical imaging, we can represent human brain with different set of features for example MRI, t-fMRI, EEG, etc.
In this thesis, we focus on supervised multiview learning, where we see multiview learning as combination of different view-specific classifiers or views.
Therefore, according to our point of view, it is interesting to tackle multiview learning issue through PAC-Bayesian framework.
It is a tool derived from statistical learning theory studying models expressed as majority votes.
One of the advantages of PAC-Bayesian theory is that it allows to directly capture the trade-off between accuracy and diversity between voters, which is important for multiview learning.
The first contribution of this thesis is extending the classical PAC-Bayesian theory (with a single view) to multiview learning (with more than two views).
To do this, we considered a two-level hierarchy of distributions over the view-specific voters and the views.
Based on this strategy, we derived PAC-Bayesian generalization bounds (both probabilistic and expected risk bounds) for multiview learning.
It iteratively learns the weights over the views by optimizing the multiview C-Bound which controls the trade-off between the accuracy and the diversity between the views.
The second algorithm is based on late fusion approach where we combine the predictions of view-specific classifiers using the PAC-Bayesian algorithm CqBoost proposed by Roy et al.
Finally, we show that minimization of classification error for multiview weighted majority vote is equivalent to the minimization of Bregman divergences.
This allowed us to derive a parallel update optimization algorithm (referred as MωMvC2) to learn our multiview weighted majority vote.
Today we collect data from various sources both online and offline.
Usually this data is mostly referring to acts already committed and are usually analyzed a posteriori.
Given that early warning leads to early (and hopefully more effective) action, in this work we suggest to try and identify early warning signals of actions or activities that will happen in the future.
A weak signal is a past, or current, evidence with ambiguous interpretations that can be correlated to other, more important present, or future, events and tendencies.
Weak signals are unclear clues that can warn us for other emerging patterns.
The key problem is to detect such relevant signals hidden in the mass of data that is available.
Before being sure that the attention of human analysts is necessary, it is important to detect but also verify and validate these signals through a series of different methods.
After being identified and validated, a weak signal becomes an early sign that should be carefully monitored.
In this work, we want to establish methods of identifying weak signals inside massive datasets, like datasets originating from social media or other publicly available sources.
Algorithms based on statistical or machine learning or Artificial Intelligence methods combined with formal semantic descriptions of the events we are interested in, would allow us to identify early warning signals and turn them to indications of future acts.
Moreover, methods related to Named Entity Detection would allow us to bind these early warning signals to participating entities (e.g. persons, places, etc.) and use the entity descriptions to improve and extend the description of the possible events.
The work will be validated by applying its results to actual real-world data provided by PJGN, which is a partner in this work.
Identifying references to events and resolving the information around those events is an important problem for PJGN and the work will be directly applicable.
My thesis is part of Arabic sentiment analysis.
Our study focuses on the use of a neural approach to improve polarity detection, using embeddings.
These embeddings have revealed fundamental in various natural languages processing tasks (NLP).
Our contribution in this thesis concerns several axis.
First, we begin with a preliminary study of the various existing pre-trained word embeddings resources in arabic.
These embeddings consider words as space separated units in order to capture semantic and syntactic similarities in the embedding space.
We propose arabic specific embeddings that take into account agglutination and morphological richness of Arabic.
These specific embeddings have been used, alone and in combined way, as input to neural networks providing an improvement in terms of classification performance.
Finally, we evaluate embeddings with intrinsic and extrinsic methods specific to sentiment analysis task.
For intrinsic embeddings evaluation, we propose a new protocol introducing the notion of sentiment stability in the embeddings space.
We propose also a qualitaive extrinsic analysis of our embeddings by using visualisation methods.
The combination of semantic processing of knowledge and modelling steps of reasoning employed in the clinical field offers exciting and necessary opportunities to develop ontologies relevant to the practice of medicine.
In this context, multiple medical databases such as MEDLINE, PubMed are valuable tools but not sufficient because they cannot acquire the usable knowledge easily in a clinical approach.
Indeed, abundance of inappropriate quotations constitutes the noise and requires a tedious sort incompatible with the practice of medicine.
In an iterative process, the objective is to build an approach as automated as possible, the reusable medical knowledge bases is founded on an ontology of the concerned fields. In this thesis, the author will develop a series of tools for knowledge acquisition combining the linguistic analysis operators and clinical modelling based on the implemented knowledge typology and an implementation of different forms of employed reasoning.
Knowledge is not limited to the information from data, but also and especially on the cognitive operators of reasoning for making them operational in the context relevant to the practitioner.
A multi-agent system enables the integration and cooperation of the various modules used in the development of a medical ontology. The data sources are from medical databases such as MEDLINE, the citations retrieved by PubMed, and the concepts and vocabulary from the Unified Medical Language System (UMLS).
Regarding the scope of produced knowledge bases, the research concerns the entire clinical process: diagnosis, prognosis, treatment, and therapeutic monitoring of various diseases in a given medical field.
On the whole, we worked on logical aspects related to cognitive operators of used reasoning, and we organized the cooperation and integration of exploited knowledge during the various stages of the clinical process (diagnosis, prognosis, treatment, therapeutic monitoring).
This integration is based on a SMAAD: multi-agent system for decision support.
The aim of this thesis is to make an Embodied Conversational Agent (ECA) sincere in order to, on one hand, improve its believability from the human's point of view, and on the other hand make it acceptable in an affective relationship between an artificial companion and a human.
The mental states carried by the MCA are formalised in logics: our will to represent mental states stemming from complex forms of reasoning (based on counterfactual reasoning or on the agent's norms and goals) that are mainly expressed via language (Oatley 1987) led us to design the BIGRE model (Beliefs, Ideals, Goals, Responsibility, Emotions).
This model, based on a BDI-like logic (Belief, Desire, Intention), allows us to also represent some particular emotions that we call complex emotions, such as rejoicing, gratitude or regret.
We implemented the MCL in the ECA Greta, which enabled an evaluation of this language in terms of believability and sincerity perceived by the human.
The second part of this work is about the ECA's reasoning capabilities: in order to allow the agent to reason in the dialogue, that is to update its mental states and its emotions and select its communicative intention, we designed a reasoning engine.
This reasoning engine is based on the BDI behaviour cycle of Perception - Decision - Action and on the operators from the BIGRE model, thus enabling the manipulation of mental states resulting from complex reasoning (including complex emotions).
The MCA in our language are part of our reasoning engine, and are used to achieve the ECA's communicative intention: for example if the ECA intends to express its gratitude, it builds a plan to achieve this intention, that consists of the MCA thank or congratulate, depending on the intensity of the emotion.
One type of communicative intention, triggered by discourse obligations rules, participates in the local regulation of dialogue.
Further, since the ECA is emotional, its sincerity brings it to express all its emotions.
The generic character of this reasoning engine allowed us to implement it in the ECA Greta (where it is linked with the MCL) as well as in the agent MARC.
The multimodal expression of MCA by the agent MARC was made possible by integrating Scherer's checks in the reasoning engine that we adapted to the context of dialogue.
An evaluation of the reasoning engine with the agent MARC shows that the mental states deduced by the engine are appropriate to the situation, and that their expression (the expression of the agent's sincerity) is also appropriate.
Artificial Intelligence has penetrated into every aspect of our lives in this era of Big Data.
It has brought revolutionary changes upon various sectors including e-commerce and finance.
In this thesis, we present four applications of AI which improve existing goods and services, enables automation and greatly increase the efficiency of many tasks in both domains.
Firstly, we improve the product search service offered by most e-commerce sites by using a novel term weighting scheme to better assess term importance within a search query.
Then we build a predictive model on daily sales using a time series forecasting approach and leverage the predicted results to rank product search results in order to maximize the revenue of a company.
Next, we present the product categorization challenge we hold online and analyze the winning solutions, consisting of the state-of-the-art classification algorithms, on our real dataset.
We perform an extensive study on every single stocks of S&amp;P 500 index using four state-of-the-art classification algorithms and report very promising results
Electrical products and networks are subject to a regulatory and normative environment divided according to geography, voltage level and sector of application.
The growing importance of new technologies associated with the energy transition (renewable energy, electric vehicle, energy storage and optimization) is leading to an acceleration of the production of normative and regulatory benchmarks.
In this context, the design of new products, systems and installations requires constant monitoring of the reference texts to be taken into account, the evolution of the vocabulary and concepts used and obviously, the constraints that must be respected.
The objective of the proposed thesis is to extract from documentary collections the design rules and constraints to be applied by translating them into a formal actionable language (i.e. understandable and usable by experts in the field) for the design assistance.
This includes the formalization of domain knowledge, the analysis and understanding of texts, the construction of a synthesis of extracted rules, as well as the detection of conflicts and duplicates
With the ever-growing mass of published text, natural language understanding stands as one of the most sought-after goal of artificial intelligence.
In natural language, not every fact expressed in the text is necessarily explicit: human readers naturally infer what is missing through various intuitive linguistic skills, common sense or domain-specific knowledge, and life experiences.
Natural Language Processing (NLP) systems do not have these initial capabilities.
Unable to draw inferences to fill the gaps in the text, they cannot truly understand it.
This dissertation focuses on this problem and presents our work on the automatic resolution of textual inferences in the context of machine reading.
A textual inference is simply defined as a relation between two fragments of text: a human reading the first can reasonably infer that the second is true.
A lot of different NLP tasks more or less directly evaluate systems on their ability to recognize textual inference.
Among this multiplicity of evaluation frameworks, inferences themselves are not one and the same and also present a wide variety of different types.
We reflect on inferences for NLP from a theoretical standpoint and present two contributions addressing these levels of diversity: an abstract contextualized inference task encompassing most NLP inference-related tasks, and a novel hierchical taxonomy of textual inferences based on their difficulty.
Automatically recognizing textual inference currently almost always involves a machine learning model, trained to use various linguistic features on a labeled dataset of samples of textual inference.
However, specific data on complex inference phenomena is not currently abundant enough that systems can directly learn world knowledge and commonsense reasoning.
Instead, systems focus on learning how to use the syntactic structure of sentences to align the words of two semantically related sentences.
To extend what systems know of the world, they include external background knowledge, often improving their results.
But this addition is often made on top of other features, and rarely well integrated to sentence structure.
The main contributions of our thesis address the previous concern, with the aim of solving complex natural language understanding tasks.
With the hypothesis that a simpler lexicon should make easier to compare the sense of two sentences, we present a passage retrieval method using structured lexical expansion backed up by a simplifying dictionary.
This simplification hypothesis is tested again in a contribution on textual entailment: syntactical paraphrases are extracted from the same dictionary and repeatedly applied on the first sentence to turn it into the second.
We then present a machine learning kernel-based method recognizing sentence rewritings, with a notion of types able to encode lexical-semantic knowledge.
This approach is effective on three tasks: paraphrase identification, textual entailment and question answering.
Reading comprehension tests are used for evaluation: these multiple-choice questions on short text constitute the most practical way to assess textual inference within a complete context.
Our system is founded on a efficient tree edit algorithm, and the features extracted from edit sequences are used to build two classifiers for the validation and invalidation of answer candidates.
This approach reaches second place at the "Entrance Exams" CLEF 2015 challenge.
We propose an approach to detect topics, overlapping communities of interest, expertise, trends and activities in user-generated content sites and in particular in question-answering forums such as StackOverFlow.
We first describe QASM (Question &amp; Answer Social Media), a system based on social network analysis to manage the two main resources in question-answering sites: users and contents.
We also introduce the QASM vocabulary used to formalize both the level of interest and the expertise of users on topics.
We then propose an efficient approach to detect communities of interest.
It relies on another method to enrich questions with a more general tag when needed.
We compared three detection methods on a dataset extracted from the popular Q&amp;A site StackOverflow.
Our method based on topic modeling and user membership assignment is shown to be much simpler and faster while preserving the quality of the detection.
We then propose an additional method to automatically generate a label for a detected topic by analyzing the meaning and links of its bag of words.
We conduct a user study to compare different algorithms to choose the label.
Finally we extend our probabilistic graphical model to jointly model topics, expertise, activities and trends.
We performed experiments with realworld data to confirm the effectiveness of our joint model, studying the users' behaviors and topics dynamics.
We are surrounded by decisions to take, what book to read next?
What film to watch this night and in the week-end?
As the number of items became tremendous the use of recommendation systems became essential in daily life.
At the same time social network become indispensable in people's daily lives; people from different countries and age groups use them on a daily basis.
While people are spending time on social networks, they are leaving valuable information about them attracting researchers'attention.
Recommendation is one domain that has been affected by the social networks widespread; the result is the social recommenders'studies.
However, in the literature we've found that most of the social recommenders were evaluated over Epinions, flixter and other type of domains based recommender social networks, which are composed of (users, items, ratings and relations).
Smart speakers offer the possibility of interacting with smart home systems, and make it possible to issue a range of requests about various subjects.
They represent the first ambient voice interfaces that are frequently available in home environments.
Very often they are only capable of inferring voice commands of a simple syntax in short utterances in the realm of smart homes that promote home care for senior adults.
They support them during everyday situations by improving their quality of life, and also providing assistance in situations of distress.
As a result, these research projects frequently concentrate on human activity detection, resulting in a lack of attention for the communicative aspects in a smart home design.
However the availability of these corpora are crucial for developing interactive communication systems between the smart home and its inhabitants.
Such corpora at one's disposal could also contribute to the development of a generation of smart speakers capable of extracting more complex voice commands.
As a consequence, part of our work consisted in developing a corpus generator, producing home automation domain specific voice commands, automatically annotated with intent and concept labels.
The extraction of intents and concepts from these commands, by a Spoken Language Understanding (SLU) system is necessary to provide the decision-making module with the information, necessary for their execution.
As several studies have shown, the interaction between ASR and NLU in a sequential SLU approach accumulates errors.
To achieve this goal, we first develop a sequential SLU approach as our baseline approach, in which a classic ASR method generates transcriptions that are passed to the NLU module, before continuing with the development of an End-to-end SLU module.
These two SLU systems were evaluated on a corpus recorded in the home automation domain.
We investigate whether the prosodic information that the end-to-end SLU system has access to, contributes to SLU performance.
We position the two approaches also by comparing their robustness, facing speech with more semantic and syntactic variation.
The context of this thesis is the ANR VocADom project.
Because of its key societal, economic and cultural stakes, Artificial Intelligence (AI) is a hot topic.
One of its main goal, is to develop systems that facilitates the daily life of humans, with applications such as household robots, industrial robots, autonomous vehicle and much more.
The rise of AI is highly due to the emergence of tools based on deep neural-networks which make it possible to simultaneously learn, the representation of the data (which were traditionally hand-crafted), and the task to solve (traditionally learned with statistical models).
This resulted from the conjunction of theoretical advances, the growing computational capacity as well as the availability of many annotated data.
Specialization: learn representations from few specific tasks with the goal to be able to carry out very specific tasks (specialized in a certain field) with a very good level of performance; (ii) Universality: learn representations from several general tasks with the goal to perform as many tasks as possible in different contexts.
Regarding the quantification of universality, we proposed to evaluate universalizing methods in a Transferlearning scheme.
When it comes to the construction of multilingual lexico-semantic resources, the first thing that comes to mind is that the resources we want to align, should share the same data model and format (representational interoperability).
However, with the emergence of standards such as LMF and their implementation and widespread use for the production of resources as lexical linked data (Ontolex), representational interoperability has ceased to be a major challenge for the production of large-scale multilingual resources.
However, as far as the interoperability of sense-level multi-lingual alignments is concerned, a major challenge is the choice of a suitable interlingual pivot.
The use of acception-based interlingual representations, a solution proposed over 20 years ago, could be viable.
However, the manual construction of such language-independent pivot representations is very difficult due to the lack of expert speaking enough languages fluently and algorithms for their automatic constructions have never since materialized, mainly because of the lack of a formal axiomatic characterization that ensures the pre-servation of their correctness properties.
In this thesis, we address this issue by first formalizing acception-based interlingual pivot architectures through a set of axiomatic constraints and rules that guarantee their correctness.
Then, we propose algorithms for the initial construction and the update (dynamic interoperability) of interlingual acception-based multilingual resources by exploiting the combinatorial properties of pairwise bilingual translation graphs.
Secondly, we study the practical considerations of applying our construction algorithms on a tangible resource, DBNary, a resource periodically extracted from Wiktionary in many languages in lexical linked data.
This research examines the divergences and convergences across the histories of three national galleries, in England, Canada and the United States, providing evidence of a common model that emanates particularly from two types of institutions that appeared in England during the 17th century: public museums founded on scientific and populist principles, and private art galleries anchored in elitist traditions.
The continued struggle to truly integrate technologies in the documentation of art betrays the difficult heritage between these two opposing models.
Throughout the unique historical trajectories of these institutions, there was little proof that optical or digital technologies had had important repercussions on the methodologies or the philosophies of the documentation of works of art.
On the contrary, it was observed that documentation, even in digital form, continued to rely on minimal standards of data gathering, restricted groups of persons trained to collect data, and limited access to any data captured.
This research reinforces the need to redefine museum documentation in order to rethink its strategies and guiding philosophies, to enable new research into museum collections, and to enlarge the integration of digital technologies into the process.
This study is a dialectometric analysis of Berber dialects of Kabylia.
This work includes a sample of 168 Kabyle dialects spread across the Kabyle territory.
The analyzed corpus includes 130 entries (lexemes and phrases) collected in each of the varieties considered.
We opted for the Levenshtein method to calculate the distance between the variants.
We chose the algorithm of Ward's Method for grouping varieties.
We tested three methods to calculate the distance between the sounds: the binary method, the Euclidean distance and the Manhattan distance.
The analysis of the results allowed us to show the dialect continuum in Kabylia and classify Kabyle dialects into five mains areas.
The scientific activity of this thesis consists of: Understanding the nature of anaphora and coreference relationships in messages from mediated electronic communication, with priority given to emails.
This will lead to methods that can weave semantic links between different sentences and messages.
Analysing more precisely the event anaphors (Danlos, 2004) and the nature of relations between events.
This represents a major scientific lock for understanding natural language with a direct impact on the development of Emvista's Prevyo solution.
The scientific advancement and the appropriation of the resulting technologies will enable Emvista to position itself as an important player in the understanding of natural language
Our study concerns the language of tourism from a lexicographical perspective.
This corpus is composed by about 10.000 texts in three languages (French, Italian and English), aligned using “Alinea”.
Starting from terminological extraction, we analysed some collocations at the aim to create a trilingual and tri-directional glossary.
We chose this subject according to the increasing importance taken from tourism economy in the world.
Our study fields are thematic terminology, corpus linguistics and automatic language treatment.
First of all, we introduced to corpus linguistics presenting the different categories of corpus and pointing out our attention on two main notions: representativeness and context.
Therefore, we explained the link between Language for Special Purposes and tourism discourse as a Specialized Discourse.
In the second chapter, we showed the trilingual thematic corpus we created during our researches.
We described the main steps to create a corpus: collection of texts, cleaning and annotation.
In this chapter, we gave a particular attention to the presentation of “Alinea”.
Finally, the third chapter is a study of frequent collocations with the term “town” (ville).The annexes present the glossary as well as the methodological principals we followed in the redaction.
Differences between training and testing conditions may significantly degrade recognition accuracy in automatic speech recognition (ASR) systems.
Adaptation is an efficient way to reduce the mismatch between models and data from a particular speaker or channel.
There are two dominant types of acoustic models (AMs) used in ASR: Gaussian mixture models (GMMs) and deep neural networks (DNNs).
The GMM hidden Markov model (GMM-HMM) approach has been one of the most common technique in ASR systems for many decades.
Speaker adaptation is very effective for these AMs and various adaptation techniques have been developed for them.
The main purpose of this thesis is to develop a method for efficient transfer of adaptation algorithms from the GMM framework to DNN models.
The proposed technique provides a general framework for transferring adaptation algorithms, developed for GMMs, to DNN adaptation.
It is explored for various state-of-the-art ASR systems and is shown to be effective in comparison with other speaker adaptation techniques and complementary to them.
Associating word senses with temporal orientation to grasp the temporal information in language is relatively straightforward task for humans by using world knowledge.
With this in mind, a lexical temporal knowledge-base associating word senses automatically with their underlying temporal orientation would be crucial for the computational tasks aiming at interpretation of language of time in text.
In this research, we introduce a temporal ontology namely TempoWordNet where all the synsets of WordNet are augmented with their intrinsic temporal dimensions: atemporal, past, present, and future.
The resource is evaluated both intrinsically and extrinsically, the underlying idea being that a reliable resource must evidence high quality time-tagging as well as improved performance for some external tasks.
Both the evaluations results confirm the quality and usefulness of the resource.
To complement our research we also experiment how a search application can benefit from this resource.
Feedback from TempoWordNet users advocate for more reliable resource.
At the end, we propose a strategy that shows steady improvements over the previous versions of TempoWordNet.
After an overview of what underpins the notion of meaning, this thesis intends to identify a number of linguistic features that could serve as an abstract characterisation informing the uses of the English verb want.
It is claimed that a vestige of its original use as a verb expressing lack has been kept in its modern uses as a verb expressing desire in that it marks a prime operation of localisation by which the grammatical subject connects to an object or a property p that is not immediately available to him in the speaking situation.
It is shown that this relation relies on a subjectification process which culminates in a strong tendency towards the modalisation of the want sentence resulting in both epistemic and deontic effects.
It is argued that this organisation can be described as a notional domain centring on the missing object p, acting as a starting point for a second localisation operation whose aim is to locate a relation to palliate the lack of p.
It is thus possible to account for the difference between desire and volition as degrees in the determinations that can be constructed from the missing object.
This thesis then correlates the syntactic constructions in the internal argument of want with those types of determination.
These knowledge resources, described according to different representation models and for different contexts of use, raise the problem of complexity of their interoperability, especially for actual public health problematics such as personalized medicine, translational medicine and the secondary use of medical data.
Indeed, these knowledge resources may represent the same notion in different ways or represent different but complementary notions.
For being able to use knowledge resources jointly, we studied three processes that can overcome semantic conflicts (difficulties encountered when relating distinct knowledge resources): the alignment, the integration and the semantic enrichment of the integration.
The integration aims not only to find mappings but also to organize all knowledge resources' entities into a unique and coherent structure.
Finally, the semantic enrichment of integration consists in finding all the required mapping relations between entities of distinct knowledge resources (equivalence, subsumption, transversal and, failing that, disjunction relations).In this frame, we firstly realized the alignment of laboratory tests terminologies:
Then, we suppressed erroneous mappings (confounding conflicts) using the structure of LOINC.Secondly, we integrated RxNorm to SNOMED CT.
We constructed formal definitions for each entity in RxNorm by using their definitional features (active ingredient, strength, dose form, etc.) according to the design patterns proposed by SNOMED CT.
Our process resolved some cases of naming conflicts but was confronted to confounding and scaling conflicts, which highlights the need for improving RxNorm and SNOMED CT.Finally, we performed a semantically enriched integration of ICD-10 and ICD-O3 using SNOMED CT as support.
As ICD-10 describes diagnoses and ICD-O3 describes this notion according to two different axes (i.e., histological lesions and anatomical structures), we used the SNOMED CT structure to identify transversal relations between their entities (resolution of open conflicts).
During the process, the structure of the SNOMED CT was also used to suppress erroneous mappings (naming and confusion conflicts) and disambiguate multiple mappings (scale conflicts).
Recent advances in artificial intelligence have seen limited adoption in systematic reviews,and much of the systematic review process remains manual, time-consuming, and expensive.
Authors conducting systematic reviews face issues throughout the systematic review process.
It is difficult and time-consuming to search and retrieve,collect data, write manuscripts, and perform statistical analyses.
Screening automation has been suggested as a way to reduce the workload, but uptake has been limited due to a number of issues,including licensing, steep learning curves, lack of support, and mismatches to workflow.
There is a need to better a lign current methods to the need of the systematic review community.
Diagnostic test accuracy studies are seldom indexed in an easily retrievable way, and suffer from variable terminology and missing or inconsistently applied database labels.
Methodological search queries to identify diagnostic studies therefore tend to have low accuracy, and are discouraged for use in systematic reviews.
Consequently, there is a particular need for alternative methods to reduce the workload in systematic reviews of diagnostic test accuracy.
In this thesis we have explored the hypothesis that automation methods can offer an efficient way tomake the systematic review process quicker and less expensive, provided we can identify and overcomebarriers to their adoption.
Automated methods have the opportunity to make the process cheaper as well as more transparent, accountable, and reproducible.
This study concerns a comparative analysis of the syntactic units (syntagms) and the fundamental constructions of French and Persian, with regards to Controlled Languages and problematic and ambiguous cases for translation.
After a historical survey of these languages and a brief presentation of the writing and phonetic systems of Persian, the ward classes (parts of speech) and their traditional and modern classifications are compared.
The structures of determinant, nominal, adjectival, prepositional, adverbial and verbal syntagms and the nature of their component, as well as the fundamental constructions of the basic sentence in thesetwo languages are then analysed.
During the study, as a result of translation tests carried out by Persian students, some problematic cases for translation have been recognized and analysed for a potential French-Persian controlled language.
In the final synthesis, the syntagmatic structures and some instructions for developing a controlled language relating French and Persian languages have been assembled.
The aviation market is facing nowadays a fast growth of innovative airborne systems.
Drone cargo, drone taxi, airships, stratospheric balloons, to cite a few, could be part of the next generation of air transportation.
In the same time, Small and Medium-sized Enterprises (SMEs) are becoming more and more involved in designing and developing new forms of air transportation, transitioning from the traditional role of supplier to those of system designer and integrator.
This situation changes drastically the scope of SMEs' responsibility.
As integrators they become responsible for certification of the components and the manufacturing process, an area in which they have little experience.
Certification mandates very specific knowledge, regarding the regulations, norms and standards. Certification is a mandatory process and a critical activity for the enterprises in the aerospace industry.
It constitutes a major challenge for SMEs who have to take on this certification responsibility with only limited resources.
In this thesis, two major needs are identified: methodological support is not easily available for SMEs; and certification requirements are not easily comprehensive and adaptable to each situation.
We examine alternate paths, reducing the complexity and bringing one step closer to solving the problem for the innovative SMEs.
The objective is to provide support so that they can be more efficient to comprehend and integrate rules, legislations and guidelines to their internal processes in a simpler way.
This thesis proposes then a methodological approach to support such organisation.
Developed in close cooperation with a French SME in this situation, the approach is composed of a set of models (metamodel, structural, and behavioural models) covered by a certification governance mechanism.
This trend is reinforced by the Cloud based economy that allows sharing of costs and resources.
However, the lack of trust in such cloud environments, that involve higher security requirements, is often seen as a braking force to the development of such services.
The objective of this thesis is to study the concepts of service orchestration and trust in the context of the Cloud.
It proposes an approach which supports a trust model in order to allow the orchestration of trusted business process components on the cloud.
The contribution is threefold and consists in a method, a model and a framework.
The method categorizes techniques to transform an existing business process into a risk-aware process model that takes into account security risks related to cloud environments.
The model formalizes the relations and the responsibilities between the different actors of the cloud.
This allows to identify the different information required to assess and quantify security risks in cloud environments.
The framework is a comprehensive approach that decomposes a business process into fragments that can automatically be deployed on multiple clouds.
The framework also integrates a selection algorithm that combines security information with other quality of service criteria to generate an optimized configuration.
Finally, the work is implemented in order to validate the approach.
The framework is implemented in a tool.
The security assessment model is also applied over an access control model.
The last part presents the results of the implementation of our work on a real world use case.
The context of this PhD thesis is the mining of biomedical texts.
The main purpose is the extraction of interactions between food and drugs.
Indeed, food may have interactions with drugs, and those can lead to harmful consequences on the patient's health and well-being.
But, contrary to drug-drug interactions, food-drug interactions are less known and studied.
Hence, it is necessary to propose text mining methods for the analysis of scientific literature and for the extraction of information related to intractions between food and drugs.
Informations available in existing terminologies and linked data should be used.
Our thesis has for objective the processing of the polysemic adjective applicable to the domain of Natural Language Processing (NLP) and particularly to automatic translation (French-Korean).
The various meaning of the lexicon used in the stock market generates several different semantic interpretations and therefore several translations.
To resolve this problem in NLP, we propose a disambiguisation model making use of co-textual information and more particularly the noun.
The latter forms a noun clause with the adjective thus allowing one to remove the semantic ambiguity of the adjective and determine its correct meaning.
The co-textual hints that we have used are the following: the syntactic order of the adjective, the property of the noun and the semantic class to which the noun belongs.
Our work consists in attributing to each polysemic adjective selected the co-textual information mentioned above so that the machine can find its correct meaning.
The methodology can be generalized to other parts of speech, we have already applied it to the verb and its co-text, the noun
The strong emergence of smartphones on human daily life as well as the high broadband access supplied by operators have triggered pervasive demand on video streaming mobile services, requiring the exploration of novel approaches on video content delivery.
To afford video streaming services at sustainable quality, the idea of adjusting the streaming to the time-varying users' contexts has been actively investigated during the recent years.
Today streaming solutions mostly rely on the user's contextual information such as his link capacity or his available bandwidth to provide an acceptable final QoE.
Such contextual information can be easily acquired thanks to the existence of wireless sensors and dedicated smart applications on today mobile devices.
Various studies on users' mobility patterns also showed that people daily routes exhibit a high degree of spatial and temporal regularity, especially on public transportation or on road ways to/from frequently visited places.
Coupled with radio maps, these mobility patterns can give high accuracy on context predictability along users' trips.
In this thesis, we analyse the impact of adapting video streaming to the user's context on the final QoE.
We start by proposing CAMS (Context Aware Mode Switching),a context-aware resource allocation mechanism, for real (i.e, non adaptive) video streaming delivery to reduce the number of video stalling.
CAMS is designed to be applied in a particular network topology under a particular mobility of users.
Then,we explore the impact of knowing the future throughput variations on video quality adaptation and delivery cost in adaptive video streaming.
We propose NEWCAST (a Nticipating qoE With threshold scheme And a Scending biTrate levels) as a proactive algorithm for cost adjustment and quality adaptation under the assumption of a perfect throughput estimation.
We then extend the study to the case where throughput prediction errors may exist and propose a bench of adaptive algorithms inspired from NEWCAST.
To explore the feasibility of implementing these algorithms in real world streaming, we conduct some experiments with the DASH-If Reference player in an emulated environment.
Finally, we explore the impact of knowing the future throughput variations when exploited with machine learning on the global QoE enhancement in adaptive video streaming.
We propose a closed-loop framework based on users' feedbacks to progressively learn their QoE profiles and to fittingly optimize video deliveries.
This framework is in particular suited for heterogenous populations where the QoE profiles of users are quite different and unknown in advance.
This thesis offers a conventional model of the structure of a human-machine collaborative task to design an interactive system helping a human performing complex tasks.
More specifically, we focus on tasks whose resolution is highly opportunistic and can not be planned.
We introduce a task representation model based on dialogue games, dialogue patterns making it possible to describe the interaction structure with sequences of conventionally acceptable dialogue acts.
We organize these dialogue patterns in states, structures describing the expected behaviors from each interlocutor during the different sub-tasks of the collaborative task.
These states group dialogue patterns together in a coherent set in regard to the sub-task they are associated to and enrich them with locally relevant rules.
They make it possible to describe the effects of the execution of a dialogue pattern by the interlocutors in a given context of the task.
The states are used by the system to link an utterance of the human to the current state of the dialogue and of the task and to take the initiative of behaviors relevant with the interaction and helpful for the task.
The decisions taken by the system are based on the concept of maturity, a value associated to each state representing the system's ability to take initiatives in this state.
The decision model of the system is designed to be resilient and give as much freedom as possible to the user.
This model is implemented in CoCoA, a system that collaborates with a human to assist him performing a complex task.
From the study of a human-human dialogue corpus on a medical collaborative information retrieval task, we use our model to describe the task.
This use case is implemented with CoCoA and an evaluation is performed by simulating a human user with an information need and varying its actions according to behaviors identified in the corpus.
This thesis proposes a morphosyntactic, prosodic and conversational analysis of discourse markers in presidential debates and televised political talkshows in the United States of America.
Discourse markers, in correlation with gestures and intonation, contribute significantly to the construction of the verbal exchange between the various participants in mediated political discourse.
For the analysis, the approach proposed by Haselow (2017) was adopted because it considers all the other approaches mentioned above and takes into account cognitive theories.
Concerning the organisation of interaction during presidential debates or talkshows, discourse markers, accompanied by gestures and intonation, play an important role in the management of turn takings and in the sequential organization of actions.
On the enunciative level, in correlation with manual gestures and gaze, they participate in expressing emphasis, reformulation and opposition.
They also help candidates to draw the attention of their interlocutors in order to introduce a justification, a change of point of view, a reported speech, or with the aim of interrupting them.
Discourse markers in presidential debates, also allow speakers to anticipate or consider the point of view of their interlocutors (passive or active) by positioning themselves in a logic of acceptance or opposition with them.
Video interpretation and understanding is one of the long-term research goals in computer vision.
However, in order to deploy visual recognition systems on large-scale in practice it becomes important to address the scalability of the techniques.
The main goal is this thesis is to develop scalable methods for video content analysis (eg for ranking, or classification).
Restoring natural speech in paralyzed and aphasic people could be achieved using a brain-computer interface controlling a speech synthesizer in real-time.
The aim of this thesis was thus to develop three main steps toward such proof of concept.
First, a prerequisite was to develop a speech synthesizer producing intelligible speech in real-time with a reasonable number of control parameters.
We thus developed a speech synthesizer that produced intelligible speech from articulatory data.
This was achieved by first recording a large dataset of synchronous articulatory and acoustic data in a single speaker.
Then, we used machine learning techniques, especially deep neural networks, to build a model able to convert articulatory data into speech.
This synthesizer was built to run in real time.
Finally, as a first step toward future brain control of this synthesizer, we tested that it could be controlled in real-time by several speakers to produce intelligible speech from articulatory movements in a closed-loop paradigm.
Second, we investigated the feasibility of decoding speech and articulatory features from neural activity essentially recorded in the speech motor cortex.
We built a tool that allowed to localize active cortical speech areas online during awake brain surgery at the Grenoble Hospital and tested this system in two patients with brain cancer.
Results show that the motor cortex exhibits specific activity during speech production in the beta and gamma bands, which are also present during speech imagination.
The recorded data could be successfully analyzed to decode speech intention, voicing activity and the trajectories of the main articulators of the vocal tract above chance.
Finally, we addressed ethical issues that arise with the development and use of brain-computer interfaces.
Born-analog documents contain enormous knowledge which is valuable to our society.
For the purpose of preservation and easy accessibility, several digitisation projects have converted these documents into digital texts by using optical character recognition (OCR) software.
A notable limitation is the fact that OCRed books are often split into pages with paragraphs, lines, and words.
Thus, it is not convenient for users to navigate or search information inside books.
Another constraint is that the accuracy of modern OCR engines on historical documents substantially decreases.
Erroneous OCR output considerably impacts on the performance of search engines and natural language processing systems.
This thesis facilitates access to historical digitised documents by addressing such problems.
In order to facilitate access to historical documents, several approaches are proposed within this thesis, aiming to reconstruct the logical book structures and to improve the quality of digitised text.
Experimental results show that our approach outperforms the state-of-the-art for both evaluation metrics.
The major contribution of this thesis is to provide methodologies to reduce OCR errors.
Common and different features between OCR errors and human misspellings are clarified for better designing post-OCR processing.
Normally, a post-processing system detects and corrects remaining errors.
However, it is reasonable to treat them separately in some applications which allow to filter out, flag, or selectively reprocess such data.
Results reveal that the performance of our proposals is comparable to several strong baselines on English datasets of the first two rounds of the competition on post-OCR text correction organised in the International Conference on Document Analysis and Recognition in 2017 and 2019.
Social Media has changed the way we communicate between individuals, within organizations and communities.
The availability of these social data opens new opportunities to understand and influence the user behavior.
Therefore, Social Media Mining is experiencing a growing interest in various scientific and economic circles.
In this thesis, we are specifically interested in the users of these networks whom we try to characterize in two ways: (i) their expertise and their reputations and (ii) the sentiments they express.
Conventionally, social data is often mined according to its network structure.
However, the textual content of the exchanged messages may reveal additional knowledge that can not be known through the analysis of the structure.
Until recently, the majority of work done for the analysis of the textual content was proposed for English.
The originality of this thesis is to develop methods and resources based on the textual content of the messages for French Social Media Mining.
In the first axis, we initially suggest to predict the user expertise.
For this, we used forums that recruit health experts to learn classification models that serve to identify messages posted by experts in any other health forum.
We demonstrate that models learned on appropriate forums can be used effectively on other forums.
Then, in a second step, we focus on the user reputation in these forums.
The idea is to seek expressions of trust and distrust expressed in the textual content of the exchanged messages, to search the recipients of these messages and use this information to deduce users'reputation.
We propose a new reputation measure that weighs the score of each response by the reputation of its author.
Automatic and manual evaluations have demonstrated the effectiveness of the proposed approach.
In the second axis, we focus on the extraction of sentiments (emotions and polarity).
For this, we started by building a French lexicon of sentiments and emotions that we call FEEL (French Expanded Emotions Lexicon).
This lexicon is built semi-automatically by translating and expanding its English counterpart NRC EmoLex.
We then compare FEEL with existing French lexicons from literature on reference benchmarks.
The results show that FEEL improves the classification of French texts according to their polarities and emotions.
Finally, we propose to evaluate different features, methods and resources for the classification of sentiments in French.
The conducted experiments have identified useful features and methods in the classification of sentiments for different types of texts.
The learned systems have been particularly efficient on reference benchmarks.
Generally, this work opens promising perspectives on various analytical tasks of Social Media Mining including: (i) combining multiple sources in mining Social Media users; (ii) multi-modal Social Media Mining using not just text but also image, videos, location, etc. and (iii) multilingual sentiment analysis.
Kernel methods are known to be effective to analyse complex objects by implicitly embedding them into some feature space.
This work proposes a pre-image estimation method for time series kernel analytics that consists of two steps.
In the first step, a time warp function, driven by distance constraints in the feature space, is defined to embed time series in a metric space where analytics can be performed conveniently.
In the second step, the time series pre-image estimation is cast as learning a linear (or a nonlinear) transformation that ensures a local isometry between the time series embedding space and the feature space.
The proposed method is compared to state of the art through three major tasks that require pre-image estimation: 1) time series averaging, 2) time series reconstruction and denoising, and 3) time series representation learning.
We then try to give as much colors as possible on the credit default swap market, a relatively unknown market from the general public but for its role in the contagion of bank failures during the global financial crisis of 2007-2008, while introducing the datasets that have been used in the empirical studies.
For that purpose, we discuss their consistency and propose alternative measures of similarity that can be plugged in the clustering methodologies.
We study empirically their impact on the clusters.
Results of the empirical studies can be explored at www.datagrapple.com.
Linguistic skills are essential for translators.
However, a profound knowledge of source and target cultures is also fundamental for transmitting all the richness of a work.
Researchers have been working on the importance of translating culture for thirty years, and the criteria they define are more and more precise.
However, translation from Polish into French has been little studied.
Therefore, we examine the expression of cultural and contextual knowledge in Polish-to-French translations.
To do that, we mainly use a corpus of broadcasting texts and question the importance of both individual and national culture when the primary goal of text is not cultural transmission: how to translate such cultural elements?
We focus our research on the works of the famous Polish fantasy writer Andrzej Sapkowski and their French translations.
We also include thrillers and historical novels in our research.
In Polish texts where the primary objective is not transmitting culture but entertaining a wide range of public,what kind of cultural and contextual knowledge is expressed, what importance does it have and how is it translated into French?
We also question the impact of the absence of such cultural elements in the translations.
To answer all these questions, we establish a classification of cultural references that can be considered as a possible typology for the cultureme in translation from Polish into French that may apply to other types of texts.
For each category and subcategory, we analyze specific examples from our corpus of texts.
This allows us to recognize most frequent practices in broadcasting literature translations, and perhaps offer new approaches.
In this computer-assisted analysis of herzog by saul bellow, three different domains are involved: literature, linguistics and computer sciences.
The notoriety of clickbait can be partially attributed to misinformation as clickbait use an attractive headline that is deceptive, misleading or sensationalized.
A major type of clickbait are in the form of spam and advertisements that are used to redirect users to web sites that sells products or services (often of dubious quality).
Another common type of clickbait are designed to appear as news headlines and redirect readers to their online venues intending to make revenue from page views, but these news can be deceptive, sensationalized and misleading.
In my Thesis, I propose two innovative approaches to explore clickbait generated by news media in social media.
Image Retrieval is still a very active field of image processing as the number of available image datasets continuously increases.
One of the principal objectives of Content-Based Image Retrieval (CBIR) is to return the most similar images to a given query with respect to their visual content.
Our work fits in a very specific application context: indexing small expert image datasets, with no prior knowledge on the images.
Because of the image complexity, one of our contributions is the choice of effective descriptors from literature placed in direct competition.
Two strategies are used to combine features: a psycho-visual one and a statistical one.
In this context, we propose an unsupervised and adaptive framework based on the well-known bags of visual words and phrases models that select relevant visual descriptors for each keypoint to construct a more discriminative image representation.
Experiments show the interest of using this this type of methodologies during a time when convolutional neural networks are ubiquitous.
We also propose a study about semi interactive retrieval to improve the accuracy of CBIR systems by using the knowledge of the expert users.
Advances in computer technology and recent advances in sensing and storage technology have created many high-volume, high-dimensional data sets.
This increase in both the volume and the variety of data calls for advances in methodology to understand, process, summarize and extract information from such kind of data.
From a more technical point of view, understanding the structure of large data sets arising from the data explosion is of fundamental importance in data mining and machine learning.
Unlike supervised learning, unsupervised learning can provide generic tools for analyzing and summarizing these data sets when there is no welldefined notion of classes.
In this thesis, we focus on three important techniques of unsupervised learning for data analysis, namely data dimensionality reduction, data clustering and data co-clustering.
Our major contribution proposes a novel way to consider the clustering (resp. coclustering) and the reduction of the dimension simultaneously.
The main idea presented is to consider an objective function that can be decomposed into two terms where one of them performs the dimensionality reduction while the other one returns the clustering (resp.
We have further introduced the regularized versions of our approaches with graph Laplacian embedding in order to better preserve the local geometry of the data.
Finally, we have investigated the integration of some selected instance-level constraints in the graph Laplacians of both data samples and data features.
By doing that, we show how the addition of priory knowledge can assist in data co-clustering and improves the quality of the obtained co-clusters.
In this thesis I shall deal with the issues of confidence estimation for machine translation and statistical machine translation of large vocabulary spontaneous speech translation.
I review the performances yielded by different techniques, present the results obtained during the WMT2012 internation evaluation campaign and give the details of an application to post edition of automatically translated documents.
I then deal with the issue of speech translation.
After going into the details of what makes it a very specific and particularly challenging problem, I present original methods to partially solve it, by using phonetic confusion networks, confidence estimation techniques and speech segmentation.
To meet carbon reduction goals in Europe but worldwide too, a large number of renewable distributed energy resources (DER) still need to be deployed.
Aiming at mobilizing private capitals, several plans have been developed to put end-customers at the heart of the energy transition, hoping to accelerate the adoption of green energy by increasing its attractiveness and profitability.
These prosumers will still be connected to the main power grid and they will have the option, as they do today, to buy and sell to/from their utility company at a fixed price (a flat rate or a Time-of-Use, for example).
For these agents to fully benefit from the advantages of local energy trading, we shall assume that they own appliances (such as batteries) that, without changing their perceived energy demand, can enable them to change their net energy demand as seen from outside their homes.
Modeling prosumers as rational utility maximizers, they will schedule their battery to decrease the cost associated with their net energy demand (as their perceived demand remains unchanged).In the first part of the thesis, we investigate competitive models in which prosumers sell their surplus to their neighbors via a local energy market.
For the studied model we show that a stable solution (in the core of the game) exists in which all participants cooperate and we provide an efficient algorithm to find it.
Furthermore, we also show that cooperation is stable for participants that already own batteries and PVs but prefer to operate them in coordination to increase their value, effectively implementing collective auto-consumption.
For this later model, we propose to decouple the return over investments (ROI) obtained between the ROI produced by the investment in hardware and the ROI obtained by cooperation itself.
By doing so, we can offer the former profit to external investors to raise the required capital (although nothing forbids the member of the coalition to contribute) and the latter to the actual consumers.
This thesis, carried out within the framework of a CIFRE contract with the OctopusMind company, is focused on developing a set of automated tools dedicated and optimized to assist call for tender databases processing, for the purpose of strategic intelligence monitoring.
It contains more than 2 million documents translated into 24 languages published over the last 9 years.
The second chapter presents a study on the questions of words, sentences and documents embedding, likely to capture semantic features at different scales.
We proposed two approaches: the first one is based on a combination between a word embedding (word2vec) and latent semantic analysis (LSA).
The second one is based on a novel artificial neural network architecture based on two-level convolutional attention mechanisms.
These embedding methods are evaluated on classification and text clustering tasks.
The end of the third chapter concerns the application aspect, in particular with the implementation of some solutions deployed within OctopusMind's software environment, including information extraction, a recommender system, as well as the combination of these different modules to solve some more complex problems
The operationalization of terminologies for information processing purposes requires a computational representation of the conceptual system.
The theory of concept of the theory currently defined in ISO Terminology does not allow a computer representation [Roche TKE 2012].
Although ISO standards on Terminology does not aim to operationalize terminologies but communication between humans and used to model information and data [ISO 704: 2009].
The results of disciplines such as engineering knowledge helped to highlight the need for a theory of concept that could lead to a computer representation.
In this context, ontologies, from knowledge engineering, is one of the most interesting perspectives to model the conceptual system of Terminology [Roche, 2015, Handbook of Terminology]
This thesis presents new computational tools for quantifying deformations and motion of anatomical structures from medical images as required by a large variety of clinical applications.
Generic deformable registration tools are presented that enable deformation analysis useful for improving diagnosis, prognosis and therapy guidance.
These tools were built by combining state-of-the-art medical image analysis methods with cutting-edge machine learning methods.
First, we focus on difficult inter-subject registration problems.
Second, we develop a diffeomorphic deformation model that allows for accurate multiscale registration and deformation analysis by learning a low-dimensional representation of intra-subject deformations.
The unsupervised method uses a latent variable model in form of a conditional variational autoencoder (CVAE) for learning a probabilistic deformation encoding that is useful for the simulation, classification and comparison of deformations.
Third, we propose a probabilistic motion model derived from image sequences of moving organs.
This generative model embeds motion in a structured latent space, the motion matrix, which enables the consistent tracking of structures and various analysis tasks.
For instance, it leads to the simulation and interpolation of realistic motion patterns allowing for faster data acquisition and data augmentation.
Finally, we demonstrate the importance of the developed tools in a clinical application where the motion model is used for disease prognosis and therapy planning.
It is shown that the survival risk for heart failure patients can be predicted from the discriminative motion matrix with a higher accuracy compared to classical image-derived risk factors.
Expert finding consists in the identification of a set of individuals who are considered tobe experts in a particular topic.
This is an essential problem in the academic world.
Indeed,it is constantly necessary to identify suitable researchers when setting up reading orevaluation committees for research projects, for example.
Indeed, it is particularly useful to automatically identify experts on a specific field from the scientific literature.
We suggest an approach for knowledge discovery and enrichment based on a semantic annotation of scientific articles, on their representation in the form of scientific collaboration networks and their exploration using a graph abstraction method.
This method makes it possible to focus on dense areas of networks and to discover experts and their associated expertise using connectivity constraints.
The latter make it possible to take into account a validationby peers, materialized by the density of scientific collaboration relations that individuals maintain with each other.
We test our approach on a corpus of scientific publications, propose an original method for evaluating our results and compare our performance to expert research methods implemented in the LT ExpertFinder evaluation framework.
We obtain better performance than the state of the art and discover that the most decisive indicators of expertise are the writing of highly cited articles but also the ability to cite the appropriate scientific literature.
The success of deep learning in recent year has depended in large part on large-scale annotated data collection which is very expensive and severely limit the amount of available data.
Self-supervised learning machines have become popular in the computer vision, machine learning, and natural language processing communities in the past ten years.
They leverage the power of supervised learning machinery but request much less human intervention since the supervisory signal is provided by the input data itself (e.g., predicting missing frames from a video) without additional manual annotation.
A promising alternative is to consider instead a family of techniques that measure the discrepancy between predicted and actual inputs using a function parameterized by some latent variable, regularized to avoid trivial solutions.
In particular the corresponding implicit model allows for multimodal predictions that reflect non-deterministic aspects of the real world.
This thesis will develop novel techniques adapting this framework to computer vision, and apply them to tasks such as self-supervised learning of photoconsistency functions for multi-view stereo, and self-supervised learning of object models for visual recognition.
As image recognition systems are becoming more and more relevant, researchers in artificial intelligence now seek for the next generation vision systems that can perform high-level scene understanding.
In this thesis, we are interested in Visual Question Answering (VQA), which consists in building models that answer any natural language question about any image.
To tackle this problem, typical approaches involve modern Deep Learning (DL) techniques.
In the first part, we focus on developping multi-modal fusion strategies to model the interactions between image and question representations.
More specifically, we explore bilinear fusion models and exploit concepts from tensor analysis to provide tractable and expressive factorizations of parameters.
These fusion mechanisms are studied under the widely used visual attention framework: the answer to the question is provided by focusing only on the relevant image regions.
In the last part, we move away from the attention mechanism and build a more advanced scene understanding architecture where we consider objects and their spatial and semantic relations.
All models are thoroughly experimentally evaluated on standard datasets and the results are competitive with the literature.
Query evaluation over probabilistic databases (probabilistic query evaluation, or PQE) is known to be intractable in many cases, even in data complexity, i.e., when the query is fixed.
Although some restrictions of the queries and instances have been proposed to lower the complexity, these known tractable cases usually do not apply to combined complexity, i.e., when the query is not fixed.
My thesis investigates the question of which queries and instances ensure the tractability ofPQE in combined complexity.
My first contribution is to study PQE of conjunctive queries on binary signatures, which we rephrase as a probabilistic graph homomorphism problem.
We restrict the query and instance graphs to be trees and show the impact on the combined complexity of diverse features such as edge labels, branching,or connectedness.
While the restrictions imposed in this setting are quite severe, my second contribution shows that,if we are ready to increase the complexity in the query, then we can evaluate a much more expressive language on more general instances.
Specifically, I show that PQE for a particular class of Data log queries on instances of bounded tree width can be solved with linear complexity in the instance and doubly exponential complexity in the query.
To prove this result, we use techniques from tree automata and knowledge compilation.
The third contribution is to show the limits of some of these techniques by proving general lower bounds on knowledge compilation and tree automata formalisms.
CSR reporting standardisation has become an increasing issue for companies when they address their stakeholders.
GRI standards are, at the moment, the most widely-used standards for CSR reporting.
G3C corpus has been built to be representative of the genre " GRI standardised CSR reports ".
A huge amount of data is collected during the course of clinical care in electronic health records (EHRs) softwares.
EHRs can be useful data sources to support clinical research and can eliminate the need for duplicate data collection.
However secondary use of healthcare data is still technically difficult because of the heterogeneity between systems in a hospital.
In 2015, the Bordeaux Hospital University Center created an i2b2 data warehouse to integrate data from several disparate sources.
Still, many barriers remain to the reuse of routinely recorded clinical data like the unstructured nature of data and the semantic heterogeneity of data sources.
Architecting seeks now to be distinct from its original domain, systems engineering, becoming an emergent domain.
Far from being recognized as a science or a discipline, its practice is nowadays more and more widespread.
However, this practice is still poorly formalized, and insufficiently being taught, lacking a well-established and accessible corpus of knowledge, techniques or approaches.
This thesis contributes filling that gap by proposing a paradigm of the architectural design of artificial complex systems.
The latter is built based on existing paradigms that are combined, then completed.
It aims at providing architects with an effective, even performative framework.
It results in an approach of the architectural design structured in four levels.
A so-called archetypal level grasps the core principles of any approach of architectural design of artificial complex systems.
These principles are derived from various approaches already applied, mainly in the field of system or product design, but also of architectural design of buildings.
This vision of the architect's way of working directly impacts the kind of artefacts he handles.
We sho how to aggregate these artefacts into models, reflecting either his perception of the present, or his development of futures while progressing through some identified processes.
A so-called particular level aims at allowing the storytelling of a given design.
To achieve this goal, a notation of the design process is suggested.
Many efforts have been done these last two decades to facilitate the management and representation of cultural heritage data.
However, many systems used in cultural institutions are still based on flat models and are generally isolated which prevents any reuse or validation of information.
This Ph.D. aims at proposing new solutions for enhancing the representation and enrichment of cultural entities using the Semantic Web technologies.
This study is based on a real-world case study using the concepts from the Functional Requirements for Bibliographic Records (FRBR) which allows to generate graph-based knowledge bases.
However, in this case, the aggregation of information from heterogeneous sources requires additional steps to match and merge both correspondences at schema and instance level
The collection of palm leaf manuscripts is an important part of Southeast Asian people's culture and life.
Following the increasing of the digitization projects of heritage documents around the world, the collection of palm leaf manuscripts in Southeast Asia finally attracted the attention of researchers in document image analysis (DIA).
The research work conducted for this dissertation focused on the heritage documents of the collection of palm leaf manuscripts from Indonesia, especially the palm leaf manuscripts from Bali.
This dissertation took part in exploring DIA researches for palm leaf manuscripts collection.
This collection offers new challenges for DIA researches because it uses palm leaf as writing media and also with a language and script that have never been analyzed before.
These systems aim at making palm leaf manuscripts more accessible, readable and understandable to a wider audience and, to scholars and students all over the world.
This research developed a DIA system for document images of palm leaf manuscripts, that includes several image processing tasks, beginning with digitization of the document, ground truth construction, binarization, text line and glyph segmentation, ending with glyph and word recognition, transliteration and document indexing and retrieval.
We also developed the glyph recognition system and the automatic transliteration system for the Balinese palm leaf manuscripts.
This dissertation proposed a complete scheme of spatially categorized glyph recognition for the transliteration of Balinese palm leaf manuscripts.
The proposed scheme consists of six tasks: the text line and glyph segmentation, the glyph ordering process, the detection of the spatial position for glyph category, the global and categorized glyph recognition, the option selection for glyph recognition and the transliteration with phonological rules-based machine.
An implementation of knowledge representation and phonological rules for the automatic transliteration of Balinese script on palm leaf manuscript is proposed.
In this thesis, I investigate how lexical resources based on the organisation of lexical knowledge in classes which share common (syntactic, semantic, etc. features support natural language processing and in particular symbolic recognition of textual entailment.
First, I present a robust and wide coverage approach to lexico-structural verb paraphrase recognition based on Levin's (1993) classification of English verbs.
Then, I show that by extending Levin's framework to general inference patterns, a classification of English adjectives can be obtained that compared with previous approaches, provides a more fine grained semantic characterisation of their inferential properties.
Further, I develop a compositional semantic framework to assign a semantic representation to adjectives based on an ontologically promiscuous approach (Hobbs, 1985) and thereby supporting first order inference for all types of adjectives including extensional ones.
Finally, I present a test suite for adjectival inference I developed as a resource for the evaluation of computational systems handling natural language inference.
With the availability of massive amounts of digital images in personal and on-line collections, effective techniques for navigating, indexing and searching images become more crucial.
In this thesis, we rely on the image visual content as the main source of information to represent images.
First, we enhance the BOW representation by characterizing the spatial-color constitution of an image with a mixture of n Gaussians in the feature space.
This leads to propose a novel descriptor, the Edge Context, which plays a role as a complementary descriptor in addition to the SURF descriptor.
Second, we introduce a new probabilistic topic model, Multilayer Semantic Significance Analysis (MSSA) model, in order to study a semantic inference of the constructed visual words.
Consequently, we generate the Semantically Significant Visual Words (SSVWs).
Finally, we propose a new spatial weighting scheme and a Multiclass Vote-Based Classifier (MVBC) based on the proposed SSVIG representation.
The large-scale extensive experimental results show that the proposed higher-level visual representation outperforms the traditional part-based image representations in retrieval, classification and object recognition.
This thesis is in the context of the ANR project GEONTO covering the constitution, alignment, comparison and exploitation of heterogeneous geographic ontologies.
The goal is to automatically extract terms from topographic travelogues to enrich a geographical ontology originally designed by IGN.
The proposed method allows identification and extraction of terms contained in a text with a topographical connotation.
Our method is based on a model that relies on certain grammatical relations to locate these terms.
The implementation of this model requires the use of methods or techniques of NLP (Processing of Language).
Our model represents the relationships between terms to extract and other elements of the texts that can be identified by using external predefined resources, such as specific lexicons: verbs of travelogue (verbs of displacement, verbs of perceptions, topographical verbs), pre-positions (prepositions of place, adverbs, adjectives), place name, generic thesauri, ontologies of domain (in our case the geographical ontology originally designed by IGN).
The advantage of our approach is that the method can extract not only the terms related directly to place name but also those embedded in sentence structure in which other terms coexisted.
Experiments on a corpus consisting of 12 travel stories (2419 pages, provided by the library of Pau) showed that our method is robust.
As a result, it was used to extract 2173 distinct terms with 1191 valid terms, with a precision of 0.55.
This demonstrates that the use of the proposed relationships is more effective than that of couples (term, place name) (which gives 733 distinct terms valid with an accuracy of 0.38).
Our method can also be used for other applications such as geographic named entity recognition, spatial indexing of textual documents.
The integration of Information and Communication Technologies in Education (ICTEs) has often been described as a thorny endeavour.
Digital Work Environments (DWE) can be considered as a particularly interesting ICTE tool because of their widespread and compulsory deployment in the French educational system.
This study investigates how the DWE tool has integrated professional representations based on the theoretical approach of social representations.
We use a lexicometric approach in order to identify the transversal themes and the idiosyncrasies of these discourses.
The second part of this study is based on a survey by questionnaires on 625 secondary education teachers of Toulouse academy.
Here, the hierarchical evocation method is used to examine the content of professional representations on four objects: the DWE, the teaching profession, the notions of information and of communication.
The structure analyses of the responses and the interrelations study between these objects show the peculiarity of the teachers'universes compared to the ones identified in the three social discourses.
The complex system formed by the four objects allows us to observe that although the representations of the teaching profession and the DWE have few common contents, they share numerous elements with the representations of communication and of information.
Finally, these results enlighten us on two larger tendencies on the way secondary teachers describe the investigated objects: one part of the population seem to systematically privilege functional elements whereas the other part of the population prefers to associate evaluative elements when relating to each one of these objects.
A nautical chart is a kind of map used to describe the seafloor morphology and the shoreline of adjacent lands.
One of its main purposes is to guaranty safety of maritime navigation.
As a consequence, construction of a nautical chart follows very specific rules.
The cartographer has to select and highlight undersea features according to their relevance to navigation.
In an automated process, the system must be able to identify and classify these features from the terrain model.
An undersea feature is a subjective individuation of a part of the seafloor.
Landform recognition is a difficult task because its definition usually relies on a qualitative and fuzzy description.
Achieving automatic recognition of landforms requires a formal definition of the landforms properties and their modelling.
This terminology is here used as a starting point for the automatic classification of the features from a terrain model.
In order to integrate knowledge about the submarine relief and its representation on the chart, this research aims to define ontologies of the submarine relief and nautical chart.
Then, the ontologies are applied to generalisation of nautical chart.
In the first part of the research, an ontology is defined to organize geographical and cartographic knowledge for undersea feature representation and nautical chart generalisation.
First, a domain ontology of the submarine relief introduces the different concepts of undersea features with their geometric and topological properties.
This ontology is required for the classification of features.
Second, a representation ontology is presented, which describes how bathymetric entities are portrayed on the map.
Third, a generalisation process ontology defines constraints and operations in nautical chart generalisation.
In the second part, a generalisation process based on the ontology is designed relying on a multi-agent system.
Four kinds of agents (isobath, sounding, feature and group of features) are defined to manage cartographic objects on the chart.
A database model was generated from the ontology.
At first, geometrical properties describing the feature shape are computed from soundings and isobaths and are used for feature classification.
Then, conflicts are evaluated in a MAS and generalisation plans are provided.
The aim of this thesis is to propose a speech emotion recognition (SER) system for application in classroom.
This system has been built up using novel features based on the amplitude and frequency (AM-FM) modulation model of speech signal.
This model is based on the joint use of empirical mode decomposition (EMD) and the Teager-Kaiser energy operator (TKEO).
In this system, the discrete (or categorical) emotion theory was chosen to represent the six basic emotions (sadness, anger, joy, disgust, fear and surprise) and neutral emotion.
Automatic recognition has been optimized by finding the best combination of features, selecting the most relevant ones and comparing different classification approaches.
Two reference speech emotional databases, in German and Spanish, were used to train and evaluate this system.
A new database in French, more appropriate for the educational context was built, tested andvalidated.
Disease staging and monitoring of chronic lung diseases are two major challenges for patient care and evaluation of new therapies.
Monitoring mainly relies on pulmonary function testing but morphological assessment is a key point for diagnosis and staging In the first part, we propose different models to score bronchial disease severity on computed tomography (CT) scan.
A simple thresholding approach using adapted thresholds and a more sophisticated machine learning approach with radiomics are evaluated In the second part, we evaluate deep learning methods to segment lung fibrosis on chest CT scans in patients with systemic sclerosis.
We combine elastic registration to atlases of different thoracic morphology and deep learning to produce a model performing as well as radiologists In the last part of the thesis, we demonstrate that lung deformation assessment between inspiratory and expiratory magnetic resonance images can be used to depict fibrotic lung areas, which show less deformation during respiration and that CT assessment of lung deformation on serial CT scans can be used to diagnose lung fibrosis worsening
In this work, we focus on eyebrows movements in face to face interaction as a resource usedby one speaker to inform the other that there is an understanding problem.
Our aim is to describe the interactional sequence in which this movement appears, and to describe the interactional trajectories used by the speaker to solve the misunderstanding that took place.
This method consists in first annotating the eyebrows movements (raising and frowning) that are produced as a response by the interlocutor in 3 different interaction corpora.
Then, the annotation of the different sequences that include an understanding problem is made from pre-existing criteria from the literature (Weigand, 1999 ; Antaki, 2012).This work will allow a better understanding of the mechanisms of social interaction, taking all of the multimodal complexity that they imply into account.
This study sheds light on the role played by eyebrows movements as a practice to initiate understanding-problem sequences, and also as a practice used as a disalignment and realignment cues during the aforementioned sequences.
This work is introduced in the framework of natural language processing.
Two results were obtained: first, the negation causes coherence in the direction where it east defines as operation within the "notional domain" introduced by the speech.
The structure of "domain" facilitates the interpretation of the negative statements because it makes it possible to express them in a positive way.
Second, the negation causes inconsistency and it can be a factor of construction of a new space or world.
We proposed a fonnalisation of the two types of negations in the oriented-object formalism, whose theoretical bases are the lesniewski's logical systems.
The concept of domain was introduced into the two universes (intension and extension) of the model, in terms of the calculus of the names and the mereology.
This formalisation can enable us to return account various inferential mechanisms of the negation.
Most of text-classification methods use the ``bag of words” paradigm to represent texts.
However Bloahdom and Hortho have identified four limits to this representation: (1) some words are polysemics, (2) others can be synonyms and yet differentiated in the analysis, (3) some words are strongly semantically linked without being taken into account in the representation as such and (4) certain words lose their meaning if they are extracted from their nominal group.
To overcome these problems, some methods no longer represent texts with words but with concepts extracted from a domain ontology (Bag of Concept), integrating the notion of meaning into the model.
Models integrating the bag of concepts remain less used because of the unsatisfactory results, thus several methods have been proposed to enrich text features using new concepts extracted from knowledge bases.
Using the naive Bayes classifier algorithm, I tested and compared my contributions on the Ohsumed corpus using the domain ontology ``Disease Ontology”.
The satisfactory results led me to analyse more precisely the role of semantic relations in the enrichment step.
These new works have been the subject of a second experiment in which we evaluate the contributions of the hierarchical relations of hypernymy and hyponymy.
With the evolution of technology, the use of smart Internet-of-Things (IoT) devices, sensors, and social networks result in an overwhelming volume of IoT data streams, generated daily from several applications, that can be transformed into valuable information through machine learning tasks.
In practice, multiple critical issues arise in order to extract useful knowledge from these evolving data streams, mainly that the stream needs to be efficiently handled and processed.
In this context, this thesis aims to improve the performance (in terms of memory and time) of existing data mining algorithms on streams.
We focus on the classification task in the streaming framework.
The first part of the thesis surveys the current state-of-the-art of the classification and dimensionality reduction techniques as applied to the stream setting, by providing an updated view of the most recent works in this vibrant area.
In the second part, we detail our contributions to the field of classification in streams, by developing novel approaches based on summarization techniques aiming to reduce the computational resource of existing classifiers with no--or minor--loss of classification accuracy.
To address high-dimensional data streams and make classifiers efficient, we incorporate an internal preprocessing step that consists in reducing the dimensionality of input data incrementally before feeding them to the learning stage.
We present several approaches applied to several classifications tasks: Naive Bayes which is enhanced with sketches and hashing trick, k-NN by using compressed sensing and UMAP, and also integrate them in ensemble methods.
In the past years, combinatorial optimization techniques have been successfully applied to computationally challenging NLP tasks.
We follow this line of work in the case of LTAG parsing.
More precisely, in our setting, a given NLP problem is reduced to a subgraph selection problem.
Then we formulate the generic graph problem as an Integer Linear Program.
Integer Linear Programing has been widely studied and many optimization methods exist.
We focus on Lagrangian relaxation which previously received much attention from the NLP community.
Information fusion systems are mainly composed from mathematical tools allowing to realize data representation and combination.
The aim of these systems can be expressed as a decision problem on the truth or plausibility of a proposition based on several information coming from different sources.
Fusion try to manage the characteristics of the sources taking into account the information imperfection (inaccurate, incomplete, ambiguous, uncertain, etc.) and the redundant aspect, the complement and the conflictual aspect of information.
Fusion systems concerned by this thesis have the ability to integrate the expert knowledge in their treatments.
They are called cooperative fusion systems.
Since these systems are trying to associate experts, it is important to provide to the users some informations that help them to better understand the fusion process.
One of the major problems associated to information fusion systems concerns the evaluation of their performance.
A pertinent evaluation will allow to improve the quality of the fusion, to improve expert/system interaction and to better adjust the parameters of the system.
Generally, the evaluation of such systems is made in the ouput of the processing chain by a global evaluation of the results.
But it does not allow to know the precise subpart of the treatement chain that requires an adjustment of its parameters.
Another difficulty releases in the fact that a complete ground truth of the result is not always available, which complicates the performance evaluation task.
The application context of this work is the interpretation of 3D images (tomographic images, seismic images, synthetic images,...).
In this context, a local evaluation of the information fusion systems has been implemented.
The approach has shown its interest in the efficient adjustment of parameters and the cooperation with expert.
Nowadays it is very common to represent a system in terms of relationships between objects.
One of the common applications of such relational data is Recommender System (RS), which usually deals with the relationships between users and items.
Probabilistic Relational Models (PRMs) can be a good choice for modeling probabilistic dependencies between such objects.
A growing trend in recommender systems is to add spatial dimensions to these objects, and make recommendations considering the location of users and/or items.
This thesis deals with the (not much explored) intersection of three related fields – Probabilistic Relational Models (a method to learn probabilistic models from relational data), spatial data (often used in relational settings), and recommender systems (which deal with relational data).
The first contribution of this thesis deals with the overlapping of PRM and recommender systems.
We have proposed a PRM-based personalized recommender system that is capable of making recommendations from user queries in cold-start systems without user profiles.
Our second contribution addresses the problem of integrating spatial information into a PRM.
The task of automatically extracting insights or building computational models from knowledge on complex systems greatly relies on the choice of appropriate representation.
This work makes an effort towards building a framework suitable for representation of fragmented knowledge on complex systems and its semi-automated curation --- continuous collation, integration, annotation and revision.
We propose a knowledge representation system based on hierarchies of graphs related with graph homomorphisms.
Individual graphs situated in such hierarchies represent distinct fragments of knowledge and the homomorphisms allow relating these fragments.
Their graphical structure can be used efficiently to express entities and their relations. We focus on the design of mathematical mechanisms, based on algebraic approaches to graph rewriting, for transformation of individual graphs in hierarchies that maintain consistent relations between them.
Such mechanisms provide a transparent audit trail, as well as an infrastructure for maintaining multiple versions of knowledge.
We describe how the developed theory can be used for building schema-aware graph databases that provide schema-data co-evolution capabilities.
The proposed knowledge representation framework is used to build the KAMI (Knowledge Aggregation and Model Instantiation) framework for curation of cellular signalling knowledge.
The framework allows for semi-automated aggregation of individual facts on protein-protein interactions into knowledge corpora, reuse of this knowledge for instantiation of signalling models indifferent cellular contexts and generation of executable rule-based models.
In many scientific fields, studied data have an underlying graph or manifold structure such as communication networks (whether social or technical), knowledge graphs or molecules.
A graph is composed of nodes, also called vertices, connected together by edges.
Recently, deep learning algorithms have become state-of-the-art models in many fields and in particular in natural language processing and image analysis.
It led the way to a great line of studies to generalize deep learning models to graphs.
In particular, several formulations of convolutional neural networks were proposed and research is carried to develop new layers and network architectures to graphs.
These embeddings at different scales encode hierarchical representations of graphs.
Based on these embedding techniques, we propose new deep learning architectures to tackle node classification or graph classification tasks.
Reusing healthcare administrative databases for public health research is relevant and opens new perspectives.
This thesis deals with the joint use of healthcare administrative databases and biomedical knowledge for the study of patient care trajectories.
This includes both (1) exploration and identification through queries of relevant care pathways in voluminous flows, and (2) analysis of retained trajectories.
Semantic Web technologies and biomedical ontologies from the Linked Data allowed to identify care trajectories containing a drug interaction or a potential contraindication between a prescribed drug and the patient's state of health.
In addition, we have developed the R queryMed package to enable public health researchers to carry out such studies by overcoming the difficulties of using Semantic Web technologies and ontologies.
After identifying potentially interesting trajectories, knowledge from biomedical nomenclatures and ontologies has also enriched existing methods of analysing care trajectories to better take into account the complexity of data.
This resulted notably in the integration of semantic similarities between medical concepts.
Semantic Web technologies have also been used to explore obtained results.
It is a question of studying the United Nations discourse on the Iranian nuclear crisis during the ten years between 2005 and 2015.
The study is conducted on a closed and predefined corpus, in order to discern the various linguistic and discursive processes that command the discourse.
It is also a question of apprehending the stakes as well as the legal and political origins of this diplomatic crisis.
Our major challenge is to understand the discourse in its multiple dimensions, linguistic, discursive, political and legal.
These are the questions we respond in this thesis.
The apprehension of linguistic and discursive impacts is realized in the light of the political and legal data that constitute an interpretive framework for the analysis.
The objective is to identify the construction of the United Nations identity through notions of values, by discursive mechanisms.
The quantity of user-generated content on the Web is constantly growing at a fast pace.
A great share of this content is made of opinions and reviews on products and services.
This electronic word-of-mouth is also an important factor in decisions about purchasing these products or services.
Users tend to trust other users, especially if they can compare themselves to those who wrote the reviews, or, in other words, they are confident to share some characteristics.
For instance, families will prefer to travel in places that have been recommended by other families.
We assume that reviews that contain lived experiences are more valuable, since experiences give to the reviews a more subjective cut, allowing readers to project themselves into the context of the writer.
With this hypothesis in mind, in this thesis we aim to identify, extract, and represent reported lived experiences in customer reviews by hybridizing Knowledge Extraction and Natural Language Processing techniques in order to accelerate the decision process.
Forthis, we define a lived user experience as an event mentioned in a review, where the authoris among the participants.
This definition considers that mentioned events in the text are the most important elements in lived experiences: all lived experiences are based on events,which on turn are clearly defined in time and space.
There fore, we propose an approach to extract events from user reviews, which constitute the basis of an event-based system to identify and extract lived experiences.
For the event extraction approach, we transform user reviews into their semantic representations using machine reading techniques.
We perform a deep semantic parsing of reviews, detecting the linguistic frames that capture complex relations expressed in there views.
The event-based lived experience system is carried out in three steps.
The first step operates an event-based review filtering, which identifies reviews that may contain lived experiences.
The second step consists of extracting relevant events together with their participants.
In order to test our hypothesis, we carried out some experiments to verify whether lived experiences can be considered as triggers for the ratings expressed by users.
Therefore, we used lived experiences as features in a classification system, comparing with the ratings of the reviews in a dataset extracted and manually annotated from Tripadvisor.
The results show that lived experiences are actually correlated with the ratings.
In conclusion, this thesis provides some interesting contributions in the field of opinionmining.
First of all, the successful application of machine reading to identify lived experiences.
Second, the confirmation that lived experiences are correlated to ratings.
Finally,the dataset produced to test our hypothesis constitutes also an important contribution of the thesis.
While deep neural networks are now used in almost every component of a speech recognition system, from acoustic to language modeling, the input to such systems are still fixed, handcrafted, spectral features such as mel-filterbanks.
This contrasts with computer vision, in which a deep neural network is now trained on raw pixels.
Mel-filterbanks contain valuable and documented prior knowledge from human auditory perception as well as signal processing, and are the input to state-of-the-art speech recognition systems that are now on par with human performance in certain conditions.
However, mel-filterbanks, as any fixed representation, are inherently limited by the fact that they are not fine-tuned for the task at hand.
We hypothesize that learning the low-level representation of speech with the rest of the model, rather than using fixed features, could push the state-of-the art even further.
We first explore a weakly-supervised setting and show that a single neural network can learn to separate phonetic information and speaker identity from mel-filterbanks or the raw waveform, and that these representations are robust across languages.
Moreover, learning from the raw waveform provides significantly better speaker embeddings than learning from mel-filterbanks.
These encouraging results lead us to develop a learnable alternative to mel-filterbanks, that can be directly used in replacement of these features.
In the second part of this thesis we introduce Time-Domain filterbanks, a lightweight neural network that takes the waveform as input, can be initialized as an approximation of mel-filterbanks, and then learned with the rest of the neural architecture.
Across extensive and systematic experiments, we show that Time-Domain filterbanks consistently outperform melfilterbanks and can be integrated into a new state-of-the-art speech recognition system, trained directly from the raw audio signal.
Fixed speech features being also used for non-linguistic classification tasks for which they are even less optimal, we perform dysarthria detection from the waveform with Time-Domain filterbanks and show that it significantly improves over mel-filterbanks or low-level descriptors.
Finally, we discuss how our contributions fall within a broader shift towards fully learnable audio understanding systems.
The work, presented in this thesis, aims to propose solutions to the problems of textual data warehousing.
The interest in the textual data is motivated by the fact that they cannot be integrated and warehoused by using the traditional applications and the current techniques of decision-making systems.
In order to overcome this problem, we proposed a text warehouses approach which covers the main phases of a data warehousing process adapted to textual data.
We focused specifically on the integration of textual data and their multidimensional modeling.
For the textual data integration, we used information retrieval (IR) techniques and automatic natural language processing (NLP).
Thus, we proposed an integration framework, called ETL-Text which is an ETL (Extract-Transform-Load) process suitable for textual data.
The ETL-Text performs the extracting, filtering and transforming tasks of the original textual data in a form allowing them to be warehoused.
Some of these tasks are performed in our RICSH approach (Contextual information retrieval by topics segmentation of documents) for pretreatment and textual data search.
It extends the classical constellation model to support the representation of textual data in a multidimensional environment.
TWM includes a semantic dimension defined for structuring documents and topics by organizing the semantic concepts into a hierarchy.
Also, we depend on a Wikipedia, as an external semantic source, to achieve the semantic part of the model.
Furthermore, we developed WikiCat, which is a tool permit to feed the TWM semantic dimension with semantics descriptors from Wikipedia.
These last two contributions complement the ETL-Text framework to establish the text warehouse device.
To validate the different contributions, we performed, besides the implementation works, an experimental study for each model.
For the emergence of large data, we developed, as part of a case study, a parallel processing algorithms using the MapReduce paradigm tested in the Apache Hadoop environment.
The main goal of this thesis is to propose a complete framework for automatic discovery, modeling and recognition of human activities in videos.
In order to model and recognize activities in long-term videos, we propose a framework that combines global and local perceptual information from the scene and accordingly constructs hierarchical activity models.
In the first variation of the framework, a supervised classifier based on Fisher vector is trained and the predicted semantic labels are embedded in the constructed hierarchical models.
In the second variation, to have a completely unsupervised framework, rather than embedding the semantic labels, the trained visual codebooks are stored in the models.
Finally, we evaluate the proposed frameworks on two realistic Activities of Daily Living datasets recorded from patients in a hospital environment.
Furthermore, to model fine motions of human body, we propose four different gesture recognition frameworks where each framework accepts one or combination of different data modalities as input.
We evaluate the developed frameworks in the context of medical diagnostic test namely Praxis.
We suggest a new challenge in gesture recognition, which is to obtain an objective opinion about correct and incorrect performances of very similar gestures.
The experiments show effectiveness of our deep learning based approach in gesture recognition and performance assessment tasks.
This thesis aims at exploiting the potential of spatial and temporal data present at the heart of scientific papers, in order to introduce an ontology of spatial and temporal information contained in scientific papers from a Semantic Web point of view.
These new metadata could serve the production of new representations in the form of graphs or text syntheses, with applications in geoparsing, chronological analysis and visualizations.
This project responds to the need to exploit the information in scientific corpora at a large scale by the analysis and semantic processing of the full text of the articles.
The main topic of investigation for this CIFRE Ph.D., in partnership with the retail consultancy firm OneTeam, is Natural Language Processing (NLP).
Its specificity is the conception and implementation of a ChatBot for internal use.
More precisely, its use case will be to assist in the relationship between OneTeam's clients and its online customer service, to help handling technical tickets concerning client difficulties with OneTeam's retail support software.
A ChatBot is a NLP interface between a human and the machine.
It is supposed to understand a limited subset of natural language relevant to the application context, provide a dialogue-like interaction capability, and be able to translate the human requests into a either a semi-formal language (to be communicated to a technical manager), or a formal language (e.g. a search query), or a pragmatic context (e.g. implement a set of actions such as launch a piece of software or run a SQL query), or a combination thereof.
In the context of this Ph.D. proposal, the ChatBot should understand an informal description of a client-side technical problem to do with one of the applications from OneTeam's software suites, and provide a conditional interface based on a boolean choice: either the description matches an existing issue in stored closed tickets database, or not.
In the first case, the ChatBot should construct the formal pragmatic description necessary to solve the client problem at hand.
In the second case, the ChatBot should translate the informal description to a semi-formal open ticket to be written in the ticket database.
While the conception and deployment of ChatBots is not by itself new, the overall system required by OneTeam has some severely challenging features:
1. the informal description in input may be a transcription from a telephone message, which makes the text much more error-ridden, punctuation-challenged, and hence more difficult to understand;
2. the limited language to be understood includes several acronyms that may be mis-spelled in the informal description, as well as ungrammatical sentences;
3. in the case the ChatBot is to directly engender the solution, there is no margin of error in the understanding and translation to the correct formal pragmatics.
Addressing these features will require scientific research at the interface between algorithmics and computational linguistics.
A further challenge is that the text may be in French or English, with French being by far more likely.
While this feature is in itself not necessarily original, there is a severely more limited amount of available NLP software resources in French as compared to English.
This will undoubtedly call for some supplementary low-level NLP task conception, design and implementation.
This work focuses on the representations of adolescence in contemporary independent cinema.
It is a question of examining the presentation in images and in discourse of this social phenomenon in six films coming from Europe and America and released in room between 2009 and 2014.
By situating ourselves in the domains of Sciences of the Information and Communication, we articulate a semiotic and pragmatic approach to account for the plastic, aesthetic and communicational aspects of the films of the corpus.
Our poetic and socio-political analysis simultaneously explores the cinematographic language, the conditions of production and the horizon of expectation to which the works refer.
The first part sets out the theoretical categories and the method to follow to detect the relations between adolescence, cinema and representation.
The second part captures the forms and effects of meaning of the films.
The body is taken as the foundation of the representation of adolescence, a semiotic, social and political object.
The third part focuses on the emergence of images of adolescent bodies and discourses likely to engender a look at cinematographic adolescence as a symbolic and social construction.
The thesis concludes that these films constitute discourses on the adolescent universe, invite to deliberation, using a set of filmic forms that lead to reflections on the social, historical and cultural world that is deployed there.
Software Product Lines (SPLs) enable the derivation of a family of products based on variability management techniques.
Inspired by the manufacturing industry, SPLs use feature configurations to satisfy different customer needs, along with reusable assets to allow systematic reuse.
Capitalizing on existing variants by extracting the common and varying elements is referred to as extractive approaches for SPL adoption.
Also, to identify the associated implementation elements of the features, their location is needed.
In addition, feature constraints should be identified to guarantee that customers are not able to select invalid feature combinations.
This dissertation presents Bottom-Up Technologies for Reuse (BUT4Reuse), a unified, generic and extensible framework for mining software artefact variants.
Special attention is paid to model-driven development scenarios.
We also focus on benchmarks and in the analysis of variants, in particular, in benchmarking feature location techniques and in identifying families of variants in the wild for experimenting with feature identification techniques.
We present visualisation paradigms to support domain experts on feature naming and to support on feature constraints discovery.
Finally, we investigate and discuss the mining of artefact variants for SPL analysis once the SPL is already operational.
Concretely, we present an approach to find relevant variants within the SPL configuration space guided by end user assessments.
Our thesis proposes data mining within the framework of Natural Language Processing(NLP) according to the theory of Cognitive Grammar (Langacker 1987, 1991).
Providing examples from French, English and Modern Greek, we investigate theappropriate units for data mining.
A function is defined as an operation involving adependent morpheme, for ex. a verb or a suffix, and an autonomous morpheme, for ex.a noun ;
certain functions are primary in the sense that they are psychologically more salient ;
these are verbal or deverbal dependent morphemes that profile differently one same processual scene.
In order to get an automatic data mining at the phrastic level, wefirst need to prepare dictionaries of inflection and of deverbal derivation.
We are taking the first steps by analysing exhaustively the verbal inflection of English, French and Modern Greek and we build electronic dictionaries of all their inflected forms.
This work takes place in the research of the group sydo-lyon, which principal activity deals with information retrieval system design.
A text indexing of french textual corpus is set up by the description of the noun phrases extracted from the texts and of syntactic connections between these noun phrases.
The anaphora resolution is very important for automatic text analysis because some anaphora build up syntactic connections between a noun phrase antecedent and an anaphoric, and anaphora is a relation which contributes to represent text content as it points out some noun phrases (themes).
The main prospect of this work is to identify and formalise principles of anaphora resolution, so that they can be used in a computational process.
It has so to study relations between anaphora and coreference, to index all anaphoric forms and antecedent forms, and to propose solution for automatic recognizing anaphorics and antecedents in texts.
At last, we present the realisation of a program in prolog which gives a solution for personal pronouns.
In conclusion, this work now can be pursued with improving resolution of personal pronouns and elaboration of an automatic analysis of others anaphorics.
Question answering systems find and extract a precise answer to a question in natural language.
Both current question-answering systems and evaluation campaigns often assume that only one single answeris expected for a question.
Our corpus studies show that this is rarely the case, specially when answers are extracted from the Web instead of a frozen collection of documents.
We therefore focus on questions expecting multiple correct answers from the Web by developping the question-answering system Citron.
Citron is dedicated to extracting multiple answers in open domain and identifying the shifting criteria (date, location) which is often the reason of this answer multiplicity
Our corpus studies show that the answers of this kind of questions are often located in structures such as tables and lists which cannot be analysed without a suitable preprocessing.
Consequently we developed the Kitten software which aims at extracting text information from HTML documents and also both identifying and formatting these structures.
We finally evaluate Citron through two experiments involving users.
The first experiment evaluates both Citron and human beings on a multiple answer extraction task: results show that Citron was faster than humans and that the quality difference between answers extracted by Citron and humans was reasonable.
The second experiment evaluates user satisfaction regarding the presentation of multiple answers: results show that user shave a preference for Citron presentation aggregating answers and adding the shifting criteria (if it exists) over the presentation used by evaluation campaigns.
MATLAB is a computing environment with an easy programming language and a vast library of functions commonly used in Computation Science and Engineering (CSE) for fast prototyping.
However, some features of its environment, such as its dynamic language or interactive style of programming affect how fast the programs can execute.
Current approaches to improve MATLAB programs either translate the code to faster static languages like C or Fortran, or apply code transformations to MATLAB code systematically without considering their impact on the performance.
In this thesis, we fill this gap by developing techniques for the analysis and code transformation of MATLAB programs in order to improve their performance.
More precisely, we analyse and model the behaviour of the black-box MATLAB environment by measuring the execution characteristics of programs on CPU.
From the resulting data, we formalise a static model which predicts the type and order of instructions scheduled by the Just-In-Time (JIT)compiler.
This model allows us to propose several code transformations which increase the performance of MATLAB programs by influencing how the JIT compiler generates the machine code.
The obtained results demonstrate the practical benefits of the presented methodology.
With the rise of the so-called cognitive robotics, the need of advanced tools to store, manipulate, reason about the knowledge acquired by the robot has been made clear.
But storing and manipulating knowledge requires first to understand what the knowledge itself means to the robot and how to represent it in a machine-processable way.
In a second part, the thesis presents in depth a particular instantiation of a knowledge representation and manipulation system called ORO, that has been designed and implemented during the preparation of the thesis.
We elaborate on the inner working of this system, as well as its integration into several complete robot control stacks.
A particular focus is given to the modelling of agent-dependent symbolic perspectives and their relations to theories of mind.
The third part of the study is focused on the presentation of one important application of knowledge representation systems in the human-robot interaction context: situated dialogue.
The thesis concludes on considerations regarding the viability and importance of an explicit management of the agent's knowledge, along with a reflection on the missing bricks in our research community on the way towards "human level robots"
Our resarch lies in the wake of the efforts to build French Sign Language public announcement systems by virtual signers, using combined prerecorded chunks of utterances.
Our study focuses on modelling coarticulation for it to be used in this system.
We detail the various aspects of corpus creation and annotation, and annotation analysis.
Quantitative and qualitative statistics allows us to propose a coarticulation model, based on tensions and relaxations of hand configurations.
We propose and implement a methodology for evaluating our mode!.
Finally, we propose prospects for this model in image processing and 3d character animation in French Sign Language.
As soon as the robots step out in the real and uncertain world, they have to adapt to various unanticipated situations by acquiring new skills as quickly as possible.
Unfortunately, on robots, current state-of-the-art reinforcement learning (e.g., deep-reinforcement learning) algorithms require large interaction time to train a new skill.
Our primary focus is to incorporate prior knowledge from a simulator with real-world experiences of a robot to achieve rapid learning and adaptation.
In our first contribution, we propose a novel model-based policy search algorithm called Multi-DEX that (1) is capable of finding policies in sparse reward scenarios (2) does not impose any constraints on the type of policy or the type of reward function and (3) is as data-efficient as state-of-the-art model-based policy search algorithm in non-sparse reward scenarios.
In our third contribution, we introduce a gradient-based meta-learning algorithm called FAMLE.
FAMLE meta-trains the dynamical model of the robot from simulated data so that the model can be adapted to various unseen situations quickly with the real-world observations.
By using FAMLE with a model-predictive control framework, we show that our approach outperforms several model-based and model-free learning algorithms, and solves the given tasks in less interaction time than the baselines.
With the advance of the Semantic Web and the Open Linked Data initiatives, a huge quantity of RDF data is available on Internet.
The goal is to make this data readable for humans and machines, adopting special formats and connecting them by using International Resource Identifiers (IRIs), which are abstractions of real resources of the world.
However, studies about anonymization in the context of RDF data, are really limited.
These studies are initial works for protecting individuals on RDF data, since they show a practical anonymization approach for simple scenarios as the use of generalization and suppression operations based on hierarchies.
However, for complex scenarios, where a diversity of data is presented, the existing anonymization approaches does not ensure an enough privacy.
Thus, in this context, we propose an anonymization framework, which analyzes the neighbors according to the background knowledge, focused on the privacy of entities represented as nodes in the RDF data.
Our anonymization approach is able to provide better privacy, since it takes into account the l-diversity condition as well as the neighbors (nodes and edges) of entities of interest.
Also, an automatic anonymization process is provided by the use of anonymization operations associated to the datatypes.
The aims of this thesis are two-fold, and centered on synthetic metabolic circuits, which perform sensing and computation using enzymes.
The first part consisted in developing reinforcement and active learning tools to improve the design of metabolic circuits and optimize biosensing and bioproduction.
In order to do this, a novel algorithm (RetroPath3.0) based on similarity-guided Monte Carlo Tree Search to improve the exploration of the search space is presented.
This algorithm, combined with data-derived reaction rules and varying levels of enzyme promiscuity, allows to focus exploration on the most promising compounds and pathways for bio-retrosynthesis.
The second part consisted in developing analysis tools, to generate knowledge from biological data and model biosensor response.
First, the effect of plasmid copy number on sensitivity of a transcription-factor based biosensor was modeled.
Then, using cell-free systems allowing for broader control over the experimental factors such as DNA concentration, resource usage was modeled to ensure our current knowledge of underlying phenomenons is sufficient to account for circuit behavior, using either empirical models or mechanistic models.
Coupled with metabolic circuit design, those models allowed us to develop a new biocomputation approach, called metabolic perceptrons.
Overall, this thesis presents tools to design and analyse synthetic metabolic circuits, which are a novel way to perform computation in synthetic biology.
Building discourse parsers is currently a major challenge in Natural Language Processing.
The identification of the relations (such as Explanation, Contrast...) linking spans of text in the document is the main difficulty.
In this thesis, we use raw data to improve automatic identification of implicit relations.
First, we propose to use discourse markers in order to automatically annotate new data.
We report improvements on the English corpus Penn Discourse Treebank, and especially we show that this method alleviates the need for rich resources, available but for a few languages.
Neural networks have become a flagship algorithm of Artificial intelligence (AI), with major successes in solving complex tasks such as image recognition and game playing.
Digital computers unfortunately consume an inordinate amount of energy when they run neural networks.
For example, training a state-of-the art natural language processing model on a modern supercomputer consumes 1000 kW.h [1], which is the energy consumed by a human brain for the entirety of its tasks over a duration of six years.
In the brain, neurons -- which can roughly be seen as carrying out the computation -- have a direct access to memory, supported by synapses.
Current electronics, on which rely the GPUs and CPUs used in AI, intrinsically separates memory and computing into distinct physical units, between which data must be carried back and forth.
This "von Neuman bottleneck" is an issue for artificial intelligence algorithms which require reading considerable amounts of data at each step, performing advanced operations on this data, and then writing the results back to memory [2],[4].
This process slows down computing and considerably increases the energy consumption for learning and inference.
The general paradigm in neuromorphic computing is therefore to take inspiration from the topology of the brain to build circuits composed of physical neurons interconnected by physical synapses that implement memory in-situ, in a non-volatile way, thus drastically cutting the need to move data around the circuit and allowing huge gains in speed and energy efficiency.
One of the major challenges in neuromorphic computing is to achieve on chip training of the networks in an efficient way.
Implementing state-of-the art training algorithm such as backpropagation requires bulky and energy-hungry circuits to store activations, compute gradients, store gradients, then sequentially modify each physical synapse in the network.
This is an immense hurdle towards the development of an embedded AI that can learn.
For this reason, the AI chips developed today in academia and industry mostly target inference with off-chip learning [5],[7], or implement training algorithms with performance far from state-of-the-art gradient descent methods [8].
In this context, works by AI pioneers such as Geoffrey Hinton [9] and Yoshua Bengio [10],[12] to understand how gradient descent could be performed in the brain gives inspirational guidelines to achieve the same in neuromorphic systems.
Algorithms such as Equilibrium propagation, first proposed by Scellier and Bengio [12], and E-Prop by Wolfgang Maas [13] show that in spiking neural networks gradients can be carried through the network directly by neuron activity.
C2N and CNRS/Thales in collaboration with the team of Yoshua Bengio at Mila have shown that the gradients computed by Equilibrium Propagation are equal to those derived by backpropagation through time, and highlighted the potential of this result for training neuromorphic systems with state-of-the-art performance in a publication in NeurIPS last year (accepted as oral presentation) [14].
The ultimate objective of the thesis is to fabricate neuromorphic spiking circuits that learn locally and autonomously through algorithms such as Equilibrium Propagation that encode gradients in the spiking activity of neurons and can be directly applied to the synapses they connect.
For this purpose, the doctoral student will first develop, code and test through simulations novel algorithms to train recurrent spiking neural networks, inspired by Equilibrium propagation but adapted to neuromorphic hardware.
Then he or she will realize these networks in hardware, through lab-scale experiments and then in larger systems.
The building blocks that will be used for developing this hardware are nanoscale electronic components called resistive switching devices or memristors (short for memory-resistors) [15].
They are made of an insulating oxide material sandwiched between two metallic electrodes (Fig. 1(a-b)).
These nanoscale devices are very promising for neuromorphic computing because they can imitate both synapses and neurons with very low energy in a very small area.
They exhibit different behaviours depending on the materials that are used for the oxide and the electrodes.
They can be passive and exhibit non-volatile resistance switching (e.g., with Pt/TaOx/Pt heterostructures), which is useful for synaptic features (multilevel memory and plasticity, (Fig. 1(d)).
They can also be active and display volatile switching through negative differential resistance (e.g., with Pt/NbOx/Pt heterostructures), which can generate oscillating neuron features (like spiking and bursting, Fig. 1(c)).
C2N and CNRS/Thales have a long standing expertise in the fabrication and study of these nanodevices [3], [16],[20].
For now, studies have mostly focused on circuits that use these devices either as synapses, or as neurons, but not the two at the same time.
It has been shown that memristive synapses can learn through the bio-inspired learning rule called Spike Timing Dependent Plasticity [3], [21].
It has also been demonstrated that memristive neurons can imitate many features of neurons including spiking and bursting [15].
The goal of the thesis is to assemble these individual components in spiking neural networks that learn autonomously through state-of-the-art algorithms emulating backpropagation in materio.
This research thesis is focusing in creativity, innovation and collaborative creative process.
There is a need to build new technical and innovative systems that can emulate a valuable human behavior consisting of visualizing and giving life to their ideas.
This approach involves creativity ability and involves, as a process, many concepts such as discovery, creation, sociability, refinement and communication.
This research's missions to design informatics and technological tool to support creativity during the creative phases in a creativity workshop.
To achieve the design of this tool, there are several objectives to achieve during the time requested for this research.
The research context is a creativity workshop “48H”, mobilizing students, professors, industrials, experts working with ideas by mean of exploratory combinatory, and transformational techniques and collaborative creative methods.
To build a tool for that kind of creative event, we need research how to consider the Human Knowledge Sharing Model and how a proposed System can develop a Semantic Approach to compare Idea Cards in 48H Creativity Workshop.
The main purposes of this research are to understand the knowledge sharing during a creativity workshop and to study the human organization in a creativity workshop and its general process, particularly.
We study the event 48 Hours of creativity (48H) as a creative and innovative process where ideas are the final product.
This event is performed in Nancy, France at the University of Lorraine (ERPI Laboratory).
We design an intelligent system based in multi-agents (MAS) also to develop and annotation system.
We propose a semantic approach to manage ideas and the global design system.
The practical implications of our model permit to use creativity, collaborative creative methods and new technologic modern methodologies, as new tools designed to build creative and innovative systems.
Some pertinent fields and models take part on this research such as multi-agent system (MAS), semantic web, ontology, organizational model to highlight knowledge and knowledge reuse organizational model KROM.
With the massive increase of video content on Internet and beyond, the automatic understanding of visual content could impact many different application fields such as robotics, health care, content search or filtering.
The goal of this thesis is to provide methodological contributions in Computer Vision and Machine Learning for automatic content understanding from videos.
We emphasis on problems, namely fine-grained human action recognition and visual reasoning from object-level interactions.
In the first part of this manuscript, we tackle the problem of fine-grained human action recognition.
We introduce two different trained attention mechanisms on the visual content from articulated human pose.
The first method is able to automatically draw attention to important pre-selected points of the video conditioned on learned features extracted from the articulated human pose.
We show that such mechanism improves performance on the final task and provides a good way to visualize the most discriminative parts of the visual content.
The second method goes beyond pose-based human action recognition.
We develop a method able to automatically identify unstructured feature clouds of interest in the video using contextual information.
Furthermore, we introduce a learned distributed system for aggregating the features in a recurrent manner and taking decisions in a distributed way.
We demonstrate that we can achieve a better performance than obtained previously, without using articulated pose information at test time.
In the second part of this thesis, we investigate video representations from an object-level perspective.
Given a set of detected persons and objects in the scene, we develop a method which learns to infer the important object interactions through space and time using the video-level annotation only.
That allows to identify important objects and object interactions for a given action, as well as potential dataset bias.
Finally, in a third part, we go beyond the task of classification and supervised learning from visual content by tackling causality in interactions, in particular the problem of counterfactual learning.
We introduce a new benchmark, namely CoPhy, where, after watching a video, the task is to predict the outcome after modifying the initial stage of the video.
We develop a method based on object-level interactions able to infer object properties without supervision as well as future object locations after the intervention.
This study aims at a classification and analysis of the greek frozen (idiomatic) sentences.
It is carried out within the lexicon-grammar framework and hopes to be a contribution to the greek lexicon-grammar elaboration.
The adopted theoretical approach is that of the transformation grammar as defined by z. Harris and m. Gross.
The syntactic structure of frozen sentences is generally that of free sentences and they undergo the same syntactic operations which are applied to free sentences.
Thus, in this study, the frozen sentences are analyzed as their "free" (non-frozen) counterparts (e.g. Subject, object, etc.).
The final classification includes 13 classes; these in turn, contain about 4500 entries.
Besides the classification may be used for different applications such as natural language processing, translation and teaching.
Recognizing a speaker's opinions in an oral interaction is a crucial step in improving communication between a human and a virtual agent.
In this thesis, we find ourselves in a problematic of automatic speech processing (APT) on opinion phenomena in natural spontaneous oral interactions.
Opinion analysis is a task that is not often addressed in TAP that focused until recently on emotions using voice and non-verbal content.
In addition, most existing legacy systems do not use the interactional context to analyze the speaker's opinions.
In this thesis, we focus on these topics.
We are in the context of automatic detection using statistical learning models.
A study on modeling the dynamics of opinion by a model with latent states within a monologue, we study how to integrate the context interactional dialogical, and finally to integrate audio to text with different types of fusion.
We worked on a basic Vlogs data at a global sense, and on the basis of multimodal data dyadic interactions composed of open conversations, at the turn of speech and word pair of towers.
Finally, we annotated database in opinion because existing database were not satisfactory vis-à-vis the task addressed, and did not allow a clear comparison with other systems in the state art.
At the dawn of significant change brought by the advent of neural methods, we study different types of representations: the ancient representations built by hand, rigid, but precise, and new representations learned statistically, and general semantics.
We study different segmentations to take into account the asynchronous nature of multi-modality.
Recently, we are using a latent state learning model that can adapt to a small database, for the atypical task of opinion analysis, and we show that it allows both an adaptation of the descriptors of the written domain to the oral domain, and serve as an attention layer via its clustering power.
Complex multimodal fusion is not well managed by the classifier used, and audio being less impacting on opinion than text, we study different methods of parameter selection to solve these problems.
Conceptual models (CM) serve as the blueprints of information systems and their quality plays decisive role in the success of the end system.
It has been witnessed that majority of the IS change-requests result due to deficient functionalities in the information systems.
Therefore, a good analysis and design method should ensure that CM are correct and complete, as they are the communicating mediator between the users and the development team.
Our approach targets the problems related to conceptual modeling quality by proposing a comprehensive solution.
We designed multiple artifacts for different aspects of CM quality.
Most of the existing literature on CM quality evaluation represents disparate and autonomous quality frameworks proposing non-converging solutions.
ii. Formulation of quality patterns to encapsulate past-experiences and good practices as the selection of relevant quality criteria (including quality attributes and metrics) with respect to a particular requirement (or goal) remains trickier for a non-expert user.
These quality patterns encapsulate valuable knowledge in the form of established and better solutions to resolve quality problems in CM.
iii. Designing of the guided quality driven process encompassing methods and techniques to evaluate and improve the conceptual models with respect to a specific user requirement or goal.
iv. Development of a software prototype “CM-Quality”.
There are several Berber languages in the south west of Algeria.
Some of them are situated in the so-called Sud-Oranais and they can be categorized as endangered languages.
So I have decided to describe them before they disappear.
That's why, I have carried out several fieldworks.
But, this linguistic documentation work and cultural heritage conservation are just one of aspects of our thesis.
I have used the methods which are applied in Geographic Information Science (GIS) and in Data Science (DS) to carry out a dialectological study.
A geolinguistic study has been undertaken and has enabled to visualize the expansion of the linguistic variation of certain consonants through GIS.
Based on these data, I have debated the phonological reality of the simple and geminate consonants.
From this research, a dialectometric study was carried out on the basis of data partitioning methods.
I have used the Unsupervised Learning Methods (HAC, k-mean, MDS,...) and the Supervised Learning Methods (CART) known in DS.
Then, I have undertaken a phonetic analysis, which is based on an acoustic study of alveolar rhotics: [ɾ], [r], [ɾˤ] and [rˤ].
These phonic unities are distinguished by their temporality and their articulatory realization.
Thus, the spectrograms enabled to examine the distribution of these sounds and to distinguish what was related to phonetic and phonology.
The standard discourse produced by official organisations is confronted with the unofficial or informal discourse of the social web.
Empowering people to express themselves results in a new balance of authority, when it comes to knowledge and changes the way people learn.
Social web discourse is available to each and everyone and its size is growing fast, which opens up new fields for both humanities and social sciences to investigate.
The latter, however, are not equipped to engage with such complex and little-analysed data.
The aim of this dissertation is to investigate how far social web discourse can help supplement official discourse.
In it we set out a method to collect and analyse data that is in line with the characteristics of a digital environment, namely data size, anonymity, transience, structure.
This field of investigation encompasses several related questions that have to do with health, society, the evolution of morals, the mismatch between different kinds of discourse.
Our study is also grounded in the analysis of a comparable French corpus dealing with the same topic, whose genre and discourse characteristics are equivalent to those of the Vietnamese one: this two-pronged research highlights the specific features of different socio-cultural environments.
The goal of this project is to fully exploit the audio stream to automatically enrich speech transcripts and subtitles of TV series and movies with the name and position of the characters.
speaker A: 'Nice to meet you, I am Leonard, and this is Sheldon. We live across the hall.'
speaker B: 'Oh. Hi. I'm Penny.'
speaker A: 'Sheldon, what the hell are you doing?'
speaker C: 'I am not quite sure yet. Do you know where Howard lives?'
Just looking at these two short conversations, a human can easily infer that 'speaker A' is actually 'Leonard', 'speaker B' is Penny and 'speaker C' is Sheldon.
The objective of this project is to combine natural language processing and speech processing to do the same automatically.
This thesis investigates how a machine can be taught a new task from unlabeled human in structions, which is without knowing beforehand how to associate the human communicative signals with their meanings.
It therefore removes the need for an expert to tune the system for each specific user, which constitutes an important step towards flexible personalized teaching interfaces, a key for the future of personal robotics.
Our approach assumes the robot has access to a limited set of task hypotheses, which include the task the user wants to solve.
Our method consists of generating interpretation hypotheses of the teaching signals with respect to each hypothetic task.
By building a set of hypothetic interpretation, i.e. a set of signal label pairs for each task, the task the user wants to solve is the one that explains better the history of interaction.
We consider different scenarios, including a pick and place robotics experiment with speech as the modality of interaction, and a navigation task in a brain computer interaction scenario.
In these scenarios, a teacher instructs a robot to perform a new task using initially unclassified signals, whose associated meaning can be a feedback (correct/incorrect) or a guidance (go left, right, up,...).
Our results show that a) it is possible to learn the meaning of unlabeled and noisy teaching signals, as well as a new task at the same time, and b) it is possible to reuse the acquired knowledge about the teaching signals for learning new tasks faster.
We further introduce a planning strategy that exploits uncertainty from the task and the signals' meanings to allow more efficient learning sessions.
We present a study where several real human subjects control successfully a virtual device using their brain and without relying on a calibration phase.
Our system identifies, from scratch, the target intended by the user as well as the decoder of brain signals.
Based on this work, but from another perspective, we introduce a new experimental setup to study how humans behave in asymmetric collaborative tasks.
In this setup, two humans have to collaborate to solve a task but the channels of communication they can use are constrained and force them to invent and agree on a shared interaction protocol in order to solve the task.
These constraints allow analyzing how a communication protocol is progressively established through the interplay and history of individual actions.
The main objective of this thesis is to propose representation learning techniques with little to no need for labeled data: unsupervised, distant or even active learning are all research pathways that could be explored.
Unsupervised learning is an extreme case in which unlabeled data is abundantly available, but in which no labeled data is.
The term 'self-supervised'is being increasingly used to describe this kind of approaches, as their task often consists on reconstructing the original data from an altered version of it (BERT [Devlin et al., 2018], auto-encoders, etc.).
We talk about distant supervision when (sometimes inaccurate) labels are in fact available, but not for the specific targeted task.
For instance, for speaker verification, it is possible to use an automatic transcription system in order to obtain phonetic labels for the voice signal.
This information, orthogonal to the main task, must be able to disentangle contradictory signals to improve speaker representation [Zeghidour et al., 2016].
Active learning is a semi-supervised model where an oracle (generally a human) participates in the learning process.
More precisely, starting from non-annotated data, the learning algorithm needs to determine what has to be annotated by the oracle, so as to get the best performance at the least cost [Lowell et al., 2018, Drugman et al., 2019, Feyisetan et al., 2019, Settles and Craven, 2008, Duong et al., 2018, Kholghi et al., 2016,Settles, 2009].
As healthcare informations systems evolve, practitioners are required to enter more and more information digitally through a variety of specialized software.
Handling these systems always involves a training and adaptation phase that requires immediate acces to a workstation, which takes time on their assignment time.
Additionally, these constraints greatly modify the work habits of physicians who are used to prescribing drugs directly on a blank page or by using a prescription writing software.
The objective of this thesis is to free prescribers from a certain number of constraints, and to offer them a tool allowing them to get as close as possible to the most "natural" and "cultural" way of prescribing medicine in a near-natural language.
In order to address this problem, we position ourselves in a vocal interaction context on smartphone which makes it possible treat two observed problems: unnatural interactions with prescription writing softwares and the lack of immediate access to a connected computer.
Recommendation systems try to infer their users' interests in order to suggest items relevant to them.
These systems thus offer a valuable service to users in that they automatically filter non-relevant information, which avoids the nowadays common issue of information overload.
This is why recommendation systems are now popular, if not pervasive in some domains such as the World Wide Web.
However, an individual's interests are personal and private data, such as one's political or religious orientation.
Therefore, recommendation systems gather private data and their widespread use calls for privacy-preserving mechanisms.
In this thesis, we study the privacy of users' interests in the family of recommendation systems called Collaborative Filtering (CF) ones.
Our first contribution is Hide &amp; Share, a novel privacy-preserving similarity mechanism for the decentralized computation of K-Nearest-Neighbor (KNN) graphs.
It is a lightweight mechanism designed for decentralized (a.k.a. peer-to-peer) user-based CF systems, which rely on KNN graphs to provide recommendations.
Our second contribution also applies to user-based CF systems, though it is independent of their architecture.
This contribution is two-fold: first we evaluate the impact of an active Sybil attack on the privacy of a target user's profile of interests, and second we propose a counter-measure.
This counter-measure is 2-step, a novel similarity metric combining a good precision, in turn allowing for good recommendations,with high resilience to said Sybil attack.
The Center for Data Science of the University of Paris-Saclay deployed a platform compatible with Linked Data in 2016.
Because researchers face many difficulties utilizing these technologies, an approach and then a platform we call LinkedWiki were designed and tested over the university's cloud (IAAS) to enable the creation of modular virtual search environments (VREs) compatible with Linked Data.
We are thus able to offer researchers a means to discover, produce and reuse the research data available within the Linked Open Data, i.e., the global information system emerging at the scale of the internet.
This experience enabled us to demonstrate that the operational use of Linked Data within a university is perfectly possible with this approach.
However, some problems persist, such as (i) the respect of protocols and (ii) the lack of adapted tools to interrogate the Linked Open Data with SPARQL.
We propose solutions to both these problems.
In order to be able to verify the respect of a SPARQL protocol within the Linked Data of a university, we have created the SPARQL Score indicator which evaluates the compliance of the SPARQL services before their deployments in a university's information system.
In addition, to help researchers interrogate the LOD, we implemented a SPARQLets-Finder, a demonstrator which shows that it is possible to facilitate the design of SPARQL queries using autocompletion tools without prior knowledge of the RDF schemas within the LOD.
Among the designations that flourish in the economic press through the attractive name of «new business models» (in English in the text), two of them are specifically based on property sharing: «functional economy» and «sharing economy».
They both connect technological innovations with the evolution of social practices.
These approaches intend to take advantage of a contemporary transformation of consumption patterns, characterized by a desecration of the role given to material goods.
Our research focuses on the construction and meaning of several socio-economic models, in principle aiming to foster sustainable development.
Although the multiplication of designations muddles the definitions of the models, each of them is connected to its own network of actors.
Even if the French translations for «functional economy», «économie de fonctionnalité» and «économie de la fonctionnalité», are distinguished only by a definite article, they refer to two contradictory approaches.
Similarly, while the term «sharing economy» firstly evokes the «peer-to-peer» (in English in the text) banner, it rapidly spreads to describe a form of connexionist capitalism.
The deployment of the studied models makes it possible to capture certain transformations of contemporary representations.
The relative success of the models depends on the correspondence of the ideals attached to each of them and the socio-economic facts experienced by actors.
Evolution of work, changes in the outlines of property or distrust towards the political class are revealed by the analysis of discourses related to our topics.
This thesis has been carried out in the field of information and communication sciences and focuses on emotion in information retrieval in health fora.
The success of these systems results from an informational and emotional motivation from the participants who can access testimonials, punctual information or medical information filtered through the experience of the patients who are writing.
The messages often have emotional content.
This thesis focuses on developments in information activities and especially on the role that emotional has in the structuring and evaluation of information.
The aim of the first analysis is to highlight the organization of messages and their emotional content through a corpus analysis (from different threads of different French-language health fora).
A second study focuses on the analysis of data collected during some interviews regarding the use of health fora and the way in which participants evaluate information.
The results show that medical information is very present and mostly interspersed with emotion of fear.
However, the joy is the emotion mostly present in all the collected corpus.
Finally, if the evaluation criteria markers are emotion, it appears that medical information are also evaluation criteria and not the information evaluated.
This thesis addresses the problem of learning with non-modular losses.
In a prediction problem where multiple outputs are predicted simultaneously, viewing the outcome as a joint set prediction is essential so as to better incorporate real-world circumstances.
In empirical risk minimization, we aim at minimizing an empirical sum over losses incurred on the finite sample with some loss function that penalizes on the prediction given the ground truth.
In this thesis, we propose tractable and efficient methods for dealing with non-modular loss functions with correctness and scalability validated by empirical results.
We then introduce an alternating direction method of multipliers (ADMM) based decomposition method for loss augmented inference, that only depends on two individual solvers for the loss function term and for the inference term as two independent subproblems.
Second, we propose a novel surrogate loss function for submodular losses, the Lovász hinge, which leads to O(p log p) complexity with O(p) oracle accesses to the loss function to compute a subgradient or cutting-plane.
Finally, we introduce a novel convex surrogate operator for general non-modular loss functions, which provides for the first time a tractable solution for loss functions that are neither supermodular nor submodular.
This surrogate is based on a canonical submodular-supermodular decomposition.
The work presented in this thesis is part of a broader issue of study and design of MOOCs (Massive Open Online Courses).
It focuses more particularly on the didactic study of an algorithmic MOOC designed for undergraduate students at Hassan First University (Morocco).
This work is part of a comprehensive approach and aims more specifically to understand the process of developing algorithmic content conveyed by the MOOC and how learners construct basic knowledge essential to the course.
Considering MOOC as a didactic device, two approaches: didactic and epistemological of algorithmic have been articulated.
The notion of didactic performance is mobilized to examine the learning strategies adopted by students.
By using discussion forums and mobilizing a questionnaire and semi-structured interviews, the discourses of students were analysed in order to characterize the constructed content, the didactic performance and the difficulties encountered by students.
The characterization of the design of the MOOC highlights two steps:
1) identification of the essential concepts in algorithmics:variable, basic instructions, conditions, loops and their organization into learning units
2) development of a pedagogical scenario by describing the learning tasks of the pedagogical units and their organisation, and, on the other hand, that the course is also adapted to the massification of audiences, in particular by decreasing the hourly workload and demanding few prerequisites.
The results show that students constructed two types of content: conceptual knowledge (condition, loop) and procedural knowledge (analysis of a problem, decomposition of a problem, etc.). Students showed more interest in cognitive and technical didactic performances to build, step by step, content.
More specifically, students constructed algorithmic content by making greater use of these learning strategies: 1) elaboration strategies (linking the content with previous knowledge) and organization strategies such as the use of flowcharts 2) technical strategies in terms of mobilizing MOOC videos.
The results also show that although students have been particularly successful in analysing problems (determination of input and output objects), some difficulties remain, such as passing from analysing problems to elaborating algorithms.
These results can not only provide MOOC instructional designers with the necessary elements for content development, but also improve didactic research on MOOCs by providing researchers with elements for the study of MOOCs, taking into account the specificity of their content.
The present thesis locates itself in the interdisciplinary field of computational stylistics, namely the application of statistical and computational methods to the study of literary style.
Historically, most of the work done in computational stylistics has been focused on lexical aspects especially in the early decades of the discipline.
However, in this thesis, our focus is put on the syntactic aspect of style which is quite much harder to capture and to analyze given its abstract nature.
As main contribution, we work on an approach to the computational stylistic study of classic French literary texts based on a hermeneutic point of view, in which discovering interesting linguistic patterns is done without any prior knowledge.
Following the hermeneutic line of thought, we propose a knowledge discovery process for the stylistic characterization with an emphasis on the syntactic dimension of style by extracting relevant patterns from a given text.
This knowledge discovery process consists of two main steps, a sequential pattern mining step followed by the application of some interestingness measures.
In particular, the extraction of all possible syntactic patterns of a given length is proposed as a particularly useful way to extract interesting features in an exploratory scenario.
We propose, carry out an experimental evaluation and report results on three proposed interestingness measures, each of which is based on a different theoretical linguistic and statistical backgrounds.
This dissertation is a contribution to the description of Kmhmouʔ, a language with an oral tradition spoken in Laos.
It presents the general characteristics of this language, which is among the less described languages of the region of Southeast Asia, as shown by the bibliography.
Kmhmouʔ, an isolating language, does not exhibit any grammatical morphology.
Words are mostly multi-categorial as regards parts of speech, and multi-functional: the verb-noun distinction is based essentially on combinatory properties of words.
This grammatical description is based on spontaneous data collected during fieldwork in Kmhmouʔ villages and from the Lao national radio (Kmhmouʔ broadcast).
The fieldwork was conducted in Kmhmouʔ, and the analysis and interpretation of the corpus benefited from the fact that the author is native speaker of Kmhmouʔ.
This thesis, besides making data and grammar of the Eastern Kmhmouʔ dialect available for the first time, opens up some new and challenging paths for the study of the typology of isolating languages and transcategoriality, as well as for studies of the role of language contact in grammaticalization.
In recent years, security in Information Systems (IS) has become an important issue that needs to be taken into account in all stages of IS development, including the early phase of Requirement Engineering (RE).
Considering security during early stages of IS development allows IS developers to envisage threats, their consequences and countermeasures before a system is in place.
Security requirements are known to be “the most difficult of requirements types”, and potentially the ones causing the greatest risk if they are not correct.
Moreover, requirements engineers are not primarily interested in, or knowledgeable about, security.
Their tacit knowledge about security and their primitive knowledge about the domain for which they elicit security requirements make the resulting security requirements poor and too generic.
This thesis explores the approach of eliciting requirements based on the reuse of explicit knowledge.
First, the thesis proposes an extensive systematic mapping study of the literature on the reuse of knowledge in security requirements engineering identifying the different knowledge forms.
This is followed by a review and classification of security ontologies as the main reuse form.
In the second part, AMAN-DA is presented.
AMAN-DA is the method developed in this thesis.
It allows the elicitation of domain-specific security requirements of an information system by reusing knowledge encapsulated in domain and security ontologies.
Besides that, the thesis presents the different elements of AMANDA: (I) a core security ontology, (II) a multi-level domain ontology, (III) security goals and requirements'syntactic models, (IV) a set of rules and mechanisms necessary to explore and reuse the encapsulated knowledge of the ontologies and produce security requirements specifications.
The last part reports the evaluation of the method.
AMAN-DA was implemented in a prototype tool.
Its feasibility was evaluated and applied in case studies of three different domains (maritime, web applications, and sales).
The ease of use and the usability of the method and its tool were also evaluated in a controlled experiment.
The experiment revealed that the method is beneficial for the elicitation of domain specific security requirements, and that the tool is friendly and easy to use.
The development and multiplication of information systems and platforms for information access has been accentuated over the past thirty years.
The large volume of information available has raised many scientific challenges in different areas such as information retrieval.
To access documents grouped in a digital corpus, one must be able to express his/her information need, often in the form of a query, to associate the relevant documents and present them in the best possible way to users.
Document research in a thematic digital corpus presenting a high level of technicality in the concerned discipline can be considered as a browsing process driven by some information needs.
Such browses requires the use of traditional information retrieval tools to select relevant documents based on a query But they can be improved by the use of customization and adaptation mechanisms in order to refine the representation of information needs according to the specificities of a user, his current browsing or the corpus considered.
Indeed, access to digital documents raises problems related to the search for information, the visualization of the results of a query and the browsing between the documents.
The process of information retrieval requires to be improved and especially by the integration of the user as a main factor to take into account in the search for satisfaction of his/her information needs.
We consider several approaches to help users in their search for documents.
A first assistance concerns the reformulation of queries by targeting an audience of users unfamiliar with the technical terms of the field and struggling to express in the form of a query their need.
The second approach that we propose is not to consider the user in isolation but to bring it closer to those who have expressed similar research to find the documents they considered relevant.
Finally, we include works from the field of the recommendation to better understand the informational needs of the user and help them find what they are looking for by recommending documentary resources.
In this thesis, we propose to treat this diversity of influence by a multi-agent system interacting with a shared environment representing the users browsing so that the system may be adapted to use either assistance facilities according to the user's expertise.
We applied our work for document research in a digital corpus of legal documents.
Robots are more and more used in a social context.
They are required not only to share physical space with humans but also to interact with them.
In this context, the robot is expected to understand some verbal and non-verbal ambiguous cues, constantly used in a natural human interaction.
In particular,knowing who or what people are looking at is a very valuable information to understand each individual mental state as well as the interaction dynamics.
It is called Visual Focus of Attention or VFOA.
In this thesis, we are interested in using the inputs from an active humanoid robot – participating in a social interaction – to estimate who is looking at whom or what.
On the one hand, we want the robot to look at people, so it can extract meaningful visual information from its video camera.
We propose a novel reinforcement learning method for robotic gaze control.
The model is based on a recurrent neural network architecture.
The robot autonomously learns as trategy for moving its head (and camera) using audio-visual inputs.
It is able to focus on groups of people in a changing environment.
On the other hand, information from the video camera images are used to infer the VFOAs of people along time.
We estimate the 3D head poses (location and orientation) for each face, as it is highly correlated with the gaze direction.
We use it in two tasks.
First, we note that objects may be looked at while not being visible from the robot point of view.
Under the assumption that objects of interest are being looked at, we propose to estimate their locations relying solely on the gaze direction of visible people.
We formulate an ad hoc spatial representation based on probability heatmaps.
We design several convolutional neural network models and train them to perform a regression from the space of head poses to the space of object locations.
In this context, we introduce a Bayesian probabilistic model, inspired from psychophysics, that describes the dependency between head poses, object locations, eye-gaze directions, and VFOAs, along time.
The formulation is based on a switching state-space Markov model.
A specific filtering procedure is detailed to infer the VFOAs, as well as an adapted training algorithm.
The proposed contributions use data-driven approaches, and are addressed within the context of machine learning.
All methods have been tested on publicly available datasets.
Some training procedures additionally require to simulate synthetic scenarios; the generation process is then explicitly detailed.
The acquisition of attributive adjectives is subject to two major difficulties in French.
First, adjectives express a property of a larger unit: children must be able to conceive an object as a whole and as a set of properties to use a NP with an adjective.
Also, although speakers are aware of this possibility, they tend to choose a fixed position in usage.
These facts raise the question of whether the input allows children to construct the notion of attributive adjectives without a resort to innate linguistic knowledge.
To answer this, I propose a comparative study of the productions of three children interacting with their family at two times of their development (T1: 3 ;8, T2: 4 ;6).
I examine four phenomena concerning attributive adjectives (lexicon, placement, combination with other modifiers or adjectival dependents), and I compare adjectives with other nominal modifiers.
All of these phenomena show the same evolution.
At T1, the children use the most frequent construction in the adult data, with a high degree of lexical specificity.
T2 shows the appearance of other constructions according to their order of frequency in the adult data.
The construction from T1 is also used with a greater choice of lexical units, but within the same semantic classes.
The children thus show sensitivity to quantitative information and a gradual abstraction of constructions by semantic analogy, which pleads for a progressive construction of the knowledge of adjectives based on the input data.
Our society has been rapidly growing its presence on the Web, as a consequence we are digitizing a large collection of our daily happenings.
In this scenario, the Web receives virtual occurrences of various events corresponding to their real world occurrences from all around the world.
Scale of these events can vary from locally relevant ones up to those that receive global attention.
News and social media of current times provide all essential means to reach almost a global diffusion.
This big data of complex societal events provide a platform to many research opportunities for analyzing and gaining insights into the state of our society.
In this thesis, we investigate a variety of social event impact analytics tasks.
Specifically, we address three facets in the context of events and the Web, namely, diffusion of events in foreign languages communities, automated classification of Web contents, and news virality assessment and visualization.
We hypothesize that the named entities associated with an event or a Web content carry valuable semantic information, which can be exploited to build accurate prediction models.
We have shown with the help of multiple studies that raising Web contents to the entity-level captures their core essence, and thus, provides a variety of benefits in achieving better performance in diverse tasks.
We report novel findings over disparate tasks in an attempt to fulfill our overall goal on societal event impact analytics.
Machine learning is the study of designing algorithms that learn from training data to achieve a specific task.
The resulting model is then used to predict over new (unseen) data points without any outside help.
This data can be of many forms such as images (matrix of pixels), signals (sounds,...), transactions (age,amount, merchant,...), logs (time, alerts,...).
Datasets may be defined to address a specific task such as object recognition, voice identification, anomaly detection,etc.
In these tasks, the knowledge of the expected outputs encourages a supervised learning approach where every single observed data is assigned to a label that defines what the model predictions should be.
For example, in object recognition,an image could be associated with the label "car" which suggests that the learning algorithm has to learn that a car is contained in this picture, somewhere.
This is in contrast with unsupervised learning where the task at hand does not have explicit labels.
For example, one popular topic in unsupervised learning is to discover underlying structures contained in visual data (images) such as geometric forms of objects, lines, depth, before learning a specific task.
This kind of learning is obviously much harder as there might be potentially an infinite number of concepts to grasp in the data.
In this thesis, we focus on a specific scenario of the supervised learning setting: 1) the label of interest is under represented (e.g. anomalies) and 2) the dataset increases with time as we receive data from real-life events (e.g. credit card transactions).
In fact, these settings are very common in the industrial domain in which this thesis takes place.
The development of consumer robotics comes with a new kind of telecommunications systems: telepresence robots.
These are mobile robots representing a person who is able to control their movements remotely.
The aim is not only to allow remote communication, but to create a sense of social and physical presence, which are not sufficiently transmitted by telephone or videoconferencing.
In this context, it is especially important to ensure that the users' « social touch » is well transmitted, meaning that they are able to exchange a wide range of socio-affective signals, which are the vectors of social links.
In particular, this thesis deals with a key element of social touch, which is deeply impacted by telepresence: vocal earshot, by which speakers are normally able to control who can hear them, and to adapt to varying acoustic environment conditions.
In a first study, we will explore the link between vocal touch and proxemics, by asking whether a blind listener's spatial perception of an interlocutor can be influenced by the expressed socio-affects.
We will then show that vocal earshot can be modified by the Lombard effect in ubiquitous telepresence, because the pilot is perceiving both the local and remote environments at the same time, and therefore adapts to noise, even if it is not noticeable by the interlocutors.
Lastly, we will present our participation in an Arts-Sciences performance called Aporia, during which a unique actor embodies different characters, helped by a voice transforming algorithm.
Many measurement methods have been developed to assess the user experience, but remain immature and even contradictory.
This is why studies should be conducted in order to increase the validity and reliability of this new approach.
The main aim of this research is to use several methods and to combine and articulate them using various techniques to improve the measurement quality.
These studies was based on a broad spectrum of indicators (physiological, behavioral and self-reported) and two triangulation strategies in particular: multi-faceted and multi-measures.
Finally, these methods were tested in real applied cases and under an increasing procedure complexity and statistical processing.
This research has resulted in three separate studies.
The first aims to evaluate the relevance of a movie recommendation algorithm against its competitor using a multifaceted evaluation strategy.
A second study was designed to test the relevance of a multi-measures evaluation model by assessing the usability of university sites based on a remote user testing software (Evalyzer) and a multimodal combination of various indicators of usability.
A final study was conducted to validate multi-measures immersion protocol (questionnaire, facial expression, skin conductance, heart rate, eye behavior).
These three studies have highlighted the relevance of numerous measures (of usability and user experience), the added value of some of their combinations, as well as a critical return to the multi-facet validation procedure used in this thesis
Nowadays, there are many geographic databases, (GDB), covering the same reality.
The geographical data are represented differently (for example a river can be represented by a line or a polygon), they are used in different applications (visualisation, analysis) and they are created using various modes of acquisition (sources, processes).
All these factors create independence between GDB, which causes problems for both producers and users.
Thus, a solution is to clarify the relationships between various database objects, i.e. to match homologous objects, which represent the same reality.
Because of the complexity of the matching process, the existing approaches depend on the types of data (points, lines or polygons) and the level of detail of the GDB.
We realised, that most of the approaches are based on the geometry and the topology of the geographical objects, and very few approaches take into account the descriptive information of geographical objects.
Besides, for most approaches, the criteria are applied one after the other and knowledge is contained within the process.
Following this analysis, we proposed a matching approach that is guided by knowledge and takes into account all criteria at the same time exploiting the geometry, descriptive information and relations between geographical objects.
In order to formalise knowledge and model their imperfections (imprecision, uncertainty and incompleteness), we used the Belief Theory [Shafer, 1976].
After a selection of candidates, the masses of beliefs are initialised by analysing each candidate separately from the others using different knowledge expressed by various matching criteria.
Then, the matching criteria and candidates are fusioned.
Finally, a decision is taken.
Our approach has been tested on real data having different levels of detail and representing relief (data points) and road networks (linear data)
The work of this thesis concerns the modeling of emotions for expressive audiovisual textto-speech synthesis.
Today, the results of text-to-speech synthesis systems are of good quality, however audiovisual synthesis remains an open issue and expressive synthesis is even less studied.
As part of this thesis, we present an emotions modeling method which is malleable and flexible, and allows us to mix emotions as we mix shades on a palette of colors.
In the first part, we present and study two expressive corpora that we have built.
The recording strategy and the expressive content of these corpora are analyzed to validate their use for the purpose of audiovisual speech synthesis.
In the second part, we present two neural architectures for speech synthesis.
We used these two architectures to model three aspects of speech: 1) the duration of sounds, 2) the acoustic modality and 3) the visual modality.
First, we use a fully connected architecture.
This architecture allowed us to study the behavior of neural networks when dealing with different contextual and linguistic descriptors.
We were also able to analyze, with objective measures, the network's ability to model emotions.
The second neural architecture proposed is a variational auto-encoder.
This architecture is able to learn a latent representation of emotions without using emotion labels.
After analyzing the latent space of emotions, we presented a procedure for structuring it in order to move from a discrete representation of emotions to a continuous one.
We were able to validate, through perceptual experiments, the ability of our system to generate emotions, nuances of emotions and mixtures of emotions, and this for expressive audiovisual text-to-speech synthesis.
Nowadays, information related on displacement and mobility in a transport network represents certainly a significant potential.
So, this work aims to modeling, to optimize and to implement an Information System of Services to Aid the Urban Mobility (ISSAUM).
The ISSAUM has firstly to decompose each set of simultaneous requests into a set of sub-requests called tasks.
Each task corresponds to a service which can be proposed different by several information providers with different.
The dynamic and distributed aspects of the problem incite us to adopt a multi-agent approach to ensure a continual evolution and a pragmatic flexibility of the system.
So, we proposed to automate the modeling of services by using ontology idea.
Our ISSAUM takes into account possible disturbance through the ETMN.
In order to satisfy user requests, we developed a negotiation protocol between our system agents.
The proposed ontology mapping negotiation model based on the knowledge management system for supporting the semantic heterogeneity and it organized as follow: Negotiation Layer (NL), the Semantic Layer (SEL), and the Knowledge Management Systems Layer(KMSL).
We detailed also the reassignment process by using Dynamic Reassigned Tasks (DRT) algorithm supporting by ontology mapping approach.
Finally, the experimental results presented in this thesis, justify the using of the ontology solution in our system and its role in the negotiation process
In this thesis we shed the light on the danger of privacy leakage on social network.
We investigate privacy breaches, design attacks, show their feasibility and study their accuracies.
This approach helps us to track the origin of threats and is a first step toward designing effective countermeasures.
We have first introduced a subject sensitivity measure through a questionnaire survey.
Then, we have designed on-line friendship and group membership link disclosure (with certainty) attacks on the largest social network “Facebook”.
These attacks successfully uncover the local network of a target using only legitimate queries.
We have also designed sampling techniques to rapidly collect useful data around a target.
The collected data are represented by social-attribute networks and used to perform attribute inference (with uncertainty) attacks.
To increase the accuracy of attacks, we have designed cleansing algorithms.
These algorithms quantify the correlation between subjects, select the most relevant ones and combat data sparsity.
Finally, we have used a shallow neural network to classify the data and infer the secret values of a sensitive attribute of a given target with high accuracy measured by AUC on real datasets.
The proposed algorithms in this work are included in a system called SONSAI that can help end users analyzing their local network to take the hand over their privacy
When developing a software, maintenance and evolution represents an important part of the development's life-cycle, making up to 80% of the overall cost and effort.
During the maintenance effort, it happens that developers have to resort to copying and pasting source code fragments in order to reuse them.
Such practice, seemingly harmless is more frequent than we expect.
Commonly referred to as ``clones''in the literature, these source code duplicates are a well-known and studied topic in software engineering.
In this thesis, we aim at shedding some light on copy-paste practices on software artifacts.
In particular, we chose to focus our contributions on two specific types of software artifacts: API documentation and build files (i.e. Dockerfiles).
For both contributions, we follow a common empirical study methodology.
First, We show that API documentations and software build files (i.e. Dockerfiles) actually face duplicates issues and that such duplicates are frequent.
Secondly, we identify the reasons behind the existence of such duplicates.
Finally, We show that both software artifacts lack reuse mechanisms to cope with duplicates, and that some developers even resort to ad-hoc tools to manage them.
This document proposes to learn the behaviour of the dialogue manager of a spoken dialogue system from a set of rated dialogues.
This learning is performed through reinforcement learning.
Our method does not require the definition of a representation of the state space nor a reward function.
These two high-level parameters are learnt from the corpus of rated dialogues.
It is shown that the spoken dialogue designer can optimise dialogue management by simply defining the dialogue logic and a criterion to maximise (e.g user satisfaction).
The methodology suggested in this thesis first considers the dialogue parameters that are necessary to compute a representation of the state space relevant for the criterion to be maximized.
For instance, if the chosen criterion is user satisfaction then it is important to account for parameters such as dialogue duration and the average speech recognition confidence score.
The state space is represented as a sparse distributed memory.
The Genetic Sparse Distributed Memory for Reinforcement Learning (GSDMRL) accommodates many dialogue parameters and selects the parameters which are the most important for learning through genetic evolution.
The resulting state space and the policy learnt on it are easily interpretable by the system designer.
These two algorithms consider the criterion to be the return for the entire dialogue.
These functions are discussed and compared on simulated dialogues and it is shown that the resulting functions enable faster learning than using the criterion directly as the final reward.
A spoken dialogue system for appointment scheduling was designed during this thesis, based on previous systems, and a corpus of rated dialogues with this system were collected.
This corpus illustrates the scaling capability of the state space representation and is a good example of an industrial spoken dialogue system upon which the methodology could be applied
Given the rapid emergence of new mobile technologies and the growth of needs of a moving society in training, works are increasing to identify new relevant educational platforms to improve distant learning.
The next step in distance learning is porting e-learning to mobile systems.
So far, learning environment was either defined by an educational setting, or imposed by the educational content.
In our approach, in m-learning, we change the paradigm where the system recommends content and adapts learning follow to learner's context.
A design platform (DP) is a total solution to build a System-On-Chip (SOC).
DP consists of a set of libraries/IPs, CAD tools and design kits in conformity with the supported design flows and methodologies.
The DP specifications provide a wide range of information from technology parameters like Process-Voltage-Temperature (PVT) corners to CAD tools' information for library/IP development.
However, the library/IP developers have difficulties in obtaining the desired data from the existing specifications due to their informality and complexity.
In this thesis, we propose methodologies, flows and tools to formalize the DP specifications for their unification and to deal with it.
The proposed description is targeting to be used as a reference to generate and validate libraries (standard cells, I/O, memory) as well as complex IPs (PLL, Serdes, etc.).
Furthermore, we introduce a reference-based method to create a reliable specification in LDSpecX and task-based keywords to efficiently extract data from it.
On the basis of the proposed solutions, we develop a specification platform.
Experimentally, we develop a standard cell library from the specification creation to library validation by using the specification platform.
We show that our approach enables to create a complete and consistent specification with a considerable reduction in time.
It also bridges the gap between the specification and current automatic system for rapid library/IP development.
All the work in this thesis has been developed in the context of the Cherenkov Telescope Array (CTA), which is going to be the major next-generation observatory for ground-based very-high-energy gamma-ray astronomy.
The plan for this work is to use GPUs and Cloud Computing in order to speed up the computing demanding tasks, developing and optimizing data analysis pipelines.
The thesis consists on two main parts: the first one is dedicated to the estimation of the future performances of CTA towards the observation of violent phenomena such as those generating Gamma Ray Bursts and Gravitational Waves, with a initial work done for the creation of the models for the First CTA Data Challenge.
The second part of the thesis is related to the development of the pipelines for the reconstruction of the low-level data coming from the Monte Carlo simulations using the software library called ctapipe.
In chapter 1 I go into the details of the CTA project, the telescopes and the performances of the array, together with the methods used to derive them from Monte Carlo simulations.
More than 500 AGNs have been modelled for CTA. This Challenge has been important both to involve more people in the analysis of CTA data and to compute the observation time needed by the different KSP.
The simulations for the gravitational waves and gamma-ray bursts Consortium papers have been created with the ctools_pipe pipeline (presented in chapter 4), implemented around the libraries ctools and gammalib.
The pipeline is composed of two main parts: the task to be executed (background simulation, model creation and detection) and in which computing centre.
The second part of the thesis is focused on the development and optimization of the analysis pipelines to be used for the event reconstruction of simulated raw data and for the visualization of the events in a 3D space.
This analyses have been performed using ctapipe, a framework for prototyping the low-level data processing algorithms for CTA.
The structure of the library is presented in chapter 5 together a focus on the reconstruction methods that are implemented in ctapipe, including the so called ImPACT.
This method uses a template of images created from the Monte Carlo simulations and a seed from the standard reconstruction method to fit between the templates to find a better estimation of the shower parameters.
The time profiling and the strategies adopted to optimize the ImPACT pipeline are presented in chapter 6.
The implementation of the a pipeline for the analysis of the Large Size Telescope observing in monoscopic mode and its GPU implementation with PyTorch is also presented.
ctapipe has also been used and developed to estimate the performances of CTA when observing using the “divergent pointing” mode, in which the pointing directions are slightly different with respect to the parallel pointing mode, so that the final hyper field-of-view of all the telescopes is larger with respect to the parallel pointing mode.
The angular and energy resolutions and also the sensitivity are worse in this scenario, but having a wider hyper field-of-view can be good for other topics, such are searching for transient sources.
The modifications to the reconstruction code introduced in ctapipe and some angular resolution plots for the simulated point source gammas are presented in chapter 7.The results presented in this thesis are a demonstration of the usage of advanced software techniques in very high energy astrophysics.
Adaptive discretizations are important in compressible/incompressible flow problems since it is often necessary to resolve details on multiple levels, allowing large regions of space to be modeled using a reduced number of degrees of freedom (reducing the computational time).
There are a wide variety of methods for adaptively discretizing space, but Cartesian grids have often outperformed them even at high resolutions due to their simple and accurate numerical stencils and their superior parallel performances.
Such performance and simplicity are in general obtained applying a finite-difference scheme for the resolution of the problems involved, but this discretization approach does not present, by contrast, an easy adapting path.
In a finite-volume scheme, instead, we can incorporate different types of grids, more suitable for adaptive refinements, increasing the complexity on the stencils and getting a greater flexibility.
The Laplace operator is an essential building block of the Navier-Stokes equations, a model that governs fluid flows, but it occurs also in differential equations that describe many other physical phenomena, such as electric and gravitational potentials, and quantum mechanics.
So, it is a very important differential operator, and all the studies carried out on it, prove its relevance.
In this work will be presented 2D finite-difference and finite-volume approaches to solve the Laplacian operator, applying patches of overlapping grids where amore fined level is needed, leaving coarser meshes in the rest of the computational domain.
These overlapping grids will have generic quadrilateral shapes.
Specifically, the topics covered will be: 1) introduction to the finite difference method, finite volume method, domain partitioning, solution approximation; 2) overview of different types of meshes to represent in a discrete way the geometry involved in a problem, with a focus on the octree data structure, presenting PABLO and PABLitO.
The first one is an external library used to manage each single grid's creation, load balancing and internal communications, while the second one is the Python API of that library written ad hoc for the current project;
3) presentation of the algorithm used to communicate data between meshes (being all of them unaware of each other's existence) using MPI inter-communicators and clarification of the monolithic approach applied building the final matrix for the system to solve, taking into account diagonal, restriction and prolongation blocks;
4) presentation of some results; conclusions, references.
It is important to underline that everything is done under Python as programming framework, using Cython for the writing of PABLitO, MPI4Py for the communications between grids, PETSc4py for the assembling and resolution parts of the system of unknowns, NumPy for contiguous memory buffer objects.
The choice of this programming language has been made because Python, easy to learn and understand, is today a significant contender for the numerical computing and HPC ecosystem, thanks to its clean style, its packages, its compilers and, why not, its specific architecture optimized versions.
Biological invasions contribute to the degradation of biodiversity globally.
Invasive alien plants have impacted on natural resources management and have generated substantial costs of control and economic loss.
Various management options have been put in place to control the level of invasions of targeted species.
The public's perception of invasive species varies among stakeholders.
Controversies and conflicts emerged as a consequence of diverging opinions on the management of invasions.
We conducted an inter-disciplinary study on the socio-ecological and economic dimensions related to the management of the invasive Rubus alceifolius, following a biological control programme in Réunion Island (France).
Firstly, we carried out an economic analysis of the management options for R. alceifolius with future scenario on the cost of invasion.
Secondly we assessed the impact of the recovery of native species post biological control.
We found that the biological control programme of R. alceifolius was successful within the elevation limit of 800 m, from both an economic and ecological perspective.
Given the shortfall in the decision-making process and implementation, this study demonstrated the crucial need to identify and involve stakeholders in all stages of a biological control programme.
We concluded with key recommendations for successful biological programmes.
In information retrieval tasks from image databases where content representation is based on graphs, the evaluation of similarity is based both on the appearance of spatial entities and on their mutual relationships.
In this thesis we present a novel scheme of Attributed Relational Adjacency Graphs representation and mining, which has been applied in content-based retrieval of comic images.
We propose a graph representation that yields stable graphs and allow to retain high-level and structural information of objects of interest in comic images.
Next, we extend the indexing and matching problem to graph structures representing the comic image, and apply it to the problem of retrieval.
The graphs in the graph database representing the whole comic volume are then mined for frequent patterns (or frequent substructures).
This step is to overcome the non-repeatability problem caused by the unavoidable errors introduced into the graph structure during the graph construction stage, which ultimately create a semantic gap between the graph and the content of the comic image.
Experiments of performance measures is addressed to evaluate the performance of this CBIR system.
We study defferent types of data as a stream:
Databases
Social Networks
Text data.
For a database that follows a relational schema, an OLAP (Online Analytical Processing) analysis schema defines one of the database tables as an analysis table.
We suppose that the tuples of the analysis table arrive as a stream.
We study the approximation of OLAP queries, by sampling with weights the tuples of the stream without storing the analysis data. We give a preference model in this context.
For the social network Twitter, we observe a stream of tweets that contain any given tag and transform it into a stream of edges of a graph.
We study the existence of large clusters in the generated graph.
We propose a uniform sampling method that will associate a random subgraph of the graph and study the giant components of this random subgraph as a witness of the large clusters of the original graph.
For a stream of text, we consider the pairs of words in a lemmatized sentence as edges of a graph where the nodes are the words.
We transform the stream of text into a stream of edges.
We sample the edges proportionally to the Word2vec similarity of the words.
We then analyze the giant components.
We extend the classical Word2vec vectors, in order to take into account the morphology of the words, i.e. the prefixes, roots and suffixes.
This study is based on the teaching model of reading French as a foreign language by university students who are beginners in this language in Brazil.
Regardless of its dissemination among Latin America students and of its continuous reformulation to account for new learning situations, this teaching method contains some gaps.
The gaps refer to a critical issue for a large number of readers: the lack of lexical knowledge.
This issue relates not only to a lack of vocabulary knowledge, but also to the ability of word learning.
In this study, this question has been further studied based on two experimental researches.
The first one, using a more prospective approach, aims at identifying the role of the dictionary–especially bilingual dictionaries–in the reading process and the effects of dictionary use in the construction of meaning.
The second one compares the use of two lexicographic instruments: a bilingual dictionary and a pedagogical dictionary for learners of French as a foreign language.
Both researches yield important information about the integration of a lexical study with this teaching model; additionally, they are complemented by the analysis of the dictionaries available to this target audience.
In this way, we can set methodological principles to build a pedagogical dictionary that helps students in reading activities and that supports vocabulary acquisition on the basis of functional lexicography.
Our goal in this thesis is to build a system that answers a natural language question (NL) by representing its semantics as a logical form (LF) and then computing the answer by executing the LF over a knowledge base.
The core part of such a system is the semantic parser that maps questions to logical forms.
Our focus is how to build high-performance semantic parsers by learning from (NL, LF) pairs.
We propose to combine recurrent neural networks (RNNs) with symbolic prior knowledge expressed through context-free grammars (CFGs) and automata.
By integrating CFGs over LFs into the RNN training and inference processes, we guarantee that the generated logical forms are well-formed; by integrating, through weighted automata, prior knowledge over the presence of certain entities in the LF, we further enhance the performance of our models.
Experimentally, we show that our approach achieves better performance than previous semantic parsers not using neural networks as well as RNNs not informed by such prior knowledge.
Automatic short text classification is more and more used nowadays in various applications like sentiment analysis or spam detection.
We present two new approaches to improve short text classification.
Our first approach is "Semantic Forest".
The first step of this approach proposes a new enrichment method that uses an external source of enrichment built in advance.
Contrarily to the methods proposed in the literature, the second step of our approach does not use traditional learning algorithm but proposes a new one based on the semantic links among words in the Random Forest classifier.
Our second contribution is "IGLM" (Interactive Generic Learning Method).
It is a new interactive approach that recursively updates the classification model by considering the new data arriving over time and by leveraging the user intervention to correct misclassified data.
An abstraction method is then combined with the update mechanism to improve short text quality.
The experiments performed on these two methods show their efficiency and how they outperform traditional algorithms in short text classification.
Finally, the last part of the thesis concerns a complete and argued comparative study of the two proposed methods taking into account various criteria such as accuracy, speed, etc.
Among these pre-conceptions, many linguistic patterns have been said to be representative of male or female features, like tag questions, deference, turn-taking for example.
Of all the gendered linguistic characteristics, the one which may have been the most debated is that of swear words.
Because of a complex interplay between social expectations and power relations, swearing has traditionally been associated with men.
Moreover, swearing is often considered as an act of power and a way of affirming oneself.
Thus, the fact that one gender may be perceived as more frequent users of swear words, or on the other hand as swear word eschewers, may have an impact on other qualities related to power that we would inherently attribute to one gender or the other, whether these differences are real or not.
Some studies have showed that contrary to what has long been widely believed, women do not swear less frequently than men, nor do they use a drastically different register.
Some even envisioned that the use of “strong” swear words by women would increase in certain contexts, specifically on social media; this seemed especially true for younger generations of users.
It was even predicted that “gender equality in swearing or a reversal in gender patterns for strong swearing, will slowly become more widespread, at least in social network sites” (Thelwall, 2008: 102), such that the use of strong swear words among young women will eventually be more frequent than among (young) men.
Thus, the following question arises: has the prediction made by Thelwall in 2008 been fulfilled eight years later, in a society where computer-mediated communication in the context of social media is firmly rooted in people's everyday lives?
This study is based specifically on a corpus composed of just over eighteen million tweets issued by roughly 739 000 users.
The corpus was populated with tweets by British users of both genders and from different age groups throughout the United Kingdom.
Corpus linguistic methodology and tools have been used to address the sociolinguistic issues raised earlier.
Also, because Twitter does not provide us with a direct access to the gender or the age of the users, using computer-programming methods has been necessary to be able to study these age and gender differences.
We are currently experiencing an exceptional growth of visual data, for example, millions of photos are shared daily on social-networks.
Image understanding methods aim to facilitate access to this visual data in a semantically meaningful manner.
In this dissertation, we define several detailed goals which are of interest for the image understanding tasks of image classification and retrieval, which we address in three main chapters.
First, we aim to exploit the multi-modal nature of many databases, wherein documents consists of images with a form of textual description.
In order to do so we define similarities between the visual content of one document and the textual description of another document.
These similarities are computed in two steps, first we find the visually similar neighbors in the multi-modal database, and then use the textual descriptions of these neighbors to define a similarity to the textual description of any document.
Second, we introduce a series of structured image classification models, which explicitly encode pairwise label interactions.
Such an interactive scenario offers an interesting trade-off between accuracy and manual labeling effort.
We explore structured models for multi-label image classification, for attribute-based image classification, and for optimizing for specific ranking measures.
Finally, we explore k-nearest neighbors and nearest-class mean classifiers for large-scale image classification.
We propose efficient metric learning methods to improve classification performance, and use these methods to learn on a data set of more than one million training images from one thousand classes.
Since both classification methods allow for the incorporation of classes not seen during training at near-zero cost, we study their generalization performances.
We show that the nearest-class mean classification method can generalize from one thousand to ten thousand classes at negligible cost, and still perform competitively with the state-of-the-art.
Textual Entailment aims at capturing major semantic inference needs across applications in Natural Language Processing.
Since 2005, in the Textual Entailment recognition (RTE) task, systems are asked to automatically judge whether the meaning of a portion of text, the Text - T, entails the meaning of another text, the Hypothesis - H.
This thesis we focus a particular case of entailment, entailment by generality.
For us, there are various types of implication, we introduce the paradigm of Textual Entailment by Generality, which can be defined as the entailment from a specific sentence towards a more general sentence, in this context, the Text T entailment Hypothesis H, because H is more general than T.
We propose methods unsupervised language-independent for Recognizing Textual Entailment by Generality, for this we present an Informative Asymmetric Measure called the Simplified Asymmetric InfoSimba, which we combine with different asymmetric association measures to recognizingthe specific case of Textual Entailment by Generality.
This thesis, we introduce the new concept of implication, implications by generality, in consequence, the new concept of recognition implications by generality, a new direction of research in Natural Language Processing.
This PhD thesis deals with variational inference and robustness.
More precisely, it focuses on the statistical properties of variational approximations and the design of efficient algorithms for computing them in an online fashion, and investigates Maximum Mean Discrepancy based estimators as learning rules that are robust to model misspecification.
In recent years, variational inference has been extensively studied from the computational viewpoint, but only little attention has been put in the literature towards theoretical properties of variational approximations until very recently.
In this thesis, we investigate the consistency of variational approximations in various statistical models and the conditions that ensure the consistency of variational approximations.
In particular, we tackle the special case of mixture models and deep neural networks.
We also justify in theory the use of the ELBO maximization strategy, a model selection criterion that is widely used in the Variational Bayes community and is known to work well in practice.
Moreover, Bayesian inference provides an attractive online-learning framework to analyze sequential data, and offers generalization guarantees which hold even under model mismatch and with adversaries.
Unfortunately, exact Bayesian inference is rarely feasible in practice and approximation methods are usually employed, but do such methods preserve the generalization properties of Bayesian inference?
In this thesis, we show that this is indeed the case for some variational inference algorithms.
We propose new online, tempered variational algorithms and derive their generalization bounds.
Our theoretical result relies on the convexity of the variational objective, but we argue that our result should hold more generally and present empirical evidence in support of this.
Our work presents theoretical justifications in favor of online algorithms that rely on approximate Bayesian methods.
Another point that is addressed in this thesis is the design of a universal estimation procedure.
This question is of major interest, in particular because it leads to robust estimators, a very hot topic in statistics and machine learning.
We tackle the problem of universal estimation using a minimum distance estimator based on the Maximum Mean Discrepancy.
We show that the estimator is robust to both dependence and to the presence of outliers in the dataset.
We also highlight the connections that may exist with minimum distance estimators using L2-distance.
Finally, we provide a theoretical study of the stochastic gradient descent algorithm used to compute the estimator, and we support our findings with numerical simulations.
We also propose a Bayesian version of our estimator, that we study from both a theoretical and a computational points of view.
In the context of a digital culture that affects the actual audiences and potential audiences of the museum, this thesis proposes to examine the ways in which the visitor's experience can be transformed.
We wonder specifically about the appropriation and sharing processes that induce the museum practice in the era of digital culture.
To explore this research item, we build an interdisciplinary methodological device between IT and Communication Sciences.
It helps us to develop an analytical framework able to capture practices that are divided and combined between cyberspace and physical space.
In this way, we propose hybrid and exploratory research protocols that provide the ability to capture, through their association, multiple practices of the visitor's experience in a triple temporality of the before, the during and the after visit of the museum.
Through this thesis, we highlight how digital devices and culture play a part in the practices of the visitor's experience as a specific technical object and an exchange system and in which the visitor and potential visitor reconfigure their relationship to museum, its collection, exhibitions and other visitors and potential visitors.
This study is a new adaption of the systemic linguistic theory, created in 1987 by Sylviane CARDEY, where we propose a system of rules based on the linguistic facts and definitions and presented in the form of model that takes into account the principles of mathematical modelling.
The objective of this thesis is to demonstrate to foreign language learners the mechanism of the language's operation, including its grammar, by appropriate methods and computer programs carefully formulated to provide a coherent teaching style.
In this study, we propose a specific and detailed analysis for the rules of grammar, which allows us to obtain clear and comprehensive teaching material with which the learner can study the subject in its entirety, identifying its various rules and the correlation between them and between its domains.
These items will subsequently be used in a practical framework which is the teaching of grammar.
We have also the aim of building a web site accessible to teachers who might use it for their courses.
In the second chapter, after having treated the theoretical part, we apply the theory to the conjugation of the Arabic verb.
The corpus that we have chosen for this purpose consists of the 127 verbs of the ¨Bescherelle arabe¨.
In this chapter, we use our system of rules to describe the Arabic conjugation system.
The application of our theory has also highlighted some faults that we have found in various linguistic or other grammar manuals
Recording measurements about various phenomena and exchanging information about it, participate in the emergence of a type of data called time series.
A time series is characterized by numerous points and interactions can be observed between those points.
A time series is multivariate when multiple measures are recorded at each timestamp, meaning a point is, in fact, a vector of values.
Even if univariate time series, one value at each timestamp, are well-studied and defined, it's not the case of multivariate one, for which the analysis is still challenging.
None of the current techniques of classifying multivariate time series satisfies the following criteria, which are a low complexity of computation, dealing with variation in the number of points and good classification results.
In our approach, we explored a new tool, which has not been applied before for MTS classification, which is called M-histogram.
A M-histogram is a visualization tool using M axis to project the density function underlying the data.
We have employed it here to produce a new representation of the data, that allows us to bring out the interactions between dimensions.
Searching for links between dimensions correspond particularly to a part of learning techniques called multi-view learning.
A view is an extraction of dimensions of a dataset, which are of same nature or type.
Then the goal is to display the links between the dimensions inside each view in order to classify all the data, using an ensemble classifier.
So we propose a multi-view ensemble model to classify multivariate time series.
The model creates multiple M-histograms from differents groups of dimensions.
Then each view allows us to get a prediction which we can aggregate to get a final prediction.
In this thesis, we show that the proposed model allows a fast classification of multivariate time series of different sizes.
This thesis aims to demonstrate the emergence of a fourth wave of feminism in France, beginning in the early 2010s.
A wave is here defined as a cycle of feminist protest that can be attested by the convergence of three interdependent and empirically testable criteria.
The first criterion requires a noticeable growth in media interest for the women's cause over a given period.
From a qualitative research point of view, this media interest should be met with an adoption of frames favorable to gender equality.
The second criterion implies to observe a change in the ideas and / or practices of feminist movements.
These changes are the sign of a sucessful adaptation of the movements to social and technical evolutions, proving that they have managed to create a discourse which is audible in a given environment – of what the existence of the first criterion attests.
Finally, the third criterion is about generational renewal: its main interest is to historicize the analysis by confronting the emergence of a protest cycle to the evolution of the mobilizing discourse and of the conditions leading individuals to protest.
This research focuses on the integration of theatrical activities into the teaching and learning of oral expression in French as a foreign language. It also relies on the use of mobile phone and social network applications.
The research questions the potential of the theatre as a teaching tool to help develop and improve student learners of French oral expression skills, with a particular focus on pronunciation.
As part of an ethnographic action-research based on task-based learning, a blended language-learning programme combining face-to-face and distance learning was set up for French language and literature undergraduate students in Morocco.
In order to evaluate the impact of the blended language-learning programme on the development of oral language skills, learners' oral productions were analyzed through phonological accuracy and fluency measures using a pre-test and a post-test. Similar measures were used to analyze audio recordings made during face-to-face learning sessions, in which learners practiced the reading and the rehearsal of extracts from a play in view of a final performance at the end of the training.
The results show that the phonological accuracy developed significantly for all the learners, whereas fluidity developed for some learners only.
The blended language-learning programme was perceived as a positive learning experience and students were globally satisfied with their participation.
However, they did not give the same consideration for group work, which is essential to bring such a project to a successful end, nor were they all involved in the project in the same way.
Automatic speech processing is an active field of research since the 1950s.
Within this field the main area of research is automatic speech recognition but simpler tasks such as speech activity detection, language identification or speaker identification are also of great interest to the community.
The most recent breakthrough in speech processing appeared around 2010 when speech recognition systems using deep neural networks drastically improved the state-of-the-art.
In this work, we closely look at a specific model for the RNNs: the Long Short Term Memory (LSTM) which mitigates a lot of the difficulties that can arise when training an RNN.
We augment this model and introduce optimization methods that lead to significant performance gains for speech activity detection and language identification.
This research focuses on the possibility to compare prelinguistic utterances and linguistic utterances of the first words period.
The definition of protoword and word notions is not clear; while we consider different approaches to determine them: historical, epistemological and experimental.
The contribution of the historical approach is essential to identify the problem and to bethink how a society in different historical periods considers speech.
This section allows us to highlight two elements: the question of the speech emergence implies the notion of social representation, and nowadays, the word emergence is during the first words period.
The analysis of this period leads to our epistemological part which defines units of this period: protowords and words.
Once units identified, we perform a longitudinal analysis of four children, from one to two years.
Firstly, we identify a phenomenon of substitution of protowords in words.
Secondly, we observe two common elements in these productions: prosody and phonology.
We show that prosody provides a common framework to ensure the transition between protowords and words, and that phonology is the area where differences are observed: the words are the place for the development of complex phonological structures, unlike protowords.
We consider that the emergence of speech is when the children prefer using words as verbal communication medium, instead of protowords, and that this feature is the object of phonological development.
As Twitter evolves into a ubiquitous information dissemination tool, understanding tweets in foreign languages becomes an important and difficult problem.
Because of the inherent code-mixed, disfluent and noisy nature of tweets, state-of-the-art Machine Translation (MT) is not a viable option (Farzindar &amp; Inkpen, 2015).
Indeed, at least for Hindi and Japanese, we observe that the percentage of "understandable" tweets falls from 80% for natives to below 30% for target (English or French) monolingual readers using Google Translate.
Our starting hypothesis is that it should be possible to build generic tools, which would enable foreigners to make sense of at least 70% of “native tweets”, using a versatile “active reading” (AR) interface, while simultaneously determining the percentage of understandable tweets under which such a system would be deemed useless by intended users.
We have thus specified a generic "SUFT" (System for Helping Understand Tweets), and implemented SUFT-1, an interactive multi-layout system based on AR, and easily configurable by adding dictionaries, morphological modules, and MT plugins.
It is capable of accessing multiple dictionaries for each source language and provides an evaluation interface.
For evaluations, we introduce a task-related measure inducing a negligible cost, and a methodology aimed at enabling a «continuous evaluation on open data», as opposed to classical measures based on test sets related to closed learning sets.
We propose to combine understandability ratio and understandability decision time as a two-pronged quality measure, one subjective and the other objective, and experimentally ascertain that a dictionary-based active reading presentation can indeed help understand tweets better than available MT systems.
In addition to gathering various lexical resources, we constructed a large resource of "word-forms" appearing in Indian tweets with their morphological analyses (viz.163221 Hindi word-forms from 68788 lemmas and 72312 Marathi word-forms from 6026 lemmas) for creating a multilingual morphological analyzer specialized to tweets, which can handle code-mixed tweets, compute unified features, and present a tweet with an attached AR graph from which foreign readers can intuitively extract a plausible meaning, if any.
Computer systems involving anomaly detection are emerging in both research and industry.
Thus, fields as varied as medicine (identification of malignant tumors), finance (detection of fraudulent transactions), information technologies (network intrusion detection) and environment (pollution situation detection) are widely impacted.
Machine learning offers a powerful set of approaches that can help solve these use cases effectively.
It also involves several experts who will work together to find the right approaches.
In addition, the possibilities opened today by the world of semantics show that it is possible to take advantage of web technologies to reason intelligently on raw data to extract information with high added value.
The lack of systems combining numeric approaches to machine learning and semantic techniques of the web of data is the main motivation behind the various works proposed in this thesis.
Finally, the anomalies detected do not necessarily mean abnormal situations in reality.
Indeed, the presence of external information could help decision-making by contextualizing the environment as a whole.
Exploiting the space domain and social networks makes it possible to build contexts enriched with sensor data.
These spatio-temporal contexts thus become an integral part of anomaly detection and must be processed using a Big Data approach.
Its originality lies in its ability to reason intelligently on raw data in order to infer implicit information from explicit information and assist in decision-making.
This platform was developed as part of a FUI project whose main use case is the detection of anomalies in a drinking water network.
RAMSSES:
Hybrid machine learning system whose originality is to combine advanced numerical approaches as well as proven semantic techniques.
SCOUTER: Intelligent system of "web scrapping" allowing the contextualization of singularities related to the Internet of Things by exploiting both spatial information and the web of data
The consideration of environmental issues is a very strong topic in our society nowadays.
In the field of Product Development, Ecodesign is a methodology that allows the consideration of these issues by proposing to reduce the environmental impacts of products throughout their life cycle.
The use phase of the life cycle is a critical step as the way how the products are managed can have significant impact on their environmental performance.
We propose in this thesis to enrich the understanding of the use phase by highlighting the Kansei Information that can play a major role in the interaction between the user and the product and thus can be integrated in the early phase of the Ecodesign process.
Our approach provides a better understanding the use phase, by contributing to the mastery of the environmental performance of products.
Our research points out that the user can be defined not only from basic information commonly used in the User Centered Design, but also from subjective information brought by the Kansei dimension.
We carry out experiments in which we implement two EcoKansei attributes corresponding to the values and emotions in order to illustrate user modeling for Ecodesign.
The approach we propose is a part of EcoUse project that aims to develop a methodology for user-centered Ecodesign.
Various contributions of our approach can be pointed out.
For the Research, linking Ecodesign with the Kansei studies, which are basically unconnected leads to mutual enrichment between these two approaches.
For Industry, an Eco-design approach supported by Kansei Information enables the design of new products which will have the benefits of both providing less impacts to the environment, and at the same time being more accepted by users as these products match with their environmental sensitivity.
All possible information sources will potentially be used in addition to the data provided by AMF, public information sources (specialized news agencies, social networks, WEB, etc.) and professionnal information sources like 'order books'(stock market transaction orders in natural language).
Identification of the potentential infringement will be completed with a quantitative assessment of the considered risk and a ranking by decreasing order of importance of the documents justifying the alert, as well as the generation of a synthesis document recapitulating the extracted information that explain the alert (in a form yet to be defined, textual abstract in natural language, fact database, graphics,...) depending on the specificities of the systems using the detection algorithms (database hypercubes, watch dashboards etc.).
The design of the algorithms will have to take into consideration scalability up to a Big Data context, requiring parallel computations [Bhatotia14].
The function expected of the algorithms developed during the thesis is to offer to inhouse AMF experts a preliminary analysis (weak signal detection [Sidhom11]) of the risk with all the justifying and complementary information.
Digital accessibility plays a crucial role for the education, the social inclusion and the autonomy of impaired people.
This work focused on a universal component of digital documents: text formatting.
Colors, fonts and text disposition are far more than just an ornament; text formatting conveys important meaning for content comprehension, and allows reader to optimize their activity.
For instance, a specific set of colors and font can be enough to indicate the presence of a title, which allows a global representation of the content themes.
Thus, we aimed at making text formatting meaning accessible to visually impaired people, so they can obtain the same information as sighted readers, and also benefit from the same optimizations when accessing the document with synthetic voices.
Nowadays, Artificial Intelligence (AI) is a widespread concept applied to many fields such as transportation, medicine and autonomous vehicles.
The main AI algorithms are artificial neural networks, which can be divided into two families: Spiking Neural Networks (SNNs), which are bio-inspired models resulting from neuroscience, and Analog Neural Networks (ANNs), which result from machine learning.
The ANNs are experiencing unprecedented success in research and industrial fields, due to their recent successes in many application contexts such as image classification and object recognition.
However, they require considerable computational capacity for their deployment which is not adequate to very constrained systems.
To overcome these limitations, many researchers are interested in brain-inspired computing, which would be the perfect alternative to conventional computers based on the Von Neumann architecture (CPU/GPU).
This paradigm meets computing performance but not energy efficiency requirements.
Hence, it is necessary to design neuromorphic hardware circuits adaptable to parallel and distributed computing.
In this context, we have set criteria in terms of accuracy and hardware implementation cost to differentiate the two neural families (SNNs and ANNs).
In the case of simple network topologies, we conducted a study that has shown that the spiking models have significant gains in terms of hardware cost when compared to the analog networks, with almost similar prediction accuracies.
Therefore, the objective of this thesis is to design a generic neuromorphic architecture that is based on spiking neural networks.
In an energy efficiency context, a thorough exploration of different neural coding paradigms for neural data representation in SNNs has been carried out.
Moreover, new derivative versions of rate-based coding have been proposed that aim to get closer to the activity produced by temporal coding, which is characterized by a reduced number of spikes propagating in the network.
In this way, the number of spikes can be reduced so that the number of events to be processed in the SNNs gets smaller. The aim in doing this approach is to reduce the underlying energy consumption.
The proposed coding approaches are: First Spike, which is characterized using at most one single spike to present an input data, and Spike Select, which allows to regulate and minimize the overall spiking activity in the SNN.In the RTL design exploration, we quantitatively compared three SNN architectural models having different levels of computing parallelism and multiplexing.
Using Spike Select coding results in a distribution regulation of the spiking data, with most of them generated within the first layer and few of them propagate into the deep layers.
Such distribution benefits from a so-called 'hybrid architecture' that includes a fully-parallel part for the first layer and multiplexed parts to the other layers.
Therefore, combining the Spike Select and the Hybrid Architecture would be an effective solution for embedded AI applications, with an efficient hardware and latency trade-off.
Finally, based on the architectural and neural choices resulting from the previous exploration, we have designed a final event-based architecture dedicated to SNNs supporting different neural network types and sizes.
The architecture supports the most used layers: convolutional, pooling and fully-connected.
Using this architecture, we will be able to compare analog and spiking neural networks on realistic applications and to finally conclude about the use of SNNs for Embedded Artificial Intelligence.
This work studies a new situation of communication: communication via e-mail.
Our study is more precisely focussed on e-mails sent by customers to firms and this, in the field of air tourism (concept of e-crm).
To carry out our linguistic analyses, we constituted an important corpus of messages collected on Internet forums and dealing with travel.
Our goal is to manage the categorization and thematisation of e-mails.
We thus gathered lexical, syntactic, morpho-syntactic and semantic features which are specific to the concept of spatiality, toponymy and characteristic of air tourism sub-language.
We also underline how a linguistic analysis of spatiality is linked to a temporal analysis of the sentence.
Moreover, we choose to analyze emotional informations contained in our messages.
In the last part of our work, we show how our work deals with mining systems.
We show how statistical techniques are limited as soon as it is a question of treating linguistically complex statements such as ours.
Our approach is hybrid: it is made of key words, synonyms dictionaries, scripts on the model of SCHANK and ABELSON, but especially knowledge modeling.
We give some examples of computerization of our system thanks to XML, PROLOG and Perl
Video surveillance systems are of a great value for public safety.
As one of the most import surveillance applications, person re-identification is defined as the problem of identifying people across images that have been captured by different surveillance cameras without overlapping fields of view.
The attributes are defined as semantic mid-level descriptions of persons, such as gender, accessories, clothing etc.
The same person shows very different appearances from different points of view.
To deal with this issue, we consider that the images under various orientations are from different domains.
The combined orientation-specific CNN feature representations are used for the person re-identification task.
Thirdly, learning a similarity metric for person images is a crucial aspect of person re-identification.
As the third contribution, we propose a novel listwise loss function taking into account the order in the ranking of gallery images with respect to different probe images.
In this case, using only the appearance of single person leads to strong ambiguities.
As the last contribution, we propose to learn a deep feature representation with displacement invariance for group context and introduce a method to combine the group context and single-person appearance.
It is important to regularly assess the technological innovation products in order to estimate the level of maturity reached by the technology and study the applications frameworks in which they can be used.
For years, the different technological modules from the NLP were developed separately.
The new challenge in terms of evaluation is then to evaluate the different modules while taking into account the applicative context.
We describes the tasks of ASR and NER proposed during several evalution campaigns and we discuss the protocols established for their evaluation.
We also point the limitations of modular evaluation approaches and we expose the alternatives measures proposed in the literature.
In the second part we describe the studied task of named entities detection, classification and decomposition and we propose a new metric ETER (Entity Tree Error Rate) which allows to take into account the specificity of the task and the applicative context during the evaluation.
ETER also eliminates the biases observed with the existing metrics.
Rather than directly comparing reference and hypothesis transcriptions, ATENE measure how harder it becames to identify entities given the differences between hypothesis and reference by comparing an estimated likelihood of presence of entities.
It is composed of two elementary measurements.
This thesis presents a generic method for automatic recognition of emotions from a bimodal system based on facial expressions and physiological signals.
This data processing approach leads to better extraction of information and is more reliable than single modality.
The proposed algorithm for facial expression recognition is based on the distance variation of facial muscles from the neutral state and on the classification by means of Support Vector Machines (SVM).
And the emotion recognition from physiological signals is based on the classification of statistical parameters by the same classifier.
In order to have a more reliable recognition system, we have combined the facial expressions and physiological signals.
The direct combination of such information is not trivial giving the differences of characteristics (such as frequency, amplitude, variation, and dimensionality).
To remedy this, we have merged the information at different levels of implementation.
At feature-level fusion, we have tested the mutual information approach for selecting the most relevant and principal component analysis to reduce their dimensionality.
For decision-level fusion we have implemented two methods; the first based on voting process and another based on dynamic Bayesian networks.
The optimal results were obtained with the fusion of features based on Principal Component Analysis.
These methods have been tested on a database developed in our laboratory from healthy subjects and inducing with IAPS pictures.
A self-assessment step has been applied to all subjects in order to improve the annotation of images used for induction.
The obtained results have shown good performance even in presence of variability among individuals and the emotional state variability for several days
A very broad definition of music structure is to consider what distinguishes music from random noise as part of its structure.
In this thesis, we take interest in the macroscopic aspects of music structure, especially the decomposition of musical pieces into autonomous segments (typically, sections) and their characterisation as the result of the grouping process of jointly compressible units.
An important assumption of this work is to establish a link between the inference of music structure and information theory concepts such as complexity and entropy.
We thus build upon the hypothesis that structural segments can be inferred through compression schemes.
In a first part of this work, we study Straight-Line Grammars (SLGs), a family of formal grammars originally used for structure discovery in biological sequences (Gallé, 2011), and we explore their use for the modelisation of musical sequences.
The SLG approach enables the compression of sequences, depending on their occurrence frequencies, resulting in a tree-based modelisation of their hierarchical organisation.
We develop several adaptations of this method for the modelisation of approximate repetitions and we develop several regularity criteria aimed at improving the efficiency of the method.
The second part of this thesis develops and explores a novel approach for the inference of music structure, based on the optimisation of a tensorial compression criterion.
This approach aims to compress the musical information on several simultaneous time-scales by exploiting the similarity relations, the logical progressions and the analogy systems which are embedded in musical segments.
The proposed method is first introduced from a formal point of view, then presented as a compression scheme rooted in a multi-scale extension of the System &amp; Contrast model (Bimbot et al., 2012) to hypercubic tensorial patterns
Furthermore, we generalise the approach to other, irregular, tensorial patterns, in order to account for the great variety of structural organisations observed in musical segments.
The methods presented in this thesis are tested on a structural segmentation task using symbolic data, chords sequences from pop music (RWC-Pop).
The methods are evaluated and compared on several sets of chord sequences, and the results establish an experimental advantage for the approaches based on a complexity criterion for the analysis of structure in music information retrieval, with the best variants offering F-measure scores around 70%.
Extracting information from linguistic data has gain more and more attention in the last decades in relation with the increasing amount of information that has to be processed on a daily basis in the world. Since the 90's, this interest for information extraction has converged to the development of researches on speech data.
In fact, speech data involves extra problems to those encountered on written data. In particular, due to many phenomena specific to human speech (e.g. hesitations, corrections, etc.). But also, because automatic speech recognition systems applied on speech signal potentially generates errors.
Thus, extracting information from audio data requires to extract information by taking into account the "noise" inherent to audio data and output of automatic systems.
Thus, extracting information from speech data cannot be as simple as a combination of methods that have proven themselves to solve the extraction information task on written data.
It comes that, the use of technics dedicated for speech/audio data processing is mandatory, and epsecially technics which take into account the specificites of such data in relation with the corresponding signal and transcriptions (manual and automatic).
This problem has given birth to a new area of research and raised new scientific challenges related to the management of the variability of speech and its spontaneous modes of expressions.
Furthermore, robust analysis of phone conversations is subject to a large number of works this thesis is in the continuity.
More specifically, this thesis focuses on edit disfluencies analysis and their realisation in conversational data from EDF call centres, using speech signal and both manual and automatic transcriptions.
This work is linked to numerous domains, from robust analysis of speech data to analysis and management of aspects related to speech expression.
The aim of the thesis is to propose appropriate methods to deal with speech data to improve text mining analyses of speech transcriptions (treatment of disfluencies).
To address these issues, we have finely analysed the characteristic phenomena and behavior of spontaneous speech (disfluencies) in conversational data from EDF call centres and developed an automatic method for their detection using linguistic, prosodic, discursive and para-linguistic features.
The contributions of this thesis are structured in three areas of research.
First, we proposed a specification of call centre conversations from the prespective of the spontaneous speech and from the phenomena that specify it.
Second, we developed (i) an enrichment chain and effective processings of speech data on several levels of analysis (linguistic, acoustic-prosodic, discursive and para-linguistic) ; (ii) an system which detect automaticaly the edit disfluencies suitable for conversational data and based on the speech signal and transcriptions (manual or automatic).
Third, from a "resource" point of view, we produced a corpus of automatic transcriptions of conversations taken from call centres which has been annotated in edition disfluencies (using a semi-automatic method).
Recommendation system for personalized sightseeing tours
Diffusion Magnetic Resonance Imaging (dMRI) is a meaningful technique for white matter (WM) fiber-tracking and microstructural characterization of axonal/neuronal integrity and connectivity.
By measuring water molecules motion in the three directions of space, numerous parametric maps can be reconstructed.
Among these, fractional anisotropy (FA), mean diffusivity (MD), and axial (λa) and radial (λr) diffusivities have extensively been used to investigate brain diseases.
Overall, these findings demonstrated that WM and grey matter (GM) tissues are subjected to numerous microstructural alterations in multiple sclerosis (MS).
However, it remains unclear whether these tissue alterations result from global processes, such as inflammatory cascades and/or neurodegenerative mechanisms, or local inflammatory and/or demyelinating lesions.
Furthermore, these pathological events may occur along afferent or efferent WM fiber pathways, leading to antero- or retrograde degeneration.
Thus, for a better understanding of MS pathological processes like its spatial and temporal progression, an accurate and sensitive characterization of WM fibers along their pathways is needed.
By merging the spatial information of fiber tracking with the diffusion metrics derived obtained from longitudinal acquisitions, WM fiber-bundles could be modeled and analyzed along their profile.
Such signal analysis of WM fibers can be performed by several methods providing either semi- or fully unsupervised solutions.
In the first part of this work, we will give an overview of the studies already present in literature and we will focus our analysis on studies showing the interest of dMRI for WM characterization in MS.
In the second part, we will introduce two new string-based methods, one semi-supervised and one unsupervised, to extract specific WM fiber-bundles.
We will show how these algorithms allow to improve extraction of specific fiber-bundles compared to the approaches already present in literature.
Moreover, in the second chapter, we will show an extension of the proposed method by coupling the string-based formalism with the spatial information of the fiber-tracks.
In the third, and last part, we will describe, in order of complexity, three different fully automated algorithms to perform analysis of longitudinal changes visible along WM fiber-bundles in MS patients.
These methods are based on Gaussian mixture model, nonnegative matrix and tensor factorisation respectively.
Moreover, in order to validate our methods, we introduce a new model to simulate real longitudinal changes based on a generalised Gaussian probability density function.
For those algorithms high levels of performances were obtained for the detection of small longitudinal changes along the WM fiber-bundles in MS patients.
In conclusion, we propose, in this work, a new set of unsupervised algorithms to perform a sensitivity analysis of WM fiber bundle that would be useful for the characterisation of pathological alterations occurring in MS patients
Multi-day events such as conventions, festivals, cruise trips, to which we refer to as distributed events, have become very popular in recent years, attracting hundreds or thousands of participants.
Their programs are usually very dense, making it challenging for the attendees to make a decision which events to join.
Recommender systems appear as a common solution in such an environment.
While many existing solutions deal with personalised recommendation of single items, recent research focuses on the recommendation of consecutive items that exploits user's behavioural patterns and relations between entities, and handles geographical and temporal constraints.
Second, we propose an approach (ANASTASIA) to solve this problem, which aims at providing an integrated support for users to create a personalised itinerary of activities.
ANASTASIA brings together three components, namely: (1) estimation of the user's interest in single items, (2) use of sequential influence on activity performance, and (3) building of an itinerary that takes into account spatio-temporal constraints.
Thus, the proposed solution makes use of the methods based on sequence learning and discrete optimisation.
Moreover, stating the lack of publicly available datasets that could be used for the evaluation of event and itinerary recommendation algorithms, we have created two datasets, namely: (1) event attendance on board of a cruise (Fantasy_db) based on a conducted user study, and (2) event attendance at a major comic book convention (DEvIR).
This allows to perform evaluation of recommendation methods, and contributes to the reproducibility of results.
Social interactions are in the core of economic activities.
Their treatment in Economies is however often limited to a focus on the market (Manski, 2000).
The role social interactions themselves play for the behavior of agents as well as the formation of their attitudes is often neglected.
This is despite the fact that already early contributions in economic literature have identified them as important determinants for the decision making of economic agents as for example Sherif (I936), Hyman (1942), Asch (1951), Jahoda (1959) or Merton (1968).
ln consumer research, a field on the intersection between Economies, Sociology and Psychology, on the other hand social interactions (social influences) are considered to be the"... most pervasive determinants [...] of individual 's behaviour... " (Bumkrant and Cousineau, 1975).
The thesis at hand bridges the gap between social interactions and their influence on agents expectation formation and behavior.
In this thesis, we present contributions to the challenging issues which are encountered in question answering and locating information in complex textual data, like log files.
Question answering systems (QAS) aim to find a relevant fragment of a document which could be regarded as the best possible concise answer for a question given by a user.
In this work, we are looking to propose a complete solution to locate information in a special kind of textual data, i.e., log files generated by EDA design tools.
Nowadays, in many application areas, modern computing systems are instrumented to generate huge reports about occurring events in the format of log files.
Log files are generated in every computing field to report the status of systems, products, or even causes of problems that can occur.
Log files may also include data about critical parameters, sensor outputs, or a combination of those.
Analyzing log files, as an attractive approach for automatic system management and monitoring, has been enjoying a growing amount of attention [Li et al., 2005].
Although the process of generating log files is quite simple and straightforward, log file analysis could be a tremendous task that requires enormous computational resources, long time and sophisticated procedures [Valdman, 2004].
Indeed, there are many kinds of log files generated in some application domains which are not systematically exploited in an efficient way because of their special characteristics.
In this thesis, we are mainly interested in log files generated by Electronic Design Automation (EDA) systems.
Electronic design automation is a category of software tools for designing electronic systems such as printed circuit boards and Integrated Circuits (IC).
In this domain, to ensure the design quality, there are some quality check rules which should be verified.
Verification of these rules is principally performed by analyzing the generated log files.
In the case of large designs that the design tools may generate megabytes or gigabytes of log files each day, the problem is to wade through all of this data to locate the critical information we need to verify the quality check rules.
We investigate throughout this work the main concern "how the specificities of log files can influence the information extraction and natural language processing methods?".
In this context, a key challenge is to provide approaches that take the log file specificities into account while considering the issues which are specific to QA in restricted domains.
We present different contributions as below:
Within this approach, we propose an original type of descriptor to model the textual structure and layout of text documents.
> Proposing an approach to locate the requested information in the log files based on passage retrieval.
To improve the performance of passage retrieval, we propose a novel query expansion approach to adapt an initial query to all types of corresponding log files and overcome the difficulties like mismatch vocabularies.
Our method is based on a new term weighting function, called TRQ (Term Relatedness to Query), introduced in this work, which gives a score to terms of corpus according to their relatedness to the query.
We also investigate how to apply our query expansion approach to documents from general domains.
> Studying the use of morpho-syntactic knowledge in our approaches.
For this purpose, we are interested in the extraction of terminology in the log files.
Thus, we here introduce our approach, named Exterlog (EXtraction of TERminology from LOGs), to extract the terminology of log files.
To evaluate the extracted terms and choose the most relevant ones, we propose a candidate term evaluation method using a measure, based on the Web and combined with statistical measures, taking into account the context of log files.
This dissertation examines the construction of meaning and the crosslinguistic influence on the representation of the lexical signification and discursive meaning of the words work and travail.
This study is based on definitional discourse and significations such as they are proposed by speakers articulating their metalinguistic semantic knowledge in the context of experimental research.
The cultural concept of work is analysed by using Galatanu's theoretical framework The Semantics of Argumentative Possibilities (SAP).
A French/English constractive study allowed us to determine the similarities as well as the differences between the representations of the lexical signification and discursive meaning of the words “work” and “travail” proposed by four groups of speakers.
Each group of speakers having its own linguistic profile (two groups of monolingual speakers – Francophone and Anglophone, one group of bilingual speakers whose native language is French and who speak English, and one group of bilingual speakers whose native language is English and who speak French) presents the opportunity to not only compare the variations between the representations of the lexical signification and discursive meaning of the words “work” and “travail”, but also to take note of the potential influence that competence in a foreign language can have on these representations.
The second part of this dissertation includes an analysis of the representation of the insult loser as used in the United States.
Inspired by the presence of an element linking work and said insult, the choice to present loser lends itself to exhibiting the discursive mobilisation of the social values instilled in the words work and loser.
Knowledge bases are huge collections of primarily encyclopedic facts.
They are widely used in entity recognition, structured search, question answering, and other tasks.
These knowledge bases have to be curated, and this is a crucial but costly task.
In this thesis, we are concerned with curating knowledge bases automatically using constraints.
Our first contribution aims at discovering constraints automatically.
We improve standard rule mining approaches by using (in-)completeness meta-information.
We show that this information can increase the quality of the learned rules significantly.
Our second contribution is the creation of a knowledge base, YAGO 4, where we statically enforce a set of constraints by removing the facts that do not comply with them.
Our last contribution is a method to correct constraint violations automatically.
Our method uses the edit history of the knowledge base to see how users corrected violations in the past, in order to propose corrections for the present.
Artificial Intelligence (AI) and Human-Computer Interactions (HCIs) are two research fields with relatively few common work.
HCI specialists usually design the way we interact with devices directly from observations and measures of human feedback, manually optimizing the user interface to better fit users'expectations.
This process is hard to optimize: ergonomy, intuitivity and ease of use are key features in a User Interface (UI) that are too complex to be simply modelled from interaction data.
This drastically restrains the possible uses of Machine Learning (ML) in this design process.
Currently, ML in HCI is mostly applied to gesture recognition and automatic display, e.g. advertisement or item suggestion.
It is also used to fine tune an existing UI to better optimize it, but as of now it does not participate in designing new ways to interact with computers.
Our main focus in this thesis is to use ML to develop new design strategies for overall better UIs.
We want to use ML to build intelligent – understand precise, intuitive and adaptive – user interfaces using minimal handcrafting.
We propose a novel approach to UI design: instead of letting the user adapt to the interface, we want the interface and the user to adapt mutually to each other.
The goal is to reduce human bias in protocol definition while building co-adaptive interfaces able to further fit individual preferences.
In order to do so, we will put to use the different mechanisms available in ML to automatically learn behaviors, build representations and take decisions.
We will be experimenting on touch interfaces, as these interfaces are vastly used and can provide easily interpretable problems.
The very first part of our work will focus on processing touch data and use supervised learning to build accurate classifiers of touch gestures.
The second part will detail how Reinforcement Learning (RL) can be used to model and learn interaction protocols given user actions.
Lastly, we will combine these RL models with unsupervised learning to build a setup allowing for the design of new interaction protocols without the need for real user data.
The proliferation of digital data has enabled scientific and practitioner communities to create new data-driven technologies to learn about user behaviors in order to deliver better services and support to people in their digital experience.
The majority of these technologies extensively derive value from data logs passively generated during the human-computer interaction.
A particularity of these behavioral traces is that they are structured.
However, the pro-actively generated text across Internet is highly unstructured and represents the overwhelming majority of behavioral traces.
To date, despite its prevalence and the relevance of behavioral knowledge to many domains, such as recommender systems, cyber-security and social network analysis,the digital text is still insufficiently tackled as traces of human behavior to automatically reveal extensive insights into behavior.
Multiple original contributions are made.
The only systematic study to date on the automatic modeling of asynchronous communication with speech intentions is conducted.
A corpus-independent, automatic method to annotate utterances of asynchronous communication with the proposed speech intention taxonomy is designed based on supervised machine learning.
For this, validated ground-truth corpora arecreated and groups of features—discourse, content and conversation-related, are engineered to be used by the classifiers.
In particular, some of the discourse features are novel and defined by considering linguistic means to express speech intentions, without relying on the corpus explicit content, domain or on specificities of the asynchronous communication types.
Then, an automatic method based on process mining is designed to generate process models of interrelated speech intentions from conversation turns, annotated with multiple speech intentions per sentence.
As process mining relies on well-defined structured event logs, an algorithm to produce such logs from conversations is proposed.
Additionally, an extensive design rationale on how conversations annotated with multiple labels per sentence could be transformed in event logs and what is the impact of different decisions on the output behavioral models is released to support future research.
We address the difficulty of creating a digitised corpus by using a crowdsourced approach for annotating comic books.
The resulting XML-based encodings assist researchers, publishers and collection curators equally.
To achieve our data collection goal, we develop an online crowdsourcing engine for annotating comics.
The tasks are designed to mirror the page reading experience, with participants asked to identify and annotate structural (panel layout, splash pages, meta-panels) and content (characters, places, events, onomatopoeia) elements of comic books.
Curators and collectors of physical or online comics collections are provided with a structured content which could enable the creation of artefacts such as comic books dictionaries, search indices and dictionaries of onomatopoeia.
From a publishing perspective, current standards for digital comics are taking care exclusively of the presentation layer (i.e. rendering a publication on the screen of a device).
But the artistic nature of comics and the great potential digital comics have already showcased allow us to go beyond simple content presentation.
To this respect we present our contributions with enhancements to current semantic (CBML) and presentation (EPUB) open standards that will allow publishers and digital comics authors to create an improved reading experience.
The human faculties of understanding are essentially multimodal.
To understand the world around them, human beings fuse the information coming from all of their sensory receptors.
The aim of this thesis is to propose joint processes applying mainly to text and image for the processing of multimodal documents through two studies: one on multimodal fusion for the speaker role recognition in television broadcasts, the other on the complementarity of modalities for a task of linguistic analysis on corpora of images with captions.
In the first part of this study, we interested in audiovisual documents analysis from news television channels.
We propose an approach that uses in particular deep neural networks for representation and fusion of modalities.
In the second part of this thesis, we are interested in approaches allowing to use several sources of multimodal information for a monomodal task of natural language processing in order to study their complementarity.
We propose a complete system of correction of prepositional attachments using visual information, trained on a multimodal corpus of images with captions.
Our perspectives are educational, to create grammar exercises for French.
Paraphrasing is an operation of reformulation.
Our work tends to attest that sequence-to-sequence models are not simple repeaters but can learn syntax.
First, by combining various models, we have shown that the representation of information in multiple forms (using formal data (RDF), coupled with text to extend or reduce it, or only text) allows us to exploit a corpus from different angles, increasing the diversity of outputs, exploiting the syntactic levers put in place.
We also addressed a recurrent problem, that of data quality, and obtained paraphrases with a high syntactic adequacy (up to 98% coverage of the demand) and a very good linguistic level.
We obtain up to 83.97 points of BLEU-4*, 78.41 more than our baseline average, without syntax leverage.
This rate indicates a better control of the outputs, which are varied and of good quality in the absence of syntax leverage.
The transition to French text was also an imperative for us.
Working from plain text, by automating the procedures, allowed us to create a corpus of more than 450,000 sentence/representation pairs, thanks to which we learned to generate massively correct texts (92% on qualitative validation).
Anonymizing everything that is not functional contributed significantly to the quality of the results (68.31 of BLEU, i.e. +3.96 compared to the baseline, which was the generation of text from non-anonymized data).
The formal representation of information in a language-specific framework is a challenging task.
This thesis offers some ideas on how to automate this operation.
Moreover, we were only able to process relatively short sentences.
The use of more recent neural modelswould likely improve the results.
The use of appropriate output strokes would allow for extensive checks.
*BLEU: quality of a text (scale from 0 (worst) to 100 (best), Papineni et al. (2002))
An innumerable number of documents is being printed, scanned, faxed, photographed every day.
These documents are hybrid: they exist as both hard copies and digital copies.
Moreover their digital copies can be viewed and modified simultaneously in many places.
With the availability of image modification software, it has become very easy to modify or forge a document.
This creates a rising need for an authentication scheme capable of handling these hybrid documents.
Current solutions rely on separate authentication schemes for paper and digital documents.
In order to overcome all these issues we propose to create a semantic hashing algorithm for document images.
This hashing algorithm should provide a compact digest for all the visually significant information contained in the document.
This digest will allow current hybrid security systems to secure all the document.
This can be achieved thanks to document analysis algorithms.
After defining the context of this study and what is a stable algorithm, we focused on producing stable algorithms for layout description, document segmentation, character recognition and describing the graphical parts of a document.
The goal of this thesis is to develop models linking Markov models and Neural Networks, and to study their applications, particularly for Question Answering. Then, we want to use these new models in conversational agents, also called chatbots.
Today, the most used algorithms for Question Answering are based on Neural Networks.
Those can be interpreted as hidden probabilistic models, where the same problem can be addressed by Markov Models.
Indeed, for fifteen years, those had been extended to Pairwise and Triplet Markov Models, allowing us to obtain results sometimes impressive.
The PhD student will be inspired by the steps bringing the creation of pairwise and triplet Markov models from the hidden Markov model to propose new neural network models "pairwise" and "triplet", and hybrids models with neural networks and Markov models.
He will search applications for these new models, particularly for Question Answering, and will study the contribution of these new models to the classic Neural Networks.
Creating an emergency call centre dedicated to deaf and hard of hearing users has required the design and implementation of a specific socio-technical system in order to take into account the diversity of their communication practices.
Our research is grounded on such technical project specifically oriented towards the accessibility to an organization acting mainly through telephonic interactions and therefrom emerged for us the opportunity to conduct a cross-reflection on socio-technical devices apprehended as a working tool and an accessibility tool.
Based on elements obtained through an ethnographic study conducted in emergency call centres which has given empirical material that we are bringing into discussion with traces of the process of the product design, we are questioning the place and status of communication devices in the local environment. Along with this questioning, we are also observing how communication devices practically achieve accessibility within the social model of disability.
Two main themes are therefore explored: the organization of work activities in situations of mediated communication and the process, engaged by the design and implementation of an accessibility device, by which conventional practices are reformulated.
Through a refined description of the multiple courses of action and a subtle attention to the materiality of interactional practices, we demonstrate that communication technical devices are a piece of an heterogeneous assembly, that make sense in situation, and act as an invisible scaffolding that organize and stabilize communication practices.
We propose to designate this form of stabilization under the name of communicational infrastructure and to use it as a conceptual tool for approaching the introduction of new communication devices in organizations and to support a reflection on accessibility-in-practices
Natural language processing systems often rely on the idea that language is compositional, that is, the meaning of a linguistic entity can be inferred from the meaning of its parts.
This expectation fails in the case of multiword expressions (MWEs).
For example, a person who is a "sitting duck" is neither a duck nor necessarily sitting.
Modern computational techniques for inferring word meaning based on the distribution of words in the text have been quite successful at multiple tasks, especially since the rise of word embedding approaches.
However, the representation of MWEs still remains an open problem in the field.
In particular, it is unclear how one could predict from corpora whether a given MWE should be treated as an indivisible unit (e.g. "nut case") or as some combination of the meaning of its parts (e.g. "engine room").
This thesis proposes a framework of MWE compositionality prediction based on representations of distributional semantics, which we instantiate under a variety of parameters.
We present a thorough evaluation of the impact of these parameters on three new datasets of MWE compositionality, encompassing English, French and Portuguese MWEs.
Finally, we present an extrinsic evaluation of the predicted levels of MWE compositionality on the task of MWE identification.
Our results suggest that the proper choice of distributional model and corpus parameters can produce compositionality predictions that are comparable to the state of the art.
In preparation for the usage of distributed infrastructures to further accelerate the computation, this study aims at exploring the possibility of executing the Frank-Wolfe algorithm in a star network with the Bulk Synchronous Parallel (BSP) model and investigating its efficiency both theoretically and empirically.
In the theoretical aspect, this study revisits Frank-Wolfe's fundamental deterministic sublinear convergence rate and extends it to nondeterministic cases.
In particular, it shows that with the linear subproblem appropriately solved, Frank-Wolfe can achieve a sublinear convergence rate both in expectation and with high probability.
This contribution lays the theoretical foundation of using power iteration or Lanczos iteration to solve the linear subproblem for trace norm minimization.
In the algorithmic aspect, within the BSP model, this study proposes and analyzes four strategies for the linear subproblem as well as methods for the line search.
Moreover, noticing Frank-Wolfe's rank-1 update property, it updates the gradient recursively, with either a dense or a low-rank representation, instead of repeatedly recalculating it from scratch.
All of these designs are generic and apply to any distributed infrastructures compatible with the BSP model.
In the empirical aspect, this study tests the proposed algorithmic designs in an Apache SPARK cluster.
According to the experiment results, for the linear subproblem, centralizing the gradient or averaging the singular vectors is sufficient in the low-dimensional case, whereas distributed power iteration, with as few as one or two iterations per epoch, excels in the high-dimensional case.
The Python package developed for the experiments is modular, extensible and ready to deploy in an industrial context.
This study has achieved its function as proof of concept.
Following the path it sets up, solvers can be implemented for various infrastructures, among which GPU clusters, to solve practical problems in specific contexts.
Besides, its excellent performance in the ImageNet dataset makes it promising for deep learning.
An ontology mapping is a set of correspondences.
Each correspondence relates artifacts, such as concepts and properties, of one ontology to artifacts of another ontology.
In the last few years, a lot of attention has been paid to establish mappings between source ontologies.
Ontology mapping is widely and effectively used for interoperability and integration tasks (data transformation, query answering, or web-service composition, to name a few), and in the creation of new ontologies.
On the one side, checking the (logical) correctness of ontology mappings has become a fundamental prerequisite of their use.
On the other side, given two ontologies, there are several ontology mappings between them that can be obtained by using different ontology matching methods or just stated manually.
Using ontology mappings between two ontologies in combination within a single application or for synthesizing one mapping taking the advantage of two original mappings, may cause errors in the application or in the synthesized mapping because those original mappings may be contradictory (conflicting).
In both situations, correctness is usually formalized and verified in the context of fully formalized ontologies (e.g. in logics), even if some “weak” notions of correctness have been proposed when ontologies are informally represented or represented in formalisms preventing a formalization of correctness (such as UML).
Verifying correctness is usually performed within one single formalism, requiring on the one side that ontologies need to be represented in this unique formalism and, on the other side, a formal representation of mapping is provided, equipped with notions related to correctness (such as consistency).
In practice, there exist several heterogeneous formalisms for expressing ontologies, ranging from informal (text, UML and others) to formal (logical and algebraic).
This implies that, willing to apply existing approaches, heterogeneous ontologies should be translated (or just transformed if, the original ontology is informally represented or when full translation, keeping equivalence, is not possible) in one common formalism, mappings need each time to be reformulated, and then correctness can be established.
This is possible but possibly leading to correct mappings under one translation and incorrect mapping under another translation.
Indeed, correctness (e.g. consistency) depends on the underlying employed formalism in which ontologies and mappings are expressed.
Specifically ontologies are represented as lattices and mappings as functions between those lattices.
Lattices are natural structures for directly representing ontologies, without changing the original formalisms in which ontologies are expressed.
As a consequence, the (unified) notion of correctness has been reformulated by using Galois connection condition, leading to the new notion of compatible and incompatible mappings.
It is formally shown that the new notion covers the reviewed correctness notions, provided in distinct state of the art formalisms, and, at the same time, can naturally cover heterogeneous ontologies.
The usage of the proposed unified approach is demonstrated by applying it to upper ontology mappings.
Notion of compatible and incompatible ontology mappings is also applied on domain ontologies to highlight that incompatible ontology mappings give incorrect results when used for ontology merging.
Several recent discoveries, notably from cognitive neuroscience, have had repercussions in the humanities and more particularly in the linguistics' field.
They have given rise to a renewed interest in the theme of phonological iconicity, which tackles the entirety of phenomena of similarity between signifier and signified inside a language.
A multitude of studies then emerged, attesting to the existence of phonosymbolic phenomena in the languages of the world.
Despite this considerable growth, the content of these works, mainly written in English, remains to this day rather unknown to the French-speaking public, still relatively anchored in the tradition of Structuralism.
This dissertation starts by presenting a synthesis of the international work carried out in the field of phonological iconicity in order to retain the main achievements.
Building on this, the dissertation will bring new empirical evidence of the existence of iconic phenomena in a corpus of French monosyllabic verbs through two methods.
Machine Translation (MT) systems, which generate automatically the translation of a target language for each source sentence, have achieved impressive gains during the recent decades and are now becoming the effective language assistances for the entire community in a globalized world.
Nonetheless, due to various factors, MT quality is still not perfect in general, and the end users therefore expect to know how much should they trust a specific translation.
Building a method that is capable of pointing out the correct parts, detecting the translation errors and concluding the overall quality of each MT hypothesis is definitely beneficial for not only the end users, but also for the translators, post-editors, and MT systems themselves.
Such method is widely known under the name Confidence Estimation (CE) or Quality Estimation (QE).
This thesis mostly focuses on the CE methods at word level (WCE).
The WCE classifier tags each word in the MT output a quality label.
Nowadays, WCE shows an increasing importance in many aspects of MT.
Firstly, it assists the post-editors to quickly identify the translation errors, hence improve their productivity.
Secondly, it informs readers of portions of sentence that are not reliable to avoid the misunderstanding about the sentence's content.
Thirdly, it selects the best translation among options from multiple MT systems.
Last but not least, WCE scores can help to improve the MT quality via some scenarios: N-best list re-ranking, Search Graph Re-decoding, etc.
In this thesis, we aim at building and optimizing our baseline WCE system, then exploiting it to improve MT and Sentence Confidence Estimation (SCE).
Compare to the previous approaches, our novel contributions spread of these following main points.
Firstly, we integrate various types of prediction indicators: system-based features extracted from the MT system, together with lexical, syntactic and semantic features to build the baseline WCE systems.
We also apply multiple Machine Learning (ML) models on the entire feature set and then compare their performances to select the optimal one to optimize.
Secondly, the usefulness of all features is deeper investigated using a greedy feature selection algorithm.
Thirdly, we propose a solution that exploits Boosting algorithm as a learning method in order to strengthen the contribution of dominant feature subsets to the system, thus improve of the system's prediction capability.
Lastly, we explore the contributions of WCE in improving MT quality via some scenarios.
In N-best list re-ranking, we synthesize scores from WCE outputs and integrate them with decoder scores to calculate again the objective function value, then to re-order the N-best list to choose a better candidate.
In the decoder's search graph re-decoding, the proposition is to apply WCE score directly to the nodes containing each word to update its cost regarding on the word quality.
Furthermore, WCE scores are used to build useful features, which can enhance the performance of the Sentence Confidence Estimation system.
In total, our work brings the insightful and multidimensional picture of word quality prediction and its positive impact on various sectors for Machine Translation.
The promising results open up a big avenue where WCE can play its role, such as WCE for Automatic Speech Recognition (ASR) System, WCE for multiple MT selection, and WCE for re-trainable and self-learning MT systems.
Recent progresses in animation have allowed the use of virtual character to many extents.
However, automatic generation of animation for such characters strongly rely on the lexical description of signs.
Signs described through these models are usually perfect and geometric performances leading to robotic and unrealistic movements.
These thesis focuses on adding information to the control skeleton of the signer to help him perform signs in a more human and realistic way.
Such information are grouped under the name of "anatomic model" and are divided in five main contributions: a new computer-based description of the skeleton, an anthropometric study of the hand, the merging of articulatory dependencies, a new model of the carpo-metacarpal complex allowing an easier opposition of the thumb and finally a model computing posture comfort.
These contributions are then implemented in a generation system through linguistic contraints-adapted techniques.
The study ends with an evaluation of the system and a the presentation of future prospects.
In computational linguistics, the relation between different languages is often studied through automatic alignment techniques.
Such alignments can be established at various structural levels.
In particular, sentential and sub-sentential bitext alignments constitute an important source of information in various modern Natural Language Processing (NLP) applications, a prominent one being Machine Translation (MT).
Effectively computing bitext alignments, however, can be a challenging task.
Discrepancies between languages appear in various ways, from discourse structures to morphological constructions.
Automatic alignments would, at least in most cases, contain noise harmful for the performance of application systems which use the alignments.
To deal with this situation, two research directions emerge: the first is to keep improving alignment techniques; the second is to develop reliable confidence measures which enable application systems to selectively employ the alignments according to their needs.
Both alignment techniques and confidence estimation can benefit from manual alignments.
Manual alignments can be used as both supervision examples to train scoring models and as evaluation materials.
The creation of such data is, however, an important question in itself, particularly at sub-sentential levels, where cross-lingual correspondences can be only implicit and difficult to capture.
This thesis focuses on means to acquire useful sentential and sub-sentential bitext alignments.
Chapter 1 provides a non-technical description of the research motivation, scope, organization, and introduces terminologies and notation.
State-of-the-art alignment techniques are reviewed in Part I.
Chapter 2 and 3 describe state-of-the-art methods for respectively sentence and word alignment.
Chapter 4 summarizes existing manual alignments, and discusses issues related to the creation of gold alignment data.
The remainder of this thesis, Part II, presents our contributions to bitext alignment, which are concentrated on three sub-tasks.
Chapter 5 presents our contribution to gold alignment data collection.
For sentence-level alignment, we collect manual annotations for an interesting text genre: literary bitexts, which are very useful for evaluating sentence aligners.
We also propose a scheme for sentence alignment confidence annotation.
For sub-sentential alignment, we annotate one-to-one word links with a novel 4-way labelling scheme, and design a new approach for facilitating the collection of many-to-many links.
All the collected data is released on-line.
Improving alignment methods remains an important research subject.
We pay special attention to sentence alignment, which often lies at the beginning of the bitext alignment pipeline.
Chapter 6 presents our contributions to this task.
Starting by evaluating state-of-the-art aligners and analyzing their models and results, we propose two new sentence alignment methods, which achieve state-of-the-art performance on a difficult dataset.
The other important subject that we study is confidence estimation.
In Chapter 7, we propose confidence measures for sentential and sub-sentential alignments.
Experiments show that confidence estimation of alignment links is a challenging problem, and more works on enhancing the confidence measures will be useful.
Finally, note that these contributions have been employed in a real world application: the development of a bilingual reading tool aimed at facilitating the reading in a foreign language.
This dissertation is devoted to a social-media-mining problem named the activity-prediction problem.
In this problem one aims to predict the number of user-generated-contents that will be created about a topic in the near future.
In order to study the activity-prediction problem without referring directly to a particular social-media, a generic framework is proposed.
Three defi-nitions of the activity-prediction problem are proposed.
Firstly the magnitude prediction problem defines the activity-prediction as a regression problem.
These three definitions of the activity prediction problem are tackled with state-of-the-art machine learning approaches applied to generic features.
Indeed, these features are defined with the help of the generic framework.
Therefore these features are easily adaptable to various social-media.
The data was prepared so that commercial-contents and technical failure are not sources of noise.
A cross-validation method that takes into account the time of observations is used.
In addition an unsupervised method to extract buzz candidates is proposed.
Indeed the training-sets are very ill-balanced for the buzz classification problem, and it is necessary to preselect buzz candidates.
The activity-prediction problems are studied within two different experimental settings.
The first experimental setting includes data from Twitter and the bulletin-board-system, on a long time-scale, and with three different languages.
The second experimental setting is dedicated specifically to Twitter.
This second experiment aims to increase the reproducibility of experiments as much as possible.
Hence, this experimental setting includes user-generated-contents collected with respect to a list of unambiguous English terms.
This PhD thesis deals with knowledge discovery from Displacement Field Time Series (DFTS) obtained by satellite imagery.
Such series now occupy a central place in the study and monitoring of natural phenomena such as earthquakes, volcanic eruptions and glacier displacements.
These series are indeed rich in both spatial and temporal information and can now be produced regularly at a lower cost thanks to spatial programs such as the European Copernicus program and its famous Sentinel satellites.
Our proposals are based on the extraction of grouped frequent sequential patterns.
Nevertheless, they cannot use the confidence indices coming along with DFTS and the swap method used to select the most promising patterns does not take into account their spatiotemporal complementarities, each pattern being evaluated individually.
Our contribution is thus double.
A first proposal aims to associate a measure of reliability with each pattern by using the confidence indices.
This measure allows to select patterns having occurrences in the data that are on average sufficiently reliable.
We propose a corresponding constraint-based extraction algorithm.
It relies on an efficient search of the most reliable occurrences by dynamic programming and on a pruning of the search space provided by a partial push strategy.
A second contribution for the selection of the most promising patterns is also made.
This one, based on an informational criterion, makes it possible to take into account at the same time the confidence indices and the way the patterns complement each other spatially and temporally.
For this aim, the confidence indices are interpreted as probabilities, and the DFTS are seen as probabilistic databases whose distributions are only partial.
The informational gain associated with a pattern is then defined according to the ability of its occurrences to complete/refine the distributions characterizing the data.
On this basis, a heuristic is proposed to select informative and complementary patterns.
This method provides a set of weakly redundant patterns and therefore easier to interpret than those provided by swap randomization.
It has been implemented in a dedicated prototype.
In addition to being constructed from different data and remote sensing techniques, these series differ drastically in terms of confidence indices, the series covering the Mont-Blanc massif being at very low levels of confidence.
In both cases, the proposed methods operate under standard conditions of resource consumption (time, space), and experts' knowledge of the studied areas is confirmed and completed.
Embodied Conversational Agents are virtual characters which main purpose is to interact with a human user.
They are used in various domains such as personal assistance, social training or video games for instance.
The users, even if they are aware that they interact with a machine, are still capable of analyzing and identifying social behaviors through the signals produced by these virtual characters.
The research in Embodied Conversational Agents has focused for a long time on the reproduction and recognition of emotions by virtual characters and now the focus is on the ability to express different social attitudes.
These attitudes show a behavioral style and are expressed through different modalities of the body, like the facial expressions, the gestures or the gazes for instance.
We proposed a model that allows an agent to produce different nonverbal behaviors expressing different social attitudes in a conversation.
The whole set of behaviors produced by our model allows a goup of agents animated by it to simulate a conversation, without any verbal content.
Two evaluations of the model were conducted, one on the Internet and one in a Virtual Reality environment, to verify that the attitudes produced are well recognized
In this thesis, we focus on how SAR images can be used to study vegetation.
Vegetation lies at the core of human lives by providing both food and economic resources as well as participating in regulating climate.
Traditionally, vegetation is classified into three categories: fields, flooded pastures, and forests.
We follow this classification in our study.
The aim of the first part is to provide a better understanding of the capabilities of Sentinel-1 radar images for agricultural land cover mapping through the use of deep learning techniques.
We revealed that even with classical machine learning approaches (K nearest neighbors, random forest and support vector machines), good performance classification could be achieved with F-measure/Accuracy greater than 86% and Kappa coefficient better than 0.82.
We found that the results of the two deep recurrent neural network (RNN)-based classifiers clearly outperformed the classical approaches.
In the second part, the objective is to study the capabilities of multitemporal radar images for rice height and dry biomass retrievals using Sentinel-1 data.
To do this, we train Sentinel-1 data against ground measurements with classical machine learning techniques (Multiple Linear Regression (MLR), Support Vector Regression (SVR) and Random Forest (RF)) to estimate rice height and dry biomass.
Such results indicate that the highly qualified Sentinel-1 radar data could be well exploited for rice biomass and height retrieval and they could be used for operational tasks.
Finally, reducing carbon emissions from deforestation and degradation (REDD) requires detailed insight into how the forest biomass is measured and distributed.
We aim to improve on previous approaches by using radar satellite ALOS PALSAR (25-m resolution) and optical Landsat-derived tree cover (30-m resolution) observations to estimate forest biomass stocks in Madagascar, for the years 2007-2010.
The radar signal and in situ biomass were highly correlated (R2 = 0.71) and the root mean square error was 30% (for biomass ranging from 0 to 500 t/ha).
Combining radar signal with optical tree cover data appears to be a promising approach for using by L-band SAR to map forest biomass (and hence carbon) over broad geographical scales.
Our work is presented in three separate parts which can be read independently.
Firstly we propose three active learning heuristics that scale to deep neural networks: We scale query by committee, an ensemble active learning methods.
We speed up the computation time by sampling a committee of deep networks by applying dropout on the trained model.
Another direction was margin-based active learning.
We propose to use an adversarial perturbation to measure the distance to the margin.
We also establish theoretical bounds on the convergence of our Adversarial Active Learning strategy for linear classifiers.
Secondly, we focus our work on how to fasten the computation of Wasserstein distances.
We propose to approximate Wasserstein distances using a Siamese architecture.
From another point of view, we demonstrate the submodular properties of Wasserstein medoids and how to apply it in active learning.
First, we hijack an active learning strategy to confront the relevance of the sentences selected with active learning to state-of-the-art phraseology techniques.
These works help to understand the hierarchy of the linguistic knowledge acquired during the training of CNNs on NLP tasks.
Secondly, we take advantage of deconvolution networks for image analysis to present a new perspective on text analysis to the linguistic community that we call Text Deconvolution Saliency.
The aim of this dissertation is to say how the linguistic system of our mind interacts with its conceptual system.
I argue for the thesis that the linguistic system embodies a set of rules applying directly to semantically loaded mental representations.
I first consider proposals by the formalists of the 20th century (Frege, Russell), formal semanticists (Montague, Lewis) and Grice before developing my own account.
After showing that this theory could account for neuropsychological data concerning language processing, I apply it to the problem of the embedded implicatures and say how it could be used in Natural Language Processing (NLP), to clarify the Language of Thought Hypothesis and to study the semantics/pragmatics interface.
This thesis presents a novel method for ensuring cooperation between humans and robots in public spaces, under the constraint of human behavior uncertainty.
The cooperation part can be solved independently from the task and executed as a finite state machine in order to contain online planning effort.
The developed framework has been implemented in a real application scenario as part of the COACHES project.
The thesis describes the Escort mission used as testbed application and the details of implementation on the real robots.
This scenario has as well been used to carry several experiments and to evaluate our contributions.
This thesis aims to establish a discourse interpretation theory named formal hermeneutics that applies rigorous mathematical methods in studying the process of interpretation of natural language texts supposed to be written “with a good grace” as the messages intended for human understanding; we call them admissible.
In the phonocentric paradigm, a natural language is described in the category of textual spaces Logos.
A particular genre of texts defines there a full subcategory of formal discourse schemes.
For a given admissible text X, we introduce the category Schl(X) of sheaves of fragmentary meanings, called category of Schleiermacher, in termes of which a generalized Frege's compositionality principle is formulated, and we also introduce the category Context(X) of étale bundles of contextual meanings in termes of which a generalized Frege's contextuality principle is formulated.
Established by the section-functor and the germ-functor, an equivalence of categories Schl(X)?Context(X), called Frege duality, gives rise to a functional representation for fragmentary meanings that allows one to describe the process of text understanding.
We consider as linguistic universals the connectidness and the Kolmogoroff's T0–separability of the phonocentric topology underlying to a text.
Lexical databases play a significant role in natural language processing (NLP), however, they require permanent development and enrichment through the exploitation of free resources from the semantic web, among others, Wikipedia, DBpedia, Geonames and Yago2.
Prolexbase, which issued of numerous studies on NLP, has ten languages, three of which are well covered: French, English and Polish.
It was manually designed; the first semiautomatic attempt was made by the ProlexFeeder project (Savary et al., 2013).
The objective of our work was to create an automatic updating and extension tool for Prolexbase, and to introduce the Arabic language.
In addition, a fully automatic system has been implemented to calculate, via Wikipedia, the notoriety of the entries of Prolexbase.
This notoriety is language dependent, is the first step in the construction of an Arabic module of Prolexbase, and it takes a part in the notoriety revision currently present for the other languages in the database.
A graph is a set of nodes, together links connecting pairs of nodes.
With the accumulating amount of data collected, there is a growing interest in understanding the structures and behavior of very large graphs.
Nevertheless, the rapid increasing in size of large graphs makes studying the entire graphs becomes less and less efficient.
Thus, there is a compelling demand for more effective methods to study large graphs without requiring the knowledge of the graphs in whole.
One promising method to understand the behavior of large graphs is via exploiting specific properties of local structures, such as the size of clusters or the presence locally of some specific pattern, i.e. a given (usually small) graph.
A classical example from Graph Theory (proven cases of the Erdos-Hajnal conjecture) is that if a large graph does not contain some specific pattern, then it must have a set of nodes pairwise linked or not linked of size exponentially larger than expected.
This thesis will address some aspects of two fundamental questions in Graph Theory about the presence, abundantly or scarcely, of a given pattern in some large graph:
- Can the large graph be partitioned into copies of the pattern?
- Does the large graph contain any copy of the pattern?
We will discuss some of the most well-known conjectures in Graph Theory on this topic: the Tutte's flow conjectures on flows in graphs and the Erdos-Hajnal conjecture mentioned above, and present proofs for several related conjectures -- including the Barát-Thomassen conjecture, a conjecture of Haggkvist and Krissell, a special case of Jaeger-Linial-Payan-Tarsi's conjecture, a conjecture of Berger et al, and another one by Albouker et al.
Analyse of entities representation over the Web 2.0
Every day, millions of people publish their views on Web 2.0 (social networks,blogs, etc.).
These comments focus on subjects as diverse as news, politics,sports scores, consumer objects, etc.
This idea carries a priori on a particular subject and is only valid in context for a given time.
This perceived image is different from the entity initially wanted to broadcast (e.g. via a communication campaign).
Moreover,in reality, there are several images in the end living together in parallel on the network, each specific to a community and all evolve differently over time(imagine how would be perceived in each camp together two politicians edges opposite).
Finally, in addition to the controversy caused by the voluntary behavior of some entities to attract attention (think of the declarations required or shocking).
It also happens that the dissemination of an image beyond the framework that governed the and sometimes turns against the entity (for example,«marriage for all» became «the demonstration for all»).
The views expressed then are so many clues to understand the logic of construction and evolution of these images.
The aim is to be able to know what we are talking about and how we talk with filigree opportunity to know who is speaking.
In this thesis we propose to use several simple supervised statistical automatic methods to monitor entity's online reputation based on textual contents mentioning it.
More precisely we look the most important contents and theirs authors (from a reputation manager point-of-view).
We introduce an optimization process allowing us to enrich the data using a simulated relevance feedback(without any human involvement).
We also compare content contextualization method using information retrieval and automatic summarization methods.
We also propose a reflection and a new approach to model online reputation, improve and evaluate reputation monitoring methods using Partial Least Squares Path Modelling (PLS-PM).
Thiopurines are cytotoxic and immunosuppressive drugs widely prescribed, mainly in inflammatory bowel disease (IBD).
Optimization of thiopurine response is challenging because of its large interindividual variability such as inefficacy and toxicities.
This thesis has explored, on one hand, the relationships between TPMT activity and metabolite concentrations, and on the other hand, factors associated with thiopurine inefficacy.
Furthermore, a retrospective study in pediatric IBD identified factors predicting the occurrence of lymphopenia during thiopurine therapy.
Finally, using a lymphoblastoid cell line (LCL) in vitro model, we established a transcriptomic signature, including 32 genes predicting thiopurine cellular resistance.
A bioinformatic functional analysis identified metabolic pathways in relation with p53 and cell cycle, as well as molecular mechanisms associated with thiopurine resistance.
To conclude, this research work, focusing on the variability of thiopurine response and mainly therapeutic resistance, provides new hypotheses to individualize and optimize therapeutic response to thiopurines.
Although there are several models of visual saliency, in terms of contrast and cognition, there is no hybrid model integrating both mechanisms of attention: the visual aspect and the cognitive aspect.
To carryout such a model, we have explored existing approaches in the field of visual attention, as well as several approaches and paradigms in related fields (such as object recognition, artificial learning, classification, etc.).A functional architecture of a hybrid visual attention system, combining principles and mechanisms derived from human visual attention with computational and algorithmic methods, was implemented, explained and detailed.
The carried out studies and experimental validation of the proposed models confirmed the relevance of the proposed approach in increasing the autonomy of robotic systems within a real environment
Deep neural networks have achieved impressive results in many AI-related fields, ranging from computer games, to computer vision and natural language processing, to mention a few.
Architecture engineering is a major part of neural network development.
It allowed for the error rate on ImageNet to go down from 16.4% (AlexNet) to 3.57% (ResNet-152).
Designing architectures is time consuming and difficult; automatic method have been developed to learn an architectures, and in recent years they used massive computational power for reaching state of the art performance on various problems.
The aim of this PhD project, in collaboration between Paris Dauphine and Facebook AI Research Paris, includes:
Contributing to the state of the art by designing and developing novel derivative-free optimization methods for architecture search.
Find new architectures offering better accuracy or interesting compromises between size, computational speed and performance.
Generalizing architecture to learn more efficient architectures for reinforcement learning, domain adaptation, or to handle issues arising when the distribution of the test data is different from that of the train data (covariate shift).
During the development of complex systems, several enterprises exchange a large number of heterogeneous models and requirements.
During the phases of the system's life cycle, these artifacts, linked to each other and derived from different modelling tools, are constantly evolving.
In such environment, it is necessary to manage the impact of the different changes occurring in the different design spaces.
Traceability meets this need.
However, establishing links between requirements and models in complex systems engineering requires dealing with a large volume of artifacts.
For example, a specification of an autonomous vehicle with 3,000 requirements and 400 model elements, it would theoretically be necessary to check about one million of potential links.
Although several approaches have been proposed for identifying traceability links, the validation process is always time-consuming and error-prone.
This approach provides a quantitative confidence measure on each candidate link.
While information is abundant in the world, structured, ready-to-use information is rare.
This work proposes Information Extraction (IE) as an efficient approach for producing structured,usable information on biology, by presenting a complete IE task on a model biological organism, Arabidopsis thaliana.
Information Extraction is the process of extracting meaningful parts of text and identifying their semantic relations.
In collaboration with experts on the plant A. Thaliana, a knowledge model was conceived.
The goal of this model is providing a formal representation of the knowledge that is necessary to sufficiently describe the domain of grain development.
This model contains all the entities and the relations between them which are essential and it can directly be used by algorithms.
In parallel, this model was tested and applied on a set of scientific articles of the domain. These documents constitute the corpus which is needed to train machine learning algorithms.
The experts annotated the text using the entities and relations of the model.
This corpus and this model are the first available for grain development and among very few on A. Thaliana, despite the latter's importance in biology.
This model manages to answer both needs of being complex enough to describe the domain well, and of having enough generalization for machine learning.
A relation extraction approach (AlvisRE) was also elaborated and developed.
After entity recognition, the relation extractor tries to detect the cases where the text mentions that two entities are in a relation, and identify precisely to which type of the model these relations belong to.
AlvisRE's approach is based on textual similarity and it uses all types of information available:lexical, syntactic and semantic.
In the tests conducted, AlvisRE had results that are equivalent or sometimes better than the state of the art.
Additionally, AlvisRE has the advantage of being modular and adaptive by using semantic information that was produced automatically.
This last feature allows me to expect similar performance in other domains.
Inflexion rules cannot envisage all possible models of the Macedonian conjugaison and their approach is too synthetic to be fully operational from a didactic point of view.
For all these reasons, the purpose of this doctoral thesis is to study a large number of conjugated verbs in order to map stable patterns opening up new forays into the teaching of the Macedonian verbal system.
Supervisory Control Theory (SCT) is one of the most important formal paradigms for developing controllers for Discrete Event Systems (DESs).
The large number of scientific contributions shows that SCT catches extensive academic interest and this theory has been proved to be applicable in various industrial domains such as manufacturing systems, embedded systems, transportation systems and energy systems.
With SCT, the requirements which are checked afterward in conventional engineering are used as input for generation of the design of the controller the verification of this model can be eliminated.
However the SCT suffers from an important lack of integration in a global design process which leads to the gaps between the theoretical development and applications of SCT within engineering practice.
The Model-Based System Engineering (MBSE) provides the solutions to deal with the limitations of SCT.
The objective of this study is to propose a novel framework for automatic control (AC) which integrates both SCT and MBSE to bridge the gaps formal paradigm and engineering process.
In the proposed framework, different SysML diagrams are used to as complementary models which present the indispensible views of the system to be studied in the global modeling process.
Secondly, in order to keep the consistency between SysML models and formal models, methods for formal modeling and verification are proposed.
A case study introduced at the end prove that the proposed framework which provides a global development process from requirement analysis to controller implementation can well meet the needs of engineering practice.
Text is one of the most pervasive and persistent sources of information.
Content analysis of text in its broad sense refers to methods for studying and retrieving information from documents.
Nowadays, with the ever increasing amounts of text becoming available online is several languages and different styles, content analysis of text is of tremendous importance as it enables a variety of applications.
To this end, unsupervised representation learning methods such as topic models and word embeddings constitute prominent tools.
The goal of this dissertation is to study and address challenging problems in this area, focusing on both the design of novel text mining algorithms and tools, as well as on studying how these tools can be applied to text collections written in a single or several languages.
In the first part of the thesis we focus on topic models and more precisely on how to incorporate prior information of text structure to such models.
Topic models are built on the premise of bag-of-words, and therefore words are exchangeable.
While this assumption benefits the calculations of the conditional probabilities it results in loss of information.
To overcome this limitation we propose two mechanisms that extend topic models by integrating knowledge of text structure to them.
We assume that the documents are partitioned in thematically coherent text segments.
The first mechanism assigns the same topic to the words of a segment.
The second, capitalizes on the properties of copulas, a tool mainly used in the fields of economics and risk management that is used to model the joint probability density distributions of random variables while having access only to their marginals.
Typically, a document collection for such models is in the form of comparable document pairs.
The documents of a pair are written in different languages and are thematically similar.
Unless translations, the documents of a pair are similar to some extent only.
Meanwhile, representative topic models assume that the documents have identical topic distributions, which is a strong and limiting assumption.
To overcome it we propose novel bilingual topic models that incorporate the notion of cross-lingual similarity of the documents that constitute the pairs in their generative and inference processes.
The last part of the thesis concerns the use of word embeddings and neural networks for three text mining applications.
First, we discuss polylingual document classification where we argue that translations of a document can be used to enrich its representation.
Using an auto-encoder to obtain these robust document representations we demonstrate improvements in the task of multi-class document classification.
Second, we explore multi-task sentiment classification of tweets arguing that by jointly training classification systems using correlated tasks can improve the obtained performance.
To this end we show how can achieve state-of-the-art performance on a sentiment classification task using recurrent neural networks.
The third application we explore is cross-lingual information retrieval.
Given a document written in one language, the task consists in retrieving the most similar documents from a pool of documents written in another language.
In this line of research, we show that by adapting the transportation problem for the task of estimating document distances one can achieve important improvements.
In this thesis, we consider the problem of situation awareness, and more specifically, in crisis situation.
We propose an automated situation awareness system (ASAAP system) which tries to mitigate these problems.
We present a dynamic environment modelling allowing an analysis of most valuable variables for situation assessment in order to reduce the field of possibilities and maximise the information gain.
In the crisis situation field, we also propose to apply our system on a more specific domain, the threat assessment.
This contribution allows to define and analyse the threat level of each exposed zones with the capacity of understanding the enemy's strategy by both representing its targets and the path to reach them.
Finally, we present a preliminary approach about the optimisation of the coverage of most valuable variables by the sensors to maximise the information gain.
We generated scenarios inspired from real situations to evaluate ours approaches in maritime and military field.
The ASAAP system results show an improvement in situation awareness in its complexity and by its capacity to describes the enemy's strategy in reasonable time.
This thesis focuses on learning methods for automatic transcription of the battery.
They are based on a transcription algorithm using a non-negative decomposition method, NMD.
This thesis raises two main issues: the adaptation of methods to the analyzed signal and the use of deep learning.
Taking into account the information of the signal analyzed in the model can be achieved by their introduction during the decomposition steps.
A first approach is to reformulate the decomposition step in a probabilistic context to facilitate the introduction of a posteriori information with methods such as SI-PLCA and statistical NMD.
A second approach is to implement an adaptation strategy directly in the NMD: the application of modelable filters to the patterns to model the recording conditions or the adaptation of the learned patterns directly to the signal by applying strong constraints to preserve their physical meaning.
The second approach concerns the selection of the signal segments to be analyzed.
The results obtained being very interesting, the detector is trained to detect only one instrument allowing the transcription of the three main drum instruments with three CNNs.
Finally, the use of a CNN multi-output is studied to transcribe the part of battery with a single network.
Since then, their propularity constantly raised in communication systems.
Being images representing either an idea, a concept, or an emotion, emojis are available to the users in multiple software contexts: instant messaging, emails, forums, and other types of social medias.
Their usage grew constantly and, associated to the constant addition of new emojis, there are now more than 2,789 standard emojis since winter 2018.
To access a specific emoji, scrolling through huge emoji librairies or using a emoji search engines is not enough to maximize their usage and their diversity.
An emoji recommendation system is required.
To answer this need, we present our research work facused on the emoji recommendation topic.
The objectives are to create an emoji recommender system adapted to a private and informal conversationnal context.
This system must enhance the user experience, the communication quality, and take into account possible new emerging emojis.
Our first contribution is to show the limits of a emoji prediction for the real usage case, and to demonstrate the need of a more global recommandation.
We also veifie the correlation between the real usage of emojis representing facial expressions and a related theory on facial expressions.
We also tackle the evaluation part of this system, with the metrics'limits and the importance of a dedicated user interface.
Alzheimer's disease (AD) is the first cause of dementia worldwide, affecting over 20 million people.
Its diagnosis at an early stage is essential to ensure a proper care of patients, and to develop and test novel treatments.
AD is a complex disease that has to be characterized by the use of different measurements: cognitive and clinical tests, neuroimaging including magnetic resonance imaging (MRI) and positron emission tomography (PET), genotyping, etc.
There is an interest in exploring the discriminative and predictive capabilities of these diverse markers, which reflect different aspects of the disease and potentially carry complementary information, from an early stage of the disease.
The objective of this PhD thesis was thus to assess the potential and to integrate multiple modalities using machine learning methods, in order to automatically classify patients with AD and predict the development of the disease from the earliest stages.
More specifically, we aimed to make progress toward the translation of such approaches toward clinical practice.
The thesis comprises three main studies.
The first one tackles the differential diagnosis between different forms of dementia from MRI data.
This study was performed using clinical routine data, thereby providing a more realistic evaluation scenario.
The second one proposes a new framework for reproducible evaluation of AD classification algorithms from MRI and PET data.
Indeed, while numerous approaches have been proposed for AD classification in the literature, they are difficult to compare and to reproduce.
The third part is devoted to the prediction of progression to AD in patients with mild cognitive impairment through the integration of multimodal data, including MRI, PET, clinical/cognitive evaluations and genotyping.
In particular, we systematically assessed the added value of neuroimaging over clinical/cognitive data only.
Since neuroimaging is more expensive and less widely available, this is important to justify its use as input of classification algorithms.
This thesis outlines a semantic approach to the mining and analysis of ideological discourse from electronic texts.
This approach integrates a qualitative social scientific method of textual analysis (Critical Discourse Analysis) with a quantitative ontology-based reasoning and information retrieval method using semi-automatic natural language processing techniques.
It is applied to the analysis of Marxist discourse, as represented in the Marxists Internet Archive (http://www.marxists.org) thematic collection, containing nearly 15,000 texts.
The application focuses on the acquisition of emerging schemas, which can contribute to the classification of unknown texts by ideological perspective.
However, the physical laboratories hosting these activities rely on expensive infrastructures that make very difficult for institutions to cope with the high increase of the students'population.
Within this context, virtual and remote laboratories (VRL) bring an affordable alternative to provide practical activities at scale.
Numerous research works have come up for the last decade; they mainly focused on technological issues such as the federation of remote laboratories, their standardization, or the pooling of the resources they provide.
Nevertheless, the recent literature reviews highlight the need to pay more attention to the educational facets of these innovative learning environments.
With that purpose in mind, our works make use of the learners'traces collected through their practical learning sessions to sustain socio-constructivist theories, on which practical activities rely on, and thus to engage students in their learning tasks and further their reflection.
Starting from the study of scientific research, we identify as a first step a set of criteria required to design practical learning systems that support social interactions between learners.
Indeed, Lab4CE builds on learning analytics to enable different forms of learning such as collaboration, cooperation, or peer assistance, but also to supply learners as well as teachers awareness and reflection tools that aim at promoting deep learning during and after practical activities.
Moreover, theses experimentations suggested a significant correlation between, on the one hand, student's activity in the environment and the learning strategies they apply and, on the other hand, their academic performance.
These first results allow us to assess that socio-constructivist theories leverage engagement and reflection within VRL.
They also invite us to put our approach into practice in other learning settings, but also to extend the sources of information to deal with our behavioral analyses in depth, and thus to enhance our contributions regarding the adoption of practical learning within technological environments.
The most recent linguistic studies show that proper names are often translated or adapted, in contrast to the traditional theories of untranslatability of proper names.
What distinguishes toponyms from the rest of onomastics are political, sociological and historical implications that affect many geographical names.
Using the name "Breslau" for the Polish city "Wrocław" may convey negative connotations depending on the context.
Nevertheless, this form is often found on the Internet.
Moreover, due to globalization we are witnessing the multiplication of many versions of the same toponym.
This work is composed of three parts.
The first part presents theories of proper names,but adapted to toponyms, and the second describes their functions and their linguistic status according to the international standardization.
The concept of synchronic-contrastive toponymy and methods of analysing toponyms according to this approach are introduced as well in the second part.
The third part is an analysis of the corpus with the purpose of observing the structure and the integration of Polish toponyms into the French language, as well as their popular use in current publications (Internet, tourist brochures, in applications and social networks, etc.) different from the official use, which is supposed to be politically correct.
With the advent of the "big data", many repercussions have taken place in all fields of information technology, advocating innovative solutions with the best compromise between cost and accuracy.
In this thesis we discuss two main problems: first, the problem of partitioning graphs is approached from a perspective big data, where massive graphs are partitioned in streaming.
We study and propose several models of streaming partitioning and we evaluate their performances both theoretically and empirically.
In a second step, we are interested in querying distributed / partitioned graphs.
This thesis studies the production of nabol, the narrative practices of the Nisvai language community, located in southeast Malekula, Vanuatu.
Based on a request from Nisvai speakers for language resources to be produced for the local school, a corpus of oral texts was compiled to show that nabol are produced according to the situation of enunciation and local social issues related to the speaker's age set.
The corpus of annotated oral texts results from research trips carried out between 2011 and2015, totalling 14 months in the field with the Nisvai community.
On one hand, nabol are studied using concepts from textual linguistics to describe the discursive processes used by Nisvai speakers.
From these processes, it was possible to compare the organisation of the nabol and to highlight significant variations according to the situation of enunciation.
On the other hand, participant observation and guided interviews helped identify social issues that speakers associated with their narrative practices.
The use of proper nouns of characters or places in the narrative is part of a regime of truth.
Depending on his or her age set, the speaker must name or omit the names of the characters who take part in the plot at the risk of being criticised by their peers.
Practices and standards from the language documentation program and Natural Language Processing have provided tools to develop language resources relevant for the study of narratives and their use by the Nisvai community.
Two paper resources are joined as appendices: a bilingual Nisvai-French lexicon and a bilingual collection of texts from the corpus. They are designed for Nisvai speakers and their French-speaking school.
In addition, two online resources, a reading-listening interface and an annotation consultation interface, have been developed to communicate the work to researchers working on oral narrative practices or Vanuatu languages.
Additionally, a custom, hardened data glove was built and used to demonstrate successful gesture recognition and real-time robot control.
We finally show this work's flexibility by furthermore using it beyond robot control to drive other kinds of controllable systems.
Internet of Things (IoT) is leading to a paradigm shift within the logistics industry.
The advent of IoT has been changing the logistics service management ecosystem.
Logistics services providers today use sensor technologies such as GPS or telemetry to collect data in realtime while the delivery is in progress.
The realtime collection of data enables the service providers to track and manage their shipment process efficiently.
The key advantage of realtime data collection is that it enables logistics service providers to act proactively to prevent outcomes such as delivery delay caused by unexpected/unknown events.
Data from such external sources enrich the dataset and add value in analysis.
Besides, collecting them in real-time provides an opportunity to use the data for on-the-fly analysis and prevent unexpected outcomes (e.g., such as delivery delay) at run-time.
However, data are collected raw which needs to be processed for effective analysis.
Collecting and processing data in real-time is an enormous challenge.
The main reason is that data are stemming from heterogeneous sources with a huge speed.
The high-speed and data variety fosters challenges to perform complex processing operations such as cleansing, filtering, handling incorrect data, etc.
The variety of data – structured, semi-structured, and unstructured – promotes challenges in processing data both in batch-style and real-time.
Different types of data may require performing operations in different techniques.
A technical framework that enables the processing of heterogeneous data is heavily challenging and not currently available.
Therefore, in order to exploit Big Data in logistics service processes, an efficient solution for collecting and processing data in both realtime and batch style is critically important.
In this thesis, we developed and experimented with two data processing solutions: SANA and IBRIDIA.
SANA is built on Multinomial Naïve Bayes classifier whereas IBRIDIA relies on Johnson's hierarchical clustering (HCL) algorithm which is hybrid technology that enables data collection and processing in batch style and realtime.
SANA is a service-based solution which deals with unstructured data.
It serves as a multi-purpose system to extract the relevant events including the context of the event (such as place, location, time, etc.).
In addition, it can be used to perform text analysis over the targeted events.
IBRIDIA was designed to process unknown data stemming from external sources and cluster them on-the-fly in order to gain knowledge/understanding of data which assists in extracting events that may lead to delivery delay.
According to our experiments, both of these approaches show a unique ability to process logistics data.
Scientists have developed the Virtual Observatory (VO) concept in order to make the most of the large masses of heterogeneous data produced by the modern scientific instruments of astrophysics.
It is a service-oriented architecture, aiming to facilitate the identification and interoperability of astrophysical data.
Despite the development and advances made by VO in the exploitation of these data, some objectives are partially such as interoperability, service selection and identification of related services, etc.
In addition, the ergonomics of the tools available to the end user can be improved.
Similarly, the current use of VO resources, based on human skills, would benefit from being automated.
As not all the astrophysical data services are included in the VO, it would also be desirable to allow a wider use of these tools, as they also rely on services available outside the VO.
In order to automate the use of online resources, information sciences have been working since 2001 on the development of the Semantic Web.
This evolution provides the Web with automatic reasoning abilities, based on algorithms using a new form of content description.
This new form of semantic description is expressed in computer representations called ontologies.
Unfortunately, the current semantic Web development methods are not fully compatible with VO services that use data models, formats and protocols for accessing services that differ from those typically encountered in information sciences.
In this context, this thesis describes a generic methodology for the composition of stateless services, based on the description of services by a global ontology, the definition of which is proposed in this document.
This ontology represents both Web services and services that are not accessible via the Web.
It takes into account certain specificities that may be encountered in preexisting service infrastructures.
The population of this ontology, by services possibly distant from the standards usually used in the information sciences, is also treated.
The methodology was applied successfully in the framework of astrophysics, and allowed to develop a Web application allowing the automatic composition of services usable by an uninformed public.
Computer-assisted translation and terminology management tools are often used to meet the needs in management of multilingual and monolingual writings.
These tools facilitate the access to technical terms and expressions that are related to areas of specialty, and essential to any communication process.
The understanding of technical terms can be potentiated by their “contextualization”.
However, having access to a term or its translation is not enough, since it is also necessary to be able to use it properly and to understand its exact meaning.
Thus, this contextualization is estabilished on two levels: in texts and in the terminology.
In texts, the user must have access to information regarding the use of terms, namely linguistic knowledge-rich contexts.
In the terminology, the user requires access to semantic or conceptual relationships between the terms to better understand its meaning, namely conceptual rich-knowlegde contexts.
We continued our work in a bilingual phase part of specialized translation, under continuous revision.
We propose a new generation of bilingual concordancers that take as input a term and its translation, and provides not parallel, but aligned Knowledge-Rich Contexts from specialized comparable corpora.
The evaluation show that our concordancer can assist revisers despite the difficulty of the task.
Online Social Networks have taken a huge place in the informational space and are often used for advertising, e-reputation, propaganda, or even manipulation, either by individuals, companies or states.
The amount of information makes difficult the human exploitation, while the need for social network analysis remains unsatisfied: trends must be extracted from the posted messages, the user behaviours must be characterised, and the social structure must be identified.
To tackle this problem, we propose a system providing analysis tools on three levels.
First, the message analysis aims to determine the opinions they bear.
The first is constituted of messages published on Twitter, gathering every activity performed by a set of 5,000 accounts on a long period.
The second stems from a ToR-based social network, named Galaxy2, and includes every public action performed on the platform during its uptime.
We evaluate the relevance of our system on these two datasets, showing the complementarity of user account characterisation tools (influence, behaviour and role), and user account communities (interaction strength, thematic cohesion), enriching the social graph exploitation with textual content elements.
The methodology is based on three pillars: definition, search / analysis and innovation.
A comprehensive definition of the main function of the industrial system delimits the research field and allows the retrieval of initial keywords through a detailed analysis of what is currently available.
The iterative patent search is based on functional decomposition and physical analysis.
The analysis phase uses energy functional decomposition to identify energies, transmitted functional flows and physical phenomena involved in the energy conversion process in order to select potentially relevant physical effects.
To delineate the exploration field we formulate search queries from a keywords database composed by initial, physical, and technological keywords.
A discovery matrix based on the intersections between these keywords allows the classification of pertinent patents.
The research for innovation opportunities exploits the discovery matrix in order to decipher the evolutionary trends followed by inventions.
Opportunities are deduced from an analysis of the discovery matrix empty cells, an analysis of the evolution trends, and from changing the concept by energy converter substitution.
We propose evolution trends constructed from the evolution laws of TRIZ theory, design heuristics, and rules of the art of the engineering.
An application case concerning the study of the evolution and the proposal of innovative biphasic separation systems in deep offshore highlights the method.
Our thesis unifes paradigms of universal darwinism, developmental psycholinguistics and computational linguistics in order to furnish a novel account of language development in human as well as artificial language-acquiring agents.
Thesis is preceded by a supplementary volume called "Conceptual Foundations" which presents a so-called "Theory of Intramental Evolution" which postulates that ontogeny of an individual mind can be interpreted and even simulated as a process involving replication, variation and selection of information-encoding cognitive structures.
The dissertation itself presents four distinct simulations addressing four distinct problems.
Zeroth simulation illustrates how evolutionary optimization could lead to discovery of useful insights concerning the cryptological riddle known as Voynich Manuscript.
The first simulation shows how theory of prototypes, vector symbolic architectures and evolutionary optimization can be mutually combined in order to yield a novel supervised machine-learning method.
Second simulation uses a similiar approach in order to indicate that evolutionary optimization can discover minimalist and lightweight constellations of part-of-speech taggers.
The last simulation targets the "holy grail" of computational linguistics, i.e. the problem of "grammar induction" and shows that the problem can be potentially solved by using an evolutionary strategy able to bridge the gap between subsymbolic realm of vector spaces and symbolic realm of grammar-representing regular expressions.
Internet as well as all the modern media of communication, information and entertainment entails a massive increase of digital data quantities.
Automatically processing and understanding these massive data enables creating large knowledge bases, more efficient search, social medial research, etc.
Natural language processing research concerns the design and development of algorithms that allow computers to process natural language in texts, audios, images or videos automatically for specific tasks.
Due to the complexity of human language, natural language processing of text can be divided into four levels: morphology, syntax, semantics and pragmatics.
Current natural language processing technologies have achieved great successes in the tasks of the first two levels, leading to successes in many commercial applications such as search.
However, advanced structured search engine would require computers to understand language deeper than at the morphology and syntactic levels.
Information extraction is designed to extract meaningful structural information from unannotated or semi-annotated resources to enable advanced search and automatically create knowledge bases for further use.
This thesis studies the problem of information extraction in the specific domain of biomedical event extraction.
We propose an efficient solution, which is a trade-off between the two main trends of methods proposed in previous work.
This solution reaches a good balance point between performance and speed, which is suitable to process large scale data.
It achieves competitive performance to the best models with a much lower computational complexity.
While designing this model, we also studied the effects of different classifiers that are usually proposed to solve the multi-class classification problem.
We also tested two simple methods to integrate word vector representations learned by deep learning method into our model.
Even if different classifiers and the integration of word vectors do not greatly improve the performance, we believe that these research directions carry some promising potential for improving information extraction.
Following these constraints and using adapted automatic art production systems, notably based on artificial ant colony algorithms, we developed two computer programs.
The first one is a virtual music instrument, allowing most people to play music and providing and automatic accompaniment.
The second one is a drawing workshop with generative methods-based tools provide complex results from simple actions.
This PhD thesis details the development of this two programs and their evaluations, with real users meetings.
This thesis studies the link between institutions, gender and politics.
Three questions are studied: can institutions undo gender norms?
In particular, we study the consequences of institutions on the perpetuation of gender norms.
We study the norm according to which a woman should earn less than her husband.
Using the German division as a natural experiment, we show that East German institutions have undone gender.
East German women can earn more than their husband without increasing their number of housework hours, put their marriage at risk, or withdraw from the labor market.
By contrast, the norm of higher male income and its consequences are still prevalent in the West.
The second chapter studies whether institutions would be more gender-egalitarian if more women were heading them.
In particular, I test whether female politicians have the same priorities than their male counterparts.
The context studied is the French Parliament from 2001 to 2017.
Using text analysis and quasi-experimental variations to randomize legislators'gender, this chapter shows that women are twice more likely to initiate women-related amendments in the Lower House.
Women's issues constitute the key topic on which women are more active, followed by health and childhood issues whereas men are more active on military issues.
I provide supporting evidence that these results are driven by the individual interest of legislators.
Finally, I replicate these results in the Upper House by exploiting the introduction of a gender quota.
The third chapter studies the reasons behind the underrepresentation of women in positions of power.
I investigate whether the persistence of incumbents hinders female access to political positions when incumbents are predominantly men.
Despite a context increasingly favorable to the election of women, I find that the persistence of incumbents does not block female access to the position of mayor.
I investigate the mechanisms and show that it is more difficult for a woman to replace a female incumbent than a male one.
Digital system are now part of our society.
They are used in a wide range of domains and in particular they have to handle delicate tasks.
Already used in domains such as transportation, surgery or economy, we speak now of using digital systems for social or political matters: electronic vote, selection algorithms, electoral profiling dots.
For task handled by algorithm, the responsibility is moved from the executioner to the designer, developer and tester of those algorithms.
It is also the responsibility of computer scientists who study those algorithms to propose reliable techniques of verification which will be applicable in the design, the development or the testing phase.
Formal verification methods provide mathematical tools to prevent executions error in all phases.
Among them, fault-diagnosis consist on the construction of a diagnoser based on a formal model of the system we aim to check.
The diagnoser runs in parallel with the real system and emit a warning anytime it detect a dangerous behavior.
For systems modeled by timed automata, it is not always possible to construct a timed automaton to diagnose it.
Indeed timed automata,introduce in the nineties by cite{AD94} and widely studied and used since to model timed systems, are not determinizable.
A machine, more powerful than a timed automaton,can still be used to construct the diagnoser of a timed automaton as it is done in cite{Tripakis02}.
This thesis work aim at constructing a diagnoser for any one-clock timed automata.
This diagnoser is constructed with the help of a machine more powerful than timed automata, following the idea of cite{Tripakis02}.
Part~I of this thesis introduce a formal framework for the modeling of quantitative systems and the study of their determinization.
In this framework we introduce automata on timed structures,the model used to construct the diagnoser.
Part~II study the determinization problem of automata on timed structures, and particularly the one of timed automata determinization in this framework.
Part~III illustrate how automata on timed structures can be used to construct in a generic way a diagnoser for one clock timed automata.
This technique is implemented in a tool, DOTA, and is compared to the technique used in cite{Tripakis02}.
Large dimensional data and learning systems are ubiquitous in modern machine learning.
As opposed to small dimensional learning, large dimensional machine learning algorithms are prone to various counterintuitive phenomena and behave strikingly differently from the low dimensional intuitions upon which they are built.
Nonetheless, by assuming the data dimension and their number to be both large and comparable, random matrix theory (RMT) provides a systematic approach to assess the (statistical) behavior of these large learning systems, when applied on large dimensional data.
The major objective of this thesis is to propose a full-fledged RMT-based framework for various machine learning systems: to assess their performance, to properly understand and to carefully refine them, so as to better handle large dimensional problems that are increasingly needed in artificial intelligence applications.
Precisely, we exploit the close connection between kernel matrices, random feature maps, and single-hidden-layer random neural networks.
Under a simple Gaussian mixture modeling for the input data, we provide a precise characterization of the performance of these large dimensional learning systems as a function of the data statistics, the dimensionality, and most importantly the hyperparameters (e.g., the choice of the kernel function or activation function) of the problem.
Further addressing more involved learning algorithms, we extend the present RMT analysis framework to access large learning systems that are implicitly defined by convex optimization problems (e.g., logistic regression), when optimal points are assumed reachable.
To find these optimal points, optimization methods such as gradient descent are regularly used.
Aiming to have a better theoretical grasp of the inner mechanism of optimization methods and their impact on the resulting learning model, we further evaluate the gradient descent dynamics in training convex and non-convex objects.
These preliminary studies provide a first quantitative understanding of the aforementioned learning algorithms when large dimensional data are processed, which further helps propose better design criteria for large learning systems that result in remarkable gains in performance when applied on real-world datasets.
Emotions and their expressions by virtual characters are two important issues for future affective human-machine interfaces.
Recent advances in psychology of emotions as well as recent progress in computer graphics allow us to animate virtual characters that are capable of expressing emotions in a realistic way through various modalities.
Existing virtual agent systems are often limited in terms of underlying emotional models, visual realism, and real-time interaction capabilities.
In our research, we focus on virtual agents capable of expressing emotions through facial expressions while interacting with the user.
Our work raises several issues: How can we design computational models of emotions inspired by the different approaches to emotion in Psychology?
What is the level of visual realism required for the agent to express emotions?
How can we enable real-time interaction with a virtual agent?
How can we evaluate the impact on the user of the emotions expressed by the virtual agent?
Our work focuses on computational modeling of emotions inspired by psychological theories of emotion and emotional facial expressions by a realistic virtual character.
Facial expressions are known to be a privileged emotional communication modality.
Our main goal is to contribute to the improvement of the interaction between a user and an expressive virtual agent.
For this purpose, our research highlights the pros and cons of different approaches to emotions and different computer graphics techniques.
We worked in two complementary directions.
First, we explored different approaches to emotions (categorical, dimensional, cognitive, and social).
For each of these approaches, a computational model has been designed together with a method for real-time facial animation.
Our second line of research focuses on the contribution of visual realism and the level of graphic detail of the expressiveness of the agent.
This axis is complementary to the first one, because a greater level of visual detail could contribute to a better expression of the complexity of the underlying computational model of emotion.
Our work along these two lines was evaluated by several user-based perceptual studies.
The combination of these two lines of research is seldom in existing expressive virtual agents systems.
Our work opens future directions for improving human-computer interaction based on expressive and interactive virtual agents.
MARC has been used in various kinds of applications: games, ubiquitous intelligence, virtual reality, therapeutic applications, performance art, etc.
Thousands of neuroimaging studies are published every year.
Exploiting this huge amount of results is difficult.
Indeed, individual studies lack statistical power and report many spurious findings.
Even genuine effects are often specific to particular experimental settings and difficult to reproduce.
Meta-analysis aggregates studies to identify consistent trends in reported associations between brain structure and behavior.
The standard approach to meta-analysis starts by gathering a sample of studies that investigate a same mental process or disease.
Then, a statistical test delineates brain regions where there is a significant agreement among reported findings.
In this thesis, we develop a different kind of metaanalysis that focuses on prediction rather than hypothesis testing.
We build predictive models that map textual descriptions of experiments, mental processes or diseases to anatomical regions in the brain.
Our supervised learning approach comes with a natural quantitative evaluation framework, and we conduct extensive experiments to validate and compare statistical models.
We collect and share the largest existing dataset of neuroimaging studies and stereotactic coordinates.
This dataset contains the full text and locations of neurological observations for over 13 000 publications.
Standard meta-analysis is an essential tool to distinguish true discoveries from noise and artifacts.
This thesis introduces methods for predictive metaanalysis, which complement the standard approach and help interpret neuroimaging results and formulate hypotheses or formal statistical priors.
The ever-expanding volume of available audio and multimedia data has elevated technologies related to content indexing and structuring to the forefront of research.
Speaker diarization involves the detection of speaker turns within an audio document (segmentation) and the grouping together of all same-speaker segments (clustering).
Indeed we first introduce a new purification component leading to competitive performance to the bottom-up approach.
Moreover, while investigating the two diarization approaches more thoroughly we show that they behave differently in discriminating between individual speakers and in normalizing unwanted acoustic variation, i.e.\ that which does not pertain to different speakers.
This difference of behaviours leads to a new top-down/bottom-up system combination outperforming the respective baseline system.
Finally, we introduce a new technology able to limit the influence of linguistic effects, responsible for biasing the convergence of the diarization system.
With the development of e-commerce,consumers have posted large number of online reviews on the internet.
These user-generated data are valuable for product designers, as information concerning user requirements and preference can be identified.
The objective of this study is to develop an approach to guide product design by analyzing automatically online reviews.
The proposed approach consists of two steps: data structuration and data analytics.
In data structuration, the author firstly proposes an ontological model to organize the words and expressions concerning user requirements in review text.
Then, a rule-based natural language processing method is proposed to automatically structure review text into the propose ontology.
In data analytics, two methods are proposed based on the structured review data to provide designers ideas on innovation and to draw insights on the changes of user preference over time.
In these two methods, traditional affordance-based design, conjoint analysis, the Kano model are studied and innovatively applied in the context of big data.
To evaluate the practicability of the proposed approach, the online reviews of Kindle e-readers are downloaded and analyzed, based on which the innovation path and the strategies for product improvement are identified and constructed.
The primary goal of the systems engineering is the creation of a set of high quality products and services that enable the accomplishment of desired tasks and needs of the clients or user groups.
A typical systems engineering project can be divided in to three phases: definition, development, and deployment.
The definition phase involves the activities of requirement elicitation and refinement.
By the end of system definition phase, we have all the system functional and non functional requirements.
One of the results of development phase is initial working model of the system.
The deployment phase consists of activities of operational implementation, operational testing and evaluation, and operational functioning and maintenance.
In a project life cycle there are numerous issues to be sorted out during the various phases to finally deliver a successful product.
We proposed solution to the problems of requirements engineering &amp; management, design conflict detection,and stakeholders conflict resolution.
This thesis is based on the recent advances in industrial practices and research in the field of system design engineering.
The objective of this thesis work is to propose an innovative and holistic conception methodology taking into account the multidisciplinary environment and multiple stakeholders.
We have proposed a requirements modeling language based on the GORE techniques.
We have proposed a few of tools for reducing the ambiguity of requirements such as: using negation and test cases using negation for contracting difficult requirements.
Requirement management techniques are proposed to provide better requirements traceability and aid for other systems engineering activities.
Using the same criteria weighting technique a flexible multi criteria multi participant decision methodology is proposed for various decision problems arising during the life cycle of systems engineering project.
Finally, a comprehensive prescriptive systems engineering approach is proposed using all the previously made contributions and an illustrative case study of a real ongoing project is presented developed using the supporting tool SysEngLab, which implements majority of the methods and techniques proposed during thesis.
In the framework of speech therapy for articulatory troubles associated with tongue misplacement, providing a visual feedback might be very useful for both the therapist and the patient, as the tongue is not a naturally visible articulator.
In the last years, ultrasound imaging has been successfully applied to speech therapy in English speaking countries, as reported in several case studies.
The assumption that visual articulatory biofeedback may facilitate the rehabilitation of the patient is supported by studies on the links between speech production and perception.
During speech therapy sessions, the patient seems to better understand his/her tongue movements, despite the poor quality of the image due to inherent noise and the lack of information about other speech articulators.
We develop in this thesis the concept of augmented lingual ultrasound.
We propose two approaches to improve the raw ultrasound image, and describe a first clinical application of this device.
The first approach focuses on tongue tracking in ultrasound images.
We propose a method based on supervised machine learning, where we model the relationship between the intensity of all the pixels of the image and the contour coordinates.
The size of the images and of the contours is reduced using a principal component analysis, and a neural network models their relationship.
We developed speaker-dependent and speaker-independent implementations and evaluated the performances as a function of the amount of manually annotated contours used as training data.
We obtained an error of 1.29 mm for the speaker-dependent model with only 80 annotated images, which is better than the performance of the EdgeTrak reference method based on active contours.
First, we build a mapping model between ultrasound images and tongue control parameters acquired on the reference speaker.
We then adapt this model to new speakers referred to as source speakers.
This approach is compared to a direct GMR regression between the source speaker data and the control parameters of the talking head.
We show that C-GMR approach achieves the best compromise between amount of adaptation data and prediction quality.
We also evaluate the generalization capability of the C-GMR approach and show that prior information of the reference speaker helps the model generalize to articulatory configurations of the source speaker unseen during the adaptation phase.
Finally, we present preliminary results of a clinical application of augmented ultrasound imaging to a population of patients after partial glossectomy.
The first results show an improvement of the patients' performance, especially for tongue placement.
With the rapid proliferation of data platforms collecting and curating data related to various domains such as governments data, education data, environment data or product ratings, more and more data are available online.
This offers an unparalleled opportunity to study the behavior of individuals and the interactions between them.
In the political sphere, being able to query datasets of voting records provides interesting insights for data journalists and political analysts.
In particular, such data can be leveraged for the investigation of exceptionally consensual/controversial topics.
Consider data describing the voting behavior in the European Parliament (EP).
This dataset offers opportunities to study the agreement or disagreement of coherent subgroups, especially to highlight unexpected behavior.
It is to be expected that on the majority of voting sessions, MEPs will vote along the lines of their European party alliance.
However, when matters are of interest to a specific nation within Europe, alignments may change and agreements can be formed or dissolved.
For instance, when a legislative procedure on fishing rights is put before the MEPs, the island nation of the UK can be expected to agree on a specific course of action regardless of their party alliance, fostering an exceptional agreement where strong polarization exists otherwise.
In this thesis, we aim to discover such exceptional (dis)agreement patterns not only in voting data but also in more generic data, called behavioral data, which involves individuals performing observable actions on entities.
These two approaches called Debunk and Deviant, ideally, enables the implementation of a sufficiently comprehensive tool to highlight, summarize and analyze exceptional comportments in behavioral data.
We thoroughly investigate the qualitative and quantitative performances of the devised methods.
Furthermore, we motivate their usage in the context of computational journalism.
The dematerialization of health data, which started several years ago, now generates na huge amount of data produced by all actors of health.
These data have the characteristics of being very heterogeneous and of being produced at different scales and in different domains.
Their reuse in the context of clinical research, public health or patient care involves developing appropriate approaches based on methods from data science.
The aim of this thesis is to evaluate, through three use cases, what are the current issues as well as the place of data sciences regarding the reuse of massive health data.
To meet this objective, the first section exposes the characteristics of health big data and the technical aspects related to their reuse.
The second section presents the organizational aspects for the exploitation and sharing of health big data.
The third section describes the main methodological approaches in data sciences currently applied in the field of health.
Finally, the fourth section illustrates, through three use cases, the contribution of these methods in the following fields: syndromic surveillance, pharmacovigilance and clinical research.
Finally, we discuss the limits and challenges of data science in the context of health big data.
The semantic Web proposes standards and tools to formalize and share knowledge on the Web, in the form of ontologies.
A first contribution of this thesis describes a method based on pattern structures, an extension of formal concept analysis, to extract associations between adverse drug events from patient data.
In this context, a phenotype ontology and a drug ontology cooperate to allow a semantic comparison of these complex adverse events, and leading to the discovery of associations between such events at varying degrees of generalization, for instance, at the drug or drug class level.
A second contribution uses a numeric method based on semantic similarity measures to classify different types of genetic intellectual disabilities, characterized by both their phenotypes and the functions of their linked genes.
We study two different similarity measures, applied with different combinations of phenotypic and gene function ontologies.
In particular, we investigate the influence of each domain of knowledge represented in each ontology on the classification process, and how they can cooperate to improve that process.
Finally, a third contribution uses the data component of the semantic Web, the Linked Open Data (LOD), together with linked ontologies, to characterize genes responsible for intellectual deficiencies.
These contributions illustrates the possibility of having several ontologies cooperate to improve various data mining processes
This dissertation deals with the co-construction of meaning in interaction and the ways in which conversationalists exhibit their interpretative processes.
The focus of this study is the process of explicitation, i.e. the process through which an informational content becomes explicit in conversation.
By offering a multi-level analysis of conversational sequences engaged in this practice, the study approaches the co-construction of meaning from the point of view of informational transformation and inference.
The analyses presented here have been conducted on a corpus of spoken French in interaction, within the setting of informal encounters between friends around a meal or a drink.
The practice of making a content explicit is here being explored according to three analytical lines: (a) the sequential analysis, focusing on the deployment of the explicitation sequence and its components; (b) the analysis according to a device elaborated by means of modeling information management in these sequences; and (c) the analysis of the linguistic designs used when exhibiting the inference.
One of themain challenges of the present study is that of a proposition of a conversationalist model, dealing with information management and its enforcement through analysis of talk in interaction.
Our research is related both to Second Language Acquisition Research and to Foreign Language Didactics, and has a twofold objective.
First, we described crosslinguistic influences, from Spanish L1 and English L2, in the acquisition processes of some structures related to the verb construction in French L3, i.e. the verb construction with noun phrases, the selection of prepositions that potentially introduce these complements, and the acquisition of complement pronouns.
To carry out this identification of syntactic transfers, we consider theoretical contributions from both multilingual acquisition research and studies that define developmental stages in French acquisition.
Our study is based on written productions taken from first-year students, beginners in French, of the Translation/Interpretation university program at Universidad de Concepción in Chile.
The second objective of our research consists of a didactic transposition of empirical results in order to better support the learning of French in our training context.
Our teaching proposals deal with the selection and sequencing of grammatical contents, the methodological approach of grammar instruction, and the correction/evaluation of learners' grammatical competence.
In the end, our didactic purpose is to contribute to multilingual learners training and self-conscience of their acquisition processes, to support them with the reflexive use of their linguistic knowledge, in particular their grammatical knowledge.
Artificial intelligence (AI) with recent progress in statistical machine learning (ML) is currently aiming to revolutionise how experimental science is conducted.
In physics, chemistry, biology, neuroscience or medicine, data is now the driver of new theoretical insights and new scientific hypotheses.
Supervised learning and predictive models are now used to assess if something is *predictable*: Can I predict what people *think* from neural signals?
ML is now used as a replacement for classical statistical hypothesis testing.
In healthcare, one talks about precision medicine, virtual patients with the vision that artificial intelligence will allow to have individualised predictions from genomic, physiological or imaging data.
After pioneering breakthroughs in computer vision, speech processing or natural language processing, ML has now to face new challenges in order to impact various scientific disciplines and in particular health related applications.
When considering medical applications, statistical and computational problems emerge.
The particular problem investigated in this project is related to the absence or limited amount supervision for algorithms: supervised predictive models need so-called annotations or labels to be trained and tested, and unfortunately too few medical applications can provide enough of these.
The approach considered will be based on self-supervised learning and non-linear ICA.
The increase of textual sources over the Web offers an opportunity for knowledge extraction and knowledge base creation.
Recently, several research works on this topic have appeared or intensified.
They generally highlight that to extract relevant and precise information from text, it is necessary to define a collaboration between linguistic approaches, e.g., to extract certain concepts regarding named entities, temporal and spatial aspects, and methods originating from the field of semantics'processing.
Moreover, successful approaches also need to qualify and quantify the uncertainty present in the text.
Finally, in order to be relevant in the context of the Web, the linguistic processing need to be consider several sources in different languages.
This PhD thesis tackles this problematic in its entirety since our contributions cover the extraction, representation of uncertain knowledge as well as the visualization of generated graphs and their querying.
Bioinformatic analyses of transcriptomic data aims to identify genes with variations in their expression level in different tissue samples, for example tissues from healthy versus seek patients, and to characterize these genes on the basis of their functional annotation.
In this thesis, I present four contributions for taking into account domain knowledge in these methods.
Firstly, I define a new semantic and functional similarity measure which optimally exploits functional annotations from Gene Ontology (GO).
Then, I show, thanks to a rigorous evaluation method, that this measure is efficient for the functional classification of genes.
In the third contribution, I propose a differential approach with fuzzy assignment for building differential expression profiles (DEPs).
I define an algorithm for analyzing overlaps between functional clusters and reference sets such as DEPs here, in order to point out genes that have both similar functional annotation and similar variations in expression.
This method is applied to experimental data produced from samples of healthy tissue, colorectal tumor and cancerous cultured cell line.
Finally the similarity measure IntelliGO is generalized to another structured vocabulary organized as GO as a rooted directed acyclic graph, with an application concerning the semantic reduction of attributes before mining.
Based on a case study built around the analysis of the speeches produced by the various actors in the institutional fields concerned (the field of tourist accommodation for Airbnb, that of passenger transportation for Uber), I seek to highlight the strategies of the platforms to build their place in the social space.
I use different analytical grids to understand the strategies implemented by Airbnb and Uber: the dynamics of platform business ecosystems developed by strategic management research, the megamarketing grid defined by Kotler in 1986, neo-institutional theory and its latest developments concerning institutional work and the question of legitimacy.
I find that Airbnb and Uber have each mobilized their megamarketing skills in their own way to build their business ecosystem and legitimacy system, which is a real support for their institutional conquest.
These different expressions of their strategies are also embodied in the institutional work process, which is oriented towards negotiation for Airbnb and confrontation for Uber.
The results of the institutional process have similarities between the two cases: constitution of the legitimacy systems necessary to interpret the role of these two platforms, legal recognition of the activities permitted by the platforms and their producer sides, adjustment of the offers of established professionals.
This work provides a glimpse of a platform life cycle model, taking into account the dynamics of these organizational forms and those resulting from institutional work and their quest for legitimacy.
This project aims to develop expert tools able to automatically detect or suggest (portions of) texts suited to children's understanding abilities.
These tools will rely on the analysis of time and emotions linguistic expression in texts for children, in relation to findings of psycholinguistic works on developmental stages of these two dimensions'understanding.
However, this signal requires extensive fitting and noise reduction steps to extract useful information.
The complexity of these analysis pipelines yields results that are highly dependent on the chosen parameters.
The computation cost of this data deluge is worse than linear: as datasets no longer fit in cache, standard computational architectures cannot be efficiently used.
To speed-up the computation time, we considered dimensionality reduction by feature grouping.
We use clustering methods to perform this task.
We introduce a linear-time agglomerative clustering scheme, Recursive Nearest Agglomeration (ReNA).
Unlike existing fast agglomerative schemes, it avoids the creation of giant clusters.
We then show empirically how this clustering algorithm yields very fast and accurate models, enabling to process large datasets on budget.
In neuroimaging, machine learning can be used to understand the cognitive organization of the brain.
The idea is to build predictive models that are used to identify the brain regions involved in the cognitive processing of an external stimulus.
However, training such estimators is a high-dimensional problem, and one needs to impose some prior to find a suitable model.
To handle large datasets and increase stability of results, we propose to use ensembles of models in combination with clustering.
We study the empirical performance of this pipeline on a large number of brain imaging datasets.
Finally, we show that ensembles of models improve the stability of the weight maps and reduce the variance of prediction accuracy.
We propose two novel approaches for recommender systems and networks.
In the first part, we first give an overview of recommender systems and concentrate on the low-rank approaches for matrix completion.
Building on a probabilistic approach, we propose novel penalty functions on the singular values of the low-rank matrix.
The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefficients associated to the singular values.
The algorithm is simple to implement and can scale to large matrices.
We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low-rank matrix completion.
In the second part, we first introduce some background on Bayesian nonparametrics and in particular on completely random measures (CRMs) and their multivariate extension, the compound CRMs.
We then propose a novel statistical model for sparse networks with overlapping community structure.
The model is based on representing the graph as an exchangeable point process, and naturally generalizes existing probabilistic models with overlapping block-structure to the sparse regime.
Our construction builds on vectors of CRMs, and has interpretable parameters, each node being assigned a vector representing its level of affiliation to some latent communities.
We develop methods for simulating this class of random graphs, as well as to perform posterior inference.
We show that the proposed approach can recover interpretable structure from two real-world networks and can handle graphs with thousands of nodes and tens of thousands of edges.
Medical imaging is a principal data source for different applications.
Even though medical images represent a lot of knowledge concerning the studied case, all the a priori knowledge known by the specialist remains implicit.
Nevertheless this a priori knowledge has a major role in the interpretation and the use of the images.
In this thesis, anatomical a priori knowledge is integrated in two medical applications.
First, an automatic processing pipeline is proposed in order to detect, quantify and localize aneurysms on a segmented cerebrovascular tree.
Centerlines of blood vessels are extracted and then used to automatically detect aneurysms and quantify them.
To localize aneurysm, a matching is made between the cerebrovascular tree of the patient and a healthy one.
The a priori knowledge, in this case, is represented by a graph.
A new algorithm using this ontology to accomplish the segmentation task is then proposed.
Potentially avoidable hospitalizations (PAHs) are the hospital admissions that could have been prevented with timely and effective treatments.
The high rates of PAHs are associated with many factors.
These preventable hospitalizations are associated with a cost of several hundred million Euros for the Health Insurance.
In other words, reducing PAHs not only enhances patients' quality of life but also could save substantial costs due to patient treatments.
Therefore, health authorities are highly interested in solutions improving health care services to reduce PAHs.
Some recent studies in France have suggested that increasing the number of nurses in selected geographic areas could lead to the reduction of the rates of PAHs in those areas.
In our approach, after evaluating some common regression methods, we extended the support vector machine for regression to spatial information.
This approach allows us to select not only the geographic areas but also the number of to-be-added nurses in these areas for the biggest reduction in the number of PAHs.
Specifically, our approach is applied in the Occitanie region, France and geographic areas mentioned above are the cross-border living areas (fr. Bassins de vie-BVs).
On the other side, the extreme temperature could be one potential factor associated with high rates of PAHs.
Therefore, a part of our works is to measure the impact of the extreme temperature to PAHs as well as to include this environmental data in our approach above.
In our works, we used the temperature values measured hourly by sensors at the weather stations.
However, these values are sometimes discontinuous and we need an imputation method for these missing values.
In the literature, two most popular approaches dealing with this processing step exploit either the spatial component or temporal component of the temperature data.
Respectively, these approaches are spatial interpolation methods such as Inverse Distance Weighted (IDW) and time-series models such as Autoregressive Integrated Moving Average (ARIMA).
The results show that compared with IDW and ARIMA methods, our approach performs better at 100% and 99.8% (604 over 605) weather stations respectively.
In addition, as mentioned at the beginning, improving the coordination between the health care providers could lead to the reduction of the PAHs.
Moreover, in the cases that the patients change hospitals for treatments, to ensure efficient and high-quality treatments, doctors would need access to the patients' medical records at the previous hospitals.
Therefore, we propose a graph-based approach to address this problem.
Particularly, we first model data flows of patients between hospitals as an undirected weighted graph in which nodes and edges present the hospitals and the amount of patient flows respectively.
Then, after evaluating two common graph clustering methods, we customize the more suitable one for our needs.
Our result provides interesting insights compared with approaches based on administrative boundaries
This study focuses on language acquisition of a bilingual child growing up in a French-Russian speaking family.
Recent research has shown that a range of factors such as parental input frequency, family discourse strategies can explain the language development processes which take place withing a bilingual family (Döpke, 1998; Lanza, 1997, 2004; De Houwer, 2009; King et Fogle, 2013, etc.)
The bilingual child was being recorded during spontaneous and natural interaction with both of her parents for the period of two years (from 2;00 to 4;00).
The overall corpora are composed of 68 recording hours, while the analysed sample is based on 28 hours of transcribed data.
The data gathered in this study strongly suggest the existence of clear correlation between input frequency, parental discourse strategies and child's linguistic competence in both languages.
The findings from the research show a shift from dominant bilingualism to the harmonious use of both languages at the age of 3.
This shift is accompanied by the changes of the child's linguistic soundscape, the use of parental discourse strategies and input frequency in Russian.
The grammatical-categories emergence is characterized by a strong discrepancy in both languages: the acquisition of French follows developmental paths of French monolingual children, while Russian is acquired with a substantial time delay.
The crosslinguistic influences, lexical, morphological and syntactic, support the idea of a common underlying proficiency.
The main topics of this thesis involve the development of stochastic algorithms for optimization under uncertainty, the study of their theoretical properties and applications.
We study their convergence using the tools developed in the theory of Markov processes: we use properties of infinitesimal generators and functional inequalities to measure the distance between their probability law and a target one.
The first part is concerned with quantum graphs endowed with a probability measure on their vertex set.
Quantum graphs are continuous versions of undirected weighted graphs.
The starting point of the present work was the question of finding Fréchet means on such a graph.
The Fréchet mean is an extension of the Euclidean mean to general metric spaces and is defined as an element that minimizes the sum of weighted square distances to all vertices.
Our method relies on a Langevin formulation of a noisy simulated annealing dealt with using homogenization.
In order to establish the convergence in probability of the process, we study the evolution of the relative entropy of its law with respect to a convenient Gibbs measure.
Using functional inequalities (Poincare and Sobolev) and Gronwall's Lemma, we then show that the relative entropy goes to zero.
We test our method on some real data sets and propose an heuristic method to adapt the algorithm to huge graphs, using a preliminary clustering.
In the same framework, we introduce a definition of principal component analysis for quantum graphs.
This implies, once more, a stochastic optimization problem, this time on the space of the graph's geodesics.
We suggest an algorithm for finding the first principal component and conjecture the convergence of the associated Markov process to the wanted set.
On the second part, we propose a modified version of the simulated annealing algorithm for solving a stochastic global optimization problem on a finite space.
We prove the algorithm's convergence in probability towards the optimal set, provide convergence rate and its optimized parametrization to ensure a minimal number of evaluations for a given accuracy and a confidence level close to 1.
This work is completed with a set of numerical experiments and the assessment of the practical performance both on benchmark test cases and on real world examples.
Aligning macromolecules such as proteins, DNAs and RNAs in order to reveal, or conversely exploit, their functional homology is a classic challenge in bioinformatics, with far-reaching applications in structure modelling and genome annotation.
In the specific context of complex RNAs, featuring pseudoknots, multiple interactions and non-canonical base pairs, multiple algorithmic solutions and tools have been proposed for the structure sequence alignment problem.
However, such tools are seldom used in practice, due in part to their extreme computational demands, and because of their inability to support general types of structures.
Recently, Rinaudo et al. gave a fully general parameterised algorithm for structure-sequence comparison, which is able to take as input any type of pseudoknotted structures.
The parameterised algorithm is a tree decomposition based dynamic programming.
To accelerate the dynamic programming algorithm without losing two much accuracy, we introduced a banded dynamic programming.
Then three algorithms are introduced to get the suboptimal structure-sequence alignments.
The algorithms are implemented in a software named LiCoRNA (aLignment of Complex RNAs).
We first evaluate the performance of LiCoRNA on the seed alignment in the pseudoknotted RFAM families.
Compared to the state-of-the-art algorithms, LiCoRNA shows generally equivalent or better results than its competitors.
With the high accuracy showed by LiCoRNA, we further curate RFAM full pseudoknotted alignment.
The biological brain is an ensemble of individual components which have evolved over millions of years.
Neurons and other cells interact in a complex network from which intelligence emerges.
Many of the neural designs found in the biological brain have been used in computational models to power artificial intelligence, with modern deep neural networks spurring a revolution in computer vision, machine translation, natural language processing, and many more domains.
However, artificial neural networks are based on only a small subset of biological functionality of the brain, and often focus on global, homogeneous changes to a system that is complex and locally heterogeneous.
In this work, we examine the biological brain, from single neurons to networks capable of learning.
We examine individually the neural cell, the formation of connections between cells, and how a network learns over time.
For each component, we use artificial evolution to find the principles of neural design that are optimized for artificial neural networks.
We then propose a functional model of the brain which can be used to further study select components of the brain, with all functions designed for automatic optimization such as evolution.
Our goal, ultimately, is to improve the performance of artificial neural networks through inspiration from modern neuroscience.
However, through evaluating the biological brain in the context of an artificial agent, we hope to also provide models of the brain which can serve biologists.
In the past years, robots have been a part of our every day lives.
And, with the growth of autonomous cars, we see them driving autonomously on highways and cities.
Another area of growth is social robotics.
We can see a lot of studies such as robots helping children with autism.
Other robots are being used to receive people in hotels or to interact with people in shopping centers.
In the latter examples, robots need to understand people behavior.
In addition, in the case of mobile robots, they need to know how to navigate in human environments.
In the context of human environments, this thesis explores socially acceptable navigation of robots towards people.
The human is an entity that needs to be considered based on social norms that we (humans) use on a daily basis.
In a first time, we explore how a robot can approach one person.
A person is an entity that can be bothered if someone or something approaches invading her personal space.
The person also will feel distressed when she is approached from behind.
These social norms have to be respected by the robot.
For this reason, we decided to model the behavior of the robot through learning algorithms.
We manually approach a robot to a person several times and the robot learns how to reproduce this behavior.
In a second time, we present how a robot can understand what is a group of people.
We, humans, have the ability to do this intuitively.
However, for a robot, a mathematical model is essential.
Lastly, we address how a robot can approach a group of people.
We use exemplary demonstrations to teach this behavior to the robot.
We evaluate then the robot's movements by for example, observing if the robot invades people's personal space during the trajectory.
This dissertation presents a novel approach to automatic text summarization, one of the most challenging tasks in Natural Language Processing (NLP).
Until now, no one had ever created a summarization method capable of producing summaries comparable in quality with those produced by humans.
Even many of state-of-the-art approaches form the summary by selecting a subset of sentences from the original text.
Since some of the selected sentences might still contain superfluous information, a finer analysis is needed.
We propose an Automatic Sentence Compression method based on the elimination of intra-phrase discourse segments.
After applying both algorithms in documents in Spanish, our method is able to produce high quality results.
Finally, we evaluate the produced summaries using the Turing test to determine if human judges can distinguish between human-produced summaries and machine-produced summaries.
Opinion mining has emerged as a hot topic in the machine learning community due to the recent availability of large amounts of opinionated data expressing customer's attitude towards merchandisable goods.
Yet, predicting opinions is not easy due to the lack of computational models able to capture the complexity of the underlying objects at hand.
Current approaches consist in predicting simple representations of the affective expressions, for example by restricting themselves to the valence attribute.
We propose a deep learning based approach able to take advantage of the different labeled parts of the output objects by learning to jointly predict them.
We propose a novel hierarchical architecture composed of different state-of-the-art multimodal neural layers and study the effect of different learning strategies in this joint prediction context.
The resulting model is shown to improve over the performance of separate opinion component predictors and raises new questions concerning the optimal treatment of hierarchical labels in a structured prediction context.
We specifically analyzed the case of preference based learning and joint entity and valence detection under a 2 layer binary tree representation in order to derive excess risk bounds and an analysis of the learning procedure algorithmic complexity.
A second aspect of this thesis is to handle a newly released multimodal dataset containing entity and valence annotations at different granularity levels providing a complex representation of the underlying expressed opinions.
Hence, we propose a deep learning based approach able to take advantage of the different labeled parts of the output objects by learning to jointly predict them.
We propose a novel hierarchical architecture composed of different state-of-the-art multimodal neural layers and study the effect of different learning strategies in this joint prediction context.
The resulting model is shown to improve over the performance of separate opinion component predictors and raises new questions concerning the optimal treatment of hierarchical labels in a structured prediction context.
The Spanish verb dar is a lexical item particularly rich in nuances of meaning.
Some of its senses have been studied under the «semantic bleaching» hypothesis consequent upon the «grammaticalization» of the verb.
This process would reveal mainly in the discourse marker dale and in the use of dar as a «support verb» in nominal predicates like dar un golpe, dar un paseo, dar una mano de pintura, etc.
According to the theoretical framework for this research, which accounts for the distinction between language and discourse as well as the unicity of the linguistic sign, an existential notion underlies every discourse realization of this verb.
The notion conveyed by the signified of dar is that an entity B gains access to existence in the sphere of an entity C as a result of the action of an entity A.
Hence, this existential meaning appears in every use of a lexeme whose semantic content is never affected.
Quality of service is a huge issue for telecommunications operators since they have to master and evaluate it in order to satisfy their customers.
To replace expensive and time-consuming human judgment methods, objective methods, integrating objective models providing a prediction of the perceived quality, have been conceived.
Our research aimed at developing a technical diagnostic method, complementary to objective voice quality models, which provides specific information about the nature of the perceived voice quality impairments and identifies the underlying technical causes.
Assuming that speech quality is a multidimensional phenomenon, our technical diagnostic method is built on the modelling of the four perceptual dimensions identified in the literature: “Noisiness” relative to the perceived background noise, “Continuity” linked to discontinuity, “Coloration” related to frequency–response degradations and “Loudness” corresponding to the impact of the speech level, each one being quantified by quality degradation indicators based on audio signal analysis.
A crucial step of our research was to find and/or to develop relevant quality degradation indicators to perfectly characterize each dimension.
To do so, we identified quality degradation indicators in the most recent objective voice quality models (particularly the ITU-T P.863 recommendation, known as POLQA) and we analysed the performance of identified indicators.
Finally, for each dimension, we proposed a detection block which automatically classifies a perceived degradation according to the nature of the defect detected in the audio signal, and an additional block providing information about the impact of degradations on speech quality.
The proposed technical diagnostic method is designed to cover three bandwidths (Narrowband, Wideband and Super Wideband) used in telecommunications systems with a priority investigation to Super Wideband speech signals which remain very useful for future telephony applications.
Modern nuclear reactors utilize core calculations that implement a thermo-hydraulic feedback requiring accurate homogenized few-group cross sections.
They describe the interactions of neutrons with matter, and are endowed with the properties of smoothness and regularity, steaming from their underling physical phenomena.
This thesis is devoted to the modeling of these functions by industry state-of-theart and innovative machine learning techniques.
Convenient is intended in terms of computational performance, such as the model's size, evaluation speed, accuracy, robustness to numerical noise, complexity,etc; always with respect to the engineering modeling objectives that specify the multidimensional spaces of interest.
In this thesis, a standard UO₂ PWR fuel assembly is analyzed for three state-variables, burnup,fuel temperature, and boron concentration.
A full grid is used as usually donein the industry.
Kernel methods, that are a very general machine learning framework able to pose in a normed vector space, a large variety of regression or classification problems.
Kernel functions can reproduce different function spaces using an unstructured support,which is optimized with pool active learning techniques.
The approximations are found through a convex optimization process simplified by the kernel trick.
The intrinsic modular character of the method facilitates segregating the modeling phases: function space selection, application of numerical routines and support optimization through active learning.
Artificial neural networks which are“model free” universal approximators able Artificial neural networks which are“model free” universal approximators able to approach continuous functions to an arbitrary degree without formulating explicit relations among the variables.
With adequate training settings, intrinsically parallelizable multi-output networks minimize storage requirements offering the highest evaluation speed.
These strategies are compared to each other and to multi-linear interpolation in a Cartesian grid, the industry standard in core calculations.
The data set, the developed tools, and scripts are freely available under aMIT license.
Since 2004, online social medias have grown hugely.
This fast development had interesting effects to increase the connection and information exchange between users, but some negative effects also appeared, including fake accounts number growing day after day.
Sockpuppets are multiple fake accounts created by a same user.
They are the source of several types of manipulation such as those created to praise, defend or support a person or an organization, or to manipulate public opinion.
Experiments have been performed on the adaptation stage using real data crawled from English Wikipedia.
In order to find the best machine learning algorithm for sockpuppet's detection phase, the results of six machine learning algorithms are compared.
In addition, they are compared with the literature, and the results show that our proposition improves the accuracy of the detection of sockpuppets.
Furthermore, the results of five community detection algorithms are compared for sockpuppet's grouping phase, in order to find the best community detecton algorithm that will be used in real-time stage.
Affective communication plays a major role in our interpersonal interactions.
We communicate emotions through multiple non-verbal channels.
Researches on human-computer interaction have exploited these communication channels in order to design systems that automatically recognize and display emotional signals.
Touch has receivers less interest then other non-verbal modalities in this area of research.
The intrusive aspect of current haptic interfaces is one of the main obstacles to their use in mediated emotional communication.
In fact, the user is must physically connected to mechanical systems to receive the stimulation.
This configuration affects the transparency of the mediated interaction and limits the perception of certain emotional dimensions as the Valence.
On the basis of the state of the art of haptic interfaces, we proposed a strategy of tactile stimulation based on the use of a mobile air jet.
This technique provides a non-intrusive tactile stimulation on different areas of the body.
In addition, this tactile device would allow effective stimulation of some mechanoreceptors that play an important role in perceptions of positive affect.
We conducted an experimental study to understand the relationships between the physical characteristics of tactile stimulation by air jet and the emotional perception of the users.
The results highlight the main effects of the intensity and the velocity of movement of the air stream on the subjective evaluation measured in space affective (namely, Valence, Arousal and Dominance).The communication of emotions is clearly multi-modal.
We use touch jointly with other modalities to communicate different emotional messages.
We conducted two experimental studies to examine the combination of air jet tactile stimulation with facial and vocal expressions for perception of the valence.
These experiments were conducted in a theoretical and experimental framework called integration of information theory.
This framework allows modelling the integration of information from multiple sources using a cognitive algebra.
Our work suggests that tactile stimulation by air jet can be used to transmit emotional signals in the context of the human-machine interactions.
Perceptual bimodal integration models can be exploited to build computational models to display affects by combining tactile stimulation to facial expressions or the voice.
Safety arguments, also called Safety Cases, are commonly used to present that adequate efforts have been made to achieve the safety goals.
Thus, the system safety is often justified through assessing the safety arguments.
The assessment of such arguments is usually implemented by experts without any dedicated tool or method.
This leads to a questionable validity of the results.
In this thesis, a quantitative framework is proposed based on Dempster-Shafer theory (D-S theory) to assess our confidence in Safety Cases.
An application in railway domain realises the parameter estimation of the framework by a survey with safety experts.
This dissertation proposes a method and a System for the identification of entities (persons, locations, organizations) mentioned in the textual production of the news agency Agence France Presse, in the prospect of the automatic content enrichment.
The various fields concerned by this task are viewed through their relationship: Semantic Web, Information Extraction and in particular Named Entity Recognition (NER), Semantic Annotation, Entity Linking.
Following this study, the industrial need expressed by the Agence France Presse is the subject of specifications, useful for the development of a solution relying on Natural Language Processing tools.
The approach adopted for the identification of the target entities is then described: we propose a System taking charge of the NER step using any existing module, whose results, possibly combined with those of other modules, are evaluated by a linking module able to (i) align a given mention with the entity it denotes among an inventory, built prior to the task, (ii) to spot denotations without alignment in the inventory and (iii) to reconsider denotational readings of mentions (false positive detection).
The Nomos System is developed to this end for the processing of French data.
Its conception also gives rise to the building and use of resources integrated into the Linked Data network, as well as a rich knowledge base about the target entities.
In the context of Industry 4.0 and the More than Moore's paradigm, delivery precision and short cycle times are essential to the competitiveness of High Mix Low Volume semiconductor manufacturing and future industries in general.
We first conducted, in the first part of the manuscript, an in-depth study of “variability”: we approached the notion through its consequences in manufacturing systems, clarified that the variability was about the workflow, introducing the notion of workflow variability and measures that come with it, and identified the main sources of variability through a literature review and real-world examples.
We focused in the second part of this manuscript on the integration of workflow variability in production management tools: We showed how integrating the stable consequences of workflow variability can improve WIP projections in complex systems and increase the control on such systems, proposed a new tool (the Concurrent WIP) to better measure the performances of systems subject to high workflow variability, and showed that complex “dependency” mechanisms play a key role in workflow variability yet are not integrated in any model.
Finally, the third and last part of the manuscript organized perspectives for variability reduction: based on the work of this manuscript, we showed a framework for variability reduction on the short term, and proposed a direction for medium and long-term research.
The PhD student will take part in the ANR JCJC MAOI (Multimodal Analysis of Opinions in Interactions) at Telecom-ParisTech.
The role of the PhD will consist in developing machine learning methods for the multimodal (i.e. speech and text) analysis of the user's opinion during his/her interaction with an agent.
In the first algorithm, we exploit the joint autoregressive model that models short and long (periodic) correlations of Gaussian speech signals to formulate a state space model with unknown parameters.
The EM-Kalman algorithm is then used to estimate jointly the sources (involved in the state vector) and the parameters of the model.
In the second algorithm, we use the same speech model but this time in the frequency domain (quasi-periodic Gaussian sources with AR spectral envelope).
Classical frequency domain asymptotic methods replace linear convolution by circulant convolution leading to approximation errors.
We show how the introduction of windows can lead to slightly more complex frequency domain techniques, replacing diagonal covariance matrices by banded covariance matrices, but with controlled approximation error.
The sources are then estimated using the Wiener filtering.
Alternating MAP/ML which suffers from inconsistent parameter bias, EM which converges to ML and VB that we prove converges asymptotically to the ML solution for parameter estimation.
Language diversity is under considerable pressure: half of the world's languages could disappear by the end of this century.
This realization has sparked many initiatives in documentary linguistics in the past two decades, and 2019 has been proclaimed the International Year of Indigenous Languages by the United Nations, to raise public awareness of the issue and foster initiatives for language documentation and preservation.
Yet documentation and preservation are time-consuming processes, and the supply of field linguists is limited.
Consequently, the emerging field of computational language documentation (CLD) seeks to assist linguists in providing them with automatic processing tools.
The Breaking the Unwritten Language Barrier (BULB) project, for instance, constitutes one of the efforts defining this new field, bringing together linguists and computer scientists.
This thesis examines the particular problem of discovering words in an unsegmented stream of characters, or phonemes, transcribed from speech in a very-low-resource setting.
This primarily involves a segmentation procedure, which can also be paired with an alignment procedure when a translation is available.
Using two realistic Bantu corpora for language documentation, one in Mboshi (Republic of the Congo) and the other in Myene (Gabon), we benchmark various monolingual and bilingual unsupervised word discovery methods.
We then show that using expert knowledge in the Adaptor Grammar framework can vastly improve segmentation results, and we indicate ways to use this framework as a decision tool for the linguist.
We also propose a tonal variant for a strong nonparametric Bayesian segmentation algorithm, making use of a modified backoff scheme designed to capture tonal structure.
To leverage the weak supervision given by a translation, we finally propose and extend an attention-based neural segmentation method, improving significantly the segmentation performance of an existing bilingual method.
We develop a data mining and visualisation toolkit to study how the information is shared on online social network services.
This software allows to observe relationships between conversational, semantical, temporal and geographical dimensions of online communication acts.
Internet memes are short messages that spread quickly through the Web.
Following models that remain largely unknown, they articulate personal discussions, societal debates and large communication campaign.
We analyse a set of Internet memes by using methods from social network analysis and Chinese natural language processing on a large corpus of 200 million tweets which represents/reflects the overall activity on the Chinese social network Sina Weibo in 2012.
An interactive visualisation interface showing networks of words, user exchanges and their projections on geographical maps provides a detailed understanding of actual and textual aspects of each meme spread.
An analysis of hashtags in the corpus shows that the main content from Sina Weibo is largely similar to the ones in traditional media (advertisement, entertainment, etc.).
Therefore, we decided to not consider hashtags as memes representatives, being mostly byproducts of well planned strategic or marketing campaigns.
Our final approach studies a dozen of memes selected for the diversity of their topic: humor, political scandal, breaking news and marketing.
Most of current recommendation systems are based on ratings (i.e. numbers between 0 and 5) and try to suggest a content (movie, restaurant...) to a user.
These systems usually allow users to provide a text review for this content in addition to ratings.
It is hard to extract useful information from raw text while a rating does not contain much information on the content and the user.
In this thesis, we tackle the problem of suggesting personalized readable text to users to help them make a quick decision about a content.
More specifically, we first build a topic model that predicts personalized movie description from text reviews.
We evaluate our model on an IMDB dataset and illustrate its performance through comparison of topics.
We then study parameter inference in large-scale latent variable models, that include most topic models.
We propose a unified treatment of online inference for latent variable models from a non-canonical exponential family, and draw explicit links between several previously proposed frequentist or Bayesian methods.
Finally, we propose a new class of determinantal point processes (DPPs) which can be manipulated for inference and parameter learning in potentially sublinear time in the number of items.
This class, based on a specific low-rank factorization of the marginal kernel, is particularly suited to a subclass of continuous DPPs and DPPs defined on exponentially many items.
We apply this new class to modelling text documents as sampling a DPP of sentences, and propose a conditional maximum likelihood formulation to model topic proportions, which is made possible with no approximation for our class of DPPs.
We present an application to document summarization with a DPP on 2 to the power 500 items, where the summaries are composed of readable sentences.
Billions of “things” connected to the Internet constitute the symbiotic networks of communication devices (e.g., phones, tablets, and laptops), smart appliances (e.g., fridge, coffee maker and so forth) and networks of people (e.g., social networks).
So, the concept of traditional networks (e.g., computer networks) is expanding and in future will go beyond it, including more entities and information.
These networks and devices are constantly sensing, monitoring and generating a vast amount of data on all aspects of human life.
One of the main challenges in this area is that the network consists of “things” which are heterogeneous in many ways, the other is that their state of the interconnected objects is changing over time, and there are so many entities in the network which is crucial to identify their interdependency in order to better monitor and predict the network behavior.
In this research, we address these problems by combining the theory and algorithms of event processing with machine learning domains.
Our goal is to propose a possible solution to better use the information generated by these networks.
It will help to create systems that detect and respond promptly to situations occurring in urban life so that smart decision can be made for citizens, organizations, companies and city administrations.
Social media is treated as a source of information about situations and facts related to the users and their social environment.
At first, we tackle the problem of identifying the public opinion for a given period (year, month) to get a better understanding of city dynamics.
The second challenge is combing network data with various properties and characteristics in common format that will facilitate data sharing among services.
To solve it we created common event model that reduces the representation complexity while keeping the maximum amount of information.
This model has two major additions: semantic and scalability.
The semantic part means that our model is underlined with an upper-level ontology that adds interoperability capabilities.
While the scalability part means that the structure of the proposed model is flexible in adding new entries and features.
We validated this model by using complex event patterns and predictive analytics techniques.
To deal with the dynamic environment and unexpected changes we created dynamic, resilient network model.
It always chooses the optimal model for analytics and automatically adapts to the changes by selecting the next best model.
We used qualitative and quantitative approach for scalable event stream selection, that narrows down the solution for link analysis, optimal and alternative best model.
Based on electronic death certificates from 2012 to 2016 in France, this thesis aims to implement and evaluate the performance of natural language processing methods, to classify medical causes of death in free text format into mortality syndromic groups (MSG) relevant for reactive mortality surveillance and health impact assessment.
Close to 100 MSGs meeting the objectives of the surveillance were defined.
Two classification methods were developed: a rule-based method and a supervised machine learning method (SVM).
Two SVM models were developed using different combinations of features.
The development and evaluation of the performances of the methods were based on 4,500 annotated death certificates.
The evaluation was initially based on 7 MSGs and was then extended to 60 other MSGs.
The variations of the monthly number of MSGs assigned by the two methods were compared using the whole death certificates from 2012 to 2016 (204,000 deaths).
The rule-based method and the SVM model including all the features obtained high performances (F-mesure≥0.95) for the classification of causes into 31 MSGs.
The monthly variations of those MSGs were comparable.
In average, the causes of death within a certificate are classified into 3.7 MSG.
We proposed a balanced weighting method of the MSG to take these multiple causes into account in the routine analysis of mortality for alert and impact assessment.
These results complete and enrich the reactive surveillance currently based on administrative data.
Lexical knowledge acquisition of verbal constructions is an important issue for natural language processing as well as lexicography, which aims at referencing emerging linguistic usages.
Such a task implies numerous challenges, technical as well as theoretical.
In this thesis, we had a closer look at two fundamental aspects of the description of the verb: the notion of lexical item and the distinction between arguments and adjuncts.
Following up on studies in natural language processing and linguistics, we embrace the hypothesis that there is no clear distinction between homonyms and quasi-synonyms, and the hypothesis of a continuum between arguments and adjuncts.
We provide a complete approach to lexical knowledge acquisition of verbal constructions from an untagged news corpus.
The acquisition process makes use of the notion of argumenthood, and builds models of the two continuums.
Our lexicon has been evaluated on a qualitative and comparative basis. Siding with lexicography anchored in the theoretical framework of corpus linguistics, we show the difficulty of using lexical resources to describe as yet unseen data.
Despite its recent successes in computer vision or machine translation, the use of deep learning in the biomedical field faces many challenges.
Among them, we have the difficult access to data in sufficient quantity and quality, as well as the need of having interoperable and the interpretable models.
In this thesis, we are interested in these different issues from the perspective of the creation of models predicting future glucose values of diabetic patients.
Such models would allow patients to anticipate daily glucose variations, helping its regulation in order to avoid states of hypoglycemia or hyperglycemia.
To this end, we use three datasets.
While the first was collected during this thesis on several type-2 diabetic patients, the other two are composed of type-1 diabetic patients, both real and virtual.
Across the studies, we use each patient's past glucose, insulin, and carbohydrate data to build personalized models that predict the patient's glucose values 30 minutes into the future.
First, we do a detailed state-of-the-art analysis by building an open-source benchmark of glucosepredictive models.
While promising, we highlight the difficulty deep models have in making predictions that are at the same time accurate and safe for the patient.
In order to improve the clinical acceptability of the models, we investigate the integration of clinical constraints within the training of the models.
We explore its practical use through an algorithm that enables the training of a model maximizing the precision of the predictions while respecting the clinical constraints set beforehand.
Then, we study the use of transfer learning to improve the performance of glucose-predictive models.
It eases the learning of personalized models by reusing the knowledge learned on other patients.
In particular, we propose the adversarial multi-source transfer learning framework.
It significantly improves the performance of the models by allowing the learning of a priori knowledge which is more general, by being agnostic of the patients that are the source of the transfer.
We investigate different transfer scenarios through the use of our three datasets.
We show that it is possible to transfer knowledge using data coming from different experimental devices, from patients of different types of diabetes, but also from virtual patients.
Finally, we are interested in improving the interpretability of deep models through the attention mechanism.
In particular, we explore the use of a deep and interpretable model for the prediction of glucose.
It implements a double attention mechanism enabling the estimation of the contribution of each input variable to the model to the final prediction.
We empirically show the value of such a model for the prediction of glucose by analyzing its behavior in the computation of its predictions.
This work deals with the automatic identification of Romance Languages.
The aim of our study is to provide linguistic patterns potentially robust for the discrimination of 5 languages from the latin family (i. E., Spanish, French, Italian, Portuguese and Romanian).
The Romance Languages have the advantage of a secular linguistic tradition and represents official languages in several countries of the world, the study of the taxonomist approaches devoted to this linguistic family shows a spécial relevance of the typological classification.
The first group includes languages with prototypical vocalic systems, whereas the second group, languages with complex vocalic systems in terms of number of oppositions.
In addition to the vocalic criteria, these hierarchy is supported by consonantal and prosodic particularities.
We conducted two experimental paradigms to test the correspondence between the perceptual patterns used by nai͏̈f listeners to differentiate the Romance languages and the linguistic patterns employed by the typological classification.
Romance native language] (i. E., French, Romanian vs. Japanese, Americans), showed different perceptual strategies related both to the native language and to the familiarity with the Romance languages.
The linguistic strategies lead to a macro-discrimination of the languages in two groups similar to those obtained via the typological taxonomy based on vocalic particularities (i. E., Spanish, Italian vs. Romanian, French, Portuguese).
The second series of perceptual experiments on two groups of subjects (French and American) consisted in the evaluation of the acoustic similarity of the have languages.
The results confirmed the division of Romance Languages in the same two groups as those via the discrimination experiments.
We concluded that the vocalic patterns may be a robust clue for the discrimination of the Latin idioms into two major linguistic groups: Italian, Spanish vs. Romanian, French,
In the field of chemistry, it is interesting to be able to estimate the physicochemical properties of molecules, especially for industrial applications.
These are difficult to estimate by physical simulations, as their implementation often present prohibitive time complexity.
However, the emergence of data (public or private) opens new perspectives for the treatment of these problems by statistical methods and machine learning.
The main difficulty lies in the characterization of molecules: these are more like a network of atoms (in other words a colored graph) than a vector.
The aim of this thesis is to take advantage of public corpora to learn the best possible representations of these structures, and to transfer this global knowledge to smaller datasets.
We adapted methods used in automatic processing of natural languages to achieve this goal.
To implement them, more theoretical work was needed, especially on the graph isomorphism problem.
The results obtained on classification / regression tasks are at least competitive with the state of the art, and even sometimes better, in particular on restricted data sets, attesting some opportunities for transfer learning in this field.
Is it possible to reconcile the need for language assistance to all non-French speakers with the lack of standardized language resources for translating combinations of languages which are genetically and culturally remote?
This is the issue raised by Hindi and Urdu translation in France, in the judicial context.
The judicial systems in which they are used come from the British colonial heritage based upon common law.
Through the analysis of a corpus of various documents, this work is aimed at producing terms and phraseological resources in order to assist the translator-interpreter in finding out translation equivalences between languages.
First, we will explore the differences between the three countries' judiciaries, as well as the status of the Hindi, Urdu and French languages.
Eventually, we will propose a method for term extraction and sub-corpus alignment in order to stress term or translation equivalences between these languages in the judicial genre.
This work, which sheds light on the relations between text, words and context, provides actual field-specific resources for judicial translation and interpretation professionals.
This thesis comes within the scope of talking heads.
We are particularly interested in the prediction of labial and jaw coarticulation movements.
After analyzing intra and inter speaker variability using two corpora, we defined a prediction algorithm for anticipatory coarticulation based on phonetic rules which takes into account interactions between articulators.
It consists in concatenating elementary VC...CV sequences selected by our prediction algorithm and either extracted from the corpus or rebuilt by completion.
With the aim of estimating the quality of our synthesis process, we measured differences between real and predicted data for all the sentences of the corpus et we compared our solution with Cohen and Massaro 's algorithm.
It turns out that our solution is better for specific VCCV sequences in which anticipation is more complex.
Large variations in writing styles and difficulties in segmenting cursive words are the main reasons for handwritten cursive words recognition for being such a challenging task.
An Indian postal document reading system based on a segmentation-free context based stochastic model is presented.
The originality of the work resides on a combination of high-level perceptual features with the low-level pixel information considered by the former model and a pruning strategy in the Viterbi decoding to reduce the recognition time.
While the low-level information can be easily extracted from the analyzed form, the discriminative power of such information has some limits as describes the shape with less precision.
For that reason, we have considered in the framework of an analytical approach, using an implicit segmentation, the implant of high-level information reduced to a lower level.
This enrichment can be perceived as a weight at pixel level, assigning an importance to each analyzed pixel based on their perceptual properties.
The challenge is to combine the different type of features considering a certain dependence between them.
To reduce the decoding time in the Viterbi search, a cumulative threshold mechanism is proposed in a flat lexicon representation.
Instead of using a trie representation where the common prefix parts are shared we propose a threshold mechanism in the flat lexicon where based just on a partial Viterbi analysis, we can prune a model and stop the further processing.
The cumulative thresholds are based on matching scores calculated at each letter level, allowing a certain dynamic and elasticity to the model.
As we are interested in a complete postal address recognition system, we have also focused our attention on digit recognition, proposing different neural and stochastic solutions.
To increase the accuracy and robustness of the classifiers a combination scheme is also proposed.
Programs are everywhere in our daily life: computers and phones but also fridges, planes and so on.
The main actor in the process of creating these programs is human beings.
As thorough as they can be, humans are known to make involuntary errors without their awareness.
All long their development task, developers have to continuously face their (or their colleagues) errors.
This key observation arises the need of aiding developers in their development/maintenance tasks.
Professionals who have to peruse documents in a limited amount of time or private individuals who want to be informed about a specific topic without having the time to read all the texts about it both need summaries.
The increase in electronic documents available have made there search in automatic summarization an important domain in the field of natural language processing.
We propose a method based on a sentence classification in semantic clusters, using similarity calculation between sentences.
This step allows us to identify the sentences which convey the same information and to remove redundancy from the automatically generated summaries.
This method has been evaluated on the # opinion summarization # task of TAC2008 evaluation campaing, and on the # news summarization # task of TAC2008 and TAC2009 campaigns.
Our system ranks itself among the first quarter of the participating systems.
We also propose to integrate newswire articles structure to our summarization system in order to improve the quality of the summaries it generates.
Our summarization method has also been integrated to a larger application which aims to help the user to visualize the main topics of a corpus and to automatically extract the essential information.
Depending on the input representation, this dissertation investigates issues from two classes: meaning representation (MR) to text and text-to-text generation.
In the first class (MR-to-text generation, "Generating Sentences"), we investigate how to make symbolic grammar based surface realisation robust and efficient.
We propose an efficient approach to surface realisation using a FB-LTAG and taking as input shallow dependency trees.
To further improve our robustness, we propose two error mining algorithms: one, an algorithm for mining dependency trees rather than sequential data and two, an algorithm that structures the output of error mining into a tree to represent them in a more meaningful way.
We show that our realisers together with these error mining algorithms improves on both efficiency and coverage by a wide margin.
In the second class (text-to-text generation, "Simplifying Sentences"), we argue for using deep semantic representations (compared to syntax or SMT based approaches) to improve the sentence simplification task.
We use the Discourse Representation Structures for the deep semantic representation of the input.
We propose two methods: a supervised approach (with state-of-the-art results) to hybrid simplification using deep semantics and SMT, and an unsupervised approach (with competitive results to the state-of-the-art systems) to simplification using the comparable Wikipedia corpus
Multiple sclerosis (MS) is a chronic disease of the central nervous system, leading cause of nontraumatic disability in young adults.
MS is characterized by inflammation, demyelination and neurodegenrative pathological processes which cause a wide range of symptoms, including cognitive deficits and irreversible disability.
Concerning the diagnosis of the disease, the introduction of Magnetic Resonance Imaging (MRI) has constituted an important revolution in the last 30 years.
In particular, new approaches based on the representation of MR images of the brain as graph have been used to study and quantify damages in the brain white matter network, achieving promising results.
Due to their effectiveness in analyzing large amount of data, detecting latent patterns and establishing functional relationships between input and output, these artificial intelligence techniques have gained particular attention in the scientific community and is nowadays widely applied in many context, including computer vision, speech recognition, medical diagnosis, among others.
In this work, deep learning methods were developed to support biomedical image analysis, in particular for the classification and the characterization of MS patients based on structural connectivity information.
Graph theory, indeed, constitutes a sensitive tool to analyze the brain networks and can be combined with novel deep learning techniques to detect latent structural properties useful to investigate the progression of the disease.
In the first part of this manuscript, an overview of the state of the art will be given.
We will focus our analysis on studies showing the interest of DTI for WM characterization in MS.
An overview of the main deep learning techniques will be also provided, along with examples of application in the biomedical domain.
In a second part, two deep learning approaches will be proposed, for the generation of new, unseen, MRI slices of the human brain and for the automatic detection of the optic disc in retinal fundus images.
In the third part, graph-based deep learning techniques will be applied to the study of brain structural connectivity of MS patients.
Semisupervised and unsupervised approaches were also investigated with the aim of reducing the human intervention in the pipeline
They can represent a basis for automated risk assessment, relying on an identification and valuation of critical assets in the network.
This allows to design pro-active and reactive counter-measures for risk mitigation and can be leveraged for security monitoring and network hardening.
Our thesis aims to apply a similar approach in Cloud environments, which implies to consider new challenges incurred by these modern infrastructures, since the majority of attack graph methods were designed with traditional environments in mind.
Novel virtualization attack scenarios, as well as inherent properties of the Cloud, namely elasticity and dynamism are a cause for concern.
To realize this objective, a thorough inventory of virtualization vulnerabilities was performed, for the extension of existing vulnerability templates.
Based on an attack graph representation model suitable to the Cloud scale, we were able to leverage Cloud and SDN technologies, with the purpose of building Cloud attack graphs and maintain them in an up-to-date state.
Algorithms able to cope with the frequent rate of change occurring in virtualized environments were designed and extensively tested on a real scale Cloud platform for performance evaluation, confirming the validity of the methods proposed in this thesis, in order to enable Cloud administrator to dispose of an up-to-date Cloud attack graph.
The analysis of airborne and satellite images is one of the core subjects in remote sensing.
In recent years, technological developments have facilitated the availability of large-scale sources of data, which cover significant extents of the earth's surface, often at impressive spatial resolutions.
In addition to the evident computational complexity issues that arise, one of the current challenges is to handle the variability in the appearance of the objects across different geographic regions.
For this, it is necessary to design classification methods that go beyond the analysis of individual pixel spectra, introducing higher-level contextual information in the process.
In this thesis, we first propose a method to perform classification with shape priors, based on the optimization of a hierarchical subdivision data structure.
We then delve into the use of the increasingly popular convolutional neural networks (CNNs) to learn deep hierarchical contextual features.
We investigate CNNs from multiple angles, in order to address the different points required to adapt them to our problem.
Among other subjects, we propose different solutions to output high-resolution classification maps and we study the acquisition of training data.
We also created a dataset of aerial images over dissimilar locations, and assess the generalization capabilities of CNNs.
Finally, we propose a technique to polygonize the output classification maps, so as to integrate them into operational geographic information systems, thus completing the typical processing pipeline observed in a wide number of applications.
Throughout this thesis, we experiment on hyperspectral, atellite and aerial images, with scalability, generalization and applicability goals in mind.
This PHD thesis offers a synchronic description of Reunion Creole's determiner system as well as an analysis of the interpretation of its noun phrases.
The thesis includes new data from two kinds of sources: a small collection of oral corpora, and grammaticality/felicity judgments.
We investigate the distribution of the different kinds of NP, the morphosyntactic status of pre- and postnominal elements, the number system, and the expression of definiteness in Reunion Creole.
The present thesis offers a corpus study of the alternation between do it, do this and do that in their use as 'Verb Phrase anaphors' (VPAs), in which they refer to a salient action mentioned in previous discourse, typically by means of a VP, or exophorically to a salient action in the speech situation that is not explicitly mentioned in previous discourse.
Do it/this/that have been little studied in the otherwise extant literature on anaphora and especially VP ellipsis (VPE, e.g., Kim knows the answer and Pat does too).
This is because it has long been assumed that they are largely interchangeable with each other as well as with do so and VPE, so that detailed analysis of their discourse properties was not deemed worth pursuing.
The examples below show that this assumption is flawed: in (1), an attested example from the BNC, do this/that so could be used instead of do it, but in (3), do that is strongly preferred.
As for VPE, it is unnatural in (1) and prefers a context of the type in (2).
1. They've been rescuing companies for so long they do it automatically now, I expect. (AB9, ok: they do this/that/so automatically…) 2. They've been rescuing companies for so long that whenever they do, it's always a success. 3. He closes his eyes when he speaks and I don't trust anyone who does that. (…anyone who #does this/#it/#so)
Based on a sample of annotated data from the British National corpus (BNC, Davies 2004-), our study will examine the factors driving the alternation between do it/this/that.
Amongst others, VPA choice is influenced by register, the presence of an adjunct after the VPA, whether or not the antecedent has already been mentioned prior to the antecedent clause, and, to a lesser extent, the saliency of the antecedent and its presumed familiarity to the addressee.
Do it typically refers to highly salient actions which are then further described by means of an adjunct.
Do that typically occurs without an adjunct, and sometimes bears much resemblance to VPE in its usage.
Learning-a vital principle of evolution-ensures the transformation of primary data captured by our senses into useful knowledge or abstract and general ideas that can be used in new situations and contexts.
Cognitive neuroscience shows that the mechanisms of learning are stimulated by cognitive (e.g. wondering, evaluating errors), physical (e.g. manipulating, moving) and social (e.g. debating, collaborating) engagement.
The learner builds knowledge through experience, by exploring his environment, formulating hypotheses and experimenting.
Learning is crucial in a context where the exponential evolution of information and communication technologies is changing objects, practices and uses.
The development of the Internet of Things (IoT) transforms common objects (e.g. light bulbs, watches, cars) into connected devices (CD) that can collect data and act on the user's environment.
Learning becomes both biological and artificial and allows the creation of artificial intelligence systems (AIS) that analyse large volumes of data to automate tasks and assist individuals.
Technologies can support learning when the technical possibilities they offer are used to support the process of knowledge construction.
Thus, this thesis focuses on learning in the context of IoT and examines how the specificities of CD can be articulated with the mechanisms of learning.
In order to identify the characteristics of learning in the context of IoT, we studied existing uses of CD.
Based on the state of the art, we proposed a conceptual tool describing the IoT through four dimensions of analysis: Data, Interfaces, Agents and Pervasiveness.
This tool enabled us to identify, list, classify and ultimately analyse the uses of CD for learning.
In the context of these uses, learning is characterised by physical commitment, contextualisation of knowledge and bringing pedagogical activities closer to reality.
Building on the results of this initial work, we have developed an approach to put the specificities of CD to learn sciences.
The abstract and often counter-intuitive aspect of scientific knowledge hinders their learning, partly because our perception of reality is subjective and limited by our senses.
However, data collected by CD and analysed by AIS provide information about the environment that can be used to extend human perception.
Therefore, the objective of our approach, translated by the Data-Representations-Interactions (DRI) model, aims at exploiting OCs and SIAs to facilitate the observation of physical phenomena.
According to the DRI model, the learner interacts with representations of a physical phenomenon generated by CD and AIS.
In accordance with the mechanisms of learning (e.g. constructivism, role of experience), the learner is led to make observations and manipulations, formulate hypotheses and test them.
In order to evaluate the effects and constraints of the DRI model, we have designed LumIoT devices dedicated to the learning of photometric quantities (e.g. luminous flux, luminous intensity, illuminance).
Then, we conducted an experiment with 17 students of the Master 1 Multimedia Products and Services of the University of Franche-Comté (Montbéliard).
The results of the experiment show that the LumIoT devices, based on the DRI model, have facilitated the observation and understanding of photometric quantities.
By making abstract knowledge accessible, the DRI model paves the way for learning devices using CD and AIS to mediate knowledge.
This thesis follows on from a recent study conducted by a few researchers from University of Montpellier, with the aim of proposing to the scientific community an inversion procedure capable of noninvasively estimating patient-specific blood pressure in cerebral arteries.
Its first objective is, on the one hand, to examine the accuracy and robustness of the inversion procedure proposed by these researchers with respect to various sources of uncertainty related to the models used, formulated assumptions and patient-specific clinical data, and on the other hand, to set a stopping criterion for the ensemble Kalman filter based algorithm used in their inversion procedure.
For this purpose, uncertainty analysis and several sensitivity analyses are carried out.
The second objective is to illustrate how machine learning, mainly focusing on convolutional neural networks, can be a very good alternative to the time-consuming and costly inversion procedure implemented by these researchers for cerebral blood pressure estimation.
An approach taking into account the uncertainties related to the patient-specific medical images processing and the blood flow model assumptions, such as assumptions about boundary conditions, physical and physiological parameters, is first presented to quantify uncertainties in the inversion procedure outcomes.
Uncertainties related to medical images segmentation are modelled using a Gaussian distribution and uncertainties related to modeling assumptions choice are analyzed by considering several possible hypothesis choice scenarii.
From this approach, it emerges that the uncertainties on the procedure results are of the same order of magnitude as those related to segmentation errors.
Furthermore, this analysis shows that the procedure outcomes are very sensitive to the assumptions made about the model boundary conditions.
In particular, the choice of the symmetrical Windkessel boundary conditions for the model proves to be the most relevant for the case of the patient under study.
Next, an approach for ranking the parameters estimated during the inversion procedure in order of importance and setting a stopping criterion for the algorithm used in the inversion procedure is presented.
The results of this strategy show, on the one hand, that most of the model proximal resistances are the most important parameters for blood flow estimation in the internal carotid arteries and, on the other hand, that the inversion algorithm can be stopped as soon as a certain reasonable convergence threshold for the most influential parameter is reached.
Finally, a new numerical platform, based on machine learning and allowing to estimate the patient-specific blood pressure in the cerebral arteries much faster than with the inversion procedure but with the same accuracy, is presented.
The application of this platform to the patient-specific data used in the inversion procedure provides noninvasive and real-time estimate of patient-specific cerebral pressure consistent with the inversion procedure estimation.
Context: One of the major challenges for treating neuro-psychiatric pathologies is the follow-up of chronic patients in order to measure early relapses as well as observance and compliance to the treatment.
Such a monitoring is possible thanks to the use of connected medical devices (measuring for instance weight, blood pressure or physical activities) but crucial information about how the patients feel are difficult to measure. Regular in-person appointments between doctors and patients are thus required.
The growing number of patients however increases the queuing time and often results in episodic followups with unevenly spaced interviews.
Apart from the clinical interviews, it is nonetheless possible to measure some symptoms (e.g.: sadness or sleepiness) with a range of techniques: looking at eye movements, with electroencephalographic measurements, or by watching verbal expressions or body movements.
Thanks to recent advances in speech processing, it is now possible to detect precise cues in the voice allowing to characterise the state of a speaker (e.g. to measure the sleepiness level).
This method has the following advantages: recording voice data is not invasive and does not require specific sensors nor complex calibration processes.
It can thus be set up in various environments, outside laboratories, and allows regular and non-restrictive monitoring of patients.
In this project, we wish to propose a solution using speech processing that, associated with Artificial Intelligence algorithms, will help us define new biomarkers. These biomarkers will then be used for personalised at-home monitoring of the quality of life of the patients.
Aims: Since 2016, we considered using speech analysis for the followup of patients suffering for excessive sleepiness problems.
This study, carried out with a collaboration between the SANSPY Lab (CNRS USE 3413) and the LaBRI (CNRS UMR 5800), is currently funded by the Nouvelle Aquitaine region in the framework of the IS-OSA project.
We thus could set up a recording procedure allowing us to be the only teams to possess an audio database in French of patients along with their clinical diagnoses.
During this preliminary study, we devised an analysis method that employs a reduced number of interpretable features while keeping a good performance level.
These results will soon be published in an international journal.
These promising results open the path for improvements to be made to the system in order to alleviate the technological barriers of the project:
1. Study the coherence of the different clinical sleepiness measurements and how they are related to voice features.
2. Optimise the research for relevant voice features and link them to clinical symptoms.
3. Increase the size of the database to improve the quality of the automatic classification by using artificial intelligence and deep learning techniques.
4. Embed the system to the virtual companion developed at SANPSY and test it in real conditions at the patient's home.
A Textometric method, however, uses several statistical models to study the distribution of words in large corpora, with the goal of shedding light on significant characteristics of the textual data.
In this research, Textometry, an approach traditionally considered incompatible with information extraction methods, is applied to the same corpus as an information extraction procedure in order to obtain information on economic events.
Several textometric analyses (characteristic elements, co-occurrences) are examined on a corpus of online news feeds.
Both approaches contribute differently to processing textual data, producing complementary analyses of the corpus.
Following the comparison, this research presents the advantages for these two text mining methods in strategic monitoring of current events.
The discovery of elementary linguistic units (phonemes, words) only from sound recordings is an unresolved problem that arouses a strong interest from the community of automatic speech processing, as evidenced by the many recent contributions of the state of the art.
During this thesis, we focused on using neural networks to answer the problem.
We approached the problem using neural networks in a supervised, poorly supervised and multilingual manner.
We have developed automatic phoneme segmentation and phonetic classification tools based on convolutional neural networks.
The automatic segmentation tool obtained 79% F-measure on the BUCKEYE conversational speech corpus.
This result is similar to a human annotator according to the inter-annotator agreement provided by the creators of the corpus.
In addition, it does not need a lot of data (about ten minutes per speaker and 5 different speakers) to be effective.
In addition, it is portable to other languages (especially for poorly endowed languages such as xitsonga).
The phonetic classification system makes it possible to set the various parameters and hyperparameters that are useful for an unsupervised scenario.
In the unsupervised context, the neural networks (Auto-Encoders) allowed us to generate new parametric representations, concentrating the information of the input frame and its neighboring frames.
We studied their utility for audio compression from the raw signal, for which they were effective (low RMS, even at 99% compression).
We also carried out an innovative pre-study on a different use of neural networks, to generate vectors of parameters not from the outputs of the layers but from the values of the weights of the layers.
These parameters are designed to mimic Linear Predictive Coefficients (LPC).
In the context of the unsupervised discovery of phoneme-like units (called pseudo-phones in this memory) and the generation of new phonetically discriminative parametric representations, we have coupled a neural network with a clustering tool (k-means).
The iterative alternation of these two tools allowed the generation of phonetically discriminating parameters for the same speaker: low rates of intra-speaker ABx error of 7.3% for English, 8.5% for French and 8, 4% for Mandarin were obtained.
These results allow an absolute gain of about 4% compared to the baseline (conventional parameters MFCC) and are close to the best current approaches (1% more than the winner of the Zero Resource Speech Challenge 2017).
The inter-speaker results vary between 12% and 15% depending on the language, compared to 21% to 25% for MFCCs.
Temporal aspects to allow Senegalese communities to share and to co-construct their sociocultural knowledge.
Indeed, with the globalization it is very common to meet Senegalese youth knowing more about the geography of the West than its own country.
Thus, to refresh the memory of our fellow citizens, we initiated the establishment of an online application that allows them to share and coconstruct their cultural heritage.
Our proposals are based on social and semantic web technologies.
Indeed, social web proposes a framework where value is created by the aggregation of many individual user contributions.
The semantic web enables to find, to combine and to share resources, not only between humans but also between machines.
The combination of these two technologies enables Senegalese communities to share and co-construct their cultural heritage in a collaborative and semantic framework.
Our contributions include to (i) propose ontologies to annotate sociocultural resources and (ii) provide a collaborative framework to Senegalese communities.
Ontologies are backbone of the semantic web and allow to characterize a domain.
Thus, we defined two ontologies: 1) a sociocultural ontology based on cultural-historical activity theory and 2) a temporal ontology to annotate temporally sociocultural resources.
We also defined a virtual community called cultural knowledge-building community and proposed a prototype that integrates our contributions.
Corpora which are text collections selected for specific purposes, are playing an increasing role in Linguistics and Natural Language Processing (NLP).
They are conceived as knowledge sources on natural language use, as much as knowledge on the entities designated by linguistic expressions, and they are used in particular to evaluate NLP application performances.
The criteria prevailing on their constitution have an obvious, though still delicate to characterize, impact on (i) the major linguistic structures they contain, (ii) the knowledge conveyed, and, (iii) computational systems' success on a give task.
This thesis studies methodologies of automatic extraction of semantic relations on written text corpora.
Such a topic calls for a detailed examination of the context in which a given expression holds, as well as for the discovery of the features which determine its meaning, in order to be able to link semantic units.
Generally, contextual models are built from the co-occurrence analysis of linguistic informations, drawn from resources and NLP tools.
The benefits and limits of these informations are evaluated in a task of relation extraction from corpora belonging to different genres (press article, fairy tale, biography).
The results show that these informations are insufficient to reach a satisfying semantic representation as well as to design robust systems.
Two problems are particularly addressed.
On the one hand, it seems indispensable to add informations related to text genre.
So as to characterize the impact of genre on semantic relations, an automatic classification method, which relies on the semantic restrictions holding between verbs and nouns, is proposed.
The method is experimented on a fairy tale corpus and on a press corpus.
On the other hand, contextual models need to deal with problems which come under discourse surface variation.
In a text, related linguistic expressions are not always close to one another and it is sometimes necessary to design complex algorithms in order to detect long dependencies.
To answer this problem in a coherent manner, a method of discourse segmentation based on surface structure triggers in written corpora, is proposed.
It paves the way for grammars operating on macro-syntactic categories in order to structure the discursive representation of a sentence.
This method is applied prior to a syntactic analysis and its improvement is evaluated.
The solutions proposed to these problems help us to approach Information Extraction from a particular angle: the implemented system is evaluated on a task of Named Entity correction in the context of a Question-Answering System.
This specific need entails the alignment of a category definition on the type of answer expected by the question.
In the case of a software version upgrade, a technical migration or the implementation of new connectors to ensure a proper communication with third-party services, it is crucial to be able to keep the clients data, ensure its consistency and communicate it according to heterogeneous structures (i.e. metamodels) that are not known in advance.
In such a context, it is then important to efficiently handle the data 'conversion'from on metamodel to another.
This PhD thesis concerns the implementation of an automated model transformation to ensure a federate interoperability: the objective is to on-the-fly infer a set of rules that allow convert data from a source metamodel to a target metamodel.
For this, this thesis aims to study techniques coming from various disciplines such as semantic web, machine learning, natural language processing or datagraph exploitation thanks to opimization algorithms, for example.
This PhD thesis is funded by the Forterro Sylob company that will also bring real case studies.
This document presents the problems I have been interested in during my PhD thesis.
I begin with a concise presentation of the main results, followed by three relatively independent parts.
In the first part, I consider statistical inference problems on an i.i.d. sample from an unknown distribution over a countable alphabet.
The first chapter is devoted to the concentration properties of the sample's profile and of the missing mass.
This is a joint work with Stéphane Boucheron and Mesrob Ohannessian.
After obtaining bounds on variances, we establish Bernstein-type concentration inequalities and exhibit a vast domain of sampling distributions for which the variance factor in these inequalities is tight.
The second chapter presents a work in progress with Stéphane Boucheron and Elisabeth Gassiat, on the problem of universal adaptive compression over countable alphabets.
We give bounds on the minimax redundancy of envelope classes, and construct a quasi-adaptive code on the collection of classes defined by a regularly varying envelope.
In the second part, I consider random walks on random graphs with prescribed degrees.
I first present a result obtained with Justin Salez, establishing the cutoff phenomenon for non-backtracking random walks.
Under certain degree assumptions, we precisely determine the mixing time, the cutoff window, and show that the profile of the distance to equilibrium converges to the Gaussian tail function.
Then I consider the problem of comparing the mixing times of the simple and non-backtracking random walks.
The third part is devoted to the concentration properties of weighted sampling without replacement and corresponds to a joint work with Yuval Peres and Justin Salez.
For food security systems, data on cultivated surfaces and yields are a prerequisite for agricultural production forecast.
Moderate resolution satellite remote-sensing systems offer a synoptic vision that makes them a particularly appropriate information source for the estimation of such data.
However, the estimation of cultivated surfaces is still challenging in West Africa, because of highly fragmented farmland, specific weather conditions resulting in high regional variability in terms of agricultural systems and practices, and synchronized phenology of crops and natural vegetation due to the rainfall regime.
In this context, this thesis presents three original methodological approaches for the characterization of agricultural systems in West Africa by remote sensing.
These methods were developed using MODIS time series (from 250 to 500 m spatial resolution) acquired for Mali.
(i) The mapping of cultivated areas was carried out with spectral, spatial and textural indices derived from the images.
Two approaches were chosen: one of ISODATA type following a segmentation of the territory based on MODIS imagery, and the other of data mining type based on 'sequential patterns'.
The crop map obtained showed a better precision than that of the existing land cover global products (70% vs 50% in average).
Furthermore, it was shown that a significant part of user and producer errors (20 to 40%) could not be compressed due to farmland fragmentation.
(ii) The mapping of agricultural system types first required the definition of a typology derived from an IER (Institute of Rural Economy in Mali) field survey data base on 100 villages.
Three types of agricultural systems were determined at the village scale: mainly cereals (millet, sorghum), mainly intensive crops (maize, cotton) and a mixture of sorghum and cotton.
The classification of agricultural systems using the aforementioned remote sensing indicators was carried out by a Random Forest type algorithm with an overall accuracy of 62%.
Results bring to light the important part played by temporal NDVI and texture in agricultural system characterization.
(iii) Finally, for crop monitoring, the MODIS phenological product was tested and assessed using phenological variables obtained from agro-meteorological simulations made by the SARRA-H plant model.
Results show that this product contains inconsistencies due to the significant cloud cover linked with the start of the raining season.
After the suppression of incongruous data, the phenological transition dates for crop land derived from MODIS were shown to be earlier by 20 days than the SARRA-H-simulated transition dates, due mainly to the 'agro-ecosystem'mixed nature of surfaces at MODIS pixels scale.
The results of this thesis highlight new possibilities for the combinination of remote sensing, field data and agro-meteorological modelling, delivering nonstop information in time and space on the characterization of “Sahel” farmland.
While speech communication is a faculty that seems natural, a lot remainsto be understood about the nature of the cognitive representations and processes that are involved.
Central to this PhD research is the study of interactions between perception and action during production or perception of syllables.
We choose Bayesian Programming as a rigorous framework within which we provide a mathematical definition of the COSMO model ("Communicating Objects using Sensori-Motor Operations"), which allows to formalize motor, auditory and perceptuo-motor theories of speech communication and to study them quantitatively.
This approach first leads to a strong theoretical result:we prove an indistinguishability theorem, according to which, given some ideal learning conditions, motor and auditory theories make identical predictions for perception tasks, and therefore cannot be distinguished empirically.
This algorithm, which learns by mimicking acoustic targets, allows to acquire motor skills from acoustic inputs only, with the remarkable property of focusing its learning on the adequate regions.
We use syllables synthesized by a vocal tract model (VLAM) to analyse how thedifferent models evolve through learning and how robust they are to degradations.
Metagenomic data from human microbiome is a novel source of data for improving diagnosis and prognosis in human diseases.
Machine Learning has obtained great achievements on important metagenomics problems linked to OTU-clustering, binning, taxonomic assignment, etc.
The contribution of this PhD thesis is multi-fold: 1) a feature selection framework for efficient heterogeneous biomedical signature extraction, and 2) a novel deep learning approach for predicting diseases using artificial image representations.
The framework is efficient on a real and heterogeneous datasets containing metadata, genes of adipose tissue, and gut flora metagenomic data with a reasonable classification accuracy compared to the state-of-the-art methods.
The second approach is a method to visualize metagenomic data using a simple fill-up method, and also various state-of-the-art dimensional reduction learning approaches.
The new metagenomic data representation can be considered as synthetic images, and used as a novel data set for an efficient deep learning method such as Convolutional Neural Networks.
The results show that the proposed methods either achieve the state-of-the-art predictive performance, or outperform it on public rich metagenomic benchmarks.
Online reviewing websites help users decide what to buy or places to go.
These platforms allow users to express their opinions using numerical ratings as well as textual comments.
The numerical ratings give a coarse idea of the service.
On the other hand, textual comments give full details which is tedious for users to read.
In this dissertation, we develop novel methods and algorithms to generate personalized, aspect-based summaries of movie reviews for a given user.
The first problem we tackle is extracting a set of related words to an aspect from movie reviews.
Our evaluation shows that our method is able to extract even unpopular terms that represent an aspect, such as compound terms or abbreviations, as opposed to the methods from the related work.
We then study the problem of annotating sentences with aspects, and propose a new method that annotates sentences based on a similarity between the aspect signature and the terms in the sentence.
The third problem we tackle is the generation of personalized, aspect-based summaries.
We propose an optimization algorithm to maximize the coverage of the aspects the user is interested in and the representativeness of sentences in the summary subject to a length and similarity constraints.
Finally, we perform three user studies that show that the approach we propose outperforms the state of art method for generating summaries.
Chapter 1 introduces the dissertation, establishes the research questions and the methodology, questions the stakes of studying the markers um and uh, and lays out the study organization.
Chapter 2 defines the main types of disfluencies, clinical and naturally occurring, summarizes the state of the art on the topic, and presents the different positions on their discourse role.
Chapter 3 establishes the challenges regarding the fillers um and uh and summarizes studies that support the idea of different pragmatic and functional roles, suggesting that they are markers rather than just fillers.
Chapter 5 introduces the two corpora used in this dissertation, ATAROS and Switchboard (SWB), and establishes their contribution.
This chapter presents the methodologies for the annotations, the two versions of SWB, as well as the methodology adopted to construct an interoperability between the corpora to analyze um and uh.
Chapter 6 analyzes the distribution and the duration of the two markers in SWB and ATAROS depending on speaker and dyad gender, on the conversationÕs naturalness, and on speaker participation.
This chapter shows that um and uh are different from each other, that they have different distribution and duration cues depending on the variables, and therefore indicates that they are not used randomly.
Chapter 7 focuses on the production of um and uh in SWB, and on the perception of the two markers by comparing two transcription versions of the corpus.
The results of this chapter show that um and uh are more often missed than other frequent words such as function words, and that SWB transcribers make more transcription errors on uh than on um, suggesting that um plays a more important role in discourse than uh.
Chapter 8 investigates the relationship between stance and the presence and the position of um and uh in an utterance, and reveals that the presence and the position of the two markers is dependent with stance.
Chapter 9 looks at the relationship between stance and the acoustic realization of the vowel of the markers, compared to the vowel of other monosyllabic words.
The results indicate that the stance values affect the vowel realization to different extents.
Chapter 10 consists of a classification experiment that incorporates the findings from previous experiments to find out which features pertinent to um and uh (lexical, position, and acoustics) improve the systemÕs performance.
The results also indicate that different acoustic features improve the scores.
Chapter 11 concludes the dissertation by summarizing the results from chapters 6 through 10, underlying the impact of this study, and addressing the future directions of this project.
In this thesis, we harvest knowledge of two different types from online resources.
The first one is commonsense knowledge, i.e. intuitive knowledge shared by most people like "the sky is blue".
We extract salient statements from query logs and question-answering by carefully designing question patterns.
Next, we validate our statements by querying other web sources such as Wikipedia, Google Books, or image tags from Flickr.
We aggregate these signals to create a final score for each statement.
We obtain a knowledge base, QUASIMODO, which, compared to its competitors, has better precision and captures more salient facts.
The other kind of knowledge we investigate is hidden knowledge, i.e. knowledge not directly given by a data provider.
More concretely, some Web services allow accessing the data only through predefined access functions.
To answer a user query, we have to combine different such access functions, i.e., we have to rewrite the query in terms of the functions.
We study two different scenarios: In the first scenario, the access functions have the shape of a path, the knowledge base respects constraints called "Unary Inclusion Dependencies", and the query is atomic.
We show that the problem is decidable in polynomial time, and we provide an algorithm with theoretical evidence.
In the second scenario, we remove the constraints and create a new class of relevant plans called "smart plans".
We show that it is decidable to find these plans and we provide an algorithm.
This dissertation addresses the study of French regional accents based on two large corpora totalling over 100 hours of face-to-face speech (PFC) and telephone speech.
We investigated the perception, acoustic characteristics as well as automatic classification of French varieties (standard French, French spoken in the South of France, Alsace, Belgium and Switzerland) in order to model segmental and prosodic levels.
First, perceptual experiments were conducted to determine French listeners' capacities to discriminate between French accents.
Results were analysed using scaling and clustering techniques.
Next, acoustic analyses relying on automatic phonemic alignment were carried out measuring formants, fundamental frequency, duration and intensity so as to produce linguistically-motivated segmental and prosodic features.
Furthermore, region-specific pronunciation tendencies were quantified exploiting new alignments with augmented pronunciation dictionaries including relevant variants.
The different methods allowed us to highlight characteristic pronunciation traits such as the realisation of nasal vowels, /O/ fronting, the devoicing of voiced consonants and word-initial stress.
Most successful deep neural models are the ones with many layers which highly increases their number of parameters.
Training such models requires a large number of training samples which is not always available.
One of the fundamental issues in neural networks is overfitting which is the issue tackled in this thesis.
Such problem often occurs when the training of large models is performed using few training samples.
Many approaches have been proposed to prevent the network from overfitting and improve its generalization performance such as data augmentation, early stopping, parameters sharing, unsupervised learning, dropout, batch normalization, etc.
In this thesis, we tackle the neural network overfitting issue from a representation learning perspective by considering the situation where few training samples are available which is the case of many real world applications.
We propose three contributions.
The first one presented in chapter 2 is dedicated to dealing with structured output problems to perform multivariate regression when the output variable y contains structural dependencies between its components.
Our proposal aims mainly at exploiting these dependencies by learning them in an unsupervised way.
Validated on a facial landmark detection problem, learning the structure of the output data has shown to improve the network generalization and speedup its training.
The second contribution described in chapter 3 deals with the classification task where we propose to exploit prior knowledge about the internal representation of the hidden layers in neural networks.
This prior is based on the idea that samples within the same class should have the same internal representation.
We formulate this prior as a penalty that we add to the training cost to be minimized.
Empirical experiments over MNIST and its variants showed an improvement of the network generalization when using only few training samples.
Our last contribution presented in chapter 4 showed the interest of transfer learning in applications where only few samples are available.
In this application, the task consists in localizing the third lumbar vertebra in a 3D CT scan.
A pre-processing of the 3D CT scan to obtain a 2D representation and a post-processing to refine the decision are included in the proposed system.
With the development of information and Internet technology, human society has stepped into an era of information overload.
When people exchange information and resources, they leave traces in some way or other.
The modelled traces represent knowledge as well as experience concerning the interactive actions among users and resources.
Such traces can be defined, modelled and exploited in return to offer a clue on a variety of deductions.
This thesis focuses on implementing a recommender system by exploiting various collaborative traces in the group shared/collaborative workspace.
As a practical experience we tested our prototype in the context of the E-MEMORAe collaborative platform.
This work aims at presenting methods of construction of electronic dictionaries of frozen nominal sequences in Korean and their inflected forms, and at justifying their validity by applying our dictionary in applied fields of automatic analysis of Korean texts.
For lexicon-based recognition of frozen nominal sequences, we classified them in three categories according to typographical conventions: compact nouns (CN), frozen nouns with optional spacing (FNO) and frozen nouns with obligatory spacing (FNS).
Since inflected forms of frozen nominal sequences appear in Korean texts, we built (i) an electronic dictionary of FNO with 45000 entries and (ii) a transducer of sequences of nominal postpositions with their segmentation, and finally these two data sets were merged into a single dictionary through the aid of inflectional codes attached to each frozen nominal sequence and of the inflection module in INTEX.
Our dictionary built according to these methods has the principal following advantages compared to the preexistent systems: 1)
The dictionary of inflected forms of FNO allows automatic recognition of all the alternatives of FNO related to spacing 2)
The dictionary of inflected forms of FNO allows segmentation of inflected forms of FNO into a FNO and a sequence of nominal postpositions without error 3)
The dictionary of sequences of nominal postpositions represented by graphs allows their segmentation in nominal postpositions without error 4)
The dictionary of FNO is used for segmentation of the free nominal sequences written without delimiters 5)
The dictionary of FNO can be extended into a bilingual dictionary for machine translation 6) Each entry of the dictionary of FNO comprises useful codes for the natural language processing applications: codes indicating semantic features, status of predicative noun, head noun of each entry, origin and grammatical
In this study I examine different types of reciprocal nominal predicates or noun of reciprocity in order to give a full account on their syntactic and semantic properties.
These nouns can be found in constructions with the structure "These is Npred between A and B", correlating obligatory at least two elements (humans, concrets, ideas, etc.).
They can permute within the sentence without any effect of meaning.
My analysis aims to contribute to a more general research working out a semantic typology of a part of the lexicon, the predicative nouns in the perspective of a natural language processing.
The last years witnessed the success of the Linked Open Data (LOD) project as well as a significantly growing amount of semantic data sources available on the web.
However, there are still a lot of data not being published as fully materialized knowledge bases like as sensor data, dynamic data, data with limited access patterns, etc.
Such data is in general available through web APIs or web services.
Integrating such data to the LOD or in mashups would have a significant added value.
However, discovering such services requires a lot of efforts from developers and a good knowledge of the existing service repositories that the current service discovery systems do not efficiently overcome.
In this thesis, we propose novel approaches and frameworks to search for semantic web services from a data integration perspective.
Firstly, we introduce LIDSEARCH, a SPARQL-driven framework to search for linked data and semantic web services.
To achieve such a purpose, we apply natural language processing techniques and deep-learning-based text similarity techniques to leverage I/O relations from text to ontologies.
For instance in visual scene understanding, structured prediction allows to provide a natural language sentence describing the visual scene.
Other tasks can be found in many other areas such bioinformatics (prediction of a biomolecule) or natural language processing (machine translation, sequence labelling)...
Another angle to structured prediction is to consider it as a regression task in some structured output space.
This work examines the effects of different kinds of training for the acquisition of the British English monophthongs /æ, ʌ, ɑː, ɪ and iː/ by French late learners.
Behavioral measures were computed before and after training in order to evaluate changes occurring on perception and production abilities.
Furthermore, an Event Related Potentials study (MMN) was conducted in order to observe the neural correlates of the acquisition of the trained /æ-ʌ/ contrast.
A first High Variability Phonetic Training including identification and discrimination tasks with feedback was designed.
It was proposed to 16 French learners constituting the so-called PE-Group.
The feedback was based on the analysis of the acoustic parameters of the vowel produced by the learners, allowing them to evaluate their own production.
A third group, who did not receive any training program was also included (C-Group).
The PE-group was shown to perform better after training in identification and discrimination compared to the C-Group, and the PR-Group improved in identification only.
Production performances were evaluated through two different methods: 1) one based on the acoustic parameters of the vowels; 2) the other based on the classification of the produced vowels by native English speakers.
It was shown that the HVPT was better to improve production performances.
However, it was observed that production improvement was restricted to intelligibility, as evaluated by native speakers, and that foreign accent reflected by acoustic measures was not significantly reduced in experimental groups (PE and PR) compared to the C-Group.
The expected Event Related Potentials were not observed, but we discuss potential improvements to our protocol.
Weakly-supervised learning studies the problem of minimizing the amount of human effort required for training state-of-the-art models.
However, in practice weakly-supervised methods perform significantly worse than their fully-supervised counterparts.
This is also the case in deep learning, where the top-performing computer vision approaches remain fully-supervised, which limits their usage in real world applications.
This thesis attempts to bridge the gap between weakly-supervised and fully-supervised methods by utilizing motion information.
It also studies the problem of moving object segmentation itself, proposing one of the first learning-based methods for this task.
This is especially challenging due to the need to precisely capture object boundaries and avoid local optima, as for example segmenting the most discriminative parts.
In contrast to most of the state-of-the-art approaches, which rely on static images, we leverage video data with object motion as a strong cue.
In particular, our method uses a state-of-the-art video segmentation approach to segment moving objects in videos.
The approximate object masks produced by this method are then fused with the semantic segmentation model learned in an EM-like framework to infer pixel-level semantic labels for video frames.
Thus, as learning progresses, the quality of the labels improves automatically.
We then integrate this architecture with our learning-based approach for video segmentation to obtain a fully trainable framework for weakly-supervised learning from videos.
In the second part of the thesis we study unsupervised video segmentation, the task of segmenting all the objects in a video that move independently from the camera.
This task presents challenges such as strong camera motion, inaccuracies in optical flow estimation and motion discontinuity.
We address the camera motion problem by proposing a learning-based method for motion segmentation: a convolutional neural network that takes optical flow as input and is trained to segment objects that move independently from the camera.
It is then extended with an appearance stream and a visual memory module to improve temporal continuity.
The appearance stream capitalizes on the semantic information which is complementary to the motion information.
The visual memory module is the key component of our approach: it combines the outputs of the motion and appearance streams and aggregates a spatio-temporal representation of the moving objects.
The final segmentation is then produced based on this aggregated representation.
The resulting approach obtains state-of-the-art performance on several benchmark datasets, outperforming the concurrent deep learning and heuristic-based methods.
Human centered and concurring product design is based on the collaboration between mechanical engineers, human factor experts and industrial designers.
This collaboration is often difficult and can be eased through the use of intermediary objects such as Virtual Reality (VR).
Nevertheless, even though VR is widely used in the industry, it suffers from a lack of acceptance by product designers.
In the context of this research work, we suggest to use VR in the form of immersive multidisciplinary convergence support tools. These tools are developed in accordance with an anthropocentered approach, as a function of each specific product design project's needs.
In order to optimize development times, we propose a dedicated immersive software design methodology: the ASAP methodology (As Soon As Possible).
A first experiment, aiming to demonstrate the feasibility of the ASAP methodology and the effectiveness of the implemented immersive tools, has been conducted in the context of industrial product design projects.
A second experiment, involving more than 50 participants, has been conducted in the context of educational product design projects and led to the development of 12 immersive tools.
It demonstrated quantitatively the contribution of immersive tools to the perceived effectiveness of interdisciplinary convergence phases and the contribution of the ASAP methodology on the acceptation of VR by product designers.
This research work describes a first approach that could, according to us, allow a better integration of VR within product design processes.
Manual corpus annotation has become a key issue for Natural Langage Processing (NLP), as manually annotated corpora are used both to create and to evaluate NLP tools.
However, the process of manual annotation remains underdescribed and the tools used to support it are often misused.
This situation prevents the campaign manager from evaluating and guarantying the quality of the annotation.
We propose in this work a unified vision of manual corpus annotation for NLP.
It results from our experience of annotation campaigns, either as a manager or as a participant, as well as from collaborations with other researchers.
We first propose a global methodology for managing manual corpus annotation campaigns, that relies on two pillars: an organization for annotation campaigns that puts evaluation at the heart of the process and an innovative grid for the analysis of the complexity dimensions of an annotation campaign.
A second part of our work concerns the tools of the campaign manager.
We evaluated the precise influence of automatic pre-annotation on the quality and speed of the correction by humans, through a series of experiments on part-of-speech tagging for English.
Furthermore, we propose practical solutions for the evaluation of manual annotations, that proche che vide the campaign manager with the means to select the most appropriate measures.
Finally, we brought to light the processes and tools involved in an annotation campaign and we instantiated the methodology that we described.
Speaker diarization is the task of determining "who speaks when" in an audio stream that usually contains an unknown amount of speech from an unknown number of speakers.
Speaker diarization systems are usually built as the combination of four main stages.
First, non-speech regions such as silence, music, and noise are removed by Voice Activity Detection (VAD).
Next, speech regions are split into speaker-homogeneous segments by Speaker Change Detection (SCD), later grouped according to the identity of the speaker thanks to unsupervised clustering approaches.
Finally, speech turn boundaries and labels are (optionally) refined with a re-segmentation stage.
In this thesis, we propose to address these four stages with neural network approaches.
In the speech turn clustering stage, we propose to use affinity propagation on top of neural speaker embeddings.
Experiments on a broadcast TV dataset show that affinity propagation clustering is more suitable than hierarchical agglomerative clustering when applied to neural speaker embeddings.
The LSTM-based segmentation and affinity propagation clustering are also combined and jointly optimized to form a speaker diarization pipeline.
Compared to the pipeline with independently optimized modules, the new pipeline brings a significant improvement.
In addition, we propose to improve the similarity matrix by bidirectional LSTM and then apply spectral clustering on top of the improved similarity matrix.
The proposed system achieves state-of-the-art performance in the CALLHOME telephone conversation dataset.
To better understand its behavior, the analysis is based on a proposed encoder-decoder architecture.
Our proposed systems bring a significant improvement compared with traditional clustering methods on toy examples.
This research paper, situated at the crossroads of linguistics, language teaching and automated language processing tools, focuses firstly on the classification of the textual materials used in the teaching of French as a Foreign Language (FFL), and secondly on the analysis of the various criteria required for such a classification.
Our analysis centres chiefly on the lexical criterion, the determining factor in the reading process.
Starting from a pre-selected sample of textual materials drawn from FFL materials, we therefore set out to determine whether a classification along these lines was viable.
To this end, we analysed these texts from the point of view of their correspondence with the levels drawn up by the Common European Framework of References (CEFR), levels A1 to B2, using the inventories developed in the Reference Frameworks for French.
In the first part, we consider the questions of interdisciplinarity, textual materials, text typologies and reference frameworks, by seeking to determine to what extent the lexical approach may justifiably be adopted as the chief criterion of this classification.
In the second part, we use information technologies to analyse the correspondence between the textual materials and the CEFR levels based on the lexical criterion, with a view to examining the scope for developing a dynamic corpus of textual materials for use in FFL.
Nowadays, there are a lot of applications related to machine vision and hearing which tried to reproduce human capabilities on machines.
These problems are mainly amenable to a temporal signals classification problem, due our interest to this subject.
In fact, we were interested to two distinct problems, humain gait recognition and audio signal recognition including both environmental and music ones.
In the former, we have proposed a novel method to automatically learn and select the dynamic human body-parts to tackle the problem intra-class variations contrary to state-of-art methods which relied on predefined knowledge.
To achieve it a group fused lasso algorithm is applied to segment the human body into parts with coherent motion value across the subjects.
The method we propose takes into account various linguistic and discursive cues calling on different levels of analysis.
As obsolescence is a non-linguistic phenomenon, our hypothesis is that linguistic and discursive cues must be considered in terms of combinations.
A machine learning system is then implemented to bring out relevant cue configurations in the obsolescent segments characterized by the experts.
Both our objectives have been achieved: we propose a detailed description of obsolescence in our corpus of encyclopaedic texts as well as a prototype aid to updating.
A double evaluation was carried out: by cross validation on the corpus used for machine learning and by experts on a test corpus.
Radiologists use medical imaging solutions on a daily basis for diagnosis.
Improving user experience is a major line of the continuous effort to enhance the global quality and usability of software products.
Monitoring applications enable to record the evolution of various software and system parameters during their use and in particular the successive actions performed by the users in the software interface.
These interactions may be represented as sequences of actions.
Based on this data, this work deals with two industrial topics: software crashes and software usability.
Both topics imply on one hand understanding the patterns of use, and on the other developing prediction tools either to anticipate crashes or to dynamically adapt software interface according to users'needs.
For this purpose, we propose to use a binomial test to determine which type of patterns is the most appropriate to represent crash signatures.
The improvement of software usability through customization and adaptation of systems to each user's specific needs requires a very good knowledge of how users use the software.
In order to highlight the trends of use, we propose to group similar sessions into clusters.
We compare 3 session representations as inputs of different clustering algorithms.
The second contribution of our thesis concerns the dynamical monitoring of software use.
Both methodologies take advantage of the recurrent structure of LSTM neural networks to capture dependencies among our sequential data as well as their capacity to potentially handle different types of input representations for the same data.
This doctoral dissertation presents, discusses and proposes tools for the automatic information retrieval in big musical databases.
The main application is the supervised classification of musical themes to generate thematic playlists.
The first chapter introduces the different contexts and concepts around big musical databases and their consumption.
The second chapter focuses on the description of existing music databases as part of academic experiments in audio analysis.
This chapter notably introduces issues concerning the variety and unequal proportions of the themes contained in a database, which remain complex to take into account in supervised classification.
The third chapter explains the importance of extracting and developing relevant audio features in order to better describe the content of music tracks in these databases.
This chapter explains several psychoacoustic phenomena and uses sound signal processing techniques to compute audio features.
New methods of aggregating local audio features are proposed to improve song classification.
The fourth chapter describes the use of the extracted audio features in order to sort the songs by themes and thus to allow the musical recommendations and the automatic generation of homogeneous thematic playlists.
This part involves the use of machine learning algorithms to perform music classification tasks.
The contributions of this dissertation are summarized in the fifth chapter which also proposes research perspectives in machine learning and extraction of multi-scale audio features.
This linguistic approach has been implemented by a generator of expert system called "snark".
Opinion mining is a sub-discipline within Information Retrieval (IR) and Computational Linguistics. It refers to the computational techniques for extracting, classifying, understanding, and assessing the opinions expressed in various online sources like news articles, social media comments, and other user-generated content.
It is also known by many other terms like opinion finding, opinion detection, sentiment analysis, sentiment classification, polarity detection, etc.
Defining in more specific and simpler context, opinion mining is the task of retrieving opinions on an issue as expressed by the user in the form of a query.
There are many problems and challenges associated with the field of opinion mining.
In this thesis, we focus on some major problems of opinion mining.
Location Based Services (LBS) had been involved to deliver relevant geospatial information based on a geographic position or address.
We assume that integrating geospatial data from several sources may improve the quality of information offered to users.
The increasing development of networks and especially the Internet has greatly expanded the gap between heterogeneous information systems.
In a review of studies of interoperability of heterogeneous information systems, we find that all the work in this area tends to be in solving the problems of semantic heterogeneity.
The W3C (World Wide Web Consortium) standards proposed to represent the semantic ontology.
Ontology is becoming an indispensable support for interoperability of information systems, and in particular the semantics.
The structure of the ontology is a combination of concepts, properties and relations.
This combination is also called a semantic graph.
The OWL (Ontology Web Language) and RDF (Resource Description Framework) are the most important languages of the Semantic Web, and are based on XML.
RDF is the first W3C standard for enriching resources on the Web with detailed descriptions, and increases the facility of automatic processing of Web resources.
Descriptions may be characteristics of resources, such as the author or the content of a website.
These descriptions are metadata.
Enriching the Web with metadata allows the development of the so-called Semantic Web.
RDF is used to represent semantic graphs corresponding to a specific knowledge modeling.
RDF files are typically stored in a relational database and manipulated using SQL, or derived languages such as SPARQL.
This solution is well suited for small RDF graphs, but is unfortunately not well suited for large RDF graphs.
These graphs are rapidly evolving, and adapting them to change may reveal inconsistencies.
Driving the implementation of changes while maintaining the consistency of a semantic graph is a crucial task, and costly in terms of time and complexity.
An automated process is essential.
For these large RDF graphs, we propose a new way using formal verification entitled "Model Checking".
Model Checking is a verification technique that explores all possible states of the system.
In this way, we can show that a model of a given system satisfies a given property.
This thesis provides a new method for checking and querying semantic graphs.
We propose an approach called ScaleSem which transforms semantic graphs into graphs understood by the Model Checker (The verification Tool of the Model Checking method).
It is necessary to have software tools to perform the translation of a graph described in a certain formalism into the same graph (or adaptation) described in another formalism
The behaviour of machines is difficult to define, especially when machines have to adapt to a changing environment.
For example, this is the case when human-machine interactions are concerned.
Indeed, the machine has to deal with several sources of uncertainty to exhibit a consistent behaviour to the user.
First, it has to deal with the different behaviours of the users and also with a change in the behaviour of a user when he gets used to the machine.
Secondly, the communication between the user and the machine can be noisy, which makes the transfer of information more complicated.
The objective is thus to deal with the different sources of uncertainty to show a consistent behaviour.
Usually, dealing with uncertainties is performed by introducing models: models of the users, the task concerned or the decision.
However, the accuracy of the solution depends on the accuracy of expert knowledge used to build the models.
If machine learning, through reinforcement learning, has successfully avoided the use of model for the decision and removed \textit{ad hoc} knowledge about it, expert knowledge is still necessary.
The thesis presented in this work is that some constraints related to human expertise can be slackened without a loss of generality related to the introduction of models
Online Social Networks are increasing and piercing our lives such that almost every person in the world has a membership at least in one of them.
Among famous social networks, there are online shopping websites such as Amazon, eBay and other ones which have members and the concepts of social networks apply to them.
This thesis is particularly interested in the online shopping websites and their networks.
One of the challenging issues is providing useful information to help the customers in their shopping.
Thus, an underlying question the thesis seeks to answer is how to provide comprehensive information to the customers in order to help them in their shopping.
This is important for the online shopping websites as it satisfies the customers by this useful information and as a result increases their customers and the benefits of both sides.
To do so, the users are ranked based on two scores namely optimist and pessimist.
In the second part, a novel opinion propagation methodology is presented to reach an agreement and maintain the consistency among users which subsequently, makes the aggregation feasible.
The propagation is conducted considering the impacts of the influential users and the neighbors.
Ultimately, in the third part, the opinion aggregation is proposed to gather the existing opinions and present it as the valuable information to the customers regarding each product of the online shopping website.
To this end, the weighted averaging operator and fuzzy techniques are used.
The thesis presents a consensus opinion model in signed and unsigned networks.
This solution can be applied to any group who needs to find a plenary opinion among the opinions of its members.
This action-research in applied linguistics investigates the evolution of adolescent learners of German as a foreign language during a project involving corpus exploration.
It focuses on the role played by resources, tools, face-to-face learning versus distance learning and by the pedagogical relationship.
My research analyzes how the learners perceived and exploited three small specialized corpora containing texts representing each one sub-genre of the «tourist information» genre.
Secondly, I describe how the language and the discourse used in the texts changed during the project.
The tasks and tools of the project helped some of the learners to develop reading habits which focus on information retrieval more than on complete comprehension of the text.
The effects are texts written in a German ha! ving linguistic features that resemble those attested in the explored corpora.
The intertextual writing method offered by the project supports acculturation to the foreign language.
By means of concordancing and the import of collocations, the learners are now able to deal with some language features, especially with the domain of German adjective inflection, better than before.
The task of speaker diarization, as defined by NIST, considers the recordings from a corpus as independent processes.
The recordings are processed separately, and the overall error rate is a weighted average.
In this context, detected speakers are identified by anonymous labels specific to each recording.
Therefore, a speaker appearing in several recordings will be identified by a different label in each of the recordings.
Yet, this situation is very common in broadcast news data: hosts, journalists and other guests may appear recurrently.
Consequently, speaker diarization has been recently considered in a broader context, where recurring speakers must be uniquely identified in every recording that compose a corpus.
This generalization of the speaker partitioning problem goes hand in hand with the emergence of the concept of collections, which refers, in the context of speaker diarization, to a set of recordings sharing one or more common characteristics.
The work proposed in this thesis concerns speaker clustering of large audiovisual collections (several tens of hours of recordings).
The main objective is to propose (or adapt) clustering approaches in order to efficiently process large volumes of data, while detecting recurrent speakers.
The effectiveness of the proposed approaches is discussed from two point of view: first, the quality of the produced clustering (in terms of error rate), and secondly, the time required to perform the process.
For this purpose, we propose two architectures designed to perform cross-show speaker diarization with collections of recordings.
We propose a simplifying approach to decomposing a large clustering problem in several independent sub-problems.
Solving these sub-problems is done with either of two clustering approaches which takeadvantage of the recent advances in speaker modeling.
A coreference chain is the set of linguistic expressions — or mentions — that refer to the same entity or discourse object in a given document.
Coreference resolution consists in detecting all the mentions in a document and partitioning their set into coreference chains.
Coreference chains play a central role in the consistency of documents and interactions, and their identification has applications to many other fields in natural language processing that rely on an understanding of language, such as information extraction, question answering or machine translation.
Natural language processing systems that perform this task exist for many languages, but none for French — which suffered until recently from a lack of suitable annotated resources — and none for spoken language.
In this thesis, we aim to fill this gap by designing a coreference resolution system for spoken French.
To this end, we propose a knowledge-poor system based on an end-to-end neural network architecture, which obviates the need for the preprocessing pipelines common in existing systems while maintaining performances comparable to the state-of-the art.
We then propose extensions on that baseline, by augmenting our system with external knowledge obtained from resources and preprocessing tools designed for written French.
Finally, we propose a new standard representation for coreference annotation in corpora of written and spoken languages, and demonstrate its use in a new version of ANCOR, the first coreference corpus of spoken French.
As a first step, theoretical issues of such a research are clarified.
It leads to focus on the concept of semantic invariant to give an insight into the semantic identity of a given noun regardless of its contextual variations.
As a second step, the empirical object of this research – human body part nouns in contemporary French – is delimited.
The rest of the dissertation consists in the empirical investigation itself.
First of all, an overall description of the semantic variation of the French human body part nouns is provided.
Eventually, four more nouns (artère “artery”, épaule “shoulder”, bouche “mouth” and pied “foot”) are studied from a semantic point of view.
Each of these four studies offers a new opportunity to test the relevance of the semantic invariant concept in order to give an account of the polysemy in the nominals.
In certain sensitive environments, such as the healthcare domain, where users are generally trusted and where particular events may occur, such as emergencies, the implemented security controls in the corresponding information systems should not block certain decisions and actions of users.
This could have serious consequences.
Indeed, it is important to be able to identify and trace these actions and decisions in order to detect possible violations of the security policy put in place and fix responsibilities.
These functions are ensured by the a posteriori access control that lies on a monitoring mechanism based on logs.
In the literature, this type of access control hasbeen divided into three stages: log processing, log analysis, and accountability.
In this thesis, we cover these three areas of the a posteriori access control by providing new solutions, and we introduce new aspects that have not been addressed before.
Criminal analysis is a discipline that supports investigations practiced within the National Gendarmerie.
However, criminal analysis relies on entities to formalize its practice.
The presentation of the research context details the practice of criminal analysis as well as the constitution of judicial procedure files as textual corpora.
We then propose perspectives for the adaptation of natural language processing(NLP) and information extraction methods to the case study, including a comparison of the concepts of entity in criminal analysis and named entity in NLP.
This comparison is done on the conceptual and linguistic plans.
A first approach to the detection of entities in witness interviews is presented.
Finally, since textual genre is a parameter to be taken into account when applying automatic processing to text, we develop a structure of the «legal» textual genre into discourse, genres, and sub-genres through a textometric study aimed at characterizing different types of texts (including witness interviews) produced by the field of justice
Computer vision is an interdisciplinary field that investigates how computers can gain a high level of understanding from digital images or videos.
In artificial intelligence, and more precisely in machine learning, the field in which this thesis is positioned,computer vision involves extracting characteristics from images and then generalizing concepts related to these characteristics.
This field of research has become very popular in recent years, particularly thanks to the results of the convolutional neural networks that form the basis of so-called deep learning methods.
Today, neural networks make it possible, among other things, to recognize different objects present in an image, to generate very realistic images or even to beat the champions at the Go game.
Their performance is not limited to the image domain, since they are also used in other fields such as natural language processing (e. g. machine translation) or sound recognition.
In this thesis, we study convolutional neural networks in order to develop specialized architectures and loss functions for low-level tasks (color constancy) as well as high-level tasks (semantic segmentation).
In computer vision, the main approach consists in estimating the color of the illuminant and then suppressing its impact on the perceived color of objects.
Our experience shows that our method makes it possible to obtain competitive performances with the state of the art.
Nevertheless, our architecture requires a large amount of training data.
In order to partially correct this problem and improve the training of neural networks, we present several techniques for artificial data augmentation.
We are also making two contributions on a high-level issue: semantic segmentation.
This task, which consists of assigning a semantic class to each pixel of an image, is a challenge in computer vision because of its complexity.
On the one hand, it requires many examples of training that are costly to obtain.
On the other hand, it requires the adaptation of traditional convolutional neural networks in order to obtain a so-called dense prediction, i. e., a prediction for each pixel present in the input image.
To do this, we define a selective loss function that has the advantage of allowing the training of a convolutional neural network from data from multiple databases.
We also developed self-context approach that captures the correlations between labels in different databases.
Finally, we present our third contribution: a new convolutional neural network architecture called GridNet specialized for semantic segmentation.
Unlike traditional networks, implemented with a single path from the input (image) to the output (prediction), our architecture is implemented as a 2D grid allowing several interconnected streams to operate at different resolutions.
In addition, we empirically demonstrate that our architecture generalize many of well-known stateof- the-art networks.
We conclude with an analysis of the empirical results obtained with our architecture which, although trained from scratch, reveals very good performances, exceeding popular approaches often pre-trained
Oedipus, the character in sophocle's tragedy, solves the sphinx's enigma by + his own intelligence ;.
This is the starting point of a general reflection on the linguistic status of language games, the practice of which could be seen throughout all periods and in all cultures.
Oedipus's intelligence is based on a capacity for + calculating ; the interpretation of the enigma by giving up inductive reasoning (by recurrence) so as to adopt analogical reasoning instead.
In the second part, it is shown that the calculation of the meaning of the polysemous messages enables us to propose a pattern of a combinatory analysis which is a tool for the automatic treatment of language (atl), able to help calculate riddles and to interpret coded definitions of crosswords.
This pattern is used as a touchstone for an analysis of the semantic structures underlying interpretations and shows which lexical items are concerned by isotopy.
Isotopy is not in that case considered to be an element of the message but a process of the interpretation.
The whole approach is then based on interpretative semantics.
The third part is the developement of the reflection including the treatment of enigmatic messages in the issues of the man-machine dialogue (mmd) which enables us to deal with the ambiguities of some utterances and is able to understand + strange messages ; on the basis of propositions of interpretation extrapolated from the pattern.
Then little by little we analyse the calculation of the one who gets messages like an activity which consists in analysing graphematic and acoustic signs.
Taking the signs into account is a confrontation with what is expected in the linguistic system and it enables us to carry out a series of decisions leading to the identification of a coherent analysis.
This coherence and the analysis are compared to the approach adopted when + reading ; an anamorphosis (in art painting) or when decoding the organisation rules in suites of cards in eleusis'game.
We find a similar approach when we have to interpret the + scriptio continua ; on paleographic inscriptions, the technique of which serves as a basis for some literary experiences under duress and for hidden puns.
Nowadays, social media has widely affected every aspect of human life.
The most significant change in people's behavior after emerging Online Social Networks (OSNs) is their communication method and its range.
Having more connections on OSNs brings more attention and visibility to people, where it is called popularity on social media.
Depending on the type of social network, popularity is measured by the number of followers, friends, retweets, likes, and all those other metrics that is used to calculate engagement.
Studying the popularity behavior of users and published contents on social media and predicting its future status are the important research directions which benefit different applications such as recommender systems, content delivery networks, advertising campaign, election results prediction and so on.
This thesis addresses the analysis of popularity behavior of OSN users and their published posts in order to first, identify the popularity trends of users and posts and second, predict their future popularity and engagement level for published posts by users.
To this end, i) the popularity evolution of ONS users is studied using a dataset of 8K Facebook professional users collected by an advanced crawler.
The collected dataset includes around 38 million snapshots of users'popularity values and 64 million published posts over a period of 4 years.
Clustering temporal sequences of users'popularity values led to identifying different and interesting popularity evolution patterns.
The identified clusters are characterized by analyzing the users'business sector, called category, their activity level, and also the effect of external events.
Then ii) the thesis focuses on the prediction of user engagement on the posts published by users on OSNs.
A novel prediction model is proposed which takes advantage of Point-wise Mutual Information (PMI) and predicts users'future reaction to newly published posts.
Finally, iii) the proposed model is extended to get benefits of representation learning and predict users'future engagement on each other's posts.
The proposed prediction approach extracts user embedding from their reaction history instead of using conventional feature extraction methods.
The performance of the proposed model proves that it outperforms conventional learning methods available in the literature.
The models proposed in this thesis, not only improves the reaction prediction models to exploit representation learning features instead of hand-crafted features but also could help news agencies, advertising campaigns, content providers in CDNs, and recommender systems to take advantage of more accurate prediction results in order to improve their user services
This thesis deals with the application of artificial intelligence to the automatic classification of audio sequences according to the emotional state of the customer during a commercial phone call.
The goal is to improve on existing data preprocessing and machine learning models, and to suggest a model that is as efficient as possible on the reference IEMOCAP audio dataset.
We draw from previous work on deep neural networks for automatic speech recognition, and extend it to the speech emotion recognition task.
We are therefore interested in End-to-End neural architectures to perform the classification task including an autonomous extraction of acoustic features from the audio signal.
Traditionally, the audio signal is preprocessed using paralinguistic features, as part of an expert approach.
We choose a naive approach for data preprocessing that does not rely on specialized paralinguistic knowledge, and compare it with the expert approach.
In order to apply a neural network to a prediction task, a number of aspects need to be considered.
On the one hand, the best possible hyperparameters must be identified.
On the other hand, biases present in the database should be minimized (non-discrimination), for example by adding data and taking into account the characteristics of the chosen dataset.
We study these aspects in order to develop an End-to-End neural architecture that combines convolutional layers specialized in the modeling of visual information with recurrent layers specialized in the modeling of temporal information.
We propose a deep supervised learning model, competitive with the current state-of-the-art when trained on the IEMOCAP dataset, justifying its use for the rest of the experiments.
Our model is evaluated on two English audio databases proposed by the scientific community: IEMOCAP and MSP-IMPROV.
A first contribution is to show that, with a deep neural network, we obtain high performances on IEMOCAP, and that the results are promising on MSP-IMPROV.
Another contribution of this thesis is a comparative study of the output values ​​of the layers of the convolutional module and the recurrent module according to the data preprocessing method used: spectrograms (naive approach) or paralinguistic indices (expert approach).
We analyze the data according to their emotion class using the Euclidean distance, a deterministic proximity measure.
We try to understand the characteristics of the emotional information extracted autonomously by the network.
The idea is to contribute to research focused on the understanding of deep neural networks used in speech emotion recognition and to bring more transparency and explainability to these systems, whose decision-making mechanism is still largely misunderstood.
In natural language processing, two main approaches are used: machine learning and data mining.
In this context, cross-referencing data mining methods based on patterns and statistical machine learning methods is apromising but hardly explored avenue.
Formal approaches to representing the signs of Sign Languages are traditionally parametric and this work shows that they are inappropriate for use in computer science.
The main reasons are: the parameters used are neither all necessary nor do they form a sufficient set; parameters take on fixed values whereas signs are dynamic in nature and values change through time; parametric descriptions do not account for the signs'adaptability to context, hence are not reusable, which brings them to disregard the power in concision of sign languages.
We propose a model called Zebedee, which describes signs in a sequence of timing units, each of which specifies a set of necessary and sufficient constraints to apply to a skeleton.
The signing space is regarded as a Euclidean geometric space where any auxiliary geometric object may be built.
Dependencies between elements of the descriptions or indeed on context are not only possible but also made relevant, and are based on articulatory, semantic and cognitive issues.
We then give two complementary processes for evaluation: in computer science with the implementation of Zebedee in a signing avatar animation platform and an information display system for train stations, and in linguistics with a data base and new possibilities of queries that linguists may want to test.
As prospects, we discuss different computational fields in which Zebedee should be useful, and several present linguistic problems for which it holds pieces of solutions.
Wireless sensing has evolved since the discovery of radio wave echo detection and radar in 1886.
However, for the longest time, its usefulness was seldom for human-centric applications because of technical limitations, impracticality or costliness.
Introducing wireless networks awakened a newfound interest in developing new wireless sensing services for their seamlessness and versatility.
Integrating such functionalities would contribute to resolving some prominent societal issues.
Localization, motion detection, and vital signs monitoring have great potential for promoting healthy aging, public safety, and retail.
Contactless sensing offers an appreciable degree of freedom, enabling remote monitoring of the isolated elderly without hampering their daily lives.
It could assist public safety services for crowd counting and detection of survivors inside buildings during emergencies.
Retail and public facilities would benefit from passive and active localization to offer an enhanced experience to their visitors and to help their logistical efforts.
While other works provide coarse-grained solutions for resolving such issues, we use MIMO radar techniques to provide an accurate orientation estimation system for Wi-Fi infrastructures.
To be more precise, we analyze the phase information of signals received on the antenna array to compute the heading of a Wi-Fi terminal.
Current solutions are complex, costly, or not energy-efficient.
To address this problem, we introduce MIMO capabilities to LoRa LPWAN systems that provide accurate localization with limited startup costs.
We enable the angle of arrival estimation by leveraging a second antenna on the LoRaWAN gateway.
We also prove the usefulness of such information for wireless communication efficiency.
A third challenge for wireless localization is the inefficiency of current model-based approaches in case of non-line-of-sight conditions and the rigidity of data-driven approaches in case of propagation environment changes.
To address this challenge, we propose a new data-driven solution for passive localization to address the limitations of model-based localization techniques.
The ongoing climate change is modifying the environmental conditions and species have to adapt to these new constraints, either on the same site or migrating in new suitable sites leading to a modification of distribution area.
This repositioning has two main dimensions: (i) the species capacity to adapt to the new conditions (modification of life history traits) which is linked to the species resilience and (ii) the species capacity to explore new suitable habitats.
The objective of this study was to build a mechanistic model incorporating these two dimensions in order to evaluate, understand and predict the repositioning possibilities of European diadromous fish facing climate change.
In their life cycles, diadromous fish species have to use freshwater, estuarine and marine ecosystems.
These specific life history strategies represent a great repositioning potential in comparison to freshwater fish species.
A database of diadromous fish life history traits, incorporating those that could be influenced by climate change and those that could have an importance in the species repositioning potential, has been built.
An Analytic Hierarchy Process has been suggested to develop a composite score based on life traits aiming at assessing the diadromous species repositioning potential.
Then, the GR3D model (Global Repositioning Dynamics for Diadromous fish Distribution) has been developed in order to study with a dynamic approach the repositioning potential of diadromous fish, at a large scale, in a context of climate change.
This model is a simulation, stochastic and individual-based model incorporating the main population dynamics processes of a diadromous fish (reproduction, mortality, growth, upstream migration with dispersal and downstream migration).
A first exploratory application case simulating the repositioning of a virtual allis shad (Alosa alosa) population between two river catchments under a scenario of temperature increase has been carried out and the associated global sensitivity analysis has been performed in order to determine the influence of uncertain population dynamics parameters and of parameters defining the landscape stucture.
The results showed that dispersal distance and parameters related to sea lifespan and to survival at sea were crucial to determine the success of colonization.
Finally, the use of GR3D in a real application case allowed improving the understanding of allis shad persistence at the scale of its distribution area (i.e. the Atlantic coast) in a context of climate change.
Over time, simulation results of GR3D should be relevant and useful in management and conservation of diadromous fish species.
In this thesis, we propose different model-checking techniques for pushdown system models.
Pushdown systems (PDSs) are indeed known to be a natural model for sequential programs, as they feature an unbounded stack that can simulate the assembly stack of an actual program.
Our first contribution consists in model-checking the logic HyperLTL that adds existential and universal quantifiers on path variables to LTL against pushdown systems (PDSs).
The model-checking problem of HyperLTL has been shown to be decidable for finite state systems.
We prove that this result does not hold for pushdown systems nor for the subclass of visibly pushdown systems.
Therefore, we introduce approximation algorithms for the model-checking problem, and show how these can be used to check security policies.
In the second part of this thesis, as pushdown systems can fail to accurately represent the way an assembly stack actually operates, we introduce pushdown systems with an upper stack (UPDSs), a model where symbols popped from the stack are not destroyed but instead remain just above its top, and may be overwritten by later push rules.
We prove that the sets of successors post* and predecessors pre* of a regular set of configurations of such a system are not always regular, but that post* is context-sensitive, hence, we can decide whether a single configuration is forward reachable or not.
Finally, in order to analyse multi-threaded programs, we introduce in this thesis a model called synchronized dynamic pushdown networks (SDPNs) that can be seen as a network of pushdown processes executing synchronized transitions, spawning new pushdown processes, and performing internal pushdown actions.
We then apply this abstraction framework to a iterative abstraction refinement scheme.
The Arabic language is poor in electronic semantic resources.
Among those resources there is Arabic WordNet which is also poor in words and relationships.
This thesis focuses on enriching Arabic WordNet by synsets (a synset is a set of synonymous words) taken from a large general corpus.
In order to validate the method, we had to create a gold standard corpus (there are none in Arabic for this area) from Arabic WordNet, and then compare the GraPaVec method with Word2Vec and Glove ones.
The result shows that GraPaVec gives for this problem the best results with a F-measure 25 % higher than the others.
The generated classes will be used to create new synsets to be included in Arabic WordNet.
This thesis studies the use of ontology and knowledge base for guiding various steps of the Knowledge Discovery in Databases (KDD) process in the domain of pharmacogenomics.
This approach has been implemented using semantic Web technologies and leads finally to populating a pharmacogenomic knowledge base.
As a result, data to analyze are represented in the knowledge base, which is a benefit for guiding following steps of the knowledge discovery process.
Firstly, I study this benefit for feature selection by illustrating how the knowledge base can be used for this purpose.
Secondly, I describe and apply to pharmacogenomics a new method named Role Assertion Analysis (or RAA) that enables knowledge discovery directly from knowledge bases.
This method uses data mining algorithms over assertions of our pharmacogenomic knowledge base and results in the discovery of new and relevant knowledge.
With the rise of Web 2.0 and collaborative technologies that are attached to,the Web has now become a broad platform of exchanges between users.
The majority of websites is now dedicated to social interactions of their users, or offers tools to develop these interactions.
Our work focuses on the understanding of these exchanges, as well as emerging community structures arising, through a semantic approach.
To meet the needs of web analysts, we analyze these community structures to identify their essential characteristics as their thematic centers and central contributors.
Our semantic analysis is mainly based on reference light ontologies to define several new metrics such as the temporal semantic centrality and the semantic propagation probability.
We employ an online approach to monitor user activity in real time in our community analysis tool WebTribe.
We have implemented and tested our methods on real data from social communication systems on the Web
The present dissertation focuses on the acquisition of highly polysemous verbs by French as a second language learners.
I am particularly interested in the polysemy of the verb prendre.
This dissertation has two complementary objectives.
On the one hand, the first objective is to describe the polysemy of prendre through a lexical semantic analysis within a cognitive framework.
On the other hand, the second objective is to evaluate the impact of this polysemy on learners'knowledge of the verb's different senses in order to isolate those which are the most problematic for French L2 learners.
Furthermore, I also investigate whether the problems associated with polysemy for Anglophone learners can be explained through cross-linguistic influence.
Prior to carrying out the semantic analysis, a battery of syntactic tests was used in order to categorize the different uses of the verb prendre into the following categories: support verb constructions, idiomatic expressions and predicative uses.
I use the cognitive semantic concept of the windowing of attention in order to explain how each of the parts of the core meaning can be windowed in order to obtain the different senses of the verb.
Moreover, this analysis allowed me to formulated hypotheses about the differences between the verb prendre and its equivalents in English.
In order to meet the second objective of this dissertation, I conducted an experiment on 191 learners of French as a second language.
All participants completed three tasks: a cloze test to measure their proficiency level in French, a guided written production task and an acceptability judgement task.
The results of the logistic and multinomial regression analyses show not only that the semantic analysis proposed can predict participants' knowledge of the different senses of the verb, but also that participants whose first language is English behave differently from those whose first language is another language, confirming the presence of cross-linguistic influence for the English-speaking participants.
Statistical physics, originally developed to describe thermodynamic systems, has been playing for the last decades a central role in modelling an incredibly large and heterogeneous set of different phenomena taking for instance place on social, economical or biological systems.
Such a vast field of possible applications has been found also for networks, as a huge variety of systems can be described in terms of interconnected elements.
After an introductory part introducing these themes as well as the role of abstract modelling in science, in this dissertation it will be discussed how a statistical physics approach can lead to new insights as regards three problems of interest in network theory: how some quantity can be optimally spread on a graph, how to explore it and how to reconstruct it from partial information.
Some final remarks on the importance such themes will likely preserve in the coming years conclude the work.
This thesis offers a contrastive analysis of the notion of definiteness as conveyed by the system of the article in English and Standard Arabic.
The corpus, The Brook Kerith, by the Irish writer, George Moore, is chosen for geo-historical and literary reasons: the story takes place in the Holy Land at the dawn of this Christian era.
A contrastive analysis of the first chapter along with its translation is analyzed from a pragmatic and semantic perspective.
The analysis is followed by statistical and computational analyses.
It is found that the article “the” and the Arabic article “al'are used for seemingly the same purpose in the proportion of 76%.
The occurrence of the article “a/an” is 96% consistent with indefiniteness in Arabic.
However, the use of the “zero article” shows discrepancy as whether to use the article “al” or no article in Arabic.
In the last analysis, the cognitive operations underlying usage in both languages are similar.
The differences are on the level of the semiotic transformation of these deep operations.
This thesis addresses the subject of event detection in temporal signals for elderly monitoring by the use of a floor pressure sensor.
We first show that most proposed systems do not meet main practical issues and that floor systems constitute promising candidates for monitoring tasks.
Since complex signals require sophisticated models, we propose a random-forest-based approach that detects falls with state-of-the-art accuracy and meets hardware constraints with a feature selection procedure.
The model performance is improved with data augmentation and time aggregation of the random forest outputs.
Methods are tested on several data sets, showing interesting potential continuation, and a Python implementation is made available.
Finally, motivated by the issue of elderly monitoring while dealing with one-dimensional signals for a large areas, we propose to distinguish elderly persons from younger individuals with a model based on convolutional neural network and convolutional dictionary learning.
Since signals are mainly made of walks, the first part of the model is trained to recognize steps, and the last part of the model is trained with all previous layers frozen.
This novel approach to gait classification allows to isolate elderly-generated signals with very high accuracy.
Recommender Systems aim at pre-selecting and presenting first the information in which users may be interested.
This has raised the attention of the e-commerce, where the interests of users are analysed in order to predict future interests and to personalize the offers (a.k.a. items).
Recommender systems exploit the current preferences of users and the features of items/users in order to predict their future preference in items.
Although they demonstrate accuracy in many domains, these systems still face great challenges for both academia and industry: they require distributed techniques to deal with a huge volume of data, they aim to exploit very heterogeneous data, and they suffer from cold-start, situation in which the system has not (enough) information about (new) users/items to provide accurate recommendations.
Among popular techniques, Matrix Factorization has demonstrated high accurate predictions and scalability to parallelize the analysis among multiple machines.
However, it has two main drawbacks: (1) difficulty of integrating external heterogeneous data such as items'features, and (2) the cold-start issue.
Our contributions to this area are given by four aspects: (1) we improve the distribution of a matrix factorization recommendation algorithm in order to achieve better scalability, (2) we enhance recommendations performed by matrix factorization by studying the implicit interest of the users in the attributes of the items, (3) we propose an accurate and low-space binary vector based on Bloom Filters for representing users/items through a high quantity of features in low memory-consumption, and (4) we cope with the new user cold-start in collaborative filtering by using active learning techniques.
The experimentation phase uses the publicly available MovieLens dataset and IMDb database, what allows to perform fair comparisons to the state of the art.
Our contributions demonstrate their performance in terms of accuracy and efficiency.
In this thesis, we present a methodology for interactive and iterative extracting knowledge from texts-the KESAM system: A tool for Knowledge Extraction and Semantic Annotation Management.
KESAM is based on Formal Concept Analysis for extracting knowledge from textual resources that supports expert interaction.
In the KESAM system, knowledge extraction and semantic annotation are unified into one single process to benefit both knowledge extraction and semantic annotation.
Semantic annotations are used for formalizing the source of knowledge in texts and keeping the traceability between the knowledge model and the source of knowledge.
The knowledge model is, in return, used for improving semantic annotations.
The KESAM process has been designed to permanently preserve the link between the resources (texts and semantic annotations) and the knowledge model.
The core of the process is Formal Concept Analysis that builds the knowledge model, i.e. the concept lattice, and ensures the link between the knowledge model and annotations.
In order to get the resulting lattice as close as possible to domain experts' requirements, we introduce an iterative process that enables expert interaction on the lattice.
Experts are invited to evaluate and refine the lattice; they can make changes in the lattice until they reach an agreement between the model and their own knowledge or application's need.
Thanks to the link between the knowledge model and semantic annotations, the knowledge model and semantic annotations can co-evolve in order to improve their quality with respect to domain experts' requirements.
Moreover, by using FCA to build concepts with definitions of sets of objects and sets of attributes, the KESAM system is able to take into account both atomic and defined concepts, i.e. concepts that are defined by a set of attributes.
In order to bridge the possible gap between the representation model based on a concept lattice and the representation model of a domain expert, we then introduce a formal method for integrating expert knowledge into concept lattices in such a way that we can maintain the lattice structure.
The expert knowledge is encoded as a set of attribute dependencies which is aligned with the set of implications provided by the concept lattice, leading to modifications in the original lattice.
The method also allows the experts to keep a trace of changes occurring in the original lattice and the final constrained version, and to access how concepts in practice are related to concepts automatically issued from data.
The method uses extensional projections to build the constrained lattices without changing the original data and provide the trace of changes.
From an original lattice, two different projections produce two different constrained lattices, and thus, the gap between the representation model based on a concept lattice and the representation model of a domain expert is filled with projections.
The vast majority of skin cancer deaths are due to malignant melanoma.
It is considered as one of the most dangerous cancers.
In its early stages, malignant melanoma is completely curable with a simple biopsy.
Therefore, an early detection is the best solution to improve skin cancer prognostic.
Medical imaging such as dermoscopy and standard camera images are the most suitable tools available to diagnose melanoma at early stages.
To help radiologists in the diagnosis of melanoma cases, there is a strong need to develop computer aided diagnosis (CAD) systems.
The accurate segmentation and classification of pigment skin lesions still remains a challenging task due to the various colors and structures developed randomly inside the lesions.
The model is adapted to segment the variations of the pigment inside the lesion and not only the main border.
The method has been implemented on DermIs and DermQues, two databases of standard camera images, and achieved an accuracy of 86.54% with a sensitivity of 80% and a specificity of 95.45%.
The method has been implemented on the PH2 database for dermoscopy images with 1000-random sampling cross validation.
Metric distance learning is a branch of re-presentation learning in machine learning algorithms.
We summarize the development and current situation of the current metric distance learning algorithm from the aspects of the flat database and nonflat database.
For a series of algorithms based on Mahalanobis distance for the flat database that fails to make full use of the intersection of three or more dimensions, we propose a metric learning algorithm based on the submodular function.
For the lack of metric learning algorithms for relational databases in non-flat databases, we propose LSCS(Relational Link-strength Constraints Selection) for selecting constraints for metric learning algorithms with side information and MRML (Multi-Relation Metric Learning) which sums the loss from relationship constraints and label constraints.
Through the design experiments and verification on the real database, the proposed algorithms are better than the current algorithms.
The issue at stake is the automated meaning allocation.
In a first time, a theoretical scheme is elaborated to describe meaning change for a lexical unit already defined in a lexical resource.
Our model relies on quantitative evidence and it is inspired from text semantics.
The preexisting meaning is represented as a structured set of semantic features.
The context modifies it dueto salient semantic featuresin texts.
These dynamic change is comprehended through description strata ranging from coarse-grained to fine-grained semantic units.
In a second time, we dwell on relevant resources and tools from corpus linguistics.
Concretely, we use the Trésor de la Langue Française informatisé as a dictionary.
Its entries are automatically converted into bags of semantic features.
The textual dataconsists in three recent journalistic corpus.
The resources are considered are mathematic spaces and statistical tools are used to extract significant units and to structure information.
In a last time, we give an outline of a process to allocate automatically a new meaning.
Experiments illustrate each step.
Through this approach, it is possible to qualify the new meaning in a precise and structured way.
The use of multiple choice questions to assess knowledge is a reliable and widely used method, even in official contexts.
Such a method offers many advantages, including equality of marking between candidates, or, more pragmatically, the possibility of automatic correction.
With the emergence of MOOCs (courses delivered in a digital format), the need for automatic evaluation has increased.
The scope of this thesis is part of this context, by proposing a solution that enables automatic thematic question generation.
The work presented in this thesis uses knowledge bases as data sources to automatically generate thematic multiple-choice questions.
This thesis presents a method based on Wikipedia metadata to identify and sort knowledge base entities according to predefined themes.- In order to be intelligible, a question must be grammatically correct, and must include enough information to remove any ambiguity about the correct answer.
In a last contribution, we present the method used to select distractors that are not only relevant to the question's statement, but also to its context.
Script is a structure describes an appropriate sequence of events or actions in our daily life.
A story, is invoked a script with one or more interesting deviations, which allows us to deeper understand about what were happened in routine behaviour of our daily life.
Therefore, it is essential in many ambient intelligence applications such as healthmonitoring and emergency services.
Hence, human activity recognition (HAR) has become a hot topic interest of research over the past decades.
In order to do HAR, most researches used machine learning approaches such as Neural network, Bayesian network, etc.
Therefore, the ultimate goal of our thesis is to generate such kind of stories or scripts from activity data of wearable sensors using machine learning approach.
However, to best of our knowledge, it is not a trivial task due to very limitation of information of wearable sensors activity data.
Hence, there is still no approach to generate script/story using machine learning, even though many machine learning approaches were proposed for HAR in recent years (e.g., convolutional neural network, deep neural network, etc.) to enhance the activity recognition accuracy.
Secondly, we introduce a novel scheme to automatically generate scripts from wearable sensor human activity data using deep learning models, and evaluate the generated method performance.
Finally, we proposed a neural event embedding approach that is able to benefit from semantic and syntactic information about the textual context of events.
The approach is able to learn the stereotypical order of events from sets of narrative describing typical situations of everyday life.
The framework of this thesis is speaker-independent automatic speech recognition.
This document begins with a short survey of speech signal processing applied to speech recognition.
Then, we describe several common architectures: dynamic time warping of acoustic shapes, artificial intelligence, neural networks and hidden Markov models.
This document is made of two main parts.
The first part is devoted to the recognition of words.
We are using finite state automata for modeling the eleven American spoken digits.
As the speech database TiDigits has been used by other teams we can compare our results against thoose obtained with other approaches.
The first step is concerned with isolated word recognition.
Then, we describe how sentences of the database have been segmented.
Last sections of this part describe continuous speech recognition of word sequences, as well as a discussion about strong and weak points of our approach.
The second part treats of the use of production models for speech recognition.
We begin with a description of the acoustic equations which drive the air flow inside the vocal tract and we present several articulatory models.
Then, we justify the choice of Maeda's model.
We describe the adaptation of this model to a male speaker.
Next, we describe the variational method used for recovering articulatory trajectories from the speech.
Finally, the software we built, is described.
In the conclusion, we give the results obtained and we exhibit sorne perspectives for future work towards a better speaker indepedent continuous speech recognition system.
Nowadays, remotely sensed images constitute a rich source of information that can be leveraged to support several applications including risk prevention, land use planning, land cover classification and many other several tasks.
In this thesis, Satellite Image Time Series (SITS) are analysed to depict the dynamic of natural and semi-natural habitats.
We introduce an object-oriented method to analyse SITS that consider segmented satellites images.
Firstly, we identify the evolution profiles of the objects in the time series.
Then, we analyse these profiles using machine learning methods.
To analyse these evolution graphs, we introduced three contributions.
The first contribution explores annual SITS.
It analyses the evolution graphs using clustering algorithms, to identify similar evolutions among the spatio-temporal entities.
We consider several study areas described by multi-annual SITS.
In the third contribution, we introduce à semi-supervised method based on constrained clustering.
We propose a method to select the constraints that will be used to guide the clustering and adapt the results to the user needs.
Our contributions were evaluated on several study areas.
The experimental results allow to pinpoint relevant landscape evolutions in each study sites.
We also identify the common evolutions among the diﬀerent sites.
In addition, the constraint selection method proposed in the constrained clustering allows to identify relevant entities.
Thus, the results obtained using the unsupervised learning were improved and adapted to meet the user needs.
Pharmacovigilance suffers from chronic underreporting of drug's adverse effects from health professional's part.
The FDA (US Food and Drug Administration), The EMA (European Medicines Agency), and other health agencies, suggest that social media could constitute an additional data source for detection of weak pharmacovigilance signals.
The WHO (World Health Organization) published a report in 2003 outlining the problem of non-compliance with treatment over long term and its prejudicial effectiveness on health systems worldwide.
The necessary data for development of an information extraction system from patient's forums are made available by the company Kappa Sante.
The first proposed approach fits into a context of pharmacovigilance case detection from patient's online discussions on health forums.
We propose a filter based on the number of words separating the name of the mentioned drug in the message from the term considered as a potential adverse effect.
We propose a second approach based on topic models to target groups of messages addressing topics dealing with non-compliance.
In terms of pharmacovigilance, the proposed Gaussian filter identifies 50.03% of false positives with a precision of 95.8% and a recall of 50%.
The case detection approach of non-compliance allows the identification of messages describing this kind of behaviors with a precision of 32.6% and a recall of 98.5%.
Behavioral Strategy suggests that an explanation may be found in the psychology of decision makers, and particularly in their cognitive biases.
This, however, calls for a link between individual-level cognition and affects, and organization-level choices.
We propose “Strategic Choice Routines” as a middle level of analysis to bridge this gap, and identify three broad types of Strategic Choice Routines.
This leads us to formulate hypotheses on how Strategic Choice Routines can be modified to minimize strategic errors.
We illustrate these hypotheses through case studies; test some of them quantitatively; and analyze preferences that drive their adoption by executives.
Finally, we discuss theoretical and managerial implications.
Recent developments in information technologies have made the web an important data source.
However, the web content is very unstructured.
Therefore, it is a difficult task to automatically process this web content in order to extract relevant information.
This is a reason why research work related to Information Extraction (IE) on the web are growing very quickly.
Our research work is at the crossroads of both areas.
The main goal of our work is to develop strategies and techniques for crawling the web in order to extract complex Named Entities (NEs) (NEs with several properties that may be text or other NEs).
We then propose to index them and to query them in order to answer information needs.
This work was carried out within the T2I team of the LIUPPA laboratory, in collaboration with Cogniteev, a company which core business is focused on the analysis of web content.
The issues we had to deal with were the extraction of complex NEs on the web and the development of IR services supplied by the extracted data.
Our first contribution is related to complex NEs extraction from text content.
For this contribution, we take into consideration several problems, in particular the noisy context characterizing some properties (the web page describing an event for example, may contain more than one dates: the event's date and the date of ticket's sales opening).
For this particular problem, we introduce a block detection module that focuses property's extraction on relevant text blocks.
Our experiments show an improvement of system's performances.
We also focused on address extraction where the main issue arises from the fact that there is not a standard way for writing addresses in general and on the web in particular.
We therefore propose a pattern-based approach which uses some lexicons for extracting addresses from text, regardless of proprietary resources.
Our second contribution deals with similarity computation between complex NEs.
In the state of the art, this similarity computation is generally performed in two steps: (i) first, similarities between properties are calculated; (ii) then the obtained similarities are aggregated to compute the overall similarity.
Our main proposals focuses on the second step.
We propose three techniques for aggregating property's similarities.
The first two are based on the weighted sum of these property's similarities (simple linear combination and logistic regression).
The third technique however, uses decision trees for the aggregation.
We also propose a similarity computation function between spatial EN, one represented by a point and the other by a polygon.
The approach adopted to build COATIS system aims at finding an operational method for constructing representations from a text, taking into account the causality notion.
Underwater robots can nowadays operate in complex environments in a broad scope of missions where the use of human divers is difficult for cost or safety reasons.
However the complexity of aquatic environments requires to give the robotic vector an autonomy sufficient to perform its mission while preserving its integrity.
This requires to design control laws according to application requirements.
They are built on knowledge from several scientific fields, underlining the interdisciplinarity inherent to robotics.
Once the control law designed, it must be implemented as a control Software working on a real-time Software architecture.
Nonetheless the current conception of control laws, as "monolithic" blocks, makes difficult the adaptation of a control from an application to another and the integration of knowledge from various scientific fields which are often not fully understood by control engineers.
Moreover this will allow us a more efficient projection on the Software architecture.
We thus propose a new formalism for control laws description as a modular composition of basic entities named Atoms used to encapsulate the knowledge items.
We propose as well a methodology relying on our formalism to guide the implementation of control on a real-time Middleware.
We will focus on the ContrACT Middleware developed at LIRMM.Finally we illustrate our approach on several robotic functionalities that can be used during aquatic environments exploration and especially for wall avoidance during the exploration of a karst aquifer.
At the time of the beginning of this work presented in this document (2003), genome annotation was a long and tedious task.
With the advent of new sequencing technologies, many tools have been developed to facilitate and accelerate this process.
At best, the annotation of an automatic genome can take less than 3 minutes, making manual annotation the more time consuming activity.
Thus, many genomes are deposited into sequence banks without expert manual annotation.
It quickly became clear that annotators needed to be provided with the possibility of accessing consolidated databases specific to their field of expertise.
This paper presents a modular annotation and visualization tool, GenoBrowser, which we created as part of the research in our microbiology team.
This allows us to easily integrate new functionalities related to Omics data generated in the team.
The architecture of our tool and the creation of a specific API (Application Programming Interface) enabled us to develop and make available to the scientific community two databases (P2CS and P2TF) dedicated to regulation networks in bacteria, as well as the associated web server for prediction of these systems for genomes sequenced de novo.
This work has led to the development of a set of tools within a research team to support expertise in environmental genomics research.
It allowed us to work on the consolidation and reuse of the growing amount of Omics data and to carry out a new research theme to help team members: bibliomics, the study of all scientific publications using NLPs (Natural Language Processing) approaches.
Application of spoken language understanding aim to extract relevant items of meaning from spoken signal.
There is two distinct types of spoken language understanding: understanding of human/human dialogue and understanding in human/machine dialogue.
Given a type of conversation, the structure of dialogues and the goal of the understanding process varies.
However, in both cases, most of the time, automatic systems have a step of speech recognition to generate the textual transcript of the spoken signal.
Speech recognition systems in adverse conditions, even the most advanced one, produce erroneous or partly erroneous transcript of speech.
Those errors can be explained by the presence of information of various natures and functions such as speaker and ambience specificities.
They can have an important adverse impact on the performance of the understanding process.
The first part of the contribution in this thesis shows that using deep autoencoders produce a more abstract latent representation of the transcript.
This latent representation allow spoken language understanding system to be more robust to automatic transcription mistakes.
In the other part, we propose two different approaches to generate more robust representation by combining multiple views of a given dialogue in order to improve the results of the spoken language understanding system.
The second one introduce new autoencoders architectures that use supervision in the denoising autoencoders.
By means of a qualitative and inductive method, this study acknowledges a special interest in ordinary speakers' metalinguistic evaluative statements and attempts to determine the different meanings and functions attached to those evaluating terms.
Machine learning (ML) algorithms are designed to learn models that have the ability to take decisions or make predictions from data, in a large panel of tasks.
However, when data distribution is complex (e.g. high-dimensional with nonlinear interactions between observed variables), the simplifying assumptions can be counterproductive.
In this situation, a solution is to feed the model with an alternative representation of the data.
Recently, a branch of ML called deep learning (DL) completely shifted the paradigm.
In this thesis, we are interested in learning representations of multivariate time series (MTS) and graphs.
MTS and graphs are particular objects that do not directly match standard requirements of ML algorithms.
They can have variable size and non-trivial alignment, such that comparing two MTS or two graphs with standard metrics is generally not relevant.
Hence, particular representations are required for their analysis using ML approaches.
The contributions of this thesis consist of practical and theoretical results presenting new MTS and graphs representation learning frameworks.
Two MTS representation learning frameworks are dedicated to the ageing detection of mechanical systems.
First, we propose a model-based MTS representation learning framework called Sequence-to-graph (Seq2Graph).
We show that using this method, under few assumptions, we identify the true state underlying the studied mechanical system, up-to monotone scalar transform.
Two graph representation learning frameworks are dedicated to the classification of graphs.
We show that this feature matches minimal requirements to classify graphs when all the meaningful information is contained in the structure of the graphs.
This study is mainly concerned with the description of the portuguese noun phrase, in a scientific and technical context provided by medical texts.
Whilst being traditionally linguistic in nature, the analysis also means to bring to light a certain number of linguistic resources which may ultimately serve in langage processing activities, and in particular those of document processing a machine-aided translation.
Sign Languages (SLs) have developed naturally in Deaf communities.
With no written form, they are oral languages, using the gestural channel for expression and the visual channel for reception.
These poorly endowed languages do not meet with a broad consensus at the linguistic level.
These languages make use of lexical signs, i.e. conventionalized units of language whose form is supposed to be arbitrary, but also - and unlike vocal languages, if we don't take into account the co-verbal gestures - iconic structures, using space to organize discourse.
Other corpora consist of interpreted SL, which may also differ significantly from natural SL, as it is strongly influenced by the surrounding vocal language.
In this thesis, we wish to show the limits of this approach, by broadening this perspective to consider the recognition of elements used for the construction of discourse or within illustrative structures.
We then propose the redesign of a French Sign Language dialogue corpus, Dicta-Sign-LSF-v2, with rich and consistent annotations, following an annotation scheme shared by many linguists.
We then propose a redefinition of the problem of automatic SLR, consisting in the recognition of various linguistic descriptors, rather than focusing on lexical signs only.
At the same time, we discuss adapted metrics for relevant performance assessment.
In order to perform a first experiment on the recognition of linguistic descriptors that are not only lexical, we then develop a compact and generalizable representation of signers in videos.
This is done by parallel processing of the hands, face and upper body, using existing tools and models that we have set up.
Besides, we preprocess these parallel representations to obtain a relevant feature vector.
We then present an adapted and modular architecture for automatic learning of linguistic descriptors, consisting of a recurrent and convolutional neural network.
Finally, we show through a quantitative and qualitative analysis the effectiveness of the proposed model, tested on Dicta-Sign-LSF-v2.
The study of the model predictions then demonstrates the merits of the proposed approach, with a very interesting performance for the continuous recognition of four linguistic descriptors, especially in view of the uncertainty related to the annotations themselves.
The segmentation of the latter is indeed subjective, and the very relevance of the categories used is not strongly demonstrated.
Indirectly, the proposed model could therefore make it possible to measure the validity of these categories.
With several areas for improvement being considered, particularly in terms of signer representation and the use of larger corpora, the results are very encouraging and pave the way for a wider understanding of continuous Sign Language Recognition.
This thesis focuses on SMS language and information extraction from the point of view of natural language processing.
The starting point of our study is the observation of the differences that most short messages have, using the alpes4science corpora, in comparison with the standard language.
The differences are highlighted by the particular morphology of words and by the syntactic and grammar rules that are not respected when the issuer considers that it would not impair the intelligibility of the message.
Because of the deviations from the standard language, processing and analyzing noisy messages is still a challenge for any NLP task.
The obtained result from this model was evaluated, afterwards, for named entities recognition through a series of tests applied thanks to three other systems.
The results have shown that these performances of named entity recognition systems are significantly improved when applied to automatically normalized SMS in comparison with raw and manually normalized corpora.
Keywords: computer-mediated communication, SMS language, SMS normalization
This thesis work focuses on audio-visual detection of emotional (laugh and smile) and attentional markers for elderly people in social interaction with a robot.
To effectively understand and model the pattern of behavior of very old people in the presence of a robot, relevant data are needed.
I participated in the collection of a corpus of elderly people in particular for recording visual data.
The system used to control the robot is a Wizard of Oz, several daily conversation scenarios were used to encourage people to interact with the robot.
These scenarios were developed as part of the ROMEO2 project with the Approche association.
We described at first the corpus collected which contains 27 subjects of 85 years'old on average for a total of 9 hours, annotations and we discussed the results obtained from the analysis of annotations and two questionnaires.
My research then focuses on the attention detection and the laughter and smile detection.
The motivations for the attention detection are to detect when the subject is not addressing to the robot and adjust the robot's behavior to the situation.
After considering the difficulties related to the elderly people and the analytical results obtained by the study of the corpus annotations, we focus on the rotation of the head at the visual index and energy and quality vote for the detection of the speech recipient.
The laughter and smile detection can be used to study on the profile of the speaker and her emotions.
My interests focus on laughter and smile detection in the visual modality and the fusion of audio-visual information to improve the performance of the automatic system.
Spontaneous expressions are different from posed or acted expression in both appearance and timing.
Designing a system that works on realistic data of the elderly is even more difficult because of several difficulties to consider such as the lack data for training the statistical model, the influence of the facial texture and the smiling pattern for visual detection, the influence of voice quality for auditory detection, the variety of reaction time, the level of listening comprehension, loss of sight for elderly people, etc.
The systems of head-turning detection, attention detection and laughter and smile detection are evaluated on ROMEO2 corpus and partially evaluated (visual detections) on standard corpus Pointing04 and GENKI-4K to compare with the scores of the methods on the state of the art.
We also found a negative correlation between laughter and smile detection performance and the number of laughter and smile events for the visual detection system and the audio-visual system.
This phenomenon can be explained by the fact that elderly people who are more interested in experimentation laugh more often and therefore perform more various poses.
The variety of poses and the lack of corresponding data bring difficulties for the laughter and smile recognition for our statistical systems.
The experiments show that the head-turning can be effectively used to detect the loss of the subject's attention in the interaction with the robot.
For the attention detection, the potential of a cascade method using both methods in a complementary manner is shown.
Source camera identification has recently received a wide attention due to its important role in security and legal issue.
The problem of establishing the origin of digital media obtained through an imaging device is important whenever digital content is presented and is used as evidence in the court.
Source camera identification is the process of determining which camera device or model has been used to capture an image.
Our first contribution for digital camera model identification is based on the extraction of three sets of features in a machine learning scheme.
These features are the co-occurrences matrix, some features related to CFA interpolation arrangement,and conditional probability statistics computed in the JPEG domain.
These features give high order statistics which supplement and enhance the identification rate.
The experiments prove the strength of our proposition since it achieves higher accuracy than the correlation-based method.
The second contribution is based on using the deep convolutional neural networks(CNNs).
Unlike traditional methods, CNNs can automatically and simultaneously extract features and learn to classify during the learning process.
A layer of preprocessing is added to the CNN model, and consists of a high pass filter which is applied to the input image.
The obtained CNN gives very good performance for a very small learning complexity.
Experimental comparison with a classical two steps machine learning approach shows that the proposed method can achieve significant detection performance.
The well known object recognition CNN models, AlexNet and GoogleNet, are also examined.
Any buzz has one or more starting point(s), i.e. primary source(s).
The information is then passed on by secondary sources which may speed or slow down its spreading depending on their influence.
Throughout the buzz lifecycle, the semantic content can evolve.
To understand a buzz on the Internet, one needs to analyze what is said and qualify who speaks.
This thesis will focus on two main points: a topological analysis of the sources (graph theory and networks), and an analysis of the textual content (corpus linguistics).
Clustering is the task of finding a partition of items, such that items in the same cluster are more similar than items in different clusters.
One challenge consists in designing a system capable of taking benefit of the different sources of data.
Objects can also be described by a graph which captures the relationships objects have with each others.
In addition to this, some constraints can be known about the data.
It can be known that an object is of a certain type or that two objects share the same type or are of different types.
It can also be known that on a global scale, the different types of objects appear with a known frequency.
In this thesis, we focus on clustering with three different types of constraints: label constraints, pairwise constraints and power-law constraint.
A label constraint specifies in which cluster an object belong.
Pairwise constraints specify that pairs of object should or should not share the same cluster.
Finally, the power-law constraint is a cluster-level constraint that specifies that the distribution of cluster sizes are subject to a power-law.
We want to show that introducing semi-supervision to clustering algorithms can alter and improve the solutions returned by unsupervised clustering algorithms.
We contribute to this question by proposing algorithms for each type of constraints.
Our experiments on UCI data sets and natural language processing data sets show the good performance of our algorithms and give hints towards promising future works.
The different dialects of the arabic language have a large phonological, morphological, lexical and syntactic variations when compared to the standard written arabic language called MSA (Modern Standard Arabic).
Until recently, these dialects were presented only in their oral form and most of the existing resources for the Arabic language is limited to the Standard Arabic (MSA), leading to an abundance of tools for the automatic processing of this variety.
Given the significant differences between the MSA and DA, the performance of these tools fall down when processing AD.
The presence of the latter in a disorderly manner in the discourse poses a serious problem for NLP (Natural Language Processing) and makes this oral a less resourced language.
However, the resources required to model this oral are almost nonexistent.
Thus, the objective of this thesis is to fill this gap in order to build a language model dedicated to an automatic recognition system for the oral spoken in the Tunisian media.
For this reason, we describe in this thesis a resource generation methodologyand we evaluate it relative to a language modeling task.
The results obtained are encouraging.
This work belongs to the field of performative control of voice synthesis, and more precisely of real-time modification of pre-recorded voice signals.
In a context where such systems were only capable of modifying parameters such as pitch, duration and voice quality, our work was carried around the question of performative modification of voice rhythm.
One significant part of this thesis has been devoted to the development of Vokinesis, a program for performative modification of pre-recorded voice.
It has been developed under 4 goals: to allow for voice rhythm control, to obtain a modular system, usable in public performances situations as well as for research applications.
To achieve this development, a reflexion about the nature of voice rhythm and how it should be controlled has been carried out.
It appeared that the basic inter-linguistic rhtyhmic unit is syllable-sized, but that syllabification rules are too language-dependant to provide a invariant inter-linguistic rhythmic pattern.
We showed that accurate and expressive sequencing of vocal rhythm is performed by controlling the timing of two phases, which together form a rhythmic group: the rhythmic nucleus and the rhythmic link.
We developed several rhythm control methods, tested with several control interfaces.
An objective evaluation showed that one of our methods allows for very accurate control of rhythm.
New strategies for voice pitch and quality control with a graphic tablet have been established.
The use of Vokinesis as a musical instrument has been successfully evaluated in public representations of the Chorus Digitalis ensemble, for various singing styles (from pop to contemporary music).
Application perspectives are diverse: scientific studies (research in prosody, expressive speech, neurosciences), sound and music production, language learning and teaching, speech therapies.
The present research takes issue with the supposed dichotomy between alternations on the one hand and surface generalisations on the other, within the framework of construction grammar.
More specifically the aim of this thesis is threefold.
Through the careful analysis of a large dataset, we aim to provide a thorough description of the causative alternation in English (The fabric stretched vs. Joan stretched the fabric), suggest a method that allows for a solid measure ofa verb's alternation strength and of the amount of shared meaning between two constructions,and finally, show that in order to capture constraints at the level of the construction, one must pay attention to lower level generalisations such as the interaction between verb and arguments within the scope of each construction.
In an effort to add to the discussion on alternation vs. surface generalisations, we propose a detailed study of the two constructions that make up the causative alternation: the intransitive non-transitive causative construction and the transitive causative construction.
Our goal is to measure the amount of meaning shared by the two constructions and also show the differences between the two.
In order to do so we take three elements into account: construction, verb and theme (i.e. the entity that undergoes the event denoted by the verb).
We use distributional semantics to measure the semantic similarity of the various themes found with each verb and each construction in our corpus.
This grouping highlights the different verb senses used with each construction and allows us to draw generalisations as to the constraints on the theme in each construction.
The amount of available scientific literature is constantly growing.
If the experts of a domain want to easily access this information, it must be extracted and structured.
To obtain structured data, both entities and relations of the texts must be detected.
Our research is about the problem of complex relation extraction which represent experimental results, and detection and classification of binary relations between biomedical entities.
We are interested in experimental results presented in scientific papers.
These results are important for biology experts, for example for doing modelization.
In the domain of renal physiology, a database was created to centralize these experimental results, but the base is manually populated, therefore the population takes a long time.
We propose a solution to automatically extract relevant knowledge for the database from the scientific papers, that is experimental results which are represented by a n-ary relation.
The method proceeds in two steps: automatic extraction from documents and proposal of information extracted for approval or modification by the experts via an interface.
We also proposed a method based on machine learning for extraction and classification of binary relations in specialized domains.
We focused on the variations of the expression of relations, and how to represent them in a machine learning system.
We studied the way to take into account syntactic structure of the sentence and the sentence simplification guided by the task of relation extraction.
In particular, we developed a simplification method based on machine learning, which uses a series of classifiers.
For a decade now, convolutional deep neural networks have demonstrated their ability to produce excellent results for computer vision.
For this, these models transform the input image into a series of latent representations.
In this thesis, we work on improving the "quality''of the latent representations of ConvNets for different tasks.
Then, we propose to structure the information in two complementary latent spaces, solving a conflict between the invariance of the representations and the reconstruction task.
This structure allows to release the constraint posed by classical architecture, allowing to obtain better results in the context of semi-supervised learning.
Finally, we address the problem of disentangling, i.e. explicitly separating and representing independent factors of variation of the dataset.
We pursue our work on structuring the latent spaces and use adversarial costs to ensure an effective separation of the information.
This allows to improve the quality of the representations and allows semantic image editing.
Seismic probabilistic risk assessment (SPRA) is one of the most widely used methodologies to assess and to ensure the performance of critical infrastructures, such as nuclear power plants (NPPs), faced with earthquake events.
The thesis provides discussions on the following aspects:
(i) Construction of meta-models with ANNs to build the relations between seismic IMs and engineering demand parameters of the structures, for the purpose of accelerating the fragility analysis.
The uncertainty related to the substitution of FEMs models by ANNs is investigated.
(ii) Proposal of a Bayesian-based framework with adaptive ANNs, to take into account different sourcesof information, including numerical simulation results, reference values provided in the literature and damage data obtained from post-earthquake observations, in the fragility analysis.
(iii) Computation of GMPEs with ANNs. The epistemic uncertainties of the GMPE input parameters, such as the magnitude and the averaged thirty-meter shear wave velocity, are taken into account in the developed methodology.
(iv) Calculation of the annual failure rate by combining results from the fragility and hazard analyses.
The fragility curves are determined by the adaptive ANN, whereas the hazard curves are obtained from the GMPEs calibrated with ANNs.
The proposed methodologies are applied to various industrial case studies, such as the KARISMA benchmark and the SMART model.
The relations between sets of paraphrases can be described as series of textual transformations.
To rephrase, an initial lexical substitution starts, then triggers other syntactic, lexical and morphological changes.
After having described the frequent paraphrasing mechanisms in our corpus, we propose two formalisations.
The first one is theoretical, explaining the different paraphrasing relationships maintained by the paraphrases between each other.
The second formalises paraphrase structures as predicate-argument ones.
We consider the latter suitable for paraphrase processing.
Finally we have implemented a paraphrase structures extraction system.
This is a compact operational system for the volume of data within our domain, the aim of which is to provide a concrete example of a possible use of our formalisation.
For the past two decades, electronic devices have revolutionized the traceability of social phenomena.
Social dynamics now leave numerical footprints, which can be analyzed to better understand collective behaviors.
The development of large online social networks (like Facebook, Twitter and more generally mobile communications) and connected physical structures (like transportation networks and geolocalised social platforms) resulted in the emergence of large longitudinal datasets.
These new datasets bring the opportunity to develop new methods to analyze temporal dynamics in and of these systems.
Nowadays, the plurality of data available requires to adapt and combine a plurality of existing methods in order to enlarge the global vision that one has on such complex systems.
The purpose of this thesis is to explore the dynamics of social systems using three sets of tools: network science, statistical physics modeling and machine learning.
The third chapter shows the added value of using longitudinal data.
We study the behavior evolution of bike sharing system users and analyze the results of an unsupervised machine learning model aiming to classify users based on their profiles.
The fourth chapter explores the differences between global and local methods for temporal community detection using scientometric networks.
The last chapter merges complex network analysis and supervised machine learning in order to describe and predict the impact of new businesses on already established ones.
We explore the temporal evolution of this impact and show the benefit of combining networks topology measures with machine learning algorithms.
The aim of this thesis is to build a piece of French grammar explaining the use of the infinitive syntax within the framework of polychrome tree grammars.
The thesis comprises two parts.
The first part studies problems associated with the infinitive.
The infinitive, like a verb, has complements and the component it forms with its complements can in turn be the complement to a verb, a noun or an adjective.
Traditional grammars express this as the double nature of an infinitive: it has the properties of a verb and of a noun/ it has both verbal and nominative properties.
It is therefore difficult to insert the infinitive syntax into a formal model allowing the use of computational linguistics.
We define the infinitive component as the unit comprising an infinitive verb and its complements.
We explain why this term is preferable to “infinitive subordinated proposition”.
The second part examines the syntactic analysis of infinitive components within the framework of polychrome tree grammars and is organized around the contexts in which the infinitive is used (a verb, a noun, an adjective).
A polychrome tree grammar representation makes it possible to test this formalism and demonstrates the benefits of separating the syntactic function of each category.
One can thus explain the cases in which the component of a non-nominal category performs a role generally performed by a noun.
Finally, this work on the syntax of the infinitive, which revisits certain studies from traditional grammars, enriches the formalism constructed within the framework of computational linguistics.
The present thesis aims to develop an efficient strategy for impact detection and classification in the presence of modeling uncertainties of the robot and its environment and using a minimum number of sensors, in particular in the absence of force/torque sensor.
The first part of the thesis deals with the detection of an impact that can occur at any location along the robot arm and at any moment during the robot trajectory.
Impact detection methods are commonly based on a dynamic model of the system, making them subject to the trade-off between sensitivity of detection and robustness to modeling uncertainties.
In this respect, a quantitative methodology has first been developed to make explicit the contribution of the errors induced by model uncertainties.
This methodology has been applied to various detection strategies, based either on a direct estimate of the external torque or using disturbance observers, in the perfectly rigid case or in the elastic-joint case.
A comparison of the type and structure of the errors involved and their consequences on the impact detection has been deduced.
In a second step, novel impact detection strategies have been designed: the dynamic effects of the impacts are isolated by determining the maximal error range due to modeling uncertainties using a stochastic approach.
Once the impact has been detected and in order to trigger the most appropriate post-impact robot reaction, the second part of the thesis focuses on the classification step.
In particular, the distinction between an intentional contact (the human operator intentionally interacts with the robot, for example to reconfigure the task) and an undesired contact (a human subject accidentally runs into the robot), as well as the localization of the contact on the robot, is investigated using supervised learning techniques and more specifically feedforward neural networks.
The challenge of generalizing to several human subjects and robot trajectories has been investigated.
This PhD dissertation is dedicated to the issue of 'exclusivist' growth and vulnerability characterizing Vietnam as well as developing Asia today.
The chapters address three important aspects of vulnerability and inclusive development, namely: Informality (chapter 1), Education dilemma (chapter 2) and Non-standard employment (chapter 3).
Chapter 2 focuses on the variation of the returns to higher education across the Vietnamese population with different estimation models.
Chapter 3 is the first study that systematically examines the wage differentials induced by temporary job status in Asian developing countries.
Overall, the whole thesis implies that human capital, employment, and income are interrelated facets of individual well-being, and that some development phenomena should be analyzed in their heterogeneity.
The protection of the natural environment is a key issue for the future of humanity.
SSE, which shares the principles of sustainable development, is particularly well suited to implement more environmentally friendly development alternatives.
The thesis looks at SSE organisations from the perspective of organisational identity and focuses on environmental communication on the one hand, and concrete actions on the other.
The study of environmental communication is based on the social network Twitter.
It is based on a program coded in Python, and on automatic text mining techniques.
It highlights several rhetorical strategies.
A second study deals with seven cases, based on semi-directive interviews.
It sheds light on the role of individual commitment but also on collective logic in environmental action.
This work makes a methodological contribution by developing the approach of automatic text mining, which is rarely used in Management Sciences.
On the theoretical level, the thesis introduces the collective dimension as anborganisational identity of the SSE.
We then adapt an environmental action model by identifying an additional determinant specific to these organizations.
Finally, the research invites the SSE to put ecological issues back at the centre, and gives suggestions for supporting organisations in their efforts to protect the environment.
However, the majority of this research has focused on static datasets, and the analysis and visualisation tasks tend to be carried out by trained expert users.
In more recent years, social changes and technological advances have meant that data have become more and more dynamic, and are consumed by a wider audience.
Examples of such dynamic data streams include e-mails, status updates, RSS 1 feeds, versioning systems, social networks and others.
These new types of data are used by populations that are not specifically trained in information visualization.
Some of these people might consist of casual users, while others might consist of people deeply involved with the data, but in both cases, they would not have received formal training in information visualization.
For simplicity, throughout this dissertation, I refer to the people (casual users, novices, data experts) who have not been trained in information visualisation as non-experts.
These social and technological changes have given rise to multiple challenges because most existing visualisation models and techniques are intended for experts, and assume static datasets.
Few studies have been conducted that explore these challenges.
In this dissertation, with my collaborators, I address the question: Can we empower non-experts in their use of visualisation by enabling them to contribute to data stream analysis as well as to create their own visualizations?
The first step to answering this question is to determine whether people who are not trained in information visualisation and the data sciences can conduct useful dynamic analysis tasks using a visualisation system that is adapted to support their tasks.
In the first part of this dissertation I focus on several scenarios and systems where different sized crowds of InfoVis non-experts users (20 to 300 and 2 000 to 700 000 people) use dynamic information visualisation to analyse dynamic data.
Another important issue is the lack of generic design principles for the visual encoding of dynamic visualization.
In this dissertation I design, define and explore a design space to represent dynamic data for non-experts.
This design space is structured by visual tokens representing data items that provide the constructive material for the assembly over time of different visualizations, from classic represen-tations to new ones.
This paradigm is inspired by well-established developmental psychological theory as well as past and existing practices of visualisation authoring with tangible elements.
I describe the simple conceptual components and processes underlying this paradigm, making it easier for the human computer interaction community to study and support this process for a wide range of visualizations.
Finally, I use this paradigm and tangible tokens to study if and how non-experts are able to create, discuss and update their own visualizations.
This study allows us to refine our previous model and provide a first exploration into how non-experts perform a visual mapping without software.
In summary, this thesis contributes to the understanding of dynamic visualisation for non-expert users.
In the future, robots will become our companions and co-workers.
However, we are still far from a real autonomous robot, which would be able to act in a natural, efficient and secure manner with humans.
To endow robots with the capacity to act naturally with human, it is important to study, first, how humans act together.
Consequently, this manuscript starts with a state of the art on joint action in psychology and philosophy before presenting the implementation of the principles gained from this study to human-robot joint action.
We will then describe the supervision module for human-robot interaction developed during the thesis.
Part of the work presented in this manuscript concerns the management of what we call a shared plan.
Here, a shared plan is a a partially ordered set of actions to be performed by humans and/or the robot for the purpose of achieving a given goal.
First, we present how the robot estimates the beliefs of its humans partners concerning the shared plan (called mental states) and how it takes these mental states into account during shared plan execution.
It allows it to be able to communicate in a clever way about the potential divergent beliefs between the robot and the humans knowledge.
Second, we present the abstraction of the shared plans and the postponing of some decisions.
Indeed, in previous works, the robot took all decisions at planning time (who should perform which action, which object to use…) which could be perceived as unnatural by the human during execution as it imposes a solution preferentially to any other.
This work allows us to endow the robot with the capacity to identify which decisions can be postponed to execution time and to take the right decision according to the human behavior in order to get a fluent and natural robot behavior.
The complete system of shared plans management has been evaluated in simulation and with real robots in the context of a user study.
Thereafter, we present our work concerning the non-verbal communication needed for human-robot joint action.
This work is here focused on how to manage the robot head, which allows to transmit information concerning what the robot's activity and what it understands of the human actions, as well as coordination signals.
Finally, we present how to mix planning and learning in order to allow the robot to be more efficient during its decision process.
The idea, inspired from neuroscience studies, is to limit the use of planning (which is adapted to the human-aware context but costly) by letting the learning module made the choices when the robot is in a "known" situation.
The first obtained results demonstrate the potential interest of the proposed solution.
Microblog service (such as Twitter and Sina Weibo) have become an important platform for Internet content sharing.
The analysis of the information diffusion in Microblogs involves the data collection from Microblog, the modeling on information spreading and using the resulting models.
Dealing with the huge amount of data flowing through microblogs is by itself a challenge.
Designing an efficient and unbiased sampling algorithm for Microblog is therefore essential.
Besides, the retweeting process in Microblog is complex because of the ephemerality of information, the topology of Microblog network and the particular features (such as number of followers) of publisher and retweeters.
Two traditional models have been used for information diffusion : Independent Cascades and Linear Threshold models.
However no one of them can describe completely the retweeting process in Microblog accurately.
The analysis and design of new models to characterize the information diffusion in Microblog is therefore necessary.
Moreover, a comprehensive description of the correlation between the information diffusion in Microblog and the searching trends of keywords on search engines is lacking although some work has been found some preliminary relationships.
This work presnets a complete analysis of information diffusion in Microblog from.
The contributions and innovations of this thesis are as follows:
1)There are two popular unbiased Online Social Network (OSN) sampling algorithms, Metropolis-Hastings Random Walk (MHRW) and Unbiased Sampling for Directed Social Graph (USDSG) method.
However they are both likely to yield considerable self-sampling probabilities when applied to Microblogs where there is local.
To solve this problem, I have modelled the process of OSN sampling as a Markov process and have deduced the sufficient and necessary conditions of unbiased sampling.
The experimental evaluation demonstrate that the average node degree of samples of MHRW and USDSG is 2 - 4 times as high as the ground truth while USDE can provide the approximation of ground truth when the sampling repetitions are removed.
2)A second contribution targets the shortages of Independent Cascades (IC) and Linear Threshold (LT) models in characterizing the retweeting process in Microblogs.
I achieve this by introducing a Galton Watson with Killing (GWK) model which considers all the three important factors including the ephemerality of information, the topology of network and the features of publisher and retweeters accurately.
Besides, the GWK model is useful for revealing the endogenous and exogenous factors which affect the popularity of tweets.
3) Motivated by the correlation between popularity and trendiness of topics in Microblog and search trends, I have developed an economic analysis of the market involving a third-party ad broker, which is a popular market in current SEM, and finds that the adwords augmenting strategy with the trending and popular topics in Twitter enables the broker to achieve, on average, four folds larger return on investment than with a non-augmented strategy, while still maintaining the same level of risk.
These last years, with the advent of sites such as Youtube, Dailymotion or Blip TV, the number of videos available on the Internet has increased considerably.
The size and their lack of structure of these collections limit access to the contents.
Summarization is one way to produce snippets that extract the essential content and present it as concisely as possible.
In this work, we focus on extraction methods for video summary, based on audio analysis.
We treat various scientific problems related to this objective: content extraction, document structuring, definition and estimation of objective function and algorithm extraction.
On each of these aspects, we make concrete proposals that are evaluated.
On content extraction, we present a fast spoken-term detection.
The main novelty of this approach is that it relies on the construction of a detector based on search terms.
We show that this strategy of self-organization of the detector improves system robustness, which significantly exceeds the classical approach based on automatic speech recogntion.
We then present an acoustic filtering method for automatic speech recognition based on Gaussian mixture models and factor analysis as it was used recently in speaker identification.
The originality of our contribution is the use of decomposition by factor analysis for estimating supervised filters in the cepstral domain.
We then discuss the issues of structuring video collections.
We show that the use of different levels of representation and different sources of information in order to characterize the editorial style of a video is principaly based on audio analysis, whereas most previous works suggested that the bulk of information on gender was contained in the image.
The third focus of this work concerns the summary itself.
As part of video summarization, we first try, to define what a synthetic view is.
Is that what characterizes the whole document, or what a user would remember (by example an emotional or funny moment)?
We then propose an algorithm for finding the sum of the maximum interest that derives from the one introduced in previous works, based on integer linear programming.
Bilingual lexicographic resources for multiword expressions (MWE) are still rare and they are even more so for the French / Arabic language pair that interests us.
There is little research on this language pair.
In our research project, we are interested in extracting, from specialized parallel and comparable fr-ar corpuses, MWE, in particular the verbally based patterns.
These constructions, from our point of view, constitute a relevant aspect to study in the specialized texts.
The resulting corpus will relate to the medical field, a strategic field at the present time where the whole world is facing a pandemic.
We will therefore take advantage of the availability of popular scientific articles which abound at this time.
In our study we will adopt a contrastive approach which consists in identifying and analyzing the MWE and their translational equivalences, and this in a corpus based approach.
Our main objective is to further research on this still under-studied language pair in this field, and to provide resources for translators and learners of specialized translation.
Ultimately, it will be about exploiting the results in the field of specialized computer-assisted translation learning, drawing inspiration from the Data Driven Learning approach.
All the steps will be carried out using the NLP tools appropriate for each language and each procedure.
This lead to a hierarchical division of the various tasks performed in order to analyze a text statement.
The traditional approach considers task-specific models which are subsequently arranged in cascade within processing chains (pipelines).
This approach has a number of limitations: the empirical selection of models features, the errors accumulation in the pipeline and the lack of robusteness to domain changes.
These limitations lead to particularly high performance losses in the case of non-canonical language with limited data available such as transcriptions of conversations over phone.
Disfluencies and speech-specific syntactic schemes, as well as transcription errors in automatic speech recognition systems, lead to a significant drop of performances.
Limestone is a historical building material widely used in Loire Valley of France.
Time passed, it is facing to health problems, mainly due to water content.
To evaluate this indicator, stepped-frequency radar is an efficient technique among all non-destructive testing techniques.
It can be combined with a frequency-domain full-waveform inversion technique, which is an efficient signal processing method to achieve permittivity that is the observable of water content, after a calibration process.
To do signal inversion, an analytical model of ultrawide band radar wave propagation through limestone is built in the forward study.
This model is based on one-dipole Green's function, for the EM propagation, and 4-parameters' Jonscher dispersion model, for the EM characterization of the medium.
This direct model is validated by a numerical approach using HFSS and an experimental approach using an experimental platform of Cerema.
The inverse problem, including objective function and minimization algorithm, to achieve the permittivity, is then studied.
Finally, this method is validated on several practical applications including geophysical information acquisition (water front estimation) and material characterization (water gradient estimation and hydric mapping).
This thesis consists of two parts, a theoretical part and a practical one.
The study will also tackle the link between terminography and specialized translation on both didactic and professional levels.
Secondly, using term extractors, the operation of terms extraction from the delimited corpus will be conducted.
After sorting the generated terms, according to well-defined linguistic criteria, these terms will be the subject of lexical and morphological analyses in order to highlight the way Arabic language adopt to form terms as well as the effect of its contact with other languages.
In addition, we will conduct a field study through a questionnaire, addressed to translators and conference interpreters, in order to examine their urgent medical terminology needs and assess their use of available terminology management tools.
This study has several purposes: meeting the urgent needs of finding new terms in the medical field and providing translators and conference interpreters with an easily accessible terminology management tool to assist them in their work that requires relevance and rapidity.
The study attempts also to explore new horizons in specialized translation teaching methods by highlighting the considerable contribution of terminotics in the acquisition of professional skills in this field.
The study aims also to assess the use of NLP software in order to underline the extent of their use as well as their limitations in terminology extraction, which should help developers better improve the performance of these tools.
While the benefits of early bilingual education programs are widely praised and acknowledged, relatively little is known about the processes involved in child second language acquisition.
The aim of this research is to investigate L2 development in young children while school is their only source of exposure to the L2.
Relying on corpora collected in two French-American kindergarten and primary schools in California, this work focuses on how French is acquired by English-speaking children from 5 to 7 through immersion education.
Following interactionist theories of L2 acquisition, it is shown how this specific context bears on the course of acquisition in terms of interaction opportunities and input addressed to the children, and also how it provides them with scaffolding patterns that are instrumental in the first stages of L2 acquisition.
Instead, it seems that child second language acquisition in this context is a rather complex phenomenon involving many factors such as linguistic and metalinguistic awareness, interaction strategies, or influence from L1.
Based on these findings, pedagogical guidelines for bilingual teaching in kindergarten are provided.
The present thesis offers both apparent-time and real-time analyses of two sub-corpora of the Diachronic Electronic Corpus of Tyneside English (DECTE): one from the 1970s and another one compiled in the 1990s (Corrigan et al., 2012).
It comprises two main parts: (1) an analysis of inter and intra-speaker variation in the lexical sets FACE, GOAT, PRICE and MOUTH (Wells 1982) based on a multiple factor analysis (MFA, Escofier 2008, Husson et al. 2011) (2) a dynamic acoustic analysis of formant trajectories of these vowels using Generalised additive mixed models (GAMMs, Wood 2015) followed by a static analysis of onsets in PRICE.
The first part establishes the sociolinguistic profiles of 44 speakers from Gateshead and Newcastle based on the original phonetic transcriptions of the Tyneside Linguistic Survey (TLS, Strang 1968).
Although the profiling analysis are based on the entire phonetic system transcribed by the original TLS team, the main focus is on FACE, GOAT, PRICE and MOUTH only.
Results indicate that FACE the main determinant of TE speech.
The symmetry between FACE et GOAT as found by Watt 1999, was also observed in PRICE et MOUTH among women. While middle-class women clearly favour a closing diphthong in FACE et GOAT and have a low onset in PRICE and MOUTH, working-class women tend to have higher frequency scores of pan-northern monophthongs in the first pair of lexical sets. They also exhibit more frequent raised onsets in PRICE and MOUTH.
In addition, the central monophthong GOAT is more often used by men with a less traditional accent in the 1970s corpus, which is in line with Watt's findings for the 1990s corpus (Watt 1998).
The second part analyses formant trajectories in FACE, GOAT and PRICE.
The main aim was to compare the original phonetic transcriptions with the corresponding formant trajectories.
Results confirm the pertinence of the transcriptions in the wordlist section of the corpora (TLS and PVC).
Differences between the two main variants of PRICE ([aɪ] vs. [eɪ]) appeared to be strikingly different be in terms of both onsets / offset heights and trajectory shape.
Advances in information technologies, such as communication technology and database technology, allow us to access to vast amounts of information and are causing an exponential increase in the number of documents available online.
Effective retrieval and mining are getting more and more difficult.
Document representation aims to represent document input into a fixed-length vector to reduce the complexity of the documents and make them easier to handle.
Therefore, document representation is playing an important role in many real-world applications, e.g., information retrieval, text clustering and classification.
Simple weighted averaging of word vectors is an effective way to represent sentences.
However, when the document is long, it may be not effective and would lose fine grained distinctions.
The reason is when the document is long, it is likely to contain words from many different topics and hence creating a single vector would ignore the thematic structure.
To overcome the above mentioned limitation, we want to do something a little bit different.
That is to say, instead of getting representations by simple average, we can represent a document by a set of vectors and we can get novel representation.
It turns out to be more reasonable because the set of vectors can cover different parts of the documents.
By this mean, the representation can have the ability to cover different topics.
The core part of document representation model is the hierarchical document encoder.
We want to proposed a method to firstly pre-train document level hierarchical bidirectional transformer encoders on unlabeled data.
Further the pretrained document models can be adapted to different tasks (information retrieval, text clustering and text classification) and collections via fine-tuning.
In this thesis, we study the expressivity of read speech with a particular type of data, which are audiobooks.
Audiobooks are audio recordings of literary works made by professionals (actors, singers, professional narrators) or by amateurs.
These recordings may be intended for a particular audience (blind or visually impaired people).
The availability of this kind of data in large quantities with a good enough quality has attracted the attention of the research community in automatic speech and language processing in general and of researchers specialized in expressive speech synthesis systems.
We propose in this thesis to study three elementary entities of expressivity that are conveyed by audiobooks: emotion, variations related to discursive changes, and speaker properties.
We treat these patterns from a prosodic point of view.
Information now occupies a central place in our daily lives, it is both ubiquitous and easy to access.
Yet extracting information from data is often an inaccessible process.
Indeed, even though data mining methods are now accessible to all, the results of these mining are often complex to obtain and exploit for the user.
Pattern mining combined with the use of constraints is a very promising direction of the literature to both improve the efficiency of the mining and make its results more apprehensible to the user.
However, the combination of constraints desired by the user is often problematic because it does not always fit with the characteristics of the searched data such as noise.
In this thesis, we propose two new constraints and an algorithm to overcome this issue.
The robustness constraint allows to mine noisy data while preserving the added value of the contiguity constraint.
The extended closedness constraint improves the apprehensibility of the set of extracted patterns while being more noise-resistant than the conventional closedness constraint.
The C3Ro algorithm is a generic sequential pattern mining algorithm that integrates many constraints, including the two new constraints that we have introduced, to provide the user the most efficient mining possible while reducing the size of the set of extracted patterns.
C3Ro competes with the best pattern mining algorithms in the literature in terms of execution time while consuming significantly less memory.
C3Ro has been experienced in extracting competencies from web-based job postings
Many successes of deep learning rely on the availability of massive annotated datasets that can be exploited by supervised algorithms.
Obtaining those labels at a large scale, however, can be difficult, or even impossible in many situations.
Designing methods that are less dependent on annotations is therefore a major research topic, and many semi-supervised and weakly supervised methods have been proposed.
Meanwhile, the recent introduction of deep generative networks provided deep learning methods with the ability to manipulate complex distributions, allowing for breakthroughs in tasks such as image edition and domain adaptation.
In this thesis, we explore how these new tools can be useful to further alleviate the need for annotations.
Firstly, we tackle the task of performing stochastic predictions.
It consists in designing systems for structured prediction that take into account the variability in possible outputs.
Then, we study adversarial methods to learn a factorized latent space, in a setting with two explanatory factors but only one of them is annotated.
We propose models that aim to uncover semantically consistent latent representations for those factors.
One model is applied to the conditional generation of motion capture data, and another one to multi-view data.
Finally, we focus on the task of image segmentation, which is of crucial importance in computer vision.
Building on previously explored ideas, we propose a model for object segmentation that is entirely unsupervised.
This thesis uses multi-scalar data to create a three-dimensional (3D) representation and, to generate a complete digital record of the early hominin-bearing fossil assemblage from the lithostratigraphic Unit P at Kromdraai in the Cradle of Humankind World Heritage Site (Gauteng Province, South Africa).
We provided a multi-scalar analysis of various aspects of the study site, with the application of methods such as multi-image land and aerial photogrammetry.
In alignment with the principles and guidelines for the management of archaeological heritage mandated by international agencies such as UNESCO, we also present a protocol for heritage documentation.
We used 3D data capture technologies to record the Kromdraai site and the archaeological evidence discovered between 2014 and 2018 from its main excavation.
This research presents an original technique developed for the quantification and visualization of the volume sediments removed from the site during each excavation period.
Volume estimations computed using 3D photogrammetry and digitization, provided a temporal and spatial context to the volume and location of material removed by each excavator and, a more precise and virtual repositioning of the fossil material discovered ex situ.
Furthermore, we implemented metadata modelling to demonstrate the use of 4D relational database management systems for the fusion, organisation and dissemination of the Kromdraai site dataset and the sharing of intellectual property.
We also introduce one of the first statistical approaches of 3D spatial patterning in Plio-Pleistocene early hominin-bearing assemblages in South Africa.
Implementing classic statistical testing methods such as k-means and Density-Based Spatial Clustering and Application with Noise (DBSCAN) cluster computation in 3D, we investigated the spatial patterns of the fossil assemblage within Unit P, a sample of 810 individually catalogued specimens recovered between 2014 and 2018.
The clustering of bovids, carnivores, hominins, and non-human primates revealed a non-uniform spatial distribution pattern of fossils in-situ.
The amount of information on the Internet today overwhelms most users.
Discovering relevant information (e.g. news to read or videos to watch) is time-consuming and tedious and yet it is part of the daily job of at least 80% of the employees in North America.
Several information filtering systems for the web can ease this task for users.
Examples fall into families such as Social Networks, Social Rating Systems and Social Bookmarking Systems.
All these systems require user engagement to work (e.g. submission or rating of content).
They work well in an Internet-wide community but suffer in the case smaller communities.
Indeed, in smaller communities, the users' input is more scarce.
We focus on communities of a place that are communities that group people who live, work or study in the same area.
Examples of communities of a place are: (i) the students of a campus, (ii) the people living in a neighborhood or (iii) researchers working in the same site.
Anecdotally we know that only 0.3% of workers contribute daily to their corporate social network.
This information shows that there is a lack of user engagement in communities of a place.
This thesis is dedicated to non-convex modeling and the optimization based on the DC programming and DCA for certain classes of problems of two important domains: the Data Mining and the Cryptology.
They are non-convex optimization problems of very large dimensions for which the research of good solution methods is always of actuality.
Our work is based mainly on the DC programming and DCA that have been successfully applied in various fields of applied sciences, including machine learning.
It is motivated and justified by the robustness and the good performance of DC programming and DCA in comparison with the existing methods.
This thesis is devised in three parties.
The first part, entitling Methodology, serves as a reference for other chapters.
The first chapter concerns the programming of DC and DCA while the second chapter describes the genetic algorithms.
In the second part, we develop the DC and DCA programming to solve two classes of problems in Data Mining.
In the chapter four, we take consideration into the model of classification FCM and develop the programming DC and DCA for their resolution.
Many formulations DC in correspondence to different decompositions DC are proposed.
Our work in hierarchic classification (chapter 5) is motivated by one of its interesting and very important applications, known as muliticast communication.
It's a non-convex, non differentiable, non-convex problem in a very big dimension with which we have reformulated in the forms of 3 different DC programs and developed the DCA relative.
The 3rd part focuses on the Cryptology.
We propose a method of resolving two problems PP and PPA by DCA and a cutting plan method in the last chapter
This thesis explores the construction of sense in English particle and prepositional verbs.
It departs from the premise that meaning is something constructed during the process of situated usage.
A corpus of 286 combinations formed through the association of a verbal element with over are analysed in context in order to identify the various factors which influence final semantic interpretation.
The relationship between syntactic configuration and semantic interpretation is investigated and the various ways in which the number and nature of the verbal and/or prepositional arguments can impact semantic interpretation is explored.
The study also explores several more general areas of linguistic investigation including the conceptualisation of movement, resultativity, transitivity and polysemy.
During the course of the study a wide range of factors which influence the final semantic interpretation of particle and prepositional verbs in English are identified.
In a context of development and maintenance of the enterprise competitiveness, Business and Competitive Intelligence feeds the enterprise with informations which may be used as reference objects for environment analysis and decision-making support.
The ALSEM project aims at the design of an automatic system assisting the Competitive Intelligence professional in the exploration and combination of informational resources dealing with the enterprise's environment.
The project's goal is the integration of technics offering a methodologic framework for an information management more suitted to a specific and complex task like Business and Competitive Intelligence.
Our work stands at the bridge of three disciplines: natural language processing, semantics and knowledge engineering.
It aims at the development of information extraction technics with a semantic analysis of texts and information interpretation and modelling technics to make it operational.
Semantic Web is the vision of next generation of Web proposed by Tim Berners-Lee in 2001.
Traditional Semantic Web querying and reasoning tools are designed to run in stand-alone environment.
Therefor, Processing large-scale bulk data computation using traditional solutions will result in bottlenecks of memory space and computational performance inevitably.
Large volumes of heterogeneous data are collected from different data sources by different organizations.
In this context, different sources always exist inconsistencies and uncertainties which are difficult to identify and evaluate.
To solve these challenges of Semantic Web, the main research contents and innovative approaches are proposed as follows.
For these purposes, we firstly developed an inference based semantic entity resolution approach and linking mechanism when the same entity is provided in multiple RDF resources described using different semantics and URIs identifiers.
We also developed a MapReduce based rewriting engine for Sparql query over big RDF data to handle the implicit data described intentionally by inference rules during query evaluation.
The rewriting approach also deal with the transitive closure and cyclic rules to provide a rich inference language as RDFS and OWL.
The second contribution concerns the distributed inconsistency processing.
We extend the approach presented in first contribution by taking into account inconsistency in the data.
The third contribution concerns the reasoning and querying over large-scale uncertain RDF data.
We propose an MapReduce based approach to deal with large-scale reasoning with uncertainty.
Unlike possible worlds semantic, we propose an algorithm for generating intensional Sparql query plan over probabilistic RDF graph for computing the probabilities of results within the query.
In this thesis, we study the problem of detection of visual relations of the form (subject, predicate, object) in images, which are intermediate level semantic units between objects and complex scenes.
Our work addresses two main challenges in visual relation detection: (1) the difficulty of obtaining box-level annotations to train fully-supervised models, (2) the variability of appearance of visual relations.
We first propose a weakly-supervised approach which, given pre-trained object detectors, enables us to learn relation detectors using image-level labels only, maintaining a performance close to fully-supervised models.
Experimental results demonstrate the improvement of our hybrid model over a purely compositional model and validate the benefits of our transfer by analogy to retrieve unseen triplets.
This Ph.D. thesis is about the establishment of textual data similarities in the client relation domain.
Two subjects are mainly considered: - the automatic analysis of short messages in response of satisfaction surveys ; - the search of products given same criteria expressed in natural language by a human through a conversation with a program.
The first subject concerns the statistical information from the surveys answers.
The ideas recognized in the answers are identified, organized according to a taxonomy and quantified.
The second subject concerns the transcription of some criteria over products into queries to be interpreted by a database management system.
The number of criteria under consideration is wide, from simplest criteria like material or brand, until most complex criteria like color or price.
The two subjects meet on the problem of establishing textual data similarities thanks to NLP techniques.
The main difficulties come from the fact that the texts to be processed, written in natural language, are short ones and with lots of spell checking errors and negations.
Establishment of semantic similarities between words (synonymy, antonymy,...) and syntactic relations between syntagms (conjunction, opposition,...) are other issues considered in our work.
We also study in this Ph. D. thesis automatic clustering and classification methods in order to analyse answers to satisfaction surveys.
Bourdieu, a sociologist, defines social capital as: "The set of current or potential ressources linked to the possession of a lasting relationships network".
On Twitter,the friends, followers, users mentionned and retweeted are considered as the relationships network of each user, which ressources are the chance to get relevant information, to be read, to satisfy a narcissist need, to spread information or advertisements.
We observe that some Twitter users that we call social capitalists aim to maximize their follower numbers to maximize their social capital.
We introduce their methods, based on mutual subscriptions and dedicated hashtags.
In order to study them, we first describe a large scale detection method based on their set of followers and followees.
Then, we show with an automated Twitter account that their methods allow to gain followers and to be retweeted efficiently.
Afterwards, we bring to light that social capitalists methods allows these users to occupy specific positions in the network allowing them a high visibility.
Furthermore, these methods make these users influent according to the major tools.
We thus set up a classification method to detect accurately these user and produce a new influence score.
The information cycle, from collection to dissemination is a cornerstone in competitive intelligence.
Our work is to study about the impact that web 2.0 has on the famous cycle and propose methods and tools to take advantage of this new paradigm, and this for each stage of the cycle
The thesis has been prepared within a co-supervision agreement with the Professors Jean-Hugues Chauchat (ERIC-Lyon2) and N.V. Charonova (National Polytechnic University of Kharkov in Ukraine).The results obtained can be summarized as follows:1.
State of the art:Retrospective of theoretical foundations concerning the formalization of knowledge and natural language as precursors of ontology engineering.
Update of the state of the art on general approaches in the field of ontology learning, and on methods for extracting terms and semantic relations.
Methodological proposals:Learning morphosyntactic patterns and implementing partial taxonomies of terms.
Finding semantic classes representing concepts and relationships for the field of radiological safety.
Building a frame for the various stages of the work leading to the construction of the ontology in the field of radiological safety.3.
Implementation of the three previous methods and analysis of the results obtained.
The Data Quality Monitoring of High Energy Physics experiments is a crucial and demanding task to deliver high-quality data used for physics analysis.
At the Compact Muon Solenoid experiment operating at the CERN Large Hadron Collider, the current quality assessment paradigm, is based on the scrutiny of a large number of statistical tests.
However, the ever increasing detector complexity and the volume of monitoring data call for a growing paradigm shift.
Here, Machine Learning techniques promise a breakthrough.
This dissertation deals with the problem of automating Data Quality Monitoring scrutiny with Machine Learning Anomaly Detection methods.
Anomalies caused by detector malfunctioning are difficult to enumerate a priori and rare, limiting the amount of labeled data.
This thesis explores the landscape of existing algorithms with particular attention to semi-supervised problems and demonstrates their validity and usefulness on real test cases using the experiment data.
As part of this project, the monitoring infrastructure was further optimized and extended, delivering methods with higher sensitivity to various failure modes.
In order to open their innovation process and co-create value with consumers, companies use idea crowdsourcing platforms, allowing them to gather innovative ideas from the crowd.
This doctoral thesis studies a new platform model mixing competition with cooperation, called the «co-opetition» model.
Our research question is: does the co-opetition model enhance consumer's creative performance, comparing with classical models based on competition or cooperation?
To answer this question, we have conducted two experiments aiming at quantitatively comparing co-opetition, competition and cooperation effects on participant's creative performance.
Results show that co-opetition and competition enhance creativity, while the hypothesized positive effect of cooperation is not supported.
There is a significant interaction effect between competition and cooperation: co-opetition increases creativity more than competition alone.
We find also a significant mediating effect of motivational ambivalence, while the hypothesized mediation effect of emotional ambivalence is not supported.
Attitude towards independance moderates the direct effects of co-opetition and competition on idea quality, while attitudes towards cooperation and competition do not.
This dissertation revolves around two wider topics: social norms and production networks.
The first chapter investigates a specific modern-day case study where social norms are leveraged in the fight against online hate speech to shed light on how norms shape political behavior more broadly.
Using machine learning techniques, I show that speaking out against hateful views is an effective way of deterring further hate speech.
The mechanism that most likely explains this effect is that vociferous contradiction in fact serves as a form of non-monetary punishment that raises the salience of a social norm.
The second chapter focuses on the crucial role of image concerns in explaining the effect of social norms on behavior.
While there are now plenty of studies showing that image concerns affect people on average, we still know very little about which individuals specifically drive that effect.
I introduce a novel laboratory experiment designed to fill this gap.
It generates an individual-specific measure of image concern, shows that there is substantial heterogeneity even in a small laboratory sample, and investigates how it correlates with other social preferences.
Vertical integration give rise to anticompetitive behavior or indeed be a motive for it.
I discuss one such mechanism, called vertical foreclosure, by which vertically integrating firms disrupt the supply of critical inputs to competitors.
I leverage novel production network data to identify mergers and acquisitions between vertically related firms and show taht these mergers affect the supply chains of their rivals, which I interpret as evidence for foreclosure.
This thesis builds upon the work on the Social Semantic Web, a research perspective on the complementarity and coevolution of two aspects of the Web, the social and semantic one.
Web development in recent years has given rise to a huge graph of semantically structured data, partly resulting from user activity.
We are particularly interested in the use of this graph in order to facilitate access to information found on the Web, in a useful, informative manner.
This problem is particularly studied in scenarios related to innovation on the Web - practices to use Web technologies to contribute to the emergence of innovation.
A notable specificity of this context, so far little discussed in literature, is the need to encourage serendipity and discovery.
Beyond the simple relevance sought in any search and recommendation situation on the Web, the context of innovation requires a certain openness to allow the user to access information relevant yet unexpected, and should also open opportunities to learn and translate ideas from one domain to another.
The thesis leans on the concept of media literacy.
After defined this concept, we have tried to implement it for non-reading children (2 - 7 years) of Togo and\or deaf persons (7 in 12 years) of France.
Assuming that the media contents can contribute to the development, to the apprenticeships and to the culture of these children, an information-communication analysis of the mechanisms of appropriation of these contents is made for these two target populations.
The study has mobilized experiments in Togo and in France.
This preliminary review of the grounds of study shows that the public politics and plans of communication appropriation for our target remain insufficient on one hand, and that the field is little approached on the previous works of the other one.
The second part bends over the theoretical frame of the media literacy and specifies its conceptualization in the field of information communication.
The protocol VI.A.G.E is then elaborated to estimate the process of appropriation of media contents with the children.
The third part is dedicated to the perusal of three experiments of ground led to Togo (experimental viewing of the movie Kirikou and the witch) and in France (visio-guide on DVD and iPad, and interaction on tactile big screen, in particular at Quai Branly Museum).
Their respective results are explained.
From the hypothesis which the form informs about the meaning, a corpus of attested utterances has been constituted and collected in a database so as to allow the observation of syntactic, lexical and semantic combinatorial of prepositional groups complements ("argumentaux") introduced by French preposition dans with a verb.
The research on one hand identifies and describes all the verbs which subcategorize this preposition and on one other hand finds the interpretations which emerge from these combinations so as to further in the semantic identity of the French preposition dans and of the complement in general.
From the syntactic point of view, the result of the investigation consists of a list (of uses) of definable verbs by their complementation with dans and characterized by a certain number of properties: the study brought to a successful conclusion in the definition of a lexical class, a paradigm also united on the semantic plan.
The French preposition dans is analyzable in every case as a marker of "coi͏̈ncidence", whatever is the particular value of a complement.
The contribution strictly linguistics of the study is defined and presented by sort to allow us the using in the computational linguistics field.
The anaphora traditionally handled in man-machine dialogues are pronominal anaphora.
Next, having exploited the corpus on the retrieval level (query typology, search strategies,...) and on the linguistic level (suraces structures), anaphora, reference and coherence,...), we express our doubts about the usability of natural language in certain situations and we also state the hypothesis of the need for a new "phraseology" for man-machine communication.
Concerning natural language processing (nlp, we explain the different types of knowledge necessary for dialogue management and anaphora resolution (dialogue modeling, task modeling, dialogue dynamics,...), on the basis of different linguistic approaches of anaphorical phenomena and existing work in nlp.
Within the framework of the nlp work done at criss (centre for research in informatics applied to the social sciences), we then derive some rules for identifying anaphoric units.
This thesis will be devoted to the use of natural language processing methods in finance.
We will first study sentiment analysis methods with the aim of applying them on financial news.
Before the era of artificial intelligence, feature engineering and statistical approaches were the dominant tools in this area, e.g., Naïve Bayes Classifier, TF-IDF.
However, most of these approaches are based on static embeddings and ignore contextual information in a sentence.
Hence, we are going to study possible improvements in accuracy by introducing contextualized embeddings into the models.
We will also methods enabling us to generate contextualized embedding, either by defining and training an embedding model from scratch or fine-tuning an existing model, such as BERT or XLNet.
We will then focus on sentiment analysis on long texts.
In finance, there are different types of such documents, for example annual reports or conference transcripts.
All these documents being much longer than a title or a summary, we cannot directly apply standard methods on these corpus.
Thus, we propose summarizing the text in the first place and then training a classification model.
Existing summarization methods such as, LDA (Latent Dirichlet Allocation) or BERTSum work well on somehow uniform texts.
However they have limited performance on these financial documents with various formats and characteristics.
We will investigate the transfer learning approach in this task.
We will first train a general model based on all available texts.
Then, for each type of corpus, we will consider a much more parsimonious model with limited amount of data.
Our final result will be obtained by aggregating the outputs of the general model and the specialized model.
Finally, we will be interested in the possibility of applying NLP methods on time series dimension reduction task.
Widely used methods in NLP such as Auto-encoder, Generative Adversarial Network (GAN) and Variational Auto-Encoder (VAE) all have an encoder part, which transforms high-dimensional embedding into multiple vectors as a summary.
Hence, based on this idea, we would like to design a model structure which takes in high-dimensional time series instead of texts and outputs a lower-dimensional time series.
Patents are industrial property titles that give their holders a monopoly over the patented invention.
It is possible to find a sort of history of the evolution of the artifact.
In this context the designer often like to do research in patent documents in order to benefit from the knowledge contained inside to structure the inventive process.
Developed to assist designers in their innovation approach, the Inventive Design Method (IDM) is part of the pattern of dialectic.
IDM has clarified the concepts at stake in the description of the evolution of technical systems and artifact.
These items often interest designers and are essential to understanding the underlying problem and collecting of all features on which to act; and the effect of variations on the artifact.
This thesis, firstly, deals with patent document analysis from a linguistic point of view, in order to know its typology.
Then, it is possible to identify in the patent document, the knowledge likely to be useful to IDM and formalize it as a computer program.
The approach proposed in this paper is based on text mining techniques.
It uses a method based on linguistic markers using lexical and syntactic patterns from the field of natural language processing.
This method of extraction of useful concepts for IDM allows the establishment of a kind of initial mapping of past and possible changes in the future of the artifact characteristics.
The interest is also to greatly facilitate the preliminary analysis of knowledge on the said artifact.
This thesis deals with the question of behavioral changes, and in particular with the way this question applies to the computer domain through persuasive technologies.
In a particular application context, that of the renovation of housing, we are interested in the role that the information available to users can play in the way they develop their renovation project.
One way to change user behavior is to change the goals they pursue, either explicitly or implicitly.
Although the effectiveness of the former has been shown in an experimental context, it seems less suitable for natural situations.
We therefore propose an approach aimed at modifying the goals pursued by the users implicitly.
With this in mind, we are working first on the use of injunctive social norms to encourage users to work particularly on energy renovation.
In a first study, we compare injunctive social norm and goal setting to a control condition.
We are interested in the performance of the participants in the task (improving the energy performance of a home) as well as the way in which the project is set up throughout the study.
The results show that social norm and explicit goal have a similar effect on task performance but different on temporal organization.
We also observe a more stable behavior in the case where the social norm is activated, and an effect that seems globally less artificial than in the case where we set an explicit objective to the user.
This first study also highlights the need for the norm to be salient, or activated.
In a second study, we focus on what characterizes the salience of the normative message.
In the first study, we used two different types of information: the normative message and concrete cues of desirable behavior.
This second study aims to distinguish these two types of information and test their respective effect.
The results show that the normative message seems to have a slightly greater effect on performance but also more artificial on user behavior.
In a third study, we are interested in the characteristics of the message, assuming that a better perceived message could support the salience of the norm it carries.
As part of a collaboration with artificial intelligence researchers, we tested different types of framing to assess their respective effect on the perception of the argument to which they applied.
The mixed results essentially show that the argumentative style (rational and factual rather than emotional or moral) seems to have a significant weight on the perception of the argument.
In addition, the theme addressed by the argument seems to play a significant role and should therefore be given special attention for the development of similar interventions.
At the application level, our results first highlight the relevance of the use of injunctive social norms in a context of persuasive technology.
They also show that social standard messages must be carefully crafted, taking into account multiple factors.
On the theoretical level, we show that a social norm can have an effect comparable to that of an explicitly fixed objective, but that both generate the setting up of different cognitive processes.
Eventually, methodologically, we apply the analysis of traces of activity to the field of social influence, which, to our knowledge, had not yet been put in place.
The use of drugs is often necessary during pregnancy despite a lack of knowledge of their adverse effects on pregnancy or the fetus.
The identification of adverse reactions to marketed drugs using statistical signal detection tools classically relies on the use of large spontaneous reporting databases in which pregnancy status is unfrequent.
Health claims databases are increasingly used in pharmacoepidemiology studies, including those of pregnant women.
They are therefore a potential resource for automated pharmacovigilance.
In France, the National Health Data System covers almost the entire French population and the thesis focuses on the use of its permanent sample, the Permanent Beneficiaries Sample (EGB).
The first axis describes the prescription of drugs for pregnant women and more particularly teratogenic or fetotoxic drugs and supplementations recommended during pregnancy.
The results of this work are that pregnant women are dispensed a lot of medications.
Known risk drugs are rarely prescribed during pregnancy and recommended supplementation is increased during the study period.
Pregnant women on low incomes have more drug prescribed but fewer supplementations.
The second axis focuses on the development of a signal detection methodology based on the use of the propensity score or the prognostic score in high dimension.
This methodology is applied to identify drugs possibly associated with an increased risk of prematurity.
The results show that the use of the p-RD derived from the prognostic score makes it possible to better take into account confusion, and to limit indication and protopathic bias.
For the third axis, the p-RD is applied to twenty specific or non-specific pregnancy pathologies and combined with an automated query of MeSH keywords from MEDLINE articles.
Automated query allows easy annotation of known adverse drug reactions.
In conclusion, this thesis shows the value of medico-administrative data for analyzing the evolution of drug prescription during pregnancy and for the detection of pharmacovigilance signals in pregnancy.
Gradually, digital technologies are becoming more important in research on sociocultural phenomena.
Equipment projects are developing in all the social sciences and the humanities (SSH) and movements advocating an instrumental revolution are multiplying.
This thesis proposes to question the advent of a digitally equipped research in the SSH on the basis of a general reflection on the links between science, technology and writing.
What are the epistemological and political issues that underlie these digital instrumentation logics as they institute new writing techniques at the heart of research practices?
The thesis is composed of three main parts.
The first part questions the fundamental relationships between technical instruments and scientific knowledge.
It is also about estimating the specificities of a communication approach to scientific instrumentation.
Based on a theory of digital writing, and on a techno-semiotic analysis approach, the thir part questions the forms and powers of the digital instrumentation.
On a morphological level, what do the design and implementation of such instruments consist of?
On a political level, what are the normative effects of these "dispositifs" on the epistemology of the disciplines that seize them?
The success of the Open Access movement during the last decade shows the relevance of this model for the research community.
The new practices in scientific publishing aim to offer free access to information about research and to foster the dissemination of scientific knowledge.
Furthermore, Open Science advocates for the free accessibility of not only research publications, but also their dataset, methods and results in order to enhance the reproducibility of scientific research, facilitate collaboration between researchers and accelerate innovation.
Nowadays, scientific research benefits from the information society and Big Data, through the exploitation of big datasets, which are integral part of the current tools for the generation of new knowledge.
This thesis aims to analyse and to process scientific articles in order to extract new meta-data regarding the datasets and the research results related to these datasets.
We will study the challenges facing Open Science and the different phenomena related to scientific papers and their data, in order to propose a typology for datasets in the form of an ontology.
We will propose an approach for the automatic identification of textual segments referring to datasets in scientific articles.
The new metadata, obtained from these automatic analyses of scientific corpora, will be aggregated in the form of Open Data in order to propose new tools for the exploitation and analysis of the scientific output of a given domain.
Recommender systems play a leading role in user's choice guidance.
The search of accuracy in such systems is generally done through an optimization of a function between the items and the users.
It has been proved that maximizing only the accuracy does not produce the most useful recommendation for the users.
This can confine individuals inside the bubble of their own choices.
Additionally, it tends to emphasize the agglomaration of the users' behavior on few popular items.
Thus, it produces a lack of diversity and novelty in recommendations and a limited coverage of the platform catalog.
This non-discovery is even more crucial if the platform wants to be fair in its recommendations with all contents' producers (e.g, music artists, writers, video game developers or videographers).
The non diversity, and novelty problem is more important for the users because it has been shown that human mind appreciates when moved outside of its comfort zone.
Our two models are based on a user profile understanding prior to bring diversification.
They capture the diversity in the user profile and respond to thisdiversity by looking to create a diverse list of recommendation without loosing to much accuracy.
The first model is mainly built upon a clustering approach, while the second model is based on an wavelet function.
This wavelet function in our model helps us delimit an area where the user will find item slightly different from what he liked in the past.
Our proposals are tested on a common experimental design that consider well-known datasets and state-of-the-art algorithm.
The results of our experiments show that our approaches indeed bring diversity and novelty and are also competitive against state-of-the-art method.
We also propose a user-experiment to validate our model based on the wavelet.
The results of user centered experiments conclude that this model corresponds with human cognitive and perceptual behavior.
At the beginning the concept of a parallel corpus is defined.
French and Czech texts forming the parallel Fratchèque corpus come from literature; only texts after the year 1945 have been selected.
Fratchèque is not marked up explicitly by XML tags because the tagging is not necessary for the proper functioning of the corpus manager ParaConc.
The building-up of the corpus is thoroughly described following all steps and settings of the software used.
The process starts with the optical character recognition program FineReader and, after checking the accuracy of numerical texts by using MS Word 2002, it goes on building up a corpus managed by ParaConc.
The linguistic investigations of the thesis rely primarily on the realization of a parallel corpus.
The main purpose is to tackle a phenomenon that is known in Czech as částice but has no direct equivalent in French.
The most frequent terms used in the French approach are mots du discours and particules énonciatives.
The existing descriptions suggest a close relationship between these words and the discourse.
It is demonstrated on two Czech částice - přece, vždyt̕ and their variants - using huge Czech corpora (Analysis A) and Fratchèque (Analysis B).
The study continues analysing systematically all kind of usage of vždyt̕, přece in order to present lexicographical description for a bilingual Czech-French dictionary.
Finally, some issues concerning automatic evaluation of translation quality are discussed taking into account the work with částice
The research work presented in this thesis concerns the development of unsupervised learning approaches adapted to large relational and dynamic data-sets.
The combination of these three characteristics (size, complexity and evolution) is a major challenge in the field of data mining and few satisfactory solutions exist at the moment, despite the obvious needs of companies.
This is a real challenge, because the approaches adapted to relational data have a quadratic complexity, unsuited to the analysis of dynamic data.
We propose here two complementary approaches for the analysis of this type of data.
The second proposes to use support points among the objects in order to build a representation space to define representative prototypes of the clusters.
Finally, we apply the proposed approaches to real-time profiling of connected users.
Computer based teaching which grew out of automated teaching has been one of the central issues of the past twenty years.
The increasing use of computers in education has opened up a new series of possibilities for both the teacher and the learner of english.
In order to provide the learner with original exercises any development of computer tools must take into account the latest possibilities offered by information technology such as direct access, simulation and interactivity.
An analysis taking into account on the one hand the processing of speech by machine and on the other hand the salient features of aural comprehension will allow us to construct a structure for the teaching and the learning of the latter competence which will be based on quality, quantity, strategy and communication.
For each of these categories interactive multimedia computer based teaching offers a number of advantages.
It is particularly at this last level that a wider range of innovative activities most of which only possible using information technology can now be designed.
The aim of this study is propose a typology of predicates of motion in Hungarian.
The typology reflects a simple objective perception of motion and space.
Our classification is based on semantic properties such as directionality, mood, destination, goal, place and the aspectual properties.
These semantic properties are completed by morpho-syntactic properties needed for natural language processing.
The contrastive component of our study has made it possible to propose a better description of the classes of predicates in Hungarian and to bring out the morpho-syntactic and combinatory differences specific to both languages in the expression of motion, such as the role of verb prefixes, locative complements, and to underline the importance of noun predicates.
In this thesis, we propose a collaborative model driven methodology for designing Autonomic Cognitive IoT systems to deal with IoT design complexity.
We defined within this methodology a set of autonomic cognitive design patterns that aim at (1) delineating the dynamic coordination of the autonomic processes to deal with the system's context changeability and requirements evolution at run-time, and (2) adding cognitive abilities to IoT systems to understand big data and generate new insights.
To address challenges related to big data and scalability, we propose a generic semantic big data platform that aims at integrating heterogeneous distributed data sources deployed on the cloud and generating knowledge that will be exposed as a service (Knowledge as a Service--KaaS).
As an application of the proposed contributions, we instantiated and combined a set of patterns for the development of prescriptive cognitive system for the patient treatment management.
Thus, we elaborated two ontological models describing the wearable devices and the patient context as well as the medical knowledge for decision-making.
The proposed system is evaluated from the clinical prescriptive through collaborating with medical experts, and from the performance perspective through deploying the system within the KaaS following different configurations
Agents having to take a collective decision are often motivated by individual goals.
In such scenarios, two key aspects need to be addressed. The first is defining how to select a winning alternative from the expressions of the agents. The second is making sure that agents will not manipulate the outcome.
This dissertation studies the aggregation and the strategic component of multi-agent collective decisions where the agents use a compactly represented language.
The languages we study are all related to logic: from propositional logic, to generalized CP-nets and linear temporal logic (LTL).
Our main contribution is the introduction of the framework of goal-based voting, where agents submit individual goals expressed as formulas of propositional logic.
Classical aggregation functions from voting, judgment aggregation, and belief merging are adapted to this setting and studied axiomatically and computationally.
Desirable axiomatic properties known in the literature of social choice theory are generalized to this new type of propositional input, as well as the standard complexity problems aimed at determining the result.
Another important contribution is the study of the aggregation of generalized CP-nets coming from multiple agents, i.e., CP-nets where the precondition of the preference statement is a propositional formula.
We use different aggregators to obtain a collective ordering of the possible outcomes.
Thanks to this thesis, two lines of research are thus bridged: the one on the aggregation of complete CP-nets, and the one on the generalization of CP-nets to incomplete preconditions.
The focus is on three majoritarian voting rules which are found to be manipulable.
Therefore, we study restrictions on both the language of the goals and on the strategies allowed to the agents to discover islands of strategy-proofness.
We also present a game-theoretic extension of a recent model of opinion diffusion over networks of influence.
In the influence games defined here, agents hold goals expressed as formulas of LTL and they can choose whether to use their influence power to make sure that their goal is satisfied.
Classical solution concepts such as weak dominance and winning strategy are studied for influence games, in relation to the structure of the network and the goals of the agents.
Finally, we introduce a novel class of concurrent game structures (CGS) in which agents can have shared control over a set of propositional variables.
Such structures are used for the interpretation of formulas of alternating-time temporal logic, thanks to which we can express the existence of a winning strategy for an agent in a repeated game (as, for instance, the influence games mentioned above).
The main result shows by means of a clever construction that a CGS with shared control can be represented as a CGS with exclusive control.
In conclusion, this thesis provides a valuable contribution to the field of collective decision-making by introducing a novel framework of voting based on individual propositional goals, it studies for the first time the aggregation of generalized CP-nets, it extends a framework of opinion diffusion by modelling rational agents who use their influence power as they see fit, and it provides a reduction of shared to exclusive control in CGS for the interpretation of logics of strategic reasoning.
By using different logical languages, agents can thus express their goals and preferences over the decision to be taken, and desirable properties of the decision process can be ensured.
The increasing utilization of sensor devices in addition to human-given data make it possible to capture real world systems complexity through rich temporal descriptions.
One core characteristic of such configurations is heterogeneity that appears at different levels of the data generation process: data sources, time models and data models.
In such context, one challenging task for monitoring systems is to discover non-trivial temporal knowledge that is directly actionable and suitable for human interpretation.
In this thesis, we firstly propose to use a Temporal Abstraction (TA) approach to express information given by heterogeneous raw data streams with a unified interval-based representation, called state streams.
Such approach solves problems introduced by heterogeneity, provides a high level pattern vocabulary and also permits also to integrate expert(s) knowledge into the discovery process.
Second, we introduced the Complex Temporal Dependencies (CTD) that is a quantitative interval-based pattern model.
Third, we proposed CTD-Miner a first efficient CTD mining framework.
CTD-Miner performs an incremental dependency construction.
This research focuses on the lexicological description of pragmatemes.
They are non-free compositional phrasems constrained by the context of communication in which they are used.
In this work we adopt a contrastive French-Spanish approach.
The most common expressions of everyday life involve many constraints of which we are not aware.
Greeting someone with a Hello!, or finishing a letter by Sincerely, Regards, does not represent any difficulty for a native speaker.
These utterances, which appear very simple in terms of their content, their form and the contexts of ordinary life in which they are used, are very singular.
They are ritually used in everyday situations to which they are prototypically associated.
Pragmatemes often go unnoticed in the language, as phraseological units, and it is during the translation that we realize that they can not be translated literally into another language.
We must find an equivalent expression.
There is a link between pragmatemes and culture.
Within a linguistic community,the speakers understand each other because they share a linguistic and cultural competence.
However, in communicating in a foreign language, we must consider the cultural elements that condition the situation in which the exchange takes place.
Vertical search engines, which focus on a specific segment of the Web, become more and more present in the Internet landscape.
Topical search engines, notably, can obtain a significant performance boost by limiting their index on a specific topic.
In this thesis, we tackle the first inevitable step of a all topical search engine: focused document gathering from the Web.
A thorough study of the state of art leads us to consider two strategies to gather topical documents from the Web: either relying on an existing search engine index (focused search) or directly crawling the Web (focused crawling).The first part of our research has been dedicated to focused search.
In this context, a standard approach consists in combining domain-specific terms into queries, submitting those queries to a search engine and downloading top ranked documents.
After empirically evaluating this approach over 340 topics, we propose to enhance it in two different ways: Upstream of the search engine, we aim at formulating more relevant queries in order to increase the precision of the top retrieved documents.
To do so, we define a metric based on a co-occurrence graph and a random walk algorithm, which aims at predicting the topical relevance of a query.
Downstream of the search engine, we filter the retrieved documents in order to improve the document collection quality.
We do so by modeling our gathering process as a tripartite graph and applying a random walk with restart algorithm so as to simultaneously order by relevance the documents and terms appearing in our corpus.
Then, we consider the problem of crawl frontier ordering, which is at the very heart of a focused crawler.
Such ordering strategy allows the crawler to prioritize its fetches, maximizing the number of in-domain documents retrieved while minimizing the non relevant ones.
We propose to apply learning to rank algorithms to efficiently order the crawl frontier, and define a method to learn a ranking function from existing crawls.
The last years have known an unprecedented growth in the use of mobile devices especially smartphones.
They became omnipresent in our daily life because of the features they offer.
They allow the user to install third-party apps to achieve numerous tasks.
Smartphones are mostly governed by the Android operating system.
Mobile apps collect a huge amount of data such as email addresses, contact list, geolocation, photos and bank account credentials.
Consequently, Android has become a favorable target for cyber criminals.
Thus, understanding the issue, i.e., how Android malware operates and how to detect it, became an important research challenge.
Android malware frequently tries to bypass static analysis using multiple techniques such as code obfuscation and dynamic code loading.
To overcome these limitations, many analysis techniques have been proposed to execute the app and monitor its behavior at runtime.
Nevertheless, malware developers use time and logic bombs to prevent the malicious code from executing except under certain circumstances.
Recent approaches try to automatically characterize the malicious behavior by identifying the most suspicious locations in the code and forcing them to execute.
These approaches solely analyze the application code and miss the execution paths that occur when the application calls a framework method that in turn calls another application method.
It also gives key information about the analyzed application in order to understand how the suspicious code was injected into the application.
To validate our approach, we use GPFinder to study a collection of 14,224 malware samples, and we evaluate that 72.69% of the samples have at least one suspicious code location which is only reachable through implicit calls.
Triggering approaches mainly use one of the following strategies to run a specific portion of the application's code: the first approach heavily modifies the app to launch the targeted code without keeping the original behavioral context.
The second approach generates the input to force the execution flow to take the desired path without modifying the app's code.
However, it is sometimes hard to launch a specific code location just by fuzzing the input.
We propose in this dissertation a tool, TriggerDroid, that has a twofold goal: force the execution of the suspicious code and keep its context close to the original one.
It crafts the required framework events to launch the right app component and satisfies the necessary triggering conditions to take the desired execution path.
To validate our approach, we led an experiment on a dataset of 135 malware samples from 71 different families.
Results show that our approach needs more refinement and adaptation to handle special cases due to the highly diverse malware dataset that we analyzed.
Finally, we give a feedback on the experiments we led on different malware datasets, and we explain our experimental process.
Finally, we present the Kharon dataset, a collection of well documented Android malware that can be used to understand the malware landscape.
The objective of this thesis is to develop semantic methods of reassembly in the complicated framework of heritage collections, where some blocks are eroded or missing.
The reassembly of archaeological remains is an important task for heritage sciences: it allows to improve the understanding and conservation of ancient vestiges and artifacts.
However, some sets of fragments cannot be reassembled with techniques using contour information or visual continuities.
It is then necessary to extract semantic information from the fragments and to interpret them.
These tasks can be performed automatically thanks to deep learning techniques coupled with a solver, i.e., a constrained decision making algorithm.
This thesis proposes two semantic reassembly methods for 2D fragments with erosion and a new dataset and evaluation metrics.
The first method, Deepzzle, proposes a neural network followed by a solver.
The neural network is composed of two Siamese convolutional networks trained to predict the relative position of two fragments: it is a 9-class classification.
The solver uses Dijkstra's algorithm to maximize the joint probability.
Deepzzle can address the case of missing and supernumerary fragments, is capable of processing about 15 fragments per puzzle, and has a performance that is 25% better than the state of the art.
The second method, Alphazzle, is based on AlphaZero and single-player Monte Carlo Tree Search (MCTS).
It is an iterative method that uses deep reinforcement learning: at each step, a fragment is placed on the current reassembly.
Two neural networks guide MCTS: an action predictor, which uses the fragment and the current reassembly to propose a strategy, and an evaluator, which is trained to predict the quality of the future result from the current reassembly.
Alphazzle takes into account the relationships between all fragments and adapts to puzzles larger than those solved by Deepzzle.
Moreover, Alphazzle is compatible with constraints imposed by a heritage framework: at the end of reassembly, MCTS does not access the reward, unlike AlphaZero.
Indeed, the reward, which indicates if a puzzle is well solved or not, can only be estimated by the algorithm, because only a conservator can be sure of the quality of a reassembly.
Surface realisation is a subtask of natural language generation.
It may be viewed as the inverse of parsing, that is, given a grammar and a representation of meaning, the surface realiser produces a natural language string that is associated by the grammar to the input meaning.
This thesis presents three extensions to GenI, a realisation algorithm for Feature-Based Tree Adjoining Grammar (FB-LTAG).
The first extension improves the efficiency of the realiser with respect to lexical ambiguity.
It is an adaptation from parsing of the “electrostatic tagging” optimisation, in which lexical items are associated with a set of polarities, and combinations of those items with non-neutral polarities are filtered out.
The second extension deals with the number of outputs returned by the realiser.
Normally, the GenI algorithm returns all of the sentences associated with the input logical form.
Whilst these inputs can be seen as having the same core meaning, they often convey subtle distinctions in emphasis or style.
The extension builds off the fact that the FB-LTAG grammar used by the generator was constructed from a “metagrammar”, explicitly putting to use the linguistic generalisations that are encoded within.
The final extension provides a means for the realiser to act as a metagrammar-debugging environment.
Mistakes in the metagrammar can have widespread consequences for the grammar.
Since the realiser can output all strings associated with a semantic input, it can be used to find out what these mistakes are, and crucially, their precise location in the metagrammar.
Our contributions cover three different applications but share a common denominator: the extraction of relevant user representations.
Our first application is the item recommendation task, where recommender systems build user and item profiles out of past ratings reflecting user preferences and item characteristics.
Nowadays, textual information is often together with ratings available and we propose to use it to enrich the profiles extracted from the ratings.
Our hope is to extract from the textual content shared opinions and preferences.
The models we propose provide another opportunity: predicting the text a user would write on an item.
Our second application is sentiment analysis and, in particular, polarity classification.
Our idea is that recommender systems can be used for such a task.
Recommender systems and traditional polarity classifiers operate on different time scales.
We propose two hybridizations of these models: the former has better classification performance, the latter highlights a vocabulary of surprise in the texts of the reviews.
The third and final application we consider is urban mobility.
It takes place beyond the frontiers of the Internet, in the physical world.
Using authentication logs of the subway users, logging the time and station at which users take the subway, we show that it is possible to extract robust temporal profiles.
Word Sense Disambiguation (WSD), which is a central task in natural language processing and that can improve applications such as machine translation or information extraction.
Researches in word sense disambiguation predominantly concern the English language, because the majority of other languages lacks a standard lexical reference for the annotation of corpora, and also lacks sense annotated corpora for the evaluation, and more importantly for the construction of word sense disambiguation systems.
In English, the lexical database wordnet is a long-standing de-facto standard used in most sense annotated corpora and in most WSD evaluation campaigns.
Our contribution to this thesis focuses on several areas:first of all, we present a method for the automatic creation of sense annotated corpora for any language, by taking advantage of the large amount of wordnet sense annotated English corpora, and by using a machine translation system.
This method is applied on Arabic and is evaluated, to our knowledge, on the only Arabic manually sense annotated corpus with wordnet: the Arabic OntoNotes 5.0, which we have semi-automatically enriched.
Its evaluation is performed thanks to an implementation of two supervised word sense disambiguation systems that are trained on the corpora produced using our method.
We hence propose a solid baseline for the evaluation of future Arabic word sense disambiguation systems, in addition to sense annotated Arabic corpora that we provide as a freely available resource.
Secondly, we propose an in vivo evaluation of our Arabic word sense disambiguation system by measuring its contribution to the performance of the machine translation task.
The detection of block structures in matrices is an important challenge.
First in data analysis where matrices are a key tool for data representation, as data tables or adjacency matrices.
Indeed, for the first one, finding a co-clustering is equivalent to finding a row and column block structure of the matrix.
For the second one, finding a structure of diagonal dominant blocks leads to a clustering of the data.
In this dissertation, we focus our analysis on the detection of dominant diagonal block structures by symmetrically permuting the rows and columns of matrices.
Lots of algorithms have been designed that aim to highlight such structures.
The first one consists of algorithms that first project the matrix rows onto a low-dimensional space generated by the matrix leading eigenvectors, and then apply a procedure such as a k-means on the reduced data.
Their main drawbacks is that the knowledge of number of clusters to uncover is required.
The second kind consists of iterative procedures that look for the k-th best partition into two subblocks of the matrix at step k.
However, if the matrix structure shows more than two blocks, the best partition into two blocks may be a poor fit to the matrix groundtruth structure.
Hence, we propose a spectral algorithm that deals with both issues described above.
To that end, we preprocess the matrix with a doubly-stochastic scaling, which leverages the blocks.
First we show the benefits of using such a scaling by using it as a preprocessing for the Louvain's algorithm, in order to uncover community structures in networks.
We also investigate several global modularity measures designed for quantifying the consistency of a block structure.
We generalise them to make them able to handle doubly-stochastic matrices, and thus we remark that our scaling tends to unify these measures.
Then, we describe our algorithm that is based on spectral elements of the scaled matrix.
Our method is built on the principle that leading singular vectors of a doubly-stochastic matrix should have a staircase pattern when their coordinates are sorted in the increasing order, under the condition that the matrix shows a hidden block structure.
Tools from signal processing-that have been initially designed to detect jumps in signals-are applied to the sorted vectors in order to detect steps in these vectors, and thus to find the separations between the blocks.
However, these tools are not specifically designed to this purpose.
Hence procedures that we have implemented to answer the encountered issues are also described.
We then propose three applications for the matrices block structure detection.
For these applications, we compare the results of our algorithm with those of algorithms that have been designed on purpose.
Finally, we deal with the dialogue act detection in a discorsre, using the STAC database that consists in a chat of online players of " The Settlers of Catan ".
To that end we connect classical clustering algorithms with a BiLSTM neural network taht preprocesses the dialogue unities.
Finally, we conclude by giving some preliminary remarks about the extension of our method to rectangular matrices.
This dissertation explores the topic of generative modelling of natural images,which is the task of fitting a data generating distribution.
Such models can be used to generate artificial data resembling the true data, or to compress images.
Latent variable models, which are at the core of our contributions, seek to capture the main factors of variations of an image into a variable that can be manipulated.
Unfortunately these models struggle to capture all the modes of the original distribution, ie they do not cover the full variability of the dataset.
Conversely, likelihood based models such as VAEs typically cover the full variety of the data well and provide an objective measure of coverage.
However these models produce samples of inferior visual quality that are more easily distinguished from real ones.
The work presented in this thesis strives for the best of both worlds: to obtain compelling samples while modelling the full support of the distribution.
We propose a training procedure relying on an auxiliary loss function to control what information is captured by the latent variables and what information is left to an autoregressive decoder.
Unlike previous approaches to such hybrid models, ours does not need to restrict the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables.
The second contribution builds on the standard GAN model, which trains a discriminator network to provide feedback to a generative network.
The discriminator usually assesses the quality of individual samples, which makes it hard to evaluate the variability of the data.
Instead we propose to feed the discriminator with emph{batches} that mix both true and fake samples, and train it to predict the ratio of true samples in the batch.
In our third contribution, we show that usual parametric assumptions made in VAEs induce a conflict between them, leading to lackluster performance of hybrid models.
We propose a solution based on deep invertible transformations, that trains a feature space in which usual assumptions can be made without harm.
Our approach provides likelihood computations in image space while being able to take advantage of adversarial training.
It obtains GAN-like samples that are competitive with fully adversarial models while improving likelihood scores over existing hybrid models at the time of publication, which is a significant advancement.
This thesis focuses on acoustic model structuring for improving HMM-Based automatic speech recognition.
The structuring relies on unsupervised clustering of speech utterances of the training data in order to handle speaker and channel variability.
The idea is to split the data into acoustically similar classes.
In conventional multi-Modeling (or class-Based) approach, separate class-Dependent models are built via adaptation of a speaker-Independent model.
When the number of classes increases, less data becomes available for the estimation of the class-Based models, and the parameters are less reliable.
One way to handle such problem is to modify the classification criterion applied on the training data, allowing a given utterance to belong to more than one class.
This is obtained by relaxing the classification decision through a soft margin.
This is investigated in the first part of the thesis.
In the main part of the thesis, a novel approach is proposed that uses the clustered data more efficiently in a class-Structured GMM.
Instead of adapting all HMM-GMM parameters separately for each class of data, the class information is explicitly introduced into the GMM structure by associating a given density component with a given class.
To efficiently exploit such structured HMM-GMM, two different approaches are proposed.
The first approach combines class-Structured GMM with class-Dependent mixture weights.
In this model the Gaussian components are shared across speaker classes, but they are class-Structured, and the mixture weights are class-Dependent.
For decoding an utterance, the set of mixture weights is selected according to the estimated class.
In the second approach, the mixture weights are replaced by density component transition probabilities.
The approaches proposed in the thesis are analyzed and evaluated on various speech data, which cover different types of variability sources (age, gender, accent and noise)
Documentary production in a professional context often involves a revising process in which documents need to be proofread before validation and publication.
This important task faces new challenges when dealing with digital documents.
As an advanced digital writing technology, XML publishing chains are a relevant framework for studying proofreading of digital documents.
Part of the contribution presented here has led to the development of prototypes that have been experimented in the use of Scenari publishing chains in a pedagogical context.
These prototypes rely on linear proofreading views allowing in particular the comparison between two versions of the document based on a diff algorithm.
This thesis focuses on two Natural Language Processing tasks that require to extract semantic information from raw texts: Sentiment Analysis and Text Summarization.
Accordingly, this dissertation is composed of two parts: the first part (Neural Sentiment Analysis) deals with the computational study of people's opinions, sentiments, and the second part (Neural Text Summarization) tries to extract salient information from a complex sentence and rewrites it in a human-readable form.
Neural Sentiment Analysis.
Similar to computer vision, numerous deep convolutional neural networks have been adapted to sentiment analysis and text classification tasks.
However, unlike the image domain, these studies are carried on different input data types and on different datasets, which makes it hard to know if a deep network is truly needed.
We thus propose a new adaptation of the deepest convolutional architecture (DenseNet) for text classification and study the importance of depth in convolutional models with different atom-levels (word or character) of input.
Besides, to further improve sentiment classifiers and contextualize them, we propose to model them jointly with dialog acts, which are a factor of explanation and correlate with sentiments but are nevertheless often ignored.
We have manually annotated both dialogues and sentiments on a Twitter-like social medium, and train a multi-task hierarchical recurrent network on joint sentiment and dialog act recognition.
We show that transfer learning may be efficiently achieved between both tasks, and further analyze some specific correlations between sentiments and dialogues on social media.
Neural Text Summarization.
Detecting sentiments and opinions from large digital documents does not always enable users of such systems to take informed decisions, as other important semantic information is missing.
People also need the main arguments and supporting reasons from the source documents to truly understand and interpret the document.
To capture such information, we aim at making the neural text summarization models more explainable.
We propose a model that has better explainability properties and is flexible enough to support various shallow syntactic parsing modules.
More specifically, we linearize the syntactic tree into the form of overlapping text segments, which are then selected with reinforcement learning (RL) and regenerated into a compressed form.
Hence, the proposed model is able to handle both extractive and abstractive summarization.
We thus provide a detailed comparison of both RL-based and syntax-aware approaches and of their combination along several dimensions that relate to the perceived quality of the generated summaries such as number of repetitions, sentence length, distribution of part-of-speech tags, relevance and grammaticality.
We show that when there is a resource constraint (computation and memory), it is wise to only train models with RL and without any syntactic information, as they provide nearly as good results as syntax-aware models with less parameters and faster training convergence.
Since progress in automatic natural language analysis has made it possible to support fully finalized dialogue in a fully automatic style (e.g. travel reservation [1]), the systems are able to answer to questions about complex tasks and more open areas (eg IBM's WATSON system that won the jeopardy game show in 2011 [2]).
Recently, standalone chat agents (known as chatbots) have become essential elements in customer relationship management [3].
In this context, the conversational agent function evolves more and more towards that of a virtual adviser from whom we expect an increasingly intelligent behavior going beyond the collection of information or providing simple answers, but also being able to make decisions such as taking actions and in particular recognizing one's own limitations and knowing when to hand over to a human advisor in order to preserve the quality of service.
Technological lock
A large number of chatbots exist on the market, from simple ones often elaborated with weakly contextual rules, to more complex based on machine learning.
The former: quickly become silent if one goes beyond the limits of their field of competence, while the latter are capable of a little more generalization, but this is usually paid at the price of greater inaccuracy in their answers.
Regardless of the type of system considered, everything is based on knowledge, that is, the memory of the conversational agent.
In humans, memory is divided into several types [4] for example: episodic (lived events), semantics (knowledge), procedural (know-how), perceptive (recognition of voices, smells, etc.) or work (short-term notepad).
It can be of explicit nature (conscious access and restitution), implicit (subconscious, emotions) or autobiographical (personal combining semantic and episodic memory).
But these characteristics of the memory of an autonomous agent condition the representations that he constructs [5] and therefore its actions.
In addition to the conversational agent's memory functions and their impact on the management of the dialogue, the scientific investigations will address the problem of adaptation to the application domain1 [8] and will have to address recent advances in machine learning (deep neural approaches[6]), this at different levels of granularity (analysis of a statement, acquisition of background knowledge from corpus etc.).
The validation of the intermediate experimental results will be carried out through the evaluation of demonstrators (prototype dialogue system) according to a corpus-based quantitative evaluation procedure with a set of performance measures defined from the classical evaluation protocols. for the domain [7].
Statistical learning aims to modelize a functional link between two variables X and Y thanks to a random sample of realizations of the couple (X,Y).
When the variable Y takes a binary number of values, learning is named classification and learn the functional link is equivalent to learn the boundary of a manifold in the feature space of the variable X.
In this PhD thesis, we are placed in the context of active learning, i.e. we suppose that learning sample is not random and that we can, thanks to an oracle, generate points for learning the manifold.
In the case where the variable Y is continue (regression), previous works show that criterion of low discrepacy to generate learning points is adequat.
We show that, surprisingly, this result cannot be transfered to classification talks.
In this PhD thesis, we propose the criterion of dispersion for classification problems.
This criterion being difficult to realize, we propose a new algorithm to generate low dispersion samples in the unit cube.
After a first approximation of the manifold, successive approximations can be realized in order to refine its knowledge.
Two methods of sampling are possible: the «selective sampling» which selects points to present to the oracle in a finite set of candidate points, and the «adaptative sampling» which allows to select any point in the feature space of the variable X.
The second sampling can be viewed as the infinite limit of the first.
Nevertheless, in practice, it is not reasonable to use this method.
Then, we propose a new algorithm, based on dispersion criterion, leading both exploration and exploitation to approximate a manifold.
From XVth to XVIIth century, Portugal has been ranking first among the most advanced nations of its time.
But any contact with peoples and cultures has always been a source of various and multifaceted reciprocal influences.
In this thesis, we will study the lusitanian impressions in the Guinea Gulf.
The research has been conducted in the southern part of Ivory Coast, Ghana, Togo and Benin and is based on a corpus made up of some hundreds of words that we have listed in a bibliography and an investigation that we have carried out in the field during eight years.
The analysis of data is done according to a bipolar method which combines history and structuralism in its contrastive approach because, in reality, we are comparing two linguistic systems: portuguese, kru and kwa languages of the Niger-Congo family.
The entropy of a probability distribution on a set of discrete random variables is always bounded by the entropy of its factorisable counterpart.
This is due to the submodularity of entropy on the set of discrete random variables.
Submodular functions are also generalisation of matroid rank function; therefore, linear functions may be optimised on the associated polytopes exactly using a greedy algorithm.
In this manuscript, we exploit these links between the structures of graphical models and submodular functions: we use greedy algorithms to optimise linear functions on the polytopes related to graphic and hypergraphic matroids for learning the structures of graphical models, while we use inference algorithms on graphs to optimise submodular functions.
The first main contribution of the thesis aims at approximating a probabilistic distribution with a factorisable tractable distribution under the maximum likelihood framework.
Since the tractability of exact inference is exponential in the treewidth of the decomposable graph, our goal is to learn bounded treewidth decomposable graphs, which is known to be NP-hard.
We pose this as a combinatorial optimisation problem and provide convex relaxations based on graphic and hypergraphic matroids.
This leads to an approximate solution with good empirical performance.
As third contribution, we propose and analyse algorithms aiming at minimizing submodular functions that can be written as sum of simple functions.
Our algorithms only make use of submodular function minimisation and total variation oracles of simple functions.
In this thesis, we approach these questions by defining and implementing a multi-scale model for music segment structure description, called Polytopic Graph of Latent Relations (PGLR).
In our work, a segment is the macroscopic constituent of the global piece.
Under the PGLR scheme, relationships between musical elements within a musical segment are assumed to be developing predominantly between homologous elements within the metrical grid at different scales simultaneously.
This approach generalises to the multi-scale case the System&amp;Contrast framework which aims at describing, as a 2×2 square matrix, the logical system of expectation within a segment and the surprise resulting from that expectation.
Each vertex in the polytope corresponds to a low-scale musical element, each edge represents a relationship between two vertices and each face forms an elementary system of relationships.
The aim of the PGLR model is to both describe the time dependencies between the elements of a segment and model the logical expectation and surprise that can be built on the observation and perception of the similarities and differences between elements with strong relationships.
The approach is presented conceptually and algorithmically, together with an extensive evaluation of the ability of different models to predict unseen data, measured using the cross-perplexity value.
Our results illustrate the efficiency of the proposed model in capturing structural information within such data.
Cultural heritage is the legacy of physical artefacts and intangible attributes of a group or society that is inherited from past generations.
Vases are among the most iconic objects of cultural heritage.
Although some of these collections have been digitised, they are rarely accessible in an open format and remain isolated.
In addition, the lack of clearly identified terminologies is an obstacle to communication and knowledge sharing.
Our work aims to respond to this issue by implementing practices drawn from the semantic web and knowledge engineering, and more particularly by building in a W3C format an ontology dedicated to the Chinese vases of the Ming and Qing dynasties.
The construction of the TAO CI ("ceramic" in Chinese) ontology respects the experts'way of thinking in their conceptualization of the field, and takes into account the international standards in Terminology (ISO 1087 and ISO 704).
Both approaches are based on the notion of essential characteristic and define a concept as a unique combination of characteristics.
The search for differences between objects, combined with a morphological analysis of Chinese terms whose characters carry meaning in relation to knowledge of the field, allows to identify essential characteristics.
The definition of concept is based on the idea that a concept is a set of essential characteristics stable enough to be named in language.
We have thus proposed a specific method for building ontologies guided by the terms and essential characteristics of the domain.
We have introduced new terms (neologisms) in English and concepts without any designation in language for ontology structuring purposes.
The construction of the ontology was done using Protégé, the most widely used environment for building ontologies in the W3C format (RDF/OWL).
The terminological dimension was reduced, as is often the case, to annotations (in SKOS, RDFS) on the concepts.
The TAO CI ontology is linked to external resources such as CIDOC CRM and ATT Getty for the conceptual part, and to museums for the objects.
Finally, the TAO CI ontology was evaluated from the point of view of the domain (coverage) and its implementation.
The ontology is in open access at the following address: http://www.dh.ketrc.com/otcontainer/data/OTContainer.owlThe last phase of the project consisted in the creation of a dedicated website.
This site provides access to the different resources of the project and in particular to a bilingual (English, Chinese) electronic dictionary of the vases of the Ming and Qing dynasties.
The dictionary entries correspond to the OWL classes of the ontology: http://www.dh.ketrc.com/The TAO CI ontology is, to our knowledge, the first open and reusable ontology in the format of the semantic web of Chinese ceramic vases.
It is an illustration of an approach guided by terms and essential characteristics that can be applied to the construction of ontologies in other areas of Chinese cultural heritage.
Emotion recognition is one of the most complex scientific domains.
In the last few years, various emotion recognition systems are developed.
We focus on facial emotion recognition specially the six basic emotions namely happiness, anger, fear, disgust, sadness and surprise.
A comparative study between geometric method and appearance method is performed on CK+ database as the posed emotion database, and FEEDTUM database as the spontaneous emotion database.
We consider different constraints in this study such as different image resolutions, the low number of labelled images in learning step and new subjects.
We evaluate afterward various fusion schemes on new subjects, not included in the training set.
Good recognition rate is obtained for posed emotions (more than 86%), however it is still low for spontaneous emotions.
These ones increase spontaneous emotions recognition rates.
Analysis of textual data is facilitated by the use of text mining (TM) allowing to automate content analysis, and is implemented in several application in healthcare.
These include the use of TM to explore the content of posts shared online.
We performed a systematique literature review to identify the application of TM in psychiatry.
In addition, we used TM to explore users' concerns of an online forum dedicated to antidepressants and anxiolytics between 2013 and 2015 analysing words frequency, cooccurences, topic models (LDA) and popularity of topics.
The four TM applications in psychiatry retrieved are the analysis of patients'narratives (psychopathology), feelings expressed online, content of medical records, and biomedical literature screening.
Four topics are identified on the forum: withdrawals (most frequent), escitalopram, anxiety related to treatment effect and secondary effects.
While concerns around secondary effects of treatment declined, questions about withdrawals effects and changing medication increased related to several antidepressants.
The goal of this thesis dissertation is to show that, contrary to preconceived ideas, one can efficiently take advantage of low frequency words in natural language processing.
We use them in sub-sentential alignment, which constitutes the first step of most data-driven machine translation systems (statistical or example-based machine translation).
We show that rare words can be used as a foundation in the design of a multilingual sub-sentential alignment method, using differential techniques similar to those found in example-based machine translation.
This method is truly multilingual, in that sense, it allows the simultaneous processing of any number of languages.
Moreover, it is very simple, anytime, and scales up naturally.
We compare our implementation, Anymalign, to two statistical tools proven in the domain.
Although its current results are in average slightly behind those of state of the art methods in phrase-based statistical machine translation, we show that the intrinsic quality of our lexicons is actually superior to that of lexicons produced by state of the art methods.
In the study of the use of referring expressions, numerous studies have looked into the relationship between linguistic forms and informationally defined functions, and have put forth the early sensitivity of young children to certain aspects of information structuring, notably with regard to attentional status of referents and the topic-comment dimension.
Interactional linguistics have adopted a complementary approach on referring expressions and have shown how speakers signal and accomplish, by their choice of a referring expression, not only reference, but also various tasks pertaining to the management of interaction proper.
Our study aims to shed further light, in a multidimensional perspective, on the use of referring expressions, and especially on the contrast between week, strong and dislocated forms, by studying two languages which exploit different linguistic means to mark sentence topic.
Our analysis is based on a cross-sectional database of 12 French- and German-speaking children, aged 2 to 3 years.
We analyzed the forms and uses of referring expressions in the children's and the adult's speech, by considering morph-syntactic, informational and interactional factors.
Results show the complementary nature of these different factors when explaining referring expression use in children and in adults.
We were also able to put forth specific uses of certain linguistic means, such as French dislocations, German demonstrative der/die/das and null forms, and we described their functioning within interactional routines that aid the young children to join in on discourse and interaction.
This thesis presents Adetoa, a system designed to automatically locate temporal expressions in Web pages and tag them with semantic annotations, in the field of e-tourism.
A detailed linguistic study has revealed that the expression of temporal information in Web tourism pages is complex and has specific properties.
These analyses have led to the development of a large number of transducers (under Unitex) for the extraction and mark-up tasks.
Other tourist information is also extracted, such as tourist objects and addresses.
Linking transducers have been developed to group all the information concerning one tourist destination.
The annotation scheme is based on a tourism ontology but is not a direct replica, thus enabling the expressions to be accurately characterized on a linguistic level.
The ontology has then been adapted accordingly, so that the information can more easily be included in the corresponding knowledge base.
The evaluation of Adetoa, which is detailed in the last chapter, showed satisfying results, both on a theoretical level and for industrial purposes.
Brain-Computer Interface (BCI) allows communication between a user and a machine, by converting the user's brain activity into commands that control external devices.
Many limitations prevent the diffusion of BCI systems in real applications, such as the calibration phase that is a consequence of the issue of variability across sessions and among users.
The calibration phase is fundamental because it allows to set the main parameters to extract the relevant information from the electroencephalograpy (EEG) signal of the subject, but it is considered time consuming and tedious for the user.
The objective of this thesis is to overcome these limitations by novel methods based on the improvement or even replacement of the traditional calibration phase, proposing the development of a user-centered BCI system.
Firstly, we present a design to develop an adaptive BCI system for two different applications.
The former deals with a code-modulated Visual Evoked Potential (c-VEP) speller where an adaptive parameter setting phase is proposed to replace the standard calibration phase.
The latter application concerns the development of a Mental Imagery (MI) BCI for a disabled user, characterized by a long user-centered multi-stage training phase, in the context of a international BCI competition.
The proposed methods showed promising results and open new perspectives to the diffusion of BCI
Clinical data are produced as part of the practice of medicine by different health professionals, in several places and in various formats.
They therefore present an heterogeneity both in terms of their nature and structure and are furthermore of a particularly large volume, which make them considered as Big Data.
The work carried out in this thesis aims at proposing an effective information retrieval method within the context of this type of complex and massive data.
First, the access to clinical data constrained by the need to model clinical information.
This can be done within Electronic Health Records and, in a larger extent, within data Warehouses.
In this thesis, I proposed a proof of concept of a search engine allowing the access to the information contained in the Semantic Health Data Warehouse of the Rouen University Hospital.
In order to provide search functionalities adapted to this generic representation of data, a query language allowing access to clinical information through the various entities of which it is composed has been developed and implemented as a part of this thesis's work.
Second, the massiveness of clinical data is also a major technical challenge that hinders the implementation of an efficient information retrieval.
The initial implementation of the proof of concept highlighted the limits of a relational database management systems when used in the context of clinical data.
A migration to a NoSQL key-value store has been then completed.
Finally, the contribution of this work within the general context of the Semantic Health Data Warehouse of the Rouen University Hospital was evaluated.
The proof of concept proposed in this work was used to access semantic descriptions of information in order to meet the criteria for including and excluding patients in clinical studies.
In this evaluation, a total or partial response is given to 72.97% of the criteria.
In addition, the genericity of the tool has also made it possible to use it in other contexts such as documentary and bibliographic information retrieval in health.
In natural language processing, each analysis step has improved the way in which language can be modeled by machines.
Another step of analysis still poorly mastered resides in semantic parsing.
This type of analysis can provide information which would allow for many advances, such as better human-machine interactions or more reliable translations.
There exist several types of meaning representation structures, such as PropBank, AMR and FrameNet.
FrameNet corresponds to the frame semantic framework whose theory has been described by Charles Fillmore (1971).
In this theory, each prototypical situation and each different elements involved are represented in such a way that two similar situations are represented by the same object, called a semantic frame.
The work that we will describe here follows the work already developed for machine prediction of frame semantic representations.
We will present four prediction systems, and each one of them allowed to validate another hypothesis on the necessary properties for effective prediction.
We will show that semantic parsing can also be improved by providing prediction models with refined information as input of the system, with firstly a syntactic analysis where deep links are made explicit and secondly vectorial representations of the vocabulary learned beforehand.
Requirements analysis is the first step of the design process.
It is also an important source of innovation in companies, particularly when it is shared and fulfilled by the multidisciplinary design team.
The prospection and anticipation of future needs are therefore an important challenge for the development of new products adapted to their user.
The goal of this thesis is to optimize the anticipation of future needs in order to foster innovation.
Our assumptions include methodological and technological factors to improve the collaboration of the multidisciplinary team and the performance of requirements anticipation.
These assumptions are operationalized through three different ways of running the Persona method and tested in the context of three industrial projects.
We show that a method combining several reasoning modes adapted to the various professional backgrounds in the multidisciplinary team,the anticipation of needs is improved quantitatively and qualitatively (i.e. usefulness assessed by users).
We also show that the technological support plays an important role in the effectiveness of methods: a collaborative and playful technology such as an interactive tabletop can increase the number of strategic ideas for the company (i.e. useful and technically feasible); an immersive and playful technology such as a virtual world can shape needs anticipation inaccordance with the project priorities (Techno-Centered or User-Centered).
These results open many opportunities for the methodological and technological evolution of front-end innovation phases towards the anticipation of future needs.
This thesis focuses on speaker voice transformation in the aim to indicate the distance of it: a spoken-to-whispered voice transformation to indicate a close distance and a spoken-to-shouted voice transformation for a rather far distance.
We perform at first, in-depth analysis to determine most relevant features in whispered voices and especially in shouted voices (much harder).
The main contribution of this part is to show the relevance of prosodic parameters in the perception of vocal effort in a shouted voice.
Then, we propose some descriptors to better characterize the prosodic contours.
For the actual transformation, we propose several new transformation rules which importantly control the quality of transformed voice.
The results showed a very good quality of transformed whispered voices and transformed shouted voices for relatively simple linguistic structures (CVC, CVCV, etc.).
Decision-making is a highly researched field in science, be it in neuroscience to understand the processes underlying animal decision-making, or in robotics to model efficient and rapid decision-making processes in real environments.
In neuroscience, this problem is resolved online with sequential decision-making models based on reinforcement learning.
In robotics, the primary objective is efficiency, in order to be deployed in real environments.
However, in robotics what can be called the budget and which concerns the limitations inherent to the hardware, such as computation times, limited actions available to the robot or the lifetime of the robot battery, are often not taken into account at the present time.
We propose in this thesis to introduce the notion of budget as an explicit constraint in the robotic learning processes applied to a localization task by implementing a model based on work developed in statistical learning that processes data under explicit constraints, limiting the input of data or imposing a more explicit time constraint.
In this context, the alternation between information retrieval for location and the decision to move for a robot may be indirectly linked to the notion of exploration-exploitation compromise.
Social conventions are learned mostly at a young age, but are quite different from other domains, like for example sensorimotor skills.
The first people to define conventions just picked an arbitrary alternative between several options: a side of the road to drive on, the design of an electric plug, or inventing a new word.
Because of this, while setting a new convention in a population of interacting individuals, many competing options can arise, and lead to a situation of growing complexity if many parallel inventions happen.
How do we deal with this issue?Humans often exhert an active control on their learning situation, by for example selecting activities that are neither too complex nor too simple.
This behavior, in cases like sensorimotor learning, has been shown to help learn faster, better, and with fewer examples.
Could such mechanisms also have an impact on the negotiation of social conventions?
A particular example of social convention is the lexicon: which words we associated with given meanings.
Computational models of language emergence, called the Language Games, showed that it is possible for a population of agents to build a common language through only pairwise interactions.
In particular, the Naming Game model focuses on the formation of the lexicon mapping words and meanings, and shows a typical burst of complexity before starting to discard options and find a final consensus.
Several strategies were introduced, and have a different impact on both the time needed to converge to a consensus and the amount of memory needed by individual agents.
Firstly, we artificially constrain the memory of agents to avoid the local complexity burst.
A few strategies are presented, some of which can have similar convergence speed as in the standard case.
Secondly, we formalize what agents need to optimize, based on a representation of the average state of the population.
A couple of strategies inspired by this notion help keep the memory usage low without having constraints, but also result in a faster convergence process.
We then show that the obtained dynamics are close to an optimal behavior, expressed analytically as a lower bound to convergence time.
Eventually, we designed an online user experiment to collect data on how humans would behave in the same model, which shows that they do have an active topic choice policy, and do not choose randomly.
Contributions from this thesis also include a classification of the existing Naming Game models and an open-source framework to simulate them.
In this context, we aimed at enhancing the quality of the interaction between users and Embodied Conversational Agents ECAs by (1) endowing the ECA with the capacity to express social attitudes, such as being friendly or dominant depending its role or relationship with its interaction partners; (2) adapting the agent's behavior according to the user's behavior, hence, the conversation partners influence each others through an interaction loop, thus, enhancing the interaction quality; (3) predicting the user's engagement level and adapting the agent's behavior accordingly.
We take advantage of the recent advances in machine learning, more specifically, temporal sequence mining and neural networks to model these capacities in the ECA.
The first model is used to learn relevant patterns (sequences) of non-verbal signals that best represent attitude variations, and then reproduce them on the agent.
The latter is used to encompass the dynamics of non-verbal signals.
Two use cases have been explored using the well-known LSTM model: agent's behavior adaptation based on both agent's and user's behavior history, and user's engagement prediction based on his/her own behavior history.
The implemented models and algorithms have been validated through a number of perceptive studies as well as through rigorous quantitative analysis of the obtained results.
In addition, the realized models have been integrated into a virtual-agent platform.
In recent years, the enterprise applications interoperability has become the leitmotiv of developers and designers in systems engineering.
Most approaches to interoperability in the company have the primary objective of adjustment and adaptation of types and data structures necessary for the implementation of collaboration between companies.
In the field of manufacturing, the product is a central component.
Scientific works propose solutions taking into account information systems derived from products technical data throughout their life cycle.
But this information is often uncorrelated.
The management of product data (PDM) is commonly implemented to manage all information concerning products throughout their life cycle.
However, these models are generally independent “islands” ignoring the problem of interoperability between applications that support these models.
The objective of this thesis is to study the problem of interoperability applied to applications used in the manufacturing environment and to define a model of the ontological knowledge of enterprises related to the products they manufacture, based on technical data, ensuring the interoperability of enterprise systems.
The outcome of this research concerns the formalization of a methodology for identifying a product-centric information system in the form of an ontology, for the interoperability of applications in manufacturing companies, based on existing standard such as ISO 10303 and IEC 62264.
Randomized controlled trials and systematic reviews are essential for the practice of evidence-based medicine (EBM).
EBM will be compromised if the evidence issued from biomedical research proves to be biased.
Many authors previously denounced the poor quality of research, but in 2009 I.Chalmers and P.Glasziou integrate these criticisms into a more global concept, waste of research.
It could be defined as research that fails to help patients and their clinicians to make informed decisions as without accessible, honest and usable reports, research cannot help research end users.
They estimated that it could represent up to 85% of health research.
In this thesis, we focused on the avoidable waste of research in clinical trials.
We had a particular interest in waste of research due to poor trial planning (trials methods or choice of outcomes) or to selective and incomplete reporting of outcomes.
Then we estimated to what extent this waste could have been avoided by simple and inexpensive methodological adjustments.
Our results suggest that 1) simple and inexpensive methodological adjustments could have limited the risk of bias in 50% of clinical trials, and thus partially reduce the burden of waste of research, and 2) that many trials did not measure or report completely the important outcomes, but that this waste could have been partially avoided for the majority of the trials.
On the other hand, interest in languages defined over large and infinite alphabets has increased in recent years.
Although many theories and properties generalize well from the finite case, learning such languages is not an easy task.
As the existing methods for learning regular languages depends on the size of the alphabet, a straightforward generalization in this context is not possible.
In this thesis, we present a generic algorithmic scheme that can be used for learning languages defined over large or infinite alphabets, such as bounded subsets of N or R or Boolean vectors of high dimensions.
We restrict ourselves to the class of languages accepted by deterministic symbolic automata that use predicates to label transitions, forming a finite partition of the alphabet for every state.
Our learning algorithm, an adaptation of Angluin's L*, combines standard automaton learning by state characterization, with the learning of the static predicates that define the alphabet partitions.
We use the online learning scheme, where two types of queries provide the necessary information about the target language.
The first type, membership queries, answer whether a given word belongs or not to the target.
We study language learning over large or infinite alphabets within a general framework but our aim is to provide solutions for particular concrete instances.
For this, we focus on the two main aspects of the problem.
Initially, we assume that equivalence queries always provide a counter-example which is minimal in the length-lexicographic order when the conjecture automaton is incorrect.
Then, we drop this ``strong''equivalence oracle and replace it by a more realistic assumption, where equivalence is approximated by testing queries, which use sampling on the set of words.
Such queries are not guaranteed to find counter-examples and certainly not minimal ones.
All proposed algorithms have been implemented and their performance, as a function of automaton and alphabet size, has been empirically evaluated.
In many context of statistics it is common to rely on (generalized) linear model to perform prediction or variable selection.
Nevertheless, it is often realistic to assume that second or even third order interactions of variables might help and improve the power of the decision taken.
The same need for interactions is something occurring frequently in the context of online advertising, and especially in real time bidding/ Click Through Rate (CTR).
Indeed, due to real time prediction constraints, despite many samples can be collected, only few variables are obtained by the players in this field, eg OS type, browser type, time, etc.
Moreover, the signal to recover is weak, with only around 0.1% of click through rate (CTR) in the best cases.
Hence, it might be beneficial to incorporate interactions to work with such drastic constraints.
Last but not least, for categorical data, such interaction can model hierarchical refinements of the categories: this is for instance of high interest when dealing with natural language processing applications.
Hence, proposing efficient methods for dealing with such scenarios might have a huge impact in various applications of machine learning.
In the high dimension scenario we are aiming at, Lasso methods for interactions was proposed by Radchenko James(2010) and a variant designed to handle hierarchical interactions was studied by Bien Taylor Tibshirani (2013).
So far we believe that such methods remained under used due to their computational limits.
We believe that adapting the recent developments proposed by the last two teams mentioned, along with the refined active sets strategies investigated by Massias Gramfort Salmon(2018) could lead to implementations order of magnitude faster than the solutions currently available.
On top of the intrinsic interest of such a normalization process, the interaction model we plan to address could be an interesting prototype for understanding batch normalization
A socially assistive robot (SAR) is meant to engage people into situated interaction such as monitoring physical exercise, neuropsychological rehabilitation or cognitive training.
While the interactive behavioral policies of such systems are mainly hand-scripted, we discuss here key features of the training of multimodal interactive behaviors in the framework of the SOMBRERO project.
In our work, we used learning by demonstration in order to provide the robot with adequate skills for performing collaborative tasks in human centered environments.
There are three main steps of learning interaction by demonstration: we should (1) collect representative interactive behaviors from human coaches; (2) build comprehensive models of these overt behaviors while taking into account a priori knowledge (task and user model, etc.); and then (3) provide the target robot with appropriate gesture controllers to execute the desired behaviors.
Multimodal HRI (Human-Robot Interaction) models are mostly inspired by Human-Human interaction (HHI) behaviors.
Transferring HHI behaviors to HRI models faces several issues: (1) adapting the human behaviors to the robot's interactive capabilities with regards to its physical limitations and impoverished perception, action and reasoning capabilities; (2) the drastic changes of human partner behaviors in front of robots or virtual agents; (3) the modeling of joint interactive behaviors; (4) the validation of the robotic behaviors by human partners until they are perceived as adequate and meaningful.
In this thesis, we study and make progress over those four challenges.
In particular, we solve the two first issues (transfer from HHI to HRI) by adapting the scenario and using immersive teleoperation.
We also build and evaluate a proof-of-concept autonomous robot to perform the tasks.
This thesis addresses problems of security in the French grid operated by RTE, the French ``Transmission System Operator''(TSO).
Progress in sustainable energy, electricity market efficiency, or novel consumption patterns push TSO's to operate the grid closer to its security limits.
To this end, it is essential to make the grid ``smarter''.
To tackle this issue, this work explores the benefits of artificial neural networks.
We propose novel deep learning algorithms and architectures to assist the decisions of human operators (TSO dispatchers) that we called “guided dropout”.
This allows the predictions on power flows following of a grid willful or accidental modification.
This is tackled by separating the different inputs: continuous data (productions and consumptions) are introduced in a standard way, via a neural network input layer while discrete data (grid topologies) are encoded directly in the neural network architecture.
This architecture is dynamically modified based on the power grid topology by switching on or off the activation of hidden units.
The main advantage of this technique lies in its ability to predict the flows even for previously unseen grid topologies.
The "guided dropout" achieves a high accuracy (up to 99% of precision for flow predictions) with a 300 times speedup compared to physical grid simulators based on Kirchoff's laws even for unseen contingencies, without detailed knowledge of the grid structure.
We also showed that guided dropout can be used to rank contingencies that might occur in the order of severity.
In this application, we demonstrated that our algorithm obtains the same risk as currently implemented policies while requiring only 2% of today's computational budget.
The ranking remains relevant even handling grid cases never seen before, and can be used to have an overall estimation of the global security of the power grid.
The importance of cultural heritage documentation increases in parallel with the risks to which it is exposed, such as wars, uncontrolled urban development, natural disasters, neglect and inappropriate conservation techniques or strategies.
In addition, this documentation is a fundamental tool for the assessment, the conservation, and the management of cultural heritage.
Consequently, this tool allows us to estimate the historical, scientific, social and economic value of this heritage.
According to several international institutions dedicated to the preservation of cultural heritage, there is an urgent need to develop computer solutions to facilitate and support the documentation of poorly documented cultural heritage especially in developing countries where there is a lack of resources.
Among these countries, Palestine represents a relevant case study in this issue of lack of documentation of its heritage.
To address this issue, we propose an approach of knowledge acquisition and extraction in the context of poorly documented heritage.
We take as a case study the church of the Nativity in Palestine and we put in place our theoretical approach by the development of a platform for the acquisition and extraction of heritage knowledge.
Our solution is based on the semantic technologies, which gives us the possibility, from the beginning, to provide a rich ontological description, a better structuring of the information, a high level of interoperability and a better automatic processing without additional efforts.
Therefore, the interaction between the two components of our system as well as the heritage knowledge develop and improve over time especially that our system uses manual contributions and validations of the automatic results (in both components) by the experts to optimize its performance.
Graph theory has long been studied in mathematics and probability as a tool for describing dependence between nodes.
However, only recently it has been implemented on data, giving birth to the statistical analysis of real networks.
The topology of economic and financial networks is remarkably complex: it is generally unobserved, thus requiring adequate inferential procedures for it estimation, moreover not only the nodes, but the structure of dependence itself evolves over time.
Statistical and econometric tools for modelling the dynamics of change of the network structure are lacking, despite their increasing requirement in several fields of research.
At the same time, with the beginning of the era of “Big data” the size of available datasets is becoming increasingly high and their internal structure is growing in complexity, hampering traditional inferential processes in multiple cases.
This thesis aims at contributing to this newborn field of literature which joins probability, economics, physics and sociology by proposing novel statistical and econometric methodologies for the study of the temporal evolution of network structures of medium-high dimension.
We show that musically-trained children and young professional musicians outperform controls in a series of experiments, with faster brain plasticity and stronger functional connectivity, as measured by electroencephalography.
By contrast, advantages for old adult musicians are less clear-cut, suggesting a limited impact of music training to counteract cognitive decline.
Finally, young musicians show better long-term memory for novel words, which possibly contributes, along with better auditory perception and attention, to their advantage in word learning.
By showing transfer effects from music training to semantic processing and long-term memory, results reveal the importance of domain-general cognitive functions and open new perspectives for education and rehabilitation.
The concept of automata, central to language theory, is the natural and efficient tool to apprehendvarious practical problems.
The intensive use of finite automata in an algorithmic framework is illustrated by numerous researchworks.
The correctness and the evaluation of performance are the two fundamental issues of algorithmics.
A classic method to evaluate an algorithm is based on the controlled random generation of inputs.
The work described in this thesis lies within this context and more specifically in the field of theuniform random generation of finite automata.
This design builds on the symbolic method.
Theoretical results and an experimental study are given.
A random generator of non deterministic automata then illustrates the flexibility of the Markov ChainMonte Carlo methods (MCMC) as well as the implementation of the Metropolis-Hastings algorithm tosample up to isomorphism.
A result about the mixing time in the general framework is given.
The MCMC sampling methods raise the problem of the mixing time in the chain.
By drawing on worksalready completed to design a random generator of partially ordered automata, this work shows howvarious statistical tools can form a basis to address this issue.
Recent computing trends have been advocating for more distributed paradigms, namely Fog computing, which extends the capacities of the Cloud at the edge of the network, that is close to end devices and end users in the physical world.
The Fog is a key enabler of the Internet of Things (IoT) applications as it resolves some of the needs that the Cloud fails to provide such as low network latencies, privacy, QoS, and geographical requirements.
For this reason, the Fog has become increasingly popular and finds application in many fields such as smart homes and cities, agriculture, healthcare, transportation, etc.
The Fog, however, is unstable because it is constituted of billions of heterogeneous devices in a dynamic ecosystem.
IoT devices may regularly fail because of bulk production and cheap design.
When failures occur in such an ecosystem, the resulting inconsistencies in the application affect the physical world by inducing hazardous and costly situations.
In this Thesis, we propose an end-to-end autonomic failure management approach for IoT applications deployed in the Fog.
The approach manages IoT applications and is composed of four functional steps: (i) state saving, (ii) monitoring, (iii) failure notification,and (iv) recovery.
Each step is a collection of similar roles and is implemented, taking into account the specificities of the ecosystem (e.g., heterogeneity, resource limitations).
State saving aims at saving data concerning the state of the managed application.
These include runtime parameters and the data in the volatile memory, as well as messages exchanged and functions executed by the application.
Monitoring aims at observing and reporting information on the lifecycle of the application.
When a failure is detected, failure notificationsare propagated to the part of the application which is affected by that failure.
The propagation of failure notifications aims at limiting the impact of the failure and providinga partial service.
In order to recover from a failure, the application is reconfigured and thedata saved during the state saving step are used to restore a cyber-physical consistent state of the application.
Cyber-physical consistency aims at maintaining a consistent behaviour of the application with respect to the physical world, as well as avoiding dangerous and costly circumstances.
The approach was validated using model checking techniques to verify important correctness properties.
It was then implemented as a framework called F3ARIoT.
This framework was evaluated on a smart home application.
The results showed the feasibility of deploying F3ARIoT on real Fog-IoT applications as well as its good performances in regards to end user experience.
In the professional aeronautical field (one of the safest in the world), human error management must be improved to reach a better safety level.
However, due to the complexity of socio technical systems, the implementation of an efficient user centred design process could be challenging.
To ease this process, our study aims to develop and validate specific tools, particularly for processing large amounts of textual data.
Secondly, we will confront the control condition with results obtained automatically.
The identification of a methodology to extract risk situations that could be included in specific studies.
This step is very important for the user centred design process.
Links that we have established between our results and incident/accident studies allow us to consider positive impacts on aviation safety.
Estimating human pose and recognizing human activities are important steps in many applications, such as human computer interfaces (HCI), health care, smart conferencing, robotics, security surveillance etc.
Despite the ongoing effort in the domain, these tasks remained unsolved in unconstrained and non cooperative environments in particular.
As a second problem, we tackled the human pose estimation task in particular settings where multiple visual sensors are available and allowed to collaborate.
In the first part, we focused on indoor action recognition from videos and we consider complex activities.
To this end, we explored several methodologies and eventually introduced a 3D spatio-temporal representation for a video sequence that is viewpoint independent.
A 3D feature descriptor was employed afterwards to build a codebook and classify the actions with the bag-of-words approach.
As for the second part, we concentrated on articulated pose estimation, which is often an intermediate step for activity recognition.
Our motivation was to incorporate information from multiple sources and views and fuse them early in the pipeline to overcome the problem of self-occlusion, and eventually obtain robust estimations.
In addition to the single-view appearance of the human body and its kinematic priors, we demonstrated that geometrical constraints and appearance-consistency parameters are effective for boosting the coherence between the viewpoints in a multi-view setting.
Both methods that we proposed was evaluated on public benchmarks and showed that the use of view-independent representations and integrating information from multiple viewpoints improves the performance of action recognition and pose estimation tasks, respectively.
The extracted named entities and their textual contexts will be categorized using Natural Language Processing approaches in order to account for the semantics of the relationships expressed in the texts.
The precise description of documents through encoding formats such as XML-EAD, Dublin Core or XML-TEI thus facilitates the design of finding aids, data mining tasks but also sharing online or within a community of researchers.
An ontology will be proposed to meet these needs and allow us to formalize this issue from the point of view of the Semantic Web.
This project will be based on the use and extension of already existing ontologies such as DBPedia or ISA Programme Person Core Vocabulary, whose interest in tasks of disambiguation of named entities has already been widely demonstrated.
The digitized documents will be processed for Optical Character Recognition (OCR) and finally encoded in XML-TEI format.
The data produced will be recorded following the RDF standard, thus ensuring interoperability with all services belonging to OpenLinkedData, in order to produce text inferences based on SPARQL.
The methodology for populating the ontology will be developed specifically for the task of extracting and categorizing named entities and spatio-temporal data, in order to be able to automatically extract the relationships between these entities in documents.
My thesis concerns a study of continuous word representations applied to the automatic detection of speech recognition errors.
Our study focuses on the use of a neural approach to improve ASR errors detection, using word embeddings.
The exploitation of continuous word representations is motivated by the fact that ASR error detection consists on locating the possible linguistic or acoustic incongruities in automatic transcriptions.
The aim is therefore to find the appropriate word representation which makes it possible to capture pertinent information in order to be able to detect these anomalies.
Our contribution in this thesis concerns several initiatives.
First, we start with a preliminary study in which we propose a neural architecture able to integrate different types of features, including word embeddings.
Second, we propose a deep study of continuous word representations.
This study focuses on the evaluation of different types of linguistic word embeddings and their combination in order to take advantage of their complementarities.
On the other hand, it focuses on acoustic word embeddings.
Then, we present a study on the analysis of classification errors, with the aim of perceiving the errors that are difficult to detect.
Finally, we exploit the linguistic and acoustic embeddings as well as the information provided by our ASR error detection system in several downstream applications.
This thesis is a study of Wolof verbal constructions in a typological perspective.
Based on available descriptions of Wolof verbal conjugation, I first provide a summary of the system of verbal predication in the light of the typological literature.
The typological analysis of these periphrastic constructions provides us with the empirical basis to propose a new approach to the notion of “auxiliary”.
I argue that auxiliaries should not be cross-linguistically defined as items belonging to a specific lexical class or as items on a grammaticalisation path but rather as autonomous predicative elements with a specific function.
In addition, I propose a constructional analysis of the organisation of the verbal predication system of Wolof.
The entirety of Wolof verbal constructions is not assumed to form an unstructured set of independent entities, but it is instead taken to constitute a highly structured system (a network of constructions).
Furthermore, some apparent idiosyncrasies in the conjugation paradigm of Wolof can be explained from a diachronic point of view.
Finally, I provide a comparative analysis of verbal constructions in Atlantic languages in order to determine which elements of the Wolof conjugation are inherited from Proto-Atlantic.
This thesis proposes a practical model making it possible to implement a case-based reasoning system that adapts processes represented as natural language text in response to user queries.
While the cases and the solutions are in textual form, the adaptation itself is performed on networks of temporal constraints expressed with a qualitative algebra, using a belief revision operator.
Natural language processing methods are used to acquire case representations and to regenerate text based on the adaptation result
This research work aims to propose a creative support system according to a multi-agent architecture in order to manage the knowledge needed and produced during a creative workshop.
This work contributes to the scientific research with regard to various aspects.
Beforehand designing any system, a review of the current creative supports systems is carried out in order to determine their limits concerning the creative process and collaboration mode.
To fix these limits, a knowledge engineering approach is adopted.
Based on this organizational modeling of a creative workshop, the organization of computational agents that would contribute to manage knowledge is deduced.
Based on the same modeling, creative workshop ontology is created in order to provide a representation of the environment and shared knowledge to agents.
Multi-agent architecture for creative support system permits to explore new knowledge processing approach notably for idea evaluation.
An idea evaluation methodology based on multi-criteria analysis methods is suggested.
In addition to this methodology, automatic idea processing based on naturel language processing is experimented in order to assist evaluators.
Identifying satisfaction triggers among customers is a crucial need in today's business world, as a strong customer relationship is now a most vital asset.
The domain of opinion mining, in which this thesis falls into, offers several methods to answer this need.
These methods, however, require a continuous update of specialized resources which are the cornerstone of many opinion mining tools.
The objective of this work is to develop acquisition and structuration strategies for these resources, which can be lexicons, morphosyntactic rules or annotated data.
We then compare the benefits of each type of resource through a benchmark of several opinion mining methods, and conclude that the best performing strategy is a hybrid approach.
Finally, we present results for resource acquisition methods that answer not only the needs of opinion mining but also the constraints from the industrial setting in which this work has been conducted.
Digital content is increasingly produced nowadays in a variety of media such as news and social network sites, personal Web sites, blogs etc.
In particular, a large and dynamic part of such content is related to media-worthy events, whether of general interest (e.g., the war in Syria) or of specialized interest to a sub-community of users (e.g., sport events or genetically modified organisms).
While such content is primarily meant for the human users (readers), interest is growing in its automatic analysis, understanding and exploitation.
Within the ANR project ContentCheck, we are interested in developing textual and semantic tools for analyzing content shared through digital media.
The proposed PhD project takes place within this contract, and will be developed based on the interactions with our partner from Le Monde.
Information and relation extraction from a text which may comprise a statement to be fact-checked, with a particular focus on capturing the time dimension ; a sample statement is for instance « VAT on iron in France was the highest in Europe in 2015 ».
Building structured queries from extracted information and relations, to be evaluated against reference databases used as trusted information against which facts can be checked.
This thesis deals with sequence matching techniques, applied to word spotting (locating keywords in document images without interpreting the content).
Several sequence matching techniques exist in the literature but very few of them have been evaluated in the context of word spotting.
This thesis begins by a comparative study of these methods for word spotting on several datasets of historical images.
Thus, FSM is able to skip outliers from target sequence, which can be present at the beginning, at the end or in the middle of the target sequence.
Moreover it can perform one-to-one, one-to-many and many-to-one correspondences between query and target sequence without considering noisy elements in the target sequence.
We then also extend these characteristics to the query sequence by defining a new algorithm (ESC: Examplary Sequence Cardinality).
Finally, we propose an alternative word matching technique by using an inexact chain codes (shape code), describing the words.
This work investigates practical methods to ease training and improve performances of neural language models with large vocabularies.
The main limitation of neural language models is their expensive computational cost: it depends on the size of the vocabulary, with which it grows linearly.
Despite several training tricks, the most straightforward way to limit computation time is to limit the vocabulary size, which is not a satisfactory solution for numerous tasks.
Most of the existing methods used to train large-vocabulary language models revolve around avoiding the computation of the partition function, ensuring that output scores are normalized into a probability distribution.
Here, we focus on sampling-based approaches, including importance sampling and noise contrastive estimation.
These methods allow an approximate computation of the partition function.
After examining the mechanism of self-normalization in noise-contrastive estimation, we first propose to improve its efficiency with solutions that are adapted to the inner workings of the method and experimentally show that they considerably ease training.
Finally, we aim at improving performances on full vocabulary language models, by augmenting output words representation with subwords.
Many urban cities in Southeast Asia witness severe flooding associated to increasing rainfall intensity and rapid urbanization often due to poor urban planning.
Two important inputs required in flood hazard assessment are: (1) high accuracy Digital Elevation Model (DEM), and (2) long rainfall record.
High accuracy DEM is both expensive and time consuming to acquire.
Long rainfall records for areas of interest are often not available or not sufficiently long to determine the probable extremes.
This thesis presents a notably cost-effective and efficient approach to derive high accuracy DEM, and suggests proxies for long rainfall data.
DEM data from a publicly accessible satellite, Shuttle Radar Topography Mission (SRTM), and Sentinel 2 multispectral imagery are selected and used to train the Artificial Neural Network (ANN) to improve the quality of the DEM.
In the training of ANN, high quality observed DEM is the key leading to a well-trained ANN.
The trained ANN will then be ready to efficiently and effectively generate high quality DEM, at low cost, for places where DEM data is not available.
The DEM resulting from the latest version of improved SRTM (iSRTM_v2 DEM) shows (1) significantly better than the original SRTM DEM, a 34 % to 57 % RMSE reduction; (2) the visual clarity is so much clearer as well; and (3) much closer drainage network with the actual.
The much improved DEM allows flood modelling to proceed with high confidence.
Rainfall data resulting from a high spatial resolution Regional Climate Model (RCM), Weather Research and Forecasting driven by ERA-Interim (WRF/ERAI) dataset, is extracted, analyzed, and compared its accuracy with high quality observed rainfall data of Singapore.
The comparisons are performed, among others, on their Intensity-Duration-Frequency (IDF) curves, the essential design curves for flood risk assessment; they matched quite well.
MIKE 21 Flow Model Flexible Mesh (MIKE 21 FM) is applied to Greater Jakarta, with input data from the above mentioned much improved DEM and precipitation proxy data, for flood simulations of 2 return periods (50-and 100-years).
Finally flood maps are generated.
This demonstrates the applications of the approaches/methodologies, proposed in this thesis, on catchments where most essential data for flood risk assessment (high resolution and high accuracy DEM and long and high accuracy rainfall data) are not available.
In Natural Language Processing vectorization of words is a key that enables the use of algorithms based on mathematical models.
Recently new methods have appeared, and evaluating their quality is a necessity.
At present, evaluations are mostly effective on English, which introduces the question of multilingual evaluations.
We worked on generalizing methods, on comparing them, on devising new evaluations, and on WordNet as a multilingual resource used for evaluation.
We choose six vectorization methods: CBOW, SkipGram, GloVe, an older method as baseline, and two more recent methods.
As an indirect method, we choose semantic clustering of words for comparing the underlying vectorizations.
The chosen clustering algorithms were: the most used Kmeans, a neuronal one (SOM) and a probabilistic one (EM).Our system applies evaluation methods on big corpora in English, French and Arabic, then compares underlying vectorizations.
We propose five new evaluation methods, with four based on WordNet, and one new protocol for polling.
Our results yield three different vectorization orderings agreeing on decisive points, and invalidate some existing evaluations.
As for our own evaluations, the protocol is validated, one method is invalidated and the reason analyzed, one is validated for English and French, but not Arabic, two are validated on the three languages, and one is left for further exploration.
Advances in technology, in particular the democratization of mobile devices (PCs, smartphones and tablets), has made information accessible to anyone at any time and from anywhere while facilitating the capture of physical contextual data, thereby justifying the growing interest for pervasive computing.
The classical approach of pervasive computing has been affected by the introduction of the social dimension.
Ubiquitous systems do not meet the needs of users independently from each other but do take into account their social context.
Fostering the social dimension has given rise to a fast growing research field called Pervasive Social Computing.
Applications in this area are increasingly concerned by communities.
A community is considered in our approach as a set of distinct social entities that should be supported with services as a single user is.
In this thesis, we look into different aspects of existing centered communities applications and we identify several weaknesses and shortcomings in the notion of community, the community models, and the architecture of communities'applications.
To overcome these shortcomings, we propose three main contributions:
The ontological representation allows us to organize and represent social data, to make information searches easier for users and to infer new knowledge.
A dynamically reconfigurable architecture for fostering spontaneous communities in order to facilitate the user access to communities, information exchange between community members and service discovery.
The proposed architecture for community and service discovery have been validated through a prototype called Taldea and have been tested through several scenarios characterized by mobility and ubiquity.
The catalog takes up a special position in the supply of services of academic libraries, as a pivot for the intermediary between users and information professionals who carry the responsibility for building up collections.
For 10 years, through a serious crisis, they've been seeing their patrons preferring the general or commercial search engines.
The Web is more than a serious competitor today, ahead of the document information systems, and became the main access point for information retrieval.
Libraries are trying to structure an information space that is temporarily or permanently inhabited by users, in which the service offering is developed, but it is still presented as a series of silos, with few opportunities of navigation between them despite considerable engineering efforts and a perspective of evolution towards discovery tools.
The profession, having become aware of this deep crisis after accusing eddies caused by the breakdown of the digital switch, looking for ways to adapt and diversify its offering, streamlines the dissemination of information, and reinvents its roles, trying to take advantage of new practices of users, new expectations and new prospects.
Libraries put their hope in new data models, trying to add a level of abstraction promoting links with the world of knowledge.
The evolution towards the Semantic Web seems to be a valuable opportunity to enhance the collections and make them usable in another context, at the expense of significant efforts sized up by this analysis.
A constructivist approach based on participant observation and data collection offers a vision of the outcome within the library community on the development of catalogs and intermediation tools, and an outlook on their issues.
The term paraphrase is now commonly used to refer to textual units of equivalent meaning, down to the level of sub-sentential fragments.
Although one can envisage to manually build high-coverage lists of synonyms, enumerating meaning equivalences at the level of phrases is too daunting a task for humans.
Consequently, acquiring this type of knowledge by automatic means has attracted a lot of attention and significant research efforts have been devoted to this objective.
In this thesis we use parallel monolingual corpora for a detailed study of the task of sub-sentential paraphrase acquisition.
We argue that the scarcity of this type of resource is compensated by the fact that it is the most suited corpus type for studies on paraphrasing.
We propose a large exploration of this task with experiments on two languages with five different acquisition techniques, selected for their complementarity, their combinations, as well as four monolingual corpus types of varying comparability.
We report, under all conditions, a significant improvement over all techniques by validating candidate paraphrases using a maximum entropy classifier.
An important result of our study is the identification of difficult-to-acquire paraphrase pairs, which are classified and quantified in a bilingual typology.
Credit card fraud has emerged as major problem in the electronic payment sector.
In this thesis, we study data-driven fraud detection and address several of its intricate challenges by means of machine learning methods with the goal to identify fraudulent transactions that have been issued illegitimately on behalf of the rightful card owner.
In particular, we explore several means to leverage contextual information beyond a transaction's basic attributes on the transaction level, sequence level and user level.
On the transaction level, we aim to identify fraudulent transactions which, in terms of their attribute values, are globally distinguishable from genuine transactions.
We provide an empirical study of the influence of class imbalance and forecasting horizons on the classification performance of a random forest classifier.
We augment transactions with additional features extracted from external knowledge sources and show that external information about countries and calendar events improves classification performance most noticeably on card-not-present transaction.
On the sequence level, we aim to detect frauds that are inconspicuous in the background of all transactions but peculiar with respect to the short-term sequence they appear in.
We use a Long Short-term Memory network (LSTM) for modeling the sequential succession of transactions.
Our results suggest that LSTM-based modeling is a promising strategy for characterizing sequences of card-present transactions but it is not adequate for card-not-present transactions.
On the user level, we elaborate on feature aggregations and propose a flexible concept allowing us define numerous features by means of a simple syntax.
We provide a CUDA-based implementation for the computationally expensive extraction with a speed-up of two orders of magnitude.
Our feature selection study reveals that aggregates extracted from users'transaction sequences are more useful than those extracted from merchant sequences.
Regarding future work, we motivate the usage of simple and transparent machine learning methods for credit card fraud detection and we sketch a simple user-focused modeling approach.
The studies of diachronic allow us to study the changes and analyze the regularities in the change to understand the faculty of language.
It is a qualitative study using descriptive and comparative methodology for analyzing the corpus.
Formalization is a field of automation and simplification of language law without disturbing the language grammatical rules for the application in the field of natural language processing.
By passing the morphonology and the morphology, we are able to look clearly into the structure and the mechanism of the Malay grammar in the search for a solution for the affix-lexical combination.
This study allows us to understand clearly the internal structure of allomorph, verbal and nominal derivation from Malay verbs in helping us to formulate the model of natural language processing or as an equipment of pedagogy by a schema and the rules.
We are presenting the correlation between verbs and the system of derivation by using a study of linguistic-statistic by showing and explaining the situation of Malay linguistic.
The result of the statistics and the corpus show us the reliability of our model because there is a coherence between result and
Because of its critical impacts on performance and competitivity, organizations' knowledge is today considered to be an invaluable asset.
In this context, the development of methods and frameworks aiming at improving knowledge preservation and exploitation is of major interest.
For the purpose of highlighting this potential causal link to infer general learnings, we envisage relying on inductive reasoning techniques.
The considered work will be developed and validated through the scope of a humanitarian organization, Médecins Sans Frontières, with a focus on the logistical response in emergency situations
The LegalCluster platform proposes to define legal "clusters" in an ecosystem approach, in order to guarantee the sustainability of the information produced by legal actors.
In this context, LegalCluster seeks to provide a legal recommendation service for firms, departments and clients of legal functions, locally, in a given cluster or on a larger scale.
Moreover, several major constraints must be noted in order to deliver a relevant service that is fully integrated into the legal environment: professional secrecy, the user's legal context and the consideration of case law.
It is then necessary to propose a general solution that can be applied in a local cluster, combined with external resources and knowledge.
To do so, this approach is based on a combination of natural language processing (NLP) and information retrieval (IR).
Language processing, based on learning models, aims to categorize legal issues.
Due to the fact that we have local clusters and different legal codes to associate, our approach is fully integrated into the Topic Modeling and abstract summary generation process.
In order to integrate both the business and professional secrecy protection aspects, the model will have to be trained on a global corpus, then locally, which implies the characterization of the models in these two components while relying on implicit data such as legislative texts and jurisprudence.
The Information Retrieval section aims to facilitate the analysis of legal corpora while providing new functionalities in order to develop a dedicated legal search engine.
Thus, traditional indexing makes it possible to match keywords with the most relevant documents associated with them, but also with the metadata of the business and will make it possible to produce a synthetic "local" vision of business data.
The first step of integration is to evolve the data structure model to adapt it to metadata, but also to modify the core of the search engine to allow it to provide scoring functions adapted to the needs of LegalCluster.
The search engine will facilitate the analysis and association of these texts necessary for the Learning brick.
Modelling user preferences is crucial in many real-life problems, ranging from individual and collective decision-making to strategic interactions between agents for example.
Since agents don't come with their preferences transparently given in advance, we have only two means to determine what they are if we wish to exploit them in reasoning: we can infer them from what an agent says or from his nonlinguistic actions.
In this work, we propose a new approach to extract and reason on preferences expressed in negotiation dialogues.
After having extracted the preferences expressed in each dialogue turn, we use the discursive structure to follow their evolution as the dialogue progresses.
We use CP-nets, a model used for the representation of preferences, to formalize and reason about these extracted preferences.
The method is first evaluated on different negotiation corpora for which we obtain promising results.
We then apply the end-to-end method with principles from Game Theory to predict trades in the win-lose game The Settlers of Catan.
This work thus presents a new approach at the intersection of several research domains: Natural Language Processing (for the automatic preference extraction and the reasoning on their verbalisation), Artificial Intelligence (for the modelling and reasoning on the extracted preferences) and Game Theory (for strategic action prediction in a bargaining game)
More than 8 morphological schemas can derive event nouns from verbs in French:-age,-ment,-ion,-ure,-ance,-ade,-aison and-erie suffixations, as well as verb to noun conversion (défendre 'to defend'-&gt; défense 'defence').
These schemas have similar semantic functions and sometimes select the same verbal bases, which causes the emergence of doublets (gouverner 'to govern'-&gt; gouvernance 'governance', gouvernement 'government', gouverne 'guidance').
Conducted in the field of lexematic morphology (Matthews 1974; Anderson 1992; Aronoff 1994; Fradin 2003; Booij 2005), this contribution aims to shed light on the dynamics that support the coexistence of these rival nominalization schemas by searching for interactional constraints on multiple levels: phonological, morphological, syntactic, semantic and organizational.
These constraints are assessed using computational methods on massive web corpora (word embeddings, statistical models, analogical modeling; Arndt-Lappe 2014, Lapraye 2017, Wauquier et al. 2018, Bonami and Thuilier 2019), in order to propose a predictive model of the construction of deverbal nominalizations.
Such an enrichment which is based on Natural Language Processing and Information Retrieval technologies has several applications.
As an example, flling in the gap between a scientifc paper and a collection of highly cited papers in a domain helps the paper to be better acknowledged by the community that refers to that collection.
We present a comprehensive study over different approaches of each component.
The enrichment is performed by recommending discriminative sets of semantically relevant keywords, i.e. topics, to a user.
The topics are labeled with representative keywords and have a level of granularity that is easily interpretable.
It is also knowledge-poor and domain-independent.
To evaluate their robustness, we studied them on 10 topically diverse domains.
Our results on the keyword extraction approach showed that the statistical features are not adequate for capturing words importance within a web page.
We showed that our approach out performs a baseline approach, since the widely-used co-occurrence feature between keywords is notivenough for capturing their semantic similarity and consequently for detecting semantically consistent topics.
This thesis is part of a descriptive work in acoustic phonetics, with the aim of studying the productions of Luxembourgish vowels in native and non-native speech.
Its objective is to conciliate the variation of Luxembourgish, mainly a spoken language, composed of many regional varieties, evolving in a multilingual context, and the learning of Luxembourgish as a foreign language in the Grand-Duchy of Luxembourg.
As we assume the fact that language learning implies knowledge of sound contrast in speech, we investigate the productions of speakers whose mother tongues have different features than Luxembourgish, such as French, to see whether if the contrast are reproduced in non-native speech.
Productions of French speakers are compared to those of native speakers from the region around the capital city of the Grand-Duchy of Luxembourg, whose variety serves as a reference to the teaching of Luxembourgish as a foreign language.
The purpose of the study is the following:-to extend the descriptions on the acoustic properties of vowels produced in a regional variety of the Grand-Duchy of Luxembourg,-to highlight the specific difficulties of productions by French learners of Luxembourgish,-to interpret the results regarding the teaching of Luxembourgish as a foreign language.
Fieldwork and the creation of a corpus through recordings of 10 Luxembourg speakers and 10 French speakers are an important part of the empirical work.
We obtained a corpus of 12 hours and a half of spoken and spontaneous speech, including native speech and not native of Luxembourgish and also native speech of French.
This corpus represents a first corpus containing native and non-native speech of Luxembourgish and enables to conduct different comparative studies.
In our thesis, we did comparative analyses of the data in read speech.
The methodology we used made it possible to compare data of native and non-native speech and also data of the L1 and L2 of French speakers.
The results gave information about native and non-native productions of vowels.
These results as well as thorough descriptions of the vowels in native speech, extend knowledge not only of Luxembourgish, but also of the variety which serves as the reference for Luxembourgish as a foreign language.
In addition, they open up prospects for studying Luxembourgish by problematizing the introduction of rules for this type of education, despite the absence of language instruction in schools and the evolution of regional varieties in a concentrated geographical area.
We also show that our CNN prediction remarkably predicts the shape of the WER distribution on a collection of speech recordings.
Then, we analyze factors impacting both prediction approaches.
We also assess the impact of the training size of prediction systems as well as the robustness of systems learned with the outputs of a particular ASR system and used to predict performance on a new data collection.
Our experimental results show that both prediction approaches are robust and that the prediction task is more difficult on short speech turns as well as spontaneous speech style.
Finally, we try to understand which information is captured by our neural model and its relation with different factors.
Our experiences show that intermediate representations in the network automatically encode information on the speech style, the speaker's accent as well as the broadcast program type.
To take advantage of this analysis, we propose a multi-task system that is slightly more effective on the performance prediction task.
Nowadays, the Web is formed by two types of content which are linked: structured data of the so-called Semantic Web and users' contributions of the Social Web.
The ViewpointS approach was de-signed as an integrative formalism capable of mixing these two types of content while preserving the subjectivity of the interactions of the Social Web.
ViewpointS is a subjective knowledge repre-sention approach.
The approach also provides a second level of subjectivity.
Indeed, the viewpoints can be interpreted differently according to the user through the perspective mechanism.
In our frame-work, resources from the Web are tied by viewpoints in a Knowledge Graph.
From the Knowledge Graph containing viewpoints and Web resources a Knowledge Map consisting of “synapses” and re-sources is created as a result of the interpretation and aggregation of viewpoints.
The evolution of the ViewpointS synapses may be considered analog to the ones in the brain in the very simple sense that each viewpoint contributes to the establishment, strengthening or weakening of a syn-apse that connects two resources.
The exchange of viewpoints is the selection process ruling the synapses evolution like the selectionist process within the brain.
We investigate in this study the potential impact of our subjective representation of knowledge in various fields: information search, recommendation, multilingual ontology alignment and methods for calculating semantic distances.
We define a fragment of constraints and propose two approaches: the naive and the rewriting, which allows us to filter dynamically valid answers at the query time instead of validating them at the data source level.
These two approaches have been evaluated and have shown the feasibility of our system.
This is our main contribution: we extend the set of well-known query-rewriting systems (Chase, Chase&amp; backchase, PerfectRef, Xrewrite, etc.) with a new effective solution for the new purpose of filtering query results based on constraints in user context.
Moreover, we also enlarge the trigger condition of the constraint compared with other works by using the notion of one-way MGU.
Tabular data often contain columns with categorical variables, usually considered as non-numerical entries with a fixed and limited number of unique elements or categories.
As many statistical learning algorithms require numerical representations of features, an encoding step is necessary to transform categorical entries into feature vectors, using for instance one-hot encoding.
However, non-curated data give rise to string categorical variables with a very high cardinality and redundancy: the string entries share semantic and/or morphological information, and several entries can reflect the same entity.
Without any data cleaning or feature engineering step, common encoding methods break down, as they tend to lose information in their vectorial representation.
Also, they can create high-dimensional feature vectors, which prevent their usage in large scale settings.
In this work, we study a series of categorical encodings that remove the need for preprocessing steps on high-cardinality string categorical variables.
Experiments on real and simulated data show that the methods we propose improve supervised learning, are adapted to large-scale settings, and, in some cases, create feature vectors that are easily interpretable.
Hence, they can be applied in Automated Machine Learning (AutoML) pipelines in the original string entries without any human intervention.
In this thesis, we address the specific problem of probabilistic graphical model structure learning, that is, finding the most efficient structure to represent a probability distribution, given only a sample set D ∼ p(v).
In the first part, we review the main families of probabilistic graphical models from the literature, from the most common (directed, undirected) to the most advanced ones (chained, mixed etc.).
Then we study particularly the problem of learning the structure of directed graphs (Bayesian networks), and we propose a new hybrid structure learning method, H2PC (Hybrid Hybrid Parents and Children), which combines a constraint-based approach (statistical independence tests) with a score-based approach (posterior probability of the structure).
In the second part, we address the multi-label classification problem, which aims at assigning a set of categories (binary vector y P (0, 1)m) to a given object (vector x P Rd).
In this context, probabilistic graphical models provide convenient means of encoding p(y|x), particularly for the purpose of minimizing general loss functions.
We review the main approaches based on PGMs for multi-label classification (Probabilistic Classifier Chain, Conditional Dependency Network, Bayesian Network Classifier, Conditional Random Field, Sum-Product Network), and propose a generic approach inspired from constraint-based structure learning methods to identify the unique partition of the label set into irreducible label factors (ILFs), that is, the irreducible factorization of p(y|x) into disjoint marginal distributions.
We establish several theoretical results to characterize the ILFs based on the compositional graphoid axioms, and obtain three generic procedures under various assumptions about the conditional independence properties of the joint distribution p(x, y).
The increasing use of social and sensor networks generates a large quantity of data that can be represented as complex graphs.
There are many tasks from information analysis, to prediction and retrieval one can imagine on those data where relation between graph nodes should be informative.
All the proposed models use the representation learning framework in its deterministic or Gaussian variant.
First, we proposed two algorithms for the heterogeneous graph labeling task, one using deterministic representations and the other one Gaussian representations.
Contrary to other state of the art models, our solution is able to learn edge weights when learning simultaneously the representations and the classifiers.
Second, we proposed an algorithm for relational time series forecasting where the observations are not only correlated inside each series, but also across the different series.
We use Gaussian representations in this contribution.
This was an opportunity to see in which way using Gaussian representations instead of deterministic ones was profitable.
At last, we apply the Gaussian representation learning approach to the collaborative filtering task.
This is a preliminary work to see if the properties of Gaussian representations found on the two previous tasks were also verified for the ranking one.
The goal of this work was to then generalize the approach to more relational data and not only bipartite graphs between users and items.
In the medical field, the computerization of health professions and development of the personal medical file (DMP) results in a fast increase in the volume of medical digital information.
The need to convert and manipulate all this information in a structured form is a major challenge.
This is the starting point for the development of appropriate tools where the methods from the natural language processing (NLP) seem well suited.
The work of this thesis are within the field of analysis of medical documents and address the issue of representation of biomedical information (especially the radiology area) and its access.
We show the interest of the hypothesis of no separation between different types of knowledge through a document analysis.
This network combines weight and annotations on typed relationships between terms and concepts as well as an inference mechanism which aims to improve quality and network coverage.
We describe how from semantic information in the network, it is possible to define an increase in gross index built for each records to improve information retrieval.
We present then a method of extracting semantic relationships between terms or concepts.
This extraction is performed using lexical patterns to which we added semantic constraints.
The results show that the hypothesis of no separation between different types of knowledge to improve the relevance of indexing.
The index increase results in an improved return while semantic constraints improve the accuracy of the relationship extraction.
Molecular evolution proceeds not only by divergence from a common ancestor, but also by combining parts from evolving objects of different origins, through processes that are called introgressive.
Lateral gene transfers are probably the most well-known of these processes, but introgression has been shown to also happen at various levels of biological organization.
As a result, most biological evolving objects (genes, genomes, communities) can be composed of parts from different phylogenetic origins and can be described as composites.
Such modular evolution is inadequately modeled by trees, since composite objects are not merely the result of divergence from a common ancestor only.
Networks on the other hand are much more suited for handling modularity, and graph theory can be used to search networks for patterns that are characteristic of such reticulate evolution.
During this PhD, I developed a piece of software, CompositeSearch, that can efficiently detect composite genes in massive sequence dataset, comprising up to millions of sequences.
This algorithm was used to identify and quantify the abundance of composite genes in polluted soil environments, and in prokaryotic plasmids.
These studies show that important biological novelties and adaptations can result from processes acting at subgenic levels.
However, as shown in this manuscript, networks provide a framework that goes well beyond the boundaries of molecular evolution and I have applied them to other evolving entities, such as animals (trait networks) morphology and languages (word networks).
In both cases, modularity appears to be a major evolutionary outcome, following rules that remain to be investigated.
The last decade has seen the re-emergence of machine learning methods based on formal neural networks under the name of deep learning.
Although these methods have enabled a major breakthrough in machine learning, several obstacles to the possibility of industrializing these methods persist, notably the need to collect and label a very large amount of data as well as the computing power necessary to perform learning and inference with this type of neural network.
In this thesis, we propose to study the adequacy between inference and learning algorithms derived from biological neural networks and massively parallel hardware architectures.
We show with three contribution that such adequacy drastically accelerates computation times inherent to neural networks.
We also propose the introduction of a coarse-to-fine architecture based on complex cells.
We show that GPU portage accelerates processing by a factor of seven, while the coarse-to-fine architecture reaches a factor of one thousand.
The second contribution presents three algorithms for spike propagation adapted to parallel architectures.
We study exhaustively the computational models of these algorithms, allowing the selection or design of the hardware system adapted to the parameters of the desired network.
In our third axis we present a method to apply the Spike-Timing-Dependent-Plasticity rule to image data in order to learn visual representations in an unsupervised manner.
We show that our approach allows the effective learning a hierarchy of representations relevant to image classification issues, while requiring ten times less data than other approaches in the literature.
Using gesture commands is a new way of interacting with touch sensitive interfaces.
In order to facilitate user memorization of several commands, it is essential to let the user customize the gestures.
This applicative context gives rise to a crosslearning situation, where the user has to memorize the set of commands and the system has to learn and recognize the different gestures.
This situation implies several requirements, from the recognizer and from the system that supervizes its learning process.
For instance, the recognizer has to be able to learn from few data samples, to keep learning during its use and to follow indefinitely any change of the data now.
The supervisor has to optimize the cooperation between the recognizer and the system to minimize user interactions while maximizing recognizer learning.
This thesis presents on the one hand the evolving recognition system Evolve oo, that is capable of fast teaming from few data samples, and that follows concept drifts.
The use of forgetting in the learning process allows to maintain the learning gain indefinitely, enabling class adding at any stage of system learning, and guaranteeing lifelong evolving capacity.
The on line active supervisor IntuiSup optimizes user interactions to train a classifier when the user is in the training loop.
The proportion of data that is labeled by the user evolves to adapt to problem difficulty and to follow environment evolution (concept drift s).
The use of a boosting method optimizes the timing of user interactions to maximize their impact on classifier learning process.
The work presented here is for a first part at the cross section of deep learning and anonymization.
A full framework was developed in order to identify and remove to a certain extant, in an automated manner, the features linked to an identity in the context of image data.
Two different kinds of processing data were explored.
They both share the same Y-shaped network architecture despite components of this network varying according to the final purpose.
The first one was about building from the ground an anonymized representation that allowed a trade-off between keeping relevant features and tampering private features.
This framework has led to a new loss.
Therefore the anonymized representation shares the same nature as the initial data (e.g. an image is transformed into an anonymized image).
This task led to another type of architecture (still in a Y-shape) and provided results strongly dependent on the type of data.
The second part of the work is relative to another kind of relevant information: it focuses on the monitoring of predictor behavior.
In the context of black box analysis, we only have access to the probabilities outputted by the predictor (without any knowledge of the type of structure/architecture producing these probabilities).
This monitoring is done in order to detect abnormal behavior that is an indicator of a potential mismatch between the data statistics and the model statistics.
Two methods are presented using different tools.
The first one is based on comparing the empirical cumulative distribution of known data and to be tested data.
The second one introduces two tools: one relying on the classifier uncertainty and the other relying on the confusion matrix.
These methods produce concluding results.
The constantly changing customers'and users'needs require fast response from software teams.
This creates strong demand for seamlessness of the software processes.
Continuous integration, delivery and deployment, also known as DevOps, made a huge progress in making software processes responsive to change.
This progress had little effect on software requirements, however.
Specifying requirements still relies on the natural language, which has an enormous expressive power, but inhibits requirements'traceability, verifiability, reusability and understandability.
Promoting the problematic qualities without inhibiting the expressiveness too much introduces a challenge.
This approach has motivated and inspired the work on the present thesis.
While multirequirements focus on traceability and understandability, the Seamless Object-Oriented Requirements approach presented in the dissertation takes care of verifiability, reusability and understandability.
The dissertation explores the Martin Glinz'hypothesis that software requirements should be objects to support seamlessness.
The exploration confirms the hypothesis and results in a collection of tool-supported methods for specifying, validating, verifying and reusing object-oriented requirements.
The most significant reusable technical contribution of the dissertation is a ready-to-use Eiffel library of template classes that capture recurring software requirement patterns.
Concrete seamless object-oriented requirements inherit from these templates and become clients of the specified software.
The dissertation reflects on several experiments and shows that the new approach promotes requirements'verifiability, reusability and understandability while keeping expressiveness at an acceptable level.
The experiments rely on several examples, some of which are used as benchmarks in the requirements literature.
Each experiment illustrates a problem through an example, proposes a general solution, and shows how the solution fixes the problem.
While the experimentation relies on Eiffel and its advanced tool support, such as automated proving and testing, each idea underpinning the approach scales conceptually to any statically typed object-oriented programming language with genericity and elementary support for contracts.
In this thesis, we propose to use techniques based on factor analysis to build acoustic models for automatic speech processing, especially Automatic Speech Recognition (ASR).
Firstly, we were interested in reducing the footprint memory of acoustic models.
Our factor analysis-based method demonstrated that it is possible to pool the parameters of acoustic models and still maintain performance similar to the one obtained with the baseline models.
We propose as an alternative a vector representation of states: the factors of states.
These factors of states enable us to accurately measure the similarity between the states of the HMM by means of an euclidean distance for example.
Using this vector representation, we propose a simple and effective method for building acoustic models with shared states.
This procedure is even more effective when applied to under-resourced languages.
Finally, we concentrated our efforts on the robustness of the speech recognition systems to acoustic variabilities, particularly those generated by the environment.
In our various experiments, we examined speaker variability, channel variability and additive noise.
Through our factor analysis-based approach, we demonstrated the possibility of modeling these different types of acoustic variability as an additive component in the cepstral domain.
By compensation of this component from the cepstral vectors, we are able to cancel out the harmful effect it has on speech recognition
Landmarks are presented in the applications of different domains such as biomedical or biological.
It is also one of the data types which have been usedin different analysis, for example, they are not only used for measuring the form of the object, but also for determining the similarity between two objects.
In biology, landmarks are used to analyze the inter-organisms variations, however the supply of landmarks is very heavy and most often they are provided manually.
In recent years, several methods have been proposed to automatically predict landmarks, but it is existing the hardness because these methods focused on the specific data.
This thesis focuses on automatic determination of landmarks on biological images, more specifically on two-dimensional images of beetles.
In our research, we have collaborated with biologists to build a dataset including the images of 293 beetles.
For each beetle in this dataset, 5 images correspond to 5 parts have been taken into account, e.g., head, body, pronotum, left and right mandible.
Along with each image, a set of landmarks has been manually proposed by biologists.
First step, we have brought a method which was applied on fly wings, to apply on our dataset with the aim to test the suitability of image processing techniques on our problem.
Secondly, we have developed a method consisting of several stages to automatically provide the landmarks on the images.
These two first steps have been done on the mandible images which are considered as obvious to use the image processing methods.
Thirdly, we have continued to consider other complex remaining parts of beetles.
Accordingly, we have used the help of Deep Learning.
We have designed a new model of Convolutional Neural Network, named EB-Net, to predict the landmarks on remaining images.
In addition, we have proposed a new procedure to augment the number of images in our dataset, which is seen as our limitation to apply deep learning.
Finally, to improve the quality of predicted coordinates, we have employed Transfer Learning, another technique of Deep Learning.
In order to do that, we trained EB-Net on a public facial key points.
Then, they were transferred to fine-tuning on beetle's images.
The obtained results have been discussed with biologists, and they have confirmed that the quality of predicted landmarks is statistically good enough to replace the manual landmarks for most of the different morphometry analysis.
This thesis adresses the issue of accessing scientific and technical information conveyed by large sets of documents.
The resulting model enables a documentary immersion, thanks to three types of complementary processes: endogenous processes (exploiting the corpus to analyze the corpus), exogenous processes (using external resources) and anthropogenous ones (in which the user's skills are considered as a resource) are combined.
They all contribute to granting the user a fundamental role in the system, as an interpreting agent and as a knowledge creator, provided that he is placed in an industrial or specialised context.
The present work deals with the problem of the semantic complexity in natural language, proposing an hypothesis based on some features of natural language sentences that determine their difficulty for human understanding.
We aim at introducing a general framework for semantic complexity, in which the processing difficulty depends on the interaction between two components: a Memory component, which is responsible for the storage of corpus-extracted event representations, and a Unification component, which is responsible for combining the units stored in Memory into more complex structures.
We propose that semantic complexity depends on the difficulty of building a semantic representation of the event or the situation conveyed by a sentence, that can be either retrieved directly from the semantic memory or built dynamically by solving the constraints included in the stored representations.
In order to test our intuitions, we built a Distributional Semantic Model to compute a compositional cost for the sentence unification process.
Our tests on several psycholinguistic datasets showed that our model is able to account for semantic phenomena such as the context-sensitive update of argument expectations and of logical metonymies.
The large use of CAD systems in many industrial fields, such as automotive, naval, and aerospace, has generated a number of 3D databases making available a lot of 3D digital models.
Within enterprises, which make use of these technologies, it is common practice to access to CAD models of previously developed products.
Therefore, it is useful to have technological solutions that are able to evaluate the similarities of different products in such a way that the user can retrieve existing models and thus have access to the associated useful information for the new design.
The concept of similarity has been widely studied in literature and it is well known that two objects can be similar under different perspectives.
These multiple possibilities make complicate the assessment of the similarity between two objects.
So far, many methods are proposed for the recognition of different parts similarities, but few researches address this problem for assembly models.
Based on these requirements, we propose a system for retrieving similar assemblies according to different similarity criteria.
To achieve this goal, it is necessary having an assembly description including all the information required for the characterizations of the possible different similarity criteria between the two assemblies.
Therefore, one of the main topics of this work is the definition of a descriptor capable of encoding the data needed for the evaluation of similarity adaptable to different objectives.
In addition, some of the information included in the descriptor may be available in CAD models, while other has to be extracted appropriately.
Therefore, algorithms are proposed for extracting the necessary information to fill out the descriptor elements.
Finally, for the evaluation of assembly similarity, several measures are defined, each of them evaluating a specific aspect of their similarity.
Machine Translation (MT) has made significant progress in the recent years and continues to improve.
Today, MT is successfully used in many contexts, including professional translation environments and production scenarios.
However, the translation process requires knowledge larger in scope than what can be captured by machines even from a large quantity of translated texts.
Since injecting human knowledge into MT is required, one of the potential ways to improve MT is to ensure an optimized human-machine collaboration.
To this end, many questions are asked by modern research in MT: How to detect where human assistance should be proposed?
How to make machines exploit the obtained human knowledge so that they could improve their output?
And, not less importantly, how to optimize the exchange so as to minimize the human effort involved and maximize the quality of MT output?
Various solutions have been proposed depending on concrete implementations of the MT process.
In this thesis we have chosen to focus on Pre-Edition (PRE), corresponding to a type of human intervention into MT that takes place ex-ante, as opposed to Post-Edition (PE), where human intervention takes place ex-post.
In particular, we study targeted PRE scenarios where the human is to provide translations for carefully chosen, difficult-to-translate, source segments.
Targeted PRE scenarios involving pre-translation remain surprisingly understudied in the MT community.
Moreover, in a multilingual setting common difficulties can be resolved at one time and for many languages.
Such scenarios thus perfectly fit standard production contexts, where one of the main goals is to reduce the cost of PE and where translations are commonly performed simultaneously from one language into many languages.
A representative production context - an automatic translation of systematic medical reviews - is the focus of this work.
Given this representative context, we propose a system-independent methodology for translation difficulty detection.
We define the notion of translation difficulty as related to translation quality: difficult-to-translate segments are segments for which an MT system makes erroneous predictions.
We cast the problem of difficulty detection as a binary classification problem and demonstrate that, using this methodology, difficulties can be reliably detected without access to system-specific information.
We integrate the results of our difficulty detection procedure into a PRE protocol that enables resolution of those difficulties by pre-translation.
We assess the protocol in a simulated setting and show that pre-translation as a type of PRE can be both useful to improve MT quality and realistic in terms of the human effort involved.
Moreover, indirect effects are found to be genuine.
Results of those pilot experiments confirm the results in the simulated setting and suggest an encouraging beginning of the test phase.
This research work deals with the interdisciplinary field of the information and communication sciences (CIS) and aims to explore the use of the semantic web in digital libraries.
The web requires libraries to rethink their organizations, activities, practices and services in order to reposition themselves as reference institutes for the dissemination of knowledge.
In this thesis, we wish to understand the contexts of use of the semantic web in French digital libraries.
It questions the contributions of the semantic web within these libraries, as well as on the challenges and the obstacles that accompany its implementation.
We are also interested in documentary practices and their evolutions following the introduction of the semantic web in digital libraries.
The problem is related to the role that information professionals can play in the implementation of the semantic web in digital libraries.
After selecting 98 digital libraries following an analysis of three censuses, a questionnaire survey aims to collect data on the use of the semantic web in these libraries.
Then, a second interview-based survey consists of highlighting the representations that the information professionals have of the semantic web and its use in the library, as well as on the evolution of their professional practices.
The results show that the representation of knowledge within the semantic web requires human intervention to provide the conceptual framework to determine the links between the data.
Finally, information professionals can become actors of the semantic web, in the sense that their roles are not limited to the use of the semantic web but also to the development of its standards to ensure better organization of knowledge.
The work synthesized in this thesis aims at studying the coordination between manual gestures and speech during multimodal utterances production.
More precisely, the temporal relationship between the two modalities is considered.
The coordination is studied in a designation framework since designating is possible both manually (pointing gesture) and using speech (one can "show with the voice" using focus and/or demonstratives for example).
All the studies presented in this work are done in a lab setting thus allowing to get precise and reproducible measurements while minimizing potential external sources of variation (either between or within participants).
Participants'productions were then compared to each other focusing on factors of interest while keeping other sources of variation as low as possible.
A part of the work consisted in designing rather natural experimental protocols so as to ensure productions were not too artificial.
The first two experiments studied to co-production of manual gestures and speech containing a focused part.
Different types of gestures were compared (pointing gesture, beat, button-push) in a designation task.
It has been shown that producing focus did temporally attract manual gesture whichever its type but that this attraction was finer and less variable for pointing gesture.
Another interesting finding was that the apex of pointing gesture seems to be cooccurring with articulatory targets rather than acoustic ones.
The second study manipulates the designation link between manual gestures and speech.
By showing that participants can be split up into two groups using different multimodal coordination strategies, it put forward the complexity of underlying mechanisms of this coordination.
The last experiment focuses on the coordination in a more natural interactive and collaborative task.
Results show a co-ocurrence of the part of the gesture that shows and with the complementary information in speech (ie. the name of the object to be placed at the spot pointed at by the manual gesture) rather than with the part of speech that shows (ie. demonstrative).
The work presented in this manuscript moreover put forward a systematic way of labeling semi-constrained interactive tasks which can be generalized.
The conclusion puts in perspective the results so as to improve some manual gestures/speech co-production models and indicates paths for reflection about embodied conversational agents and early detection of pathological cases.
This dissertation work is realised as a contrastive analysis which aims the identification of the aspectual differences between two linguistic systems, French and Russian.
Our methodology is based on the analysis of two types of data corpora: comparable and parallel.
The subject of this research concerns the study of aspectuel values of Nouns of emotion and their collocative verbs, especially in the Verb+Noun constractions.
The identification of the aspectual values of these combinations comes from their lexical and syntactic combinatory.
It is composed of different parameters (settings): aspectuel features of the Noun (bi-nominal structures, adjectives-modifiers and determinants) and aspectual features of the Verb (grammatical aspect, lexical aspect and phases).
This thesis focuses on mismatches in peripheral ellipsis (RNR) and proposes an analysis based on lexeme identity between the missing material and the peripheral material.
In this thesis, we challenge this hypothesis.
We analyzed 5 types of mismatches in peripheral ellipsis: polarity mismatch, possessive mismatch, voice mismatch and verbal form mismatch.
Mismatches are quite numerous even in careful writings.
In all cases, the mismatches are resolved by the form that corresponds to the second conjunct.
The results of acceptability judgment tests and eye tracking experiments allow the integration of these mismatches into the grammar.
The results are compatible with analyses postulating semantic identity between the missing material and the antecedent for ellipsis.
We formalize peripheral ellipsis with mismatch within HPSG.We finally compare our results with lexical coordination.
We show that it obeys closest conjunct agreement (Villavicencio et al (2005)) and propose a HPSG analysis for coordination of verbs and prepositions.
The development of correct formal specifications for systems and software begins with the analysis and understanding of client requirements.
Between these requirements described in natural language and their specification defined in a specific formal language, a gap exists and makes the task of development more and more difficult to accomplish.
We are facing two different worlds.
This thesis aims to clarify and establish interactions between these two worlds and to evolve them together.
By interaction, we mean all the links, exchanges and activities taking place between the different documents.
Among these activities, we present the validation as a rigorous process that starts from the requirements analysis and continues throughout the development of their formal specification.
As development progresses, choices are made and feedbacks from verification and validation tools can detect shortcomings in requirements as well as in the specification.
The evolution of the two worlds is described via the introduction of a new requirement into an existing system and through the application of development patterns.
They facilitate the task of development and help to avoid the risk of oversights.
Whatever the choice, the proposed approach is guided by questions accompanying the evolution of the whole system and makes it possible to detect imperfections, omissions or ambiguities in the existing.
This work aims at providing efficient access to relevant information among the increasing volume of digital data.
Thus, we proposed a mixed method which combines natural language processing techniques for extracting knowledge from text and the reuse of existing semantic resources for the conceptualization step.
We have also developed a method for aligning terms in English and French in order to enrich terminologically the resulting ontology.
The application of our methodology resulted in a bilingual ontology dedicated to Alzheimer's disease.
We then proposed algorithms for supporting ontology-based semantic IR.
Thus, we used concepts from ontology for describing documents automatically and for query reformulation.
We were particularly interested in: 1) the extraction of concepts from texts, 2) the disambiguation of terms, 3) the vectorial weighting schema adapted to concepts and 4) query expansion.
These algorithms have been used to implement a semantic portal about Alzheimer's disease.
Further, because the content of documents are not always fully available, we exploited incomplete information for identifying the concepts, which are relevant for indexing the whole content of documents.
Toward this end, we have proposed two classification methods: the first is based on the k nearest neighbors' algorithm and the second on the explicit semantic analysis.
The two methods have been evaluated on large standard collections of biomedical documents within an international challenge.
Document processing is the transformation of a human understandable data in a computer system understandable format.
Document analysis and understanding are the two phases of document processing.
Considering a document containing lines, words and graphical objects such as logos, the analysis of such a document consists in extracting and isolating the words, lines and objects and then grouping them into blocks.
The subsystem of document understanding builds relationships (to the right, left, above, below) between the blocks.
A document processing system must be able to: locate textual information, identify if that information is relevant comparatively to other information contained in the document, extract that information in a computer system understandable format.
For the realization of such a system, major difficulties arise from the variability of the documents characteristics, such as: the type (invoice, form, quotation, report, etc.), the layout (font, style, disposition), the language, the typography and the quality of scanning.
This work is concerned with scanned documents, also known as document images.
We are particularly interested in locating textual information in invoice images.
Invoices are largely used and well regulated documents, but not unified.
They contain mandatory information (invoice number, unique identifier of the issuing company, VAT amount, net amount, etc.) which, depending on the issuer, can take various locations in the document.
The present work is in the framework of region-based textual information localization and extraction.
First, we present a region-based method guided by quadtree decomposition.
Our method allows to determine accurately in document images, the regions containing text information that one wants to locate and retrieve quickly and efficiently.
In another approach, we propose a textual information extraction model consisting in a set of prototype regions along with pathways for browsing through these prototype regions.
The life cycle of the model comprises five steps:
- Produce synthetic invoice data from real-world invoice images containing the textual information of interest, along with their spatial positions.
- Partition the produced data.
- Derive the prototype regions from the obtained partition clusters.
- Derive pathways for browsing through the prototype regions, from the concept lattice of a suitably defined formal context.
- Update incrementally the set of protype regions and the set of pathways, when one has to add additional data.
The study of continental surfaces is a major global challenge for the monitoring and management of territories, particularly in terms of the distribution between urban expansion, agricultural land and natural areas.
In this context, land cover maps characterizing the biophysical cover of land are an essential asset for the analysis of continental surfaces.
Supervised classification algorithms allow, from annual time series of satellite images and reference data, to automatically produce the map of the corresponding period.
However, reference data is expensive information to obtain, especially over large areas.
Indeed, field survey campaigns require a high human cost, and databases are associated with long update times.
In addition, these reference data are valid only for the corresponding period due to changes in land use.
These changes mainly concern urban expansion at the expense of natural areas, and agricultural land subject to crop rotation.
The general objective of the thesis is to propose methods for producing land cover maps without exploiting the reference data of the corresponding period.
The work carried out is based on the creation of a land cover history.
This history includes all the information available for the area of interest: land cover maps, image time series, reference data, classification models, etc.
A first part of the work considers that the history contains only one period.
Thanks to this history we proposed a \g{naïve} classification approach allowing to use a classifier already trained, over a new period.
The performances obtained shown that this approach is insufficient, thus requiring more efficient methods.
Domain adaptation makes it possible to address this type of problem.
We considered two approaches: data projection via canonical correlation analysis and optimal transport.
These two approaches allow the historical data to be projected in order to reduce differences with the year to be processed.
Nevertheless, these approaches offer results equivalent to the naive classification for much more significant production costs.
In many areas where it exists human risks, such as medicine, nuclear or avionics, it is necessary to go through a certification stage to ensure the proper functioning of a system or product.
Certification is based on normative documents that express the justification requirements to which the product and the development process must conform.
A certification audit then consists of producing documentation certifying compliance with this regulatory framework.
To cope with this need for justifications to ensure compliance with the standards in force and the completeness of the justifications provided, it must therefore be able to target the justification requirements to be claimed for a project and produce justifications during the development of the project.
In this context, eliciting the justification requirements from the standards and producing the necessary and sufficient justifications are issues to ensure compliance with standards and avoid over-justification.
In these works we seek to structure the justification requirements and then help to produce the associated justifications while remaining attentive to the confidence that can be placed in them.
To address these challenges, we have defined a formal semantics for an existing model of justifications: Justification Diagrams.
From this semantics, we have been able to define a set of operations to control the life cycle of the justifications to ensure that the justifications regarding the justification requirements.
Through this semantics, we have also been able to guide, and even automate in some cases, the production of justifications and the verification of conformance.
These contributions were applied in the context of medical technologies for the company AXONIC, the bearer of this work.
This thesis takes place in Natural Langage Processing and aims to help users in such situation.
Existing systems (as search engines on Internet) do not fully satisfied their users in repeated tasks, taking few into consideration their point of view and their interactions with the textual material.
In this thesis, we propose to consider personalization and interaction as the core of new tools to access the content of sets of texts.
Thus, we represent users'point of view on domains of their interest with sets of lexical units described and structured according to a differential lexical semantic model.
We then use such representations to build cartographic supports allowing interactions between users and theirs sets of texts in order to visualize gatherings, links and differences between texts of a set and, by this way, to reach their content.
To computerize such propositions, we have developed the ProxiDocs plat-form.
«What do I need to know about something to know it?».
It is no wonder that such a general, hard to grasp and riddle-like question remained the exclusive domain of a single discipline for centuries: Philosophy.
In this context, the distinction of the primitive components of reality – the so called "world's furniture" – and their relations is called an Ontology.
This book investigates the emergence of similar questions in two different though related fields, namely: Artificial Intelligence and Knowledge Engineering.
We show here that the way these disciplines apply an ontological methodology to either cognition or knowledge representation is not a mere analogy but raises a bunch of relevant questions and challenges from both an applied and a speculative point of view.
More specifically, we suggest that some of the technical answers to the issues addressed by Big Data invite us to revisit many traditional philosophical positions concerning the role of language or common sense reasoning in the thought or the existence of mind-independent structure in reality.
Developing natural language processing tools usually requires a large number of resources (lexica, annotated corpora,...), which often do not exist for less-resourced languages.
Another approach is to exploit existing resources of closely related languages.
Taking advantage of the closeness of standard Arabic and its dialects, one way to solve the problem of limited resources, consists in performing a conversion of Arabic dialects into standard Arabic in order to use the tools developed to handle the latter.
We propose a conversion system of Tunisian into a closely form of standard Arabic for which the application of natural language processing tools designed for the latter provides good results.
In order to validate our approach, we focused on part-of-speech tagging.
Our system achieved an accuracy of 89% which presents ∼20% of absolute improvement over a standard Arabic tagger baseline.
Children with language impairment, such as dyslexia, are often faced with important difficulties when learning to read and during any subsequent reading tasks.
Over the past fifteen years, general tools developed in the field of Natural Language Processing have been transformed into specific tools for that help with and compensate for language impaired students'difficulties.
At the same time, the use of concept maps or heuristic maps to encourage dyslexic children express their thoughts, or retain certain knowledge, has become popular.
It was important that this piece of software facilitate reading comprehension while including functionalities that are adapted to dyslexic teenagers.
A collection of documents is generally represented as a set of documents but this simple representation does not take into account cross references between documents, which often defines their context of interpretation.
This standard document model is less adapted for specific professional uses in specialized domains in which documents are related by many various references and the access tools need to consider this complexity.
We propose two models based onformal and relational concept analysis and on semantic web techniques.
The aim of this thesis is to understand how the brain computes and represents symbolic structures, such like those encountered in language or mathematics.
The existence of parts in structures like morphemes, words and phrases has been established through decades of linguistic analysis and psycholinguistic experiments.
Nonetheless the neural implementation of the operations that support the extreme combinatorial nature of language remains unsettled.
Some basic composition operations that allow the stable internal representation of sensory objects in the sensory cortex, like hierarchical pattern recognition, receptive fields, pooling and normalization, have started to be understood[5].
But models of the binding operations required for construction of complex, possibly hierarchical, symbolic structures on which precise manipulation of its components is a requisite, lack empirical testing and are still unable to predict neuroimaging signals.
In this sense, bridging the gap between experimental neuroimaging evidence and the available modelling solutions to the binding problem is a crucial step for the advancement of our understanding of the brain computation and representation of symbolic structures.
From the recognition of this problem, the goal of this PhD became the identification and experimental test of the theories, based on neural networks, capable of dealing with symbolic structures, for which we could establish testable predictions against existing fMRI and ECoG neuroimaging measurements derived from language processing tasks.
We identified two powerful but very different modelling approaches to the problem.
The first is in the context of the tradition of Vectorial Symbolic Architectures (VSA) that bring precise mathematical modelling to the operations required to represent structures in the neural units of artificial neural networks and manipulate them.
This is Smolensky's formalism with tensor product representations (TPR)[10], which he demonstrates can encompass most of the previous work in VSA, like Synchronous Firing[9], Holographic Reduced Representations[8] and Recursive Auto-Associative Memories[1].
Instead of solving binding by assuming precise and particular algebraic operations on vectors, the NBA proposes the establishment of transient connectivity changes in a circuit structure of neural assemblies, such that the potential _ow of neural activity allowed by working memory mechanisms after a binding process takes place, implicitly represents symbolic structures.
The first part of the thesis develops in more detail the theory behind each of these models and their relationship from the common perspective of solving the binding problem.
Both models are capable of addressing most of the theoretical challenges posed currently for the neural modelling of symbolic structures, including those presented by Jackendo_[3].
For the second part of the thesis, we identified the superposition principle, which consists on the addition of the neural activations of each of the sub-parts of a symbolic structure, as one of the most crucial assumptions of Smolensky's TPR.
Computer assistance became indispensable part of modern surgical procedures.
Desire of creating new generation of intelligent operating rooms incited researchers to explore problems of automatic perception and understanding of surgical situations.
A great progress was achieved in recognition of surgical phases and gestures.
Yet, there is still a blank between these two granularity levels in the hierarchy of surgical process.
Very few research is focused on surgical activities carrying important semantic information vital for situation understanding.
Two important factors impede the progress.
First, automatic recognition and prediction of surgical activities is a highly challenging task due to short duration of activities, their great number and a very complex workflow with multitude of possible execution and sequencing ways.
Secondly, very limited amount of clinical data provides not enough information for successful learning and accurate recognition.
In our opinion, before recognizing surgical activities a careful analysis of elements that compose activity is necessary in order to chose right signals and sensors that will facilitate recognition.
We used a deep learning approach to assess the impact of different semantic elements of activity on its recognition.
Through an in-depth study we determined a minimal set of elements sufficient for an accurate recognition.
Information about operated anatomical structure and surgical instrument was shown to be the most important.
We also addressed the problem of data deficiency proposing methods for transfer of knowledge from other domains or surgeries.
The methods of word embedding and transfer learning were proposed.
They demonstrated their effectiveness on the task of next activity prediction offering 22% increase in accuracy.
In addition, pertinent observations about the surgical practice were made during the study.
Despite the importance of an international nomenclature, the field of chemistry still suffers from some linguistic problems, linked in particular to its simple and complex terminological units, which can hinder scientific communication.
This is in addition to the recurring use of borrowings.
The problematic is how to represent the simple and complex terminological units of this specialized language.
In other words, formalize the terminological characteristics by studying the mechanisms of themorphosyntactic construction of the chemistry'terms in Arabic.
This study should lead to the establishment of a semantic-disambiguation tool that aims to create a tool for extracting the terms of Arabic chemistry and their relationships.
The construction of this identification grammar requires modelling of morphosyntactic patterns from their observation in corpus and leads to the definition of rules of grammar and constraints.
Technological development has its pros and cons.
Nowadays, we can easily share, download, and upload digital content using the Internet.
Also, malicious users can illegally change, duplicate, and distribute any kind of information, such as images and documents.
Therefore, we should protect such contents and arrest the perpetrator.
The goal of this thesis is to protect PDF documents and images using the Spread Transform Dither Modulation (STDM), as a digital watermarking technique, while taking into consideration the main requirements of transparency, robustness, and security.
STDM watermarking scheme achieved a good level of transparency and robustness against noise attacks.
The key to this scheme is the projection vector that aims to spreads the embedded message over a set of cover elements.
However, such a key vector can be estimated by unauthorized users using the Blind Source Separation (BSS) techniques.
In our first contribution, we present our proposed CAR-STDM (Component Analysis Resistant-STDM) watermarking scheme, which guarantees security while preserving the transparency and robustness against noise attacks.
STDM is also affected by the Fixed Gain Attack (FGA).
In the second contribution, we present our proposed N-STDM watermarking scheme that resists the FGA attack and enhances the robustness against the Additive White Gaussian Noise (AWGN) attack, JPEG compression attack, and variety of filtering and geometric attacks.
Experimentations have been conducted distinctly on PDF documents and images in the spatial domain and frequency domain.
Recently, Deep Learning and Neural Networks achieved noticeable development and improvement, especially in image processing, segmentation, and classification.
Diverse models such as Convolutional Neural Network (CNN) are exploited for modeling image priors for denoising.
CNN has a suitable denoising performance, and it could be harmful to watermarked images.
In the third contribution, we present the effect of a Fully Convolutional Neural Network (FCNN), as a denoising attack, on watermarked images.
STDM and Spread Spectrum (SS) are used as watermarking schemes to embed the watermarks in the images using several scenarios.
This evaluation shows that such type of denoising attack preserves the image quality while breaking the robustness of all evaluated watermarked schemes.
In-vivo reflectance confocal microscopy (RCM) is a powerful tool to visualize the skin layers at cellular resolution.
Aging descriptors have been highlighted from confocal images.
However, it requires visual assessment of images by experienced dermatologists to assess those descriptors.
The objective of this thesis is the development of an innovative technology to automatically quantify the phenomenon of skin aging using in vivo reflectance confocal microscopy.
First, the quantification of the epidermal state is addressed.
The proposed measurements show significant difference among groups of age and photo-exposition.
Finally, the proposed methods are validated through both clinical and cosmetic product efficacy studies
Nowadays, the use of business process management techniques within companies allows a significant improvement in the efficiency of the operational systems.
These techniques assist business experts in modelling business processes, implementation, analytics, and enhancements.
The execution context of a business process contains information to identify and understand the interactions between itself and other processes.
The first process is triggered following a customer need (operational process) and the others, following the need of the operationnal process (support processes).
The satisfaction of a customer need depends on an effective interaction between an operational process and the related support processes.
These interactions are defined through operational data, manipulated by the operational process, and support data, manipulated by the support processes.
Our work is based on the framework of Model Driven Engineering.
The approach proposed in this thesis is based on the annotation of operational or support process models.
This annotation is performed with the help of an ontology defining the business domain described by these processes.
These annotations are then exploited to constitute a set of data, called contextual data.
The analysis of the traces of the execution of the operational process and of these contextual data makes it possible to select the best sub set of contextual data, in the business sense.
Thus, an operational process can be associated with a set of support processes via the contextual data.
On a global scale, migration numbers have evolved right after the Cold War period.
Refugee populations grew and rates changed from one year to another ranging from 2.4 million in 1975 and reaching 12.1 million in the year of 2000 (UNHCR 1995; UNHCR 2000).
Today, the migration rates in Europe show a total of more than 4 million immigrants to different European countries.
Furthermore, France and The United Kingdom are considered as two of the major migration targets in Europe, according to the statistical office of the European Union situated in Luxembourg " Germany reported the largest total number of immigrants (917.1 thousand) in 2017, followed by the United Kingdom (644.2 thousand), Spain (532.1 thousand), France (370.0 thousand) and Italy (343.4 thousand)", an attraction for migration and asylum alike.
In general, the majority of migrants and refugees experience difficult living conditions; however, the impact on women is more critical due to gender and socioeconomic inequalities, despite the fact that female migrants represent 52% of the entire migrants of Europe.
Some female immigrants and refugees on the other hand, took it on themselves to express their turmoil and draw the public eye's attention to their predicament.
The aim of this research is, on the one hand, to delve deeper into the inner workings and apparatuses that are shaping the European culture and heritage as a result of migrants' cultural contributions, and on the other hand, to study and analyze the means used to channel their thoughts and share their stories with the rest of the world between the XX and the XXI centuries.
This research covers several artistic and literary disciplines - ranging from visual arts such as theatrical and cinematic productions, filmography and photography to literary works as novels, creative writings and comic books- which could be used for various ends such as sociological assertiveness, a quest for identity and belonging, and therapy.
On theater, the study covers the pioneer theatrical texts and performances that have shaped migration history within the two European regions over the 20 and 21st centuries, as well as the troupes and groups that target and tend to include refugees.
We could refer, therefore to the Franco-Algerian playwright Salika Amara who worked within the French theater crew called Kahina founded by women of Aubervilliers in 1976 and mainly consisted of female actors and their counterparts in the United Kingdom Winsomme Pinnock and contemporary English- Nigerian Bola Agbaje.
It goes without saying that the work on theater is not only limited to theater stages but covers all types of representations, formal and informal, such as the work Ornina, a company founded by individuals of French, Syrian and multiple origins that help the refugees coming to the city of Dijon in France, of Le Bureau d'Accueil et d'Accompagnement des Migrants (BAAM), theater company CK Point, The Good Chance Theatre in the UK and even plays represented in international Festivals (migrant'scène, Avignon, Edinburgh).
When it comes to cinema and filmmaking, I will over the minority cinema that reflects the conditions of exile and alienation and that are mostly produced in the margins.
This is better described by Naficy as films with open form and closed-form visual style; fragmented, multilingual, epistolary, self reflexive, and critically juxtaposed narrative structure; amphibolic, doubled, crossed, and lost characters; subject matter and themes that involve journeying, historicity, identity, and displacement; dysphoric, euphoric, nostalgic, synaesthetic, liminal, and politicized structures of feeling, interstitial and collective modes of production; and inscription of the biographical, social, and cinematic (dis)location of the filmmakers. (Naficy 2001: 4)
All those characteristics are usually present in the migration filmmaking; moreover, these films represent the state of mind and psychological condition of the immigrant or refugee especially if the producers or main actors are themselves migrants or refugees such as in the short film Unbroken Paradise played by a genuine Syrian refugee.
Other migration movies on the other hand tend to raise awareness about certain matters and act as advocates for human or civil rights to call for improvement for the sake of the young women's welfare, a sort of deconstruction of the "male gaze" and an adoption of a defensive self-reflexive approach.
Pertaining to the field of visual arts, the role of photography is undeniable.
For ages, graphics have been a medium for loud yet silent expressions.
In the case of migration, the task and the burden are heavier than usual.
It takes only one click to redirect a vision, an idea and shift a perspective of a nation, to a positive or a negative side alike.
In this research we believe that the role of graphics is very prominent and pertinent to the migrants' journey.
Just like the aforementioned works of art, photography is a means of re-identification and re-telling of migrants' stories.
Omar Imam for instance, a Syrian photographer working on the Syrian migration crisis, with his project Live, Love, Refugee within refugee camps and tells stories of refugees with creative and surrealistic shots and still pictures of different actions that intend to reflect certain emotions and thoughts.
Many other photography projects will be covered in the research with the aim of re-writing female migrants identities such as the Refugees Learning and Storytelling through Participatory Photography project launched by three female leaders in the field of migration and photography and that aims at teaching women migrants the basics of photography in order to empower them and help them re-tell their own stories (Brigham, 2018).
The literature of migration entails the analysis of women's presence within literary texts which means also novels and works of creative writings such as poems, journals, travel accounts etc. and comic books with a major reference to postcolonial theory and Orientalism in regard to refugees as well.
According to Franz Fanon "Europe is literally the creation of the Third World" (Fanon, 2002, p. 99), which refers to the continuous writing from the periphery as a means for re-writing and re-inventing identity 'the experience of migration acts as a catalyst and conduit for nascent feelings, a re-conception of our sense of self and our relationships with others' (Jacobs, 2011:142).
However, the creative writing has an important share is this research mainly because of its developing and evolving use within refugee camps and by different companies in order to bridge gaps between locals and migrants as a sociological and psychological therapy.
Comic books are also a new and fast growing trend used by journalists and artists is order to depict the real experiences of migrants from different locations in the world.
Through a minimized text but expressive graphics, they attract readers with different backgrounds and different societal status to the conditions and hardships and horrors encountered by individuals, families and children seeking shelter and refuge on foreign soils, such as the outstanding digital comic Over Under Sideways Down by Karrie Fransman, and Fleeing to the Unknown created by the organization PositiveNegative.
The analysis covers all the works created by female and male migrants as well as others pertaining to local or international artists/writers with a reference to female migrants and migration.
Following a historically and pluri-disciplinary approach, the comparative analysis aims at identifying the new trends and methods used in representing female migrants and answer the below questions: is there a difference between the discourse used by migrants on both regions?
What is the difference between the usual discourse in artistic female representation and that of the migrant female representation?
How is it used and to which ends?
Is there a difference between masculine discourse in reference to women, and the feminine discourse in reference to women (self-referentiality)?
Do all those disciplines share the same objective vis-à-vis migrants' situation and image?
In the recent years, the Web has undergone a tremendous growth regarding both content and users.
This has led to an information overload problem in which people are finding it increasingly difficult to locate the right information at the right time.
Recommender systems have been developed to address this problem, by guiding users through the big ocean of information.
The recommendation approaches have multiplied and have been successfully implemented, particularly through approaches such as collaborative filtering.
However, there are still challenges and limitations that offer opportunities for new research.
Among these challenges, the design of reading recommendation systems has become a new expanding research focus following the emergence of digital libraries.
Traditionally, libraries play a passive role in interaction with users due to the lack of effective search and recommendation tools.
In this manuscript, we will study the creation of a reading recommendation system in which we'll try to exploit the possibilities of digital access to scientific information.
This thesis deals with deep learning applied to image classification tasks.
The primary motivation for the work is to make current deep learning techniques more efficient and to deal with changes in the data distribution.
We work in the broad framework of continual learning, with the aim to have in the future machine learning models that can continuously improve.
We first look at change in label space of a data set, with the data samples themselves remaining the same.
We consider a semantic label hierarchy to which the labels belong.
We investigate how we can utilise this hierarchy for obtaining improvements in models which were trained on different levels of this hierarchy.
The second and third contribution involve continual learning using a generative model.
We analyse the usability of samples from a generative model in the case of training good discriminative classifiers.
We propose techniques to improve the selection and generation of samples from a generative model.
Following this, we observe that continual learning algorithms do undergo some loss in performance when trained on several tasks sequentially.
We analyse the training dynamics in this scenario and compare with training on several tasks simultaneously.
We make observations that point to potential difficulties in the learning of models in a continual learning scenario.
Finally, we propose a new design template for convolutional networks.
This architecture leads to training of smaller models without compromising performance.
In addition the design lends itself to easy parallelisation, leading to efficient distributed training.
In conclusion, we look at two different types of continual learning scenarios.
We propose methods that lead to improvements.
Our analysis also points to greater issues, to over come which we might need changes in our current neural network training procedure.
Understanding data is the main purpose of data science and how to achieve it is one of the challenges of data science, especially when dealing with big data.
One existing technology that has been shown as particularly relevant for modeling, simulating and solving problems in complex systems are Multi-Agent Systems.
The AMAS (Adaptive Multi-Agent Systems) theory proposes to solve complex problems for which there is no known algorithmic solution by self-organization.
The cooperative behavior of the agents enables the system to self-adapt to a dynamical environment so as to maintain the system in a functionality adequate state.
In this thesis, we apply this theory to Big Data Analytics.
The aim of this thesis is to present the AMAS4BigData analytics framework based on the Adaptive Multi-agent systems technology, which uses a new data similarity metric, the Dynamics Correlation, for dynamic data relations discovery and dynamic display.
This framework is currently being applied in the neOCampus operation, the ambient campus of the University Toulouse III - Paul Sabatier.
The focus of visual content is often people.
Automatic analysis of people from visual data is therefore of great importance for numerous applications in content search, autonomous driving, surveillance, health care, and entertainment.
The goal of this thesis is to learn visual representations for human understanding.
Particular emphasis is given to two closely related areas of computer vision: human body analysis and human action recognition.
The impressive increase in the quantity of textual data makes it difficult today to analyze them without the assistance of tools.
However, a text written in natural language is unstructured data, i.e. it cannot be interpreted by a specialized computer program, without which the information in the texts remains largely under-exploited.
To accomplish this task, we propose a new approach by aligning two types of vector representations of entities that capture part of their meanings: word embeddings for text mentions and concept embeddings for concepts, designed specifically for this work.
The alignment between the two is done through supervised learning.
The developed methods have been evaluated on a reference dataset from the biological domain and they now represent the state of the art for this dataset.
These methods are integrated into a natural language processing software suite and the codes are freely shared.
The rational drug discovery process has limited success despite all the advances in understanding diseases, and technological breakthroughs.
Indeed, the process of drug development is currently estimated to require about 1.8 billion US dollars over about 13 years on average.
We focus in this thesis on statistical approaches which virtually screen a large set of compounds against a large set of proteins, which can help to identify drug candidates for known therapeutic targets, anticipate potential side effects or to suggest new therapeutic indications of known drugs.
This thesis is conceived following two lines of approaches to perform drug virtual screening: data-blinded feature-based approaches (in which molecules and proteins are numerically described based on experts'knowledge), and data-driven feature-based approaches (in which compounds and proteins numerical descriptors are learned automatically from the chemical graph and the protein sequence).
We discuss these approaches, and also propose applications of virtual screening to guide the drug discovery process.
Business processes are everywhere and, as such, we must acknowledge them.
Among all of them, hospital processes are of vital importance.
Healthcare organizations invest huge amount of efforts into keeping these processes under control, as the allowed margin of error is so slight.
This research work seeks to develop a methodology to endorse improvement of patient pathways inside healthcare organizations.
Along the former functions, the contribution of this thesis are: The DIAG methodology which, through four different states, extracts knowledge from location data; the DIAG meta-model which supports both the interpretation of location data (from raw data to usable information) and the alignment of the domain knowledge (which are used for the diagnosing methods); two process discovery algorithms which explore statistical stability in event logs, application of Statistical Process Control (SPC) for the “enhancement notation” of Process Mining; the ProDIST algorithm for measuring the distance between process models; two automatic process diagnosing methods to detect causes of structural deviations in individual cases and common processes.
The state of the art in this dissertation endorses the necessity for proposing such solutions.
A case study within this research work illustrates the applicability of the DIAG methodology and its mentioned functions and methods.
The aim of this thesis is to develop techniques for segmenting strongly-structured scenes (e.g. building images) and weakly-structured scenes (e.g. natural images).
Building images can naturally be expressed in terms of grammars and inference is performed using grammars to obtain the optimal segmentation.
However, it is difficult and time consuming to write such grammars.
To alleviate this problem, a novel method to automatically learn grammars from a given training set of image and ground-truth segmentation pairs is developed.
Experiments suggested that such learned grammars help in better and faster inference.
Surprisingly, even with out using any domain specific knowledge, we observed significant improvements in terms of performance on several benchmark datasets.
Lastly,a novel technique based on convolutional neural networks is developed to segment images without any high-level structure.
Image-adaptive filtering is performed within a CNN architecture to facilitate long-range connections.
Experiments on different large scale benchmarks show significant improvements in terms of performance
Information k-means is a new mathematical framework that extends the classical k-means criterion, using the Kullback divergence as a distortion measure.
The fragmentation criterion is an even broader extension where each signal is approximated by a combination of fragments instead of a single center.
Using the fragmentation criterion as a distortion measure, we propose a new fragmentation algorithm for digital signals, conceived as a lossy data compression scheme.
We tested the method on grey level digital images, where it was possible to label successfully translated patterns and rotated patterns.
This lets us hope that transformation invariant pattern recognition could be approached in a flexible way using a general purpose data compression criterion.
From a mathematical point of view, we derived two kinds of generalization bounds.
This explains why our syntax trees may be meaningful.
Second, combining PAC-Bayesian lemmas with the kernel trick, we proved non asymptotic dimension-free generalization bounds for the various information k-means and information fragmentation criteria we introduced.
Using a new kind of PAC-Bayesian chaining, we also proved a bound of order O(log(n/k) sqrt{k log(k)/n}).
In the Big Data era, companies are moving away from traditional data-warehouse solutions whereby expensive and timeconsumingETL (Extract, Transform, Load) processes are used, towards data lakes in order to manage their increasinglygrowing data.
Yet the stored knowledge in companies' databases, even though in the constructed data lakes, can never becomplete and up-to-date, because of the continuous production of data.
Local data sources often need to be augmentedand enriched with information coming from external data sources.
Unfortunately, the data enrichment process is one of themanual labors undertaken by experts who enrich data by adding information based on their expertise or select relevantdata sources to complete missing information.
Such work can be tedious, expensive and time-consuming, making itvery promising for automation.
We present in this work an active user-centric data integration approach to automaticallyenrich local data sources, in which the missing information is leveraged on the fly from web sources using data services.
In doing so, we take into consideration a set of user preferences such as the cost threshold and the responsetime necessary to compute the desired answers, while ensuring a good quality of the obtained results.
Our thesis research is concerned with the estimation of motion saliency in image sequences.
First, we have defined an original method to detect frames in which a salient motion is present.
For this, we propose a framework relying on a deep neural network, and on the compensation of the dominant camera motion.
Second, we have designed a method for estimating motion saliency maps.
This method requires no learning.
The motion saliency cue is obtained by an optical flow inpainting step, followed by a comparison with the initial flow.
Third, we consider the problem of trajectory saliency estimation to handle progressive saliency over time.
We have built a weakly supervised framework based on a recurrent auto-encoder that represents trajectories with latent codes.
Performance of the three methods was experimentally assessed on real video datasets.
Effective management of large amounts of information has become a challenge increasingly important for information systems.
Everyday, new information sources emerge on the web.
Someone can easily find what he wants if (s)he seeks an article, a video or a specific artist.
However,it becomes quite difficult, even impossible, to have an exploratory approach to discover new content.
Recommender systems are software tools that aim to assist humans to deal with information overload.
The work presented in this Phd thesis proposes an architecture for efficient recommendation of news.
The ontology contains a formal vocabulary modeling a view on the domain knowledge.
Carried out in collaboration with the company Actualis SARL, this work has led to the marketing of a new highly competitive product, FristECO Pro'fil.
Facing the significant complexity of the mammography area and the massive changes in its data, the need to contextualize knowledge in a formal and comprehensive modeling is becoming increasingly urgent for experts.
It is within this framework that our thesis work focuses on unifying different sources of knowledge related to the domain within a target ontological modeling.
On the one hand, there is, nowadays, several mammographic ontological modeling, where each resource has a distinct perspective area of interest.
On the other hand, the implementation of mammography acquisition systems makes available a large volume of information providing a decisive competitive knowledge.
However, these fragments of knowledge are not interoperable and they require knowledge management methodologies for being comprehensive.
In this context, we are interested on the enrichment of an existing domain ontology through the extraction and the management of new knowledge (concepts and relations) derived from two scientific currents: ontological resources and databases holding with past experiences.
Our approach integrates two knowledge mining levels: The first module is the conceptual target mammographic ontology enrichment with new concepts extracting from source ontologies.
The goal is to reduce the alignment task from two full ontologies to two reduced conceptual clusters.
The second stage consists on aligning the two hierarchical structures of both source and target ontologies.
Thirdly, the validated alignments are used to enrich the reference ontology with new concepts in order to increase the granularity of the knowledge base.
The second level of management is interested in the target mammographic ontology relational enrichment by novel relations deducted from domain database.
The latter includes medical records of mammograms collected from radiology services.
The latter is to filter and classify the rules in order to facilitate their interpretation and validation by expert, vi) The enrichment of the ontology by new associations between concepts.
This approach has been implemented and validated on real mammographic ontologies and patient data provided by Taher Sfar and Ben Arous hospitals.
In this thesis we explored the ability of magnetic models of statistical physics to extract the essential information contained in texts.
Documents are represented as sets of interacting magnetic units, the intensity of such interactions are measured and they are used to calculate quantities that are evidence of the importance of information scope.
We propose two new methods.
Several adaptations were necessary to adapt the energy calculation to a wide range of tasks such as summarisation, information retrieval, document classification and thematic segmentation.
Furthermore, and even exploratory, we propose a second algorithm that defines a grammatical coupling between types of terms to retain the important terms and produce contractions.
In this way, the compression of a sentence is the ground state of the chain of terms.
As this compression is not necessarily good, it was interesting produce variants by thermal fluctuations.
We have done simulations Metropolis Monte-Carlo with the aim of finding the ground state of this system that is analogous to spin glass.
Underlying competing mechanisms drive the evolution of gas in the interstellar medium.
Electronic relaxation from the brightest excited state has been simulated for neutral polyacenes with 2 to 7 aromatic cycles.
The results display a striking alternation in decay times of the brightest singlet state computed for polyacenes with up to 6 aromatic cycles, which is correlated with a qualitatively similar alternation of energy gaps between the brightest state and the state lying just below in energy.
The results show that the electronic population of the brightest excited state in chrysene decays an order-of-magnitude faster than that in tetracene.
This is correlated with a significant difference in energy gaps between the brightest state and the state lying just below in energy, which is consistent with the previous conclusions for polyacenes.
A last major development concerns the use of Machine Learning (ML) algorithms that have been proposed as a way to avoid most of the computationally-demanding electronic structure calculations.
It aims to assess the performance of neural networks algorithms applied to excited-state dynamics.
Electronic relaxation in neutral phenanthrene has been chosen as a test case due to the diversity of available experimental results.
Several neural networks have been trained with different parameters and their respective accuracy and efficiency analyzed.
In addition, approximate trajectory surface hopping schemes have been interfaced to ML-based PESs and gradients, resulting in non-adiabatic dynamics simulations at a negligible cost.
Various simplified hopping approaches have been compared with FSSH.
Overall, ML is found to be a highly promising tool for nanosecond-long molecular dynamics in excited states.
This PhD research opens new avenues to investigate theoretical photophysics of large molecular complexes.
Last but not least, the theoretical tools developed and implemented in deMon-Nano in a modular way can be further combined with other advanced (such as Configuration Interaction) DFTB techniques better adapted to charge-transfer states.
Moreover, NLP researchers have focused much of their effort on training NLP models on the news domain, due to the availability of training data.
However, many research works have highlighted that models trained on news fail to work efficiently on out-of-domain data, due to their lack of robustness against domain shifts.
This thesis presents a study of transfer learning approaches, through which we propose different methods to take benefit from the pre-learned knowledge from high-resourced domains to enhance the performance of neural NLP models in low-resourced settings.
Indeed, despite the importance of its valuable content for a variety of applications (e.g. public security, health monitoring, or trends highlight), this domain is still lacking in terms of annotated data.
We~present different contributions.
First, we propose two methods to transfer the knowledge encoded in the neural representations of a source model -- pretrained on large labelled datasets from the source domain -- to the target model, further adapted by a fine-tuning on few annotated examples from the target domain.
Second, we perform a series of analysis to spot the limits of the above-mentioned proposed methods.
We find that even though transfer learning enhances the performance on social media domain, a hidden negative transfer might mitigate the final gain brought by transfer learning.
Besides, an interpretive analysis of the pretrained model shows that pretrained neurons may be biased by what they have learnt from the source domain, thus struggle with learning uncommon target-specific patterns.
Third, stemming from our analysis, we propose a new adaptation scheme which augments the target model with normalised, weighted and randomly initialised neurons that beget a better adaptation while maintaining the valuable source knowledge.
Finally, we propose a model that, in addition to the pre-learned knowledge from the high-resource source-domain, takes advantage of various supervised NLP tasks.
It is expected to diagnose its early stage, Mild Cognitive Impairment (MCI), then interventions can be applied to delay the onset.
Fluorodeoxyglucose positron emission tomography (FDG-PET) is considered as a significant and effective modality to diagnose AD and the corresponding early phase since it can capture metabolic changes in the brain thereby indicating abnormal regions.
For this purpose, three independent novel methods are proposed.
Such connectivities are represented by either similarities or graph measures among regions.
Then combined with each region's properties, these features are fed into a designed ensemble classification framework to tackle problems of AD diagnosis and MCI conversion prediction.
The spatial gradient is quantified by a 2D histogram of orientation and expressed in a multiscale manner.
The results are given by integrating different scales of spatial gradients within different regions.
Such an architecture can facilitate convolutional operations, from 3D to 2D, and meanwhile consider spatial relations, which is benefited from a novel mapping layer with cuboid convolution kernels.
Experiments conducted on public dataset show that the three proposed methods can achieve significant performance and moreover, outperform most state-of-the-art approaches.
This thesis addresses the problems of phonemic variability and confusability from the pronunciation modeling perspective for an automatic speech recognition (ASR) system.
In particular, several research directions are investigated.
Since the addition of alternative pronunciation may introduce homophones (or close homophones), there is an increase of the confusability of the system.
A novel measure of this confusability is proposed to analyze it and study its relation with the ASR performance.
This pronunciation confusability is higher if pronunciation probabilities are not provided and can potentially severely degrade the ASR performance.
It should, thus, be taken into account during pronunciation generation.
Discriminative training approaches are, then, investigated to train the weights of a phoneme confusion model that allows alternative ways of pronouncing a term counterbalancing the phonemic confusability problem.
The objective function to optimize is chosen to correspond to the performance measure of the particular task.
In this thesis, two tasks are investigated, the ASR task and the KeywordSpotting (KWS) task.
For experiments conducted on KWS, the Figure of Merit (FOM), a KWS performance measure, is directly maximized.
The present study is a terminological analysis of the financial domain, in a natural language processing perspective.
The derived and compounded forms of a corpus are analysed, emphasis being put on the syntactic and semantic relations between their elements, in order to pick out the constituents that are most productive in the economical language.
The research also includes the idiomatic and recurrent expressions of the corpus.
As a conclusion, korean and french economical terms are contrasted, so as to supply the translater with an editing and translation toolbox.
Vibrational spectroscopy (VS) relates to the specific optical techniques of infrared and Raman spectroscopy (RS).
These techniques probe molecular vibrations of the sample when light interacts with it, which present 'fingerprints' of the global biochemistry.
Both techniques hold great promise in disease diagnostics, especially with 'liquid biopsies' for biofluids.
This study developed bio-spectroscopic methodologies to query the serum biochemistry towards rapid diagnosis and detection of diseases.
Beyond proof of concept with investigations in to preanalytical variation (which proved no effect is seen on the serum profile) via serum freeze-thawing and environmental drying, three diagnostic studies were sought; from patient cases, i.e., hepatocellular carcinoma from cirrhotic sera, cirrhosis from fibrotic sera, and varying degrees of gliomas from brain tumour sera.
Throughout, a suite of FTIR and Raman spectroscopy techniques were employed/developed, such serum ATR-FTIR, HT-FTIR (high throughput screening), Raman microspectroscopy on liquid and dried human sera, and Raman microspectroscopy on liquid sera.
Advanced multivariate analysis and chemometric approaches were employed such as PCA, HCA, PLS-DA, forward LDA, radial basis function SVM-LOOCV, Random forest classifiers, all towards developing a robust disease classifier.
Across all diagnostic studies, results showed moderate-good diagnostic ability with one method succeeding the other in various cases.
It was shown that spectroscopy combined with advanced chemometric methods can provide a good adjunct to clinical screening settings, such as point-of-care areas.
This thesis considers the role of lexical cohesion in various approaches of discourse analysis.
Two main hypotheses are studied:
- distributional analysis, which allows to bring together lexical units based on the syntactic contexts they share, highlights diverse semantic relations which can be employed in the detection of lexical cohesion in texts;
- lexical cues are involved in discourse signalization and can be used both at a local level (identification of rhetorical relations between elementary discourse units) and at a global level (detection or characterization of higher level segments).
In reference to the first hypothesis, we show that a distributional resource is strongly relevant in the analysis of a wide panel of relations having lexical cohesion roles in texts.
We introduce projection and filtering methods for this distributional resource.
In reference to the second hypothesis, we provide a series of outlooks showing the improvement brought by careful consideration of lexical cohesion in a large panel of settings within the study of textual organisation and its automatic detection: thematic segmentation of texts, enumerative structures characterization, study of the correlation between lexicon and the rhetorical structure of discourse, and finally detection of realisations of a specific discourse relation, the Elaboration relation.
Despite the development of syndromic surveillance and of modern decision making methods, outbreak surveillance within hospitals remains mainly manual.
Yet, the use of detection algorithms could improve surveillance efficiency, broaden its scope and overall reduce the time spent to accomplish this task.
However, the methodological quality of the evaluations published to this date is too low to conclude about the real utility of these tools.
We propose in this memoir several key elements of an adequate methodological framework for early outbreak detection, as well as a first data set containing labelled potential outbreaks.
This data set can be used by researchers to develop, evaluate and compare detection algorithms.
However, as research on surveillance systems and healthcare artificial intelligence tools has shown, implementing these tools can be difficult, and the results observed in real life can be quite different from those of the development phase.
It is therefore necessary to use alternative indicators and evaluation methods to assess the real utility of these tools in real life, measuring for example their acceptability, ease of use, costs and impacts on daily practices.
In the context of the aging population, the aim of this thesis is to include in the living environment of the elderly people an automatic speech recognition (ASR) system, which can recognize calls to alert the emergency services.
The acoustic models of ASR systems are mostly learned with non-elderly speech, delivered in a neutral way, and read.
However, in our context, we are far from these ideal conditions (aging and expressive voice).
From these corpora, a study on the differences between young and old voices, and between neutral and emotional voice permit to develop an ASR system adapted to the task.
This system was then evaluated on data recorded during an experiment in realistic situation, including falls played by volunteers.
The acoustic-to-articulatory inversion of speech consist in the recovery of the vocal tract shape from the speech signal.
This problem is tackled with an analysis-by-synthesis method depending on a physical model of speech production controlled by a small number of parameters describing the vocal tract shape: the jaw opening, the shape and the position of the tongue and the position of lips and larynx.
In order to approach the geometry of the speaker, the articulatory model is built with articulatory contours from cineradiographic images of the sagittal view of the vocal tract.
This articulatory synthesizer allows us to create a table made up with couples associating a articulatory vector with the corresponding acoustic vector.
The formants (resonance frequency of the vocal tract shape) are not used as acoustic vector because their extraction is not always reliable causing errors during inversion.
The cepstral coefficients are used as acoustic vector.
Moreover, the source effect and the mismatch between the speaker vocal tract and the articulatory model are considered explicitly comparing the natural spectrum with those produced by the synthesizer because we have the both signals
This thesis proposes a method for supporting flexible coordination in multi-agent systems (MASs).
In other words, we aim at influencing societies of artificial agents such that they can handle complex or evolving environments and collective goals (e.g. robots providing an emergency support capable of handling various hazards, climatic conditions, status of victims).
Towards achieving this goal, we first investigated why in human societies, for which MASs can be seen as an "artificial" counterpart, humans manage to coordinate relatively flexibly comparatively with artificial agents in MASs.
We discovered that culture is a key factor of this relative success.
Briefly, when humans share a cultural background, they manage to coordinate more flexibly because they share a common idea about what ``working together''means.
The lack raises our goal: we want to better understand how culture can be integrated within and used for coordinating artificial societies.
This goal raises the following research question: how can human-like culture be used as a tool for supporting coordination in artificial societies?
As a preliminary step for answering this question, we need first to answer this question: (how) can the influence human-like cultures be integrated within artificial societies?
In turn, this question raises a third one to be answered first: how does culture influence coordination in human societies?
As a first step, we expand general theories of culture for conceptualizing its influence in the context of coordination.
From a generic perspective, we explain that culture influences individual decisions that support matching expectations and coherent interaction patterns, leading in turn to (generally) better collective performance.
From a more specific perspective, we specify how the core acknowledged patterns of the influence of culture (e.g. cultural importance given to power status, to rules) apply in the context of coordination (e.g. culture influences the likeliness that leaders are (made) responsible for making decisions for subordinates vs. proposing alternatives).
As a second step, we study how to replicate human-like influences of culture on coordination within artificial societies.
First, since culture is grounded within individual decisions, we investigate the core culturally-sensitive decision aspects that impact the most (flexible) coordination in human societies.
We discover that values, what people consider as ``good''or ``important''(e.g. honesty, obedience, autonomy), constitute such an aspect by deeply supporting a wide range of (interaction-related) decisions.
Then, for illustrating how to replicate influence of culture within artificial societies, we build an value-sensitive agent decision architecture that can make coordination-related decisions.
Finally, we illustrate that our architecture can replicate the influence of culture on coordination through two simulations that replicate known coordination-related cultural phenomena.
As a third step, we study how human-like values can be used for supporting coordination in artificial societies.
First, we investigate the range of coordination problems for which values can offer an operational means for supporting coordination.
As in human societies, values are particularly adequate for problems with complex and dynamic environments, requiring agents to make coordination-related decisions.
Then, towards concretely implementing values, we study the technical details to consider when using values for supporting flexible coordination (e.g. how to concretely design values and integrating them within decision processes).
Psychiatry is a medical speciality that aims at providing diagnosis and treating mental disorders.
Despite internationally acknowledged criteria leading to diagnostic categories, most psychiatric disorders are syndromes with common symptoms or dimensions between these diagnostic categories.
In addition, the analysis of the prevalence and incidence of social and environmental risk factors of diseases is crucial to understand and treat them and might have significant impacts on policy decisions (therapeutic as well as the length or the cost of hospitalisation).
This overlap between diagnoses and the heterogeneity within the defined diagnoses stresses the need to improve our capability to detect, to quantify the behaviour and to model the symptoms and the social and environmental risk factors associated to psychiatric disorders.
To that end, we propose OntoPsychia, an ontology for psychiatry, divided in two modules: social and environmental factors of mental disorders and mental disorders.
OntoPsychia associated with dedicated tools will help to perform semantic research in Patient Discharges Summaries (PDS), to represent comorbidity, to reach a consensus on descriptive categories of mental disorders.
In a first step, we developed two ontological modules using two different methods.
The first proposes an analysis of PDS, while the second proposes an alignment of psychiatric classifications to meet the need for consensus.
In a second step, we have developed a methodological framework to validate the structure and semantics of ontologies.
However, multimodality remains limited in current human-computer interfaces.
For example, posture is less explored than other modalities, such as speech and facial expressions.
The postural expressions of others have a huge impact on how we situate and interpret an interaction.
Devices and interfaces for representing full-body interaction are available (e.g., Kinect and full-body avatars), but systems still lack computational models relating these modalities to spatial and emotional communicative functions.
The goal of this thesis is to lay the foundation for computational models that enable better use of posture in human-computer interaction.
How can these representations inform the design of virtual characters' postural expressions?
In our approach, we start with the manual annotation of video corpora featuring postural expressions.
We define a coding scheme for the manual annotation of posture at several levels of abstraction and for different body parts.
These representations were used for analyzing the spatial and temporal relations between postures displayed by two human interlocutors during spontaneous conversations.
Next, representations were used to inform the design of postural expressions displayed by virtual characters.
For studying postural expressions, we selected one promising, relevant component of emotions: the action tendency.
Finally, postural expressions were designed for a virtual character used in an ambient interaction system.
These postural and spatial behaviors were used to help users locate real objects in an intelligent room (iRoom).
The goal of this thesis is to show the various aspects of corpus annotation in the Arabic language.
We present our publications on corpus annotation and lexical resources creation in the Arabic language.
First, we discuss the methods, the language difficulties, the annotation guidelines, the annotation effort optimization limits and how we adapted some of the existing annotation procedures to the Arabic language.
Furthermore, we show the complementarity between the different layers of annotations.
Finally, we illustrate the importance of our work for natural language processing by illustrating some examples of resources and applications.
Since the introduction of Google's PageRank method for Web searches in the late 1990s, graph algorithms have been part of our daily lives.
In the mid 2000s, the arrival of social networks has amplified this phenomenon, creating new use-cases for these algorithms.
Relationships between entities can be of multiple types: user-user symmetric relationships for Facebook or LinkedIn, follower-followee asymmetric ones for Twitter or even user-content bipartite ones for Netflix or Amazon.
They all come with their own challenges and the applications are numerous: centrality calculus for influence measurement, node clustering for knowledge discovery, node classification for recommendation or embedding for link prediction, to name a few.
In the meantime, the context in which graph algorithms are applied has rapidly become more constrained.
On the one hand, the increasing size of the datasets with millions of entities, and sometimes billions of relationships, bounds the asymptotic complexity of the algorithms for industrial applications.
On the other hand, as these algorithms affect our daily lives, there is a growing demand for explanability and fairness in the domain of artificial intelligence in general.
For example, the European Union has published a set of ethics guidelines for trustworthy AI.
This calls for further analysis of the current models and even new ones.
This thesis provides specific answers via a novel analysis of not only standard, but also extensions, variants, and original graph algorithms.
Scalability is taken into account every step of the way.
Following what the Scikit-learn project does for standard machine learning, we deem important to make these algorithms available to as many people as possible and participate in graph mining popularization.
Therefore, we have developed an open-source software, Scikit-network, which implements and documents the algorithms in a simple and efficient way.
With this tool, we cover several areas of graph mining such as graph embedding, clustering, and semi-supervised node classification.
One of the core human abilities is that of interpreting symbols.
Notwithstanding decades of neuropsychological and neuroimaging work on the cognitive and neural substrate of semantic representations, many questions are left unanswered.
In the first part, I review the different theoretical positions and empirical findings on the cognitive and neural correlates of semantic representations.
Crucially, I propose an operational distinction between motor-perceptual dimensions (i.e., those attributes of the objects referred to by the words that are perceived through the senses) and conceptual ones (i.e., the information that is built via a complex integration of multiple perceptual features).
In the second part, I present the results of the studies I conducted in order to investigate the automaticity of retrieval, topographical organization, and temporal dynamics of motor-perceptual and conceptual dimensions of word meaning.
The results suggest that the neural substrates of different components of symbol meaning can be dissociated in terms of localization and of the feature of the signal encoding them, while sharing a similar temporal evolution.
Continuous word representations (word type embeddings) are at the basis of most modern natural language processing systems, providing competitive results particularly when input to deep learning models.
However, important questions are raised concerning the challenges they face in dealing with the complex natural language phenomena and regarding their ability to capture natural language variability.
To better handle complex language phenomena, much work investigated fine-tuning the generic word type embeddings or creating specialized embeddings that satisfy particular linguistic constraints.
While this can help distinguish semantic similarity from other types of semantic relatedness, it may not suffice to model certain types of relations between texts such as the logical relations of entailment or contradiction.
The first part of the thesis investigates encoding the notion of entailment within a vector space by enforcing information inclusion, using an approximation to logical entailment of binary vectors.
We further develop entailment operators and show how the proposed framework can be used to reinterpret an existing distributional semantic model.
Evaluations are provided on hyponymy detection as an instance of lexical entailment.
Another challenge concerns the variability of natural language and the necessity to disambiguate the meaning of lexical units depending on the context they appear in.
For this, generic word type embeddings fall short of being successful by themselves, with different architectures being typically employed on top to help the disambiguation.
As type embeddings are constructed from and reflect co-occurrence statistics over large corpora, they provide one single representation for a given word, regardless of its potentially numerous meanings.
Furthermore, even given monosemous words, type embeddings do not distinguish between the different usages of a word depending on its context.
In that sense, one could question if it is possible to directly leverage available linguistic information provided by the context of a word to adjust its representation.
Would such information be of use to create an enriched representation of the word in its context?
And if so, can information of syntactic nature aid in the process or is local context sufficient?
In the second part of the thesis, we investigate one possible way to incorporate contextual knowledge into the word representations themselves, leveraging information from the sentence dependency parse along with local vicinity information.
We propose syntax-aware token embeddings (SATokE) that capture specific linguistic information, encoding the structure of the sentence from a dependency point of view in their representations.
This enables moving from generic type embeddings (context-invariant) to specific token embeddings (context-aware).
While syntax was previously considered for building type representations, its benefits may have not been fully assessed beyond models that harvest such syntactical information from large corpora.
The obtained token representations are evaluated on natural language understanding tasks typically considered in the literature: sentiment classification, paraphrase detection, textual entailment and discourse analysis.
We empirically demonstrate the superiority of the token representations compared to popular distributional representations of words and to other token embeddings proposed in the literature.
The work proposed in the current thesis aims at contributing to research in the space of modelling complex phenomena such as entailment as well as tackling language variability through the proposal of contextualized token embeddings.
This work introduces the Kabyle language to the field of Natural Language Processing by giving it a database for the NooJ software that allows the automatic recognition of linguistic units in a written corpus.
We have divided the work in four parts.
The first part is the place to give a snapshot on the history of formal linguistics, to present the field of NLP and the NooJ software and the linguistic units that have been treated.
The second part is devoted to the description of the process that has been followed for the treatment and the integration of Kabyle verbs in NooJ.
We have built a dictionary that contains 4508 entries and 8762 derived components and some models of flexion for each type which have been linked with each entry.
In the third part, we have explained the processing of nouns and other units.
We have built, for the nouns, a dictionary (3508 entries, 501 derived components) that have been linked to the models of flexion and for the other units (870 entries including adverbs, prepositions, conjunctions, interrogatives, personal pronouns, etc.).
The second and third part are completed by examples of applications on a text, this procedure has allowed us to show with various sort of annotations the ambiguities.
Regarding the last part we have devoted it to ambiguities, after having identified a list of various types of amalgams, we have tried to show, with the help of some examples of syntactic grammars, some of the tools used by NooJ for disambiguation.
Introduced as part of the last Message Understanding Conferences dedicated to information extraction, Named Entity extraction is a well-studied task in Natural Language Processing.
Following this success, named entity treatment is moving towards new research prospects with, among others, disambiguation,and fined-grained annotation.
However, this new challenges make even more crucial the question of named entity definition, which was not much discussed until now.
Two main lines were explored during this PhD project: first we tried to propose a definition of named entities and then we experimented disambiguation methods.
After a presentation and a state of the art of the named entity recognition task, we had to examine, from a methodological point of view, how to tackle the question of the definition of named entities.
Our approach led us to study, firstly, the linguistic side, with proper names and definite descriptions and, secondly, the Computing side, this development aiming at, finally, proposing a named entity definition that takes into account language aspects but also informatic Systems capacities and requirements.
The continuation of the dissertation is about more experimental works, with a presentation of experiments about fined-grained named entity annotation and metonymy resolution methods.
A research on the etymology and history of linguistics terminology will provide an important gain in historical and etymological knowledge.
This gain concerning the description of particular scientific terms has broader implications: improved knowledge on the coining process of the linguistics vocabulary, as well as indications about the possible consequences that a massive progress of the description of an important portion of the internationalisms could have on the etymological sub-discipline itself.
This thesis also contributes to a methodological progress in a weak area of the etymological science, a progress all the more significant as it is crucial to our understanding of the current functioning of lexical creation in French.
An outstanding characteristic of this thesis is that it lays at the intersection of several fields, covering lexicology (and lexicography, since it encompasses a section devoted to lexicographical entries), terminology (since the subject of these articles is linguistic terms) and etymology.
Furthermore, the philological aspect of this research is important because the work on the beginning of the existence of terms led us to analyze various types of texts belonging to different periods.
We then exploit the new data offered by the lexicographical entries in order to provide a synthetic view of the constitution of the French linguistics terminology.
The main goal of this thesis is the development of event-based algorithms for visual detection and tracking.
This algorithms are specifically designed to work on the output of neuromorphic event-based cameras.
This type of cameras are a new type of bioinspired sensors, whose principle of operation is based on the functioning of the retina: every pixel is independent and generates events asynchronously when a sufficient amount of change is detected in the luminance at the corresponding position on the focal plane.
This new way of encoding visual information calls for new processing methods.
The resulting virtual mechanical system is simulated with every incoming event.
Next, a line and segment detection algorithm is introduced, which can be employed as an event-based low level feature.
Two event-based methods for 3D pose estimation are then presented.
The first of these 3D algorithms is based on the assumption that the current estimation is close to the true pose of the object, and it consequently requires a manual initialization step.
The second of the 3D methods is designed to overcome this limitation.
All the presented methods update the estimated position (2D or 3D) of the tracked object with every incoming event.
This thesis investigates acoustic-to-articulatory inversion, i.e. recovering articulatory movements from the speech signal.
In this work, we present an important evolution of codebooks methods, i.e. methods using acoustic-articulatory tuples precomputed using an acoustic synthesis model.
Apart from the inversion method, we present the introduction of two types of constraints: generic phonetic constraints, derived from the analysis by human experts of articulatory invariance for vowels, and visual constraints, i.e. constraints derived automatically from a video signal, in our case a stereo video signal, thus allowing us to perform multimodal inversion.
Recent years have witnessed a growing renewed interest in the use of graphs as a reliable means for representing and modeling data.
Thereby, graphs enable to ensure efficiency in various fields of computer science, especially for massive data where graphs arise as a promising alternative to relational databases for big data modeling.
In this regard, querying data graph proves to be a crucial task to explore the knowledge in these datasets.
In this dissertation, we investigate two main problems.
In the first part we address the problem of detecting patterns in larger graphs, called the top-k graph pattern matching problem.
We introduce a new graph pattern matching model named Relaxed Graph Simulation (RGS), to identify significant matches and to avoid the empty-set answer problem.
We formalize and study the top-k matching problem based on two classes of functions, relevance and diversity, for ranking the matches according to the RGS model.
We also consider the diversified top-k matching problem, and we propose a diversification function to balance relevance and diversity.
Moreover, we provide efficient algorithms based on optimization strategies to compute the top-k and the diversified top-k matches according to the proposed model.
The proposed approach is optimal in terms of search time and flexible in terms of applicability.
The analyze of the time complexity of the proposed algorithms and the extensive experiments on real-life datasets demonstrate both the effectiveness and the efficiency of these approaches.
In the second part, we tackle the problem of graph querying using aggregated search paradigm.
Firstly, we give the motivation behind the use of such a paradigm, and we explain the potential benefits compared to traditional querying approaches.
Furthermore, we propose a new method for aggregated tree search, based on approximate tree matching algorithm on several tree fragments, that aims to build, the extent possible, a coherent and complete answer by combining several results.
The proposed solutions are shown to be efficient in terms of relevance and quality on different real-life datasets
Multi-objective problems arise in many real world scenarios where one has to find an optimal solution considering the trade-off between different competing objectives.
Having motivated our work, we study two multi-objective learning tasks in detail.
We study the problem as finding an optimal trade-off between different classification errors, and propose an algorithm based on cost-sensitive classification.
In the second task, we study the problem of diverse ranking in information retrieval tasks, in particular recommender systems.
Secondly, we built a corpus of translations done by domain experts and we studied it to reinforce the analysis.
The most obvious difference are the use of machine translation (MT) and the production context.
By looking at current MT technologies, it appears that they can either post-edit the texts that are in target language, after the translation process or pre-edit the texts that are in source language, before the translation process.
We propose to take advantage of the unprecedented situation of having the "writer" and the "translator" working together, to use the writer expertise during the translation process by creating a new MT feature that allow editing during the process.
This thesis deals with environmental scene analysis, the auditory result of mixing separate but concurrent emitting sources.
The sound environment is a complex object, which opens the field of possible research beyond the specific areas that are speech or music.
For a person to make sense of its sonic environment, the involved process relies on both the perceived data and its context.
For each experiment, one must be, as much as possible,in control of the evaluated stimuli, whether the field of investigation is perception or machine learning.
Nevertheless, the sound environment needs to be studied in an ecological framework, using real recordings of sounds as stimuli rather than synthetic pure tones.
We therefore propose a model of sound scenes allowing us to simulate complex sound environments from isolated sound recordings.
The high level structural properties of the simulated scenes -- such as the type of sources, their sound levels or the event density -- are set by the experimenter.
The usefulness of the proposed model is assessed on two areas of investigation.
The first is related to the soundscape perception issue, where the model is used to propose an innovative experimental protocol to study pleasantness perception of urban soundscape.
The second tackles the major issue of evaluation in machine listening, for which we consider simulated data in order to powerfully assess the generalization capacities of automatic sound event detection systems.
This work focuses on the non-standard linguistic usages of less-literate writers during the First World War, based on their private correspondence.
As of 2017, we live in a data-driven world where data-intensive applications are bringing fundamental improvements to our lives in many different areas such as business, science, health care and security.
This has boosted the growth of the data volumes (i.e., deluge of Big Data).
To extract useful information from this huge amount of data, different data processing frameworks have been emerging such as MapReduce, Hadoop, and Spark.
Traditionally, these frameworks run on largescale platforms (i.e., HPC systems and clouds) to leverage their computation and storage power.
Usually, these largescale platforms are used concurrently by multiple users and multiple applications with the goal of better utilization of resources.
Though benefits of sharing these platforms exist, several challenges are raised when sharing these large-scale platforms, among which I/O and failure management are the major ones that can impact efficient data processing.
To this end, we first focus on I/O related performance bottlenecks for Big Data applications on HPC systems.
We start by characterizing the performance of Big Data applications on these systems.
We identify I/O interference and latency as the major performance bottlenecks.
Next, we zoom in on I/O interference problem to further understand the root causes of this phenomenon.
Then, we propose an I/O management scheme to mitigate the high latencies that Big Data applications may encounter on HPC systems.
Second, we focus on the impact of failures on the performance of Big Data applications by studying failure handling in shared MapReduce clusters.
We introduce a failure-aware scheduler which enables fast failure recovery while optimizing data locality thus improving the application performance.
The thesis takes part within works on the automated interpretation of complex manifestations of discourse facts that are not graspable by the usual methods of discourse analysis.
Based on political interviews transcripts, the study focuses on the linguistic mechanism of "nomination".
As part of the ANR TALAD project, the aim is to do theoretical and descriptive works on discourse facts to carry out prototyping, implementation, experimentation and validation of approaches for the detection and the characterization of nominations.
In particular, the purpose is to study the outputs of entity and co-reference recognition systems to determine their contribution to the nomination detection.
One of the challenges of the thesis is to propose a classification system for the Reticular company - which is interested in the qualification of political actors as "doctrine designers" or "influencers".
This thesis focus on the automatic translation approaches for User-Generated Contents (e.g. texts found in social media).
It addresses and explores the state-of-the-art methods on automatic translation to cope with the two key scientific challenges: 1) the need to overcome limitations of a phrase-by-phrase translation using a larger context information, and 2) the very noisy characteristics of such texts compared to the canonical and edited contents usually used in NLP, the former being extremely diverse, abounding in abbreviations, orthographic or typographic mistakes and grammatical errors.
This thesis comes within the framework of information retrieval on the Web with the help of methods inspired by the computational linguistics researchs of LaLICC laboratory.
The purpose of our work is to develop a tool that allows to assist in an interactive way, during an information retrieval session, a user who wants to collect some information available on the Web on a given notion or topic.
The main idea which is realized through this tool called RAP (Research, Analyse, Propose), consists in foàcusing the search according to one or more points of view which allow to satisfy, in a gradual way, the user information needs.
Conceptually, a great part of our work consisted to study how to characterize the user need notion that constitute the intuitive foundation on which rely the notion of points de view.
In this order, the linguistic knowledge we use allows as to not see the need notion as necessarily related to a particular community of users.
Our reflexions led us to pose the elementary or complex informational need notions as a theoritical framework of our research.
To these needs correspond the points of view that a user can select to focus his search.
Technically, direct the search according to a point of view means that we reformulate the user query by integrating there the linguistics markers relating to the chosen point of view, for example that of Causality or that of Quotation.
The purpose of this reformulation is in one part, to reduce significantly the noise, and in the other part to target Web pages having rich semantic contents.
The realization of the points of view by this reformulation technique implies the use of linguistics markers resulting from the team LaLICC works on the semantic filtering of the texts.
Each class of these markers relating to the selected point of view intervenes in the process of user query reformulation through the reformulation technique that we developed; then, in the part of extraction of the paragraphs or textual segments of the document where signs of the chosen point of view are detected, thus to help the user to make a better choice of Web pages among the resulted pages computed by the search engine (AltaVista in our case).
The whole of the processes was realized by the construction of the RAP tool written in Java and including a convivial interface, in which, 27 points of view resulting from the various approaches of the sixth principal points of view: Causality, Descriptive Relations, Quotation, Theme/Position, Problem/Solution and Actors.
Probabilistic parsing is one of the most attractive research areas in natural language processing.
Current successful probabilistic parsers require large treebanks which are difficult, time consuming, and expensive to produce.
Therefore, we focused our attention on less-supervised approaches.
We suggested two categories of solution: active learning and semi-supervised algorithm.
Active learning strategies allow one to select the most informative samples for annotation.
Most existing active learning strategies for parsing rely on selecting uncertain sentences for annotation.
We show in our research, on four different languages (French, English, Persian, and Arabic), that selecting full sentences is not an optimal solution and propose a way to select only subparts of sentences.
As our experiments have shown, some parts of the sentences do not contain any useful information for training a parser, and focusing on uncertain subparts of the sentences is a more effective solution in active learning.
We are interested in multimodal human-computer communication systems that use the following modes: speech, gesture and vision.
The user communicates with the system by oral utterance in natural language and/or by gesture.
The user's request contains his/her goal and the designation of objects (referents) required to the goal realisation.
The system should identify in a precise and non ambiguous way the designated objects.
The main aspects of the realisation consist in modeling the natural language processing in speech environment, the gesture processing and the visual context (visual salience use) while taking into account the difficulties in multimodal context: speech recognition errors, natural language ambiguity, gesture imprecision due to the user performance, designation ambiguity due to the perception of the displayed objects or to the display topology.
To complete the interpretation of the user's request, we propose a method for fusion/verification of modalities processing results to find the designated objects by the user.
Automatic emotion recognition in speech is a relatively recent research subject in the field of natural language processing considering that the subject has been proposed for the first time about ten years ago.
This subject is nowadays the object of much attention, not only in academia but also in industry, thank to the increased models performance and system reliability.
The first studies were based on acted data and non spontaneous speech.
Up until now, most experiments carried out by the research community on emotions were realized pre-segmented sequences and with a unique speaker and not on spontaneous speech with several speaker.
With this methodology the models built on acted data are hardly usable on data collected in natural context
The studies we present in this thesis are based on call center's conversation with about 1620 hours of dialogs and with at least two human speakers (a commercial agent and a client) for each conversation.
Our aim is the detection, via emotional expression, of the client satisfaction.
In the first part of this work we present the results we obtained from models using only acoustic or linguistic features for emotion detection.
We show that to obtain correct results an approach taking into account only one of these features type is not enough.
To overcome this problem we propose the combination of three type of features (acoustic, lexical and semantic).
We show that the use of models with features fusion allows higher score for the recognition step in all case compared to the model using only acoustic features. This gain is also obtained if we use an approach without manual pre-processing (automatic segmentation of conversation, transcriptions based on automatic speech recognition).
In the second part of our study we notice that even if models based on features combination are relevant for emotion detection the amount of data we use in our training set is too small if we used it on large amount of data test.
To overcome this problem we propose a new method to automatically complete training set with new data.
These additions allow us to double the amount of data in our training set and increase emotion recognition rate compare to the non-enrich models.
Finally, in the last part we choose to evaluate our method on entire conversation and not only on conversations turns as in most studies.
To define the classification of a dialog we use models built on the previous steps of this works and we add two new features group:
i) structural features including information like the length of the conversation, the proportion of speech for each speaker in the dialog
ii) dialogic features including informations like the topic of a conversation and a new concept we call “affective implication”.
The aim of the affective implication is to represent the impact of the current speaker's emotional production on the other speakers.
We show that if we combined all information we can obtain results close to those of humans.
The vision of pervasive computing of building interactive smart spaces in the physical environment is gradually heading from the research domain to reality.
Computing capacity is moving beyond personal computers to many day-to-day devices, and these devices become, thanks to multiple interfaces, capable of communicating directly with one another or of connecting to the Internet.
In this thesis, we are interested in a kind of pervasive computing environment that forms what we call an Intermittently Connected Hybrid Network (ICHN).
An ICHN is a network composed of two parts: a fixed and a mobile part.
The fixed part is formed of some fixed infostations (potentially connected together with some fixed infrastructure, typically the Internet).
The mobile part, on the other hand, is formed of smartphones carried by nomadic people.
While the fixed part is mainly stable, the mobile part is considered challenging and form what is called an Opportunistic Network.
Indeed, relying on short-range communication means coupled with the free movements of people and radio interferences lead to frequent disconnections.
To perform a network-wide communication, the "store, carry and forward" approach is usually applied.
With this approach, a message can be stored temporarily on a device, in order to be forwarded later when circumstances permit.
Any device can opportunistically be used as an intermediate relay to facilitate the propagation of a message from one part of the network to another.
In this context, the provisioning of pervasive services is particularly challenging, and requires revisiting important components of the provisioning process, such as performing pervasive service discovery and invocation with the presence of connectivity disruptions and absence of both end-to-end paths and access continuity due to user mobility.
This thesis addresses the problems of providing network-wide service provisioning in ICHNs and proposes solutions for pervasive service discovery, invocation and access continuity.
Concerning service discovery challenge, we propose TAO-DIS, a service discovery protocol that performs an automatic and fast service discovery mechanism.
TAO-DIS takes into account the hybrid nature of an ICHN and that the majority of services are provided by infostations.
It permits mobile users to discover all the services in the surrounding environment in order to identify and choose the most convenient ones.
To allow users to interact with the discovered services, we introduce TAO-INV.
TAO-INV is a service invocation protocol specifically designed for ICHNs.
It relies on a set of heuristics and mechanisms that ensures performing efficient routing of messages (both service requests and responses) between fixed infostations and mobile clients while preserving both low values of overhead and round trip delays.
Since some infostations in the network might be connected, we propose a soft handover mechanism that modifies the invocation process in order to reduce service delivery delays.
This handover mechanism takes into consideration the opportunistic nature of the mobile part of the ICHN.
We have performed various experiments to evaluate our solutions and compare them with other protocols designed for ad hoc and opportunistic networks.
The obtained results tend to prove that our solutions outperform these protocols, namely thanks to the optimizations we have developed for ICHNs.
In our opinion, building specialized protocols that benefit from techniques specifically designed for ICHNs is an approach that should be pursued, in complement with research works on general-purpose communication protocols
This thesis presents new results in three fundamental areas of public-key cryptography: integrity, authentication and confidentiality.
In each case we design new primitives or improve the features of existing ones.
The first chapter, dealing with integrity, introduces a non-interactive proof for proper RSA public key generation and a contract co-signature protocol in which a breach in fairness provides the victim with transferable evidence against the cheater.
The second chapter, focusing on authentication, shows how to use time measurements to shorten zeroknowledge commitments and how to exploit bias in zero-knowledge challenges to gain efficiency.
This chapter also generalizes Fiat-Shamir into a one-to-many protocol and describes a very sophisticated smart card fraud illustrating what can happen when authentication protocols are wrongly designed.
The third chapter is devoted to confidentiality.
We propose public-key cryptosystems where traditional hardness assumptions are replaced by refinements of the CAPTCHA concept and explore the adaptation of honey encryption to natural language messages.
This research work is positioned in the continuity of the thesis [21] carried out in the LRI laboratory.
As part of this thesis, we will establish methods which can extract and analyse the social networks datas but also from other data sources with a view to their reuse in the Octopeek platform.
Nevertheless, the realization of such a system remains a scientific challenge which have to take into account large data sets, the time complexity of the reseach process and the language semantic in order to provide the best answers.
Indeed, for each information sought, it will be necessary to find the method, the optimized algorithm which takes into account the speed of calculation, and the associated scoring.
We will implement algorithms for a on the fly usage and on several people in parallel.
Our assumption in this research project is that scenarios and personas can be used as support methods to handle above-mentioned obstacles.
An experiment is designed and conducted in a laboratory environment in order to test this assumption.
Some qualitative and quantitative indicators are proposed to evaluate these impacts.
Based on the analysis of seven observed collaborative design sessions, the findings of research study are discussed.
However, the findings were not adequate to conclude that they have a significant impact on the perspective clarification and convergence.
Hence, the main contribution of this research lies from one part in the evaluation of the impacts of these methods in requirement elicitation activity.
This dissertation is a contribution to both the professional branch of English for Specific Purposes and English as a lingua franca.
The research takes place in the corporate world where employees exchange emails during the course of their professional routines.
In this context, English is considered as an international language and, in the situations where employees are natives of other languages than English, the lingua franca.
In the first part, the four fundamental concepts used in this study are introduced: (1) English as an international language, (2) register, (3) phraseology, and (4) professional discourse.
The second part deals with the methodological approach which consists in building a corpus comprising 500 messages extracted from a larger database which was collected while we did fieldwork in the corporate world.
The corpus is defined by the four following linguistic situations: 1. native professionals writing to native professionals 2. native professionals writing to non-native professionals 3. non-native professionals writing to native professionals 4. non-native professionals writing to non-native professionals
It is also defined by four professional situations, namely: 1. selling and purchasing 2. team management 3. human resources management 4. technical problem solving
The situations are then used to conduct a corpus-based, register analysis alongthree linguistic and paralinguistic dimensions.
Each dimension seeks to characterise professional emails as a form of minimal, embedded, and interpersonal discourse.
More generally, this thesis explores and challenges the solidity of traditional norms and that of the concept of discourse community by presenting the English used in global, ephemeral and professional networks as a fluid variety.
In this thesis, we investigate a new paradigm for text-to-speech synthesis (TTS) allowing to deliver synthetic speech while the text is being inputted: incremental text-to-speech synthesis.
Contrary to conventional TTS systems, that trigger the synthesis after a whole sentence has been typed down, incremental TTS devices deliver speech in a "piece-meal" fashion (i.e. word after word) while aiming at preserving the speech quality achievable by conventional TTS systems.
By reducing the waiting time between two speech outputs while maintaining a good speech quality, such a system should improve the quality of the interaction for speech-impaired people using TTS devices to express themselves.
The main challenge brought by incremental TTS is the synthesis of a word, or of a group of words, with the same segmental and supra-segmental quality as conventional TTS, but without knowing the end of the sentence to be synthesized.
In this thesis, we propose to adapt the two main modules (natural language processing and speech synthesis) of a TTS system to the incremental paradigm.
For the natural language processing module, we focused on part-of-speech tagging, which is a key step for phonetization and prosody generation.
We propose an "adaptive latency algorithm" for part-of-speech tagging, that estimates if the inferred part-of-speech for a given word (based on the n-gram approach) is likely to change when adding one or several words.
If the Part-of-speech is considered as likely to change, the synthesis of the word is delayed.
In the other case, the word may be synthesized without risking to alter the segmental or supra-segmental quality of the synthetic speech.
The proposed method is based on a set of binary decision trees trained over a large corpus of text.
We achieve 92.5% precision for the incremental part-of-speech tagging task and a mean delay of 1.4 words.
Objective and subjective evaluation show that the proposed method outperforms the baselines for French.
Finally, we describe a prototype developed during this thesis implementing the proposed solution for incremental part-of-speech tagging and speech synthesis.
A perceptive evaluation of the word grouping derived from the proposed adaptive latency algorithm as well as the segmental quality of the synthetic speech tends to show that our system reaches a good trade-off between reactivity (minimizing the waiting time between the input and the synthesis of a word) and speech quality (both at segmental and supra-segmental levels).
Although communication between languages has without question been made easier thanks to Machine Translation (MT), especially given the recent advances in statistical MT systems, the quality of the translations produced by MT systems is still well below the translation quality that can be obtained through human translation.
This gap is partly due to the way in which statistical MT systems operate; the types of models that can be used are limited because of the need to construct and evaluate a great number of partial hypotheses to produce a complete translation hypothesis.
Using these features in a reranking framework does often provide a better modelization of certain aspects of the translation.
However, this approach is inherently limited: reranked hypothesis lists represent only a small portion of the decoder's search space, tend to contain hypotheses that vary little between each other and which were obtained with features that may be very different from the complex features to be used during reranking.
In this work, we put forward the hypothesis that such translation hypothesis lists are poorly adapted for exploiting the full potential of complex features.
The aim of this thesis is to establish new and better methods of exploiting such features to improve translations produced by statistical MT systems.
Our first contribution is a rewriting system guided by complex features.
Sequences of rewriting operations, applied to hypotheses obtained by a reranking framework that uses the same features, allow us to obtain a substantial improvement in translation quality.
The originality of our second contribution lies in the construction of hypothesis lists with a multi-pass decoding that exploits information derived from the evaluation of previously translated hypotheses, using a set of complex features.
Our system is therefore capable of producing more diverse hypothesis lists, which are globally of a better quality and which are better adapted to a reranking step with complex features.
What is more, our forementioned rewriting system enables us to further improve the hypotheses produced with our multi-pass decoding approach.
Our third contribution is based on the simulation of an ideal information type, designed to perfectly identify the correct fragments of a translation hypothesis.
This perfect information gives us an indication of the best attainable performance with the systems described in our first two contributions, in the case where the complex features are able to modelize the translation perfectly.
Through this approach, we also introduce a novel form of interactive translation, coined "pre-post-editing", under a very simplified form: a statistical MT system produces its best translation hypothesis, then a human indicates which fragments of the hypothesis are correct, and this new information is then used during a new decoding pass to find a new best translation.
Automatic speech recognition systems are still vulnerable to non native accents.
Their precision drastically drops as non native speakers commit acoustic and pronunciation errors.
We have proposed a new approach for non native ASR based on pronunciation modelling.
This approach uses a non native speech corpus and two sets of acoustic models: the first set stands for the canoncial target language accent and the second stands for the non native accent.
It is an automated approach that associates, to each phoneme from the first set of models, one or several non native pronunciations each expressed as a sequence of phonemes from the second set of models.
These pronunciations are taken into account through adding new HMM paths to the models of each phoneme from the first set of models.
We have developed a new approach for the automatic detection of the mother tong of non native speakers.
This approach is based on the detection of discriminative phoneme sequences, and is used as a first step of the ASP process.
Besides, we have developed an approach of automatic phoneme-grapheme alignment in order to take into account the graphemic constraints within the non native pronunciation modeling.
We have studied some fast likelihood computation techinques, and we have proposed three novel appraoches that aim at enhancing likelihood computation speed without harming ASR precision.
Current recommender systems need to recommend items that are relevant to users (exploitation), but they must also be able to continuously obtain new information about items and users (exploration).
This is the exploration / exploitation dilemma.
Such an environment is part of what is called "reinforcement learning".
In the statistical literature, bandit strategies are known to provide solutions to this dilemma.
The contributions of this multidisciplinary thesis the adaptation of these strategies to deal with some problems of the recommendation systems, such as the recommendation of several items simultaneously, taking into account the aging of the popularity of an items or the recommendation in real time.
In numerous fields such as machine learning, operational research or circuit design, a task is modeled by a set of parameters to be optimized in order to take the best possible decision.
Formally, the problem amounts to minimize a function describing the desired objective with iterative algorithms.
The development of these latter depends then on the characterization of the geometry of the function or the structure of the problem.
In a first part, this thesis studies how sharpness of a function around its minimizers can be exploited by restarting classical algorithms.
Optimal schemes are presented for general convex problems.
They require however a complete description of the function that is rarely available.
Adaptive strategies are therefore developed and shown to achieve nearly optimal rates.
A specific analysis is then carried out for sparse problems that seek for compressed representation of the variables of the problem.
Their underlying conic geometry, that describes sharpness of the objective, is shown to control both the statistical performance of the problem and the efficiency of dedicated optimization methods by a single quantity.
A second part is dedicated to machine learning problems.
These perform predictive analysis of data from large set of examples.
Systematic algorithmic approaches are developed by analyzing the geometry induced by partitions of the data.
A theoretical analysis is then carried out for grouping features by analogy to sparse methods.
This thesis discusses the use of deep generative models for symbolic music generation.
We will be focused on devising interactive generative models which are able to create new creative processes through a fruitful dialogue between a human composer and a computer.
Recent advances in artificial intelligence led to the development of powerful generative models able to generate musical content without the need of human intervention.
I believe that this practice cannot be thriving in the future since the human experience and human appreciation are at the crux of the artistic production.
However, the need of both flexible and expressive tools which could enhance content creators'creativity is patent; the development and the potential of such novel A.I.-augmented computer music tools are promising.
In this manuscript, I propose novel architectures that are able to put artists back in the loop.
The proposed models share the common characteristic that they are devised so that a user can control the generated musical contents in a creative way.
In order to create a user-friendly interaction with these interactive deep generative models, user interfaces were developed.
I believe that new compositional paradigms will emerge from the possibilities offered by these enhanced controls.
The World Wide Web is a proliferating source of multimedia objects described using various natural languages.
In order to use semantic Web techniques for retrieval of such objects (images, videos, etc.), we propose a content extraction method in multilingual text collections, using one or several ontologies as parameters.
The content extraction process is used on the one hand to index multimedia objects using their textual content, and on the other to build formal requests from spontaneous user requests.
The process is based on an interlingual annotation of texts, keeping ambiguities (polysemy and segmentation) in graphs.
This first step allows using common desambiguation processes at th elevel of a pivot langage (interlingual lexemes).
Passing an ontology as a parameter of the system is done by aligning automatically its elements with the interlingual lexemes of the pivot language.
It is thus possible to use ontologies that have not been built for a specific use in a multilingual context, and to extend the set of languages and their lexical coverages without modifying the ontologies.
A demonstration software for multilingual image retrieval has been built with the proposed approach in the framework of the OMNIA ANR project, allowing to implement the proposed approaches.
It has thus been possible to evaluate the scalability and quality of annotations produiced during the retrieval process.
Even if they are uncommon, Rare Diseases (RDs) are numerous and generally sever, what makes their study important from a health-care point of view.
Few databases provide information about RDs, such as Orphanet and Orphadata.
Despite their laudable effort, they are incomplete and usually not up-to-date in comparison with what exists in the literature.
Indeed, there are millions of scientific publications about these diseases, and the number of these publications is increasing in a continuous manner.
This thesis aims at extracting information from texts and using the result of the extraction to enrich existing ontologies of the considered domain.
We studied three research directions (1) extracting relationships from text, i.e., extracting Disease-Phenotype (D-P) relationships; (2) identifying new complex entities, i.e., identifying phenotypes of a RD and (3) enriching an existing ontology on the basis of the relationship previously extracted, i.e., enriching a RD ontology.
First, we mined a collection of abstracts of scientific articles that are represented as a collection of graphs for discovering relevant pieces of biomedical knowledge.
We focused on the completion of RD description, by extracting D-P relationships.
This could find applications in automating the update process of RD databases such as Orphanet.
Accordingly, we developed an automatic approach named SPARE*, for extracting D-P relationships from PubMed abstracts, where phenotypes and RDs are annotated by a Named Entity Recognizer.
SPARE* is a hybrid approach that combines a pattern-based method, called SPARE, and a machine learning method (SVM).
It benefited both from the relatively good precision of SPARE and from the good recall of the SVM.
Second, SPARE* has been used for identifying phenotype candidates from texts.
We selected high-quality syntactic patterns that are specific for extracting D-P relationships only.
Then, these patterns are relaxed on the phenotype constraint to enable extracting phenotype candidates that are not referenced in databases or ontologies.
These candidates are verified and validated by the comparison with phenotype classes in a well-known phenotypic ontology (e.g., HPO).
This comparison relies on a compositional semantic model and a set of manually-defined mapping rules for mapping an extracted phenotype candidate to a phenotype term in the ontology.
This shows the ability of SPARE* to identify existing and potentially new RD phenotypes.
We applied SPARE* on PubMed abstracts to extract RD phenotypes that we either map to the content of Orphanet encyclopedia and Orphadata; or suggest as novel to experts for completing these two resources.
Finally, we applied pattern structures for classifying RDs and enriching an existing ontology.
First, we used SPARE* to compute the phenotype description of RDs available in Orphadata.
We propose comparing and grouping RDs in regard to their phenotypic descriptions, and this by using pattern structures.
This thesis presents a new approach to automatic prosodic boundary and prosodic structure detection based on a theoretical hierarchical representation of prosodic organization of speech in French.
We used a descriptive theory of the French prosodic System to create a rule based linguistic prosodic model suitable for the automatic treatment of spontaneous speech.
This model allows finding automatically prosodic group boundaries and structuring them hierarchically.
The prosodic structure of every phrase is thus represented in the form of a prosodic tree.
This representation proved to be efficient for automatic processing of continuous speech in French.
The resulting prosodic segmentation was compared to manual prosodic segmentation.
Prosodic structure accuracy was also verified manually by an expert.
We applied our model to different kinds of continuous spontaneous speech data with different phonemic and lexical segmentations: manual segmentation and different kinds of automatic segmentations.
In particular, the application of our prosodic model to the output of a speech recognition System showed a satisfactory performance.
There also bas been established a correlation between the level of the prosodic tree node and the boundary detection accuracy.
Thus, it is possible to improve the precision of boundary detection by attributing a degree of confidence to the boundary according to its level in prosodic tree.
With the development of capture devices and the Internet, people access to an increasing amount of images.
Assessing visual aesthetics has important applications in several domains, from image retrieval and recommendation to enhancement.
Image aesthetic quality assessment aims at determining how beautiful an image looks to human observers.
Many problems in this field are not studied well, including the subjectivity of aesthetic quality assessment, explanation of aesthetics and the human-annotated data collection.
Conventional image aesthetic quality prediction aims at predicting the average score or aesthetic class of a picture.
However, the aesthetic prediction is intrinsically subjective, and images with similar mean aesthetic scores/class might display very different levels of consensus by human raters.
Recent work has dealt with aesthetic subjectivity by predicting the distribution of human scores, but predicting the distribution is not directly interpretable in terms of subjectivity, and might be sub-optimal compared to directly estimating subjectivity descriptors computed from ground-truth scores.
Furthermore, labels in existing datasets are often noisy, incomplete or they do not allow more sophisticated tasks such as understanding why an image looks beautiful or not to a human observer.
In this thesis, we first propose several measures of subjectivity, ranging from simple statistical measures such as the standard deviation of the scores, to newly proposed descriptors inspired by information theory.
We evaluate the prediction performance of these measures when they are computed from predicted score distributions and when they are directly learned from ground-truth data.
We find that the latter strategy provides in general better results.
We also use the subjectivity to improve predicting aesthetic scores, showing that information theory inspired subjectivity measures perform better than statistical measures.
Then, we propose an Explainable Visual Aesthetics (EVA) dataset, which contains 4070 images with at least 30 votes per image.
EVA has been crowd-sourced using a more disciplined approach inspired by quality assessment best practices.
It also offers additional features, such as the degree of difficulty in assessing the aesthetic score, rating for 4 complementary aesthetic attributes, as well as the relative importance of each attribute to form aesthetic opinions.
The publicly available dataset is expected to contribute to future research on understanding and predicting visual quality aesthetics.
Additionally, we studied the explainability of image aesthetic quality assessment.
A statistical analysis on EVA demonstrates that the collected attributes and relative importance can be linearly combined to explain effectively the overall aesthetic mean opinion scores.
We found subjectivity has a limited correlation to average personal difficulty in aesthetic assessment, and the subject's region, photographic level and age affect the user's aesthetic assessment significantly.
This work aims toward an improved understanding of the seismic signals derived from the inter-receiver correlation functions of seismic noise, which is valuable and critical for a reliable noise-based deep Earth imaging.
The thesis consists of seven chapters.
Chapter 1 introduces background knowledge on seismic noise, from its classifications to various origins.
Chapter 2 provides a literature overview on the history and development of the emerging noise correlation method, and reviews various techniques for the pre-processing of seismic noise data and post-processing of noise correlation functions.
Statistics-based noise processing methods and a modified scheme for computing correlation function are developed in this chapter.
Chapter 3 proposes several Radon-based techniques to analyze the slownesses of correlated wavefields and to unveil the origin of noise-derived seismic signals.
Chapter 6 discusses several situations that bring ambiguities into the noise-derived seismic signals and can potentially bias the noise-based imaging of subsurface structure.
The last chapter provides a summarization over the contributions of this thesis and an outlook of several ongoing and prospected works.
Dialogue systems in natural language are error-prone.
We advocate the resolution of these errors within the collaborative model of Clark &amp; Schaefer (1989): each participant is willing to reach the mutual understanding by providing evidence he understands his partner's utterances.
This raises the problem of recursive acceptance as noted by Traum (1994,1999) but existing models that solve this problem either make important simplifications or are too complex to implement.
We propose to consider shared beliefs of understanding in order to guide the grounding process.
We implemented the model and made an evaluation of its accuracy within the simulation paradigm.
Eventually, the evaluation results show the efficiency of grounding while measuring the gain of understanding.
In this thesis, we present a study about Web image results visualization on mobile devices.
Our main findings were inspired by the recent advances in two main research areas-Information Retrieval and Natural Language Processing.
In the former, we considered different topics such as search results clustering, Web mobile interfaces, query intent mining, to name but a few.
In the latter, we were more focused in collocation measures, high order similarity metrics, etc.
Particularly in order to validate our hypothesis, we performed a great deal of different experiments with task specific datasets.
Many characteristics are evaluated in the proposed solutions.
First, the clustering quality in which classical and recent evaluation metrics are considered.
Secondly, the labeling quality of each cluster is evaluated to make sure that all possible query intents are covered.
Thirdly and finally, we evaluate the user's effort in exploring the images in a gallery-based interface.
An entire chapter is dedicated to each of these three aspects in which the datasets-some of them built to evaluate specific characteristics-are presented.
For the final results, we can take into account two developed algorithms, two datasets and a SRC evaluation tool.
From the algorithms, Dual C-means is our main product.
It can be seen as a generalization of our previously developed algorithm, the AGK-means.
Both are based in text-based similarity metrics.
A new dataset for a complete evaluation of SRC algorithms is developed and presented.
Similarly, a new Web image dataset is developed and used together with a new metric to measure the users effort when a set of Web images is explored.
Finally, we developed an evaluation tool for the SRC problem, in which we have implemented several classical and recent SRC metrics.
Our conclusions are drawn considering the numerous factors that were discussed in this thesis.
However, additional studies could be motivated based in our findings.
Some of them are discussed in the end of this study and preliminary analysis suggest that they are directions that have potential.
When expressing a concrete relationship, they are generally categorized as topological localizers which express an inclusion or a contact relationship between two entities.
When expressing a metaphorical meaning, all of these localizers can express an abstract framework, although each one has its own specific uses.
They are sometimes interchangeable for both concrete and metaphorical meanings.
Our comparative study in contemporary Chinese is based on a corpus of texts of different styles: written, oral and written with oral characteristics.
The statistics show the importance of text styles in the choice of localizer.
Pragmatic and semantic contexts are also determining factors in the uses of localizers.
The diachronic analysis performed on a corpus of 26 documents representing the archaic, medieval and pre-modern periods shows the evolution of each localizer and their tendency of use throughout History, both in concrete and metaphorical expressions.
In this thesis, I investigate the role of expectations in business cycles by studying three different kinds of expectations.
First, I focus on a theoretical explanation of business cycles generated by changes in expectations which turn out to be self-fulfilling.
This chapter improves a puzzle from the sunspot literature, thereby giving more evidence towards an interpretation of business cycles based on self-fulfilling prophecies.
Second, I empirically analyze the propagation mechanisms of central bank announcements through changes in market participants'beliefs.
This chapter shows that credible announcements about future unconventional monetary policies can be used as a coordination device in a sovereign debt crisis framework.
Third, I study a broader concept of expectations and investigate the predictive power of political climate on the pricing of sovereign risk.
This chapter shows that political climate provides additional predictive power beyond the traditional determinants of sovereign bond spreads.
In order to interrogate the role of expectations in business cycles from multiple angles, I use a variety of methodologies in this thesis, including theoretical and empirical analyses, web scraping, machine learning, and textual analysis.
In addition, this thesis uses innovative data from the social media platform Twitter.
Regardless of my methodology, all my results convey the same message: expectations matter, both for economic research and economically sound policy-making.
In this thesis, we are interested in how we can leverage fuzzy logic to improve the interactions between relational database systems and humans.
Cooperative answering techniques aim to help users harness the potential of DBMSs.
These techniques are expected to be robust and always provide answer to users.
Empty set (0,00 sec) is a typical example of answer that one may wish to never obtain.
The informative nature of explanations is higher than that of actual answers in several cases, e.g. empty answer sets and plethoric answer sets, hence the interest of robust cooperative answering techniques capable of both explaining and improving an answer set.
Using terms from natural language to describe data --- with labels from fuzzy vocabularies --- contributes to the interpretability of explanations.
Offering to define and refine vocabulary terms increases the personalization experience and improves the interpretability by using the user's own words.
These axes define cooperative techniques where the interest of explanations is to enable users to understand how results are computed in an effort of transparency.
The informativeness of the explanations brings an added value to the direct results, and that in itself represents a cooperative answer.
Translation techniques constitute an important subject in translation studies and in linguistics.
When confronted with a certain word or segment that is difficult to translate, human translators must apply particular solutions instead of literal translation, such as idiomatic equivalence, generalization, particularization, syntactic or semantic modulation, etc.
However, this subject has received little attention in the field of Natural Language Processing (NLP).
Our research problem is twofold: is it possible to automatically recognize translation techniques?
To verify our hypothesis, we annotated a parallel English-French corpus with translation techniques, while establishing an annotation guide.
Our typology of techniques is proposed based on previous typologies, and is adapted to our corpus.
The inter-annotator agreement (0.67) is significant but slightly exceeds the threshold of a strong agreement (0.61), reflecting the difficulty of the annotation task.
Based on annotated examples, we then worked on the automatic classification of translation techniques.
Even if the dataset is limited, the experimental results validate our working hypothesis regarding the possibility of recognizing the different translation techniques.
We have also shown that adding context-sensitive features is relevant to improve the automatic classification.
In order to test the genericity of our typology of translation techniques and the annotation guide, our studies of manual annotation have been extended to the English-Chinese language pair.
This pair shares far fewer linguistic and cultural similarities than the English-French pair.
The annotation guide has been adapted and enriched.
With the aim to validate the benefits of these studies, we have designed a tool to help learners of French as a foreign language in reading comprehension.
An experiment on reading comprehension with Chinese students confirms our working hypothesis and allows us to model the tool.
Other research perspectives include helping to build paraphrase resources, evaluating automatic word alignment and evaluating the quality of machine translation.
AI nowadays relies largely on using large data and enhanced machine learning methods which consist in developing classification and inference algorithms leveraging large datasets of large sizes.
These large dimensions induce many counter-intuitive phenomena, leading generally to a misunderstanding of the behavior of many machine learning algorithms often designed with small data dimension intuitions.
By taking advantage of (rather than suffering from) the multidimensional setting, random matrix theory (RMT) is able to predict the performance of many non-linear algorithms as complex as some random neural networks as well as many kernel methods such as Support Vector Machines, semi-supervised classification, principal component analysis or spectral clustering.
To characterize the performance of these algorithms theoretically, the underlying data model is often a Gaussian mixture model (GMM) which seems to be a strong assumption given the complex structure of real data (e.g., images).
Furthermore, the performance of machine learning algorithms depends on the choice of data representation (or features) on which they are applied.
Once again, considering data representations as Gaussian vectors seems to be quite a restrictive assumption.
Relying on random matrix theory, this thesis aims at going beyond the simple GMM hypothesis, by studying classical machine learning tools under the hypothesis of Lipschitz-ally transformed Gaussian vectors also called concentrated random vectors, and which are more generic than Gaussian vectors.
This hypothesis is particularly motivated by the observation that one can use generative models (e.g., GANs) to design complex and realistic data structures such as images, through Lipschitz-ally transformed Gaussian vectors.
This notably suggests that making the aforementioned concentration assumption on data is a suitable model for real data and which is just as mathematically accessible as GMM models.
Therefore, we demonstrate through this thesis, leveraging on GANs, the interest of considering the framework of concentrated vectors as a model for real data.
In particular, we study the behavior of random Gram matrices which appear at the core of various linear models, kernel matrices which appear in kernel methods and also classification methods which rely on an implicit solution (e.g., Softmax layer in neural networks), with concentrated random inputs.
Analyzing these methods for concentrated data yields to the surprising result that they have asymptotically the same behavior as for GMM data (with the same first and second order statistics).
This result strongly suggest the universality aspect of large machine learning classifiers w.r.t.
Named Entity (NE) Recognition is a recurring problem in the different domain of Natural Language Processing.
With these processes, Nemesis performance achieves about 90% of precision and 80% of recall.
To increase the recall, we put forward optional modules (analysis of the wide context and utilization of the Web as a source of new contexts) and investigate in setting up a disambiguation and grammar rules inference module.
Reading is a crucial learning in primary school.
Initially focused on decoding and accuracy during the first years, reading teaching is then based on automaticity and comprehension.
However this method gives only an assessment of accuracy and automaticity skills, while flluency includes also the abilility to read with apropriate phrasing and expressivity, that means to read with a prosody adapted to the text.
Omitting the prosodic dimensions of fluency results in confusion between speed and fluency.
Prosodic dimensions of reading have long been neglected in reading studies.
Only recently, a few new studies have been interested in reading prosody development in various langages, but not in French.
That's why the prosodic dimensions of fluency deserve more interest, especially while developping in young readers, to better understand its connection with written comprehension.
For this purpose, we used three complementary assessment methods for reading prosody : a subjective assessment, using a multidimensionnal scale, and two objective assessments: one using acoustic markers and another one, automatic, based on raw speech signal analyses.
These scores enabled us to characterize subjectively reading prosody development and to highlight, in French, the link between reading prosody and comprehension, mentionned in various studies.
These data were then used to explore the link between acoustic markers and subjective scoring and have cues of which acoustic markers have an influence on our perception of readings.
Then we used the pausing and breathing data to characterize the link between reading prosody and comprehension.
Using these data, we built growth model for each dimension of reading fluency and studied the causal link between automaticity, prosody and comprehension.
The work presented here, on the development of reading prosody and its link to comprehension in French speaking children, enables us to promote new fluency assessment tools including reading prosody and to consider how to develop training tools.
It also gives us new prospect on reading teaching and on identifying and helping children who need reading intervention.
This thesis aims at defining a unified adaptation of the document selection and answer extraction strategies, based on the document and question types, in a Question-Answering (QA) context.
We develop and investigate a method based on an Information Retrieval approach for the selection of relevant documents in QA.
The method is based on a language model and a binary model of textual classification in relevant or irrelevant category.
It is used to filter unusable documents for answer extraction by matching lists of a priori relevant documents to the question type automatically.
First, we present the method along with its underlying models and we evaluate it on the QA task with RITEL in French.
The evaluation is done on a corpus of 500,000 unsegmented web pages with factoid questions provided by the Quaero program (i.e. evaluation at the document level or D-level).
Then, we evaluate the methodon segmented web pages (i.e. evaluation at the segment level or S-level).
The idea is that information content is more consistent with segments, which facilitates answer extraction.
D-filtering brings a small improvement over the baseline (no filtering).
S-filtering outperforms both the baseline and D-filtering but not significantly.
Finally, we study at the S-level the links between RITEL's performances and the key parameters of the method.
In order to apply the method on segments, we created a system of web page segmentation.
We present and evaluate it on the QA task with the same corpora used to evaluate our document selection method.
This evaluation follows the former hypothesis and measures the impact of natural web page variability (in terms of size and content) on RITEL in its task.
In general, the experimental results we obtained suggest that our IR-based method helps a QA system in its task, however further investigations should be conducted – especially with larger corpora of questions – to make them significant.
Projects in digital humanities increasingly employ public-oriented collaboration methods such as crowdsourcing to achieve objectives that include research, conservation and scholarly editing in the humanities and social sciences.
For example, crowdsourcing presents an opportunity to quicken the pace of progress for transcription projects for research communities that have traditionally operated within closed circuits.
Meanwhile, literature that evaluates the efficacy of crowdsourcing for digital humanities projects is insufficient.
Questions as to whether the public can produce material that can be used for scholarly editions, in which cases, for which types of projects, and how much post-processing or corrections will be required, continue to occupy discussions on the matter.
This doctoral thesis will examine the potential benefits of crowdsourced transcription for scholarly editing projects in the digital humanities.
Firstly, by exploring the technologies and techniques available to render online transcription in XML possible.
Secondly, by developing and testing an online transcription platform, which will allow to examine user needs for collaborative work environments based on user responses and existing industrial crowdsourcing environments.
Thirdly, the data collected will be subjected to digital analysis to compare the productions of non-expert transcribers to those of expert transcribers on the basis of document distance measurements.
The results will be interpreted to determine the potential benefits of crowdsourcing for digital scholarly editing projects.
Finally, the work will conclude by discussing the implications of current work and presenting opportunities for future research in the field.
The research work in this thesis is related to Topic Map construction and their use in semantic annotation of web resources in order to help users find relevant information in these resources.
The amount of information sources available today is very huge and continuously increasing, for that, it is impossible to create and maintain manually a Topic Map to represent and organize all these information.
Many Topic Maps building approaches can be found in the literature [Ellouze et al. 2008a].
However, none of these approaches takes as input multilingual document content.
In addition, although Topic Maps are basically dedicated to users navigation and information search, no one approach takes into consideration users requests in the Topic Map building process.
In this context, we have proposed ACTOM, a Topic Map building approach based on an automated process taking into account multilingual documents and Topic Map evolution according to content and usage changes.
To enrich the Topic Map, we are based on a domain thesaurus and we propose also to explore all potential questions related to source documents in order to represent usage in the Topic Map.
In our approach, we extend the Topic Map model that already exists by defining the usage links and a list of meta-properties associated to each Topic, these meta-properties are used in the Topic Map pruning process.
In our approach ACTOM, we propose also to precise and enrich semantics of Topic Map links so, except occurrences links between Topics and resources, we classify Topic Map links in two different classes, those that we have called “ontological links” and those that we have named “usage links”.
The present dissertation deals with didactic description of linguistic variation in a constraint-based approach.
This approach allows us to consider variation in a functional perspective rather than in a normative perspective and to describe “non-standard” variants as more or less appropriate to certain tasks rather than deviations from the norm.
To illustrate our approach, we are applying it to the description of clitic left dislocation in French.
We claim that these constraints are all pragmatic in nature and that their interaction weight on the use of clitic left dislocation in French.
These claims are tested empirically via a corpus studies, a series of acceptability judgment tests and a matched guise test.
Furthermore, we argue that the learning of pragmatic constraints in foreign language is dependent of their explicit teaching and repeated expositions to the construction in felicitous contexts.
Following the dynamic interface hypothesis (Ellis, 2005), we suggest that explicit learning of the constraints of clitic left dislocation in the context of the classroom facilitates their implicit learning when the learners find themselves in a situation of communication with French native speakers.
The role of exposition is explored empirically by replicating an acceptability judgment test and the matched guise test with non-native participants.
Stylistics constraints are described using existential competencies and sociolinguistics registers (European Framework, 2001).
Starting from a dynamic and plural conception of emotions, this thesis reflects on the discursive inscription of affects in digital interactions like WhatsApp.
It is part of the field of analysis of discourses in interaction (Kerbrat-Orecchioni, 2005) by making the framework of digital discourse (Paveau, 2017) echo with the theoretical propositions of information and communication sciences (Allard, 2017).
If linguists have long stressed the infinity of the linguistic marks of emotions (Kerbrat-Orecchioni 2000) or even their heterogeneity (Plantin, 2011 ; Micheli, 2014), this postulate is even more relevant indigital ecosystems where the expression of affects is distributed throughout the digital interface integrating words and gestures indifferently (Jeanneret and Souchier, 1999).
Following a methodological reflection on the constitution of the WhatsApp digital corpus, this work focuses on the semiotic and discursive resources in the expression of emotions.
The latter are materialised in new verbal forms such as the emotiwords lol and mdr, understood as a sociolect of the affect.
Nevertheless, in the context of digital messaging, the expression of emotions goes beyond the verbal to be embodied in iconic forms marked by photo discourses, where the photographic capture is woven with theverbal to co-elaborate meaning.
Based on qualitative analyses of the collected observables, this thesis shows how the speakers constantly renew the forms of emotional expressions and how they deal with the digital affordances of the system, to explore new versions in the interactional management of affects.
Humans and robots working safely and seamlessly together in a cooperative environment is one of the future goals of the robotics community.
When humans and robots can work together in the same space, a whole class of tasks becomes amenable to automation, ranging from collaborative assembly to parts and material handling to delivery.
Proposed standards exist for collaborative human-robot safety, but they focus on limiting the approach distances and contact forces between the human and the robot.
A key enabler for human-robot safety in cooperative environments involves the field of intention recognition, in which the robot attempts to understand the intention of an agent (the human) by recognizing some or all of their actions to help predict the human's future actions.
We present an approach to inferring the intention of an agent in the environment via the recognition and representation of state information.
This approach to intention recognition is different than many ontology-based intention recognition approaches in the literature as they primarily focus on activity (as opposed to state) recognition and then use a form of abduction to provide explanations for observations.
In specialised domains, the applications such as information retrieval for machine translation rely on terminological resources for taking into account terms or semantic relations between terms or groupings of terms.
In order to face up to the cost of building these resources, automatic methods have been proposed.
Among those methods, the distributional analysis uses the repeated information in the contexts of the terms to detect a relation between these terms.
While this hypothesis is usually implemented with vector space models, those models suﬀer from a high number of dimensions and data sparsity in the matrix of contexts.
In specialised corpora, this contextual information is even sparser and less frequent because of the smaller size of the corpora.
Likewise, complex terms are usually ignored because of their very low number of occurrences.
Semantic relations acquired from corpora are used to generalise and normalise those contexts.
We evaluated the method robustness on four corpora of different sizes, different languages and different domains.
The analysis of the results shows that, while taking into account complex terms in distributional analysis, the abstraction of distributional contexts leads to defining semantic clusters of better quality, that are also more consistent and more homogeneous.
Natural Language Generation is the task of automatically producing natural language text to describe information present in non-linguistic data.
The latter task is known as Surface Realisation (SR).
In my thesis, I study the SR task in the context of input data coming from Knowledge Bases (KB).
I present two novel approaches to surface realisation from knowledge bases: a supervised approach and a weakly supervised approach.
In the first, supervised, approach, I present a corpus-based method for inducing a Feature Based Lexicalized Tree Adjoining Grammar from a parallel corpus of text and data.
In the weakly supervised approach, I explore a method for surface realisation from KB data which does not require a parallel corpus.
Instead, I build a corpus from heterogeneous sources of domain-related text and use it to identify possible lexicalisations of KB symbols and their verbalisation patterns.
I evaluate the output sentences and analyse the issues relevant to learning from non-parallel corpora.
In both these approaches, the proposed methods are generic and can be easily adapted for input from other ontologies
This research takes an interest the syntax of exceptive constructions (ECs) and its correspondence with semantic within two languages: French and Arabic.
We will focus our analysis on the markers sauf, excepté, hormis, etc. and analyze them as particular case of paradigmatic lists/piles, in which two segments of the utterance pile up on the same syntactic position and whose most famous case is coordination.
This analysis is different from the one generally associated with these markers in French grammars and dictionaries which consider them as prepositions.
Our analyzes lead us to consider the markers ʾillā, ġayr and siwā in Arabic as coordinating conjunctions.
These items, like their French counterparts, relate two elements where X on the right of the marker and Y on the left form paradigmatic lists/piles, in the sense that they fulfill the same syntactic function in the utterance.
We analyze the lexical items ʿadā (mā-ʿadā), ẖalā (mā-ẖalā), ḥāšā (mā-ḥāšā) as verbs.
These verbs introduce a clause that maintains a parataxic relation with the preceding clause.
Finally, we consider the items bistiṯnā'i and biẖilāfi as prepositive phrases introducing a sequence that maintains a subordinate relationship with the main clause.
Exchanged information in emails' texts is usually concerned by complex events or business processes in which the entities exchanging emails are collaborating to achieve the processes' final goals.
Thus, the flow of information in the sent and received emails constitutes an essential part of such processes i.e. the tasks or the business activities.
Extracting information about business processes from emails can help in enhancing the email management for users.
It can be also used in finding rich answers for several analytical queries about the employees and the organizations enacting these business processes.
None of the previous works have fully dealt with the problem of automatically transforming email logs into event logs to eventually deduce the undocumented business processes.
Towards this aim, we work in this thesis on a framework that induces business process information from emails.
We introduce approaches that contribute in the following: (1) discovering for each email the process topic it is concerned by, (2) finding out the business process instance that each email belongs to, (3) extracting business process activities from emails and associating these activities with metadata describing them, (4) improving the performance of business process instances discovery and business activities discovery from emails by making use of the relation between these two problems, and finally (5) preliminary estimating the real timestamp of a business process activity instead of using the email timestamp.
Using the results of the mentioned approaches, an event log is generated which can be used for deducing the business process models of an email log.
The efficiency of all of the above approaches is proven by applying several experiments on the open Enron email dataset.
This thesis is a part of the emergence of deep learning and focuses on spoken language understanding assimilated to the automatic extraction and representation of the meaning supported by the words in a spoken utterance.
We study a semantic concept tagging task used in a spoken dialogue system and evaluated with the French corpus MEDIA.
For the past decade, neural models have emerged in many natural language processing tasks through algorithmic advances or powerful computing tools such as graphics processors.
Many obstacles make the understanding task complex, such as the difficult interpretation of automatic speech transcriptions, as many errors are introduced by the automatic recognition process upstream of the comprehension module.
We present a state of the art describing spoken language understanding and then supervised automatic learning methods to solve it, starting with classical systems and finishing with deep learning techniques.
The contributions are then presented along three axes.
Then we study the management of automatic recognition errors and solutions to limit their impact on our performances.
Finally, we envisage a disambiguation of the comprehension task making the systems more efficient.
From these descriptions, items were created to develop a reception task using videotaped utterances of FSL, and a production task using drawings.
The production of a story from a cartoon was also proposed in order to assess the narrative skills as well as these predicative constructs in a narrative situation.
The recent trend toward Network Softwarization is driving an unprecedented technoeconomic shift in the Telecom and ICT (Information and Communication Technologies) industries.
SDN and NFV paradigms add more flexibility and enable more control over networks, thus, related technologies are expected to dominate a large part of the networking market in the next few years (estimated at USD 3.68B in 2017 and forecasted by some to reach 54B USD by 2022 at a Compound Annual Growth Rate (CAGR) of 71.4%).
However, one of the major operators' concerns about Network Softwarization is security.
In this thesis, we have first designed and implemented a pentesting (penetration testing) framework for SDN controllers.
We have proposed a set of algorithms to fingerprint a remote SDN controller without having direct connection to it.
Using our framework, network operators can evaluate the security of their SDN deployments (including Opendaylight, Floodlight and Cisco Open SDN Controller) before putting them into production.
Also, sOFTDP outperforms OFDP by several orders of magnitude which we confirmed by extensive experiments.
The second axis of our research in this thesis is smart management in softwarized networks.
Inspired by the recent breakthroughs in machine learning techniques, notably, Deep Neural Networks (DNNs), we have built a traffic engineering engine for SDN called NeuRoute, entirely based on DNNs.
Current SDN/OpenFlow controllers use a default routing based on Dijkstra's algorithm for shortest paths, and provide APIs to develop custom routing applications.
After September 2008, due to a frozen interbank market, shortage of liquidity, loss of confidence, and collapsing financial institutions, the monetary policy transmission in the euro area was severely impaired.
Under thus exceptional circumstances, the European Central Bank (ECB) had to turn to non-standard monetary policy measures.
Considering, in the euro area, the constrained range of actions and fragmented financial markets, the objective of this empirical thesis is to assess the transmission channels of ECB standard and non-standard monetary policies and their effects on both financial markets and the economy.
As banks' lending behaviors are related to their financing costs, the first essay focuses on bank lending channel.
It studies the evolution of lending activities of European financial institutions on the syndicated loan market and its reaction to the ECB standard and non-standard policies.
The communication of the central bank is of utmost importance in a monetary union with heterogeneous, in terms of economic situations and cultures, countries.
The second and third essays study the signaling channel of monetary policy.
The second essay focuses on the communication during monthly press conferences and their effects on the predictability of monetary policy decisions and on financial markets returns and volatility.
The last essay concentrates exclusively on the use of \textit{forward guidance} on interest rate, a non-standard central bank communication providing information on future short-term interest rates.
It discusses its effectiveness and ability to lower market participants expected interest rates.
The current trend in Artificial Intelligence (AI) is to heavily rely on systems capable of learning from examples, such as Deep learning (DL) models, a modern embodiment of artificial neural networks.
While numerous applications have made it to market in recent years (including self-driving cars, automated assistants, booking services, and chatbots, improvements in search engines, recommendations, and advertising, and heath-care applications, to name a few) DL models are still notoriously hard to deploy in new applications.
In particular, the require massive numbers of training examples, hours of GPU training, and highly qualified engineers to hand-tune their architectures.
This thesis will contribute to reduce the barrier of entry in using DL models for new applications, a step towards 'democratizing AI'.
The angle taken will be to develop new Transfer Learning (TL) approaches, based on modular DL architectures.
Transfer learning encompasses all techniques to speed up learning by capitalizing on exposure to previous similar tasks.
For instance, using pre-trained networks is a key TL tactic used by winners of the recent AutoDL challenge https://autodl.chalearn.org/.
The doctoral candidate will push forward the notion of reusability of pre-trained networks in whole or in part (modularity).
There are several important questions raised in this context.
From a technical standpoint, the current limitations of pre-training include that:
(T1) In many domains, there are no available pre-trained networks, due to lack of massive datasets in related domains;
(T2) Novel architectures of networks such as 'Graph Neural Networks'(GNNs) do not easily lend themselves to pre-training;
(T3) Besides merely retraining the last layer and fine-tuning inner layers, means of re-using pre-trained networks in new contexts are under-developed.
These three issues offer challenging research opportunities to efficiently use prior knowledge, data simulators, and/or data augmentation, and develop novel algorithms and architectures that learn in a modular re-usable way.
From the fundamental research point of view, modularity and inheritance of pre-trained learning modules in biologically-inspired learning systems is a burning topic in AI.
Unanswered questions include:
(F1) Does modularity of the brain increase its effectiveness or is this a legacy of evolution that plays no particular role;
(F2) Likewise, in which context and how does modularity help in artificial systems (e.g. to implement invariances, to help transfer learning, etc.);
(F3) Does module specialization hinder or help generalization to new data modalities (e.g. new sensor data), and if so, how?
In this context, the doctoral student will investigate a novel approach to transfer learning that we call 'Deep Modular Learning'.
The candidate will tackle the problem of training large artificial neural networks whose architectures are modular and whose modules are eventually reusable.
A possible method to approach the problem will be to use multi-level optimization algorithms, addressing the optimization of the overall system (achieving a higher level objective) under the constraint that the modules achieve a lower level objective (reusability).
One scientific aim will be to challenge the hypothesis that modularity is essential for learning systems, in that it accelerates learning by making possible an effective form of transfer learning, a central functionality in AI.
Several principles/conjectures/hypotheses may be guiding this research including:
(P1) The principle of parsimony or 'Ockham's razor'embodied in modern learning theory as 'regularization', which in layman's words states that ``of two theories equivalently powerful in reproducing observations, one should prefer the simpler one''; indeed modular architectures sharing identical sub-modules have fewer adjustable parameters and therefore can be considered less complex than e.g. fully connected networks.
(P2) The innateness hypothesis: Task solving capabilities are a combination of innate vs. acquired skills.
Is it a characteristic of intelligent systems to rely more on learned skills such as language rather than inherit them?
Is it true that language can be completely learned 'from scratch'?
(P3) Induction, deduction, conceptualisation, and causality: do intelligent learning systems rest upon modularity for conceptualisation, language acquisition, and causal inference?
To put this framework in practice, the student will choose practical applications from domains including biomedicine (e.g. molecular toxicity or efficacy), ecology, econometrics, speech recognition, natural language processing, image or video processing, etc.
See for instance http://snap.stanford.edu/data/.
The automatic summarization is generally based on extractive methods that select relevant sentences from the source document and merge them to create a summary.
These methods are not really adapted to the problem of the summary for the spoken conversations because of the spontaneous nature of these dialogs and the importance of the interactions between the speakers.
By selecting only a few sentences, the final summary will contain only one verbatim of what has been said, and not a synthetic description of what happened during the conversation.
This is why abstractive approaches based on concepts detection would be able to overcome those difficulties.
Then we study the interest if using semantic models in the automatic summarization task.
Finally, we propose a method of summay based on patterns.
Filling patterns methods have shown their interest in specific areas for automatic text summary.
In our case, It allows to deal with the difference in spoken data document style (transcripts of conversations) and the nature if the summaries to be generated (synthetic narration).
However, it requires manual writing of patterns and manual annotations of quantities of source data into concepts to be detected to fill these patterns.
This thesis deals with automatic Dialogue Act (DA) recognition in Czech and in French.
Dialogue acts are sentence-level labels that represent different states of a dialogue, such as questions, statements, hesitations, etc.
The experimental results confirmed that every type of feature (lexical, prosodic and word positions) bring relevant and somewhat complementary information.
The proposed methods that take into account word positions are especially interesting, as they bring global information about the structure of a sentence, at the opposite of traditional n-gram models that only capture local cues.
One of the main issue in the domain of automatic dialogue act recognition concerns the design of a fast and cheap method to label new corpora.
Experimental results showed that the proposed method is an efficient approach to create new dialogue act corpora at low costs.
Word Sense Disambiguation (WSD) and Machine Translation (MT) are two central and among the oldest tasks of Natural Language Processing (NLP).
Although they share a common origin, WSD being initially conceived as a fundamental problem to be solved for MT, the two tasks have subsequently evolved very independently of each other.
Indeed, on the one hand, MT has been able to overcome the explicit disambiguation of terms thanks to statistical and neural models trained on large amounts of parallel corpora, and on the other hand, WSD, which faces some limitations such as the lack of unified resources and a restricted scope of applications, remains a major challenge to allow a better understanding of the language in general.
Today, in a context in which neural networks and word embeddings are becoming more and more important in NLP research, the recent neural architectures and the new pre-trained language models offer not only some new possibilities for developing more efficient WSD and MT systems, but also an opportunity to bring the two tasks together through joint neural models, which facilitate the study of their interactions.
In this thesis, our contributions will initially focus on the improvement of WSD systems by unifying the ressources that are necessary for their implementation, constructing new neural architectures and developing original approaches to improve the coverage and the performance of these systems.
Then, we will develop and compare different approaches for the integration of our state of the art WSD systems and language models into MT systems for the overall improvement of their performance.
Finally, we will present a new architecture that allows to train a joint model for both WSD and MT, based on our best neural systems.
This work aims to study grammaticalization, the process by which the functional items of a language come to be replaced with time by content words or constructions, usually providing a more substantial meaning.
Grammaticalization is therefore a particular type of semantic replacement.
However, language emerges as a social consensus, so that it would seem that semantic change is at odds with the proper working of communication.
Despite of this, the phenomenon is attested in all languages, at all times, and pervades all linguistic categories, as the very existence of grammaticalization shows.
These frequencies of use are extracted from the textual database Frantext, which covers a period of seven centuries.
The statistical distribution of the different observables related to these two phenomenal features are extracted.
A random walk model is then proposed to account for this two-sided frequency pattern.
The latency period appears as a critical phenomenon in the vicinity of a saddle-node bifurcation, and quantitatively matches its empirical counter-part.
Finally, an extension of the model is sketched, in which the relationship between the structure of the semantic network and the outcome of the evolution could be discussed.
This thesis aims to study the argumental function for finding an efficient method to automatically acquire the terms.
We start with a discussion on the problematic of the corpus which is: what kind of corpus should we choose and how should we construct the web corpus.
Then, three methods are developed which are based on the morphological characteristics of lexical units and the relation between the appropriate predicates and their arguments.
The distributional method tries to exploit the predicate-argument structures for identifying the arguments of the given semantic class.
The morph-semantic method is developed on the basis of the morphological characteristics of the lexical units in order to extend the list of terms.
The mixed method which combines the two precedent approaches permit to improve the result.
Finally, we try to develop a statement on the natural language character, on the semantic class, on the specialized language and on the recursive nature of language in the perspective of natural language processing.
On the other hand, when it comes to dealing with under-resourced languages, there is often a lack of tools and data.
In this thesis, we are interested in some of the vernacular forms of Arabic used in Maghreb.
These forms are known as dialects, which can be classified as poorly endowed languages.
Except for raw texts, which are generally extracted from social networks, there is not plenty resources allowing to process Arabic dialects.
The latter, compared to other under-resourced languages, have several specificities that make them more difficult to process.
We can mention, in particular the lack of rules for writing these dialects, which leads the users to write the dialect without following strict rules, so the same word can have several spellings.
Words in Arabic dialect can be written using the Arabic script and/or the Latin script (arabizi).
For the Arab dialects of the Maghreb, they are particularly impacted by foreign languages such as French and English.
In addition to the borrowed words from these languages, another phenomenon must be taken into account in automatic dialect processing.
This is the problem known as code-switching.
This phenomenon is known in linguistics as diglossia.
This gives free rein to the user who can write in several languages in the same sentence.
He can start in Arabic dialect and in the middle of the sentence, he can switch to French, English or modern standard Arabic.
In addition to this, there are several dialects in the same country and a fortiori several different dialects in the Arab world.
It is therefore clear that the classic NLP tools developed for modern standard Arabic cannot be used directly to process dialects.
The main objective of this thesis is to propose methods to build automatically resources for Arab dialects in general and more particularly for Maghreb dialects.
This represents our contribution to the effort made by the community working on Arabic dialects.
We have thus produced methods for building comparable corpora, lexical resources containing the different forms of an input and their polarity.
In addition, we developed methods for processing modern standard Arabic on Twitter data and also on transcripts from an automatic speech recognition system operating on Arabic videos extracted from Arab television channels such as Al Jazeera, France24, Euronews, etc.
We compared the opinions of automatic transcriptions from different multilingual video sources related to the same subject by developing a method based on linguistic theory called Appraisal.
Qualitative Spatial and Temporal Reasoning is a major field of study in Artificial Intelligence and, particularly, in Knowledge Representation, which deals with the fundamental cognitive concepts of space and time in an abstract manner.
In our thesis, we focus on qualitative constraint-based spatial and temporal formalisms and make contributions to several aspects.
In particular, given a knowledge base of qualitative spatial or temporal information, we define novel local consistency conditions and related techniques to efficiently solve the fundamental reasoning problems that are associated with such knowledge bases.
Further, we enrich the field of spatio-temporal formalisms that combine space and time in an interrelated manner by making contributions with respect to a qualitative spatio-temporal logic that results by combining the propositional temporal logic (PTL) with a qualitative spatial constraint language, and by investigating the task of ordering a temporal sequence of qualitative spatial configurations to meet certain transition constraints.
In this dissertation we tackle the problem of multilingual epidemic surveillance.
We present an approach which is differential, endogenous and non-compositionnal.
Using genre properties and communication principles, we maximise the factorization in order to get a system as generic as possible.
Our local analysis does not rely on classical linguistic analyzers for morphology, syntax or semantics but on the distribution of character strings at key positions thus avoiding the problem of the definition of a "word".
We implemented a system using this approach, this system is called DAnIEL (Data Analysis for Information Extraction in any Language).
DanIEL analyzes press articles in order to check if they contain epidemic events and classifies them according to disease-location pair in order to reduce redundancy for the end-user.
DanIEL is fast, efficient in comparison to state-of-the-art systems.
It needs very few additional knowledge for processing new languages.
Attention involves distinguishing information which is useful for an activity from that which is not.
Augmented reality (AR) highlighting guides this process of information selection by increasing the salience of high-value elements.
In such systems, “value” is typically seen as linked to the overall activity of driving (e.g. traffic signs in case of poor visibility, or indications of direction).
However, several studies have shown that during an activity eye movements are specific to the immediate goal.
AR which does not respect this “natural” prioritizing of information thus risks interfering with information-processing.
The first objective of this study is to determine to what extent the allocation of visual attention in automobile driving is focused on information related to a maneuver.
The second objective is to study the impact of AR on this allocation of attention.
We set up three experiments in which participants viewed static and dynamic driving situations and had to decide whether they could perform a maneuver.
We analyzed the variations in allocation of visual attention in relation to the maneuvers in question and the AR conditions using eye movements recording.
Our results show that visual attention is strongly linked to cues which permit decision-making, but that it does not overlook cues allowing a general comprehension of the situation.
AR optimizes visual attention when it highlights cues related to the maneuver, but it disturbs visual attention in other cases.
These findings make it possible to identify and categorize various risks inherent in AR highlighting, and to discuss possible approaches to dealing with them through more effective design.
Rare Neurodegenerative Diseases with Motor Expression (RNDME) are a very heterogeneous group of neurodegenerative pathologies of acquired or innate physiopathology that can occur at any age.
They have a targeted impact on the central or peripheral neurological structures involved in motor activity.
These structures are functionally and topographically close to the centers of cognition.
Thus, under certain conditions, the neurodegeneration of motor structures impacts cognitive functions.
They result in gait and/or balance disorders, a decrease or absence of movement, a varied cognitive impairment that can go as far as dementia.
RNDME are sporadic or family based.
The scientific literature indicates an important number of genetic pathogenic variation responsible for RNDMEs, which allows classification but also makes it more complex because it can be observed that the same gene can be involved in several pathologies and a pathology can be caused by different genes.
Consequently, in RNDMEs, atypical symptoms, clinical overlaps or allelic diseases are observed.
In the French West Indies, the data in the literature on RNDMEs are limited but rich in certain observations confirming the atypical RNDMEs seach as the atypical Caribbean Parkinson's syndrome.
On the genetic level, diagnostic investigations are limited to medical fields based in metropolitan France and most often about the comparison of genomic data of Caucasian populations.
The inventory of RNDMEs in Martinique and the experimental work of data analysis of NGS, are the first works of this type, in the field and in our region.
It had almost never been used in the analysis of RNDMEs.
We used it because it is a method of choice for searching for both simple and structural variants.
The results show a diagnostic cost-effectiveness of 58% since we identified a variant probably pathogenic in 7 patients out of 12 tested.
We found these variants in known RNDMEs genes sometimes described in Asian populations, but also of African or Caucasian descent.
Some of these genes may be involved in several symptomatologies, confirming the finding of overlap in these diseases.
This work lays the epidemiological basis for RNDMEs in the West Indies and provides a basis for a registry in this area.
On the experimental level, it allow to propose a molecular cause associated with previously described or new variations.
It is also a proof of concept regarding the bioinformatics means of analyzing NGS data in Martinique.
It paves the way for other work of the same kind, susceptible to draw up the gene specificities of the RNDMEs in our region and to expand the data in the scientific and medical literature.
The thesis is focused on learning a complex manipulation robotics task using little knowledge.
More precisely, the concerned task consists in reaching an object with a serial arm and the objective is to learn it without camera calibration parameters, forward kinematics, handcrafted features, or expert demonstrations.
Deep reinforcement learning algorithms suit well to this objective.
Indeed, reinforcement learning allows to learn sensori-motor mappings while dispensing with dynamics.
Besides, deep learning allows to dispense with handcrafted features for the state space representation.
However, it is difficult to specify the objectives of the learned task without requiring human supervision.
Some solutions imply expert demonstrations or shaping rewards to guide robots towards its objective.
The latter is generally computed using forward kinematics and handcrafted visual modules.
Another class of solutions consists in decomposing the complex task.
Learning from easy missions can be used, but this requires the knowledge of a goal state.
Alternate approaches which use several agents in parallel to increase the probability of success can be used but are costly.
Indeed, humans first look at an object before reaching it.
The first learned task is an object fixation task which is aimed at localizing the object in the 3D space.
This is learned using deep reinforcement learning and a weakly supervised reward function.
The second task consists in learning jointly end-effector binocular fixations and a hand-eye coordination function.
This is also learned using a similar set-up and is aimed at localizing the end-effector in the 3D space.
In addition, without using additional priors, an object reach ability predictor is learned in parallel.
The main contribution of this thesis is the learning of a complex robotic task with weak supervision.
This thesis focuses on machine learning for data classification.
We propose a new uncertainty measure that characterizes the importance of data and improves the performance of active learning compared to the existing uncertainty measures.
This measure determines the smallest instance weight to associate with new data, so that the classifier changes its prediction concerning this data.
The existing stream-based active learning methods are initialized with some labelled instances that cover all possible classes.
However, in many applications, the evolving nature of the stream implies that new classes can appear at any time.
We propose an effective method of active detection of novel classes in a multi-class data stream.
This method incrementally maintains a feature space area which is covered by the known classes, and detects those instances that are self-similar and external to that area as novel classes.
Finally, it is often difficult to get a completely reliable labelling because the human labeller is subject to labelling errors that reduce the performance of the learned classifier.
This problem was solved by introducing a measure that reflects the degree of disagreement between the manually given class and the predicted class, and a new informativeness measure that expresses the necessity for a mislabelled instance to be re-labeled by an alternative labeller
Recommender Systems (RS) have become essential tools to deal with an endless increasing amount of data available on the Internet.
Their goal is to provide items that may interest users before they have to find them by themselves.
After being exclusively focused on the precision of users'interests prediction task, RS had to evolve by taking into account other criteria like human factors involved in the decision-making process while computing recommendations, so as to improve their quality and usefulness of recommendations.
Nevertheless, the way some human factors, such as context and diversity needs, are managed remains open to criticism.
While context-aware recommendations relies on exploiting data that are collected without any consideration for users'privacy, diversity has been coming down to a dimension which has to be maximized.
Thereby, we argue that analyzing the evolution of diversity over time would be a promising way to define a user's context, under the condition that context is now defined by item attributes.
Indeed, we support the idea that a sudden variation of diversity can reflect a change of user's context which requires to adapt the recommendation strategy.
We present in this manuscript the first approach to model the evolution of diversity over time and a new kind of context, called ``implicit contexts'', that are respectful of privacy (in opposition to explicit contexts).
We confirm the benefits of implicit contexts compared to explicit contexts from several points of view.
As a first step, using two large music streaming datasets we demonstrate that explicit and implicit context changes are highly correlated.
As a second step, a user study involving many participants allowed us to demonstrate the links between the explicit contexts and the characteristics of the items consulted in the meantime.
Based on these observations and the advantages offered by our models, we also present several approaches to provide privacy-preserving context-aware recommendations and to take into account user's needs
The objective of this research is to explore and develop machine learning methods for the analysis of continuous electroencephalogram (EEG).
Continuous EEG is an interesting modality for functional evaluation of cerebral state in the intensive care unit and beyond.
The subparts of this work hinge around post-anoxic coma prognostication, chosen as pilot application.
A small number of long-duration records were performed and available existing data was gathered from CHU Grenoble.
First, we validate the effectiveness of deep neural networks for EEG analysis from raw samples.
For this we choose the supervised task of sleep stage classification from single-channel EEG.
We use a convolutional neural network adapted for EEG and we train and evaluate the system on the SHHS (Sleep Heart Health Study) dataset.
This constitutes the first neural sleep scoring system at this scale (5000 patients).
Classification performance reaches or surpasses the state of the art.
In real use for most clinical applications, the main challenge is the lack of (and difficulty of establishing) suitable annotations on patterns or short EEG segments.
Available annotations are high-level (for example, clinical outcome) and therefore they are few.
We search how to learn compact EEG representations in an unsupervised/semi-supervised manner.
The field of unsupervised learning using deep neural networks is still young.
To compare to existing work we start with image data and investigate the use of generative adversarial networks (GANs) for unsupervised adversarial representation learning.
The quality and stability of different variants are evaluated.
We then apply Gradient-penalized Wasserstein GANs on EEG sequences generation.
The system is trained on single channel sequences from post-anoxic coma patients and is able to generate realistic synthetic sequences.
We also explore and discuss original ideas for learning representations through matching distributions in the output space of representative networks.
Finally, multichannel EEG signals have specificities that should be accounted for in characterization architectures.
Each EEG sample is an instantaneous mixture of the activities of a number of sources.
Based on this statement we propose an analysis system made of a spatial analysis subsystem followed by a temporal analysis subsystem.
The spatial analysis subsystem is an extension of source separation methods built with a neural architecture with adaptive recombination weights, i.e. weights that are not learned but depend on features of the input.
We show that this architecture learns to perform Independent Component Analysis if it is trained on a measure of non-gaussianity.
For temporal analysis, standard (shared) convolutional neural networks applied on separate recomposed channels can be used.
Knowledge discovery and inference are concepts tackled in different ways in the scientific literature.
Indeed, a large number of domains are interested such as: information retrieval, textual inference or knowledge base population.
Theses concepts are arousing increasing interest in both academic and industrial fields, promoting development of new methods.
This manuscript proposes an automated approach to infer and evaluate knowledge from extracted relations in non-structured texts.
Its originality is based on a novel framework making possible to exploit (i) the linguistic uncertainty thanks to an uncertainty detection method described in this manuscript (ii) a generated partial ordering of studied objects (e.g. noun phrases) taking into account of syntactic implications and a prior knowledge defined into taxonomies, and (iii) an evaluation step of extracted and inferred relations by selection models exploiting a specific partial ordering of relations.
This partial ordering allows to compute some criteria in using information propagation rules in order to evaluate the belief associated to a relation in taking into account of the linguistic uncertainty.
The proposed approach is illustrated and evaluated through the definition of a system performing question answering by analysing texts available on the Web.
This case study shows the benefits of structuring processed information (e.g. using prior knowledge), the impact of selection models and the role of the linguistic uncertainty for inferring and discovering new knowledge.
These contributions have been validated by several international and national publications and our pipeline can be downloaded at https: //github.com/PAJEAN/.
This work is situated in the context of information retrieval (IR) using machine learning (ML) and deep learning (DL) techniques.
It concerns different tasks requiring text matching, such as ad-hoc research, question answering and paraphrase identification.
The objective of this thesis is to propose new approaches, using DL methods, to construct semantic-based models for text matching, and to overcome the problems of vocabulary mismatch related to the classical bag of word (BoW) representations used in traditional IR models.
Indeed, traditional text matching methods are based on the BoW representation, which considers a given text as a set of independent words.
The process of matching two sequences of text is based on the exact matching between words.
The main limitation of this approach is related to the vocabulary mismatch.
This problem occurs when the text sequences to be matched do not use the same vocabulary, even if their subjects are related.
For example, the query may contain several words that are not necessarily used in the documents of the collection, including relevant documents.
BoW representations ignore several aspects about a text sequence, such as the structure the context of words.
These characteristics are important and make it possible to differentiate between two texts that use the same words but expressing different information.
Another problem in text matching is related to the length of documents.
The relevant parts can be distributed in different ways in the documents of a collection.
This is especially true in large documents that tend to cover a large number of topics and include variable vocabulary.
A long document could thus contain several relevant passages that a matching model must capture.
Unlike long documents, short documents are likely to be relevant to a specific subject and tend to contain a more restricted vocabulary.
Assessing their relevance is in principle simpler than assessing the one of longer documents.
In this thesis, we have proposed different contributions, each addressing one of the above-mentioned issues.
First, in order to solve the problem of vocabulary mismatch, we used distributed representations of words (word embedding) to allow a semantic matching between the different words.
These representations have been used in IR applications where document/query similarity is computed by comparing all the term vectors of the query with all the term vectors of the document, regardless.
Unlike the models proposed in the state-of-the-art, we studied the impact of query terms regarding their presence/absence in a document.
We have adopted different document/query matching strategies.
The intuition is that the absence of the query terms in the relevant documents is in itself a useful aspect to be taken into account in the matching process.
Indeed, these terms do not appear in documents of the collection for two possible reasons: either their synonyms have been used or they are not part of the context of the considered documents.
This thesis relates to Text-To-Speech synthesis and more particularly deals with a corpus based approach that is the unit selection speech synthesis.
We propose the Kullback-Leibler divergence to guide the iterative selection of candidate sentences: indeed, this criterion gives the possibility to control the unit distribution at each step of the algorithm.
The aim of this method is to build a corpus whose unit distribution approximates a given target distribution.
We also propose an efficient implementation of this method which incrementally update the KL divergence in the sentence selection process.
We then use our method for database reduction adapted to a specific domain TTS synthesis applications.
We show that this adaptive database pruning method is a promising reduction method.
One daunting challenge of Content Based Image Retrieval systems is the requirement of annotated databases.
To limit the burden of annotation, this thesis proposes a system of image annotation based on gaze data.
The purpose is to classify a small set of images according to a target category (binary classification) in order to classify a set of unseen images.
Among the gaze features known to be informative about the intentions of the participants, we have determined a Gaze-Based Intention Estimator (GBIE), computable in real-time; independent from both the participant and the target category.
This implicit annotation is better than random annotation but is inherently uncertain.
In a second part, the images annotated by the GBIE from the participants' gaze data are used to classify a bigger set of images with an algorithm that handles label uncertainty: P-SM combining classification and regression SVM.
We have determined among different strategies a criterion of relevance in order to discriminate the most reliable labels, involved in the classification part, from the most uncertain labels, involved in the regression part.
The average accuracy of P-SVM is evaluated in different contexts and can compete with the performances of standard classification algorithm trained with true-class labels.
These evaluations were first conducted on a standard benchmark for comparing with state-of-the-art results and later conducted on food image dataset.
France has a large nationwide longitudinal database with claims and hospital data, the Système National des Données de Santé (French National healthcare database – SNDS), which currently covers almost the complete French population, from birth or immigration to death or emigration, and includes all reimbursed medical and paramedical encounters.
Since SNDS systematically and prospectively captures drug dispensings, death, and events leading to hospital stays, it has a strong potential for drug assessment in real life settings.
Following the worldwide withdrawal of rofecoxib in 2004, several initiatives aiming to develop and evaluate methodologies for drug safety monitoring on healthcare databases emerged.
The EU-ADR alliance (Exploring and Understanding Adverse Drug Reactions by integrative mining of clinical records and biomedical knowledge) and OMOP (Observational Outcomes Partnership) were respectively launched in Europe and in the Unites-States.
These experiments demonstrated the usefulness of pharmacoepidemiological approaches in drug safety signal detection.
However the SNDS had never been tested in this scope.
Self-controlled case series showed the best performances in both outcomes, ALI and UGIB, with AUCs reaching respectively 0.80 and 0.94 and MSEs 0.07 and 0.12.
For UGIB optimal performances were observed when adjusting for multiple drugs and using a risk window corresponding to the 30 first days of exposure.
For ALI, optimal performances were also observed when adjusting for multiple drugs but using a risk window corresponding to the overall period covered by drug dispensings.
Negative drug control implementation highlighted that a low systematic error seemed to be generated by the optimum variants in the SNDS but that protopathic bias and confounding by indication remained unaddressed issues.
These results showed that self-controlled case series were well suited to detect drug safety alerts associated with UGIB and ALI in the SNDS in an accurate manner.
A clinical perspective remains necessary to rule out potential false positive signals from residual confounding.
The application in routine of such approaches extended to other outcomes of interest could result in substantial progress in pharmacovigilance in France.
Restricted Boltzmann machines (RBM) are graphical models that learn jointly a probability distribution and a representation of data.
Despite their simple architecture, they can learn very well complex data distributions such the handwritten digits data base MNIST.
However, not all variants of RBM perform equally well, and little theoretical arguments exist for these empirical observations.
In the first part of this thesis, we ask how come such a simple model can learn such complex probability distributions and representations.
By analyzing an ensemble of RBM with random weights using the replica method, we have characterised a compositional regime for RBM, and shown under which conditions (statistics of weights, choice of transfer function) it can and cannot arise.
Both qualitative and quantitative predictions obtained with our theoretical analysis are in agreement with observations from RBM trained on real data.
In a second part, we present an application of RBM to protein sequence analysis and design.
Owe to their large size, it is very difficult to run physical simulations of proteins, and to predict their structure and function.
It is however possible to infer information about a protein structure from the way its sequence varies across organisms.
For instance, Boltzmann Machines can leverage correlations of mutations to predict spatial proximity of the sequence amino-acids.
Here, we have shown on several synthetic and real protein families that provided a compositional regime is enforced, RBM can go beyond structure and extract extended motifs of coevolving amino-acids that reflect phylogenic, structural and functional constraints within proteins.
Moreover, RBM can be used to design new protein sequences with putative functional properties by recombining these motifs at will.
Monitoring animal health worldwide, especially the early detection of outbreaks of emerging and exotic pathogens, is one of the means of preventing the introduction of infectious diseases in France.
Recently, there is an increasing awareness among health authorities for the use of unstructured information published on the Web for epidemic intelligence purposes.
Our approach is generic; however, it was elaborated using five exotic animal infectious diseases: african swine fever, foot-and-mouth disease, bluetongue, Schmallenberg, and avian influenza.
We show that the text mining techniques, supplemented by the knowledge of domain experts, are the foundation of an efficient and reactive system for monitoring animal health emergence on the Web.
Our tool will be used by the French epidemic intelligence team for international monitoring of animal health, and will facilitate the early detection of events related to emerging health hazards identified from media reports on the Web.
Main results are: (1) the perception and production of the vowels are greatly influenced by the L1 of the learners, both at the beginning and after nine months of learning; (2) the perception and production performances of the learners are better predicted by the identity of the vowel rather than its status in L2 compared to L1 (new, similar or identical vowels); (3) the phonetic training we gave showed no benefit on the perceptual or production performances of the children.
Today users can interact with popular virtual assistants such as Siri to accomplish their tasks on a digital environment.
In these systems, links between natural language requests and their concrete realizations are specified at the conception phase.
A more adaptive approach would be to allow the user to provide natural language instructions or demonstrations when a task is unknown by the assistant.
An adaptive solution should allow the virtual assistant to operate a much larger digital environment composed of multiple application domains and providers and better match user needs.
We have previously developed robotic systems, inspired by human language developmental studies, that provide such a usage-based adaptive capacity.
Here we extend this approach to human interaction with a virtual assistant that can first learn the mapping between verbal commands and basic action semantics of a specific domain.
Then, it can learn higher level mapping by combining previously learned procedural knowledge in interaction with the user.
The flexibility of the system is demonstrated as the virtual assistant can learn actions in a new domains (Email, Wikipedia,...), and can then learn how email and Wikipedia basic procedures can be combined to form hybrid procedural knowledge
Current musical improvisation systems are able to generate unidimensional musical sequences by recombining their musical contents.
However, considering several dimensions (melody, harmony...) and several temporal levels are difficult issues.
In this thesis, we propose to combine probabilistic approaches with formal language theory in order to better assess the complexity of a musical discourse, both from a multidimensional and multi-level point of view in the context of improvisation where the amount of data is limited.
First, we present a system able to follow the contextual logic of an improvisation modelled by a factor oracle whilst enriching its musical discourse with multidimensional knowledge represented by interpolated probabilistic models.
Then, this work is extended to create another system using a belief propagation algorithm representing the interaction between several musicians, or between several dimensions, in order to generate multidimensional improvisations.
Finally, we propose a system able to improvise on a temporal scenario with multi-level information modelled with a hierarchical grammar.
We also propose a learning method for the automatic analysis of hierarchical temporal structures.
Every system is evaluated by professional musicians and improvisers during listening sessions
The increasing quantity of video material available requires automatic structuring techniques to facilitate access to the information contained in documents, generic enough to be able to structure different kinds of videos.
To this end, we develop two kinds of thematic structuring of TV shows, linear or hierarchical, based on the automatic transcripts of the speech pronounced in the programs.
These transcripts, independent of the type of documents considered, are exploited using natural language processing methods adapted to the peculiarities of professional videos transcripts – transcription errors, limited number of repetition – by taking into account linguistic knowledge and automatic speech recognition and signal information.
Experiments conducted on three corpora composed of broadcast news and reports on current affairs, manually and automatically transcribed, show that the proposed adjustments lead to improved performance of the structuring methods developed
The purpose of this PhD thesis is to study how neology manifests itself in a french language dictionary.
I have chosen to work on the electronic version of the "Nouveau Petit Robert", since it is the epitome of a reference dictionary.
I studied the new attested words that were added between 1990 and 2012 in the "Nouveau Petit Robert Électronique" 2012.
In doing so, the corpus of this study is now composed of 477 new attested words added between 1990 and 2012.This work intends to bring out the formation modes of these new words as well as the experience areas to which they are linked.
In this section the lexicographical particularities of the "Nouveau Petit Robert Électronique" 2012 are highlighted by comparing a few words extracted from its corpus with the same words in two other dictionaries: "Le Petit Larousse Illustré" 2016 and the "Wiktionnaire".
The most efficient formation modes for the corpus' words are those pertaining to the internal matrix with a total of 266 words, as well as those pertaining to the external matrix with a total of 186 words.
The corpus'words belonging to the internal matrix are the most numerous, and it indicates that the lexicon renews itself on its own.
There is a distinct decline of the social sciences in favor of technical sciences to be observed.
This thesis is a study of the prosodic interlanguage of students of English as a foreign language whose native language is French or Spanish.
It is organized in two main parts.
The first part is a study of the methods of conception and representation of prosody for the analysis of interlanguage – a hybrid linguistic system which includes characteristics of the student's native language, characteristics of the target language, and intermediate developmental or characteristics.
This provides a methodological framework for the phonetic analysis and phonological interpretation of this type of prosodic systems.
The results show traces of the influence of their respective native languages at the phonetic and phonological levels, as well as developmental characteristics common to both groups of learners.
The results serve as a basis for reflection on the levels of abstraction in the study of prosody and on the didactic priorities for teaching oral English at a university level.
Computationnal linguistics have questioned former studies on fixation.
Indeed, it has become essential to describe set expressions given their frequency in texts.
This study sets about examining predicate adjectivals in this setting.
Taking object classes as a model, a semantico-syntactic typology has been worked out for these sequences.
The final objective of our analysis is to create an electronic dictionary of predicate adjectivals.
This dissertation communicates three essential points: fixation of adjectivals, their definitions and their classification based on common distributional properties.
This thesis focus on a proposition that helps humans during the exploration of database.
The particularity of this proposition relies on a co-evolution principle between the user and an intelligent interface.
It provides a support to the understanding of the domain represented by the data.
A metaphor of living virtual museum is adopted.
This museum evolves incrementally according to the user's interactions.
It incarnates both the data and the semantic information which are expressed by a knowledge model specific to the domain of the data.
Through the topological organization and the incremental evolution, the museum personalizes online the user's exploration.
The approach is insured by three main mechanisms: the evaluation of the user profile modelled by a dynamical weighting of the semantic information, the use of this dynamic profile to establish a recommendation as well as the incarnation of the data in the living museum.
The approach has been applied to the heritage domain as part of the ANTIMOINE project, funded by the National Research Agency (ANR).
The genericity of the latter has been demonstrated through its application to a database of publications but also using various types of interfaces (website, virtual reality).Experiments have validated the hypothesis that our system adapts itself to the user behavior and that it is able, in turn, to influence him.
They also showed the comparison between a 2D interface and a 3D interface in terms of quality of perception, guidance, preference and efficiency.
This hypothesis is particularly convenient when it comes to developing theoretical guarantees that the learning is accurate.
However, it is not realistic from the point of view of applicative domains that have emerged in the last years.
In this thesis, we focus on four distinct problems in artificial intelligence, that have mainly one common point: All of them imply knowledge transfer from one domain to the other.
The first problem is analogical reasoning and concerns statements of the form "A is to B as C is to D".
The second one is transfer learning and involves classification problem in situations where the training data and test data do not have the same distribution (nor even belong to the same space).
The third one is data stream mining, ie. managing data that arrive one by one in a continuous and high-frequency stream with changes in the distributions.
The last one is collaborative clustering and focuses on exchange of information between clustering algorithms to improve the quality of their predictions.
This framework is based on the notion of Kolmogorov complexity, which measures the inner information of an object.
This tool is particularly adapted to the problem of transfer, since it does not rely on probability distributions while being able to model the changes in the distributions.
Apart from this modeling effort, we propose, in this thesis, various discussions on aspects and applications of the different problems of interest.
These discussions all concern the possibility of transfer in multiple domains and are not based on complexity only.
Anticipating clients'needs is crucial to any business — this is particularly true for corporate and institutional banks such as BNP Paribas Corporate and Institutional Banking due to their role in the financial markets.
This thesis addresses the problem of future interests prediction in the financial context and focuses on the development of ad hoc algorithms designed for solving specific financial challenges.
This manuscript is composed of five chapters:
- Chapter 1 introduces the problem of future interests prediction in the financial world.
The goal of this chapter is to provide the reader with all the keys necessary to understand the remainder of this thesis.
These keys are divided into three parts: a presentation of the datasets we have at our disposal to solve the future interests prediction problem and their characteristics, an overview of the candidate algorithms to solve this problem, and the development of metrics to monitor the performance of these algorithms on our datasets.
This chapter finishes with some of the challenges that we face when designing algorithms to solve the future interests problem in finance, challenges that will be partly addressed in the following chapters;
- Chapter 2 proposes a benchmark of some of the algorithms introduced in Chapter 1 on a real-word dataset from BNP Paribas CIB, along with a development on the difficulties encountered for comparing different algorithmic approaches on a same dataset and on ways to tackle them.
This benchmark puts in practice classic recommendation algorithms that were considered on a theoretical point of view in the preceding chapter, and provides further intuition on the analysis of the metrics introduced in Chapter 1;
- Chapter 3 introduces a new algorithm, called Experts Network, that is designed to solve the problem of behavioral heterogeneity of investors on a given financial market using a custom-built neural network architecture inspired from mixture-of-experts research.
In this chapter, the introduced methodology is experimented on three datasets: a synthetic dataset, an open-source one and a real-world dataset from BNP Paribas CIB.
The chapter provides further insights into the development of the methodology and ways to extend it;
- Chapter 4 also introduces a new algorithm, called History-augmented Collaborative Filtering, that proposes to augment classic matrix factorization approaches with the information of users and items'interaction histories.
This chapter provides further experiments on the dataset used in Chapter 2, and extends the presented methodology with various ideas.
Notably, this chapter exposes an adaptation of the methodology to solve the cold-start problem and applies it to a new dataset;
- Chapter 5 brings to light a collection of ideas and algorithms, successful or not, that were experimented during the development of this thesis.
This chapter finishes on a new algorithm that blends the methodologies introduced in Chapters 3 and 4.
Multivalued Decision Diagrams (MDDs) are efficient data structures widely used in several fields like verification, optimization and dynamic programming.
In this thesis, we first focus on improving the main algorithms such as the reduction, allowing MDDs to potentially exponentially compress set of tuples, or the combination of MDDs such as the intersection of the union.
We go further by designing parallel algorithms, and algorithms handling non-deterministic MDDs.
We then investigate relaxed MDDs, that are more and more used in optimization, and define the notions of relaxed reduction or operation and design efficient algorithms for them.
The sampling of solutions stored in a MDD is solved with respect to probability mass functions or Markov chains.
These new propagators outperform the existing ones and allow the reformulation of several other constraints such as the dispersion constraint, and even to define new ones easily.
We finally apply our algorithm to several real world industrial problems such as text and music generation and geomodeling of a petroleum reservoir.
The extraction of structured information from a document is one of the main parts of natural language processing (NLP).
This extraction usually consists in three steps: named entities recognition relation extraction and event extraction.
The notion of event covers a broad list of different phenomena which are characterized through a varying number of roles.
Thereupon, Event extraction consists in detecting the occurrence of an event then determining its argument, that is, the different entities filling specific roles.
The current best approaches, based on neural networks, focus on the direct neighborhood of the target word in the sentence.
Information in the rest of the document is then usually ignored.
This thesis presents different approaches aiming at exploiting this document-level context.
We begin by reproducing a state of the art convolutional neural network and analyze some of its parameters.
We then present an experiment showing that, despite its good performances, our model only exploit a narrow context at the intra-sentential level.
Subsequently, we present two methods to generate and integrate a representation of the inter-sentential context in a neural network operating on an intra-sentential context.
We also show that this task-specific representation is better than an existing generic representation of the inter-sentential context.
Our second contribution, in response to the limitations of the first one, allows for the dynamic generation of a specific context for each target word.
This method yields the best performances for a single model on multiples datasets.
Finally, we take a different tack on the exploitation of the inter-sentential context.
We try a more direct modelisation of the dependencies between multiple event instances inside a document in order to produce a joint prediction.
To do so, we use the PSL (Probabilistic Soft Logic) framework which allows to model such dependencies through logic formula.
Antimicrobial resistance (AMR) is a major global public health concern.
The Viet Nam National Action Plan on AMR recognised surveillance as one of critical components for control.
However, the current AMR surveillance system (AMRSS) in Viet Nam is likely to be over-representing severe and hospital acquired infections (HAI), potentially resulting in an overestimation of resistance among community acquired infection (CAI).
This thesis aims to evaluate the AMRSS in Viet Nam and to make suggestions to optimize the AMRSS effectiveness in providing accurate and representative AMR data for CAI patients in this setting.
A systematic litterature review was conducted to generate an overview of the AMRSSs that have been implemented globally and any evaluations of such systems.
There is no standardized framework or guidelines for conducting evaluation of AMRSS.
Less than 10% of the systems reported some system evaluation, focusing on few attributes such as representativeness, timeliness, bias, cost, coverage, and sensitivity.
The sensitivity of the AMRSS was in the 2-5% range and remained similar between the two periods.
There was a delay in data submission from the hospitals, which affected surveillance timeliness.
No evaluation of the surveillance system was carried out to identify problems and implement prompt resolutions.
Overall, the results showed that the accuracy of AMR data is enhanced when the number of hospitals increases (0.6% decrease in mean squared error for one additional hospital (CI 0.6% - 0.7%)).
For a given amount of budget, the optimal numbers of hospitals by type can be determined using this modelling approach to identify a system with the best values for each performance attribute.
The results indicate that the current AMRSS can increase the proportions of specialized and provincial hospitals to increase accuracy of data and system representativeness.
The models were based on VINARES data, therefore the results are likely to be valid for an AMRSS with similar organizational structures and data collection protocols.
The amount of budget that the government and foreign development partners are willing to spend on AMR surveillance is also an important factor in identifying the optimal hospital combination for the AMRSS.
The present study deals with word frequencies distributions and their relation to probabilistic Information Retrieval (IR) models.
We examine the burstiness phenomenon of word frequencies in textual collections.
We propose to model this phenomenon as a property of probability distributions and we study the Beta Negative Binomial and Log-Logistic distributions to model word frequencies.
We then focus on probabilistic IR models and their fundamental properties.
Our analysis reveals that probability distributions underlying most state-of-the-art models do not take this phenomenon into account, even if fundamental properties of IR models such as concavity enable implicitly to take it into account.
Lastly, we study empirically and theoretically pseudo relevance feedback models.
We propose a theoretical framework which explain well the empirical behaviour and performance of pseudo relevance feedback models.
Overall, this highlights interesting properties for pseudo relevance feedback and shows that some state-of-the-art model are inadequate.
This thesis deals with the detection of credit card fraud.
According to the European Central Bank, the value of frauds using cards in 2016 amounted to 1.8 billion euros.
The challenge for institutions is to reduce these frauds.
In general, fraud detection systems consist of an automatic system built with "if-then" rules that control all incoming transactions and trigger an alert if the transaction is considered suspicious.
An expert group checks the alert and decides whether it is true or not.
The criteria used in the selection of the rules that are kept operational are mainly based on the individual performance of the rules.
This approach ignores the non-additivity of the rules.
We propose a new approach using power indices.
This approach assigns to the rules a normalized score that quantifies the influence of the rule on the overall performance of the group.
The indexes we use are the Shapley Value and Banzhaf Value.
Their applications are 1) Decision support to keep or delete a rule; 2) Selection of the number k of best-ranked rules, in order to work with a more compact set.
Using real credit card fraud data, we show that:
1) This approach performs better than the one that evaluates the rules in isolation.
2) The performance of the set of rules can be achieved by keeping one-tenth of the rules.
We observe that this application can be considered as a task of selection of characteristics: We show that our approach is comparable to the current algorithms of the selection of characteristics.
It has an advantage in rule management because it assigns a standard score to each rule.
This is not the case for most algorithms, which focus only on an overall solution.
We propose a new version of Banzhaf Value, namely k-Banzhaf; which outperforms the previous in terms of computing time and has comparable performance.
Finally, we implement a self-learning process to reinforce the learning in an automatic learning algorithm.
We compare these with our power indices to rank credit card fraud data.
In conclusion, we observe that the selection of characteristics based on the power indices has comparable results with the other algorithms in the self-learning process.
Incremental dialogue systems are able to process the user's speech as it is spoken (without waiting for the end of a sentence before starting to process it).
This makes them able to take the floor whenever they decide to (the user can also speak whenever she wants, even if the system is still holding the floor).
As a consequence, they are able to perform a richer set of turn-taking behaviours compared to traditional systems.
Several contributions are described in this thesis with the aim of showing that dialogue systems'turn-taking capabilities can be automatically improved from data.
First, human-human dialogue is analysed and a new taxonomy of turn-taking phenomena in human conversation is established.
Based on this work, the different phenomena are analysed and some of them are selected for replication in a human-machine context (the ones that are more likely to improve a dialogue system's efficiency).
Then, a new architecture for incremental dialogue systems is introduced with the aim of transforming a traditional dialogue system into an incremental one at a low cost (also separating the turn-taking manager from the dialogue manager).
To be able to perform the first tests, a simulated environment has been designed and implemented.
It is able to replicate user and ASR behaviour that are specific to incremental processing, unlike existing simulators.
Combined together, these contributions led to the establishement of a rule-based incremental dialogue strategy that is shown to improve the dialogue efficiency in a task-oriented situation and in simulation.
A new reinforcement learning strategy has also been proposed.
It is able to autonomously learn optimal turn-taking behaviours throughout the interactions.
The simulated environment has been used for training and for a first evaluation, where the new data-driven strategy is shown to outperform both the non-incremental and rule-based incremental strategies.
In order to validate these results in real dialogue conditions, a prototype through which the users can interact in order to control their smart home has been developed.
Comparing and matching probability distributions is a crucial in numerous machine learning (ML) algorithms.
Optimal transport (OT) defines divergences between distributions that are grounded on geometry: starting from a cost function on the underlying space, OT consists in finding a mapping or coupling between both measures that is optimal with respect to that cost.
Despite those advantages, the applications of OT in data sciences have long been hindered by the mathematical and computational complexities of the underlying optimization problem.
To circumvent these issues, one approach consists in focusing on particular cases that admit closed-form solutions or that can be efficiently solved.
In particular, OT between elliptical distributions is one of the very few instances for which OT is available in closed form, defining the so-called Bures-Wasserstein (BW) geometry.
This thesis builds extensively on the BW geometry, with the aim to use it as basic tool in data science applications.
To do so, we consider settings in which it is alternatively employed as a basic tool for representation learning, enhanced using subspace projections, and smoothed further using entropic regularization.
In a first contribution, the BW geometry is used to define embeddings as elliptical probability distributions, extending on the classical representation of data as vectors in R^d.
In the second contribution, we prove the existence of transportation maps and plans that extrapolate maps restricted to lower-dimensional projections, and show that subspace-optimal plans admit closed forms in the case of Gaussian measures.
Our third contribution consists in deriving closed forms for entropic OT between Gaussian measures scaled with a varying total mass, which constitute the first non-trivial closed forms for entropic OT and provide the first continuous test case for the study of entropic OT.
Finally, in a last contribution, entropic OT is leveraged to tackle missing data imputation in a non-parametric and distribution-preserving way.
Voice based personal assistants are part of our daily lives.
Their performance suffers in the presence of signal distortions, such as noise, reverberation, and competing speakers.
This thesis addresses the problem of extracting the signal of interest in such challenging conditions by first localizing the target speaker and using the location to extract the target speech.
In a first stage, a common situation is considered when the target speaker utters a known word or sentence such as the wake-up word of a distant-microphone voice command system.
A method that exploits this text information in order to improve the speaker localization performance in the presence of competing speakers is proposed.
The proposed solution uses a speech recognition system to align the wake-up word to the corrupted speech signal.
A model spectrum representing the aligned phones is used to compute an identifier which is then used by a deep neural network to localize the target speaker.
Results on simulated data show that the proposed method reduces the localization error rate compared to the classical GCC-PHAT method.
Similar improvements are observed on real data.
Given the estimated location of the target speaker, speech separation is performed in three stages.
In the first stage, a simple delay-and-sum (DS) beamformer is used to enhance the signal impinging from that location which is then used in the second stage to estimate a time-frequency mask corresponding to the localized speaker using a neural network.
This mask is used to compute the second-order statistics and to derive an adaptive beamformer in the third stage.
A multichannel, multispeaker, reverberated, noisy dataset---inspired from the famous WSJ0-2mix dataset---was generated and the performance of the proposed pipeline was investigated in terms of the word error rate (WER).
To make the system robust to localization errors, a Speaker LOcalization Guided Deflation (SLOGD) based approach which estimates the sources iteratively is proposed.
At each iteration the location of one speaker is estimated and used to estimate a mask corresponding to that speaker.
The estimated source is removed from the mixture before estimating the location and mask of the next source.
The proposed method is shown to outperform Conv-TasNet.
Finally, we consider the problem of explaining the robustness of neural networks used to compute time-frequency masks to mismatched noise conditions.
We employ the so-called SHAP method to quantify the contribution of every time-frequency bin in the input signal to the estimated time-frequency mask.
We define a metric that summarizes the SHAP values and show that it correlates with the WER achieved on separated speech.
To the best of our knowledge, this is the first known study on neural network explainability in the context of speech separation.
This thesis presents several contributions on the theme of recognising textual entailment (RTE).
The RTE is the human capacity, given two texts, to determine whether the meaning of the second text could be deduced from the meaning of the first or not.
One of the contributions made to the field is a hybrid system of RTE taking analysis of an existing stochastic parser to label them with semantics roles, then turning obtained structures in logical formulas using rewrite rules to finally test the entailment using proof tools.
Another contribution of this thesis is the generation of finely annotated tests suites with a uniform distribution of phenomena coupled with a new methodology of systems evaluation using error minning techniques developed by the community of parsing allowing better identification of systems limitations.
For this, we create a set of formulas, then we generate annotated syntactics realisations corresponding by using an existing generation system.
Then, we test whether or not there is an entailment between each pair of possible syntactics realisations.
Finally, we select a subset of this set of problems of a given size and a satisfactory a certain number of constraints using an algorithm that we developed
This thesis is dedicated to the study of Recommendation Systems for implicit feedback (clicks) mostly using Learning-to-rank and neural network based approaches.
In this line, we derive a novel Neural-Network model that jointly learns a new representation of users and items in an embedded space as well as the preference relation of users over the pairs of items and give theoretical analysis.
In addition we contribute to the creation of two novel, publicly available, collections for recommendations that record the behavior of customers of European Leaders in eCommerce advertising, Kelkoofootnote{url{https://www.kelkoo.com/}} and Purchfootnote{label{purch}url{http://www.purch.com/}}.
Both datasets gather implicit feedback, in form of clicks, of users, along with a rich set of contextual features regarding both customers and offers.
Therefore, we propose a simple yet effective strategy on how to overcome the popularity bias introduced while designing an efficient and scalable recommendation algorithm by introducing diversity based on an appropriate representation of items.
Further, this collection contains contextual information about offers in form of text.
Keywords.
Recommendation Systems, Data Sets, Learning-to-Rank, Neural Network, Popularity Bias, Diverse Recommendations, Contextual information, Topic Model.
As semantically non-compositional phrases, idioms are lexical units. Consequently, they must have their own entries in a lexical resource, with a lexicographic definition and grammatical characteristics.
Furthermore, because of their phrasal signifier, idioms show – to varying degrees – a formal flexibility (passivization, attachment of modifiers, substitution of components, etc.)
Our thesis defends the view that a description of idioms that combine identification of their lexical components and identification of dependency links between these components will permit to predict their formal variations.
Such a description is possible only in a model of lexicon that describes precisely combinatorial proprieties of lexical units.
The thesis makes two principal contributions to the study of phraseology.
The first contribution is the development of a precise description of idioms' lexico-syntactic structures.
The second contribution is the identification and the study of structural, syntactic and lexical variations linked to idioms' formal flexibility.
Idioms' formal variations are correlated with their lexico-syntactic structures, but also with their lexicographic definitions.
This work leads us to introduce the notion of structural projection, that plays a central role in the continuum of idiom's formal flexibility
The overall objective of this thesis is to foster the deployment of supervised learning in detection systems to strengthen detection.
To that end, we consider the whole machine learning pipeline (data annotation, feature extraction, training, and evaluation) with security experts as its core since it is crucial to pursue real-world impact.
First, we provide methodological guidance to help security experts build supervised detection models that suit their operational constraints.
Moreover, we design and implement DIADEM, an interactive visualization tool that helps security experts apply the methodology set out.
DIADEM deals with the machine learning machinery to let security experts focus mainly on detection.
Besides, we propose a solution to effectively reduce the labeling cost in computer security annotation projects.
We design and implement an end-to-end active learning system, ILAB, tailored to security experts needs.
Our user experiments on a real-world annotation project demonstrate that they can annotate a dataset with a low workload thanks to ILAB.
Finally, we consider automatic feature generation as a means to ease, and thus foster, the use of machine learning in detection systems.
We define the constraints that such methods should meet to be effective in building detection models.
We compare three state-of-the-art methods based on these criteria, and we point out some avenues of research to better tailor automatic feature generation to computer security experts needs.
Learning a new language means making use, more or less consciously, of the sphere ofproximity between the target linguistic system and the other languages.
We focus onthese transfers and on their influence over interlanguage, in the case of the Romanian learnersstudying French as a foreign language, but also Italian and Spanish.
The data analysis enables us, on the one hand, to give an account of the connectionexisting between the conformity to the norm and the complexity of interlanguage and, on theother hand, to ascertain the role of both positive and negative linguistic transfers against thesetwo dimensions of interlanguage (the complexity and the conformity)
The main goal of this thesis resides in using rich and efficient profiling to improve the adequation between the retrieved information and the user's expectations.
We focus on exploiting as much feedback as we can (being clicks, ratings or written reviews) as well as context.
In the meantime, the tremendous growth of ubiquitous computing forces us to rethink the role of information access platforms.
Therefore, we took interest not solely in performances but also in accompanying users through their access to the information.
Not only it improves the system performances but it also brings some kind of explicativity to the recommendation.
Thus, we propose to accompany the user through his experience accessing information instead of constraining him to a given set of items the systems finds fitting.
They have given rise to numerous studies in Natural Language Processing.
Indeed, their study and precise identification are essential, both from a theoretical and applicative perspective.
However, most of the researches about the subject relate to everyday uses of language: "small talk" dialogs, requests for schedule, speeches, etc.
But what about spontaneous speech production made in a restrained framework?
To our knowledge, no study has ever been carried out in this context.
However, we know that using a "language specialty" in the framework of a given task leads to specific behaviours.
Our thesis work is devoted to the linguistic and computational study of disfluencies within such a framework.
These dialogs concern air traffic control, which entails both pragmatic and linguistic constraints.
We carry out an exhaustive study of disfluencies phenomena in this context.
At first we conduct a subtle analysis of these phenomena.
Then we model them to a level of abstraction, which allows us to obtain the patterns corresponding to the different configurations observed.
Finally we propose a methodology for automatic processing.
It consists of several algorithms to identify the different phenomena, even in the absence of explicit markers.
It is integrated into a system of automatic processing of speech.
Eventually, the methodology is validated on a corpus of 400 sentences.
Taking into account the semantic aspect of the textual data during the classification task has become a real challenge in the last ten years.
This difficulty is in addition to the fact that most of the data available on social networks are short texts, which in particular results in making methods based on the "bag of words" representation inefficient.
The approach proposed in this research project is different from the approaches proposed in previous work on the enrichment of short messages for three reasons.
First, we do not use external knowledge like Wikipedia because typically short messages that are processed by the company come from specific domains.
Secondly, the data to be processed are not used for the creation of resources because of the operation of the tool.
In this thesis, we propose the creation of resources enabling to enrich the short messages in order to improve the performance of the tool of the semantic grouping of the company Succeed Together.
The tool implements supervised and unsupervised classification methods.
To build these resources, we use sequential data mining techniques.
This thesis aimed at conducting research on the synthesis and expressive transformations of the singing voice, towards the development of a high-quality synthesizer that can generate a natural and expressive singing voice automatically from a given score and lyrics.
This thesis provides some contributions in each of those 3 directions.
First, a fully-functional synthesis system has been developed, based on diphones concatenations.
The modular architecture of this system allows to integrate and compare different signal modeling approaches.
Then, the question of the control is addressed, encompassing the automatic generation of the f0, intensity, and phonemes durations.
The modeling of specific singing styles has also been addressed by learning the expressive variations of the modeled control parameters on commercial recordings of famous French singers.
Finally, some investigations on expressive timbre transformations have been conducted, for a future integration into our synthesizer.
Semantic information retrieval (SIR) aims to propose models that allow us to rely, beyond statistical calculations, on the meaning and semantics of the words of the vocabulary, in order to better represent relevant documents with respect to user's needs, and better retrieve them.
The aim is therefore to overcome the classical purely statistical («bag of wordsé») approaches, based on strings'matching and the analysis of the frequencies of the words and their distributions in the text.
To do this, existing SIR approaches, through the exploitation of external semantic resources (thesauri, ontologies, etc.), proceed by injecting knowledge into the classical IR models (such as the vector space model) in order to disambiguate the vocabulary or to enrich the representation of documents and queries.
The semantic resources thus exploited are «ﬂattened», the calculations are generally confined to calculations of semantic similarities.
In order to better exploit the semantics in RI, we propose a new model, which allows to unify in a coherent and homogeneous way the numerical (distributional) and symbolic (semantic) information without sacrificing the power of the analyzes of the one for the other.
The semantic-documentary network thus modeled is translated into a weighted graph.
The matching mechanism is provided by a Spreading activation mechanism in the graph.
This new model allows to respond to queries expressed in the form of key words, concepts or even examples of documents.
The propagation algorithm has the merit of preserving the well-tested characteristics of classical information retrieval models while allowing a better consideration of semantic models and their richness.
Depending on whether semantics is introduced in the graph or not, this model makes it possible to reproduce a classical IR or provides, in addition, some semantic functionalities.
The co-occurrence in the graph then makes it possible to reveal an implicit semantics which improves the precision by solving some semantic ambiguities.
The explicit exploitation of the concepts as well as the links of the graph allow the resolution of the problems of synonymy, term mismatch, semantic coverage, etc.
These semantic features, as well as the scaling up of the model presented, are validated experimentally on a corpus in the medical field.
The analyser performs automatic Chinese word segmentation based on linguistic rules and dictionaries, part-of-speech tagging based on n-gram statistics and dependency grammar parsing.
The module allows to extract information around named entities and activities.
In order to achieve these goals, we have tackled the following main issues: segmentation and part-of-speech ambiguity; unknown word identification in Chinese text; attachment ambiguity in parsing.
Chinese texts are analysed sentence by sentence.
Given a sentence, the analyzer begins with typographic processing to identify sequences of Latin characters and numbers.
Then, dictionaries are used for preliminary segmentation into words.
An n-gram language model is created from a training corpus and selects the best word segmentation and parts-of-speech.
Dependency grammar parsing is used to annotate relations between words.
A first step of named entity recognition is performed after parsing.
Its goal is to identify single-word named entities and noun-phrase-based named entities and to determine their semantic type.
These named entities are then used in knowledge extraction.
Knowledge extraction rules are used to validate named entities or to change their types.
Knowledge extraction consists of two steps: automatic content extraction and tagging from analysed text; extracted contents control and ontology-based co-reference resolution.
The work presented in this thesis is about TTS speech synthesis and, more particularly, about statistical speech synthesis for French.
We present an analysis on the impact of the linguistic contextual factors on the synthesis achieved by the HTS statistical speech synthesis system.
To conduct the experiments, two objective evaluation protocols are proposed.
The first one uses Gaussian mixture models (GMM) to represent the acoustical space produced by HTS according to a contextual feature set.
By using a constant reference set of natural speech stimuli, GMM can be compared between themselves and consequently acoustic spaces generated by HTS.
The second objective evaluation that we propose is based on pairwise distances between natural speech and synthetic speech generated by HTS.
Results obtained by both protocols, and confirmed by subjective evaluations, show that using a large set of contextual factors does not necessarily improve the modeling and could be counter-productive on the speech quality.
This thesis presents a methodology to solve certain classification problems, particularly those involving sequential classification for Natural Language Processing tasks.
It proposes the use of an iterative, error-based approach to improve classification performance, suggesting the incorporation of expert knowledge into the learning process through the use of knowledge rules.
We applied and evaluated the methodology to two tasks related with the detection of hedging in scientific articles: those of hedge cue identification and hedge cue scope detection.
Additionally, this thesis proposes a class schema for representing sentence analysis in a unique structure, including the results of different linguistic analysis.
This allows us to better manage the iterative process of classifier improvement, where different attribute sets for learning are used in each iteration.
We also propose to store attributes in a relational model, instead of the traditional text-based structures, to facilitate learning data analysis and manipulation.
The inevitable emergence of the necessity to estimate the size of a software thus estimating the probable cost and effort is a direct outcome of increasing need of complex and large software in almost every conceivable situation.
Furthermore, due to the competitive nature of the software development industry, the increasing reliance on accurate size estimation at early stages of software development becoming a commonplace practice.
Traditionally, estimation of software was performed a posteriori from the resultant source code and several metrics were in practice for the task.
However, along with the understanding of the importance of code size estimation in the software engineering community, the realization of early stage software size estimation, became a mainstream concern.
Once the code has been written, size and cost estimation primarily provides contrastive study and possibly productivity monitoring.
On the other hand, if size estimation can be performed at an early development stage (the earlier the better), the benefits are virtually endless.
This research focuses on functional size estimation metrics commonly known as Function Point Analysis (FPA) that estimates the size of a software in terms of the functionalities it is expected to deliver from a user's point of view.
One significant problem with FPA is the requirement of human counters, who need to follow a set of standard counting rules, making the process labour and cost intensive (the process is called Function Point Counting and the professional, either analysts or counters).
Moreover, these rules, in many occasion, are open to interpretation, thus they often produce inconsistent counts.
Furthermore, the process is entirely manual and requires Function Point (FP) counters to read large specification documents, making it a rather slow process.
Automation of the process of identifying the FPs in a document accurately, will at least reduce the reading requirement of the counters, making the process faster and thus shall significantly reduce the cost.
Moreover, consistent identification of FPs will allow the production of consistent raw function point counts.
To the best of our knowledge, the works presented in this thesis is an unique attempt to analyse specification documents from early stages of the software development, using a generic approach adapted from well established Natural Language Processing (NLP) practices.
Future applications of robotics, especially personal service robots, will require continuous adaptability to the environment, and particularly the ability to recognize new objects and learn new words through interaction with humans.
Though having made tremendous progress by using machine learning, current computational models for object detection and representation still rely heavily on good training data and ideal learning supervision.
In contrast, two year old children have an impressive ability to learn to recognize new objects and at the same time to learn the object names during interaction with adults and without precise supervision.
Therefore, following the developmental robotics approach, we develop in the thesis learning approaches for objects, associating their names and corresponding features, inspired by the infants'capabilities, in particular, the ambiguous interaction with humans, inspired by the interaction that occurs between children and parents.
The general idea is to use cross-situational learning (finding the common points between different presentations of an object or a feature) and to implement multi-modal concept discovery based on two latent topic discovery approaches: Non Negative Matrix Factorization (NMF) and Latent Dirichlet Association (LDA).
This thesis highlights the algorithmic solutions required to be able to perform efficient learning of these word-referent associations from data acquired in a simplified but realistic acquisition setup that made it possible to perform extensive simulations and preliminary experiments in real human-robot interactions.
We also gave solutions for the automatic estimation of the number of topics for both NMF and LDA.We finally proposed two active learning strategies, Maximum Reconstruction Error Based Selection (MRES) and Confidence Based Exploration (CBE), to improve the quality and speed of incremental learning by letting the algorithms choose the next learning samples.
We compared the behaviors produced by these algorithms and show their common points and differences with those of humans in similar learning situations.
In a first part, we consider the problem of similarity measure learning for two tasks where sequential structure is at stake: (i) the multivariate change-point detection and (ii) the time warping of pairs of time series.
The methods generally used to solve these tasks rely on a similarity measure to compare timestamps.
Using standard structured prediction methods, we present algorithmically efficient ways for learning.
We consider the score as a symbolic representation giving: (i) a complete information about the order of events or notes played and (ii) an approximate idea about the expected shape of the alignment.
We propose to learn a classifier for each note using this information.
Our learning problem is based onthe optimization of a convex function that takes advantage of the weak supervision and of the sequential structure of data.
Our approach is validated through experiments on the task of audio-to-score on real musical data.
The rapid growth in population, combined with the increased mobility of people has created a need for sophisticated identity management systems.
For this purpose, biometrics refers to the identification of individuals using behavioral or biological characteristics.
The most popular approaches, i.e. fingerprint, iris or face recognition, are all based on computer vision methods.
The adoption of deep convolutional networks, enabled by general purpose computing on graphics processing units, made the recent advances incomputer vision possible.
These advances have led to drastic improvements for conventional biometric methods, which boosted their adoption in practical settings, and stirred up public debate about these technologies.
In this respect, biometric systems providers face many challenges when learning those networks.
In this thesis, we consider those challenges from the angle of statistical learning theory, which leads us to propose or sketch practical solutions.
First, we answer to the proliferation of papers on similarity learningfor deep neural networks that optimize objective functions that are disconnected with the natural ranking aim sought out in biometrics.
Precisely, we introduce the notion of similarity ranking, by highlighting the relationship between bipartite ranking and the requirements for similarities that are well suited to biometric identification.
We then extend the theory of bipartite ranking to this new problem, by adapting it to the specificities of pairwise learning, particularly those regarding its computational cost.
The thesis tackles all of those three examplesby proposing their careful statistical analysis, as well as practical methods that provide the necessary tools to biometric systems manufacturers to address those issues, without jeopardizing the performance of their algorithms.
This work deals with the confrontation between data-processing tools for natural language processing and the problem arising from automatic treatment to construct sense.
The language is always moving and the linguistic descriptions must be ajusted.
Our work deals with a process of representation which does not predefines all the knowledge that can be associated to the words.
We use the prototype as part of prototype-based language as a tool for representation of linguistic facts, in so far as we think that it can propose an answer for the problem we face.
This tool leads us to build simple and ajustable structures of representation to suit to the adjustement dimension of natural languages.
Prototype-based language encourage an approach for representation which consists in successive leaps and which improves the quality of the representation that is produced.
Futhermore, these processes go together with the 'home-made" but necessary- work of the linguist and his will to describe the way language events work.
The study compares the functioning of non-verbal utterances in German and Kabyle (Berber) using the Zemb's (1978) semantico-logical triad as a theoretical framework, i.e. the theme (what is being talked about), the rheme (what is said about the theme) and the phème (place of articulation of modalisation and negation), applied by Behr and Quintin (1996) and Behr (2013) to categorisation of German non-verbal utterances.
We posit that each language has morphosyntactic, contextual and situational means allowing the construction of non-verbal utterances and that these means are more extensive in Kabyle.
We also hypothesise that there are unique semantico-logical structures which could be expressed through varied morphosyntactical structures.
Finally, we presume that non-verbal utterances express all the modalities; they have morphological and/ or contextual possibilities which locate them within the temporal framework.
We have observed, among other results, that the frequency of non-verbal utterances is higher in Kabyle due to grammaticalized predicative structures, except for those depending syntactically on a main sentence, which could be explained by the scrambling-process.
At the syntactic level, the pre-/postposition of the rheme in relation to the theme is subject to language specific constraints, i.e. changes in the noun state in Kabyle, the determination and definiteness in German; constraints concerning non-verbal utterances appear in the preference of the rheme-theme order in German.
Non-verbal utterances express all modalities; they are located in time by circumstances, by some demonstratives or by the context, and nominalisations as existential rheme express telic and atelic aspectuality.
Multidisciplinary oncology teams meet regularly in tumor board meetings, in which medical experts of all specialties review patient cases to determine the best therapeutic option.
Data preparation for these meetings is often conducted manually, which increases the risk of extraction errors and represents a burden for health professionals.
The ASIMOV project uses natural language processing and data integration to mine clinical data warehouses and narrative clinical reports to simplify and accelerate data collection, with a focus on temporal information extraction.
We address the problem of machine learning in the context of limited annotated training data and non-English languages.
We will leverage rule-based systems and machine learning approaches, and develop new methods towards generic approaches.
Energy is fundamental to maintain comfort and it shapes our modern life.
With the excess demand for energy, home energy management systems are appearing with time.
They aim at reducing or modulating energy consumption while keeping an acceptable level of comfort.
Efficient home energy management systems should embed a behavioral representation of a home system, including inhabitants.
It establishes relationships between different environmental variables and heterogeneous phenomena present in a home.
Therefore, those systems are complex to build and to understand for inhabitants.
This was justified as it was nearly impossible to implicate occupants and to create a relation between occupants and energy systems.
This concept does create different problems as occupants are detached from the energy system and they don't understand its functionality nor how it is working.
To overcome this difficulty this work promotes the concept of “doing with” as it tries to implicate the occupant in the loop with their energy management system.
This is where the explanation is needed to allow occupants to discover the knowledge in the energy system and to develop their capacity of understanding how the system is working and why it is recommending different actions.
The explanation is the way to discover new knowledge and consequently, to involve occupants.
For humans, explanation plays an important role in life.
It is one of the main tools for learning and understanding.
It is even used in communication and social aspects.
People tend to use it besides learning to show their knowledge about a subject to gain the confidence of others or to clarify a situation.
But generating explanations is not an easy task.
It is one of the ongoing scientific problems from several decades.
Explanations have numerous forms, types, and level of clearness.
This study is focusing on the causal explanations.
As it is the most intuitive form of explanation to be understood by occupants and is adapted to transfer the knowledge from complex systems like energy models.
The scientific challenge is how to construct causal explanations for the inhabitants from a flow of observed sensor data.
Recent studies show an increasing proportion of queries with geographic criteria on Web search engines.
This part is even bigger on specific corpora like cultural heritage collection (e. G. Travelogues).
We admit that the geographic information is composed of three facets: spatial, temporal and thematic.
The goal of this thesis is to combine these three facets to support multicriteria searches.
This work concerns several fields: Natural Language Processing (NLP), Geographic Information System (GIS), classic Information Retrieval (IR) and Geographic Information Retrieval (GIR).
Our first contribution is about an original combination approach of specific indexes.
In order to realize this combination, we propose to imitate the homogenization approaches used in classical IR strategies that represent terms with corresponding lemmas.
So, our second contribution concerns a generic standardization approach implemented on spatial and temporal information.
The last contribution relates to an evaluation framework for GIR systems.
Thanks to this framework, we verified and quantified the benefit of combining the different geographic information facets and also have compared several combination approaches.
Business environmental scanning and collective intelligence (VASIC) as proposed by Lesca is a method to help companies tune in to their environment to anticipate opportunities or risks.
This method requires collecting information, yet with the development of information technology, employees face a glut of information.
To help sustain VASIC, it is necessary to develop tools to manage information overload.
In this thesis, we propose a nearness measurement to estimate if two pieces of information are similar and we have created a prototype, called Alhena, based on this measurement.
We demonstrate the properties of our measurement and its relevance in the context of VASIC.
We also show that the prototype can be used in other fields such as literature, computer science and psychology.
This work is multidisciplinary as it covers aspects of business environmental scanning (management science), research information, computer linguistics and mathematics.
We focus on a concrete problem in management science to provide a tool that operationalizes computational and mathematical techniques with a goal of providing decision making support (time saving, reading assistance,...).
This thesis presents some contributions in three research domains: case-based reasoning, knowledge discovery and knowledge representation.
Case-based reasoning consists in solving new problems by reusing a set of previous problem-solving experiences, called cases.
In this thesis, a language is introduced to represent variations between cases.
We first show how this language can be used to represent adaptation knowledge and to model the adaptation phase in case-based reasoning.
This language is then applied to the task of adaptation knowledge learning.
A knowledge discovery process, called CabamakA, is proposed, that learns adaptation knowledge by generalization from a representation of variations between cases.
A discussion follows on how to make this knowledge discovery process operational in a knowledge acquisition process.
The discussion leads to the proposition of a new approach for adaptation knowledge acquisition, in which the knowledge discovery process is triggered in an opportunistic manner at problem-solving time.
The concepts introduced in the thesis are illustrated in the cooking domain through their application in the case-based reasoning system Taaable, that constitutes the application domain of the study.
Our dissertation belongs to a recently initiated line of studies seeking to characterise the advanced English L2 variety.
We present an integrated analysis of some semantic, discourse and cross-linguistic factors underlying the use of verb forms by advanced French and Catalan learners of English as a foreign language.
Using a corpus of oral picture book narratives, we explore the distribution of tense-aspect morphology in relation to the aspectual class of predicates (the Aspect Hypothesis) and the temporal information they encode in narrative discourse (the Discourse Hypothesis).
The use of tense-aspect forms is also considered from the perspective of the so called L2 rhetorical style, the systematic linguistic choices learners make in a given communicative task drawing on their learnt repertoire of L2 devices but also on information selection and organisation patterns unconsciously transferred from their L1.
While English, Catalan and French grammaticalise aspectual distinctions, this does not ensure a nativelike use of aspectual marking in English L2.
Prototypical predicate/form coalitions in learner production were found to remain strong in the use of tense-aspect morphology with durative (a)telic predicates and to lead to an across-the-board reliance on aspectual marking, often in tension with the plot-advancing role of the predicate.
The degree of grammaticalisation of the progressive aspect in learners'L1 seems to interfere with the hypotheses of use concerning the progressive form in English L2.
Only a subset of the learners, the most advanced ones, employ tense-aspect forms in a way which is genuinely liberated from the semantic congruence with the predicate, similar to what was observed in English L1.
In this case, the progressive has a discourse-specific function and becomes optional when viewpoint information can be retrieved from other elements in the context.
Form-function mappings in the domain of tense-aspect morphology remain, nevertheless, more limited or do not match the ones observed in English L1.
These findings invite to a reflection on the margins of grammaticalised contrasts, where atypical coalitions arise, and how learners can grasp such peripheral uses in an instructional setting.
They also indicate that L2 oral production at the advanced stages remains bound to a way of thinking the world which is the legacy of our L1.
The coastal environment in Normandy is conducive to a convergence of multiple hazards (erosion, marine submersion, flooding by overflowing streams or upwelling of a water table, turbid flooding by runoff, coastal or continental slope movement).
This interaction will occur within the slopes and valleys where coastal populations and their activities have tended to become more densified since the 19th century.
As part of this thesis, as well as in the ANR RICOCHET program, three study sites were selected at the outlet of the coastal rivers: from Auberville to Pennedepie, from Quiberville to Dieppe and from Criel-sur-Mer to Ault due to significant issues and strong interactions between hydrological and gravitational phenomena.
This study deals with pedestrian detection in high-density crowds from a mono-camera system.
We cast the detection problem as a Multiple Classifier System (MCS), composed by two different ensembles of classifiers, the first one based on SVM (SVM-ensemble) and the second one based on CNN (CNN-ensemble), combined relying on the Belief Function Theory (BFT) to exploit their strengths for pixel-wise classification.
BFT allows us to take into account the imprecision in addition to the uncertainty value provided by each classifier, which we consider coming from possible errors in the calibration procedure and from pixel neighbor's heterogeneity in the image space.
However, scarcity of labeled data for specific dense crowd contexts reflects in the impossibility to obtain robust training and validation sets.
By exploiting belief functions directly derived from the classifiers'combination, we propose an evidential Query-by-Committee (QBC) active learning algorithm to automatically select the most informative training samples.
On the other side, we explore deep learning techniques by casting the problem as a segmentation task with soft labels, with a fully convolutional network designed to recover small objects thanks to a tailored use of dilated convolutions.
Finally, we show that the output map given by the MCS can be employed to perform people counting.
This thesis focuses on entity recognition in documents recognized by OCR, driven by a database.
An entity is a homogeneous group of attributes such as an enterprise in a business form described by the name, the address, the contact numbers, etc. or meta-data of a scientific paper representing the title, the authors and their affiliation, etc.
Given a database which describes entities by its records and a document which contains one or more entities from this database, we are looking to identify entities in the document using the database.
This work is motivated by an industrial application which aims to automate the image document processing, arriving in a continuous stream.
We addressed this problem as a matching issue between the document and the database contents.
The difficulties of this task are due to the variability of the entity attributes representation in the database and in the document and to the presence of similar attributes in different entities.
Added to this are the record redundancy and typing errors in the database, and the alteration of the structure and the content of the document, caused by OCR.
To deal with these problems, we opted for a two-step approach: entity resolution and entity recognition.
The first step is to link the records referring to the same entity and to synthesize them in an entity model.
For this purpose, we proposed a supervised approach based on a combination of several similarity measures between attributes.
These measures tolerate character mistakes and take into account the word permutation.
The second step aims to match the entities mentioned in documents with the resulting entity model.
We proceeded by two different ways, one uses the content matching and the other integrates the structure matching.
For the content matching, we proposed two methods: M-EROCS and ERBL.
M-EROCS, an improvement / adaptation of a state of the art method, is to match OCR blocks with the entity model based on a score that tolerates the OCR errors and the attribute variability.
ERBL is to label the document with the entity attributes and to group these labels into entities.
The structure matching is to exploit the structural relationships between the entity labels to correct the mislabeling.
The proposed method, called G-ELSE, is based on local structure graph matching with a structural model which is learned for this purpose.
This thesis being carried out in collaboration with the ITESOFT-Yooz society, we have experimented all the proposed steps on two administrative corpuses and a third one extracted from the web
This dissertation studies the relationship between the expression of controlled aggressive attitudes and the perception of dominance, based on extracts from televised sessions of the Municipal Council of Montreuil during 2013; a period marked by a lively and hostile political climate.
We constituted a corpus of spontaneous speech extracts from the Mayor, Dominique Voynet, and four of her opponents.
During subsequent recording sessions, the five speakers were asked to read transcriptions of their own speech extracts in a neutral tone (25 stimuli per speaker). They also participated in a self-evaluation questionnaire that focused on the perception of emotional profiles in their own stimuli.
The original and re-read extracts were compared in their prosodic-syntactic structure as well as their temporal and melodic characteristics.
We show that: 1) some speakers seem to rely mostly on melodic parameters whereas others primarily use temporal parameters, 2) nevertheless, general trends emerge regarding the speech correlates of hostility and dominance in our corpus, notably: a) discrepancies between the syntactic and the prosodic structure of the extracts, b) reduction or absence of pre-pausal final syllabic lengthening, c) large variations in F0 range on both sides of silent pauses.
Nowadays, more and more data of different kinds is becoming available.
Formal concept analysis (FCA) and pattern structures are theoretical frameworks that allow dealing with an arbitrary structured data.
But the number of concepts extracted by FCA is typically huge.
To deal with this problem one can either simplify the data representation, which can be done by projections of pattern structures, or by introducing constraints to select the most relevant concepts.
The manuscript starts with application of FCA to mining important pieces of information from molecular structures.
With the growth of dataset size good constraints begin to be essential.
For that we explore stability of a concept, a well-founded formal constraint.
Finding stable concepts in this dataset allows us finding new possible mutagenetic candidates that can be further interpreted by chemists.
However for more complex cases, the simple attribute representation of data is not enough.
Correspondingly, we turn to pattern structures that can deal with many different kinds of descriptions.
We extend the original formalism of projections to have more freedom in data simplification.
We show that this extension is essential for analyzing patient trajectories, describing patients hospitalization histories.
Finally, the manuscript ends by an original and very efficient approach that enables to mine stable patterns directly.
In the Web of data, an increasing number of knowledge graphs are concurrently published, edited, and accessed by human and software agents.
Their wide adoption makes key the two tasks of matching and mining.
First, matching consists in identifying equivalent, more specific, or somewhat similar units within and across knowledge graphs.
This task is crucial since concurrent publication and edition may result in coexisting and complementary knowledge graphs.
However, this task is challenging because of the inherent heterogeneity of knowledge graphs, e.g., in terms of granularities, vocabularies, and completeness.
Motivated by an application in pharmacogenomics, we propose two approaches to match n-ary relationships represented in knowledge graphs: a symbolic rule-based approach and a numeric approach using graph embedding.
We experiment on PGxLOD, a knowledge graph that we semi-automatically built by integrating pharmacogenomic relationships from three distinct sources of this domain.
Second, mining consists in discovering new and useful knowledge units from knowledge graphs.
Their increasing size and combinatorial nature entail scalability issues, which we address in the mining of path patterns.
We also propose Concept Annotation, a refinement approach extending Formal Concept Analysis, a mathematical framework that groups entities based on their common attributes.
Throughout all our works, we particularly focus on taking advantage of domain knowledge in the form of ontologies that can be associated with knowledge graphs.
We show that, when considered, such domain knowledge alleviates heterogeneity and scalability issues in matching and mining approaches.
This thesis work aims at bridging the gap between the fields of Web Semantics and Experimental Particle Physics.
Taking as a use case a specific type of physics experiments, namely the irradiation experiments used for assessing the resistance of components to radiation, a domain model, what in Web Semantics is called an ontology, has been created for describing the main concepts underlying the data management of irradiation experiments.
The first part of this thesis aims at exploring deep kernel architectures for complex data.
One of the known keys to the success of deep learning algorithms is the ability of neural networks to extract meaningful internal representations.
However, the theoretical understanding of why these compositional architectures are so successful remains limited, and deep approaches are almost restricted to vectorial data.
The deep kernel architecture we propose consists in replacing the basic neural mappings functions from vector-valued Reproducing Kernel Hilbert Spaces (vv-RKHSs).
Although very different at first glance, the two functional spaces are actually very similar, and differ only by the order in which linear/nonlinear functions are applied.
Apart from gaining understanding and theoretical control on layers, considering kernel mappings allows for dealing with structured data, both in input and output, broadening the applicability scope of networks.
We finally expose works that ensure a finite dimensional parametrization of the model, opening the door to efficient optimization procedures for a wide range of losses.
The second part of this thesis investigates alternatives to the sample mean as substitutes to the expectation in the Empirical Risk Minimization (ERM) paradigm.
Indeed, ERM implicitly assumes that the empirical mean is a good estimate of the expectation.
However, in many practical use cases (e.g. heavy-tailed distribution, presence of outliers, biased training data), this is not the case.
The Median-of-Means (MoM) is a robust mean estimator constructed as follows: the original dataset is split into disjoint blocks, empirical means on each block are computed, and the median of these means is finally returned.
We propose two extensions of MoM, both to randomized blocks and/or U-statistics, with provable guarantees.
The (randomized) MoM minimizers are shown to be robust to outliers, while MoM tournament procedure are extended to the pairwise setting.
We close this thesis by proposing an ERM procedure tailored to the sample bias issue.
Today, social networking has considerably changed why people are taking pictures all the time everywhere they go.
More than 500 million photos are uploaded and shared every day, along with more than 200 hours of videos every minute.
More particularly, with the ubiquity of smartphones, social network users are now taking photos of events in their lives, travels, experiences, etc. and instantly uploading them online.
Such public data sharing puts at risk the users' privacy and expose them to a surveillance that is growing at a very rapid rate.
Furthermore, new techniques are used today to extract publicly shared data and combine it with other data in ways never before thought possible.
However, social networks users do not realize the wealth of information gathered from image data and which could be used to track all their activities at every moment (e.g., the case of cyberstalking).
Thus, the aim of this work is to provide a privacy-preserving constraint (de-linkability) to bound the amount of information that can be used to re-identify individuals using online profile information.
Firstly, we provide a framework able to quantify the re-identification threat and sanitize multimedia documents to be published and shared.
Secondly, we propose a new approach to enrich the profile information of the individuals to protect.
Specifically, our approach is able to detect and link users' elementary events using photos (and related metadata) shared within their online social networks.
A prototype has been implemented and several experiments have been conducted in this work to validate our different contributions.
The massification of the Internet and computers has changed several aspects of our daily life and the way we apply to a job is not the exception.
Since the last 15 years, the researchers in Natural Language Processing have been studying how to improve the performance of recruiters with the help of the e-Recruitment.
Several systems have been developed in this field, from the job and applicants search engines to the automatic ranking of applicants.
In the last case, most of the developed systems consist in the comparison between the résumés of applicants and a job offer.
Only one system makes use of résumés from past selection processes to rank newer applicants.
In this thesis we study whether and how we can use the résumés, without having to use past selection processes, to develop new methods for e-Recruitment systems.
More specifically, we start with the automatic treatment of a large set of résumés used during real recruitment and selection processes.
Then, we analyze and apply different proximity measures to know which are the most adequate to study the résumés of applicants.
We introduce, after, an innovative method which consists on the Relevance Feedback and the use of proximity measures over uniquely the résumés to rank applicants.
Along this thesis we show that résumés have enough information about the selection processes, in order to rank the applicants.
Nonetheless, it is important to choose correctly the proximity measure to use.
As well, we present interesting outcomes from the triple comparison between résumés and job offers.
The results obtained in this thesis are the basis for a new prototype of an e-Recruitment system and hopefully, the beginning of a new way to create these.
Earliest records of a wheeled chair used to transport a person with disability dates back to the 6m century in China.
With the exception of the collapsible X-frame wheelchairs invented in 1933, 1400 years of human scientific evolution has not radically changed the initial wheelchair design.
Meanwhile, advancements in computing and the development of artificial intelligence since the mid-1980s has inevitably led to research on Intelligent Wheelchairs.
Rather than focusing on improving the underlying design, the core objective of making a wheelchair intelligent is to make it more accessible.
Even though the invention of the powered wheelchairs have partially mitigated a user's dependence on other people for their daily routines, some disabilities that affect limb movements, motor or visual coordination, make il impossible for a user to operate a common powered wheelchair.
Accessibility can also thus be thought of as the idea, where the wheelchair adapts to the user malady such that he/she is able to utilize its assistive capabilities.
While it is certain that intelligent robots are poised to address a growing number of issues in the service and medical care industries, it is important to resolve how humans and users interact with robots in order to accomplish common objectives.
Particularly in the assistive intelligent wheelchair domain, preserving a sense of autonomy with the user is required, as individual agency is essential for his/her physical and social well-being.
This work thus aims to globally characterize the idea of assistive shared control while particularly devoting the attention to two issues within the intelligent assistive wheelchair domain viz. vision-based assistance and human-aware navigation.
Recognizing the fundamental tasks that a wheelchair user may have to execute in indoor environments, we design low­cost vision-based assistance framework for corridor navigation.
The framework provides progressive assistance for the tasks of safe corridor following and doorway passing.
Evaluation of the framework is carried out on a robotized off-the­shelf wheelchair.
From the proposed plug and play design, we infer an adaptive formulation for sharing control between user and robot.
Furthermore, keeping in mind that wheelchairs are assistive devices that operate in human environments, it is important to consider the issue of human-awareness within wheelchair mobility.
We leverage spatial social conventions from anthropology to surmise wheelchair navigation in human environments.
Moreover, we propose a motion strategy that can be embedded on a social robot (such as an intelligent wheelchair) that allows il to equitably approach and join a group of humans in interaction.
Based on the lessons learnt from the proposed designs for wheelchair mobility assistance, we can finally mathematically formalize adaptive shared control for assistive motion planning.
In closing, we demonstrate this formalism in order to design a general framework for assistive wheelchair navigation in human environments.
Named entity recognition is a crucial discipline of NLP.
It is used to extract relations between named entities, which allows the construction of knowledge bases (Surdeanu and Ji, 2014), automatic summary (Nobata et al., 2002) and so on.
Our interest in this thesis revolves around structuration phenomena that surround them.
We distinguish here two kinds of structural elements in named entities.
The first one are recurrent substrings, that we will call the caracteristic affixes of a named entity.
The second type of element is tokens with a good discriminative power, which we call trigger tokens of named entities.
We will explain here the algorithm we provided to extract such affixes, which we will compare to Morfessor (Creutz and Lagus, 2005b).
We will then apply the same algorithm to extract trigger tokens, which we will use for French named entity recognition and postal address extraction.
We propose a novel kind of linear tagger cascade which have not been used before for structured named entity recognition, generalising other previous methods that are only able to recognise named entities of a fixed depth or being unable to model certain characteristics of the structure.
This thesis, entitled: ''French as a writing academic language in Algeria.
Contrastive study between scientific fields and humanities''addresses the issue of teaching and/or learning foreign languages in Algeria through the characteristics of scientific genre.
The aim of this research is to discover if genre retains its stability in the academic writing when it is a question of practice of a foreign language, as well as French.
The French language does not exist for itself.
It is the language of studying at the Algerian university, and poses, among other factors, an obstacle to success.
Based on a heterogeneous corpus which is made up of twelve dissertations presented in Algeria, we would like to contrast, among these writings, the writing of the human and social sciences with the writing of the hard and natural sciences.
Adopting an automatic language processing through the software Hyperbase is necessary because the corpus is very large.
This tool continues to develop new techniques for scientific research.
Also, in order to acquaint ourselves with the context of using French language in Algeria, we conducted a questionnaire survey with Algerian students and teachers at the university.
The main result obtained from this research shows that genre is always dominant even in a specific context like the use of a foreign language.
This Ph.D. took the shape of a partnership between the VORTEX team in the computer science research laboratory IRIT and the company Andil, which specializes in software for e-learning.
This partnership was concluded around a CIFRE Ph.D. This plan is subsidized by the French state through the ANRT.
The Ph.D. student, Angela Bovo, worked in Université Toulouse 1 Capitole.
Another partnership was built with the training institute Juriscampus, which gave us access to data from real trainings for our experiments.
Our main goal for this project was to improve the possibilities for monitoring students in an e-learning training to keep them from falling behind or giving up.
We proposed ways to do such monitoring with classical machine learning methods, with the logs from students'activity as data.
We also proposed, using the same data, indicators of students'behaviour.
With Andil, we designed and produced a web application called GIGA, already marketed and sold, and well appreciated by training managers, which implements our proposals and served as a basis for first clustering experiments which seem to identify well students who are failing or about to give up.
However, our implementations did not get to the point of conclusive results.
This thesis focuses on the phonological aspect of the /R/ in French language spoken in Niamey, the capital of Niger, a Sub-Saharan country of Africa.
In Niamey, French coexists with others national and local languages: haousa, songhaï-zarma, touareg, peul, kanuri et arabic.
In the proposed work at first we have illustrated a phonetic and phonology classification of rhotics class, then we have classified and analyzed our data.
We have analyzed all allophones of /R/ produced by the interviewed speakers.
These data show that the largest part of the speaker pronounce a vibrant alveolar [r], followed by a fricative uvular [ʁ], and then by [ɰ], [χ], [ɻ] and [ø].
Furthermore, we have compared our results with other PFC studies conducted all around the francophone word.
Additionally, we have focused on fall of /R/ in cluster group, and we concluded that this fall depends on the lexicon, and concerns especially numbers pronunciation (for example, quatre [katR]&gt; [kat]).
Microcirculation refers to the subset of the circulatory system where extracellular gas and fluid exchanges take place.
It is composed of arterioles, capillaries and venules.
The objectives of this work are to study, understand and identify new iatrogenic etiologies to these microvascular diseases, as well as to evaluate and compare the effectiveness and safety of treatments used in these diseases.
We therefore conducted several studies using pharmacovigilance databases, clinical trial data and the literature.
This thesis work allowed us to explore the role of drugs in these microvascular pathologies, fields that were poorly studied in the literature yet.
This work has allowed us to identify many pharmacological classes whose role was unknown in these diseases.
The study of the pharmacological mechanisms underlying these adverse drug reactions also makes it possible to develop new pathophysiological hypotheses underlying these diseases.
The treatments used in these different microvascular diseases are currently not specific and important research work still needs to be carried out in order to personalize patient care.
Our thesis focuses on controlled natural languages (CNL) for software engineering.
It aims at facilitating the adoption of the business rule approach (BRA) by companies by creating a CNL in order to help business experts in the specification of their business rules.
Our solution will allow reducing the semantic gap between business experts and system experts to meet not only the need for mutual understanding between them but also to achieve an automatic transfer of the description of business rules to information systems (IS).
The CNL that we have created will also ensure the consistency and the traceability of these rules together with their implementation
Many accidents in transport, industry or healthcare result from a causal chain of events where inadvertent human errors have not been corrected in time.
In such socio-technical and dynamic systems where complexity and unpredictability widespread, training teams to risk management in real-life like situations is crucial.
This thesis aims to provide a virtual multi-player environment designed for inter-professional team training to risk management.
To that end, a method to design risk management interactive and controlled scenario has been described.
A communication system, a group decision making system and a team tracing model have been created.
They all together enable the virtual team to be free enough to manage the educational situations.
These coherent and innovative environment allows us to control the team activity and automate the edition of a personalized, individual and corporate debriefing at the end of a team training session.
The number of connected objects continues to grow to the point that billions of objects are expected in the near future.
The approach of this thesis sets up an autonomic management architecture for systems based on connected objects, combining them with other services such as weather services accessible on the Internet.
Parameters such as execution time or consumed energy are also considered in order to optimize the choices of actions to be performed and of services used.
A concrete prototype was realized in a smart city scenario with connected buses in the investment for future project: S2C2.
False information are multiplying and are spreading quickly on social networks.
In this thesis, we analyze the publications from a multimodal point of view between the text and the associated image.
Several studies were conducted during this thesis.
The first compares several types of media present on social networks and aims to discriminate them automatically.
The second one allows the detection and the localization of modifications in an image thanks to the comparison with an old version of this image.
Finally, we focused on merged knowledge based on the predictions of other research teams to create a single system.
Description of the principles of the mutual comprehension and the research projects referring itself to it, in the European Union (and in Latin America) and their applications. They are classified: Passive knowledge (receptive competences) and interactive communication.
We exploited the results of work of linguistic engineering of Patrice Pognan and Diana Lemay.
A total diagram of teaching in conformity with the concept of mutual comprehension is presented, and the list of the international, governmental or not governmental organizations concerned.
To conclude a priority list from strategies, methods and tools is established.
This thesis deals with the notion of budget to study problems of complexity (it can be computational complexity, a complex task for an agent, or complexity due to a small amount of data).
Indeed, the main goal of current techniques in machine learning is usually to obtain the best accuracy, without worrying about the cost of the task.
The concept of budget makes it possible to take into account this parameter while maintaining good performances.
We first focus on classification problems with a large number of classes: the complexity in those algorithms can be reduced thanks to the use of decision trees (here learned through budgeted reinforcement learning techniques) or the association of each class with a (binary) code.
We then deal with reinforcement learning problems and the discovery of a hierarchy that breaks down a (complex) task into simpler tasks to facilitate learning and generalization.
Here, this discovery is done by reducing the cognitive effort of the agent (considered in this work as equivalent to the use of an additional observation).
The first part of the work analyses the set of constraints involved in the exercise of sign language interpreting, as distinguished from those generally observed to apply between spoken languages (including languages syntactically far apart), such as socio-economic constraints, linguistic constraints and, finally, spatial constraints.
There follows a cognitive analysis of the interpreting process with reference to Gile's Effort model of simultaneous interpreting (Listening and Analysis Effort, Memory Effort, Production Effort, Effort of Coordination of these three simultaneous activities), with an attempt to envisage transposing its application to sign language.
In order to gain better understanding of the constituent mechanisms of the process, initial analysis of the cognitive load of the interpreter in action accords particular attention to the concept of scénarisation (scene-staging) (Séro-Guillaume, 2008).
Is this capacity for creating a visual picture from sequential meaning greater or lesser when factors such as the degree of abstraction of the speech, the technicality of its content, a lack of lexical correspondence, the interpreting context (educational setting, conference setting, etc), and the amount of preparation are taken into account?
Analysis of the process is based upon a corpus comprising several empirical studies of interpreting into sign language: a semi-experimental study, a naturalistic case study, and an experimental study, as well as on interpreter interviews and a focus group.
The observations drawn from all of these studies have enabled cross-referencing of our data and the identification of the relevant elements of our research results in order to advance understanding of the cognitive process of sign language interpreting.
Many natural language processing applications rely on word representations (also called word embeddings) to achieve state-of-the-art results.
These numerical representations of the language should encode both syntactic and semantic information to perform well in downstream tasks.
However, common models (word2vec, GloVe) use generic corpus like Wikipedia to learn them and they therefore lack specific semantic information.
Moreover it requires a large memory space to store them because the number of representations to save can be in the order of a million.
I developed dict2vec, a model that uses additional information from online lexical dictionaries when learning word representations.
The dict2vec word embeddings perform ∼15% better against the embeddings learned by other models on word semantic similarity tasks.
The second part of my work is to reduce the memory size of the embeddings.
I developed an architecture based on an autoencoder to transform commonly used real-valued embeddings into binary embeddings, reducing their size in memory by 97% with only a loss of ∼2% in accuracy in downstream NLP tasks.
Our objective is to describe the formal properties of the sentences containing support verbs in French and Malay and to compare them from the syntactic and semantic point of view.
We have bracket four tests of recognition of the support verb in order to determine the statue of these three verbs in the studied construction.
Our study is structured in six chapters.
The second chapter is a general presentation of the Malay language.
The third chapter is the study of the verb support “membuat/faire” (to do) in Malay.
The forth chapter is a study of the support verb “memberi/donner” (to give) in Malay.
The fifth chapter is a study of the support verb “mengambil/prendre” (to take) in Malay.
The sixth chapter is a comparative study between the three support verbs membuat/faire (to do), memberi/donner (to give) and mengambil/prendre (to take) in Malay and French.
The results obtained have shown that the support verbs in French and Malay share the same general characteristics.
These results also enabled us to show that in spite of the universality of the phenomenon, each language has its own mechanism concerning the function of the support verb and the system of determination.
Our work aims to build, from a deep automatic or neuronal analysis method, semantic representations of aspecto-temporal values ​​(associated with linguistic indices) of the French compound past tense.
Bitext alignment is the task of aligning a text in a source language and its translation in the target language.
Aligning amounts to finding the translational correspondences between textual units at different levels of granularity.
Many practical natural language processing applications rely on bitext alignments to access the rich linguistic knowledge present in a bitext.
While the most predominant application for bitexts is statistical machine translation, they are also used in multilingual (and monolingual) lexicography, word sense disambiguation, terminology extraction, computer-aided language learning andtranslation studies, to name a few.
Bitext alignment is an arduous task because meaning is not expressed seemingly across languages.
Current practices in bitext alignment model the alignment as a hidden variable in the translation process.
In order to reduce the complexity of the task, such approaches suppose that a word in the source sentence is aligned to one word at most in the target sentence.
However, this over-simplistic assumption results in asymmetric, one-to-many alignments, whereas alignments are typically symmetric and many-to-many.
To achieve symmetry, two one-to-many alignments in opposite translation directions are built and combined using a heuristic.
In order to use these word alignments in phrase-based translation systems which use phrases instead of words, a heuristic is used to extract phrase pairs that are consistent with the word alignment.
We improve the state of the art in several ways using discriminative learning techniques.
The interaction between alignment decisions is approximated using stackingtechniques, which allows us to account for a part of the structural dependencies without increasing the complexity.
This formulation can be seen as an alignment combination method,in which the union of several input alignments is used to guide the output alignment.
Our MaxEnt aligner obtains state of the art results in terms of alignment quality as measured by thealignment error rate, and translation quality as measured by BLEU on large-scale Arabic-English NIST'09 systems.
We reformulate the problem in the supervised framework in which we decide for each phrase pair whether we keep it or not in the translation model.
This offers a principled way to combine several features to make the procedure more robust to alignment difficulties.
We use a simple and effective method, based on oracle decoding,to annotate phrase pairs that are useful for translation.
Using machine learning techniques based on positive examples only,these annotations can be used to learn phrase alignment decisions.
Using this approach we obtain improvements in BLEU scores for recall-oriented translation models, which are suitable for small training corpora.
The development of a speech recognition system requires the availability of a large amount of resources namely, large corpora of text and speech, a dictionary of pronunciation.
Nevertheless, these resources are not available directly for Arabic dialects.
As a result, the development of a SRAP for Arabic dialects is fraught with many difficulties, namely the lack of large amounts of resources and the absence of a standard spelling as these dialects are spoken and not written.
In this perspective, the work of this thesis is part of the development of a SRAP for the Tunisian dialect.
A first part of the contributions consists in developing a variant of CODA (Conventional Orthography for Arabic Dialectal) for the Tunisian dialect.
In fact, this convention is designed to provide a detailed description of the guidelines applied to the Tunisian dialect.
Given the guidelines of CODA, we have created our corpus TARIC: Corpus of the interaction of the railways of the Tunisian Arab in the field of SNCFT.
In addition to these resources, the pronunciation dictionary is indispensable for the development of a peech recognition system.
In this regard, in the second part of the contributions, we aim at the creation of a system called conversion(Grapheme-Phonème) G2P which allows to automatically generate this phonetic dictionary.
All these resources described before are used to adapt a SRAP for the MSA of the LIUM laboratory to the Tunisian dialect in the field of SNCFT.
The evaluation of our system gave rise to WER of 22.6% on the test set.
This accident is the result of an unprecedented discrepancy between the state of the art of drilling engineers'heuristics and that of pollution response engineers.
Deepwater Horizon is in this sense a case of engineering facing extreme situation, as defined by Guarnieri and Travadel.
First, we propose to return to the overall concept of accident by means of an in-depth linguistic analysis presenting the semantic spaces in which the accident takes place.
This makes it possible to enrich its "core meaning" and broaden the shared acceptance of its definition.
Then, we bring that the literature review must be systematically supported by algorithmic assistance to process the data taking into account the available volume, the heterogeneity of the sources and the requirements of quality and relevance standards.
In fact, more than eight hundred scientific articles mentioning this accident have been published to date and some twenty investigation reports, constituting our research material, have been produced.
Our method demonstrates the limitations of accident models when dealing with a case like Deepwater Horizon and the urgent need to look for an appropriate way to formalize knowledge.
As a result, the use of upper-level ontologies should be encouraged.
The DOLCE ontology has shown its great interest in formalizing knowledge about this accident and especially in elucidating very accurately a decision-making process at a critical moment of the intervention.
The population, the creation of instances, is the heart of the exploitation of ontology and its main interest, but the process is still largely manual and not without mistakes.
This thesis proposes a partial answer to this problem by an original NER algorithm for the automatic population of an ontology.
Finally, the study of accidents involves determining the causes and examining "socially constructed facts".
This thesis presents the original plans of a "semantic pipeline" built with a series of algorithms that extract the expressed causality in a document and produce a graph that represents the "causal path" underlying the document.
It is significant for scientific or industrial research to highlight the reasoning behind the findings of the investigation team.
As a conclusion, this thesis is a work of a fitter, an architect, which offers both a prime insight into the Deepwater Horizon case and proposes the data drilling, an original method and means to address an event, in order to uncover answers from the research material for questions that had previously escaped understanding.
Our thesis falls under the enunciation theory following work of Antoine Culioli and Jean-Pierre Desclés.
Any utterance requires a enunciator.
According to whether the enunciator refers or not to the enunciative situation several types of enouncements appear.
We compared two types of utterances described by Aristotle with the representations established by Jean-Pierre Desclés in Grammar Applicative and Cognitive.
In both cases, a change is represented between two stable situations.
In both cases, a movement spreading itself is described in its non-accomplishment.
We proposed to understand the concept of entelecheia used by Aristotle when he defines the movement, starting from the concept of unaccomplished process described by Jean-Pierre Desclés.
Whereas the utterance of a change requires to validate two stable states for a single temporal reference, the utterance of a movement spreading itself requires that the enonciator constates indeed a movement that he knew possible.
A temporal parameter remains common to these two types of utterances.
The writing of a data-processing program made it possible automatically to extract the values of this temporal parameter and to distinguish the various utterances from a digitized textual corpus.
The data-processing application implements theoretical description in a software component usable by other programs to temporally organize a text for the automatic treatment of the natural languages.
The automatic processing of the Quechua language (APQL) lacks an electronic dictionary of French­ Quechua verbs.
However, any NLP project requires this important linguistic resource.
The realization of such a resource couId also open new perspectives on different domains such as multilingual access to information, distance learning,inthe areas of annotation /indexing of documents, spelling correction and eventually in machine translation.
The first challenge was the choice of the French dictionary which would be used as our basic reference.
Among the numerous French dictionaries, there are very few which are presented in an electronic format, and even less that may be used as an open source.
Among the latter, we found the dictionary Les verbes français (LVF}, of Jean Dubois and Françoise Dubois-Charlier, edited by Larousse en 1997. It is a remarkably complete dictionary. It contains 25 610 verbal senses and with open source license. It is entirely compatible with the Nooj platform.
That's why we have chosen this dictionary to be the one to translate into Quechua.
However, this task faces a considerable obstacle: the Quechua lexicon of simple verbs contains around 1,500 entries.
How to match 25,610 French verbal senses with only 1,500 Quechua verbs?
Are we condemned to produce many polysemies?
For example, in LVF, we have 27 verbal senses of the verb "tourner" to turn; should we translate them all by the Quechua verb muyuy to turn?
Or, can we make use of a particular and remarkable Quechua strategy that may allow us to face thischallenge: the generation of new verbs by suffix derivation?
As a first step, we have inventoried ail the Quechua suffixes that make possible to obtain a derived verbal form which behaves as if it was a simple verb.
This set of suffixes, which we call IPS_DRV, contains 27 elements.
Thus each Quechua verb, transitive or intransitive, gives rise to at least 27 derived verbs.
Next, we need to formalize the paradigms and grammars that will allow us to obtain derivations compatible with the morphology of the language.
This was done with the help of the NooJ platform.
The application of these grammars allowed us to obtain 40,500 conjugable atomic linguistic units (CALU) out of 1,500 simple Quechua verbs.
This encouraging first result allows us to hope to get a favorable solution to our project of translation of the 25,000 verbal senses of French into Quechua.
In order to obtain the translation of these CALUs, we first needed to know the modalities of enunciation that each IPS have and transmits to the verbal radical when it is agglutinated to it.
Each suffix can have several modalities of enunciation.
We have obtained an inventory of them from the corpus, our own experience and some recordings obtained in fieldwork.
We constructed an indexed table containing all of these modalities.
Next, we used NooJ operators to program grammars that present automatic translation into a glossed form of enunciation modalities.
Finally, we developed an algorithm that allowed us to obtain the reciprocal translation from French to Quechua of more than 8,500 Verbal senses of Level 3 and a number of verbal senses of Levels 4 and 5.
Social Media, and Twitter in particular, has become a privileged source of information for journalists in recent years.
Most of them monitor Twitter, in the search for newsworthy stories.
This thesis aims to investigate and to quantify the effect of this technological change on editorial decisions.
Second, we study different types of algorithms to automatically discover tweets that relate to the same stories.
We test several vector representations of tweets, looking at both text and text-image representations, Third, we design a new method to group together Twitter events and media events.
Finally, we design an econometric instrument to identify a causal effect of the popularity of an event on Twitter on its coverage by traditional media.
We show that the popularity of a story on Twitter does have an effect on the number of articles devoted to it by traditional media, with an increase of about 1 article per 1000 additional tweets.
This PhD thesis focuses on the automatic speech synthesis field, and more specifically on unit selection.
A deep analysis and a diagnosis of the unit selection algorithm (lattice search algorithm) is provided.
The importance of the solution optimality is discussed and a new unit selection implementation based on a A* algorithm is presented.
Three cost function enhancements are also presented.
The first one is a new way – in the target cost – to minimize important spectral differences by selecting sequences of candidate units that minimize a mean cost instead of an absolute one.
This cost is tested on a phonemic duration distance but can be applied to others.
Our second proposition is a target sub-cost addressing intonation that is based on coefficients extracted through a generalized version of Fujisaki's command-response model.
This model features gamma functions modeling F0 called atoms.
Finally, our third contribution concerns a penalty system that aims at enhancing the concatenation cost.
It penalizes units in function of classes defining the risk a concatenation artifact occurs when concatenating on a phone of this class.
This system is different to others in the literature in that it is tempered by a fuzzy function that allows to soften penalties for units presenting low concatenation costs.
The need for personalized recommendations is motivated by the overabundance of online information, products, social connections.
This typically tackled by recommender systems (RS) that learn users interests from past recorded activities.
Another context where recommendation is desirable is when estimating the relevance of an item requires complex reasoning based on experience.
Machine learning techniques are good candidates to simulate experience with large amounts of data.
The present thesis focuses on the cold-start context in recommendation, i.e. the situation where either a new user desires recommendations or a brand-new item is to be recommended.
Since no past interaction is available, RSs have to base their reasoning on side descriptions to form recommendations.
The problem of choosing an optimization algorithm in a portfolio can be cast as a recommendation problem.
We propose a two components system combining a per-instance algorithm selector and a sequential scheduler to reduce the optimization cost of a brand-new problem instance and mitigate the risk of optimization failure.
Both components are trained with past data to simulate experience, and alternatively optimized to enforce their cooperation.
The final system won the Open Algorithm Challenge 2017.Automatic job-applicant matching (JAM) has recently received considerable attention in the recommendation community for applications in online recruitment platforms.
We develop specific natural language (NL) modeling techniques and combine them with standard recommendation procedures to leverage past user interactions and the textual descriptions of job positions.
The appropriateness of various RSs on applications similar to the JAM problem are discussed.
In this thesis, we study two problems of machine learning: (I) community detection and (II) adaptive matching.
I) It is well-known that many networks exhibit a community structure.
Finding those communities helps us understand and exploit general networks.
In this thesis we focus on community detection using so-called spectral methods based on the eigenvectors of carefully chosen matrices.
We analyse their performance on artificially generated benchmark graphs.
Instead of the classical Stochastic Block Model (which does not allow for much degree-heterogeneity), we consider a Degree-Corrected Stochastic Block Model (DC-SBM) with weighted vertices, that is able to generate a wide class of degree sequences.
We consider this model in both a dense and sparse regime.
In the dense regime, we show that an algorithm based on a suitably normalized adjacency matrix correctly classifies all but a vanishing fraction of the nodes.
In the sparse regime, we show that the availability of only a small amount of information entails the existence of an information-theoretic threshold below which no algorithm performs better than random guess.
On the positive side, we show that an algorithm based on the non-backtracking matrix works all the way down to the detectability threshold in the sparse regime, showing the robustness of the algorithm.
This follows after a precise characterization of the non-backtracking spectrum of sparse DC-SBM's.
We further perform tests on well-known real networks.
II) Online two-sided matching markets such as Q&amp;A forums and online labour platforms critically rely on the ability to propose adequate matches based on imperfect knowledge of the two parties to be matched.
We develop a model of a task / server matching system for (efficient) platform operation in the presence of such uncertainty.
For this model, we give a necessary and sufficient condition for an incoming stream of tasks to be manageable by the system.
We further identify a so-called back-pressure policy under which the throughput that the system can handle is optimized.
We show that this policy achieves strictly larger throughput than a natural greedy policy.
Finally, we validate our model and confirm our theoretical findings with experiments based on user-contributed content on an online platform.
Approximate Numerical Expressions (ANE) are imprecise linguistic expressions implying numerical values, illustrated by "about 100".
We first focus on ANE interpretation, both in its human and computational aspects.
In a second step, we proposed two interpretation models, based on the same principle of a compromise between the cognitive salience of the endpoints and their distance to the ANE reference value, formalized by Pareto frontiers.
The experimental validation of the models, based on real data, show that they offer better performances than existing models.
We also show the relevance of the fuzzy model by implementing it in the framework of flexible database queries.
We then show, by the mean of an empirical study, that the semantic context has little effect on the collected intervals.
Finally, we focus on the additions and products of ANE, for instance to assess the area of a room whose walls are "about 10" and "about 20 meters" long.
We conducted an empirical study whose results indicate that the imprecisions associated with the operands are not taken into account during the calculations.
We set out to collect and combine large datasets enabling 1) the study of the spatial, temporal, linguistic and network dependencies of socioeconomic inequalities and 2) the inference of socioeconomic status (SES) from these multimodal signals.
The study of these questions is important, as much is still unclear about the root causes of SES inequalities and the deployment of ML/DL solutions to pinpoint them is still very much in its infancy.
This thesis analyses the problem of building well-founded domain ontologies for reasoning and decision support purposes.
This thesis in computer science focuses on the problem of capitalizing analysis processes of elearning traces within the Learning Analytics (LA) community.
The aim is to allow these analysis processes to be shared, adapted and reused.
This prevents them from being shared, but also from being simply reused outside their original contexts, even if the new contexts are similar.
The objective of this thesis is to provide models and methods for the capitalisation of analysis processes of elearning traces, as well as to assist the various actors involved in the analysis, particularly during the reuse phase.
Our second contribution responds to the first barrier related to the technical dependence of current analysis processes and their sharing.
We propose a meta-model that allows to describe the analysis processes independently of the analysis tools.
This meta-model formalizes the description of the operations used in the analysis processes, the processes themselves and the traces used, in order to avoid the technical constraints caused by these tools.
This formalism, common to the analysis processes, also makes it possible to consider their sharing.
It has been implemented and evaluated in one of our prototypes.
Our third contribution deals with the second lock on the reuse of analysis processes.
We propose an ontological framework for analysis processes, which allows semantic elements to be directly introduced, in a structured way, during the description of analysis processes.
This narrative approach thus enriches the previous formalism and makes it possible to satisfy the properties of understanding, adaptation and reuse necessary for capitalisation.
This ontological approach was implemented and evaluated in another of our prototypes.
Finally, our last contribution responds to the last lock identified and concerns new assistances to actors, in particular a new method of researching analysis processes, based on our previous proposals.
We also use the semantic network underlying this ontological modeling to strengthen assistance to actors by providing them with inspection and understanding tools during the research.
This assistance was implemented in one of our prototypes, and empirically evaluated.
Located in the heart of the Indian Ocean and having a surface area of 2 040 square kilometers, the Republic of Mauritius is an insular country that includes four islands: Mauritius (the main island), Rodrigues, Agaléga and Saint-Brandon.
Altogether, its nearly 1.3 million inhabitants make up a plurilingual speech community in which more than 10 languages are spoken.
In spite of this linguistic richness, shaped by its history conducive to language contact, and the plurilingual language skills of Mauritians, the most commonly spoken language at home, by 86.50% of the Mauritian population (according to the 2011's Housing and Population Census), is the Mauritian Creole: a French-based creole language.
Studies on the Mauritian Creole began during the period of colonization.
In the nineteenth century, Baissac (1880) proposed a *study on the creole patois in Mauritius* and in the twentieth century, after independence in March 1968, Baker (1972) published a book on its linguistic description.
The only contemporary grammar available is that of Police-Michel et al. (2012).
Also, various PhD research works studied and laid emphasis on the syntactic categories of the Mauritian Creole, particularly on the noun phrase (Alleesaib, 2012), the verb (Henri, 2010) and the adverb (Hassamal, 2017).
However, among all of these studies, none of them took a real interest in the field of Natural Language Processing (NLP).
David (2019) has accomplished a research work on the Part-Of-Speech (POS) tagging of the Mauritian Creole, but overall there is a lack of scientific literature in the automatic processing of this resource-poor language.
Based on these research works on the Mauritian Creole's syntax and in a view of harnessing the methods and NLP tools developed by David (2019), this thesis aims at building and exploiting a Treebank (a syntactically annotated written corpus) for the Mauritian Creole, such as the French Treebank for French (Abeillé et al., 2003) and the Penn Treebank for English (Taylor et al., 2003).
The methodological framework of this work will be organized around 5 main phases.
The first phase will focus on the constitution, standardization, and structuring of a written electronic corpus.
During the second phase, the unavailable computer tools required for this work will be newly developed while existing ones will be optimized.
The third phase will focus on the annotation of the corpus, based on a coherent and structured annotation scheme.
Then, experiments will be conducted during the fourth phase.
Finally, the fifth phase will evaluate the quality of the annotations produced and the performance of the tools developed.
Ultimately, the goal of this thesis is to achieve automatic syntactic parsing in Mauritian Creole, while providing this language with all the elements which are necessary for its automatic processing, especially under a syntactic analysis perspective.
The objective of this work is to introduce a new robust approach to treat the problem of finding the correct answer to a question.
Our first contribution is the design and implementation of a robust representation model for information.
The aim is to represent the structural information of sentences of documents and questions structural information. This representation is composed of typed groups of words (typed segments) and relations between these groups.
This model has been evaluated on several corpus (written, oral, web) and achieved good resultats, which proves his robustness.
Our second contribution consisted is the design of a re-ranking method of a set of the candidate answers output by the question-answering system.
This re-ranking method is based on the structural information representation.
The general idea is to compare a question and a passage from where a candidate answer was extracted, and to compute a similarity score by using a modified edit distance we proposed.
Our re-ranking method has been evaluated on the data of several evaluation campaigns.
The results are quite good on long and complex questions.
These results show the interest of our method: our approach is quite adapted to treat long question, whatever the type of the data.
The re-ranker has been officially evaluated on the 2010 edition of the Quaero evaluation campaign, with positives results.
We are interested in the recovery and prediction of multiple time series from partially observed and/or aggregate data.
After examining kriging from spatio-temporal statistics and a hybrid method based on the clustering of individuals, we propose a general framework based on nonnegative matrix factorization.
This frameworks takes advantage of the intrisic correlation between the multivariate time series to greatly reduce the dimension of the parameter space.
Once the estimation problem is formalized in the nonnegative matrix factorization framework, two extensions are proposed to improve the standard approach.
The first extension takes into account the individual temporal autocorrelation of each of the time series.
This increases the precision of the time series recovery.
The second extension adds a regression layer into nonnegative matrix factorization.
This allows exogenous variables that are known to be linked with electricity consumption to be used in estimation, hence makes the factors obtained by the method to be more interpretable, and also increases the recovery precision.
We produce a theoretical analysis on the framework which concerns the identifiability of the model and the convergence of the algorithms that are proposed.
The performance of proposed methods to recover and forecast time series is tested on several multivariate electricity consumption datasets at different aggregation level.
This pluridisciplinary thesis, at the interface between computer science and historical linguistics, aims to computationally model phonetic changes, based on cognate identification and prediction.
Phonological differences between cognates partly capture the divergences between the phonetic evolution of related languages.
This work is structured in three main steps.
The first one consists in the creation of an etymological lexical database large enough to allow us to train neural models of the phonetic correspondences between languages via a cognate prediction task.
The design and training of such networks is the second step of the work.
We rely on sequence-to-sequence neural architectures, similar what constitutes today the state of the art in machine translation, yet adapted to the specificities of this work, notably the small amount of available training data.
The third step is dedicated to the validation and analysis of the results produced by our neural models, in collaboration with historical linguists.
Named entities have been the topic of many researches during the 90's.
For instance, knowing that a text contains the words “Google” and “Youtube” can be relevant but being able to link them and detect an acquisition relation can be more interesting (Google has bought Youtube in 2006).
Our work is focusing on two different aspects: to define a finer perimeter around the relation between named entities definition, with linguistic aspect in mind, and to explore new techniques that make use of linguists in order to build a relation between named entities recognition system.
They play an important role in many successful applications of Natural Language Processing, such as Automatic Speech Recognition, Machine Translation and Information Extraction.
In this way, language models predict a word based on its n-1 previous words.
In spite of their prevalence, conventional n-gram based language models still suffer from several limitations that could be intuitively overcome by consulting human expert knowledge.
One critical limitation is that, ignoring all linguistic properties, they treat each word as one discrete symbol with no relation with the others.
This kind of model is constructed based on the count of n-grams in training data.
Therefore, the pertinence of these models is conditioned only on the characteristics of the training text (its quantity, its representation of the content in terms of theme, date).
These representations and the associated objective function (the likelihood of the training data) are jointly learned using a multi-layer neural network architecture.
This approach has shown significant and consistent improvements when applied to automatic speech recognition and statistical machine translation tasks.
A major difficulty with the continuous space neural network based approach remains the computational burden, which does not scale well to the massive corpora that are nowadays available.
For this reason, the first contribution of this dissertation is the definition of a neural architecture based on a tree representation of the output vocabulary, namely Structured OUtput Layer (SOUL), which makes them well suited for large scale frameworks.
The second contribution is to provide several insightful analyses on their performances, their pros and cons, their induced word space representation.
Finally, the third contribution is the successful adoption of the continuous space neural network into a machine translation framework.
New translation models are proposed and reported to achieve significant improvements over state-of-the-art baseline systems.
This thesis proposes a study the linguistic relationships between nominal predicates and causative verbs the names of emotions and causative verbs according to the methodology established in the ANR-DFG Emolex project (www.emolex.eu).
In the era of digitilization, and with the emergence of several semantic Web applications, many new knowledge bases (KBs) are available on the Web.
These KBs contain (named) entities and facts about these entities.
They also contain the semantic classes of these entities and their mutual links.
In addition, multiple KBs could be interconnected by their entities, forming the core of the linked data web.
A distinctive feature of these KBs is that they contain millions to trillions of unreliable RDF triples.
This uncertainty has multiple causes.
It can result from the integration of data sources with various levels of intrinsic reliability or it can be caused by some considerations to preserve confidentiality.
Furthermore, it may be due to factors related to the lack of information, the limits of measuring equipment or the evolution of information.
The goal of this thesis is to improve the usability of modern systems aiming at exploiting uncertain KBs.
In particular, this work proposes cooperative and intelligent techniques that could help the user in his decision-making when his query returns unsatisfactory results in terms of quantity or reliability.
The approach proposed to handle this problem is query-driven and offers a two fold advantage: (i) it provides the user with a rich explanation of the failure of his query by identifying the MFS (Minimal Failing Sub-queries) and (ii) it allows the computation of alternative queries called XSS (maXimal Succeeding Sub-queries), semantically close to the initial query, with non-empty answers.
Moreover, from a user's point of view, this solution offers a high level of flexibility given that several degrees of uncertainty can be simultaneously considered.
All our propositions have been validated with a set of experiments on different uncertain and large-scale knowledge bases (WatDiv and LUBM).
We have also used several Triplestores to conduct our tests.
Lexicon-Grammar tables, whose development was initiated by Gross (1975), are a very rich syntactic lexicon for the French language.
They cover various lexical categories such as verbs, nouns, adjectives and adverbs.
This linguistic database is nevertheless not directly usable by computer programs, as it is incomplete and lacks consistency.
To use these tables, we must make explicit the essential features appearing in each one of them.
In addition, many features must be renamed for consistency sake.
Our aim is to adapt the tables, so as to make them usable in various Natural Language Processing (NLP) applications, in particular parsing.
We describe the problems we encountered and the approaches we followed to enable their integration into a parser.
We propose LGExtract, a generic tool for generating a syntactic lexicon for NLP from the Lexicon-Grammar tables.
It relies on a global table in which we added the missing features and on a single extraction script including all operations related to each property to be performed for all tables
We also present LGLex, the new generated lexicon of French verbs, predicative nouns, frozen expressions and adverbs.
Then, we describe how we converted the verbs and predicatives nouns of this lexicon into the Alexina framework, that is the one of the Lefff lexicon (Lexique des Formes Fléchies du Français)(Sagot, 2010), a freely available and large-coverage morphological and syntactic lexicon for French.
This enables its integration in the FRMG parser (French MetaGrammar) (Thomasset et de La Clergerie, 2005), a large-coverage deep parser for French, based on Tree-Adjoining Grammars (TAG), that usually relies on the Lefff.
This conversion step consists in extracting the syntactic information encoded in Lexicon-Grammar tables.
We describe the linguistic basis of this conversion process, and the resulting lexicon.
We evaluate the FRMG parser on the reference corpus of the evaluation campaign for French parsers Passage (Produire des Annotations Syntaxiques à Grande Échelle)(Hamon et al., 2008), by comparing its Lefff-based version to our version relying on the converted Lexicon-Grammar tables.
Surveillance systems are important tools for law enforcement agencies for fighting crimes.
Surveillance control rooms have two main duties: live monitoring the surveillance areas, and crime solving by investigating the archives.
To support these difficult tasks, several significant solutions from the research and market fields have been proposed.
However, the lack of generic and precise models for video content representation make the building of fully automated intelligent video analysis and description system a challenging task.
Furthermore, the application domain still shows a big gap between the research field and the real practical needs, it also shows a lack between these real needs and the on-market video analytics tools.
Consequently, in conventional surveillance systems, live monitoring and investigating the archives still rely mostly on human operators.
This thesis proposes a novel approach for textual describing important contents in videos surveillance scenes, based on new generic context-free "VSSD ontology", with focus on two objects interactions.
The proposed ontology presents a new generic flexible and extensible ontology dedicated for video surveillance scenes description.
While analysing and understanding variety of video scenes, our approach introduces many new concepts and methods concerning mediation and action at a distant, abstraction in the description, and a new manner of categorizing the scenes.
It introduces a new heuristic way to discriminate between deformable and non-deformable objects in the scenes.
Spoken dialog systems enable users to interact with computer systems via natural dialogs, as they would with human beings.
These systems are deployed into a wide range of application fields from commercial services to tutorial or information services.
However, the communication skills of such systems are bounded by their spoken language understanding abilities.
Our work focus on the spoken language understanding module which links the automatic speech recognition module and the dialog manager.
DBN-based models allow to infer and then to compose semantic frame-based tree structures from speech transcriptions.
First, we developed a semantic knowledge source covering the domain of our experimental corpus (MEDIA, a French corpus for tourism information and hotel booking).
The inference process extracts all possible sub-trees according to lower level information and composes the hypothesized branches into a single utterance-span tree.
This work investigates a stochastic process for generating and composing semantic frames using DBNs.
The proposed approach offers a convenient way to automatically derive semantic annotations of speech utterances based on a complete frame hierarchical structure.
The amount and complexity of data generated by information systems keep increasing in Warehouses.
The domain of Business Intelligence (BI) aims at providing methods and tools to better help users in retrieving those data.
Looking for new information could be a tedious task, because business users try to reduce their work overload.
To tackle this problem, Enterprise Search is a field that has emerged in the last few years, and that takes into consideration the different corporate data sources as well as sources available to the public (e.g. World Wide Web pages).
However, corporate retrieval systems nowadays still suffer from information overload.
We believe that such systems would benefit from Natural Language (NL) approaches combined with Q&amp;A techniques.
Indeed, NL interfaces allow users to search new information in their own terms, and thus obtain precise answers instead of turning to a plethora of documents.
Major challenges for designing such a system are to interface different applications and their underlying query languages on the one hand, and to support users' vocabulary and to be easily configured for new application domains on the other hand.
This thesis outlines an end-to-end Q&amp;A framework for corporate use-cases that can be configured in different settings.
In traditional BI systems, user-preferences are usually not taken into account, nor are their specific contextual situations.
State-of-the art systems in this field, Soda and Safe do not compute search results on the basis of users' situation.
This thesis introduces a more personalized approach, which better speaks to end-users' situations.
Our main experimentation, in this case, works as a search interface, which displays search results on a dashboard that usually takes the form of charts, fact tables, and thumbnails of unstructured documents.
Depending on users' initial queries, recommendations for alternatives are also displayed, so as to reduce response time of the overall system.
This process is often seen as a kind of prediction model.
Our work contributes to the following: first, an architecture, implemented with parallel algorithms, that leverages different data sources, namely structured and unstructured document repositories through an extensible Q&amp;A framework, and this framework can be easily configured for distinct corporate settings; secondly, a constraint-matching-based translation approach, which replaces a pivot language with a conceptual model and leads to more personalized multidimensional queries; thirdly, a set of NL patterns for translating BI questions in structured queries that can be easily configured in specific settings.
In addition, we have implemented an iPhone/iPad™ application and an HTML front-end that demonstrate the feasibility of the various approaches developed through a series of evaluation metrics for the core component and scenario of the Q&amp;A framework.
To this end, we elaborate on a range of gold-standard queries that can be used as a basis for evaluating retrieval systems in this area, and show that our system behave similarly as the well-known WolframAlpha™ system, depending on the evaluation settings.
This thesis studies the problem of machine learning under budget constraints, in particular we propose to focus on the cost of the information used by the system to predict accurately.
Most methods in machine learning usually defines the quality as the performance (e.g accuracy) on the task at hand, but ignores the cost of the model itself: for instance, the number of examples and/or labels needed during learning, the memory used, or the number of features required to predict at test-time.
We present three models that learn to predict under such constraint, i.e that learn a strategy to gather only the necessary information in order to predict well but with a small cost.
We rely on representation learning techniques, along with recurrent neural networks architecture and gradient descent algorithms for learning.
In the last part of the thesis, we propose to study the problem of active-learning, where one aims at constraining the amount of labels used to train a model.
We present our work for a novel approach of the problem using meta-learning, with an instantiation using bi-directional recurrent neural networks.
One of the major research areas in computer vision is visual surveillance.
The scientific challenge in this area includes the implementation of automatic systems for obtaining detailed information about the behavior of individuals and groups.
Particularly, detection of abnormal individual movements requires sophisticated image analysis.
This thesis focuses on the problem of the abnormal events detection, including feature descriptor design characterizing the movement information and one-class kernel-based classification methods.
In this thesis, three different image features have been proposed: (i) global optical flow features, (ii) histograms of optical flow orientations (HOFO) descriptor and (iii) covariance matrix (COV) descriptor.
Based on these proposed descriptors, one-class support vector machines (SVM) are proposed in order to detect abnormal events.
Two online strategies of one-class SVM are proposed:
The notion of metric plays a key role in machine learning problems, such as classification, clustering and ranking.
Learning metrics from training data in order to make them adapted to the task at hand has attracted a growing interest in the past years.
This research field, known as metric learning, usually aims at finding the best parameters for a given metric under some constraints from the data.
The learned metric is used in a machine learning algorithm in hopes of improving performance.
Most of the metric learning algorithms focus on learning the parameters of Mahalanobis distances for feature vectors.
Current state of the art methods scale well for datasets of significant size.
On the other hand, the more complex topic of multivariate time series has received only limited attention, despite the omnipresence of this type of data in applications.
An important part of the research on time series is based on the dynamic time warping (DTW) computing the optimal alignment between two time series.
The current state of metric learning suffers from some significant limitations which we aim to address in this thesis.
The most important one is probably the lack of theoretical guarantees for the learned metric and its performance for classification.
The theory of (ℰ, ϓ, τ)-good similarity functions has been one of the first results relating the properties of a similarity to its classification performance.
A second limitation in metric learning comes from the fact that most methods work with metrics that enforce distance properties, which are computationally expensive and often not justified.
In this thesis, we address these limitations through two main contributions.
The first one is a novel general framework for jointly learning a similarity function and a linear classifier.
This formulation is inspired from the (ℰ, ϓ, τ)-good theory, providing a link between the similarity and the linear classifier.
It is also convex for a broad range of similarity functions and regularizers.
We derive two equivalent generalization bounds through the frameworks of algorithmic robustness and uniform convergence using the Rademacher complexity, proving the good theoretical properties of our framework.
Our second contribution is a method for learning similarity functions based on DTW for multivariate time series classification.
The formulation is convex and makes use of the(ℰ, ϓ, τ)-good framework for relating the performance of the metric to that of its associated linear classifier.
Using uniform stability arguments, we prove the consistency of the learned similarity leading to the derivation of a generalization bound.
The purpose of this research is to investigate a linguistic phenomenon that has been developing in Tunisia in the Social Networks cyber-territory.
This study has two main aims:
(1) to examine the Tunisian language of the capital's young people, and its realization in Arabish/Arabizi (Latin script).
(2) To build an Arabish corpus with different annotation levels (such as: tokenization, Part-of-Speech tagging) exploiting NLP approaches.
An electronic grammar is one of the most important elements in the natural language processing.
Since traditional manual grammar development is a time-consuming and labor-intensive task, many efforts for automatic grammar development have been taken during last décades.
Automatic grammar development means that a System extracts a grammar from a Treebank.
Full-scale syntactic tags and morphological analysis in Sejong Korean Treebank allow us to extract syntactic features automatically and to develop FB-LTAG.
During extraction experiments, we modify thé Treebank to improve extracted grammars and extract five différent types of grammars; four lexicalized grammars and one feature-based lexicalized grammar.
Extracted grammars are evaluated by ils size, ils coverage and ils average ambiguity.
The number of tree schemata is not stabilized at thé end of the extraction process, which seems to indicate that thé size of a Treebank is not enough to reach thé convergence of extracted grammars.
However, the number of tree schemata appeared at least twice in the Treebank is nearly stabilized at the end of the extraction process, and the number of superior grammars (the ones which are extracted after thé modification of Treebank) is also much stabilized than inferior grammars.
We also evaluate extracted grammars using LLP2 and our extracting System using other Treebank.
Finally, we compare extracted grammars with the one of Han et al.
In the last few decades, many scientists were concerned with the fast extinction of languages.
Faced with this alarming decline of the world's linguistic heritage, action is urgently needed to enable fieldwork linguists, at least, to document languages by providing them innovative collection tools and to enable them to describe these languages. Machine assistance might be interesting to help them in such a task.
This is what we propose in this work, focusing on three pillars of the linguistic fieldwork: collection, transcription and analysis.
Recordings are essential, since they are the source material, the starting point of the descriptive work.
Speech recording is also a valuable object for the documentation of the language.
LIG-AIKUMA proposes a range of different speech collection modes (recording, respeaking, translation and elicitation) and offers the possibility to share recordings between users.
Through these modes, parallel corpora are built such as "under-resourced speech - well-resourced speech", "speech - image", "speech - video", which are also of a great interest for speech technologies, especially for unsupervised learning.
We propose to use automatic techniques to help the fieldwork linguist to take advantage of all his speech collection.
Along these lines, automatic speech recognition (ASR) is a way to produce transcripts of the recordings, with a decent quality.
Once the transcripts are obtained (and corrected), the linguist can analyze his data.
In order to analyze the whole collection collected, we consider the use of forced alignment methods.
We demonstrate that such techniques can lead to fine evaluation of linguistic features.
In return, we show that modeling specific features may lead to improvements of the ASR systems.
Asthma results from multiple genetic and environmental factors and from interactions between these factors.
The global aim of this thesis was to propose gene-gene and gene-environment interaction strategies of analysis to identify new genes associated with the risk of asthma and atopy.
The tests of interactions between genetic variants are applied to the selected gene pairs.
These analyzes, conducted in three family studies (n = 3,244), identified an interaction between two genes (ADGRV1 and DNAH5) involved in ciliary mobility, an emerging mechanism in asthma.
A meta-analysis of GWAS of the time-to-asthma onset, conducted in nine studies (n = 19,348), identified a new locus associated with the risk of asthma (16q12) and confirmed four more.
Five of these nine studies included environmental factor data on early-life tobacco smoke (ELTS) exposure.
Given the amount of Arabic textual information available on the web, developing effective Information Retrieval Systems (IRS) has become essential to retrieve relevant information.
Most of the current Arabic SRIs are based on the bag-of-words representation, where documents are indexed using surface words, roots or stems.
Accordingly, we propose four contributions to improve Arabic content representation, indexing, and retrieval.
The first contribution consists of representing Arabic documents using Multi-Word Terms (MWTs).
The latter is motivated by the fact that MWTs are more precise representational units and less ambiguous than isolated SWTs.
Hence, we propose a hybrid method to extract Arabic MWTs, which combines linguistic and statistical filtering of MWT candidates.
The linguistic filter uses POS tagging to identify MWTs candidates that fit a set of syntactic patterns and handles the problem of MWTs variation.
Then, the statistical filter rank MWT candidate using our proposed association measure that combines contextual information and both termhood and unithood measures.
In the second contribution, we explore and evaluate several IR models for ranking documents using both SWTs and MWTs.
Additionally, we investigate a wide range of proximity-based IR models for Arabic IR.
Then, we introduce a formal condition that IR models should satisfy to deal adequately with term dependencies.
The third contribution consists of a method based on Distributed Representation of Word vectors, namely Word Embedding (WE), for Arabic IR.
It relies on incorporating WE semantic similarities into existing probabilistic IR models in order to deal with term mismatch.
The last contribution is a method to incorporate WE similarity into Pseud-Relevance Feedback PRF for Arabic Information Retrieval.
The main idea is to select expansion terms using their distribution in the set of top pseudo-relevant documents along with their similarity to the original query terms.
The experimental validation of all the proposed contributions is performed using standard Arabic TREC 2002/2001 collection.
Probabilistic graphical models encode the hidden dependencies between random variables for data modelling.
Parameter estimation is a crucial and necessary part of handling such probabilistic models.
These very general models have been used in plenty of fields such as computer vision, signal processing, natural language processing and many more.
We mostly focused on log-supermodular models, which is a specific part of exponential family distributions, where the potential function is assumed to be negative of a submodular function.
This property will be very handy for the maximum a posteriori and parameter learning estimations.
Despite the seemed restriction of the models of interest, they cover a broad part of exponential families, since there are plenty of functions that are submodular, e.g., graph cuts, entropy and others.
It is well known that probabilistic treatment is a challenging way for most of all models, however we were able to tackle some of the challenges at least approximately.
In this manuscript, we exploit the perturb-and-MAP ideas for the partition function approximation and thus an efficient parameter learning.
Moreover, the problem can be also interpreted as a structure learning task, where each estimated parameter or weight represents the importance of the corresponding term.
We propose a way of approximate parameter estimation and inference for the models where exact learning and inference is intractable in general case due to the partition function calculation complexity.
The first part of the thesis is dedicated to theoretical guarantees.
Given the log-supermodular models, we take advantage of the efficient minimization property related to submodularity.
Introducing and comparing two existing upper bounds of the partition function, we are able to demonstrate their relation by proving a theoretical result.
We introduce an approach for missing data as a natural subroutine of probabilistic modelling.
It appears that we can apply a stochastic technique over proposed perturb-and-map approximation approach and still maintain convergence while make it faster in practice.
The second main contribution of this thesis is an efficient and scalable generalization of the parameter learning approach.
In this section we develop new algorithms to perform the parameter estimation for various loss functions, different levels of supervision and we also work on the scalability.
In particular, working with mostly graph cuts, we were able to incorporate various acceleration techniques.
As a third contribution we deal with a general problem of learning continuous signals.
In this part, we focus on the sparse graphical models representations.
We consider common sparsity-inducing regularizers as prior-based potentials.
The proposed denoising techniques do not require choosing any precise regularizer in advance.
To perform sparse representation learning, community often use symmetric losses as l1, but we propose to parameterize the loss and learn the weight of each loss component from the data.
This is feasible via the approach we proposed in the previous sections.
For all sides of the parameter estimation mentioned above we performed computational experiments to approve the idea or compare with existing benchmarks, and demonstrate its performance in practice.
The three main results are the following.
First, we study how argumentation graphs are obtained from knowledge bases expressed in existential rules, the structural properties of such graphs and show several insights as to how their generation can be improved.
Second, we propose a framework that generates an argumentation graph with a special feature called sets of attacking arguments instead of the regular binary attack relation and show how it improves upon the current state-of-the-art using an empirical analysis.
In the latter, we set up the foundation theory for semantics that rank arguments in argumentation graphs with sets of attacking arguments.
The expansion of the radio and the development of new standards enrich the diversity and the amount of data carried by the broadcast radio waves.
It becomes wise to develop a search engine that has the capacity to make these accessible as do the search engines on the internet like Google.
Such an engine can offer many possibilities.
In that vein, the SurfOnHertz project, which was launched in 2010 and ended in 2013, aimed to develop a browser that is capable of indexing audio streams of all radio stations.
This indexing would result, among others, in the detection of keywords in the audio streams, the detection of commercials, the classification of musical genres.
The browser once developed would become the first search engine of its kind to address the broadcast content.
Taking up such a challenge requires to have a device to capture all the stations being broadcasted in the geographical area concerned, demodulate them and transmit the audio contents to the indexing engine.
Thus, the work of this thesis aim to provide digital architectures carried on a SDR platform for extracting, demodulating, and making available the audio content of each broadcast stations in the geographic area of the receiver.
Before the large number of radio standards which exist today, the thesis focuses FM and DRM30 standards.
However the proposed methodologies are extensible to other standards.
The choice of this type of component is justified by the great opportunities it offers in terms of parallelism of treatments, mastery of available resources, and embeddability.
The development of algorithms was done for the sake of minimizing the amount of the used calculations blocks.
Moreover, many implementations have been performed on a Stratix II technology which has limited resources compared to those of the FPGAs available today on the market.
This attests to the viability of the presented algorithms.
The proposed algorithms thus operate simultaneous extraction of all radio channels when the stations can only occupy uniformly spaced locations like FM in Western Europe, and also for standards of which the distribution of stations in the spectrum seems rather random as the DRM30.
Another part of the discussion focuses on the means of simultaneously demodulating it.
The applicant get insights from Parawitz'grounds (2009), dialogical Logic of Lorenzen (1961) ludic by Girard (2001) and proof theoretical semantics by Francez (2015).
It enables to draw a distinction between hitherto similar expressions 'little'/ 'a little'or 'each'/ 'every'.
The method should be developed as a module of the wide scale grammatical and logical parser for French Grail.
Such an analysis and tools are of course especially welcome for the automated analysis of argumentation, argumentative dialogues and debates.
In this thesis, we propose a new approach WCUM (Web Content and Usage Mining based approach) for linking content analysis to usage analysis of a website to better understand the general behavior of the web site visitors.
To mitigate the problem of determination of the number of clusters on rows and columns, we suggest to generalize the use of some indices originally proposed to evaluate the partitions obtained by clustering algorithms to evaluate bipartitions obtained by simultaneous clustering algorithms.
To evaluate the performance of these indices on data with biclusters structure, we proposed an algorithm for generating artificial data to perform simulations and validate the results.
Experiments on artificial data as well as on real data were realized to estimate the efficiency of the proposed approach.
Extracting Meaningful substructures from graphs has always been a key part in graph studies.
In machine learning frameworks, supervised or unsupervised, as well as in theoretical graph analysis, finding dense subgraphs and specific decompositions is primordial in many social and biological applications among many others.
In this thesis we aim at studying graph degeneracy, starting from a theoretical point of view, and building upon our results to find the most suited decompositions for the tasks at hand.
Hence the first part of the thesis we work on structural results in graphs with bounded edge admissibility, proving that such graphs can be reconstructed by aggregating graphs with almost-bounded-edge-degree.
We also provide computational complexity guarantees for the different degeneracy decompositions, i.e. if they are NP-complete or polynomial, depending on the length of the paths on which the given degeneracy is defined.
In the second part we unify the degeneracy and admissibility frameworks based on degree and connectivity.
Within those frameworks we pick the most expressive, on the one hand, and computationally efficient on the other hand, namely the 1-edge-connectivity degeneracy, to experiment on standard degeneracy tasks, such as finding influential spreaders.
Following the previous results that proved to perform poorly we go back to using the k-core but plugging it in a supervised framework, i.e. graph kernels.
Thus providing a general framework named core-kernel, we use the k-core decomposition as a preprocessing step for the kernel and apply the latter on every subgraph obtained by the decomposition for comparison.
We are able to achieve state-of-the-art performance on graph classification for a small computational cost trade-off.
Finally we design a novel degree degeneracy framework for hypergraphs and simultaneously on bipartite graphs as they are hypergraphs incidence graph.
This decomposition is then applied directly to pretrained neural network architectures as they induce bipartite graphs and use the coreness of the neurons to re-initialize the neural network weights.
This framework not only outperforms state-of-the-art initialization techniques but is also applicable to any pair of layers convolutional and linear thus being applicable however needed to any type of architecture.
Corpora are the main material of computer linguistics and natural language processing.
Web resources contain lots of noise (menus, ads, etc.).
Filtering boilerplate and repetitive data requires a large-scale manual cleaning by the researcher.
This thesis presents an automatic system that construct web corpus with a low level of noise.
The system is evaluated in terms of the efficacy of noise filtering and of computing time.
Our experiments, made on four languages, are evaluated using our own gold standard corpus.
We compare our method with three methods dealing with the same problem, Nutch, BootCat and JusText.
The performance of our system is better as regards the extraction quality, even if for computing time, Nutch and BootCat dominate.
In using Modeling and Simulation for the system Verification &amp; Validation activities, often the difficulty is finding and implementing consistent abstractions to model the system being simulated with respect to the simulation requirements.
A proposition for the unified design and implementation of modeling abstractions consistent with the simulation objectives based on the computer science, control and system engineering concepts is presented.
It addresses two fundamental problems of fidelity in simulation, namely, for a given system specification and some properties of interest, how to extract modeling abstractions to define a simulation product architecture and how far does the behaviour of the simulation model represents the system specification.
A general notion of this simulation fidelity, both architectural and behavioural, in system verification and validation is explained in the established notions of the experimental frame and discussed in the context of modeling abstractions and inclusion relations.
A semi-formal ontology based domain model approach to build and define the simulation product architecture is proposed with a real industrial scale study.
A formal approach based on game theoretic quantitative system refinement notions is proposed for different class of system and simulation models with a prototype tool development and case studies.
Challenges in research and implementation of this formal and semi-formal fidelity framework especially in an industrial context are discussed.
It is not possible for a science computing system to process a text when sequences, like words or sentences, are not annotated.
However, to date, no system has been able to automatically produce a perfect annotation of a text.
This report poses the folowing question; which is the better natural language processing system: a system designed to integrate imperfect annotations in its reasoning process or a system designed to work with perfect annotation but dealing with imperfect annotations?
To answer this, we have proposed a probabilistic inference model based on Bayesian Networks (BN), a formalism well adapted to reasoning from imperfect data.
We have worked on the resolution of the anaphoric pronoun "it" and validate our model in evaluating two BN on different corpora: a BN dedicated to the resolution of the impersonal pronoun recognition problem and a BN dealing with the choice of he antecedent problem.
Stimulated by the heavy use of smartphones, the joint use of textual and spatial data in space-textual objects (e.g., tweets, Flickr photos, POI reviews) became the mainstay of many applications, such as crisis management, tourist assistance or the finding of places of interest.
We propose to leverage geographic contexts and distributional semantics to resolve the semantic location prediction task.
Our work consists of two main contributions: (1) improving word embeddings which can be combined to construct object representations using spatial word distributions; (2) exploiting deep neural networks to perform semantic matching between tweets and POIs.
Regarding the improvement of text representations, we propose to regularize word embeddings that can be combined to construct object representations.
One based on a spatial partitioning method using the k-means algorithm, and the other one based on a probabilistic partitioning using a kernel density estimation (KDE).
Unlike existing architectures, our approach is based on joint learning of local and global interactions between tweet-POI pairs.
According to the proposed model, the exact matching signals of the local word-to-word interactions are corrected by a spatial damping factor.
This thesis studies the structuring and exploration of news collections.
While its main focus is on natural language processing and multimedia retrieval, it also deals with social studies through the study of the production of news and ergonomy through the conduct of user tests.
Hyperlinking consists in automatically finding relevant links between multimedia segments.
We apply this concept to whole news collections, resulting in the creation of a hypergraph, and study the topological properties and their influence on the explorability of the resulting structure.
In this thesis, we provide improvements beyond the state of the art along three main {axes:} a structuring of news collections by means of mutli-sources and multimodal graphs based on the creation of inter-document links, its association with a large diversity of links allowing to represent the variety of interests that different users may have, and a typing of the created links in order to make the nature of the relation between two documents explicit.
Extensive user studies confirm the interest of the methods developped in this thesis.
The web technology is in an on going growth, and a huge volume of data is generated in the social web, where users would exchange a variety of information.
In addition to the fact that social web text may be rich of information, the writers are often guided by provoked sentiments reflected in their writings.
Based on that concept, locating sentiment in a text can play an important role for information extraction.
The purpose of this thesis is to improve the book search and recommendation quality of the Open Edition's multilingual Books platform.
The Books platform also offers additional information through users generated information (e.g. book reviews) connected to the books and rich in emotions expressed in the users' writings.
Therefore, the previous analysis, concerning locating sentiment in a text for information extraction, plays an important role in this thesis, and can serve the purpose of quality improvement concerning book search, using the shared users generated information.
Accordingly, we choose to follow a main path in this thesis to combine sentiment analysis (SA) and information retrieval (IR) fields, for the purpose of improving the quality of book search.
Two objectives are summarised in the following, which serve the main purpose of the thesis in the IR quality improvement using SA:
• An approach for SA prediction, easily applicable on different languages, low cost in time and annotated data.
• New approaches for book search quality improvement, based on SA employment in information filtering, retrieving and classifying
Languages in Malaysia are dying in an alarming rate.
As of today, 15 languages are in danger while two languages are extinct.
One of the methods to save languages is by documenting languages, but it is a tedious task when performed manually.
Automatic Speech Recognition (ASR) system could be a tool to help speed up the process of documenting speeches from the native speakers.
However, building ASR systems for a target language requires a large amount of training data as current state-of-the-art techniques are based on empirical approach.
The main aim of this thesis is to investigate the effects of using data from closely-related languages to build ASR for low-resource languages in Malaysia.
Past studies have shown that cross-lingual and multilingual methods could improve performance of low-resource ASR.
In this thesis, we try to answer several questions concerning these approaches: How do we know which language is beneficial for our low-resource language?
How does the relationship between source and target languages influence speech recognition performance?
We study the effects of using data from Malay, a local dominant language which is close to Iban, for developing Iban ASR under different resource constraints.
We have proposed several approaches to adapt Malay data to obtain pronunciation and acoustic models for Iban speech.
It was based on bootstrapping techniques for improving Malay data to match Iban pronunciations.
To increase the performance of low-resource acoustic models we explored two acoustic modelling techniques, the Subspace Gaussian Mixture Models (SGMM) and Deep Neural Networks (DNN).
Results show that using Malay data is beneficial for increasing the performance of Iban ASR.
We also tested SGMM and DNN to improve low-resource non-native ASR.
We proposed a fine merging strategy for obtaining an optimal multi-accent SGMM.
In addition, we developed an accent-specific DNN using native speech data.
After applying both methods, we obtained significant improvements in ASR accuracy.
From our study, we observe that using SGMM and DNN for cross-lingual strategy is effective when training data is very limited.
This research main aim is to study reference and reference disturbance concepts, according to the dynamic and interlocutory aspects of the reference dialogical process.
Starting from a critical review of the literature related to the theoretico-methodological approach of the reference process, our first study aim to examine the actual approach of this phenomena according to structural, actional, and representational aspects of verbal interaction.
Our results lead us to the development of a new model, qualitatively more sensitive.
Its heuristic potential is tested in additional works, henceforth aiming to study referential disturbance in verbal interaction accomplished with schizophrenics.
Results obtained from two studies providing criterions giving us the possibility to characterize, in a dynamic way, the expression and emergence contexts of reference disturbance.
In addition, within the massive modularity theory framework, our results lead us to build an interpretation of the cognitive processes which are implicated.
Furthermore, results provided by a complementary study shows that reference disturbance can be considered as a relatively independent phenomena from symptomatological intensity.
Emphasizing the interest of a dynamic approach of troubles, the contribution of our results is in relation with the objectives of the pragmatic approach in psychopathology, providing trails to the renewal of mental disorders classifications and also, to a clinic of cognitive efficiency.
In 2015, the number of new cases of breast cancer in France is 54,000.
The survival rate after 5 years of cancer diagnosis is 89%.
If the modern treatments allow to save lives, some are difficult to bear.
Many clinical research projects have therefore focused on quality of life (QoL), which refers to the perception that patients have on their diseases and their treatments.
QoL is an evaluation method of alternative clinical criterion for assessing the advantages and disadvantages of treatments for the patient and the health system.
In this thesis, we will focus on the patients stories in social media dealing with their health. The aim is to better understand their perception of QoL.
This new mode of communication is very popular among patients because it is associated with a great freedom of speech, induced by the anonymity provided by these websites.
The originality of this thesis is to use and extend social media mining methods for the French language.
The main contributions of this work are: (1) construction of a patient/doctor vocabulary; (2) detection of topics discussed by patients; (3) analysis of the feelings of messages posted by patients and (4) combinaison of the different contributions to quantify patients discourse.
Firstly, we used the patient's texts to construct a patient/doctor vocabulary, specific to the field of breast cancer, by collecting various types of non-experts'expressions related to the disease, linking them to the biomedical terms used by health care professionals.
We combined several methods of the literature based on linguistic and statistical approaches.
To evaluate the relationships, we used automatic and manual validations.
Then, we transformed the constructed resource into human-readable format and machine-readable format by creating a SKOS ontology, which is integrated into the BioPortal platform.
Secondly, we used and extended literature methods to detect the different topics discussed by patients in social media and to relate them to the functional and symptomatic dimensions of the QoL questionnaires (EORTC QLQ-C30 and EORTC QLQ-BR23).
In order to detect the topics discussed by patients, we applied the unsupervised learning LDA model with relevant preprocessing.
Then, we applied a customized Jaccard coefficient to automatically compute the similarity distance between the topics detected with LDA and the items in the auto-questionnaires.
Thus, we detected new emerging topics from social media that could be used to complete actual QoL questionnaires.
This work confirms that social media can be an important source of information for the study of the QoL in the field of cancer.
Thirdly, we focused on the extraction of sentiments (polarity and emotions).
For this, we evaluated different methods and resources for the classification of feelings in French.
These experiments aim to determine useful characteristics in the classification of feelings for different types of texts, including texts from health forums.
Finally, we used the different methods proposed in this thesis to quantify the topics and feelings identified in the health social media.
In general, this work has opened promising perspectives on various tasks of social media analysis for the French language and in particular the study of the QoL of patients from the health forums.
In this thesis, we focus on bridging the semantic gap between the documents and queries representations, hence improve the matching performance.
We propose two models that integrate relational semantics into the distributed representations: a) an offline model that combines two types of pre-trained representations to obtain a hybrid representation of the document; b) an online model that jointly learns distributed representations of documents, concepts and words.
To better integrate relational semantics from knowledge resources, we propose two approaches to inject these relational constraints, one based on the regularization of the objective function, the other based on instances in the training text.
Our neural model relies on: a) a representation of raw-data that models the relational semantics of text by jointly considering objects and relations expressed in a knowledge resource, and b) an end-to-end neural architecture that learns the query-document relevance by leveraging the distributional and relational semantics of documents and queries.
Development and spread of connected devices, in particular smartphones, requires the implementation of authentication methods.
In this thesis, the evaluation of operationnal biometric systems has been studied, and an implementation is presented.
A second contribution studies the quality estimation of speech samples, in order to predict recognition performances.
This thesis is part of the deep learning applied to spoken language understanding.
Until now, this task was performed through a pipeline of components implementing, for example, a speech recognition system, then different natural language processing, before involving a language understanding system on enriched automatic transcriptions.
Recently, work in the field of speech recognition has shown that it is possible to produce a sequence of words directly from the acoustic signal.
First, we present a state of the art describing the principles of deep learning, speech recognition, and speech understanding.
Then, we describe the contributions made along three main axes.
We propose a first system answering the problematic posed and apply it to a task of named entities recognition.
Then, we propose a transfer learning strategy guided by a curriculum learning approach.
This strategy is based on the generic knowledge learned to improve the performance of a neural system on a semantic concept extraction task.
Then, we perform an analysis of the errors produced by our approach, while studying the functioning of the proposed neural architecture.
Finally, we set up a confidence measure to evaluate the reliability of a hypothesis produced by our system.
This thesis explores the usage of Abstract Categorial Grammars (ACG) for Natural Language Generation (NLG) in an industrial context.
While NLG system based on linguistic theories have a long history, they are not prominent in industry, which, for the sake of simplicity and efficiency, usually prefer more ``pragmatic" methods.
This study shows that recent advances in computational linguistics allow to conciliate the requirements of soundness and efficiency, by using ACG to build the main elements of a production grade NLG framework (document planner and microplanner), with performance comparable to existing, less advanced methods used in industry
This research project aims at better understanding the valuation of green assets as well as the financial risk embedded in sustainable investment strategies.
To achieve this, I will use machine learning algorithms, especially text mining and natural language processing in order to label green assets from brown ones and supervised algorithms to build a sustainable asset pricing model.
The motor capacities (motor skills)'evaluation is an essential activity for movement analysis.
This activity aims is to quantify the human's motor performance to be able to follow-up and control the evolution of the patient's pathology thus allowing an adapted treatment.
The physiotherapists need accurate tools able to measure this performance.
They developed their own tools based on observations and normalized exercises.
This activity can be supported and enhanced by the technological advances.
A category of motion tracking tools exists to track and record those movements.
Their use in motor evaluation system could refine the therapist's evaluations and increase their reproducibility.
To insure the correct development and use of such tools it is necessary to answer the following question: “what are the development stakes and criteria related to a system for measure and evaluation of motor capacities?”
This thesis work refined this question into the 3 following research axis: “how to measure motor capacities?”, “how to analyze and communicate the result?”, “how to integrate this system in the medical practice?”
For each of those axis the key criteria for development were investigated and contributions are presented.
To illustrate those criteria a case study was conducted: the instrumentation, with new motion capture technologies, of an assessment protocol for motor capacities also called MFM (The Motor Function Measure).
The aim of the National Cancer Registry (NCR) in Luxembourg is to collect data about cancer and the quality of cancer treatment.
To obtain high quality data that can be compared with other registries or countries, the NCR follows international coding standards and rules, such as the International Classification of Diseases for Oncology (ICD-O).
These standards are extensive and complex, which complicates the data collection process.
The operators, i.e. the people in charge of this process, are often confronted with situations where data is missing or contradictory, preventing the application of the provided guidelines.
To assist in their effort, the coding experts of the NCR answer coding questions asked by operators.
This assistance.is time consuming for experts.
To help reduce this burden on experts and to facilitate the operators' task, this project aims at implementing a coding assistant that would answer coding questions.
From a scientific point of view, this thesis tackles the problem of extracting the information from a set of data sources under a given set of rules and guidelines.
Case-based reasoning has been chosen as the method for solving this problem given its similarity with the reasoning process of the coding experts.
The method designed to solve this problem relies on arguments provided by coding experts in the context of previously solved problems.
This document presents how these arguments are used to identify similar problems and to explain the computed solution to both operators and coding experts.
A preliminary evaluation has assessed the designed method and has highlighted key areas to improve.
While this work focused on cancer registries and medical coding, this method could be generalized to other domains.
Medical sector is a dynamic domain that requires continuous improvement of its business processes and assistance to the actors involved.
This research focuses on the medical treatment process requiring prosthesis implantation.
The specificity of such a process is that it makes in connection two lifecycles belonging to medical and engineering domains respectively.
This implies several collaborative actions between stake holders from heterogeneous disciplines.
However, several problems of communication and knowledge sharing may occur because of the variety of semantic used and the specific business practices in each domain.
To do so, a conceptual framework is proposed for the analysis of links between the disease (medical domain) and the prosthesis (engineering domain) lifecycles.
Based on this analysis, a semantic ontology model for medical domain is defined as part of a global knowledge-based PLM approach proposition.
The application of the proposition is demonstrated through an implementation of useful function in the AUDROS PLM software.
This thesis aims at illustrating the realization and the use of a model for anaphora resolution in oral dialogs in the field of computational linguistics.
We don't pretend having discovered the most successful solution for the problem of anaphora in discourse.
However, we propose that the study and analysis of anaphoric structures needs the use of complex formal theories such as government and binding.
Oral language has it's own logic, but standard forms of language appears largely in it
Natural language processing and more particularly automatic understanding of documents aims to propose methods for extracting relevant information from them.
The most effective approaches today use supervised machine learning approaches and very large amounts of manually annotated examples.
This thesis topic proposes to answer two problems: 1) how to generate synthetic data?
The main objective of this thesis is to propose a recommendation system allowing retailers to improve their assortments of products distributed through numerous stores.
In this context, the problem addressed is the assortment planning which consists in eliciting the best products, e.g., those with the highest turnover.
To this end, we first propose a comparison of assortment planning with the pragmatic methods which are commonly used in the industry and the state of the art.
This comparison highlights the problem of cross-functionality of the knowledge used today to improve the assortment.
To overcome this problem, we propose knowledge structures specific to mass distribution.
Thanks to these structures, an Agile assortment optimisation method that can be integrated into a continuous improvement process is formalised.
This method makes possible to integrate human expertise, which we deem essential, in the various levers currently adopted.
To underline the modularity of our approach, we then propose a semantic analysis of the stores which, in addition to improving the accuracy of our simulations, allows us to define a new axis of assortment improvement.
This analysis is based on our proposals both for domain ontologies which are specific to each brand and on semantic similarity measures.
Finally, to perfect our method and go further in the exploitation of those structures, we propose a semantic analysis of the consumers who are the final targets of the assortment.
This second semantic analysis allows us to bring new knowledge to retailers and new constraints on assortments.
In parallel to these scientific contributions, different applications have been developed to highlight the interoperability of our contributions with concepts specific to different types of retailers (e.g. Food, DIY...).
These applications are presented in the manuscript within the limits of respect for confidentiality and intellectual property.
Pronoun resolution is the process in which an anaphoric pronoun is linked to its antecedent.
In a normal situation, humans do not experience much cognitive effort due to this process.
However, automatic systems perform far from human accuracy, despite the efforts made by the Natural Language Processing community.
Experimental research in the field of psycholinguistics has shown that during pronoun resolution many linguistic factors are taken into account by speakers.
An important question is thus how much influence each of these factors has and how the factors interact with each-other.
A second question is how linguistic theories about pronoun resolution can incorporate all relevant factors.
In this thesis, we propose a new approach to answer these questions: computational simulation of the cognitive load of pronoun resolution.
On the other hand, robust computational systems can be run on uncontrolled data such as eye movement corpora and thus provide an alternative to hand-constructed experimental material.
First, we simulated the cognitive load of pronouns by learning the magnitude of impact of various factors on corpus data.
Second, we tested whether concepts from Information Theory were relevant to predict the cognitive load of pronoun resolution.
Finally, we evaluated a theoretical model of pronoun resolution on a corpus enriched with eye movement data.
Our research shows that multiple factors play a role in pronoun resolution and that their influence can be estimated on corpus data.
We also demonstrate that the concepts of Information Theory play a role in pronoun resolution.
We conclude that the evaluation of hypotheses on corpus data enriched with cognitive data ---- such as eye movement data --- play an important role in the development and evaluation of theories.
The work of this thesis focuses on the consequences of digital technology development on research practice in the humanities in the broad sense and particularly in history.
The introduction of digital technology disrupts historical research practices by making available to the researcher a large volume of digitized sources as well as numerous analysis and writing tools.
These new capacities of research allow the discipline to adopt new approaches and renew certain points of view, but they also raise methodological and epistemological questions.
Given this observation, we have chosen to study in more detail the impact of information retrieval tools, digital libraries and search engines on historical research activity.
These systems offer access to a large volume of historical documents but they depend on computer processes that are mostly invisible to users and acting as black boxes.
The main objective of this work is to give users the means to observe and understand these processes in order to allow them to integrate their side effects in a suitable methodology.
In order to better position our object of study, we propose a conceptual framework based on the notion of digital resource.
Based on this conceptual framework, we propose an analysis of digital libraries and historical sources search engines according to each context.
These indicators are then crossed with the functioning of the system, in its contexts of production and execution, to reveal the potential methodological biases.
Following these analyzes, we propose a reinvestment of these results in the form of a software tool dedicated to teaching a critical approach to online information retrieval for student in history.
This work is evaluated by an experimental approach.
This prototype has been the subject of several experimental phases related to its development, the evaluation of these features and its impact on practice in a training context.
In this thesis, we explore inertial-based gesture recognition on Smartphones, where gestures holding a semantic value are drawn in the air with the device in hand.
In our research, speed and delay constraints required by an application are critical, leading us to the choice of neural-based models.
Thus, our work focuses on metric learning between gesture sample signatures using the "Siamese" architecture (Siamese Neural Network, SNN), which aims at modelling semantic relations between classes to extract discriminative features, applied to the MultiLayer Perceptron.
Indeed, after a preprocessing step where the data is filtered and normalised spatially and temporally, the SNN is trained from sets of samples, composed of similar and dissimilar examples, to compute a higher-level representation of the gesture, where features are collinear for similar gestures, and orthogonal for dissimilar ones.
While the original model already works for classification, multiple mathematical problems which can impair its learning capabilities are identified.
Consequently, as opposed to the classical similar or dissimilar pair; or reference, similar and dissimilar sample triplet input set selection strategies, we propose to include samples from every available dissimilar classes, resulting in a better structuring of the output space.
Furthermore, the notion of polar sine enables a redefinition of the angular problem by maximising a normalised volume induced by the outputs of the reference and dissimilar samples, which effectively results in a Supervised Non-Linear Independent Component Analysis.
Finally, we assess the unexplored potential of the Siamese network and its higher-level representation for novelty and error detection and rejection.
To summarise, the proposed SNN allows for supervised non-linear similarity metric learning, which extracts discriminative features, improving both inertial gesture classification and rejection.
In this thesis, we focus on the automatic recognition and translation of the speech of Arabic and dialectal videos.
The statistical approaches proposed in the literature for automatic speech recognition are language independent and they are applicable to modern standard Arabic.
However, this language presents some characteristics that we need to take into consideration in order to boost the performance of the speech recognition system.
Among these characteristics we can mention the absence of short vowels in the text, which makes their training by the acoustic model difficult.
We proposed several approaches to acoustic and/or language modeling in order to better recognize the Arabic speech.
In the Arab world, modern standard Arabic is not the mother tongue, that is why daily conversations are carried out with dialect, an Arabic inspired from modern standard Arabic, but not only.
We worked on the adaptation of the speech recognition system developed for the modern standard Arabic to the Algerian dialect, which is one of the most difficult variants of the Arabic language to recognize by automatic speech recognition systems.
This is mainly due to the borrowed words from other languages, the code-switching and the lack of resources.
Our approach to overcome all these problems is to take advantage from oral and textual data of other languages that have an impact on the dialect in order to train the required models for dialect speech recognition.
The resulting text from Arabic speech recognition system was then used for machine translation.
As a starting point, we conducted a comparative study between the phrase based approach and the neural approach used in machine translation.
Then, we adapted these two approaches to translate the code-switched text.
Our study focused on the mix of Arabic and English in a parallel corpus extracted from official documents of the United Nations.
In order to prevent the error propagation in the pipeline system, we worked on the adaptation of the vocabulary of the automatic speech recognition system and on the proposition of a new model that directly transforms a speech signal in language A into a sequence of words in another language B.
Making sense of textual data is an essential requirement in order to make computers understand our language.
To extract actionable information from text, we need to represent it by means of descriptors before using knowledge discovery techniques.
The goal of this thesis is to shed light into heterogeneous representations of words and how to leverage them while addressing their implicit sparse nature.
First, we propose a hypergraph network model that holds heterogeneous linguistic data in a single unified model.
In other words, we introduce a model that represents words by means of different linguistic properties and links them together accordingto said properties.
Our proposition differs to other types of linguistic networks in that we aim to provide a general structure that can hold several types of descriptive text features, instead of a single one as in most representations.
This representationmay be used to analyze the inherent properties of language from different points of view, or to be the departing point of an applied NLP task pipeline.
Secondly, we employ feature fusion techniques to provide a final single enriched representation that exploits the heterogeneous nature of the model and alleviates the sparseness of each representation.
These types of techniques are regularly used exclusively to combine multimedia data.
In our approach, we consider different text representations as distinct sources of information which can be enriched by themselves.
This approach has not been explored before, to the best of our knowledge.
Thirdly, we propose an algorithm that exploits the characteristics of the network to identify and group semantically related words by exploiting the real-world properties of the networks.
In contrast with similar methods that are also based on the structure of the network, our algorithm reduces the number of required parameters and more importantly, allows for the use of either lexical or syntactic networks to discover said groups of words, instead of the singletype of features usually employed.
We focus on two different natural language processing tasks: Word Sense Induction and Disambiguation (WSI/WSD), and Named Entity Recognition (NER).
In total, we test our propositions on four different open-access datasets.
Specifically, our experiments are twofold: first, we show that using fusion-enriched heterogeneous features, coming from our proposed linguistic network, we outperform the performance of single features' systems and other basic baselines.
While based on previous work, we improve it by obtaining better overall performance and reducing the number of parameters needed.
Contrary to other similar resources, insteadof just storing its part of speech tag and its dependency relations, we also take into account the constituency-tree information of each word analyzed.
The hope is for this resource to be used on future developments without the need to compile suchresource from zero.
In recent years, Deep Learning techniques have swept the state-of-the-art of many applications of Machine Learning, becoming the new standard approach for them.
The architectures issued from these techniques have been used for transfer learning, which extended the power of deep models to tasks that did not have enough data to fully train them from scratch.
This thesis'subject of study is the representation spaces created by deep architectures.
First, we study properties inherent to them, with particular interest in dimensionality redundancy and precision of their features.
Our findings reveal a strong degree of robustness, pointing the path to simple and powerful compression schemes.
Then, we focus on refining these representations.
We choose to adopt a cross-modal multi-task problem, and design a loss function capable of taking advantage of data coming from multiple modalities, while also taking into account different tasks associated to the same dataset.
In order to correctly balance these losses, we also we develop a new sampling scheme that only takes into account examples contributing to the learning phase, i.e. those having a positive loss.
Finally, we test our approach in a large-scale dataset of cooking recipes and associated pictures.
Our method achieves a 5-fold improvement over the state-of-the-art, and we show that the multi-task aspect of our approach promotes a semantically meaningful organization of the representation space, allowing it to perform subtasks never seen during training, like ingredient exclusion and selection.
The results we present in this thesis open many possibilities, including feature compression for remote applications, robust multi-modal and multi-task learning, and feature space refinement.
For the cooking application, in particular, many of our findings are directly applicable in a real-world context, especially for the detection of allergens, finding alternative recipes due to dietary restrictions, and menu planning.
Robot swarms are systems composed of a large number of rather simple robots.
Due to the large number of units, these systems, have good properties concerning robustness and scalability, among others.
However, it remains generally difficult to design controllers for such robotic systems, particularly due to the complexity of inter-robot interactions.
Consequently, automatic approaches to synthesize behavior in robot swarms are a compelling alternative.
In this thesis, we focus on online behavior adaptation in a swarm of robots using distributed Embodied Evolutionary Robotics (EER) methods.
To this end, we provide three main contributions:
(1) We investigate the influence of task-driven selection pressure in a swarm of robotic agents using a distributed EER approach.
We evaluate the impact of a range of selection pressure strength on the performance of a distributed EER algorithm.
The results show that the stronger the task-driven selection pressure, the better the performances obtained when addressing given tasks.
(2) We investigate the evolution of collaborative behaviors in a swarm of robotic agents using a distributed EER approach.
We perform a set of experiments for a swarm of robots to adapt to a collaborative item collection task that cannot be solved by a single robot.
Our results show that the swarm learns to collaborate to solve the task using a distributed approach, and we identify some inefficiencies regarding learning to choose actions.
(3) We propose and experimentally validate a completely distributed mechanism that allows to learn the structure and parameters of the robot neurocontrollers in a swarm using a distributed EER approach, which allows for the robot controllers to augment their expressivity.
Our experiments show that our fully-decentralized mechanism leads to similar results as a mechanism that depends on global information
Smart-phones and tablets are nowadays one of the most used objects in everyday life (personal or professional usage).
The diversity of mobile applications'sources and the easiness of downloading/using new applications make it hard for a typical user to manage or care about the security of his device.
To deal with this issue, different kinds of methods have been developed recently.
We can mention: static analysis, dynamic analysis or behavioral analysis.
Pradeo offers to its clients a solution based on advanced static analysis.
This product was developed during several PHD programs in partnership with LIRMM.
However, this solution does not include automatic learning from data collected over devices in order to prevent risky situations on mobile devices.
During this PHD program, we will apply data analysis and artificial intelligence methods in order to correlate data coming from several data sources and thus implement a robust behavioral analysis.
We think that the methods present in current literature can't be applied directly to our problem because of the velocity and diversity of received data and also the limitations imposed by mobile environments.
The following points detail what needs to be considered during the design of this solution:
- OS shouldn't be modified and the agent should be executed using simple users
- Energy consumption is an important parameter to consider ; it should be minimal during the collection of data over devices
- During the collection process, the receiving system should be capable to handle, index and query data coming from +1 million devices
- The error margin of the final model should be minimal-All mobile devices should be taken into account (iOS, Android, etc.)
The goal of this project is to enrich the Pradeo product in order to identify risky situations on mobile devices.
We aim to add an intelligent model to analyze in (almost) real-time:
- Events coming in real-time from devices
- Reports of static analysis generated by the core product of Pradeo-External sources of vulnerabilities.
Our research concerns the public policy analysis on how Cloud Computing and Big data are adopted by French and Moroccan States with a comparative approach between the two models.
The impact of the digital on the organization of States and Government ;
The concept related to the data protection, data privacy ; The limits between security, in particular home security, and the civil liberties ;
The future and the governance of the Internet ;
A use case on how the Cloud could change the daily work of a public administration ;
Our research aims to analyze how the public sector could be impacted by the current digital (re) evolution and how the States could be changed by emerging a new model in digital area called Cyber-State.
We tried to analyze the digital transformation by looking on how the public authorities treat the new economics, security and social issues and challenges based on the Cloud Computing and Big Data as the key elements on the digital transformation.
We tried also to understand how the States – France and Morocco-face the new security challenges and how they fight against the terrorism, in particular, in the cyberspace.
We studied the recent adoption of new laws and legislation that aim to regulate the digital activities.
We analyzed the limits between security risks and civil liberties in context of terrorism attacks.
We analyzed the concepts related to the data privacy and the data protection.
This rnodel gives the opportunity to extract most lipreading information according to a in depth bibliographical study.
On these images, automatic lip location without external constraints is still unsolved.
To label lips automatically, we use two repetitions of the same sentence by the same subject, with and without blue make up: onceagain, the blue sequence enables easy lip location and dynamic time warping (dtw) allows to estimate lip shape on natural images using the extracted shapes on blue images.
The appearance model obtained is very similar to the one obtained when training the same initial model with hand-Iabeled images and is quite better than other models relying on hue.
The rise of work in affective computing sees the emergence of various research questions to study agent / human interactions.
Among them raises the question of the impact of interpersonal relations on the strategies of communication.
Human/agent interactions usually take place in collaborative environments in which the agent and the user share common goals.
The interpersonal relations which individuals create during their interactions affects their communications strategies.
Moreover, individuals who collaborate to achieve a common goal are usually brought to negotiate.
This type of negotiation allows the negotiators to efficiently exchange information and their respective expertise in order to better collaborate.
The objective of this thesis is to study the impact of the interpersonal relationship of dominance on collaborative negotiation strategies between an agent and a human.
This work is based on studies from social psychology to define the behaviours related to the manifestation of dominance in a negotiation.
We propose a collaborative negotiation model whose decision model is governed by the interpersonal relation of dominance.
Depending on its position in the dominance spectrum, the agent is able to express a specific negotiation strategy.
In parallel, the agent simulates an interpersonal relationship of dominance with his interlocutor.
To this aim, we provided the agent with a model of theory of mind that allows him to reason about the behaviour of his interlocutor in order to predict his position in the dominance spectrum.
Afterwards, the agent adapts his negotiation strategy to complement the negotiation strategy detected in the interlocutor.
Our results showed that the dominance behaviours expressed by our agent are correctly perceived by human participants.
Furthermore, our model of theory of mind is able de make accurate predictions of the interlocutor behaviours of dominance with only a partial representation of the other's mental state.
Finally, the simulation of the interpersonal relation of dominance has a positive impact on the negotiation: the negotiators reach a good rate of common gains and the negotiation is perceived comfortable which increases the liking between the negotiators.
Previous research demonstrates that having access to the syntactic structure of sentences helps children to discover the meaning of novel words.
This implies that infants need to get access to aspects of syntactic structure before they know many words.
Since in all the world's languages the prosodic structure of a sentence correlates with its syntactic structure, and since function words/morphemes are useful to determine the syntactic category of words, infants might use phrasal prosody and function words to bootstrap their way into lexical and syntactic acquisition.
In this thesis, I empirically investigated the role of phrasal prosody and function words to constrain syntactic analysis in young children (PART 1) and whether infants exploit this information to learn the meanings of novel words (PART 2).
In part 1, I constructed minimal pairs of sentences in French and in English, testing whether children exploit the relationship between syntactic and prosodic structures to drive their interpretation of noun-verb homophones.
I demonstrated that preschoolers use phrasal prosody online to constrain their syntactic analysis.
When listening to French sentences such as [La petite ferme][… – [The little farm][…, children interpreted ferme as a noun, but in sentences such as [La petite][ferme…] – [The little girl][closes…, they interpreted ferme as a verb (Chapter 3).
This ability was also attested in English-learning preschoolers who listened to sentences such as 'The baby flies…': they used prosodic information to decide whether “flies” was a noun or a verb (Chapter 4).
Importantly, in further studies I demonstrated that even infants around 20-months use phrasal prosody to recover syntactic structures and to predict the syntactic category of upcoming words (Chapter 5), an ability which would be extremely useful to discover the meaning of unknown words.
This is what I tested in part 2: whether the syntactic information obtained from phrasal prosody and function words could allow infants to constrain their acquisition of word meanings.
A first series of studies relied on right-dislocated sentences containing a novel verb in French: [ili dase], [le bébéi]-'hei is dasing, the babyi'(meaning 'the baby is dasing') which is minimally different from the transitive sentence [il dase le bébé] (he is dasing the baby).
28-month-olds were shown to exploit prosodic information to constrain their interpretation of the novel verb meaning (Chapter 6).
In a second series of studies, I investigated whether phrasal prosody and function words constrain the acquisition of nouns and verbs.
I used sentences like 'Regarde la petite bamoule', which can be produced either as [Regarde la petite bamoule!]-Look at the little bamoule!, where 'bamoule'is a noun, or as [Regarde], [la petite] [bamoule!] -Look, the little (one) is bamouling, where bamoule is a verb.
18-month-olds correctly parsed such sentences and attributed a noun or verb meaning to the critical word depending on its position within the syntactic-prosodic structure of the sentences (Chapter 7).
Taken together, these studies show that infants exploit function words and the prosodic structure of an utterance to recover the sentences' syntactic structure, which in turn constrains the possible meaning of novel words.
This powerful mechanism might be extremely useful for infants to construct a first-pass syntactic structure of spoken sentences even before they know the meanings of many words.
Although prosodic information and functional elements can surface differently across languages, our studies suggest that this information may represent a universal and extremely useful tool for infants to access syntactic information through a surface analysis of the speech stream, and to bootstrap their way into language acquisition.
Even if intangible capital represents an increasingly important part of the value of our enterprises, it's not always possible to store, trace or capture knowledge and expertise, for instance in middle sized projects.
Email it still widely used in professional projects especially among geographically distributed teams.
In this study we present a novel approach to detect zones inside business emails where elements of knowledge are likely to be found.
We define an enhanced context taking into account not only the email content and metadata but also the competencies of the users and their roles.
Also linguistic pragmatic analysis is added to usual natural language processing techniques.
After describing our model and method KTR, we apply it to a real life corpus and evaluate the results based on machine learning, filtering and information retrieval
Facial expression analysis is an important problem in many biometric tasks, such as face recognition, face animation, affective computing and human computer interface.
In this thesis, we aim at analyzing facial expressions of a face using images and video sequences.
We divided the problem into three leading parts.
First, we study Macro Facial Expressions for Emotion Recognition and we propose three different levels of feature representations.
Then, we incorporate the time dimension to extract spatio-temporal features with the objective to describe subtle feature deformations to discriminate ambiguous classes.
Second, we direct our research toward transfer learning, where we aim at Adapting Facial Expression Category Models to New Domains and Tasks.
Thus we study domain adaptation and zero shot learning for developing a method that solves the two tasks jointly.
Our method is suitable for unlabelled target datasets coming from different data distributions than the source domain and for unlabelled target datasets with different label distributions but sharing the same context as the source domain.
Therefore, to permit knowledge transfer between domains and tasks, we use Euclidean learning and Convolutional Neural Networks to design a mapping function that map the visual information coming from facial expressions into a semantic space coming from a Natural Language model that encodes the visual attribute description or use the label information.
The consistency between the two subspaces is maximized by aligning them using the visual feature distribution.
Then, a statistical based model for estimating the probability density function of normal facial behaviours while associating a discriminating score to spot micro-expressions is learned based on a Gaussian Mixture Model.
Finally, an adaptive thresholding technique for identifying micro expressions from natural facial behaviour is proposed.
Our algorithms are tested over deliberate and spontaneous facial expression benchmarks.
The aim of this research is to investigate multi-modality biometric image quality assessment methods for unconstrained samples.
Studies of biometrics noted the significance of sample quality for a recognition system or a comparison algorithm because the performance of the biometric system depends mainly on the quality of the sample images.
The need to assess the quality of multi-modality biometric samples is increased with the requirement of a high accuracy multi-modality biometric systems.
Following an introduction and background in biometrics and biometric sample quality, we introduce the concept of biometric sample quality assessment for multiple modalities.
Recently established ISO/IEC quality standards for fingerprint,iris, and face are presented.
In addition, sample quality assessment approaches which are designed specific for contact-based and contactless fingerprint, near infrared-based iris and visible wavelength iris, as well as face are surveyed.
Following the survey, approaches for the performance evaluation of biometric sample quality assessment methods are also investigated.
Based on the knowledge gathered from the biometric sample quality assessment challenges, we propose a common framework for the assessment of multi-modality biometric image quality.
We review the previous classification of image-based quality attributes for a single biometric modality and investigate what are the common image-based attributes for multi-modality.
Then we select and re-define the most important image-based quality attributes for the common framework.
In order to link these quality attributes to the real biometric samples, we develop anew multi-modality biometric image quality database which has both high quality sample images and degraded images for contactless fingerprint, visible wavelength iris, and face modalities.
The degradation types are based on the selected common image-based quality attributes.
Another important aspect in the proposed common framework is the image quality metrics and their applications in biometrics.
We first introduce and classify the existing image quality metrics and then conducted a brief survey of no-reference image quality metrics, which can be applied to biometric sample quality assessment.
Plus, we investigate how no-reference image quality metrics have been used for the quality assessment for fingerprint, iris, and face biometric modalities.
The experiments for the performance evaluation of no-reference image quality metrics for visible wavelength face and iris modalities are conducted.
The experimental results indicate that there are several no-reference image quality metrics that can assess the quality of both iris and face biometric samples.
Through the work carried out in this thesis we have shown the applicability of no-reference image quality metrics for the assessment of unconstrained multi-modality biometric samples.
The topic of this Ph.D. thesis lies on the borderline between signal processing, statistics and computer science.
This framework is by nature suited for learning from distributed collections or data streams, and has already been instantiated with success on several unsupervised learning tasks such as k-means clustering, density fitting using Gaussian mixture models, or principal component analysis.
We improve this framework in multiple directions.
First, it is shown that perturbing the sketch with additive noise is sufficient to derive (differential) privacy guarantees.
Sharp bounds on the noise level required to obtain a given privacy level are provided, and the proposed method is shown empirically to compare favourably with state-of-the-art techniques.
Then, the compression scheme is modified to leverage structured random matrices, which reduce the computational cost of the framework and make it possible to learn on high-dimensional data.
Lastly, we introduce a new algorithm based on message passing techniques to learn from the sketch for the k-means clustering problem.
They enable users to pay or sign numeric documents for example.
Because they contain sensible information about their user and secrets, attackers are interested in them.
In particular, these attackers can use fuzzing.
This attack consists in sending the most possible communication messages to a program in order to detect its vulnerabilities.
This thesis aims at protecting smart cards against fuzzing.
Two approaches for detecting implementation errors are proposed.
The first one is from the state of the art, and it is adapted and improved for Java.
It is based on an automated source code analysis.
The second approach analyses source codes too, but it takes into account limitations of the first one.
In particular, the precision and the dimension reduction is improved by using Natural Language Processing techniques.
In addition, it studies plagiarsm techniques in order to reinforce its analysis against different implementations choices.
Both approaches are evaluated against three manually created oracles from OpenPGP and AES implementations for the neighborhood discovery and the anomaly detection.
Results show that the second approach is improved in precision, recall and with less execution time than the first one.
Its implementation, Confiance can be used in companies to secure source codes.
This dissertation examines C-to-V coarticulation in French and its interaction with others sources of variation in order to better understand what modulates and governs variation in speech.
Based on data from large speech corpora, we tested how C-to-V coarticulation is a function of: 1) the articulatory properties of the tested segments, i.e. 18.5k vowels /i, e, ɛ, a, x, u, o, ɔ/ (/x/=/ø, œ, ə/) in ALVeolar, UVular et VELar contexts; 2) the prosodic position occupied by the vowels, comparing the degree of coarticulation of 17k CV and VC sequences V=/i, e, a, ɔ/ and C=ALV|UV in initial position of intonational phrases, to similar sequences in internal word position; 3) the speech style, by analyzing the degree of coarticulation in 22k CV and VC sequences, V = /i, E, a, u, ɔ/ (/E/ = /e, ɛ/) and C = ALV|UV, in journalistic and conversational speech.
However, some results suggest that the modulation of coarticulation by prosodic position and speech style have different linguistic functions whose implications for speech variation will be discussed.
Finally, a reflection on sound changes related to the universal preference for the anteriorization of back closed vowels will be proposed from the observed differences between the vowels.
In this regard, incoherent interpretive schemes and majority influence are examples for the former and performance drawbacks as well as learning difficulties associated to hierarchical methodologies are instances of the latter.
Based on the results of the literature review, two experiments were conducted.
The first experiment inquired into the impact of disciplinary group composition (H1) as well as of the applied methodology (H2) on the creative group problem solving process and its outcomes.
In a laboratory experiment 60 participants, 45 with a life science background and 15 with a mechanical engineering background were trained either in instances of intuitive approaches (Brainstorming, Mind Mapping) or in analytical, hierarchical methodology (TRIZ/USIT).
Then, they had to solve an ill-defined medical problem in either mono- or multidisciplinary teams.
Statistical analyses (ANOVA, Correlation parameters and Attraction rates), to a certain extent, support H1 and H2.
More importantly however, the experiment shows differences related to method performance in general and as a function of disciplinary group composition in particular.
The model incorporates two of the most important concepts of TRIZ, and is sought to facilitate creative problem solving attempts in both, mono- and multidisciplinary teams.
The said model was tested during an industrial NCD study in the roller bearing industry.
After the case study, the participating engineers were asked to compare the applied model and the associated technology integration process with existing approaches used in the company.
The results of the experiment point to superior performance of the presented model in terms of knowledge transfer-related and idea quality-related criteria.
However, required resources for process conduction and necessary effort for the learning of the approach were considered comparable to existing approaches.
The present Ph.D. work contributes to the understanding of creative problem solving in interdisciplinary groups in general and related to technology integration in particular.
Especially the comparison of more pragmatic intuitive methods with more hierarchical analytical approaches depending on disciplinary group composition provided relevant insight for R&amp;D processes.
The goal of this thesis is to model the semantic and topical context of new proper names in order to retrieve those which are relevant to the spoken content in the audio document.
Training context models is a challenging problem in this task because several new names come with a low amount of data and the context model should be robust to errors in the automatic transcription.
Probabilistic topic models and word embeddings from neural network models are explored for the task of retrieval of relevant proper names.
A thorough evaluation of these contextual representations is performed.
The proposed Neural Bag-of-Weighted-Words (NBOW2) model learns to assign a degree of importance to input words and has the ability to capture task specific key-words.
Experiments on automatic speech recognition on French broadcast news videos demonstrate the effectiveness of the proposed models.
Evaluation of the NBOW2 model on standard text classification tasks shows that it learns interesting information and gives best classification accuracies among the BOW models
Those latest decades, the development of information end communication technologies has deeply modified die way we access knowledge.
Facing the volume end the diversity of date, it is necessary to work out robust end efficient technologies to retrieve information.
We question named entities status (related notions, typologies, evaluation end annotation) and propose properties to define their linguistic nature.
We conclude this part by describing state-of-the-art approaches end by presenting our contribution, focused on markers (tags) diet begin or end an annotation.
In die second part, we present die formalism used to mine date.
The framework we use to enrich date, explore sequences and extract annotation rules is formalized.
The lest part describes the implemented system (mXS) and the obtained results.
Specific implementation details are given and results about rule extraction from data are reported.
Finally, we provide quantitative results of the performance of mXS on Ester2 end Etape datasets, among with various indications about die behaviour of die system from diverse points of view and in diverse configurations.
They show diet our approach gives competitive results end that it opens up new perspectives for natural language processing and automatic annotation.
This thesis proposes to develop new methods to learn dialogue strategies with reinforcement learning with the objective of solving the dual challenge of task-oriented dialog systems: Designing conversational agents both efficient to solve a task and that reach human-level language communication.
As the production of digital texts grows exponentially, a greater need to analyze text corpora arises in various domains of application, insofar as they constitute inexhaustible sources of shared information and knowledge.
We therefore propose in this thesis a novel visual analytics approach for the analysis of text corpora, implemented for the real and concrete needs of investigative journalism.
Motivated by the problems and tasks identified with a professional investigative journalist, visualizations and interactions are designed through a user-centered methodology involving the user during the whole development process.
Specifically, investigative journalists formulate hypotheses and explore exhaustively the field under investigation in order to multiply sources showing pieces of evidence related to their working hypothesis.
Carrying out such tasks in a large corpus is however a daunting endeavor and requires visual analytics software addressing several challenging research issues covered in this thesis.
First, the difficulty to make sense of a large text corpus lies in its unstructured nature.
We resort to the Vector Space Model (VSM) and its strong relationship with the distributional hypothesis, leveraged by multiple text mining algorithms, to discover the latent semantic structure of the corpus.
Although the exploration of the coarse-grained topics helps locate topic of interest and its neighborhood, the identification of specific facts, viewpoints or angles related to events or stories requires finer level of structuration to represent topic variants.
This nested structure, revealed by Bimax, a pattern-based overlapping biclustering algorithm, captures in biclusters the co-occurrences of terms shared by multiple documents and can disclose facts, viewpoints or angles related to events or stories.
This thesis tackles issues related to the visualization of a large amount of overlapping biclusters by organizing term-document biclusters in a hierarchy that limits term redundancy and conveys their commonality and specificities.
We evaluated the utility of our software through a usage scenario and a qualitative evaluation with an investigative journalist.
In addition, the co-occurrence patterns of topic variants revealed by Bima. are determined by the enclosing topical structure supplied by the coarse-grained topic extraction method which is run beforehand.
Nonetheless, little guidance is found regarding the choice of the latter method and its impact on the exploration and comprehension of topics and topic variants.
Therefore we conducted both a numerical experiment and a controlled user experiment to compare two topic extraction methods, namely Coclus, a disjoint biclustering method, and hierarchical Latent Dirichlet Allocation (hLDA), an overlapping probabilistic topic model.
Over the past three decades, millions of people have been producing and sharing information on the Web,
Currently, billions of RDF descriptions are available on the Web through the Linked Open Data cloud projects (e.g., DBpedia and LinkedGeoData).
Also, several data providers have adopted the principles and practices of the Linked Data to share, connect, enrich and publish their information using the RDF standard, e.g., Governments (e.g., Canada Government), universities (e.g., Open University) and companies (e.g., BBC and CNN).
As a result, both individuals and organizations are increasingly producing huge collections of RDF descriptions and exchanging them through different serialization formats (e.g., RDF/XML, Turtle, N-Triple, etc.).
For that purpose, we have defined a framework entitled R2NR which normalizes different RDF descriptions pertaining to the same information into one normalized representation, which can then be tuned both at the graph level and at the serialization level, depending on the target application and user requirements.
We illustrate this approach by introducing use cases (real and synthetics) that need to be normalized.
The contributions of the thesis can be summarized as follows:
i. Producing a normalized (output) RDF representation that preserves all the information in the source (input) RDF descriptions,
ii. Eliminating redundancies and disparities in the normalized RDF descriptions, both at the logical (graph) and physical (serialization) levels,
iii. Computing a RDF serialization output adapted w.r.t. the target application requirements (faster loading, better storage, etc.),
iv. Providing a mathematical formalization of the normalization process with dedicated normalization functions, operators, and rules with provable properties,
and v. Providing a prototype tool called RDF2NormRDF (desktop and online versions) in order to test and to evaluate the approach's efficiency.
In order to validate our framework, the prototype RDF2NormRDF has been tested through extensive experimentations.
Experimental results are satisfactory show significant improvements over existing approaches, namely regarding loading time and file size, while preserving all the information from the original description.
Some natural language processing applications have to deal with textual data streams characterized by the use of an evolving vocabulary, whether at the creation of words as at the change in the meaning of already existing words.
In light of those observations, we have developed an incremental algorithm which can build automatically an evolving lexical database for identifying lexical units observed in a textual data stream.
This structured representation is completed with a cartographic model taking into account the continuous aspects of meaning and semantic proximity between concepts.
This property is exploited to propagate the classification of a small number of named entities (NEs: lexical units which usually refer to people, places, organizations...) to others NEs observed in unlabelled data streams during the incremental construction of the lattice.
Once the lexical database is built, the concepts are enriched with NEs labels observed in a training corpus.
The concepts and their attached labels are then respectively used for unsupervised annotation and supervised classification of NEs in test corpus.
Modeling time series has practical applications in many domains: speech, gesture and handwriting recognition, synthesis of realistic character animations etc...
The starting point of our modeling is that an important part of the variability between observation sequences may be the consequence of a few contextual variables that remain fixed all along a sequence or that vary slowly with time.
For instance a sentence may be uttered quite differently according to the speaker emotion, a gesture may have more amplitude depending on the height of the performer etc...
This method relies on sharing information between classes where in generative models classes are normally considered independent.
The structure of a neural network determines to a large extent its cost of training and use, as well as its ability to learn.
These two aspects are usually in competition: the larger a neural network is, the better it will perform the task assigned to it, but the more it will require memory and computing time resources for training.
Within this context, neural networks with various structures are trained, which requires a new set of training hyperparameters for each new structure tested.
The aim of the thesis is to address different aspects of this problem.
The first contribution is a training method that operates within a large perimeter of network structures and tasks, without needing to adjust the learning rate.
The second contribution is a network training and pruning technique, designed to be insensitive to the initial width of the network.
The last contribution is mainly a theorem that makes possible to translate an empirical training penalty into a Bayesian prior, theoretically well founded.
This work results from a search for properties that theoretically must be verified by training and pruning algorithms to be valid over a wide range of neural networks and objectives.
Through any location based services application (LBA) (i.e. m-tourism), users who request information while on the move, intentionally seek as well a quick and precise answer on any map.
Typically, these datasets had been collected from many geographic databases worldwide.
However, the increasing number of different GDBs covering the same area and the retrieval of accurate data/metadata for the requested service will imply lots of reasoning processes and databases' accesses in order to avoid nearly-duplicated records when displayed on the screen.
In other words, my ultimate goal is to generate automatically a unique map from multiple providers' portrayals such as Google Maps, Bing and Yahoo Maps while homologous features should be integrated to avoid duplicate icons on the mobile screen.
Our conceptual framework, based on some fusion algorithms, ontology reasoning for cartographic interoperability and geo-web services orchestration, had been implemented in some modular prototypes and tested for evaluation purpose.
Just-In-Time recommender systems involve all systems able to provide recommendations tailored to the preferences and needs of users in order to help them access useful and interesting resources within a large data space.
Our work falls within this framework and focuses on developing a proactive context-aware recommendation approach for mobile devices that covers many domains.
Indeed, the development of mobile devices equipped with persistent data connections, geolocation, cameras and wireless capabilities allows current context-aware recommender systems (CARS) to be highly contextualized and proactive.
We also take into consideration to which degree the recommendation might disturb the user.
It is about balancing the process of recommendation against intrusive interruptions.
As a matter of fact, there are different factors and situations that make the user less open to recommendations.
As we are working within the context of mobile devices, we consider that mobile applications functionalities such as the camera, the keyboard, the agenda, etc., are good representatives of the user's interaction with his device since they somehow stand for most of the activities that a user could use in a mobile device in a daily basis such as texting messages, chatting, tweeting, browsing or taking selfies and pictures.
In this thesis, we have investigated how to exploit user-generated-content for personalized news recommendation purpose.
The intuition behind this line of research is that the opinions provided by users, on news websites, represent a strong indicator about their profiles.
We have addressed this problem by proposing three main contributions.
Firstly, we have proposed a profile model that accurately describes both users' interests and news article contents.
Secondly, we have investigated the problem of noise on opinions and how we can retrieve only relevant opinions in response to a given query.
Results show that our approach outperforms two recent proposed opinions ranking strategies, particularly for controversial topics.
Results show that diverse opinions give the best performance over other leveraging strategies.
The discourse structure of a document is a key element to understand the content conveyed by a text.
It affects, for instance, the temporal structure of a text, or the interpretation of anaphoric expressions.
In this thesis, we will study the effects of the discourse structure on sentiment analysis.
Sentiment analysis is an extremely active research domain in natural language processing.
The last years have seen the multiplication of the available textual data conveying opinion on the web, and the automation of the summary of opinion documents became crucial for who wants to keep an overview of the opinion on a given subject.
Most of the current research efforts describe an opinion extraction at the document level or at the sentence level, ignoring the discourse structure.
In this thesis work, we address opinion extraction through the discourse framework of the SDRT (Segmented Discourse Representation Theory), and try to answer to the following questions:
- Is there a link between the discourse structure of a document and the opinions contained in that document?
- What is the role of discourse relations in the determination of whether a textual segment is objective or subjective?
- What is the impact of the discourse structure in the determination of the overall opinion conveyed by a document?
- Does a discourse based approach really bring additional value compared to a classical "bag of words" approach?
Complex Event Processing (CEP) consists of the analysis of data-streams in order to extract particular patterns and behaviours described, in general, in a logical formalism.
In the classical approach, data of a stream – or events – are supposed to be the complete and perfect observation of the system producing these events.
However, in many cases, the means for collecting such data, such as sensors, are not infallible and may miss the detection of a particular event or on the contrary produce.
In this thesis, we have studied the possible models of representation of uncertainty and, thus, to offer the CEP a robustness to this uncertainty as well as the necessary tools to allow the recognition of complex behaviours based on the chronicle formalism.
In this perspective, three approaches have been considered.
The first one is based on Markov logical networks to represent the structure of the chronicles under a set of logical formulas of a confidence value.
We show that this model, although widely applied in the literature, is inapplicable for a realistic application with regard to the dimensions of such a problem.
The second approach is based on techniques from the SAT community to enumerate all possible solutions of a given problem and thus to produce a confidence value for the recognition of a chronicle expressed, again, under a logical structure.
Finally, we propose a last approach based on the Markov chains to produce a set of samples explaining the evolution of the model in agreement with the observed data.
These samples are then analysed by a recognition system to count the occurrences of a particular chronicle.
This thesis proposes theoretical and numerical contributions to use Entropy-regularized Optimal Transport (EOT) for machine learning.
We introduce Sinkhorn Divergences (SD), a class of discrepancies between probability measures based on EOT which interpolates between two other well-known discrepancies: Optimal Transport (OT) and Maximum Mean Discrepancies (MMD).
We develop an ecient numerical method to use SD for density fitting tasks, showing that a suitable choice of regularization can improve performance over existing methods.
Making semantic parsers robust to lexical and stylistic variations is a real challenge with many industrial applications.
Nowadays, semantic parsing requires the usage of domain-specific training corpora to ensure acceptable performances on a given domain.
Transfer learning techniques are widely studied and adopted when addressing this lack of robustness, and the most common strategy is the usage of pre-trained word representations.
However, the best parsers still show significant performance degradation under domain shift, evidencing the need for supplementary transfer learning strategies to achieve robustness.
This work proposes a new benchmark to study the domain dependence problem in semantic parsing.
We use this bench to evaluate classical transfer learning techniques and to propose and evaluate new techniques based on adversarial learning.
All these techniques are tested on state-of-the-art semantic parsers.
We claim that adversarial learning approaches can improve the generalization capacities of models.
We test this hypothesis on different semantic representation schemes, languages and corpora, providing experimental results to support our hypothesis.
This thesis aims to improve the intuitiveness of human-computer interfaces.
In particular, machines should try to replicate human's ability to process streams of information continuously.
However, the sub-domain of Machine Learning dedicated to recognition on time series remains barred by numerous challenges.
Our studies use gesture recognition as an exemplar application, gestures intermix static body poses and movements in a complex manner using widely different modalities.
To do so, we reimplemented the two within a shared test-bed which is more amenable to a fair comparative work.
We propose adjustments to Neural Network training losses and the Hybrid NN-HMM expressions to accommodate for highly imbalanced data classes.
Although recent publications tend to prefer BDRNNs, we demonstrate that Hybrid NN-HMM remain competitive.
Finally, we show that input representations learned via both approaches are largely inter-compatible.
The second part of our work studies one-shot learning, which has received relatively little attention so far, in particular for sequential inputs such as gestures.
We propose a model built around a Bidirectional Recurrent Neural Network.
Its effectiveness is demonstrated on the recognition of isolated gestures from a sign language lexicon.
We propose several improvements over this baseline by drawing inspiration from related works and evaluate their performances, exhibiting different advantages and disadvantages for each
Developers are eager to create various web applications to meet people's increasing demands.
To build a web application, developers need to know some basic programming technologies.
Moreover, they prefer to use some third-party components (such as server-side libraries, client-side libraries, REST services) in the web applications.
By including those components, they could benefit from maintainability, reusability, readability, and efficiency.
In this thesis, we propose to help developers to use third-party components when they create web applications.
We present three impediments when developers using the third-party components: What are the best JavaScript libraries to use?
How to get the standard specifications of REST services?
How to adapt to the data changes of REST services?
As such, we present three approaches to solve these problems.
Those approaches have been validated through several case studies and industrial data.
We describe some future work to improve our solutions, and some research problems that our approaches can target.
Combinatorial Optimization (CO) is an area of research that is in a constant progress.
Solving a Combinatorial Optimization Problem (COP) consists essentially in finding the best solution (s) in a set of feasible solutions called a search space that is usually exponential in cardinality in the size of the problem.
To solve COPs, several methods have been proposed in the literature.
A distinction is made mainly between exact methods and approximation methods.
Since it is not possible to aim for an exact resolution of NP-Complete problems when the size of the problem exceeds a certain threshold, researchers have increasingly used Hybrid (HA) or parallel computing algorithms in recent decades.
In this thesis we consider the COP class of Survivability Network Design Problems.
We present an approximation parallel hybrid algorithm based on a greedy algorithm, a Lagrangian relaxation algorithm and a genetic algorithm which produces both lower and upper bounds for flow-based formulations.
In order to validate the proposed approach, a series of experiments is carried out on several applications: the k-Edge-Connected Hop-Constrained Network Design Problem (kHNDP) when L = 2,3, The problem of the Steiner k-Edge-Connected Network Design Problem (SkESNDP) and then, two more general problems namely the kHNDP when L >= 2 and the k-Edge-Connected Network Design Problem (kESNDP).
The experimental study of the parallelisation is presented after that.
In the last part of this work, we present a two parallel exact algorithms: a distributed Branch-and-Bound and a distributed Branch-and-Cut.
A series of experiments has been made on a cluster of 128 processors and interesting speedups has been reached in kHNDP resolution when k=3 and L=3.
Numerous chemicals are used as ingredients by the cosmetics industry and are included in cosmetics formula.
Aside from the assessment of their efficacy, the cosmetics industry especially needs to assess the safety of their chemicals for human.
Toxicological screening of chemicals is performed with the aim of revealing the potential toxic effect of the tested chemical.
Among the potential effects we want to detect, the developmental toxicity of the chemical (teratogenicity), meaning its capability of provoking abnormalities during the embryonic development, is crucial.
With respect to the international regulations that forbid the use of animal testing for the safety assessment of cosmetics, the toxicological assessment of chemicals must base on an ensemble of in silico assays, in vitro assays and alternative models based assays.
For now, a few alternative methods have been validated in the field of developmental toxicology.
The development of new alternative methods is thus required.
In addition to the safety assessment, the environmental toxicity assessment is also required.
The use of most of cosmetics and personal care products leads to their rejection in waterways after washing and rince.
This results in the exposition of some aquatic environments (surface waters and coastal marine environments) to chemicals included in cosmetics and personal care products.
Thus, the environmental assessment of cosmetics and of their ingredients requires the knowledge of their toxicity on organisms that are representative of aquatic food chains.
In this context, the fish embryo model, which is ethically acceptable according to international regulations, presents a dual advantage for the cosmetics industry.
Firstly, as a model representative of aquatic organisms, it is accurate for the environmental assessment of chemicals.
Secondly, this model is promising for the assessment of the teratogenic effect of chemicals on human.
For this reason, a teratogenicity assessment test is developed.
This test is based on the analysis of medaka fish embryos (Oryzias Latipes) at 9 days post fertilization, after balneation in a predetermined concentration of the chemical under study.
The analysis of functional and morphological parameters allows to calculate a teratogenicity index, that depends on both rates of dead and malformed embryos.
This index allows to to draw a conclusion concerning the teratogenic effect of the chemical.
The objective of this project is to automate the teratogenicity test, by automated image and video classification.
We then focus on the detection of two common malformations: axial malformations and absence of a swim bladder, based on a machine learning classification.
This analysis must be completed by the detection of other malformations so that we can measure the rate of malformed embryos and thus, calculate the teratogenicity index of the tested chemical
Our research focuses on two interrelated objectives.
The first one aims at providing assistance to the evaluation of scientific writings because of; the numbers of publication, which keep on rising, the boundaries between the areas become and it's becoming more and more difficult to find relevant publications so as the practical need for assessments is appearing as unavoidable.
We also have to find new ways to help the evaluation of the research work, through a wide range of indications different from those usually used for research assessment, notably, through the identification of the research problem.
These indications that announce the formulation of the research problem in scientific articles can be identified as "speech formulas. "
Our research does not extend to the formulation of the scientific problem because of the complexity of this concept and the difficulty of defining it from the point of view of information extraction.
We propose a model of the speech forms that we have integrated into the parser Xerox Incremental Parser (XIP) in pattern recognition rules.
We used a corpus of Educational Sciences Research articles from the Scientext corpus to detect these speeches formulas.
The choice of field is motivated by my participation in the European project EERQI which aims to strengthen and enhance the worldwide visibility and competitiveness of European research in education.
Different methodological approaches were adopted to perform a fine linguistic study of these formulas as: discourse analysis (Mr. Pecman, 2004, K. Hyland, 2005, Á, Sándor A. Kaplan, G. Randeau, 2006, D. Siepman, 2007, A. Tutin, 2007-2010), robust parsing (S. Aït-Mokhtar, J. Chanod, R. Roux, 2002).
Thus, the goal is to implement an applicative approach aiming at helping expert reading through the identification, typology and functioning of lexical associations which convey the research problem.
This thesis that the issue is part of the controversy in the number of affixes in the Fon language, it led us to ask if we can make the affixal derivation on his own names?
We solved this problem through the different parts that make up this thesis.
We input, discussed the theory of "Switch", the application starts with the identification of phonological units, their definition and their classification according to their oppositional traits, and contrastive, and also how these units are combined them.
We realize that in his language, composition and derivation are also factors training nouns and verbs.
Our approach in this area has been to follow a logical order from the smallest units up to larger: phonemes, syllables, phonological words.
A state of the art of lexicology and lexicography is done.
However there is still embrionnaire, despite the fact that it was started with the European penetration, the works of missionaries, and the implementation of new projects in the advent of independence.
Part deals with derivational morphology, it is one of the most important parts of the thesis.
In an onomastic approach, it implements the derivation of the nomenclature of the names of the kings of Abomey, the organizational structure of power in the royal court to which is added the matrix method of creating new names.
At the same orde idea, a syntactic and morphological study of the numbering system was made to facilitate counting in his language.
Through an ethno-linguistic analysis, we processed through a varied typology, event anthroponyms: choice of personal names related to life, fate, the destiny, death, family, fertility, friendship and success, the names of days, months and names of people who were born on a certain day of the week, names that reflect local and ethnological realities.
In a dynamic perspective, after taking stock of lexicology and lexicography for tabulating the period until the beginning of computerization, that is to say, lexicology and automation, dictionaric to lead to the creation bilingual etymological dictionary in fon and French languages.
Inputs are both quantitative and qualitative, because our problem has been resolved, we opened a perspective towards the computerization of languages on the one hand and the other hand on the problem of the emergence of national languages as factors of development to meet the Millennium development Goals (MDGs).
Last years, e-recruitment expansion has led to the multiplication of web channels dedicated to job postings.
In an economic context where cost control is fundamental, assessment and comparison of recruitment channel performances have become necessary.
The purpose of this work is to develop a decision-making tool intended to guide recruiters while they are posting a job on the Internet.
This tool provides to recruiters the expected performance on job boards for a given job offer.
The job offer performance predictive algorithm is based on a hybrid recommender system, suitable to the cold-start problem.
The hybrid system, based on a supervised similarity measure, outperforms standard multivariate models.
Myotonic dystrophy (DM) is considered one of the most complex neuromuscular diseases.
Although research work over the past 30 years has permitted a better understanding of its underlying molecular mechanisms, the unusual nature of its genetic anomalies, its multisystemic expression and its broad clinical spectrum do not allow, at the moment, optimal patient management.
The purpose of my work was to deepen our knowledge of this rare disease and to clarify its natural history.
The first part of my manuscript is dedicated to the presentation of the DM-Scope Registry, on which all my thesis work is based.
After the description of the concept, the functioning and the data collection platform, the manuscript features the characteristics of the DM1 cohort, from which our analyses were conducted: the clinical spectrum covered, multisystemic impairment, genotype/phenotype correlations, interrelations between symptoms and comparison to myotonic dystrophy type II (DM2).
In the second part, we focus on the major progress achieved through the existence of DM-Scope and the analyses conducted during my thesis:
(i) detailing the natural history of the disease, in particular proposing a new classification;
(ii) highlighting the phenotype's determining factors such as gender, mutation size, interrelations between symptoms.
This work has led to recommendations for care, in particular for the transition from child to adult, but also the validation of important inclusion criteria for clinical trials such as gender.
DM-Scope provides access to available biological samples for basic research studies and validates new therapeutic approaches.
DM-Scope is now a worldwide leader and an essential tool in translational research in DM.
The DM-Scope concept can be transferred to any other population and can be used for care management in other rare diseases.
Finally, we present the development of a survival model built from the DM-Scope cohort.
This model has three specificities:
(i) it is applicable to high dimensional data, in such cases as DM-Scope, where there is a large number of measurements;
(ii) it takes into account competitive risks, when patients are simultaneously exposed to several events.
In our registry, the study of respiratory-related deaths is biased if competing events such as heart disease deaths are not taken into account ;
(iii) it models the heterogeneity between patient groups probably due to divergent care, called "centres effects".
DM-Scope data analysis requires such specificity of frailty models due to its multicentric coverage (55 centres).
This model can be transferred and applied to other data, considering the following: more and more large-scaled registries are being used ; a majority of survival analyses includes censorship caused by the occurrence of the event of interest ; multicentre studies have become increasingly common.
Our prototype implements all theabove mentioned artifacts and proposes a workflow enabling its users to evaluate andimprove CMs efficiently and effectively.
We conducted a survey to validate the selection of the quality constraints through a first experience and also conducted a second experiment to evaluate the efficacy and efficiency of our overall approach and proposed improvements.
A Classification problem makes use of a training set consisting of data labeled by an oracle.
The larger the training set, the best the performance.
However, requesting the oracle may be costly.
The goal of Active Learning is thus to minimize the number of requests to the oracle while achieving the best performance.
To Increase the precision on the estimate, we need to label more data.
Thus, there is a dilemma between labeling data in order to increase the performance of the classifier or to better know how to select data.
This dilemma is well studied in the context of finite budget optimization under the name of exploration versus exploitation dilemma.
The most famous solutions make use of the principle of Optimism in the Face of Uncertainty.
In this thesis, we show that it is possible to adapt this principle to the active learning problem for classification.
Several algorithms have been developed for classifiers of increasing complexity, each one of them using the principle of Optimism in the Face of Uncertainty, and their performances have been empirically evaluated
Bilingual lexicons are central components of machine translation and cross-lingual information retrieval systems.
Their manual construction requires extensive expertise in both languages involved and it is a costly process.
Several automatic methods were proposed as an alternative but they often rely of resources available in a limited number of languages and their performances are still far behind the quality of manual translations.
Our work concerns bilingual lexicon extraction from multilingual parallel and comparable corpora, in other words, the process of finding translation pairs among the common multilingual vocabulary available in such corpora.
The advent and availability of machine learning and artificial intelligence grabbed the headlines in the past few years, opening doors to their application in various sectors such as banking, finance, medical, etc.
Machine learning can help preventing fraud and improving risk management, investment predictions and decision making.
Learning models also became more prominent in the healthcare sector with many potential applications offering high-quality and cost-effective care to patients.
The rapid evolutions in the learning models are facilitating the development of Natural Language Processing (NLP) tools that can exploit and manipulate huge amount of data.
Driven by the aforementioned motivations, the main purpose of this PhD thesis is to contribute to new learning methods for dynamic complex systems in the healthcare sector.
The candidate which is employed by a medical center in Saudi Arabia has access to a large amount of anonymous clinical data.
He has to explore a range of AI tools, techniques, and frameworks and apply them to the clinical data in order to extract valuable knowledge and generate accurate predictive results.
This process includes many steps:
Anonymizing the data
Cleansing the data: removing noise and detecting outliers
Creating learning, validation and testing data sets (balanced er imbalanced)
Proposing and comparing different language models such as Bert, doc2vec, etc.
Adding new rules to refine the existing language models
Applying machine learning algorithms to train the models
Comparing the accuracy and performance of the trained models
The trained models can be used to tackle many classification or prediction problems.
A primary outcome of the study is a clinical explainable auto-diagnosis tool that can assist medical doctors.
This thesis aims to analyze uniquely-referring names which refer to singular referents indicating a place, a person, an event, an institution or a product of human activity in a corpus of tourist guide specific to Algeria.
Information theory has influenced a large number of scientific fields since its first introduction in 1948.
Apart from Fitts'law and Hick's law, which came out when experimental psychologists were still enthusiastic about applying information theory to various areas of psychology, the relation between information theory and human-computer interaction (HCI) has rarely been explored.
This thesis demonstrates that information theory can be used as a unified tool to understand and design human-computer communication.
In reliance upon contrastive analysis, our study examines the standpoints which become manifest through the use of terminology as it relates to public policy in security matters.
Our research rests upon a selection of non-binding instruments in French and in Spanish, published between 2001 and 2018 by the European Commission and by two Member States, namely France and Spain.
Behind our decision to analyse documents issued by both European and domestic authorities, lie peculiarities in the way institutional discourse is produced.
Recent studies in the area have shown that such discourse fosters a rhetoric of consensus, and tends to drown political debate.
As it happens, these communication strategies themselves reflect clear-cut ideological standpoints.
By examining how such terms circulate in an environment as diverse as the EU, one uncovers discrepancies that prove to be typical of the varied utterances produced in collective security matters.
For our purpose here, we have adopted a theoretical approach that weaves the study of terminology into certain notions of French discourse analysis (ADF).
Terminology places the term itself, i.e. the lexical unit used in a specialised field of knowledge, at its very core.
Our own study will therefore focus on the value which the lexicon acquires when uttered by a legitimate authority – the institution – in the specific area of security policy.
Terminological research has gradually shown that like lexical units, terms are bound both to the context in which they are used and to the circumstances in which their surrounding discourse is produced.
Variants in the expressions used, as these emerge from each sub-corpus, will thus depend on the linguistic and extra-linguistic environment surrounding use of the term.
Against that background, we have set out to ascertain whether the variants might be symptomatic of discrepant ideological standpoints.
In that sense, ADF, which has traditionally dealt with the ideologies underpinning language, lends us the notions one requires to grasp the reasons that may lie behind shifts in a given term.
The methodological approach has allowed us to combine lexicometric analysis of the corpus with detailed, in-context observation of a term's inter-textual recurrences and terminographical sources.
We set out our results further to a course of analysis which in reliance on deductive method, begins with selecting certain terms: “prévention”, “détection”, “répression”, “combattant terroriste étranger” et “criminalité transfrontalière”.
The terms have been selected based upon upstream research into the literature of international relations, and typify the tensions that feed into academic, political and legal debate.
In the first section, we examine the terms which pertain to strategic action (“prévention”, “détection”, “répression”), and in the second, we reflect on how the notions of a threat and the enemy have been conceptualised (“combattant terroriste étranger” and “criminalité transfrontalière”).
In conclusion, our work has been designed to examine the mismatches and interpretative loopholes that are generated, as terms circulate and are put to the purpose of legitimising rhetorical practices.
Our thesis will demonstrate that at the end of the day, institutional discourse in security matters has obscured a debate which is nonetheless very much alive, and a worthwhile topic of concern to a far wider public
The massive increase in the availability of data generated everyday by individuals on the Internet has made it possible to address the predictability of financial markets from a different perspective.
Without making the claim of offering a definitive answer to a debate that has persisted for forty years between partisans of the efficient market hypothesis and behavioral finance academics, this dissertation aims to improve our understanding of the price formation process in financial markets through the use of Big Data analytics.
Examining users' self-reported trading characteristics, the essay provides empirical evidence of sentiment-driven noise trading at the intraday level, consistent with behavioral finance theories.
The second essay proposes a methodology to measure investor attention to news in real-time by combining data from traditional newswires with the content published by experts on the social media platform Twitter.
The essay demonstrates that news that garners high attention leads to large and persistent change in trading activity, volatility, and price jumps.
The third essay provides new insights into the empirical literature on small capitalization stocks market manipulation by examining a novel dataset of messages published on the social media plat-form Twitter.
The essay proposes a novel methodology to identify suspicious behaviors by analyzing interactions between users and provide empirical evidence of suspicious stock recommendations on social media that could be related to market manipulation.
Its aim is to model the knowledge of the field of the balance of payments and the international investment position, i.e. the concepts and the terms denoting them.
Natural language processing (NLP) tools and an ontoterminology editor will be used, and the work will imply interactions with domain experts.
The ontoterminologies built will be exported in machine readable exchange formats, e.g. W3C.
Ultimate goal of this thesis is to elaborate an e-dictionary which will facilitate knowledge sharing and transfer to non-experts.
With the rise of the number of sensors and actuators in an aircraft and the development of reliable data links from the aircraft to the ground, it becomes possible to improve aircraft security and maintainability by applying real-time analysis techniques.
However, given the limited availability of on-board computing and the high cost of the data links, current architectural solutions cannot fully leverage all the available resources limiting their accuracy.
Our goal is to provide a distributed algorithm for failure prediction that could be executed both on-board of the aircraft and on a ground station and that would produce on-board failure predictions in near real-time under a communication budget.
In this approach, the ground station would hold fast computation resources and historical data and the aircraft would hold limited computational resources and current flight's data.
In this thesis, we will study the specificities of aeronautical data and what methods already exist to produce failure prediction from them and propose a solution to the problem stated.
Our contribution will be detailed in three main parts.
First, we will study the problem of rare event prediction created by the high reliability of aeronautical systems.
Many learning methods for classifiers rely on balanced datasets.
Several approaches exist to correct a dataset imbalance and we will study their efficiency on extremely imbalanced datasets.
Second, we study the problem of log parsing as many aeronautical systems do not produce easy to classify labels or numerical values but log messages in full text.
We will study existing methods based on a statistical approach and on Deep Learning to convert full text log messages into a form usable as an input by learning algorithms for classifiers.
We will then propose our own method based on Natural Language Processing and show how it outperforms the other approaches on a public benchmark.
Last, we offer a solution to the stated problem by proposing a new distributed learning algorithm that relies on two existing learning paradigms Active Learning and Federated Learning.
We detail our algorithm, its implementation and provide a comparison of its performance with existing methods
Neuroblastoma is the most frequent solid extra-cranial cancer of childhood.
This cancer displays a high heterogeneity both at clinical and molecular levels.
In order to answer this question, identification of populations coexisting at diagnosis and/or relapse in the patients which have relapsed is a prerequisite.
This would allow, between other things, to study the pathways differently altered in clones that are specific to each time point.
On these data, our method identified differences in the functional mutation rate, i.e. the number of putative functional variants by total number of variants, between the ancestral clones, clones expanding at relapse, and clones shrinking at relapse.
The objective of this thesis is to formalize a methodology aiming to validate the architecture of system of assistive tools installation making for piloting of electric wheelchair.
We want to give again a sufficient beginning of mobility to the onboard person, by using experiments of navigations in virtual environments.
So we have to conceive a system able to find the relevant improvements to bring to the electric wheelchair.
We worked out an architecture based on cycles "Experiment?Analyze and decision-making? Modification of the wheelchair ".
This architecture contains three modules: a simulator, an evaluator, a configurator.
This work is a part of the VAHM (developed in LASC) project using commercial electric wheelchairs enhanced by several sensors and a calculator.
Our simulator replaces the physical part of VAHM.
From the analysis of the data resulting from the experiments, we have calculated criteria representative of behaviours or/and particular situations which have occurred during navigation.
We indexed as robotics engineers, the whole of the problematic situations according to several aspects.
We have identified and characterized each aspect by a whole of criteria.
The vector of criteria constitutes the input of a decision-making system which will indicate at exit the functionalities to be installed to improve the mobility of the patient in powered wheelchair.
The base of knowledge of the decision-making system is supported by reflections, concerning the behaviours and particular situations that a handicapped person will meet during navigation.
The originality of our work comes from the handicap directed application allowing to evaluate and to highlight the difficulties of a group of people with severe motor disabilities, not able to move in real worlds, by using settings in virtual environments situations.
The architecture of the system let to provide solutions of assistance using an evolutionary decision-making system based on fuzzy logic.
Nowadays, online social media has transformed the way we create, share and access information.
These platforms rely on gigantic networks that promote the free exchange of information between hundreds of millions of people around the world, and this instantly.
Whether related to a global event or in connection with a local event, these messages may influence a society and may contain information useful for the detection or prediction of real-world phenomena.
However, some broadcast messages can have a very negative impact in real life.
These messages containing false information can have disastrous consequences.
To avoid and anticipate these dramatic situations, follow rumors, avoid bad reputations, it is necessary to study and then model the propagation of information.
However, most of the diffusion models introduced are based on axiomatic hypotheses represented by mathematical models.
As a result, these models are far removed from the users'dissemination behaviors in that they do not incorporate observations made on concrete dissemination cases.
In our work, we study the phenomenon of diffusion of information at two scales.
On a microscopic scale, we observed diffusion behaviors based on the personality traits of users by analyzing the messages they post in terms of feelings and emotions.
On a macroscopic scale, we analyzed the evolution of the diffusion phenomenon by taking into account the geographical dimension of the users.
Sharing information between users constitutes the cornerstone of the Web 2.0.
This raises concerns as this data can be exploited by several entities to breach user privacy.
Our first contribution consists in demonstrating the privacy threats behind releasing personal information publicly.
Two attacks are constructed to show that a malicious attacker (i.e., any external attacker with access to the public profile) can breach user privacy and even threaten his online security.
Our first attack shows how seemingly harmless interests (e.g., music interests) can leak privacy-sensitive information about users.
In particular, we infer their undisclosed (private) attributes using the public attributes of other users sharing similar interests.
Leveraging semantic knowledge from Wikipedia and a statistical learning method, we demonstrated through experiments---based on more than 104K Facebook profiles---that our inference technique efficiently predicts attributes that are very often hidden by users.
Our second attack is at the intersection of computer security and privacy.
In fact, we show the disastrous consequence of privacy breach on security by exploiting user personal information---gathered from his public profile---to improve the password cracking process.
First, we propose a Markov chain password cracker and show through extensive experiments that it outperforms all probabilistic password crackers we compared against.
We propose a practical, yet formally proved, method to estimate the uniqueness of each profile by studying the amount of information carried by public profile attributes.
To achieve our goal, we leverage Ads Audience Estimation platform and an unbiased sample of more than 400K Facebook public profiles.
Our measurement results show that the combination of gender, current city and age can identify close to 55% of users to within a group of 20 and uniquely identify around 18% of them.
In the second part of this thesis, we investigate the privacy threats resulting from the interactions between the OSN platform and external entities.
Our findings indicate that OSN tracking is diffused among almost all website categories which allows OSNs to reconstruct a significant portion of users'web profile and browsing history.
Finally, we develop a measurement platform to study the interaction between OSN applications---of Facebook and RenRen---and fourth parties.
We show that several third party applications are leaking user information to ``fourth''party entities such as trackers and advertisers.
This behavior affects both Facebook and RenRen with varying severity.
Machine Learning is widely utilized and tested in several fields.
Such approaches show obvious potential for application in design and manufacturing.
Prospective needs of such methods imply decision-making for manufacturing technology, optimization of design and manufacturing parameters as well as dynamic control of the manufacturing process.
However, several challenging issues remain open in order to apply these techniques in those specific fields.
Indeed, Machine Learning and other popular approaches (deep learning, convolutional neural networks) do require an extensive number of data to train the models.
In design and manufacturing, efficient data collection is often impossible, especially in the early design steps where both part and product do not exist.
Furthermore, even if the data collection is feasible, such models are usually trained for specific technologies and geometries.
Hence, the generalization capability is limited for such type of models.
More specifically, how to develop Machine Learning methods approaching draw near the human capabilities by using small number of cases in the design and manufacturing field?
Answering these two questions is fundamental to apply Machine Learning in design and manufacturing.
Solving these two research issues would lead to machine learning models embedding generalization capability for the design and manufacturing fields.
Prospective and innovant applications would emerge naturally and include decision-taking process, dynamic optimization of process parameters and dynamic control.
During this PhD thesis, several potential solutions will be explored.
All of them considering meta-learning as a basis, with the aim to approach human performances in learning.
One pillar of the research strategy will be the development of specialized metrics, models and optimization methods.
Domain-based metrics able to grasp the knowledge in design and manufacturing will be utilized and can use the DACM method, a metamodeling approach developed by Tampere University.
This method creates causal graphs as graphical representations of cause-effects relationships between the variables of the problem.
DACM has been previously developed in a joint PhD thesis between Grenoble Alpes University and Tampere University.
Potential utilization is envisioned through Bayesian Networks which can be considered as the following step after a causal graph generation.
They allow to integrate and discover new correlations and provide a statistical representation of the causal model.
Signal processing, recommender systems and natural language processing make extensive use of these techniques.
They already repeatedly proved their performances in the Netflix annual competition.
Grenoble Alpes university and Tampere University have conjointly developed and submitted a proof of concept using this approach and submitted in journal.
During the joint PhD thesis, the goal is to explore these different approaches for applications in decision-making process, optimal selection of design and process parameters and dynamic control in manufacturing process.
An ambitious publication plan in journals and conferences in the fields of design and manufacturing as well in informatics is planned.
The joint PhD thesis is in form of a compilation of journal articles (four minimum).
Modelling spatial relationships is critical in a variety of applications of VREs, such as human learning environments, virtual museums, or navigation-aids systems.
However, spatial relationships have been considered as abstract information and thus, difficult to specify.
Addressing this issue, this thesis proposes an approach to model spatial relationships among virtual objects in VREs.
First, we formalise a formal model of spatial relationships dedicated to VREs.
Second, we provide a language and a framework to specify spatial relationships at a conceptual level.
We claim that the proposed language is a relevant basis to specify spatial constraints related to activities of agents and users within VREs.
This thesis is focused on Graph Matching (GM) problems and in particular the Graph Edit Distance (GED) problems.
However, these problems are known to be complex and hard to solve, as the GED is a NP-hard problem.
Operations Research (OR) field offers a wide range of exact and heuristic algorithms that have accomplished very good results when solving optimization problems.
So, basically all the contributions presented in thesis are methods inspired from OR field.
The proposed heuristic approaches are adapted versions of existing MILP-based heuristics (also known as matheuristics), by considering problem-dependent information to improve their performances and accuracy.
One of the difficulties of an unresourced language is the lack of technology services in the speech and text processing.
In this thesis, we faced the problematic of an acoustical study of the isolated and continous speech in Fongbe as part of the speech recognition.
Tonal complexity of the oral and the recent agreement of writing the Fongbe led us to study the Fongbe throughout the chain of an automatic speech recognition.
In addition to the collected linguistic resources (vocabularies, large text and speech corpus, pronunciation dictionaries) for building the algorithms, we proposed a complete recipe of algorithms (including algorithms of classification and recognition of isolated phonemes and segmentation of continuous speech into syllable), based on an acoustic study of the different sounds, for Fongbe automatic processing.
In this manuscript, we also presented a methodology for developing acoustic models and language models to facilitate speech recognition in Fongbe.
In this study, it was proposed and evaluated an acoustic modeling based on grapheme (since the Fongbe don't have phonetic dictionary) and also the impact of tonal pronunciation on the performance of a Fongbe ASR system.
Finally, the written and oral resources collected for Fongbe and experimental results obtained for each aspect of an ASR chain in Fongbe validate the potential of the methods and algorithms that we proposed.
With the enormous amount of electronically available information on the internet, the development of information extraction technology becomes and more important.
It is the linguist's task to create terminological databases and thesauri for this application.
The research presented in this thesis is situated in the domain of Natural Language Processing (NLP), in which the lexicon plays a central role.
Although this is an appropriate approach it is to some extent insufficient because it does not take into account the semantic of words and their ambiguity.
For this, one needs to build dictionaries that relate syntax and semantics...
User profiling is essential for personalization systems (e.g. personalized information retrieval systems, recommendation systems) to identify user information (preference, interests...), in order to propose relevant content based on his/her specific needs and requirements.
Many works have shown that user's social neighbors can be a meaningful source to infer his/her interests.
The user's profile built within this approach is called "social profile".
As user behaviors evolve over time, it is necessary to take into consideration the evolution of user interests in user profiling process.
In the case of social profile, user interests are extracted from the information shared by his/her social neighbors.
Hence, the evolution of extracted interests is related to the evolution of information shared on user social network and to the evolution of relationships between the user and his/her social neighbors.
To handle this, we propose to apply a time-aware method into existing social profile building process (individual based and community based approaches).
This strategy aims at weighting user's interests in the social profile based on their temporal score.
The temporal score of an interest is computed by combining the temporal score of information used to extract the interests (computed by considering their freshness) with the temporal of individuals who share the information in the network (computed by considering the freshness of the interaction with the user).
The technique and temporal function used to compute the temporal score are customizable.
Thus, we can find out the most appropriate technique or temporal function depending on the types or characteristics of the adopted social network.
The experiments conducted on DBLP and Twitter showed that the so-called time-aware social profiling process applying our proposed time-aware method outperforms the existing time-agnostic social profiling process.
We also found that the most appropriate technique, temporal function and social profiling approach vary depending on the network characteristics (size, density) and to the social network type.
Our approach opens many opportunities for future studies in social information filtering and many application domains as well as on the Web (e.g. evolution of social profile in personalization of search engines, recommender systems in e-commerce,).
This thesis focuses on the automatic construction of linguistic tools and resources for analyzing texts of low-resource languages.
We propose an approach using Recurrent Neural Networks (RNN) and requiring only a parallel or multi-parallel corpus between a well-resourced language and one or more low-resource languages.
This parallel or multi-parallel corpus is used to construct a multilingual representation of words of the source and target languages.
We used this multilingual representation to train our neural models and we investigated both uni and bidirectional RNN models.
We also proposed a method to include external information (for instance, low-level information from Part-Of-Speech tags) in the RNN to train higher level taggers (for instance, SuperSenses taggers and Syntactic dependency parsers).
We demonstrated the validity and genericity of our approach on several languages and we conducted experiments on various NLP tasks: Part-Of-Speech tagging, SuperSenses tagging and Dependency parsing.
Our approach has the following characteristics and advantages: (a) it does not use word alignment information, (b) it does not assume any knowledge about target languages (one requirement is that the two languages (source and target) are not too syntactically divergent), which makes it applicable to a wide range of low-resource languages, (c) it provides authentic multilingual taggers (one tagger for N languages).
The aim of my PhD thesis is the study of nominal designations of events for automatic extraction.
My work is part of natural language processing, or in a multidisciplinary approach that involves Linguistics and Computer Science.
The aim of information extraction is to analyze natural language documents and extract information relevant to a particular application.
In this general goal, many information extraction campaigns were conducted: for each event considered, the task of the campaign is to extract some information (participants, dates, numbers, etc..).
All these information are set around the event and the work does not care about the words used to describe the event (especially when it comes to a name).
The event is seen as an all-encompassing as the quantity and quality of information that compose it.
Unlike work in general information retrieval, our main interest is focused only on the way are named events that occur particularly in the nominal designation used.
For us, this is the event that happens that is worth talking about.
The most important events are the subject of newspaper articles or appear in the history books.
An event can be evoked by a verbal or nominal description.
In this thesis, we reflected on the notion of event.
We observed and compared the different aspects presented in the state of the art to construct a definition of the event and a typology of events generally agree that in the context of our work and designations nominal events.
We also released our studies of different types of training corpus of the names of events, we show that each can be ambiguous in various ways.
For these studies, the composition of an annotated corpus is an essential step, so we have the opportunity to develop an annotation guide dedicated to nominal designations events.
We studied the importance and quality of existing lexicons for application in our extraction task automatically.
We also focused on the context of appearance of names to determine the eventness, for this purpose, we used extraction rules.
Following these studies, we extracted an eventive relative weighted lexicon (whose peculiarity is to be dedicated to the extraction of nominal events), which reflects the fact that some names are more likely than others to represent events.
Used as a tip for the extraction of event names, this weight can extract names that are not present in the lexicons existing standards.
Finally, using machine learning, we worked on learning contextual features based in part on the syntax to extract event names.
In order to facilitate the engineers task of evaluating the fire safety level, and to allow the specialists involved in the field to use their preferred languages and tools, we propose to create a language dedicated to the field of fire safety, which automatically generates a simulation, taking into account the specific languages used by the specialists involved in the field.
This DSL requires the definition, the formalization, the composition and the integration of several models, regardig to the specific languages used by the specialists involved in the field.
The specific language dedicated to the field of fire safety is designed by composing and integrating several other DSLs described by technical and natural languages (as well as natural languages referring to technical ones).
These latter are modeled in a way that their components must be precise and based on mathematical foundations, in order to verify the consistency of the system (people and materials are safe) before it implementation.
In this context, we propose to adopt a formal approach, based on algebraic specifications, to formalize the languages used by the specialists involved in the generation system, focusing on both syntaxes and semantics of the dedicated languages.
In the algebraic approach, the concepts of the domain are abstracted by data types and the relationships between them.
The semantics of specific languages is described by the relationships, the mappings between the defined data types and their properties.
The simulation language is based on a composition of several specific DSLs previously described and formalized.
The DSLs are implemented based on the concepts of functional programming and the Haskell functional language, well adapted to this approach.
The result of this work is a software dedicated to the automatic generation of a simulation, in order to facilitate the evaluation of the fire safety level to the engineers.
This thesis presents a model for automated left-right grammar checking based on analysis of a corpus of typescript errors.
Studies in cognitive psychology have shown that the revision process works by confronting expectations with results.
For humans, detecting a grammatical error therefore relies on an unfulfilled expectation on the part of the revisor.
The model presented here is based on this principle.
In order to deal with expectations from the point of view of computational processing, two common concepts in NLP are called upon: the unification principle and chunk segmentation.
The former is particularly adapted to checking agreements, while the latter provides an intermediate computational unit to delimit, and therefore simplify, detection of grammatical inconsistencies.
Finally, the model's originality lies in the left-right analysis it provides, which is constructed as the text is produced/read.
This thesis is situated in the context of logic-based Information Retrieval (IR) models.
However, a study of current logic-based IR models shows that these models generally have some shortcomings.
First, logic-based IR models normally propose complex, and hard to obtain, representations for documents and queries.
Second, the retrieval decision d-&gt;q, which represents the matching between a document d and a query q, could be difficult to verify or check.
Finally, the uncertainty measure U(d-&gt;q) is either ad-hoc or hard to implement.
In this thesis, we propose a new logic-based IR model to overcome most of the previous limits.
We represent documents and queries as logical sentences written in Disjunctive Normal Form.
We also argue that the retrieval decision d-&gt;q could be replaced by the validity of material implication.
We then exploit the potential relation between PL and lattice theory to check if d-&gt;q is valid or not.
We first propose an intermediate representation of logical sentences, where they become nodes in a lattice having a partial order relation that is equivalent to the validity of material implication.
Accordingly, we transform the checking of the validity of d-&gt;q, which is a computationally intensive task, to a series of simple set-inclusion checking.
In order to measure the uncertainty of the retrieval decision U(d-&gt;q), we use the degree of inclusion function Z that is capable of quantifying partial order relations defined on lattices.
Finally, our model is capable of working efficiently on any logical sentence without any restrictions, and is applicable to large-scale data.
Our model also has some theoretical conclusions, including, formalizing and showing the adequacy of van Rijsbergen assumption about estimating the logical uncertainty U(d-&gt;q) through the conditional probability P(q|d), redefining the two notions Exhaustivity and Specificity, and the possibility of reproducing most classical IR models as instances of our model.
We build three operational instances of our model.
An instance to study the importance of Exhaustivity and Specificity, and two others to show the inadequacy of the term-independence assumption.
Our experimental results show worthy gain in performance when integrating Exhaustivity and Specificity into one concrete IR model.
The work presented in this thesis can be developed either by doing more experiments, especially about using relations, or by more in-depth theoretical study, especially about the properties of the Z function.
This study on segmental reduction (i.e. deletion or temporal reduction) in spontaneous French allows us to propose two research methods for linguistic studies on large corpora, to investigate different factors of variation and to bring new insights on the propensity of segmental reduction.
We applied the descendant method using forced alignment with variants when it concerns a specific reduction phenomena.
Otherwise, we used the ascendant method using absent and short segments as indicators.
Three reduction phenomena are studied: schwa elision, /ʁ/ deletion and the propensity of segmental reduction.
The descendant method was used for analyzing schwa elision and /ʁ/ deletion.
Common factors used for the two studies are post-lexical context, speech style, sex and profession.
Schwas elision at initial syllable position in polysyllabic words and post-consonantal /ʁ/ deletion at word final position are not always conditioned by the same variation factors.
Similarly, lexical schwa and epenthetic schwa are not under the influence of the same variation factors.
The study on the propensity of segmental reduction allows us to apply the ascendant method and to investigate segmental reduction in general.
Results suggest that liquids and glides resist less the reduction procedure than other consonants and nasal vowels resist better reduction procedure than oral vowels.
Among oral vowels, high rounded vowels tend to be reduced more often than other oral vowels.
Automatic identification of multiword expressions (MWEs) is a pre-requisite for many natural language processing applications.
This task is challenging because MWEs, especially verbal ones (VMWEs) like to kick the bucket (which means to die), exhibit surface variability (no buckets were kicked).
We address here a subproblem of VMWE identification, namely the identification of occurrences of VMWEs previously seen in corpora, whatever their surface form, which requires to take ambiguity into account to avoidliteral (he kicked the old bucket) or coincidental occurrences (he kicked the ball and the bucket fell down).
To this end, we considered two main approaches: The first one is based on a language independent measure of VMWE variability.
The second one consists in modeling the problem as a classification task on the basis of features relevant to the VMWE morphosyntactic variability, which led to a system (VarIDE) that participated in the PARSEME shared task on automatic identification of VMWEs in 2018.
This thesis is set within the context of Accordys, a knowledge engineering project aiming at providing a case-based reasoning system for fetopathology, i.e. the medical domain studying rare diseases and dysmorphia of fetuses.
The project is based on a corpus of french fetal exam reports.
This material consists in raw text reports diplaying a very specific vocabulary (only partially formalized in french medical terminologies), a "note taking" style that makes difficult to use tools analysing the grammar in the text, and a layout and formatting that shows a latent common structuration (organisation in sections, sub-sections, observations).
Mapping a case with the model (instanciating the case model) is done through a Monte Carlo tree matching method.
We compare this with similarity measurements obtained by representing our reports (both without further processing and after semantic enrichment through a semantic annotator) in a vector model.
This thesis tackles the problem of the automatic recognition of similes in literary texts written in English or in French and proposes a framework to describe them from a stylistic perspective.
For the purpose of this study, a simile has been defined as a syntactic structure that draws a parallel between at least two entities, lacks compositionality and is able to create an image in the receiver's mind.
Three main points differentiate the proposed approach from existing ones: it is strongly influenced by cognitive and linguistic theories on similes and comparisons, it takes into consideration a wide range of markers and it can adapt to diverse syntactic scenarios.
Concretely speaking, it relies on three interconnected modules: - a syntactic module, which extracts potential simile candidates and identifies their components using grammatical roles and a set of handcrafted rules, - a semantic module which separates creative similes from both idiomatic similes and literal comparisons based on the salience of the ground and semantic similarity computed from data automatically retrieved from machine-readable dictionaries;-and an annotation module which makes use of the XML format and gives among others information on the type of comparisons (idiomatic, perceptual…) and on the semantic categories used.
Finally, the two annotation tasks we designed show that the automatic detection of figuration in similes must take into consideration.
The aim of this thesis is to carry out a syntactic analysis of time adverbials in Korean which correspond to a time, a date or a duration (e. G. Ohu du si-ei (at two o'clock in the afternoon), 5uel 6il (on May 6th), and 3sigan dongan (for 3 hours)).
For formal linguistics as applied in natural language processing, the most exhaustive and explicit description possible is essential.
The 'lexicon-grammar'methodology (M. Gross 1975, 1986b) has provided us with a model for the formal and systematic description of natural language.
We have described the lexical combinations concerned using graphs of finite-state automata, which constitute 'local grammars'representing the various types of adverbial sequences in as much detail as possible.
Our graphs can be integrated directly into an automatic parser for the purpose of locating the Korean adverbs of time, date and duration in large corpora of texts.
We devote chapter 3 to the analysis of forms which can be interpreted as duration, and chapter 4 to the analysis of forms which can be interpreted as date or time.
We investigate how the temporal sequences studied in the second chapter can enter into sentences which allow them to be interpreted as expressing duration or date
A guided wave-based structural health monitoring (SHM) system aims at determining the integrity of a wide variety of plate-like structures, including aircraft fuselages, pipes, tanks etc.
It relies on a sparse array of piezoelectric transducers for guided waves (GWs) excitation and sensing.
This thesis presents studies conducted with the purpose of developing such a GWs-based SHM system that is capable of efficient defect detection, localization and sizing aeronautical plate-like structures made of aluminum and composite materials.
This work also provides a comprehensive overview of DAS, MV and Excitelet defect imaging algorithms, determines their performance using statistical analysis of an extensive dataset of simulated guided waves imaging (GWI) results and proposes a method for sparse defect imaging.
While defect detection and localization are straightforward from the image analysis, the defect sizing is a more complex problem due to its high dimensionality and non-linearity.
It is demonstrated that this problem can be solved by means of machine learning methods, relying on an extensive database of simulated GWI results.
They are efficient under stationary operational conditions but vulnerable to environmental variations, especially to the temperature fluctuation.
Finally, this work presents studies on the robustness of GWI methods against thermal effects, and a defect detection model capable of analyzing deteriorated GWI results is proposed.
Different techniques for thermal effects compensation are reviewed, and improvements are proposed.
Their effectiveness is validated for aluminum plates but further improvements are required to translate these techniques to composite plates.
This thesis investigates the joint modeling of visual and textual content of multimedia documents to address cross-modal problems.
Such tasks require the ability to match information across modalities.
A common representation space, obtained by eg Kernel Canonical Correlation Analysis, on which images and text can be both represented and directly compared is a generally adopted solution.
Nevertheless, such a joint space still suffers from several deficiencies that may hinder the performance of cross-modal tasks.
An important contribution of this thesis is therefore to identify two major limitations of such a space.
The first limitation concerns information that is poorly represented on the common space yet very significant for a retrieval task.
The second limitation consists in a separation between modalities on the common space, which leads to coarse cross-modal matching.
To deal with the first limitation concerning poorly-represented data, we put forward a model which first identifies such information and then finds ways to combine it with data that is relatively well-represented on the joint space.
Evaluations on emph{text illustration} tasks show that by appropriately identifying and taking such information into account, the results of cross-modal retrieval can be strongly improved.
The major work in this thesis aims to cope with the separation between modalities on the joint space to enhance the performance of cross-modal tasks.
We propose two representation methods for bi-modal or uni-modal documents that aggregate information from both the visual and textual modalities projected on the joint space.
Evaluations show that our approaches achieve state-of-the-art results on several standard and challenging datasets for cross-modal retrieval or bi-modal and cross-modal classification.
The aim of this thesis is part of the broad issue of information retrieval in Electronic Health Records (EHRs).
The aspects tackled in this topic are numerous: on the one hand clinomics information retrieval within EHRs and secondly information retrieval within unstructured data from EHRs.
As a first step, one of the objectives is to integrate in EHRs information beyond the scope of medicine to integrate data, information and knowledge from molecular biology ; omic data from genomics, proteomics or metabolomics.
The integration of this type of data improves health information systems, their interoperability and the processing and exploitation of data for clinical purposes.
An important challenge is to ensure the integration of heterogeneous data, through research on conceptual models of data, ontology and terminology servers, and semantic data warehouses.
The integration of this data and their interpretation into a conceptual data model is an important challenge.
Finally, it is important to integrate clinical research and fundamental research in order to ensure continuity of knowledge between research and clinical practice and to understand personalized medicine challenges.
This thesis thus leads to the design and development of a generic model of omics data exploited in a prototype application for information retrieval and visualization in omic and clinical data within a sample of 2,000 patients.
The second objective of this thesis is the multi-terminological indexing of medical documents through the development of the Extracting Concepts with Multiple Terminologies tool (ECMT).
It uses terminologies embedded in the Health Terminology/Ontology Portal (HeTOP) to identify concepts in unstructured documents.
From a document written by a human, and therefore potentially showing typing errors, spelling or grammar mistakes, the challenge is to identify concepts and thus structure the information contained in the text.
In health information retrieval, indexing is of great interest for information retrieval in unstructured documents, such as reports and medical notes.
It consists of autonomous mobile nodes that communicate over bandwidth-constrained wireless links.
Nodes in a MANET are free to move randomly and organize themselves arbitrarily.
They can join/quit the network in an unpredictable way; such rapid and untimely disconnections may cause network partitioning.
In such cases, the network faces multiple difficulties.
Data replication is a possible solution to increase data availability.
However, implementing replication in MANET is not a trivial task due to two major issues: the resource-constrained environment and the dynamicity of the environment makes making replication decisions a very tough problem.
In this thesis, we propose a fully decentralized replication model for MANETs.
This model is called CReaM: “Community-Centric and Resource-Aware Replication Model”.
When the consumption of one resource exceeds a predefined threshold, replication is initiated with the goal of balancing the load caused by requests over other nodes.
The data item to replicate is selected depending on the type of resource that triggered the replication process.
The best data item to replicate in case of high CPU consumption is the one that can better alleviate the load of the node, i.e. a highly requested data item.
Oppositely, in case of low battery, rare data items are to be replicated (a data item is considered as rare when it is tagged as a hot topic (a topic with a large community of interested users) but has not been disseminated yet to other nodes).
To this end, we introduce a data item classification based on multiple criteria e.g., data rarity, level of demand, semantics of the content.
To select the replica holder, we propose a lightweight solution to collect information about the interests of participating users.
Users interested in the same topic form a so-called “community of interest”.
Through a tags analysis, a data item is assigned to one or more communities of interest.
Based on this framework of analysis of the social usage of the data, replicas are placed close to the centers of the communities of interest, i.e. on the nodes with the highest connectivity with the members of the community.
The results of evaluating CReaM show that CReaM has positive effects on its main objectives.
This dissertation examines whether the different categorization processes shaping audiences' valuations in markets bring stability or variability to audiences' valuations.
While seminal research on categorization emphasized the stabilizing role of market categories, recent research suggests that audiences' valuations can vary substantially even in markets which are well-structured by pre-existing categories.
This variability notably results from audiences' heterogeneous preferences for typical offerings, from shifts in categories' meanings or from audiences' reliance on multiple models of valuation.
Taking stock of these new results, this dissertation asks why audiences' valuations are so variable and explores in more details the role that market categories play in this phenomenon.
This dissertation proposes that i) ambiguous categories, ii) the influence of temporary attractions among audiences alongside more stable categories and iii) the co-existence of different types of evaluators all contribute to produce variability in audiences' valuations.
The first two empirical essays use data from publicly listed firms in the U.S.
In these essays, firms' similarity to existing category prototypes or audiences' temporary attractions toward certain features are measured using semantics extracted from large corpora of annual reports and IPO prospectuses.
The third essay is a theoretical model.
This dissertation contributes to the literature on market categories, to the burgeoning research on optimal distinctiveness and to computational approaches to the study of organizations.
Two types of genre criteria are determined and used to define articles: genre-oriented morphosyntactic descriptors, and textual components and thematics.
The partition is methodological, as the two levels constantly interfere.
After a description of the main corpus, a contrastive analysis enables us to highlight extrinsic variation principles: personal styles, genre and domain variations and language impact.
This quantitative and qualitative study finally provides a linguistic description of the different sides of a complex and multidimensionnel object, as well as a methodology to observe and characterize genres.
The difficult nature of Una meditación has been highlighted by both scholars and Juan Benet himself.
This dissertation characterizes such a text complexity and thereby the singularity of the reading experience of Benet's novel.
Our work relies on the psycholinguistics of reading comprehension. This framework allows us to achieve a definition of standard readability to which Una meditación is implicitly compared when judged as “difficult”.
We study the two features that revealed to be the main sources of reading difficulty in Benet's text: the narrative and sentence structures, and the particular system of reference to the characters.
At the level of the text structure, the narration and—at its own scale—the sentence are characterized by a strong discontinuity, however concealed; by a spiral temporal development; and by the scrambling of the hierarchy of the fictional events.
At the level of character reference, the notion of name of the character loses its traditional meaning. Names are barely used or these are ambiguous, multiple, or inexistent.
However it is above all the omnipresence of the pronominal reference that disconcerts the reader, implicitly imposing memorizing every detail of the text.
We also analyze the figure of the narrator, and criticize a common reading of Benet's novel in which the text is the produce of a recollection.
We conclude that the “difficulty” of Una meditación is the result of a writing that, by means of the indiscernibility of the characters and their stories, goes beyond literary fiction and aims at a generic portrait of human nature.
In summer 2013, the term "Big Data" appeared and attracted a lot of interest from companies.
This thesis examines the contribution of these methods to actuarial science.
It addresses both theoretical and practical issues on high-potential themes such as textit{Optical Character Recognition} (OCR), text analysis, data anonymization and model interpretability.
Starting with the application of machine learning methods in the calculation of economic capital, we then try to better illustrate the boundary that may exist between automatic learning and statistics.
Highlighting certain advantages and different techniques, we then study the application of deep neural networks in the optical analysis of documents and text, once extracted.
The use of complex methods and the implementation of the General Data Protection Regulation (GDPR) in 2018 led us to study its potential impacts on pricing models.
By applying anonymization methods to pure premium calculation models in non-life insurance, we explored different generalization approaches based on unsupervised learning.
Finally, as regulations also impose criteria in terms of model explanation, we conclude with a general study of methods that now allow a better understanding of complex methods such as neural networks
The goal of the thesis is to evaluate new forms of Human-Computer Interaction.
Thus, behaviours and reactions of users are collected using ergonomic methods and eye-tracking technologies.
An experimental approach was adopted in order to evaluate the contribution of each part.
For that, participants (student of bachelor's degree) handled a software of animation's creation which was unknown for them (Flash) in order to carry out three scenarios.
Throughout their exploration of the software, participants were accompanied by a help system.
In the first experiment an ECA (provided by FT R &amp; D) enunciate help messages; in the second one a adaptive system (detection of intention and evolution according to knowledge) was used.
The various studies carried out show that the two innovations employed were perceived positively by the majority of the participants.
They showed in addition that a ECA has a reassuring effect and that it can probably be used in first experiences with a software.
For the adaptive system, the fact that the system evolves in an autonomous way did not disturb the participants, but hardly improves the performances.
In this thesis multi-agent argumentation debates are studied.
Our work is motivated by the issues which are raised when a large number of users interact and debate on the Web, by exchanging arguments on various topics.
These issues are raised on the levels of representing the debating users'knowledge, representing the debate, computing the debate's conclusions, evaluating the debate's quality, defining specific protocols for user interaction, and studying debate strategies which users employ in order to achieve particular goals.
This thesis'contribution consists in:
a) proposing a way to model a multi-agent argumentation debate where the participants have different types of expertise, and proposing a way to aggregate their opinions;
b) offering support to the agent who is arbitrating a debate, proposing a way to evaluate the quality of a debate on the basis of how confident we can be on its conclusions, and proposing solutions for improving the quality of a debate which lacks definite conclusions;
c) offering support to the debating agents in order to determine which arguments they should put forward, studying dynamic argumentation systems, studying the potential ways in which an agent can influence a dynamic argumentation system in order to achieve his goal, studying the minimal change allowing an agent to achieve his goal, studying several argumentation strategies based on minimal change;
d) defining, studying and evaluating multi-agent argumentation protocols, defining protocols of different types (1) based on numerical argument evaluation and (2) based on argument extensions, using different techniques to ensure a debate's coherence while ensuring some liberty of expression to the agents, and finally performing an important number of experiments (on debates) in order to test various strategies and evaluate them with respect to specific criteria.
Multichannel sound recording and processing is a research field at Orange Labs.
An ongoing thesis work concerns multichannel filtering with machine learning which shows the added value of neural networks to estimate a solid spatial enhancement filter.
This present PhD thesis aims at using machine learning to provide needed information for the filtering: phrase beginning and end, and sound sources position over time.
The idea is to couple ambisonics data and deep neural networks.
The thesis will thus focus on 3 aspects of source localisation: estimation of the number of sources, estimation of the arrival directions and sources tracking over time.
We present the creation of two resources for Hungarian NLP applications: a rule-based shallow parser and a database of verbal subcategorization frames.
Hungarian, as a non-configurational language with a rich morphology, presents specific challenges for NLP at the level of morphological and syntactic processing.
While efficient and precise morphological analyzers are already available, Hungarian is under-resourced with respect to syntactic analysis.
Our work aimed at overcoming this problem by providing resources for syntactic processing.
Hungarian language is characterized by a rich morphology and a non-configurational encoding of grammatical functions.
These features imply that the syntactic processing of Hungarian has to rely on morphological features rather than on constituent order.
More concretely, we attempt to adapt current results in argument realization and lexical semantics to the task of labeling sentence constituents according to their syntactic function and semantic role in Hungarian.
Within the syntax-semantics interface, the field of argument realization deals with the (partial or complete) prediction of verbal subcategorization from semantic properties.
We claim that contrary to the widely shared presumption, adjuncts are often not fully productive.
We therefore propose a gradual notion of productivity, defined in relation to Levin-type lexical semantic verb classes (Levin, 1993; Levin and Rappaport-Hovav, 2005).
This research investigates the worlds of contemporary French knowledge production in order to understand the different meanings of the term "open" in sciences.
Specific attention has been drawn to the qualifying adjective open in relation to the french translations (ouvert, libre gratuit) as well as associated terms (science, data, access) with this formula.
This inquiry began in 2013 and focused mainly on a specific event, the consultation on the bill for a "Digital Republic" (September-October 2015), in particular Article 9 on "open access to scientific publications in public research".
This online consultation has allowed for a national and public scope to the issue of access to knowledges.
As an "equipped" reality test via a participative website, arose the opportunity to observe almost "live" the defense of different conceptions of "what should be" the contemporary regime of knowledges in France.
Through a grounded theory approach around this particular crystallisation moment of the debates on open in sciences has led me to gradually constitute a corpus of documents, reflecting the deployment of the exchanges on different digital spaces/apparatus (consultation website, scientific blogs, academic notebooks, mainstream press, etc.).
Within an iterative research process, I combined digital methods (digital mapping of the similarity of votes) and qualitative analysis of the corpus, as well as the theoretical concepts mobilized at the crossroads between information and communication sciences and pragmatic sociology of critique.
Subsequently, by switching from modeling to transposable theorization into other fields of research, I show how the distinction between two logics (technoindustrial or processual), behind the discourses on open can be relevant to analyze the current reconfigurations of other societal arrangements.
The consultation by itself illustrates this point with the coexistence of two "digital" conceptions of democracy (extended representative or contributive), embodied in the design of the consultative platform.
In the last part, I propose to explain the dynamics between the reconfiguration of a spirit and its social arrangement, by considering the permanent coupling between cognition, technologically mediated actions and socio-technical environment.
Finally, the PhD experience narrated throughout this inquiry is also an example of an enaction process on my own conceptions of open.
In this sense, it opens further reflections on the situated and incarnated nature of any production of knowledges, which escapes neither the limits nor the potentialities of metacognition.
The study uses and informational practices of users in information retrieval process is a major focus of research in Information and Communication Sciences, judging by the work on this issue and on the Internet.
If the North has a strong tradition in this area, to the south by cons, specifically in Francophone countries of Africa in south of the Sahara, few studies have been devoted to this theme.
This study falls within this thematic and geographical context and aims as public target doctoral students from the University of Bamako.
This will be first to define a typology of this information users community (specialization, doctoral schools and laboratories attachment, thematic research, etc.) and then evaluate its information needs and practices finally, to identify the existence of possible access to information problems and propose possible solutions.
For data collection a survey of doctoral students and observing their behavior information search situation will be favored.
The results of the study will enable a better understanding of community needs and the informational practices of doctoral students from the University of Bamako and could be used by the units of information to improve the supply of information services them.
This study focuses on discursive representations of Japanese animation.
In France, Japanese cartoons, known as anime, are now embedded in the cultural habits of many generations, but faced controversial beginnings when they first reached the country in the mid-1970.
In Japan, the country where it was born, anime has long been a part of Japanese popular culture.
Through this study, we aim to find out how anime, which are depicted as well established both in Japan and France, are perceived by young people in these two countries.
We also seek to know if the culture of those two countries can exert influence on how it is described by them.
The study was carried out in the form of a survey conducted among French and Japanese students, through a questionnaire in their respective languages.
The discursive and semantic analysis of the surveyed participants' answers has enabled us to extract their representations about anime, but also about French and Japanese cultures.
This was followed by a comparative analysis of our results which allowed us to confront French students' perspective with that of their Japanese counterparts.
The building of syntactically informative Arabic linguistic resources is a major issue for the development of new machine processing tools.
We propose in this thesis to create an Arabic treebank that integrates a new type of information, which is based on the Property Grammar formalism.
A syntactic property is a relation between two units of a given syntactic structure.
We have thus been able to construct, using this grammar, other Arabic linguistic resources.
In the learning model, we integrated a probabilistic lexicalized property grammar that may positively affect the parsing result and describe its syntactic structures with its properties.
Finally, we evaluated the parsing results of this approach by comparing them to those of the Stanford Parser.
This thesis work focuses on a class of unsupervised, probabilistic deep learning methods that use variational inference to create high capacity, scalable models for time series modelling and analysis.
We present two classes of variational deep learning, then apply them to two specific problems related to the maritime domain.
The first application is the identification of dynamical systems from noisy and partially observed data.
We introduce a framework that merges classical data assimilation and modern deep learning to retrieve the differential equations that control the dynamics of the system.
Using a state space formulation, the proposed framework embeds stochastic components to account for stochastic variabilities, model errors and reconstruction uncertainties.
The second application is maritime traffic surveillance using AIS data.
We propose a multitask probabilistic deep learning architecture can achieve state-of-the-art performance in different maritime traffic surveillance related tasks, such as trajectory reconstruction, vessel type identification and anomaly detection, while reducing significantly the amount data to be stored and the calculation time.
For the most important task—anomaly detection, we introduce a geospatial detector that uses variational deep learning to builds a probabilistic representation of AIS trajectories, then detect anomalies by judging how likely this trajectory is.
The Chronicles of Narnia (1950-1956) is a well-known collection of seven novels, usually seen as belonging to the genre of Children's literature and Fantasy.
One of the main characteristics of the novels lies in their symbolic dimension, which evokes the Christian tradition and is expressed in the text through a second layer of meaning.
Our thesis involves the analysis of a corpus including the English originals of The Chronicles of Narnia and their respective French translations, entitled Le Monde de Narnia (2005).
The main markers which make the object of discussion are: deictics, modality, transitivity, lexical choice and semantic prosody.
The discourse features related to these markers are analysed with respect to the narrative instance of the narrator, which has a key role in conveying the ideology of the text and which controls the focalization process.
Our analysis draws particular attention to the sacred dimension in the texts, as well as to the themes of violence, death and gender in children's literature.
Children's literature is usually characterised by an educational goal, and the Narnia books prove to be a powerful means to convey values within society, at a given moment in time.
Our research reveals that the French translations tend to weaken the religious message of the original texts, distancing the reader or blurring space boundaries.
Moreover, the ideology in the target texts is characterised by a number of discrepancies by comparison with the source texts; different values are given prominence, among those already present in the Narnia books.
Using a method of analysis of translated texts, the thesis brings a contribution to the understanding of the challenges a translator may face when confronted with the task of translating ideology and point of view in books for children.
In Mexico, one of the priority technological problems is the preservation of cultural heritage in its digital form.
In this research, the main interest is the ordering, management and identification of intangible cultural heritage in images.
In computer vision, the integration of the Human Visual System (HVS) into automatic learning methods and classifiers has become an intensive research field for object recognition and content mining.
The so-called saliency maps, are defined as a topographic representation of visual attention on a scene, modeling attention instantaneously and assigning a degree of interest to each pixel value on the image.
Saliency maps proved to be very efficient to point out regions of interest in several tasks of visual content and its understanding.
In this context, we focus on the integration of visual attention models in the training pipeline of Deep Neural Networks (DNNs) for the recognition of Mexican architectural structures.
We consider the main contributions of this research are in the following areas of interest:
• Specific purpose dataset: gathering data related to the topic is a key task to solve the problem of architectural classification.
• Data selection: we use saliency prediction methods to select and crop context-relevant regions on images.
• Visual attention modeling: we annotate images through a real task of image observation, we record eye-fixations with an eye-tracker system to build subjective saliency maps.
• Visual attention integration: we integrate visual attention in deep neural networks in two ways; i) to filter out features in a saliency-based pooling layer and ii) in attention mechanisms.
In this research, different essential components for the training of a neural network are tackled down with the aim of recognizing Mexican cultural content and extrapolating these findings to large-scale databases in similar classification tasks, such as in ImageNet.
Finally, we show that the integration of visual attention models generated through a psycho-visual experiment allows to reduce training time and improve performances in terms of accuracy.
Pharmacovigilance is a fundamental discipline for safety and confidence in the medicines.
This discipline has evolved over time, and has been strengthened, but still suffers from imperfections.
We proposed through this work to provide an original solution for its improvement.
In the first part, we describe and analyze the evolution of pharmacovigilance and its current functioning from both the legal and the scientific point of view and at both national and European levels.
We analyzed the legal and practical weaknesses and have formulated proposals to address them.
Then we review a number of possibilities before developing in the second part an original approach: pedagogy.
Having established that the field pharmacovigilance is based on health professionals, we studied the provision of training of these professionals and offering to provide more qualitative and enough quantitative academic training in pharmacovigilance and iatrogenic, keeping in mind that we must rely on teaching methods and tools that should be adapted to current students.
The proposed pedagogical paradigm is based on a pedagogy of explanation, on research in pharmaceutical law and a hybrid innovative pedagogy, combining face to face courses and e-learning resources.
The tools used include lectures supplemented by formative assessments as tests done with electronic voting boxes and clinical cases in e-learning available on a platform.
This education should lead students to a better understanding of pharmacovigilance and practical management of iatrogenic drug events.
Consequently, the work done, allows us to better train, health professionals for drug risk management with an ultimate goal of strengthening the pharmacovigilance system.
This will be complemented in the future by simulation and role-playing.
Nowadays, huge amounts of data relative to predictive maintenance are generated by massive industrial systems instrumentation, and are under-exploited.
Therefore, developing algorithms that can continuously analyze these complex and heterogeneous data in order to predict faults and relative maintenance operations, is a major issue.
Current machine learning approaches, and particularly deep learning-based ones - that yielded impressive results in computer vision and natural language processing fields - offer thus strong potential, and some first approaches have already been tested on specific maintenance use cases.
This thesis aims at suggesting new data analysis approaches, learning a lot from data richness and available knowledge on monitorig and fault diagnosis.
This thesis presents a human-robot interaction (HRI) framework to classify large vocabularies of static and dynamic hand gestures, captured with wearable sensors.
Static and dynamic gestures are classified separately thanks to the segmentation process.
Experimental tests on the UC2017 hand gesture dataset showed high accuracy.
Online classification of dynamic gestures allows successful predictive classification.
The proposed network achieved a high accuracy on the rejection of untrained patterns of the UC2018 DualMyo dataset.
This construction is done by describing some of the reasonings taking place in the knowledge acquisition process and particularly the ones that allow to resolve the "associative anaphora".
We define a knowledge representation model, having a linguistics basis and cognitif elements.
In order to support this model, we propose an object oriented formalism, whose theoretical foundations are lesniewski's logical system: ontology and mereology.
The first system relies upon a primitif functor called "epsilon" meaning "is-a", the second one upon the "part-of" relation called "ingredience".
These logical systems constitute a more appropriate theoretical foundation than the traditional predicate calculus.
The aim of this thesis is to examine the morphosyntactic and semantic properties of abstract nouns related to verbal and adjectival predicates.
From the assumption that the stative feature common to these nouns allows a unified analysis, we propose a study relying on the idea that stative nouns are distinguished by their uses, and show that, in addition to a purely stative meaning, these nouns can also convey other information, in which they denote occurrences.
The second part is dedicated to the syntactic behaviour of stative nouns, i.e. number and determination, but also adjectival modification.
This enables us to identify two distinct morphosyntactic behaviours, that parallel the distinction between stative and occurrence understanding highlighted in the first part.
On the one hand, in their property sense, these nouns have a behaviour similar to that of massive concrete nouns and qualify as relational nouns, i.e. they require an argument with which they enter into a predication relationship (at the syntactic level).
On the other hand, in their occurrence sense, these nouns behave like concrete count nouns and are not inherently relational.
To sum up, the analysis of stative nouns shows that they share semantic properties with certain types of verbal and adjectival predicates, as well as syntactic properties with various classes of concrete nouns.
This thesis explores properties of estimations procedures related to aggregation in the problem of high-dimensional regression in a sparse setting.
It benefits from strong results in fixed and random designs with a PAC-Bayesian approach.
Chapter 2 analyses the statistical behaviour of the prediction loss of the EWA with Laplace prior in the fixed design setting.
Chapter 4 explores the statisctical behaviour of adjusted versions of the Lasso for the transductive and semi-supervised learning task in the random design setting.
Nowadays, the need for very up to date authoritative spatial data has significantly increased.
Thus, to fulfill this need, a continuous update of authoritative spatial datasets is a necessity.
This task has become highly demanding in both its technical and financial aspects.
In terms of road network, there are three types of roads in particular which are particularly challenging for continuous update: footpath, tractor and bicycle road.
They are challenging due to their intermittent nature (e.g. they appear and disappear very often) and various landscapes (e.g. forest, high mountains, seashore, etc.).Simultaneously, GPS data voluntarily collected by the crowd is widely available in a large quantity.
The number of people recording GPS data, such as GPS traces, has been steadily increasing, especially during sport and spare time activities.
The traces are made openly available and popularized on social networks, blogs, sport and touristic associations'websites.
However, their current use is limited to very basic metric analysis like total time of a trace, average speed, average elevation, etc.
Particular attention is paid on roads that exist in reality but are not represented in authoritative datasets (missing roads).
The approach we propose consists of three phases.
The first phase consists of evaluation and improvement of VGI traces quality.
The quality of traces was improved by filtering outlying points (machine learning based approach) and points that are a result of secondary human behaviour (activities out of main itinerary).
Remained points are then evaluated in terms of their accuracy by classifying into low or high accurate (accuracy) points using rule based machine learning classification.
The second phase deals with detection of potential updates.
For that purpose, a growing buffer data matching solution is proposed.
The size of buffers is adapted to the results of GPS point's accuracy classification in order to handle the huge variations in VGI traces accuracy.
As a result, parts of traces unmatched to authoritative road network are obtained and considered as candidates for missing roads.
Finally, in the third phase we propose a decision method where the “missing road” candidates should be accepted as updates or not.
This decision method was made in multi-criteria process where potential missing roads are qualified according to their degree of confidence.
Missing roads in IGN authoritative database BDTopo® were successfully detected and proposed as potential updates
This PhD thesis is part of the research project PERDIDO, which aims at extracting and retrieving displacements from textual documents.
The objective of this PhD is to propose a method for establishing a processing chain to support the geoparsing and geocoding of text documents describing events strongly linked with space.
We propose an approach for the automatic geocoding of itineraries described in natural language.
Our proposal is divided into two main tasks.
The first task aims at identifying and extracting information describing the itinerary in texts such as spatial named entities and expressions of displacement or perception.
The second task deal with the reconstruction of the itinerary.
Our proposal combines local information extracted using natural language processing and physical features extracted from external geographical sources such as gazetteers or datasets providing digital elevation models.
The geoparsing part is a Natural Language Processing approach which combines the use of part of speech and syntactico-semantic combined patterns (cascade of transducers) for the annotation of spatial named entities and expressions of displacement or perception.
The main contribution in the first task of our approach is the toponym disambiguation which represents an important issue in Geographical Information Retrieval (GIR).
We propose an unsupervised geocoding algorithm that takes profit of clustering techniques to provide a solution for disambiguating the toponyms found in gazetteers, and at the same time estimating the spatial footprint of those other fine-grain toponyms not found in gazetteers.
We propose a generic graph-based model for the automatic reconstruction of itineraries from texts, where each vertex represents a location and each edge represents a path between locations.
To build automatically this graph-based representation of the itinerary, our approach computes an informed spanning tree on a weighted graph.
Each edge of the initial graph is weighted using a multi-criteria analysis approach combining qualitative and quantitative criteria.
Criteria are based on information extracted from the text and information extracted from geographical sources.
For instance, we compare information given in the text such as spatial relations describing orientation (e.g., going south) with the geographical coordinates of locations found in gazetteers.
Finally, according to the definition of an itinerary and the information used in natural language to describe itineraries, we propose a markup langugage for encoding spatial and motion information based on the Text Encoding and Interchange guidelines (TEI) which defines a standard for the representation of texts in digital form.
Additionally, the rationale of the proposed approach has been verified with a set of experiments on a corpus of multilingual hiking descriptions (French, Spanish and Italian).
The management of knowledge and innovation are the themes to strong importance today, especially because these two topics are linked and they influence the performance of firms.
The objective of this research is to analyze the link between knowledge management and innovation from three industries of simple product in the industrial pole of Barcarena city, Pará State, Brazil.
We want to evaluate both the relationship of Knowledge Management in these companies and their innovative capacity, to understand the influence of knowledge for the innovative capacity, especially incremental innovation.
For this, we will use models of analysis which take into account factors such as culture, leadership, technology, human resources and process.
Our methodological approach is qualitative.
We choose as basic concepts and theoretical literature about the Knowledge Management and Innovation.
The axis of the industry was chosen by its economic importance in the region where the research has been developed.
In addition, according to the results of the companies that better manage the knowledge have more possibiliter to innovate.
Digitalized music production exploded in the past decade.
Huge amount of data drives the development of effective and efficient methods for automatic music analysis and retrieval.
This thesis focuses on performing semantic analysis of music, in particular mood and genre classification, with low level and mid level features since the mood and genre are among the most natural semantic concepts expressed by music perceivable by audiences.
In order to delve semantics from low level features, feature modeling techniques like K-means and GMM based BoW and Gaussian super vector have to be applied.
In this big data era, the time and accuracy efficiency becomes a main issue in the low level feature modeling.
Our first contribution thus focuses on accelerating k-means, GMM and UBM-MAP frameworks, involving the acceleration on single machine and on cluster of workstations.
To achieve the maximum speed on single machine, we show that dictionary learning procedures can elegantly be rewritten in matrix format that can be accelerated efficiently by high performance parallel computational infrastructures like multi-core CPU, GPU.
In particular with GPU support and careful tuning, we have achieved two magnitudes speed up compared with single thread implementation.
Regarding data set which cannot fit into the memory of individual computer, we show that the k-means and GMM training procedures can be divided into map-reduce pattern which can be executed on Hadoop and Spark cluster.
Our matrix format version executes 5 to 10 times faster on Hadoop and Spark clusters than the state-of-the-art libraries.
Beside signal level features, mid-level features like harmony of music, the most natural semantic given by the composer, are also important since it contains higher level of abstraction of meaning beyond physical oscillation.
Our second contribution thus focuses on recovering note information from music signal with musical knowledge.
This contribution relies on two levels of musical knowledge: instrument note sound and note co-occurrence/transition statistics.
In the instrument note sound level, a note dictionary is firstly built i from Logic Pro 9.
With the musical dictionary in hand, we propose a positive constraint matching pursuit (PCMP) algorithm to perform the decomposition.
The objective of this thesis is to evaluate the interest of a new control interface for powered wheelchairs, a force feedback joystick, intended for people with severe motor disabilities which have difficulties to pilot their wheelchair in an usual way.
This joystick will have to be implemented on a "smart" wheelchair provided with telemetric sensors.
The force feedback is calculated according to the proximity of the obstacles and assists the user, without forcing him, to move towards the free direction.
The first chapter of the report is a state of the art on the smart wheelchairs, on the control modes in teleoperation, on the haptic interfaces in robotics and on the modelling of piloting tasks.
The second chapter describes the design of a simulator of wheelchair piloting intended to test new functionalities.
The third and final chapter relates to a set of experimental results aiming at concluding on the interest of the force feedback for wheelchair piloting and on the choice of its calculation algorithm.
The parameters tested are in particular the configuration of the environment (corridor, doors passages, free space …) and the kinematics of the wheelchair (front-wheel drive, rear-wheel drive)
How are English, Irish, Scottish, Welsh Kale and American Travellers represented through a few old gadji myths which die hard.
In the last few years, deep learning has changed irrevocably the field of computer vision.
Faster, giving better results, and requiring a lower degree of expertise to use than traditional computer vision methods, deep learning has become ubiquitous in every imaging application.
Thus this thesis first focused on the topic of hyper-parameter optimization for deep neural networks, i.e. methods for automatically finding efficient neural networks on specific tasks.
The thesis includes a comparison of different methods, a performance improvement of one of these methods, Bayesian optimization, and the proposal of a new method of hyper-parameter optimization by combining two existing methods: Bayesian optimization and Hyperband.
From there, we used these methods for medical imaging applications such as the classification of field-of-view in MRI, and the segmentation of the kidney in 3D ultrasound images across two populations of patients.
This last task required the development of a new transfer learning method based on the modification of the source network by adding new geometric and intensity transformation layers.
Finally this thesis loops back to older computer vision methods, and we propose a new segmentation algorithm combining template deformation and deep learning.
The method is validated on the task of kidney segmentation in 3D US images.
In various domains, graphs represent a useful representation for many types of data.
Prominent examples entail behavioural analyses performed in cybersecurity or social network analysis.
In the former, internet user behaviour can be observed by monitoring DNS requests, interpreted as successive steps of a random walker on a graph in which nodes represent domain names and edges represent population-level average behaviour.
Therefore studying user behaviour can be done by analyzing the subgraph induced by specific user movements.
In the latter, graph representations naturally emerge from user interactions.
For example nodes can represent users, and any relation between two users (messages or common interests) can be interpreted as edges.
Understanding and analyzing graph structures appear to be a key tool in many real-world applications.
It is thus essential to find efficient and robust methods for tasks such as node or graph classification.
Coincidentally with the fact that graph neural network approaches have shown superior results in various domains, their robustness has been investigated and attacks have been developed in the context of node classification and graph classification (Zügner, Akbarnejad, and Günnemann 2018; H. Dai et al. 2018; L. Sun et al. 2018).
With respect to privacy, a number of works have started to investigate how techniques for computing over encrypted data such as homomorphic cryptography (FHE) can be applied to the inference phase of deep neural network models with encouraging results when a clear-domain network is evaluated over an encrypted-domain input (Bourse et al. 2018; Chabanne et al. 2019; Chabanne et al. 2017; Dowlin et al. 2016).
The aim of this research is to develop a generalised descriptive repertory of french direct interrogative structures.
The domain is centered around computational linguistics and information retrieval.
In the linguistics section, transformation and types of transformation used for processing interrogatives are formally defined.
The notational features which are valid throughout the research are then presented, followed by the individual transformations for the ten french interrogative elements (qui, quoi, que, pourquoi, combien, ou, quand, comment, quel, lequel) and inverted forms (est-il venu? que fait pierre? qui est-ce qui est venu?...).
The implementation as a repertory aims at incorporating the descriptive linguistic constraints.
Risif - our automatic french interrogative structures repertory, is programmed in fx, and is designed to code and diagnose interro- gative phrases.
A data-base query system (cbd) with a linguistics component is the final issue.
Cbd is used independently or in direct relation with risif.
One can view and compare structures extracted from the data with those previously analysed (via risif).
Data base modification and creation within the fx lisp environment is also possible.
However, the construction of such resources is costly.
Therefore, they are only available for a limited number of languages and domains.
Moreover, the choice of semantic formalization can differ according to the needs (SQL queries, logical formulas,...).
In this context, it is necessary to develop methods that are less dependent on this supervision data, for example by exploiting linguistic resources.
In the framework of this thesis, we are interested in developing joint approaches for semantic analysis and controlled generation of texts based on 'auto-encoder'neural architectures: a sentence is encoded into a latent representation (by semantic analysis) and then regenerated from this latent representation (controlled generation).
We will focus on weakly supervised approaches using knowledge of the language, of the domain and of the targeted formalism rather than on the accumulation of annotated data.
Machine translation aims at automatically translating documents from one language to another without human intervention.
With the advent of deep neural networks (DNN), neural approaches to machine translation started to dominate the field, reaching state-ofthe-art performance in many languages.
Combined with the architectural flexibility of DNNs, this framework paved the way for further research in multimodality with the objective of augmenting the latent representations with other modalities such as vision or speech, for example.
This thesis focuses on a multimodal machine translation (MMT) framework that integrates a secondary visual modality to achieve better and visually grounded language understanding.
I specifically worked with a dataset containing images and their translated descriptions, where visual context can be useful forword sense disambiguation, missing word imputation, or gender marking when translating from a language with gender-neutral nouns to one with grammatical gender system as is the case with English to French.
I propose two main approaches to integrate the visual modality: (i) a multimodal attention mechanism that learns to take into account both sentence and convolutional visual representations, (ii) a method that uses global visual feature vectors to prime the sentence encoders and the decoders.
Through automatic and human evaluation conducted on multiple language pairs, the proposed approaches were demonstrated to be beneficial.
Finally, I further show that by systematically removing certain linguistic information from the input sentences, the true strength of both methods emerges as they successfully impute missing nouns, colors and can even translate when parts of the source sentences are completely removed.
This thesis is dedicated to the problem of training and integration strategies of several modalities (visual, textual), in order to perform an efficient Visual Concept Detection and Annotation (VCDA) task, which has become a very popular and important research topic in recent years because of its wide range of application such as image/video indexing and retrieval, security access control, video monitoring, etc.
The human brain is made of a large number of interconnected neural networks which are composed of neurons and synapses.
With a low power consumption of only few Watts, the human brain is able to perform computational tasks that are out of reach for today's computers, which are based on the Von Neumann architecture.
Neuromorphic hardware design, taking inspiration from the human brain, aims to implement the next generation, non-Von Neumann computing systems.
In this thesis, emerging non-volatile memory devices, specifically Phase-Change Memory (PCM) and Oxide-based resistive memory (OxRAM) devices, are studied as artificial synapses in neuromorphic systems.
The use of PCM devices as binary probabilistic synapses is studied for complex visual pattern extraction applications, evaluating the impact of the PCM programming conditions on the system-level power consumption.
A programming strategy is proposed to mitigate the impact of PCM resistance drift.
It is shown that, using scaled devices, it is possible to reduce the synaptic power consumption.
The OxRAM resistance variability is evaluated experimentally through electrical characterization, gathering statistics on both single memory cells and at array level.
A model that allows to reproduce OxRAM variability from low to high resistance state is developed.
An OxRAM-based convolutional neural network architecture is then proposed on the basis of this experimental work.
This thesis focuses on the automatic recognition of human stress during stress-inducing interactions (public speaking, job interview and serious games), using audio and visual cues.
Part of this work is dedicated to the study of information fusion form those various modalities.
Stress expression and coping are influenced both by interpersonal differences (personality traits, past experiences, cultural background) and contextual differences (type of stressor, situation's stakes).
Inter-individual and inter-corpora comparisons highlight the variability of stress expression.
A possible application of this work could be the elaboration of therapeutic software to learn stress coping strategies, particularly for social phobics.
The GenderedNews project aims to propose new methods for measuring and explaining the level of gender bias in the news media in France.
These can be defined as the fact that the news media tend on the one hand to overweight men compared to women in terms of mentions and quotes, and on the other hand to attribute to women a specific social role often involving, among other things, anonymity, a reduced capacity for action in society and the confusion between this action and their marital or family status.
Numerous empirical studies have proven the existence of these biases and have made it possible to better understand them on an international scale.
However, research on this issue is often based on data limited in volume and produced by NGOs, administrations or media regulatory bodies.
They generally give rise to manual content analyses that do not allow systematic account of changes in sexist biases in the media over the long term and on a large number of sources, nor explain these biases in terms of variables such as media funding, size of newsrooms or other organizational variables.
The GenderedNews project aims to provide and analyze large and stable data sources over time as well as explore new methods for documenting gender bias in the media.
It is based on a collaborative research scheme between a media sociologist and a computer scientist with skills in media studies, gender studies, natural language processing and digital data collection.
It also has an important partnership dimension, with leading media being associated and providing access to data.
GenderedNews focuses on two kinds of biases and two different measures of those biases.
Sampling biases occur through the selection of a biased sample of people mentioned in the media.
They can be studied by merely counting how many men and women access to public visibility on one side and by studying the framing patterns of the pictured men and women on the other side.
Sourcing biases occur through the selection of a biased sample of people who, in addition to be visible, are allowed to express their views in the media.
They can also be studied using the two approaches: counting how many on one side and analysing how on the other.
Within the project, the PhD student will contribute more specifically to the study of sourcing biases.
The management of uncertainty is also an integral part of decision-making processes in the medical field.
In the case of a medical incident during an air travel, this uncertainty includes three additional sources: (1) variability of the aeronautical conditions, (2) individual variability of the patient's conditions, (3) individual variability of the intervener's skills.
Presently, medical incidents in the plane are estimated worldwide at 350 per day and when they occur, they are handled in 95 \% of cases by health professionals who are passengers.
It is often for them a first experience.
Apart from telemedicine with remote assistance, the intervener, often alone in the face of his doubts and uncertainty, has no other decision aid tool on board.
Civil aviation also has feedback systems to manage the complexity of such processes.
Event collection and analysis policies are put in place internationally, for example ECCAIRS (European Co-ordination Center for Accident and Incident Reporting Systems) and ASRS (Aviation Safety Reporting System).
Finally, we propose a Clinical Decision Support System (CDSS) architecture that integrates the management of the uncertainties present on both the collected data and the skill levels of the medical professionals involved.
To achieve this goal, an information retrieval system (IRS) must represent, store and organize information, then provide to the user the elements corresponding to the need for information expressed by his query.
Most of information retrieval systems (IRS) use simple terms to index and retrieve documents.
However, this representation is not precise enough to represent the contents of documents and queries, because of the ambiguity of terms isolated from their context.
A solution to this problem is to use multi-word terms to replace simple term.
This approach is based on the assumption that a multi-word term is less ambiguous than a simple term.
Our thesis is part of the information retrieval in Arabic specific domain.
The objective of our work was on the one hand, identifying a multi-word terms present in queries and documents.
On the other hand, exploiting the richness of language by combining several linguistic knowledge belonging at the morphological and syntax level, and showing how the contribution of syntactic and morphological knowledge helps to improve access to information.
In addition, we have defined linguistically a multi-word term in Arabic and we developed a system of identification of multi-word terms which is based on a mixed approach combining statistical model and linguistic data
Several phenomenas cause source code duplication like inter-project copying and adaptation or cloning inside a same project.
Looking for code matches allows to factorize them inside a project or to highlight plagiarism cases.
We study statical similarity retrieval methods on source code that may be transformed via edit operations like insertion, deletion, transposition, in- or out-lining of functions.
Sequence similarity retrieval methods inspired from genomics are studied and adapted to find common chunks of tokenized source.
After an explanation on alignment and n-grams lookup techniques, we present a factorization method that merge function call graphs of projects to a single graph with the creation of synthetic functions modeling nested matches.
It relies on the use of suffix indexation structures to find repeated token factors.
Syntax tree indexation is explored to handle huge code bases allowing to lookup similar sub-trees with their hash values computed via heterogeneous abstraction profiles.
Before and after match retrieval, we define similarity metrics to preselect interesting code spots, refine the search process or enhance the human understanding of results
The aim of this project is to explore and exploit document classification and natural language processing techniques through an implementation of a new approach based on multi-agent system that would answer a need for websites analysis and classification tasks at Olfeo.
Time and space representation is an important task in many domains such as natural language processing, geographic information systems (GIS), computer vision, robot navigation.
Many qualitative approaches have been proposed to represent the spatial or temporal entities and their relations.
The majority of these formalisms use qualitative constraints networks (QCNs) to represent information about a system.
In some application, e. g. multi-agent systems, spatial or temporal information come from different sources, i. e. each source provides a spatial or temporal QCN representing relative positions between objects.
The multiplicity of sources providing spatial or temporal information makes that the underlying QCNs are generally conflicting.
Merging multiple sources information has attracted much attention in the framework of propositional logic.
We take an inspiration from these works in order to define some merging process specified to QCNs, and study their logic and computational properties.
This thesis focuses on the linguistic expression of feelings and emotions in a corpus until now little questioned in this type of studies, in this case the forms of instantaneous communication made possible by a new technology and that seems predisposed to many expressive markers.
It moves the cursor studies on the linguistic expression of these emotional categories of the system to employment, interviewing a corpus formed by four forms of communication: blogs, discussion forums, Facebook and Twitter.
The work anchors reflection cognitively seeking to show, in a dynamic perspective, how to build this type of discourse in mediated interaction.
It studies the different linguistic and extra-linguistic manifestations that load these electronic writings an emotional dimension opening on a large interactive dimension.
It allows reflection on the written / oral borders and the construction of a new expressive language specific to electronic writings.
With the growth in Internet of Things, the realization of environments composed of diverse connected resources (devices, sensors, services, data, etc.) becomes a tangible reality.
Creating these applications however goes hand-in-hand with the design of tools supporting the nomadic users roaming in these spaces, in particular by enabling the efficient selection of resources.
While such a selection calls for the design of theoretically grounded descriptions, it should also consider the profile and preferences of the users.
Finally, the rise of (possibly mobile) connected resources calls for designing a scalable process underlying this selection.
Progress in the field is however sluggish especially because of the ignorance of the stakeholders (and the interactions between them) composing this eco-system of “IoT-enabled smart environments”.
Thus, the multiplicity of diverse connected resources entails interoperability and scalability problems.
While the Semantic Web helped in solving the interoperability issue, it however emphasizes the scalability one.
Revolving from our research works performed over the last 6 years, this dissertation identifies the interactions between the stakeholders of the nascent ecosystem to further propose formal representations.
The dissertation further designs a framework providing search capabilities to support the selection of connected resources through a semantic analysis.
In particular, the framework relies on a distributed architecture that we design in order to manage scalability issues.
This thesis is about the design of a complete processing chain dedicated to unconstrained handwriting recognition.
Three main difficulties are adressed: pre-processing, optical modeling and language modeling.
The pre-processing stage is related to extracting properly the text lines to be recognized from the document image.
An iterative text line segmentation method using oriented steerable filters was developed for this purpose.
The difficulty in the optical modeling stage lies in style diversity of the handwriting scripts.
Statistical optical models are traditionally used to tackle this problem such as Hidden Markov models (HMM-GMM) and more recently recurrent neural networks (BLSTM-CTC).
Using BLSTM we achieve state of the art performance on the RIMES (for French) and IAM (for English) datasets.
The language modeling stage implies the integration of a lexicon and a statistical language model to the recognition processing chain in order to constrain the recognition hypotheses to the most probable sequence of words (sentence) from the language point of view.
The difficulty at this stage is related to the finding the optimal vocabulary with minimum Out-Of-Vocabulary words rate (OOV).
Enhanced language modeling approaches has been introduced by using sub-lexical units made of syllables or multigrams.
The sub-lexical units cover an important portion of the OOV words.
Otherwise equivalent performances are obtained with a compact sub-lexical language model.
Thanks to the compact lexicon size of the sub-lexical units, a unified multilingual recognition system has been designed.
In recent years, Named Data Networking (NDN) has emerged as one of the most promising future networking architectures.
To be adopted at Internet scale, NDN needs to resolve the inherent issues of the current Internet.
Assuming that (i) a computer is located in the enterprise network that is based on an NDN architecture, (ii) the computer has already been compromised by suspicious media such as a malicious email, and (iii) the company installs a firewall connected to the NDN-based future Internet, this thesis focuses on a situation that the compromised computer (i.e., malware) attempts to send leaked data to the outside attacker.
The contributions of this thesis are fivefold.
Firstly, this thesis proposes an information leakage attack through a Data and through an Interest in NDN.
Secondly, in order to address the information leakage attack, this thesis proposes an NDN firewall which monitors and processes the NDN traffic coming from the consumers with the whitelist and blacklist.
Thirdly, this thesis proposes an NDN name filter to classify a name in the Interest as legitimate or not.
To take traffic flow to the NDN firewall from the consumer into account, fourthly, this thesis proposes an NDN flow monitored at an NDN firewall.
Fifthly, in order to deal with the drawbacks of the NDN name filter, this thesis proposes an NDN flow filter to classify a flow as legitimate or not.
The performance evaluation shows that the flow filter complements the name filter and greatly chokes the information leakage throughput
The current dissertation presents an ERP-investigation of metrical stress processing in French.
Indeed, while metrical stress is well known to play an invaluable role in speech comprehension, its functions in French speech processing are unclear.
French is a language traditionally described as having no accent.
This dissertation questions the traditional view and aligns to two metrical models on French accentuation, which propose stress to be encoded in cognitive templates underlying the abstract representation of the word.
In our interdisciplinary investigation of metrical stress processing in French, we take a functional, yet metrically rooted, approach.
We use the method of Event-Related Potentials (ERP), which provides us with a highly sensitive and temporally precise measure allowing us to determine whether there is metrical stress in French, and to what extent metrical stress aids the listener in speech comprehension.
Because of the rise of the social Web, both the research and the industry are interested in automatic processing of opinions in text.
In this work, we assume a multilingual and multidomain environment and aim at automatic and adaptive polarity classification.
We propose a method for automatic construction of multilingual affective lexicons from microblogging to cover the lack of lexical resources.
We propose a text representation model based on dependency parse trees to replace a traditional n-grams model.
In our model, we use dependency triples to form n-gram like features.
We believe this representation covers the loss of information when assuming independence of words in the bag-of-words approach.
Finally, we investigate the impact of entity-specific features on classification of minor opinions and propose normalization schemes for improving polarity classification.
The effectiveness of our approach has been proved in experimental evaluations that we have performed across multiple domains (movies, product reviews, news, blog posts) and multiple languages (English, French, Russian, Spanish, Chinese) including official participation in several international evaluation campaigns (SemEval'10, ROMIP'11, I2B2'11).
This dissertation studies the French adverb autrement, through its three main uses: adverb of manner, connective denoting negative hypothesis, and topic shifter.
The importance of anaphora resolution and discourse structure is stressed.
After a review of the literature on discourse structure and on the adverb, the characteristics of the three uses are defined thanks to spoken and written corpora, showing how context is instrumental in retrieving the antecedent and how the adverb relies on discourse and builds it at the same time.
Already in the adverb of manner, anaphor and right scope are crucial in the construction of meaning.
With the connective, referential relations leave room for logical relations holding from proposition to proposition, whereas the topic shifter is a metalinguistic use handling abstract discourse entities.
A core of meaning [anaphor and negation] is identified, common to the three uses and accounting for bridges between them.
This synchronic study is then used to reconstruct the adverb's grammaticalization, detailed observation in the present counterbalancing sparse historical data.
It is shown that the notion of a construction, i.e. the use of the adverb in some context, has made evolution possible: in particular, word order in Old French was crucial, allowing the adverb of manner to occupy the initial position where reanalysis could occur ; the use of conjunctions also favored the emergence of some of the adverb's meanings.
Augmented surgery uses medical apparatus (Augmented Surgery Devices or ASD) allowing the surgeon to improve their orientation, and thus enhance the surgical environment to facilitate carrying out their actions.
The development of these devices, their proliferation and the media exposure they receive, has led the government to question the quality associated with interventions assisted by these apparatus.
In this paper, we illustrate the issues of quality associated with ASD-assisted interventions through a chronological description of the first active medical robot used for fitting total hip replacements.
We then discuss the notion of quality in medicine in general and finally the quality of ASD in particular.
We will see that there are no specific provisions for these devices and it doesn't seem appropriate to speak of the quality of an ASD without taking into account the environment in which it is used.
This is why it is crucial to structure the use of these devices as well as the environment in which they are used.
One way to structure this environment is to use ontologies.
Using the ontology editing function of the ISIS software, we modeled surgery, as well as the associated environment, for ligament insufficiency of the anterior cruciate ligament with and without ASD.
This ontological representation consists of a set of 45 Ontological Diagrams (OD) having a total of 1072 concepts.
We describe the materials and methods used to build all of these diagrams.
To speak of the quality of ASD, a user can create their information system from our ontological model in order to have their own metrics.
The validation of our structural model was carried out by an expert through a scenario of surgery, created from the ontological model.
Finally we discuss the possible prospects for our work.
Determinantal point processes (DPPs) generate random configuration of points where the points tend to repel each other.
The notion of repulsion is encoded by the sub-determinants of a kernel matrix, in the sense of kernel methods in machine learning.
This special algebraic form makes DPPs attractive both in statistical and computational terms.
This thesis focuses on sampling from such processes, that is on developing simulation methods for DPPs.
Applications include numerical integration, recommender systems or the summarization of a large corpus of data.
In the finite setting, we establish the correspondence between sampling from a specific type of DPPs, called projection DPPs, and solving a randomized linear program.
In this light, we devise an efficient Markov-chain-based sampling method.
In the continuous case, some classical DPPs can be sampled by computing the eigenvalues of carefully randomized tridiagonal matrices.
We provide an elementary and unifying treatment of such models, from which we derive an approximate sampling method for more general models.
In higher dimension, we consider a special class of DPPs used for numerical integration.
We implement a tailored version of a known exact sampler, which allows us to compare the properties of Monte Carlo estimators in new regimes.
In the context of reproducible research, we develop an open-source Python toolbox, named DPPy, which implements the state of the art sampling methods for DPPs
In this thesis, we propose Triangular Similarity Metric Learning (TSML) for automatically specifying a metric from data.
A TSML system is loaded in a siamese architecture which consists of two identical sub-systems sharing the same set of parameters.
Each sub-system processes a single data sample and thus the whole system receives a pair of data as the input.
The TSML system includes a cost function parameterizing the pairwise relationship between data and a mapping function allowing the system to learn high-level features from the training data.
In terms of the cost function, we first propose the Triangular Similarity, a novel similarity metric which is equivalent to the well-known Cosine Similarity in measuring a data pair.
Based on a simplified version of the Triangular Similarity, we further develop the triangular loss function in order to perform metric learning, i.e. to increase the similarity between two vectors in the same class and to decrease the similarity between two vectors of different classes.
Compared with other distance or similarity metrics, the triangular loss and its gradient naturally offer us an intuitive and interesting geometrical interpretation of the metric learning objective.
In terms of the mapping function, we introduce three different options: a linear mapping realized by a simple transformation matrix, a nonlinear mapping realized by Multi-layer Perceptrons (MLP) and a deep nonlinear mapping realized by Convolutional Neural Networks (CNN).
With these mapping functions, we present three different TSML systems for various applications, namely, pairwise verification, object identification, dimensionality reduction and data visualization.
For each application, we carry out extensive experiments on popular benchmarks and datasets to demonstrate the effectiveness of the proposed systems.
The thesis presents a study of thirteen indefinite quantifiers in Mandarin Chinese noun phrases.
Chapter 2 provides an overview of some saillant characteristics of Chinese noun phrases.
In Chapter 3, the distributional properties of thirteen indefinite quantifiers.
In Chapter 4, the quantifier function of yīdiǎnr 'a little/un peu' is opposed to that of the minimizer yīdiǎnr 'the least/le moindre'.
This research formulates some thoughts that are essential to initiate students, non native speakers, to academic writing and help in mastering it.
Several questions have guided this study: What role might have the descriptive studies of scientific writing in a successful familiarization with academic writing?
What is the point of an introduction to rhetorical functions based on transdisciplinary phraseology and on a so-called genre approach?
Is it possible to submit support elements that would be beneficial to all students regardless of their disciplines?
An exploratory study of a particular rhetorical function that is "positioning" has allowed us to understand the extent to which linguistic elements, namely transdisciplinary collocations could help students to less apprehend the requirement of an essentially polyphonic and argumentative writing or to further show their positioning in their academic writings.
Crowdsourcing has proved its ability to address large scale data collection tasks at a low cost and in a short time.
However, due to the dependence on unknown workers, the quality of the crowdsourcing process is questionable and must be controlled.
Indeed, maintaining the efficiency of crowdsourcing requires the time and cost overhead related to this quality control to stay low.
Current quality control techniques suffer from high time and budget overheads and from their dependency on prior knowledge about individual workers.
In this thesis, we address these limitation by proposing the CAWS (Context-Aware Worker Selection) method which operates in two phases: in an offline phase, the correlations between the worker declarative profiles and the task types are learned.
Then, in an online phase, the learned profile models are used to select the most reliable online workers for the incoming tasks depending on their types.
Using declarative profiles helps eliminate any probing process, which reduces the time and the budget while maintaining the crowdsourcing quality.
In order to evaluate CAWS, we introduce an information-rich dataset called CrowdED (Crowdsourcing Evaluation Dataset).
The generation of CrowdED relies on a constrained sampling approach that allows to produce a dataset which respects the requester budget and type constraints.
Through its generality and richness, CrowdED helps also in plugging the benchmarking gap present in the crowdsourcing community.
Using CrowdED, we evaluate the performance of CAWS in terms of the quality, the time and the budget gain.
Results shows that automatic grouping is able to achieve a learning quality similar to job-based grouping, and that CAWS is able to outperform the state-of-the-art profile-based worker selection when it comes to quality, especially when strong budget ant time constraints exist.
Finally, we propose CREX (CReate Enrich eXtend) which provides the tools to select and sample input tasks and to automatically generate custom crowdsourcing campaign sites in order to extend and enrich CrowdED.
Exploring unexploited but newly digitized resources to find relevant information is a complicated task due to the amount of available resources.
Thanks to the ANR project CIRESFI, the most important resource for the Italian Comedy of the 18th century, is a set of accounting registers consisting of 28,000 pages.
Information retrieval is a long and complex process that requires expertise at every step: detection and segmentation in paragraphs, lines or words, features extraction, handwriting recognition.
Systems based on deep neural networks dominate these approaches.
The major issue is the need of a large amount of data to achieve their learning.
However, the registers of the Italian Comedy have no ground truth.
To overcome this lack of data, we explore approaches that involving transfer learning.
That means using heterogeneous labeled and available data, with at least one common feature with our data to drive the systems, and then applying them to our data.
All of our experiments have shown us the difficulty of carrying out this task, each choice at each stage having a strong impact on the rest of the system.
We converge on a solution separating the optical model from the language model in order to achieve independent learning with different available resources and joining together thanks to a projection of the information into a non-latent common space.
XML has become the de facto format for data exchange.
We aim at establishing a multi-system environment where some local original systems work in harmony with a global integrated system, which is a conservative evolution of local ones.
Data exchange is possible in both directions, allowing activities on both levels.
We propose a set of tools to help dealing with XML database evolution.
Experimental results are discussed, showing the efficiency of our methods in many situations.
This thesis is an evaluation of the possibilities of automatising a morphological analysis of the Russian words.
This analysis is submitted to two major constraints:
1) it can rely only on the knowledge of the word itself and not on the context from which it is extracted,
2) no stem is known a priori, with the exception of the roots forming part of: - homonym derivatives, - derivatives whose segmentation cannot be controlled by simple rules.
An important part of this work consists in defining factors leading to an appreciable reduction of this set of stems.
This analysis is done with the aid of: three sets of morphemes (prefixes, suffixes and endings) and the two forms of the reflexive pronoun, rules of recognition of foreign words, morphological incompatibilities.
The analysis produces grammatemes (sort of identification sheet of the word) of the analysed word, as reduced as possible.
The reduction of this grammateme is the result of the intersection of the sets of information bound to each one of the morphemic elements forming the word to be analysed.
This thesis addresses the challenge of designing urban mobility systems.
It aims at developing a traveler experience model to help diagnose travel problems in a design approach and improve the relevance of transportation models for travelers.
By combining the views of user-experience design and transportation, it helps to deepen the understanding of how travelers experience their journey and especially the problems they face.
The first axis of investigation is related to the modeling of the traveler experience to feed a relevant and rich diagnosis of travel problems.
In the second axis, travelers are involved, through a grounded theory approach, to identify the problems they encounter when using urban mobility systems, using appropriate stimuli.
The third axis introduces travel subjective attributes into transport models to improve their accuracy.
This research used action research as a methodology.
It combines literature review in design and transportation disciplines, four field observations, fifteen in-depth interviews with transport travelers and experts, five problem-solving workshops, and two experiments, in a cyclical improvement of results.
The various uses of the model have led to an in-depth diagnosis of three urban mobility systems (suburban train, on-demand bus, dedicated shuttle) and the development of traveler-centric attributes for an optimization model and a multi-agent simulation that was tested by a survey of over 450 participants.
Machine translation (MT) has increasingly become an indispensable tool for decoding the meaning of a text from a source language into a target language in our current information and knowledge era.
In particular, MT of proper names (PN) plays a crucial role in providing the specific and precise identification of persons, places, organizations, and artefacts through the languages.
Despite a large number of studies and significant achievements of named entity recognition in the NLP community around the world, there has been almost no research on PNMT for Vietnamese language.
Due to the different features of PN writing, transliteration or transcription and translation from a variety of languages including English, French, Russian, Chinese, etc. into Vietnamese, the PNMT from those languages into Vietnamese is still challenging and problematic issue.
This study focuses on the problems of English-Vietnamese and French-Vietnamese PNMT arising from current MT engines. First, it proposes a corpus-based PN classification, then a detailed PNMT error analysis to conclude with some pre-processing solutions in order to improve the MT quality.
Through the analysis and classification of PNMT errors from the two English-Vietnamese and French-Vietnamese parallel corpora of texts with PNs, we propose solutions concerning two major issues: (1)corpus annotation for preparing the pre-processing databases, and (2)design of the pre-processing program to be used on annotated corpora to reduce the PNMT errors and enhance the quality of MT systems, including Google, Vietgle, Bing and EVTran.
The efficacy of different annotation methods of English and French corpora of PNs and the results of PNMT errors before and after using the pre-processing program on the two annotated corpora are compared and discussed in this study.
They prove that the pre-processing solution reduces significantly PNMT errors and contributes to the improvement of the MT systems' for Vietnamese language.
The general goal of the work to be carried out is the delivery of flexible and robust methods for merging open-domain knowledge.
Organic electronics is a field of research dealing with the development of new technologies based on organic semiconductor materials (OSCs).
In general, two approaches are used for the molecular design of OSCs.
The first approach consists in assembling efficient molecular fragments, in order to synthesize functional materials for a specific application such as phosphorescent organic light-emitting diodes (PhOLEDs).
The second approach is more risky as it aims to develop new molecular fragments which may have one or several desired properties for a given application.
In this thesis work, both approaches have been developed.
On the one hand, we have developed host materials for PhOLEDs by adjusting their properties (first approach), and, on the other hand, we have been interested in a new generation of OSCs: molecular nanorings (second approach).
This work has enabled to reach the, green and blue PhOLEDs displaying the highest overall performances ever reported in literature.
This work allowed us to incorporate for the first time molecular nanorings in organic field-effect transistors in order to study their transport properties.
This thesis is part of a study that explores automatic transcription potential for the instrumentation of educational situations.
Our contribution covers several axes.
First, we describe the enrichment and the annotation of COCo dataset that we produced as part of the ANR PASTEL project.
In this multi-thematic framework, we are interested in the problem of the linguistic adaptation of automatic speech recognition systems (ASR).
The proposed language model adaptation is based both on the lecture presentation supports provided by the teacher and in-domain data collected automatically from the web.
Thus, we proposed two evaluation protocols.
The first one deals with an intrinsic evaluation, making it possible to estimate performance only for domain words of each lecture (IWER_Average).
The second protocol offers an extrinsic evaluation, which estimates the performance for two tasks exploiting transcription: information retrieval and indexability.
As LM adaptation is based on a collection of data from the web, we study the reproducibility of language model adaptation results by comparing the performances obtained over a long period of time.
Over a collection period of one year, we were able to show that, although the data on the Web changed in part from one month to the next, the performance of the adapted transcription systems remainedconstant (i.e. no significant performance changes), no matter the period considered.
Finally, we are intersted on thematic segmentation of ASR output and alignment of slides with oral lectures.
For thematic segmentation, the integration of slide's change information into the TextTiling algorithm provides a significant gain in terms of F-measure.
For alignment of slides with oral lectures, we have calculated a cosine similarity between the TF-IDF representation of the transcription segments andthe TF-IDF representation of text slides and we have imposed a constraint torespect the sequential order of the slides and transcription segments.
Existing methods for rhythmic analysis typically focus on one of those levels, failing to exploit music's rich structure and compromising the musical consistency of automatic estimations.
In this work, we propose novel approaches for leveraging multi-scale information for computational rhythm analysis.
Our models account for interrelated dependencies that musical audio naturally conveys, allowing the interplay between different time scales and accounting for music coherence across them.
Our methods are systematically evaluated on a diverse group of datasets, ranging from Western music to more culturally specific genres, and compared to state-of-the-art systems and simpler variations.
The overall results show that our models for downbeat tracking perform on par with the state of the art, while being more musically consistent.
Moreover, our model for the joint estimation of beats and microtiming takes further steps towards more interpretable systems.
The methods presented here offer novel and more holistic alternatives for computational rhythm analysis, towards a more comprehensive automatic analysis of music.
In the recent years, deep learning has become the leading approach to modern artificial intelligence (AI).
The important improvement in terms of processing time required for learning AI based models alongside with the growing amount of available data made of deep neural networks (DNN) the strongest solution to solve complex real-world problems.
The natural multidimensionality of the data is elegantly embedded within complex and hypercomplex neurons composing the model.
In particular, quaternion neural networks (QNN) have been proposed to deal with up to four dimensional features, based on the quaternion representation of rotations and orientations.
Unfortunately, and conversely to complex-valued neural networks that are nowadays known as a strong alternative to real-valued neural networks, QNNs suffer from numerous limitations that are carrefuly addressed in the different parts detailled in this thesis.
The thesis consists in three parts that gradually introduce the missing concepts of QNNs, to make them a strong alternative to real-valued NNs.
The first part introduces and list previous findings on quaternion numbers and quaternion neural networks to define the context and strong basics for building elaborated QNNs.
The second part introduces state-of-the-art quaternion neural networks for a fair comparison with real-valued neural architectures.
More precisely, QNNs were limited by their simple architectures that were mostly composed of a single and shallow hidden layer.
In this part, we propose to bridge the gap between quaternion and real-valued models by presenting different quaternion architectures.
First, basic paradigms such as autoencoders and deep fully-connected neural networks are introduced.
In a conventional QNN scenario, input features are manually segmented into three or four components, enabling further quaternion processing.
Unfortunately, there is no evidence that such manual segmentation is the representation that suits the most to solve the considered task.
Morevover, a manual segmentation drastically reduces the field of application of QNNs to four dimensional use-cases.
Therefore the third part introduces a supervised and an unsupervised model to extract meaningful and disantengled quaternion input features, from any real-valued input signal, enabling the use of QNNs regardless of the dimensionality of the considered task.
Conducted experiments on speech recognition and document classification show that the proposed approaches outperform traditional quaternion features.
Choosing appropriate database management systems (DBMS) and/or execution platforms for given database (DB) is complex and tends to be time-and effort-intensive since this choice has an important impact on the satisfaction of non-functional requirements (e.g., temporal performance or energy consumption).
Indeed, a large number of tests have been performed for assessing the quality of developed DB.
This assessment often involves metrics associated with non-functional requirement.
That leads to a mine of tests covering all life-cycle phases of the DB's design.
Tests and their environments are usually published in scientific articles or specific websites such as Transaction Processing Council (TPC).
Therefore, this thesis bas taken a special interest to the capitalization and the reutilization of performed tests to reduce and mastery the complexity of the DBMS/platforms selection process.
By analyzing the test accurately, we identify that tests concem: the data set, the execution platform, the addressed non-functional requirements, the used queries, etc.
Thus, we propose an approach of conceptualization and persistence of all dimensions as well as the results of tests.
Conseguently, this thesis leads to the following contributions.
(1) The design model based on descriptive, prescriptive and ontological concepts to raise the different dimensions.
(2) The development of a multidimensional repository to store the test environments and their results.
(3) The development of a decision making methodology based on a recommender system for DBMS and platforms selection.
In Natural Language Processing, there is many works in Word Sense Disambiguation (WSD).
It's explained in two points: first, this task is used in many application software, second, there is no agreement for the method.
We present a new model based on theory of dynamic construction of meaning.
We develop a new type of semantic classes of lexical units which depend of context.
Information retrieval and decision support systems need fast and accurate access to the content of documents and eﬃcient medical knowledge processing.
Indexing (describing using keywords) enables access to knowledge and knowledge processing.
In the medical domain, an increasing number of resources are available in electronic format, and there is a growing need for automatic solutions to facilitate knowledge access and indexing.
The objectives of my PhD work are the implementation of an automatic multi-terminology multi-document and multi-task indexing help-system namely F-MTI (French Multi-terminology Indexer).
It uses Natural Language processing methods to produce an indexing proposition for medical documents.
We applied it to resources indexing in a French online health catalogue, namely CISMeF, to therapeutical data indexing for drug medication and to diagnosis and health procedures indexing for patient medical records.
In this thesis, I will mainly focus on variational inference and probabilistic models.
In particular, I will cover several projects I have been working on during my PhD about improving the efficiency of AI/ML systems with variational techniques.
The thesis consists of two parts.
In the first part, the computational efficiency of probabilistic graphical models is studied.
In the second part, several problems of learning deep neural networks are investigated, which are related to either energy efficiency or sample efficiency
This study deals with the usage of two French linguistic variables liaison and elision, which are traditionally described as phonological variables.
They are studied during natural interactions between three children and their parents.
More precisely, the aim of this thesis is to describe the specificities of the child directed speech (CDS) concerning the usage of liaison and elision to measure their impact on the emergence of these phonological variables in the speech of the children.
After the presentation of the theoretical context of the study (Usage-Based Models and Construction Grammar) and the methodology used to collect, structure, and analyse the data, the research is divided into three analysis sections.
The aim of the first corpus based study, a descriptive one, is twofold.
The first objective is to describe the variation to which children are exposed at home.
A second objective is to compare the results of previous studies on liaison acquisition, obtained mainly from experimental tasks, with data extracted from dense corpora collected during natural interactions between the children and their parents.
In particular, this study shows that usage factors, including the frequency of items, influence the production of phonological variables.
The second study focuses on the specificities of CDS.
The results show that the usage of phonological variables is modulated in CDS, essentially at an early stage of language acquisition.
Then, this modulation attenuates during the child's development.
The aim of the third study is to connect parent's productions and children's productions.
It appears that the results concerning the development of phonological variation are in step with the assumptions provided by the usage-based models: at an early stage, the variation is memorized into specific constructions, particularly salient and frequent in CDS.
Today's fast changing environment imposes new challenges for effective management of business processes.
In such a highly dynamic environment, the business process design becomes time-consuming, error-prone, and costly.
Therefore, seeking reuse and adaptability is a pressing need for a successful business process design.
Configurable reference models recently introduced were a step toward enabling a process design by reuse while providing flexibility.
A configurable process model is a generic model that integrates multiple process variants of a same business process in a given domain through variation points.
These variation points are referred to as configurable elements and allow for multiple design options in the process model.
A configurable process model needs to be configured according to a specific requirement by selecting one design option for each configurable element.
On the one hand, as configurable process models tend to be very complex with a large number of configurable elements, many automated approaches have been proposed to assist their design.
However, existing approaches propose to recommend entire configurable process models which are difficult to reuse, cost much computation time and may confuse the process designer.
On the other hand, the research results on configurable process model design highlight the need for means of support to configure the process.
Therefore, many approaches proposed to build a configuration support system for assisting end users selecting desirable configuration choices according to their requirements.
Our objective is twofold: (i) assisting the configurable process design in a fin-grained way using configurable process fragments that are close to the designers interest and (ii) automating the creation of configuration support systems in order to release the process analysts from the burden of manually building them.
In order to achieve the first objective, we propose to learn from the experience gained through past process modeling in order to assist the process designers with configurable process fragments.
The proposed fragments inspire the process designer to complete the design of the ongoing process.
To achieve the second objective, we realize that previously designed and configured process models contain implicit and useful knowledge for process configuration.
This thesis focuses on the formalisms that make it possible to mathematically represent not only the meaning of independent sentences, but also whole texts, including the meaning relations that link sentences together.
Not only are we interested in meaning and its representation, but also on the algorithmic process of how this representation is computed using the sequence of words that constitute the text.
We thus find ourselves at a point where three disciplines intersect: discourse analysis, formal semantics and computational linguistics.
Most formal work on discourse pay little attention to reporting verbs (say, tell, etc.) and attitude verbs (think, believe, etc.).
These verbs, henceforth 'AVs', all express the attitude or stance of one person on a given proposition.
They are used frequently and introduce many subtleties that are not addressed in current theories.
The main objective of this thesis is to shed light on the principles of a formal grammar that is compatible with discourse analysis that takes AVs into account.
We therefore start by presenting a set of linguistic data illustrating the interactions between AVs and discourse relations.
Adverbial connectives (then, for example, etc.) are usually considered anaphoric.
One might wonder, however, whether, in practice, a computational linguistic system cannot deal with this particular category of anaphora as a kind of structural dependency, meaning that syntax is somehow extended above the sentence level.
This is what we try to achieve using the D-STAG formalism.
Consequently, we develop an anaphor based approach, in which the arguments of discourse relations are not determined solely by the grammatical structures of the utterances.
This is made possible by continuation semantics, which we use in conjunction with event semantics.
We suggest a number of potential answers and study the case of negation in more detail.
We argue that these difficulties originate from the standard analysis of negation, which interprets positive and negative sentences is an essentially different fashion.
Rejecting this view, we propose a novel formalisation of negative events that is relevant to the analysis of various linguistic phenomena.
These days with the increasing abundance of data with high dimensionality, high dimensional classification problems have been highlighted as a challenge in machine learning community and have attracted a great deal of attention from researchers in the field.
In recent years, sparse and stochastic learning techniques have been proven to be useful for this kind of problem.
In this thesis, we focus on developing optimization approaches for solving some classes of optimization problems in these two topics.
Our methods are based on DC (Difference of Convex functions) programming and DCA (DC Algorithms) which are wellknown as one of the most powerful tools in optimization.
The thesis is composed of three parts.
The first part tackles the issue of variable selection.
The second part studies the problem of group variable selection.
The final part of the thesis concerns the stochastic learning.
In the first part, we start with the variable selection in the Fisher's discriminant problem (Chapter 2) and the optimal scoring problem (Chapter 3), which are two different approaches for the supervised classification in the high dimensional setting, in which the number of features is much larger than the number of observations.
Continuing this study, we study the structure of the sparse covariance matrix estimation problem and propose four appropriate DCA based algorithms (Chapter 4).
Two applications in finance and classification are conducted to illustrate the efficiency of our methods.
The second part studies the L_p,0regularization for the group variable selection (Chapter 5).
Using a DC approximation of the L_p,0norm, we indicate that the approximate problem is equivalent to the original problem with suitable parameters.
Considering two equivalent reformulations of the approximate problem we develop DCA based algorithms to solve them.
Regarding applications, we implement the proposed algorithms for group feature selection in optimal scoring problem and estimation problem of multiple covariance matrices.
In the third part of the thesis, we introduce a stochastic DCA for large scale parameter estimation problems (Chapter 6) in which the objective function is a large sum of nonconvex components.
As an application, we propose a special stochastic DCA for the loglinear model incorporating latent variables
This cross-disciplinary research study covers controlled languages and French to Arabic machine translation, two intimately related concepts.
In a situation of crisis where communication must play its full role, and in the context of increasing globalisation where many languages coexist, our research findings show that the combination of these two concepts is sorely needed.
No one can deny today the predominant role played by security in people's daily life and the significant challenges it presents in modern societies.
However, and contrary to an entrenched idea that tends to associate the risk of poor communication only with oral transmission, the use of written language can also be subject to risk.
Indeed, a protocol or an alert which is badly formulated can provoke serious accidents due to misunderstanding, in particular during a crisis and under stress.
It is in this context that our research has been undertaken.
Indeed, new concepts are introduced by means of several normative methods involved not only in the controlling process but also in the machine translation process.
It introduces new concepts including controlled mirror macrostructures, where the syntax and semantics of the source and target languages are represented at the same level.
Social interaction refers to any interaction between two or more individuals, in which information sharing is carried out without any mediating technology.
In the context of testing and observational studies, multiple mechanisms are used to study these interactions such as questionnaires, direct observation and analysis of events by human operators, or a posteriori observation and analysis of recorded events by specialists (psychologists, sociologists, doctors, etc.).
In order to face the aforementioned issues, the need to automatize the social interaction analysis process is highlighted.
So, it is a question of bridging the gap between human-based and machine-based social interaction analysis processes.
Therefore, we propose a holistic approach that integrates multimodal heterogeneous cues and contextual information (complementary "exogenous" data) dynamically and optionally according to their availability or not.
Such an approach allows the analysis of multi "signals" in parallel (where humans are able only to focus on one).
This analysis can be further enriched from data related to the context of the scene (location, date, type of music, event description, etc.) or related to individuals (name, age, gender, data extracted from their social networks, etc.).
The contextual information enriches the modeling of extracted metadata and gives them a more "semantic" dimension.
Managing this heterogeneity is an essential step for implementing a holistic approach.
In this dissertation, we propose methods and data driven machine learning solutions which address and benefit from the recent overwhelming growth of digital media content.
First, we consider the problem of improving the efficiency of image retrieval.
We propose a coordinated local metric learning (CLML) approach which learns local Mahalanobis metrics, and integrates them in a global representation where the l2 distance can be used.
This allows for data visualization in a single view, and use of efficient ` 2-based retrieval methods.
Our approach can be interpreted as learning a linear projection on top of an explicit high-dimensional embedding of a kernel.
This interpretation allows for the use of existing frameworks for Mahalanobis metric learning for learning local metrics in a coordinated manner.
Our experiments show that CLML improves over previous global and local metric learning approaches for the task of face retrieval.
We explore different metric learning strategies over features from the intermediate layers of the networks, to reduce the discrepancies between the different modalities.
In our experiments we found that the depth of the optimal features for a given modality, is positively correlated with the domain shift between the source domain (CNN training data) and the target domain.
Experimental results show the that we can use CNNs trained on visible spectrum images to obtain results that improve over the state-of-the art for heterogeneous face recognition with near-infrared images and sketches.
Third, we present convolutional neural fabrics for exploring the discrete andexponentially large CNN architecture space in an efficient and systematic manner.
Instead of aiming to select a single optimal architecture, we propose a “fabric” that embeds an exponentially large number of architectures.
The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern.
The only hyperparameters of the fabric (the number of channels and layers) are not critical for performance.
The acyclic nature of the fabric allows us to use backpropagation for learning.
Learning can thus efficiently configure the fabric to implement each one of exponentially many architectures and, more generally, ensembles of all of them.
While scaling linearly in terms of computation and memory requirements, the fabric leverages exponentially many chain-structured architectures in parallel by massively sharing weights between them.
We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset
This problem is usually addressed with a two-stepped treatment: filtering the multidimensional, heterogeneous and imprecise measurements into symbolic events and then using efficient plan recognition techniques on those events.
This allows, among other things, the possibility of describing high level symbolic plan steps without being overwhelmed by low level sensor specificities.
However, the first step is information destructive and generates additional ambiguity in the recognition process.
Furthermore, splitting the behavior recognition task leads to unnecessary computations and makes the building of the plan library tougher.
Thus, we propose to tackle this problem without dividing the solution into two processes.
We present a hierarchical model, inspired by the formal language theory, allowing us to describe behaviors in a continuous way, and build a bridge over the semantic gap between measurements and intents.
Thanks to a set of algorithms using this model, we are able, from observations, to deduce the possible future developments of the monitored area while providing the appropriate explanations.
Usually, human beings are able to quickly distinguish between different places, solely from their visual appearance.
Such a semantic category can thus be used as contextual information which fosters object detection and recognition.
Recent works in semantic place recognition seek to endow the robot with similar capabilities.
Contrary to classical localization and mapping works, this problem is usually addressed as a supervised learning problem.
The question of semantic places recognition in robotics-the ability to recognize the semantic category of a place to which scene belongs to-is therefore a major requirement for the future of autonomous robotics.
It is indeed required for an autonomous service robot to be able to recognize the environment in which it lives and to easily learn the organization of this environment in order to operate and interact successfully.
If we make the hypothesis that objects are more easily recognized when the scene in which they appear is identified, the second approach seems more suitable.
It is however strongly dependent on the nature of the image descriptors used, usually empirically derived from general considerations on image coding.
Compared to these many proposals, another approach of image coding, based on a more theoretical point of view, has emerged the last few years.
Energy-based models of feature extraction based on the principle of minimizing the energy of some function according to the quality of the reconstruction of the image has lead to the Restricted Boltzmann Machines (RBMs) able to code an image as the superposition of a limited number of features taken from a larger alphabet.
It has also been shown that this process can be repeated in a deep architecture, leading to a sparse and efficient representation of the initial data in the feature space.
A complex problem of classification in the input space is thus transformed into an easier one in the feature space.
We show that after appropriate coding a softmax regression in the projection space is sufficient to achieve promising classification results.
To our knowledge, this approach has not yet been investigated for scene recognition in autonomous robotics.
We compare our methods with the state-of-the-art algorithms using a standard database of robot localization.
We study the influence of system parameters and compare different conditions on the same dataset.
These experiments show that our proposed model, while being very simple, leads to state-of-the-art results on a semantic place recognition task.
This thesis deals with the semantics of causative constructions.
It develops a semantic typology for English periphrastic causative verbs CAUSE, MAKE, HAVE, GET, and LET, based on the force-dynamics model.
The third chapter offers a discussion of some of the most commonly shared hypotheses about the semantics of English periphrastic causative verbs in literature.
In the fourth chapter, we propose a corpus study of the lexico-semantic features of the verbs CAUSE, MAKE, HAVE, GET and LET.
The last chapter presents a newly semantic typology for English periphrastic causative verbs, drawn upon the data we collected from our corpus study.
The abundance of biomedical information expressed in natural language has resulted in the need for methods to process this information automatically.
In the field of Natural Language Processing (NLP), Information Extraction (IE) focuses on the extraction of relevant information from unstructured data in natural language.
A great deal of IE methods today focus on Machine Learning (ML) approaches that rely on deep linguistic processing in order to capture the complex information contained in biomedical texts.
In particular, syntactic analysis and parsing have played an important role in IE, by helping capture how words in a sentence are related.
It focuses on a task-based approach to dependency parsing evaluation and parser selection, including a detailed error analysis.
In order to achieve a high quality of syntax-based IE, different stages of linguistic processing are addressed, including both pre-processing steps (such as tokenization) and the use of complementary linguistic processing (such as the use of semantics and coreference analysis).
This thesis also explores how the different levels of linguistics processing can be represented for use within an ML-based IE algorithm, and how the interface between these two is of great importance.
The methods and approaches described are explored using two different biomedical corpora, demonstrating how the IE results are used in real-life tasks.
Industrial clean rooms or operating rooms are critical places often hosting dangerous or complex processes.
Their design, building and use are thus difficult and constrained by a large amount of standards and rules.
Qualifying these environments, in order to ensure their quality, consequently requires a high level of expertise and lacks assisting tools.
This leads us to propose a unified approach aiming at easing the qualification process of standardized environments.
It relies on a graph-based representation of the set of standards and rules that apply to a specific case, as well as on step-by-step modelling of the whole target environment.
This approach, applied to medical environments for validation purposes, remains generic and can be applied to any kind of standardized environment.
This work presents novel techniques for parsing the structures of multi-party dialogue and argumentative texts.
Finding the structure of extended texts and conversations is a critical step towards the extraction of their underlying meaning.
The task is notoriously hard, as discourse is a high-level description of language, and multi-party dialogue involves many complex linguistic phenomena.
Historically, representation of discourse moved from local relationships, forming unstructured collections, towards trees, then constrained graphs.
Our work uses the latter framework, through Segmented Discourse Representation Theory.
We base our research on a annotated corpus of English chats from the board game The Settlers of Catan.
We discuss two corpus-related experiments.
The first expands the definition of the Right Frontier Constraint, a formalization of discourse coherence principles, to adapt it to multi-party dialogue.
The second demonstrates a data extraction process giving a strategic advantage to an artificial player of Settlers by inferring its opponents'assets from chat negotiations.
We propose new methods to parse dialogue, using jointly machine learning, graph algorithms and linear optimization, to produce rich and expressive structures with greater accuracy than previous attempts.
We describe our method of constrained discourse parsing, first on trees using the Maximum Spanning Tree algorithm, then on directed acyclic graphs using Integer Linear Programming with a number of original constraints.
We finally apply these methods to argumentative structures, on a corpus of English and German texts, jointly annotated in two discourse representation frameworks and one argumentative.
We compare the three annotation layers, and experiment on argumentative parsing, achieving better performance than similar works.
As part of ongoing work to computerize a large number of "poorly endowed" languages, especially those in the French-speaking world, we have created a French-Somali machine translation system dedicated to a journalistic sub-language, allowing to obtain quality translations from a bilingual body built by post-editing of GoogleTranslate results for the Somali and non-French speaking populations of the Horn of Africa.
The latter is an aligned corpus of very good quality, because we built in by post-editions editing pre-translations of produced by GT, which uses with a combination of the its French-English and English-Somali MT language pairs.
It That corpus was also evaluated by 9 bilingual annotators who gave assigned a quality note score to each segment of the corpus and corrected our post-editing.
On the other hand, we have set up an iMAG (multilingual interactive access gateway) that allows non-French-speaking Somali surfers on the continent to access the online edition of the newspaper "La Nation de Djibouti" in Somali.
We developed an original approach to Arabic traditional morphology, involving new concepts in Semitic lexicology, morphology, and grammar for standard written Arabic.
This new methodology for handling the rich and complex Semitic languages is based on good practices in Finite-State technologies (FSA/FST) by using Unitex, a lexicon-based corpus processing suite.
For verbs (Neme, 2011), I proposed an inflectional taxonomy that increases the lexicon readability and makes it easier for Arabic speakers and linguists to encode, correct, and update it.
Traditional grammar defines inflectional verbal classes by using verbal pattern-classes and root-classes.
In our taxonomy, traditional pattern-classes are reused, and root-classes are redefined into a simpler system.
The lexicon of verbs covered more than 99% of an evaluation corpus.
For nouns and adjectives (Neme, 2013), we went one step further in the adaptation of traditional morphology.
First, while this tradition is based on derivational rules, we found our description on inflectional ones.
Next, we keep the concepts of root and pattern, which is the backbone of the traditional Semitic model.
Still, our breakthrough lies in the reversal of the traditional root-and-pattern Semitic model into a pattern-and-root model, which keeps small and orderly the set of pattern classes and root sub-classes.
I elaborated a taxonomy for broken plural containing 160 inflectional classes, which simplifies ten times the encoding of broken plural.
Since then, I elaborated comprehensive resources for Arabic.
These resources are described in Neme and Paumier (2019).
To take into account all aspects of the rich morphology of Arabic, I have completed our taxonomy with suffixal inflexional classes for regular plurals, adverbs, and other parts of speech (POS) to cover all the lexicon.
In all, I identified around 1000 Semitic and suffixal inflectional classes implemented with concatenative and non-concatenative FST devices.
From scratch, I created 76000 fully vowelized lemmas, and each one is associated with an inflectional class.
These lemmas are inflected by using these 1000 FSTs, producing a fully inflected lexicon with more than 6 million forms.
I extended this fully inflected resource using agglutination grammars to identify words composed of up to 5 segments, agglutinated around a core inflected verb, noun, adjective, or particle.
The agglutination grammars extend the recognition to more than 500 million valid delimited word forms, partially or fully vowelized.
The flat file size of 6 million forms is 340 megabytes (UTF-16).
It is compressed then into 11 Mbytes before loading to memory for fast retrieval.
The generation, compression, and minimization of the full-form lexicon take less than one minute on a common Unix laptop.
The lexical coverage rate is more than 99%.
The tagger speed is 5000 words/second, and more than 200 000 words/s, if the resources are preloaded/resident in the RAM.
The accuracy and speed of our tools result from our systematic linguistic approach and from our choice to embrace the best practices in mathematical and computational methods.
The lookup procedure is fast because we use Minimal Acyclic Deterministic Finite Automaton (Revuz, 1992) to compress the full-form dictionary, and because it has only constant strings and no embedded rules.
The problem of automatic logical meaning representation for ambiguous natural language utterances has been the subject of interest among the researchers in the domain of computational and logical semantics.
Ambiguity in natural language may be caused in lexical/syntactical/semantical level of the meaning construction or it may be caused by other factors such as ungrammaticality and lack of the context in which the sentence is actually uttered.
The traditional Montagovian framework and the family of its modern extensions have tried to capture this phenomenon by providing some models that enable the automatic generation of logical formulas as the meaning representation.
However, there is a line of research which is not profoundly investigated yet: to rank the interpretations of ambiguous utterances based on the real preferences of the language users.
This gap suggests a new direction for study which is partially carried out in this dissertation by modeling meaning preferences in alignment with some of the well-studied human preferential performance theories available in the linguistics and psycholinguistics literature.
In order to fulfill this goal, we suggest to use/extend Categorial Grammars for our syntactical analysis and Categorial Proof Nets as our syntactic parse.
We also use Montagovian Generative Lexicon for deriving multi-sorted logical formula as our semantical meaning representation.
We use a framework called Montagovian Generative Lexicon.
During crisis events such as disasters, the need of real-time information retrieval (IR) from microblogs remains inevitable.
However, the huge amount and the variety of the shared information in real time during such events over-complicate this task.
Unlike existing IR approaches based on content analysis, we propose to tackle this problem by using user-centricIR approaches with solving the wide spectrum of methodological and technological barriers inherent to: 1) the collection of the evaluated users data, 2) the modeling of user behavior, 3) the analysis of user behavior, and 4) the prediction and tracking of prominent users in real time.
In this context, we detail the different proposed approaches in this dissertation leading to the prediction of prominent users who are susceptible to share the targeted relevant and exclusive information on one hand and enabling emergency responders to have a real-time access to the required information in all formats (i.e. text, image, video, links) on the other hand.
Based on the selected features, we have designed several engineered features qualifying user activities by considering both their on-topic and off-topic shared information.
Thirdly, based on this proposed user modeling approach, we train various prediction models to learn to differentiate between prominent and non-prominent users behavior during crisis event.
The efficiency and efficacy of these prediction models have been validated thanks to the data collections extracted by our multi-agents system MASIR during two flooding events who have occured in France and the different ground-truths related to these collections.
Our study focuses on elliptic mechanisms within associative meronymic anaphora.
We departed from the assumption that in this type of anaphora, there are two structures: a deep structure and a surface structure.
The first consists in the presence of three elements: the whole, the partitive predicate and the part.
The second, which shows the different types of ellipsis, is the topic of our work.
We focused on three types of Ellipsis, which we considered typical meronymic anaphora: ellipsis of partitive predicate, ellipsis of the second element of the structure [N De N] and that of the anaphoric antecedent.
Treated separately, nominal anaphora, verbal anaphora, and adverbial anaphora were initially submitted to a syntactic-semantic description, and then, to the three primary functions theory.
This theory allowed us to explain the ability of certain items to be elided in the associative meronymic anaphora.
Mobile devices such as smartphones, smartwatches or smart glasses have revolutionized how we interact.
We are interested in smart glasses because they have the advantage of providing a simultaneous view of both the physical and the digital worlds.
However, the interaction for smart glasses is not well explored.
More suitable interactions on this device can convince users to use it more in everyday life.
The thesis subject is focused on the study and development of new interaction techniques with smart glasses.
Indeed, it is not possible to interact so finely in mobility or in an emergency situation compared to a stable situation such as sitting in front of a desktop.
The work context is in the health field and in particular a healthcare professional visiting his patient in a hospital.
The professional must be able to access the patient's data already collected, obtain physiological data in real time, prepare his diagnosis and communicate with his colleagues.
The scientific problem for the thesis here is to find solutions that allow him to perform these tasks in a more precise and less restrictive way.
The goal is to make the diagnosis and information sharing more effective and to make a device still uncontrolled a functional system in a professional healthcare environment.
In this perspective, the work of this thesis presents theoretical and applicative contributions.
We first listed the various work performed in the context of smart glasses for the health field, while indicating potential, relevant results and limitations.
We focused on smart glasses to visualize and manipulate patient records.
From a conceptual point of view, we have proposed an eight-dimensional design space to identify gaps in existing systems and assist in the design of new systems.
From an application point of view, for output interaction, we introduced a technique called K-Fisheye on a tile-based interface that allows the user to browse a large dataset like in the health record.
We used the design space to adapt an existing system for the smart glasses.
The prototype obtained called mCAREglass.
Finally, we designed a new text entry technique called TEXTile by using a new interactive fabric communicating with glasses.
Many applications produce and receive continuous, unlimited, and high-speed data streams.
This raises obvious problems of storage, treatment and analysis of data, which are only just beginning to be treated in the domain of data streams.
The learning of this model is, contrary to that of its former self, driven not only by the novelty part in the input data but also by the data itself.
Thereby, ILoNDF can continuously extract new knowledge relating to the relative frequencies of the data and their variables.
Firstly, we focus on the study of ILoNDF's behavior for one-class classification when dealing with high-dimensional noisy data.
This study enabled us to highlight the pure learning capacities of ILoNDF with respect to the key classification methods suggested until now.
Next, we are particularly involved in the adaptation of ILoNDF to the specific context of information filtering.
Our goal is to set up user-oriented filtering strategies rather than system-oriented in following two types of directions.
The first direction concerns user modeling relying on the model ILoNDF.
This provides a new way of looking at user's need in terms of specificity, exhaustivity and contradictory profile-contributing criteria.
The second direction, complementary to the first one, concerns the refinement of ILoNDF's functionality in order to confer it the capacity of tracking drifting user's need over time.
Finally, we consider the generalization of our previous work to the case where streaming data can be divided into multiple classes.
One of the main challenges of the synthesizer market and the research in sound synthesis nowadays lies in proposing new forms of synthesis allowing the creation of brand new sonorities while offering musicians more intuitive and perceptually meaningful controls to help them reach the perfect sound more easily.
Indeed, today's synthesizers are very powerful tools that provide musicians with a considerable amount of possibilities for creating sonic textures, but the control of parameters still lacks user-friendliness and may require some expert knowledge about the underlying generative processes.
In this thesis, we are interested in developing and evaluating new data-driven machine learning methods for music sound synthesis allowing the generation of brand new high-quality sounds while providing high-level perceptually meaningful control parameters.
The first challenge of this thesis was thus to characterize the musical synthetic timbre by evidencing a set of perceptual verbal descriptors that are both frequently and consensually used by musicians.
Two perceptual studies were then conducted: a free verbalization test enabling us to select eight different commonly used terms for describing synthesizer sounds, and a semantic scale analysis enabling us to quantitatively evaluate the use of these terms to characterize a subset of synthetic sounds, as well as analyze how consensual they were.
In a second phase, we investigated the use of machine learning algorithms to extract a high-level representation space with interesting interpolation and extrapolation properties from a dataset of sounds, the goal being to relate this space with the perceptual dimensions evidenced earlier.
Following previous studies interested in using deep learning for music sound synthesis, we focused on autoencoder models and realized an extensive comparative study of several kinds of autoencoders on two different datasets.
These experiments, together with a qualitative analysis made with a non real-time prototype developed during the thesis, allowed us to validate the use of such models, and in particular the use of the variational autoencoder (VAE), as relevant tools for extracting a high-level latent space in which we can navigate smoothly and create new sounds.
However, so far, no link between this latent space and the perceptual dimensions evidenced by the perceptual tests emerged naturally.
As a final step, we thus tried to enforce perceptual supervision of the VAE by adding a regularization during the training phase.
Using the subset of synthetic sounds used in the second perceptual test and the corresponding perceptual grades along the eight perceptual dimensions provided by the semantic scale analysis, it was possible to constraint, to a certain extent, some dimensions of the VAE high-level latent space so as to match these perceptual dimensions.
A final comparative test was then conducted in order to evaluate the efficiency of this additional regularization for conditioning the model and (partially) leading to a perceptual control of music sound synthesis.
Drug non-compliance refers to situations where the patient does not follow instructions from medical authorities when taking medications.
Such situations include taking too much (overuse) or too little (underuse) of medications, drinking contraindicated alcohol, or making a suicide attempt using medication.
According to [HAYNES 2002] increasing drug compliance may have a bigger impact on public health than any other medical improvements.
However non-compliance data are difficult to obtain since non-adherent patients are unlikely to report their behaviour to their healthcare providers.
First we collect a corpus of messages written by users from medical forums.
We build vocabularies of medication and disorder names such as used by patients.
We use these vocabularies to index medications and disorders in the corpus.
Then we use supervised learning and information retrieval methods to detect messages talking about non-compliance.
We identify 3 main motivations: self-medication, seeking an effect besides the effect the medication was prescribed for, or being in addiction or habituation situation.
Self-medication is an umbrella for several situations: avoiding an adverse effect, adjusting the medication's effect, underuse a medication seen as useless, taking decisions without a doctor's advice.
Non-compliance can also happen thanks to errors or carelessness, without any particular motivation.
Our work provides several kinds of result: annotated corpus with non-compliance messages, classifier for the detection of non-compliance messages, typology of non-compliance situations and analysis of the causes of non-compliance.
In this thesis, we study the segmentation of an audio stream in speech, music and speech on music (S/M).
This is a fundamental step for all application based on automatic transcription of radiophonic stream and most commonly multimedia.
The target application here is a keyword detection system in broadcast programs.
The application performance depends on the quality of the signal segmentation given by the speech/music discrimination system.
Indeed, bad signal classification can give miss-detections or false alarms.
To improve the speech/music discrimination task, we propose a new signal parameterization method.
We use the wavelet decomposition which allows an analysis of non-stationary signal like music for instance.
We compute different energies on wavelet coefficients to construct our feature vectors.
We chose a class/non-class architecture because it allows to find independently the best parameters for each S/NS and P/NP tasks.
A fusion of the classifier ouputs is then performed to obtain the final decision: speech, music or speech on music.
The obtained results on a real broadcast program corpus show that our wavelet-based parameterization gives a significant improvement in performance in both M/NM and S/M discrimination tasks compared to the baseline parameterization using cepstral coefficients.
The present thesis subject aims to contribute to the study of translation in Egypt between 1798 and 1873 and its role in the reforms undertaken by intellectuals under the aegis of the reformist Wali's policy (Mohamed Ali Pasha), but also the al-Alsun school and its contribution to translation, lexical expansion and the acquisition of Nahḍa.
Our subject is particularly important because it deals not only with the beginnings of linguistic and cultural relations between France and Egypt which contributed to the emergence of modern legal Arabic with the school of al-Alsun in Cairo in 1834, but also represent, for some unknown reasons, an uncharted territory for researchers.
Particular attention has been given to the region of Mašriq al 'arabī with a good reason, for the latter constitutes a hub where the bulk of translation activities took place, as opposed to other areas of the Arab world.
As a matter of fact, while collecting material for this research project, no book dealing with this school has been found which, however, played a crucial role in the evolution of Arab society in general and that of Egypt in particular.
However, as far as «translation» is concerned in the Arab world, the historical dimension brings to light such great skills as the translating the person into Arabic, or the Greek into Arabic in the Omeyyad era and Abbasside, but also translating in the contemporary sense of the word, which forms the subject of this research.
The aim of this thesis is to study sentiments in comparable documents.
First, we collect English, French and Arabic comparable corpora from Wikipedia and Euronews, and we align each corpus at the document level.
We further gather English-Arabic news documents from local and foreign news agencies.
Second, we present a cross-lingual document similarity measure to automatically retrieve and align comparable documents.
Then, we propose a cross-lingual sentiment annotation method to label source and target documents with sentiments.
Finally, we use statistical measures to compare the agreement of sentiments in the source and the target pair of the comparable documents.
The methods presented in this thesis are language independent and they can be applied on any language pair
This work deals with audio-visual speech synthesis.
In the vast literature available in this direction, many of the approaches deal with it by dividing it into two synthesis problems.
One of it is acoustic speech synthesis and the other being the generation of corresponding facial animation.
But, this does not guarantee a perfectly synchronous and coherent audio-visual speech.
To overcome the above drawback implicitly, we proposed a different approach of acoustic-visual speech synthesis by the selection of naturally synchronous bimodal units.
The synthesis is based on the classical unit selection paradigm.
The main idea behind this synthesis technique is to keep the natural association between the acoustic and visual modality intact.
We describe the audio-visual corpus acquisition technique and database preparation for our system.
We present an overview of our system and detail the various aspects of bimodal unit selection that need to be optimized for good synthesis.
The main focus of this work is to synthesize the speech dynamics well rather than a comprehensive talking head.
We describe the visual target features that we designed.
We subsequently present an algorithm for target feature weighting.
This algorithm that we developed performs target feature weighting and redundant feature elimination iteratively.
This is based on the comparison of target cost based ranking and a distance calculated based on the acoustic and visual speech signals of units in the corpus.
Finally, we present the perceptual and subjective evaluation of the final synthesis system.
The results show that we have achieved the goal of synthesizing the speech dynamics reasonably well
Natural Language Processing (NLP) systems are continuously faced with the problem of generating concurrent hypotheses, of which some can be erroneous.
In order to avoid the propagation of erroneous hypotheses, it appears to be essential to apply specific control strategies, which aim to distinguishing concurrent hypotheses based on their relevance.
On most of observed indetermination cases, we have noticed that multiple heterogeneous knowledge sources have to be combined to determine the hypotheses relative relevance.
According to this observation, we show that the control of the indetermination cases can be formalised as a decisional process based on multiple criteria.
This approach differs from alternative methods by the importance granted to knowledge and preferences that an expert can express about a given problem.
From this innovative intersection between NLP and MCDA, our work has been focalised on the development of a decisional module dedicated to multicriteria control.
The integration of this module into a complete NLP system has allowed us to attest the feasibility of our approach and to perform experimentation on concrete indetermination cases.
This thesis is the interaction result of two disciplines that are the change detection in multitemporal images and the evidential reasoning using the Dempster-Shafer theory (DST).
Addressing the problem of change detection and analyzing by the DST, requires the determination of an exhaustive and exclusive frame of discernment.
This issue is complex when images lake prior information.
The idea of this algorithm is the representation of each class by a varied number of centroids in order to guarantee a better characterization of classes.
To ensure the frame of discernment exhaustiveness, we proposed a new cluster validity index able to identify the optimal number of semantic classes.
The third contribution is to exploit the position of the pixel in relation to class centroids and its membership distribution in order to define the mass distribution that represents information.
We have emphasized the capacity of evidential conflict to indicate multi-temporal transformations.
We reasoned on the decomposition of the global conflict and the estimation of the partial conflicts between the couples of focal elements to measure the conflict caused by the change.
This strategy allows to identify the couple of classes that participate in the change.
To quantify this conflict, we proposed a new measure of change noted CM.
Finally, we proposed an algorithm to deduce the binary map of changes from the partial conflicts map.
In the forensic speaker recognition field, voice disguise presents a specific interest.
Most criminals try to disguise their voice before a miscellaneous calls or a terrorist claim.
Their aim is to change the register of their voice quality in order to false their identity or to mimic the voice of another person.
This thesis proposes to analyse two different kinds of disguise: the transformation of the voice by non electronic and deliberate means and the conversion of the voice based in electronic and deliberate technique, in order to take the identity of a target person.
In one hand, the analysis of voice transformation is based on an acoustic point of view to measure specific changes in speech and on an automatic approach to detect disguise.
Four kinds of disguises which are considered as the most common in forensics are studied.
A constraint of audibility and intelligibility has been imposed to the speakers who have realized the database.
In the automatic experiment, the best way to detect a voice disguise is to use SVM technique.
The level of performance is an AUC (area under curve) at 0,79.
In another hand, voice conversion techniques are proposed and applied on two forensic scenarios: the imitation of a politician from an Internet recording, and the application of voice disguise reversibility.
Different kinds of tests are proposed to evaluate the relevance of the result, based on objective and subjective measurements.
Evidence-Based Medicine has been formalized as Clinical Practice Guidelines, which define workflows and recommendations to be followed for a given clinical domain.
These documents were formalized aiming to standardize healthcare and seeking the best patient outcomes.
Nevertheless, clinicians do not adhere as expected to these guidelines due to several clinical and implementation limitations.
On one hand, clinicians do not feel familiar, agree with and or are unaware of guidelines, hence doubting their self-efficacy and outcome expectancy compared to previous or more common practices.
On the other hand, maintaining these guidelines updated with the most recent evidence requires continuous versioning of these paper-based documents.
Clinical Decision Support Systems are proposed to help during the clinical decision-making process with the computerized implementation of the guidelines to promote their easy consultation and increased compliance.
Even if these systems help improving guideline compliance, there are still some barriers inherited from paper-based guidelines that are not solved, such as managing complex cases not defined within the guidelines or the lack of representation of other external factors that may influence the provided treatments, biasing from guidelines'recommendations (i.e. patient preferences).
This thesis proposes an advanced Clinical Decision Support System for coping with the purely guideline-based support limitations and going beyond the formalized knowledge by analyzing the clinical data, outcomes, and performance of all the decisions made over time.
To achieve these objectives, an approach for modeling the clinical knowledge and performance in a semantically validated and computerized way has been presented, leaning on an ontology and the formalization of the Decisional Event concept.
Moreover, a domain-independent framework has been implemented for easing the process of computerizing, updating and implementing Clinical Practice Guidelines within a Clinical Decision Support System in order to provide clinical support for any queried patient.
For addressing the reported guideline limitations, a methodology for augmenting the clinical knowledge using experience has been presented along with some clinical performance and quality evaluation over time, based on different studied clinical outcomes, such as the usability and the strength of the rules for evaluating the clinical reliability behind the formalized clinical knowledge.
Finally, the accumulated Real World Data was explored to support future cases, promoting the study of new clinical hypotheses and helping in the detection of trends and patterns over the data using visual analytics tools.
The presented modules had been developed and implemented in their majority within the European Horizon 2020 project DESIREE, in which the use case was focused on supporting Breast Units during the decision-making process for Primary Breast Cancer patients management, performing a technical and clinical validation over the presented architecture, whose results are presented in this thesis.
Nevertheless, some of the modules have been also used in other medical domains such as Gestational Diabetes guidelines development, highlighting the interoperability and flexibility of the presented work.
With the explosive growth of digitization in cultural heritage, many cultural heritage institutions have been converting physical objects of cultural heritage into digital representation or descriptive representation.
However, the conversion resulted in several issues such as: 1) the documents are descriptive in nature, 2) ambiguity and brevity of the documents, 3) dedicated vocabulary is used in the documents, and 4) there is also variation in the terms used in the document.
Besides, the usage of inaccurate keywords also resulted in short query problem.
Most of the time, the issues are caused by the aggregated fault in annotating the documents while the short query problem is caused by naive user who has little prior knowledge and experience in cultural heritage domain.
In this research, the main aim is to model information access system to overcome partially the issues arising from the documentation process and the background of the users of digital cultural heritage.
Therefore, three types of information access tool are introduced and established namely information retrieval system, context search, and mobile game on cultural heritage that allow the user to access, learn, and explore the information on cultural heritage.
Basically, the main idea for information retrieval system and context search is to incorporate the link relationship between terms into the Language Model by extending of Dirichlet Smoothing to solve the problems arising from both the documentation process and background of the users.
In addition, a Preference Model is introduced based on the Theory of Charging a Capacitor to quantify the cognitive context based on time and integrate into the extended Dirichlet Smoothing.
Besides, a mobile game is introduced by integrating the elements of the games of monopoly and treasure hunt to mitigate the problems arising from the background of the users especially their casual behavior.
The first and second approaches were tested on the Cultural Heritage in CLEF (CHiC) collection that consists of short queries and documents.
The results show that the approach is effective and yields better accuracy during the retrieval.
Finally, a survey was carried out to investigate the third approach, and the result suggests that the game is able to help the participants to explore and learn the information on cultural heritage.
In addition, the participants also felt that an information seeking tool that is integrated with the game can provide more information to the user in a more convenient manner while playing the game and visiting the heritage sites in the game.
In conclusion, the results show that the proposed solutions are able to solve the problems arising from the documentation process and the background of the users of digital cultural heritage.
This thesis presents a method of Micro-Systemic Linguistic Analysis of Thai compound words.
Some notable points of view are discussed.
The second chapter identifies some essential characteristics of the Thai language such as a non-space writing style resulted in ambiguity in machine translation.
Different entities between Thai and French languages are underlined by means of the micro-systematic theory of the Centre Tesnière.
The third chapter analyses Thai compound words using a hybrid method involving morpho-syntactic parsing and a rule-based system corresponding to our model of data analysis.
The fourth chapter employs a technique of lexical-syntactic and semantic control enabling the definition of efficient algorithms.
The final chapter concludes our work with some future perspectives.
This study is presented as a reliable approach which enhances the elimination of word ambiguities in machine translation.
This hybrid method allows us to reach our objective and to find an effective way to translate Thai to French automatically.
The result could be an accessible tool for international research in the Thai and French languages
Detecting abusive language online is a classification problem with critical challenges that depend on machine understanding of natural language and the variety of rich and complex contexts in which natural language occurs.
It has been shown that neural networks and standard Deep Learning (DL) techniques can detect explicit offensive content in conversations but it is more difficult to make machines detect more subtle, but also more common forms of so called 'passive toxicity'such as sarcasm, passive-aggressive behavior, and politely worded but incendiary hate speech.
Traditional Machine Learning (ML) models and more recently word-based and character-based Convolution Neural Networks (CNN) and Recurrent Neural Networks (RNN) like Long Short-Term Memory (LSTM) models and Gated Recurrent Units (GRU), coupled with word embedding have been used to assist comment moderation and hate-speech detection.
Furthermore, some work focuses on interpreting these neural networks detecting verbal aggression.
As part of the Conversation AI research initiative at Google Jigsaw, our research will focus on exploring how new ground-breaking methods in deep learning can capture passive toxicity.
Conversation AI has experience in developing both theoretical models and tools for users and websites.
The proposed thesis will explore two orthogonal directions: labelled datasets and models.
For supervised classification, we need large datasets of labeled comments from actual online conversations.
We plan to start from the public datasets provided by the Jigsaw/Conversation-AI team.
They can either be manually augmented with labels by crowdsourcing or automatically labeled from linguistic features such as tags.
From initial experiments, these datasets contain significant number of passive toxic comments that get intermediate toxicity probabilities.
We will follow the approach used by Perspective API of selecting a small number of likely categories for labelling different kinds of passive toxicity.
We will leverage and improve the transformer and BERT models based on attention mechanisms for this application and see how they compare to other deep learning models.
Because passive toxicity leverages ambiguity in natural language, we will focus on modeling the emotional impact of messages, which Conversation AI found resulted in higher levels of inter-annotator agreement (compared to more complex semantic distinctions that policy violation documents try to make).
Methods of Reinforcement Learning (RL) may also be explored in this context.
The other aspect of the research is to analyze and understand the origins and mechanisms of passive aggression.
This can be done by measuring and visualizing the impact it has on conversations.
Open Source code and publications in competitive venues will be produced during the PhD program.
The models will be implemented in Python, with the TensorFlow framework.
Metrics used for comparing architectures include accuracy, precision, recall, specificity, fall-out, F1 score, Receiver Operating Characteristic curve Area Under Curve (ROC-AUC).
This research is expected to bring answers to various questions pertaining to conversations on social networks, such as: how to develop Deep Learning to identify passive toxicity in online conversations?
Can it understand the relevance of posts in a debate?
What triggers passive toxicity in conversation?
What happens in a conversation right after passive aggression has been detected?
Is it a one-time or an ongoing behavior from interlocutors?
What are the replies to subtle toxicity that can forestall an *awry-turning* conversation?
How does the early detection of it matter for keeping the conversation on track?
We hope the resulting models will perform well enough that they end up being provided through the Perspective API so that they can be directly used by industry, and to support a wide range of other research initiatives.
In order to bring people out of their online 'filter bubbles', they need to be able to have conversations on platforms safe from aggressiveness, violence and scapegoating.
Machine Learning, Deep Learning and Natural Language Processing strategies can help us to 'disagree more constructively'(The Righteous Mind, Jonathan Haidt).
Therefore, this research should allow to increase participation, quality, and empathy in online conversation at scale by developing new models applied to new datasets suggesting unbiased behaviors, guide and educate users about fairness.
Using technology to enable more rational and informed debate enters into a goal of fighting against online harassment while defending public debates, freedom of speech, democracy and pluralism.
Nowadays, there is a growing demand for personalization and the “one-size-fits-all” approach for hypermedia systems is no longer applicable.
Adaptive hypermedia (AH) systems adapt their behavior to the needs of individual users.
However due to the complexity of their authoring process and the different skills required from authors, only few of them have been proposed.
These last years, numerous efforts have been put to propose assistance for authors to create their own AH.
However, as explained in this thesis some problems remain.
In this thesis, we tackle two particular problems.
A first problem concerns the integration of authors'materials (information and user profile) into models of existing systems.
Thus, allowing authors to directly reuse existing reasoning and execute it on their materials.
We propose a semi-automatic merging/specialization process to integrate an author's model into a model of an existing system.
Our objectives are twofold: to create a support for defining mappings between elements in a model of existing models and elements in the author's model and to help creating consistent and relevant models integrating the two models and taking into account the mappings between them.
A second problem concerns the adaptation specification, which is famously the hardest part of the authoring process of adaptive web-based systems.
We propose an EAP framework with three main contributions: a set of elementary adaptation patterns for the adaptive navigation, a typology organizing the proposed elementary adaptation patterns and a semi-automatic process to generate adaptation strategies based on the use and the combination of patterns.
Our objectives are to define easily adaptation strategies at a high level by combining simple ones.
Furthermore, we have studied the expressivity of some existing solutions allowing the specification of adaptation versus the EAP framework, discussing thus, based on this study, the pros and cons of various decisions in terms of the ideal way of defining an adaptation language.
We propose a unified vision of adaptation and adaptation languages, based on the analysis of these solutions and our framework, as well as a study of the adaptation expressivity and the interoperability between them, resulting in an adaptation typology.
The unified vision and adaptation typology are not limited to the solutions analysed, and can be used to compare and extend other approaches in the future.
Besides these theoretical qualitative studies, this thesis also describes implementations and experimental evaluations of our contributions in an e-learning application.
This dissertation concerns the description of possessive constructions in Tongugbe, one of the many dialects of the Ewe language, which is spoken in south-eastern Ghana, along the Volta River.
It presents a detailed description of the constructions; and explores the relationship that exists between clausal possessive constructions and locative and existential constructions.
In addition to this, the work presents a first outline grammar of Tongugbe.
The grammar presents notably preliminary findings on the duration contrast in tones of Tongugbe and a rich demonstrative paradigm.
The possessive constructions can be grouped into attributive, predicative and external possessor constructions.
It is shown that the structural configurations of attributive possessive constructions are functionally motivated.
It is also demonstrated that structural variations in predicative possessive and external possessor constructions correspond to differences in meaning.
Finally, it is argued that, synchronically, clausal possessive constructions and locative and existential constructions are not reducible to a single structure.
The view supported here then is that each construction is a form-meaning pair.
This thesis proposes a new approach to natural language processing.
Rather than trying to estimate directly the probability distribution of a random sentence, we will detect syntactic structures in the language, which can be used to modify and create new sentences from an initial sample.
The study of syntactic structures will be done using Markov substitute sets, sets of strings that can be freely substituted in any sentence without affecting the whole distribution.
This point of view splits the issue of language analysis into two parts, a model selection stage where Markov substitute sets are selected, and a parameter estimation stage where the actual frequencies for each set are estimated.
We show that these substitute processes form exponential families of distributions, when the language structure (the Markov substitute sets) is fixed.
On the other hand, when the language structure is unknown, we propose methods to identify Markov substitute sets from a statistical sample, and to estimate the parameters of the distribution.
Markov substitute sets show some connections with context-Free grammars, that can be used to help the analysis.
We then proceed to build invariant dynamics for Markov substitute processes.
They can among other things be used to effectively compute the maximum likelihood estimate.
Indeed, Markov substitute models can be seen as the thermodynamical limit of the invariant measure of crossing-Over dynamics.
The purpose of this study was to consider the implementation of Problem Based Learning (PBL) as an epistemologically sound teaching methodology to teach English for Specific Purposes (ESP) and particularly English for Academic Medical Purposes (EAMP).
The study examined whether PBL is compatible with language teaching and determined the benefits that this methodology can bring to ESP.
The study also attempted to solve problems with English learning that were identified in the Preparatory Year Health Colleges (Female Branch) within Hail University, Saudi Arabia.
A needs analysis was conducted in the institution to examine the English learning situation and better identify these learning problems.
Then PBL was implemented to determine if it provided a possible solution to the issue.
This entailed a change in the macro-methodological and micro-methodological levels, as Demaizière (1996) called 'le niveau macromethodologique' and 'le niveau micromethodologique' (p.66).
In the empirical part of this study, a longitudinal study was conducted with 13 students who were observed through a period of 8 weeks and over five PBL tutorials, which took place over fifteen sessions.
During these fifteen sessions, learners' behaviors or indicators of autonomy were observed at the group level for the first and third session of each PBL tutorial and at the individual level in session 2.
In general, the results favored the implementation of this approach in teaching English for Academic Medical Purposes (EAMP).
To acquire their native language, babies have to learn both the forms of words (e.g., “dog” in English, “chien” in French) and their meanings (the category of dogs).
These two aspects of language learning have typically been studied independently.
However, recent findings from developmental psychology and machine learning have pointed out that this assumption is problematic, and have suggested that form and meaning may interact with one another throughout development.
This dissertation explores this hypothesis through an interdisciplinary investigation that combines tools from speech recognition and experimental psychology.
First, I developed a computational model of joint form and meaning acquisition, capable of learning from a corpus of natural speech.
Second, I tested the cognitive plausibility of this model with adult subjects.
This analysis is tackled by training deep neural networks, which have reached competitive results in many domains.
In this work, we focus on the understanding of daily life images, in particular on the interactions between objects and people that are visible in images, which we call visual relations.
This provides a fuzzy set of relations that more accurately represents visible relations.
Using the semantic similarities between relations, the model is able to learn to detect uncommon relations from similar and more common ones.
However, the improved training does not always translate to improved detections, because the objective function does not capture the whole relation detection process.
Thus during the inference phase, we combine knowledge to model predictions in order to predict more relevant relations, aiming to imitate the behaviour of human observers
Optimal Transport is a theory that allows to define geometrical notions of distance between probability distributions and to find correspondences, relationships, between sets of points.
Many machine learning applications are derived from this theory, at the frontier between mathematics and optimization.
How can it be adapted when the data are varied and not embedded in the same metric space?
This thesis proposes a set of Optimal Transport tools for these different cases.
More broadly, we analyze the mathematical properties of the various proposed tools, we establish algorithmic solutions to compute them and we study their applicability in numerous machine learning scenarii which cover, in particular, classification, simplification, partitioning of structured data, as well as heterogeneous domain adaptation.
Massive Open Online Courses (MOOCs) have seen their numbers increase significantly since the democratization of the Internet.
Dashboards summarizing students'activites are regularly offered to instructors, but they do not allow them to understand collective activities in the forums.
From a socio-constructivist point of view, the exchanges and interactions sought by instructors in forums are essential for learning (Stephens, 2014).
So far, studies have analyzed interactions in two ways: semantically but on a small scale or statistically and on a large scale but ignoring the quality of the interactions.
The scientific contribution of this thesis relates to the proposal of an interactive detection approach of collective activities which takes into account their temporal, semantic and social dimensions.
We seek to answer the problem of detecting and observing the collective dynamics that take place in MOOC forums.
By collective dynamic, we mean all the qualitative and quantitative interactions of learners in the forums and their temporal changes.
But, unlike previous studies, our approach is not limited to global or individual-centered analysis.
This approach has a drawback: it requires long-term investment.
It is then necessary to develop methods and computational tools to help the construction of such data that are required to be directly applicable to texts.
This work focuses on a specific linguistic representation: local grammars that describe precise and local constraints in the form of graphs.
Two issues arise: How to efficiently build precise, complete and text-applicable grammars?
How to deal with their growing number and their dispersion?
To handle the first problem, a set of simple and empirical methods have been exposed on the basis of M. Gross (1975)'s lexicon-grammar methodology.
The whole process of linguistic analysis and formal representation has been described through the examples of two original phenomena: expressions of measurement (un immeuble d'une hauteur de 20 mètres) and locative prepositional phrases containing geographical proper names (à l'île de la Réunion).
Each phenomenon has been narrowed to elementary sentences.
This enables semantically classify them according to formal criteria.
The syntactical behavior of these sentences has been systematically studied according to the lexical value of their elements.
Then, the observed properties have been encoded either directly in the form of graphs with an editor or in the form of syntactical matrices then semi-automatically converted into graphs according to E. Roche (1993).
These studies led to develop new conversion algorithms in the case of matrix systems where linguistic information is encoded in several matrices.
For the second issue, a prototype on-line library of local grammars have been designed and implemented.
The objective is to centralize and distribute local grammars constructed within the RELEX network of laboratories.
We developed a set of tools allowing users to both store new graphs and search for graphs according to different criteria.
This dissertation addresses the questions of discourse modeling within a grammatical framework called Abstract Categorial Grammars (ACGs).
ACGs provide a unified framework for both syntax and semantics.
We focus on the discourse formalisms that make use of a grammatical approach to capture the discourse structure regularities.
In particular, we propose ACG encodings of two discourse formalisms: G-TAG and D-STAG.
Both G-TAG and D-STAG make use of an extra-grammatical processing to deal with discourse connectives that appear at clause-medial positions.
In contrast, the ACG encodings of G-TAG and D-STAG offer a purely grammatical approach to clause-medial connectives.
Each of these ACG encodings are second-order.
Grammars of this class have reversibility properties that allow us to use the same polynomial algorithmes both for the discourse parsing and generation tasks.
A crucial issue in statistical natural language processing is the issue of sparsity, namely the fact that in a given learning corpus, most linguistic events have low occurrence frequencies, and that an infinite number of structures allowed by a language will not be observed in the corpus.
Neural models have already contributed to solving this issue by inferring continuous word representations.
These continuous representations allow to structure the lexicon by inducing semantic or syntactic similarity between words.
However, current neural models only partially solve the sparsity issue, due to the fact that they require a vectorial representation for every word in the lexicon, but are unable to infer sensible representations for unseen words.
This issue is especially present in morphologically rich languages, where word formation processes yield a proliferation of possible word forms, and little overlap between the lexicon observed during model training, and the lexicon encountered during its use.
Today, several languages are used on the Web besides English, and engineering translation systems that can handle morphologies that are very different from western European languages has become a major stake.
The goal of this thesis is to develop new statistical models that are able to infer in an unsupervised fashion the word formation processes underlying an observed lexicon, in order to produce morphological analyses of new unseen word forms.
Most statistical methods are not designed to directly work with incomplete data.
The study of data incompleteness is not new and strong methods have been established to handle it prior to a statistical analysis.
On the other hand, deep learning literature mainly works with unstructured data such as images, text or raw audio, but very few has been done on tabular data.
Hence, modern machine learning literature tackling data incompleteness on tabular data is scarce.
This thesis focuses on the use of machine learning models applied to incomplete tabular data, in an insurance context.
We propose through our contributions some ways to model complex phenomena in presence of incompleteness schemes, and show that our approaches outperform the state-of-the-art models
Tensor factorization has been increasingly used to analyze high-dimensional low-rank data of massive scale in numerous application domains, including recommender systems, graph analytics, health-care data analysis, signal processing, chemometrics, and many others.
In these applications, efficient computation of tensor decompositions is crucial to be able to handle such datasets of high volume.
The main focus of this thesis is on efficient decomposition of high dimensional sparse tensors, with hundreds of millions to billions of nonzero entries,which arise in many emerging big data applications.
We achieve this through three major approaches.
In the first approach, we provide distributed memory parallel algorithms with efficient point-to-point communication scheme for reducing the communication cost.
These algorithms are agnostic to the partitioning of tensor elements and low rank decomposition matrices, which allow us to investigate effective partitioning strategies for minimizing communication cost while establishing computational load balance.
We use hypergraph-based techniques to analyze computational and communication requirements in these algorithms, and employ hypergraph partitioning tools to find suitable partitions that provide much better scalability.
Second, we investigate effective shared memory parallelizations of these algorithms.
Here, we carefully determine unit computational tasks and their dependencies, and express them using a proper data structure that exposes the parallelism underneath.
Third, we introduce a tree-based computational scheme that carries out expensive operations(involving the multiplication of the tensor with a set of vectors or matrices, found at the core of these algorithms) faster by factoring out and storing common partial results and effectively re-using them.
With this computational scheme, we asymptotically reduce the number of tensor-vector and -matrix multiplications for high dimensional tensors, and thereby render computing tensor decompositions significantly cheaper both for sequential and parallel algorithms.
Finally, we diversify this main course of research with two extensions on similar themes.
The first extension involves applying the tree-based computational framework to computingdense tensor decompositions, with an in-depth analysis of computational complexity and methods to find optimal tree structures minimizing the computational cost.
The second work focuses on adapting effective communication and partitioning schemes of our parallel sparse tensor decomposition algorithms to the widely used non-negative matrix factorization problem,through which we obtain significantly better parallel scalability over the state of the art implementations.
We point out that all theoretical results in the thesis are nicely corroborated by parallel experiments on both shared-memory and distributed-memory platforms.
With these fast algorithms as well as their tuned implementations for modern HPC architectures, we render tensor and matrix decomposition algorithms amenable to use for analyzing massive scale datasets.
This PhD thesis deals with the pragmatic functions of 'you know', 'then' and 'ya'nĩ' (I mean) as autonomous syntactic structures and as operational and multifunctional linguistic units in conversation.
Within a pragmatic framework, the research discusses the correlation between position and function, in which the pragmatic value of a marker in initial position is different from that conveyed in medial or final positions.
The goal of this study is to contribute to the pragmatic analysis of conversations by analyzing political verbal interactions in Arabic and English television broadcasts.
The results of this empirical study show that the multi-functionality of these linguistic items is related to their syntactic flexibility.
These discourse markers imply a variety of contextual functions which evolve gradually on a pragmatic scale.
The pragmatic evolution of 'you know', 'then' and 'ya'nĩ' generate complex semantic expressions.
These pragmatic units in fact become increasingly complex; they go beyond their basic meaning to acquire progressively contextual implications.
Thus, these markers refer to other interpretations and transcend their immediate semantic base.
In this respect, they can be substituted according to the context of their occurrence.
The results reveal that 'ya'nĩ' can be substituted with other markers from different grammatical categories in English.
Pragmatic variation depends on the illocutionary perspective of the speaker, the relation with the hearer and the organization of the verbal interaction.
In two distinct sociocultural situations, English and Arabic, it is confirmed that these linguistic items are contextual and multifunctional conversational units.
Their role is relevant in a social situation where the interaction between the speaker and the hearer is a salient factor, as in the case of political verbal exchanges in television broadcasts.
Our doctoral research focuses on the influence of phonics instruction on first-grade students' progress.
Its purpose is to identify effective teaching practices and to contribute to the training of teachers.
This research is part of a larger study conducted by Roland Goigoux, which aimed to assess the influence of reading and writing on the quality of learning.
The first part of our research examines causal relationships between the characteristics of phonics instruction and students' performances in decoding and spelling.
First, we study the influence of the speed of teaching of grapheme-phoneme relationships (tempo) and of the decodable part of texts used to teach reading (rendement effectif).
Our results reveal a significant influence of these two variables on the quality of learning, this influence being different according to students' initial levels.
Besides, we propose a planning of the phonics instruction based on the theoretical frequency of the grapheme-phoneme correspondences in texts written in standard French which can serve as references for the teachers.
We also study the effects of the teaching time allocated to encoding tasks on reading achievement, effects which appear to be significant and positive but which vary according to the nature of the tasks and to students' characteristics.
In the second part of our dissertation, we attempt to analyze and document teaching practices of experienced first-grade teachers for training purposes.
We analyze a reference situation of the teaching of reading and writing from the video recordings of thirty six collective sessions of reading.
Then, we describe prototypical teaching scenarios and lay the foundations for a training intended to develop the professional skills of the teachers.
Specifically, we raise the issue of the relationship between the resolution of decoding and understanding tasks and the autonomy that decoding success afforded the students.
We finally present the digital platform we designed, which allows calculating the decodable part of texts used during reading instruction.
This platform named Anagraph has been designed to help teachers plan the study of the grapheme-phoneme correspondences and to choose texts adapted to their teaching
Structured learning has become ubiquitous in Natural Language Processing; a multitude of applications, such as personal assistants, machine translation and speech recognition, to name just a few, rely on such techniques.
Imitation learning aims to perform approximate learning and inference in order to better exploit richer dependency structures.
In this thesis, we explore the use of this specific learning setting, in particular using the SEARN algorithm, both from a theoretical perspective and in terms of the practical applications to Natural Language Processing tasks, especially to complex tasks such as machine translation.
Concerning the theoretical aspects, we introduce a unified framework for different imitation learning algorithm families, allowing us to review and simplify the convergence properties of the algorithms.
With regards to the more practical application of our work, we use imitation learning first to experiment with free order sequence labelling and secondly to explore two step decoding strategies for machine translation.
The digital revolution has evolved the act of writing; its forms have changed.
The knowledge gathered by this research introduces a new angle for setting up a new writing system.
For that reason, sign languages offer the perfect opportunity for this kind of research.
In this study we try to understand the link between gestures and meaning for the speaker and discover what features and how much of signing (gestures, body language) can be kept in the act of writing.
Our objective is to maintain the integral meaning of gestures for the signer/writer.
First, this multidisciplinary research focuses on what can be transferred from the former gestural act (signing) to the latter (writing).
Then, we consider the tool that will enable this transfer.
To do so, we follow a phenomenological approach, or in other terms, a descriptive methodology from the firstperson point of view.
This methodology is built upon signers' feedback gathered of the experience lived during interviews.
Shaping this method to fit the French SL offers precise gestural descriptions from signers themselves.
This database is then compared with alinguistic and kinesiological analysis from the third-person point of view.
These gestural meaning results enable us to reflect on how to create a guided experience tool enabling the assimilation of SL's gestural matter and the creation of scriptural forms.
To do that, we follow a UX design, an enaction design, and a tool based approach in order to offer immersion and interaction.
This kind of device offers a new perspective to signers on their own language and more generally, offers the possibility for any user to form a new relationship with her or his own gestures.
The Frank-Wolfe algorithms, a.k.a. conditional gradient algorithms, solve constrained optimization problems.
They break down a non-linear problem into a series of linear minimization on the constraint set.
This contributes to their recent revival in many applied domains, in particular those involving large-scale optimization problems.
In this dissertation, we design or analyze versions of the Frank-Wolfe algorithms.
We notably show that, contrary to other types of algorithms, this family is adaptive to a broad spectrum of structural assumptions, without the need to know and specify the parameters controlling these hypotheses.
Two independent subjects are studied in this thesis, the first of which consists of two distinct problems.
In the first part, we begin with the Principal-Agent problem in degenerate systems, which appear naturally in partially observed random environment in which the Agent and the Principal can only observe one part of the system.
Our approach is based on the stochastic maximum principle, the goal of which is to extend the existing results using dynamic programming principle to the degenerate case.
Afterward, we use the sufficient condition of the Agent's problem to verify that the previously obtained optimal contract is indeed implementable.
Meanwhile, a parallel study is devoted to the wellposedness of path-dependent FBSDEs in the chapter IV.
Finally, we study the Principal-Agent problem with multiple Principals.
The Agent can only work for one Principal at a time and therefore needs to solve an optimal switching problem.
By using randomization, we show that the value function of the Agent's problem and his optimal control are given by an Itô process.
This representation allows us to solve the Principal's problem in the mean-field case when there is an infinite number of Principals.
We justify the mean-field formulation using an argument of backward propagation of chaos.
The second part of the thesis consists of chapter V and VI.
The motivation of this work is to give a rigorous theoretical underpinning for the convergence of gradient-descent type of algorithms frequently used in non-convex optimization problems like calibrating a deep neural network.
We show that the corresponding energy function has a unique minimiser which can be characterized by some first order condition using derivatives in measure space.
We present a probabilistic analysis of the long-time behavior of the mean-field Langevin dynamics, which have a gradient flow structure in 2-Wasserstein metric.
By using a generalization of LaSalle's invariance principle, we show that the flow of marginal laws induced by the mean-field Langevin dynamics converges to the stationary distribution, which is exactly the minimiser of the energy function.
As for deep neural networks, we model them as some continuous-time optimal control problems.
Firstly, we find the first order condition by using Pontryagin maximum principle, which later helps us find the associated mean-field Langevin system, the invariant measure of which is again the minimiser of the optimal control problem.
As last, by using the reflection coupling, we show that the marginal distribution of the mean-field Langevin system converges to the unique invariant measure exponentially.
Research has shown that non-literalness is pervasive in language and that it is not always an ornamental device (e.g. to invest time in something, to be in love, the leg of a table, etc.).
As an attempt to bridge this gap, I propose a comparative study of figurative language development in first and second language acquisition.
Then, I propose an L1/L2 comparative study where I analyze semi-guided interactions taking place between native English-speaking children (aged 7, 11 and 15), French learners of English (in their first year of high school, first year of B.A. in English studies and last year of M.A. in English studies), as well as native English-speaking adults.
The results of this PhD project revealed many similar aspects in the figurative productions of native English-speaking children and French students.
Lastly, I observed a large amount of deviant figurative forms in the leaner's productions, mainly resulting from L1 transfers and lexical overextensions.
Taking into account these observations, implications for teaching are presented.
Real-time embedded systems are increasingly omnipresent in everyday life.
The development cycle of critical systems can take months or even years.
Therefore, modeling of these systems should be analyzed at an early stage in the development cycle to verify that all requirements are met, including temporal requirements (e.g., latencies, end-to-end delays).
This thesis, which was funded as part of FUI project, offers three major contributions.
The first contribution relates to mono-processor task system with deterministic multi-periodic communication relationships.
A pattern based on Semaphore Precedence Constraint (SPC) has been extended to support cycles in the case of dynamic priority scheduling.
An unfolding graph has also been proposed in order to present the cyclicity of SPC-based systems and guarantee deadlock free.
The extended SPC pattern and the corresponding scheduling analysis tests have been implemented for the standard AADL language.
The second contribution of this thesis consists in proposing an exact calculation of end-to-end response time using the time Petri net formalism for identical multiprocessor systems.
It takes into account the dependence between the tasks: precedence and mutual exclusion.
The third contribution concerns the capitalization of the efforts of the analysis process.
Indeed, many analytical tests have been proposed, notably by academic researchers, based on scheduling theory and dedicated to the different software and hardware architectures.
However, one of the main difficulties encountered by designers is to choose the most appropriate analysis test to validate and/or correctly dimension their designs.
In industrial environment, there are few analytical tests used despite the multitude of the tests offered.
This thesis therefore aims to facilitate the analysis process, reduce the semantic gap between the business model and the entries in the analysis tests as well as accelerate technology transfer and the adoption of academic tests.
The thesis proposes an analysis repository playing the role of a dictionary of tests, their contexts for correct use, the tools implementing them, as well as a mechanism for choosing tests according to the input business model.
In a user-centered design process, artifacts evolve in iterative cycles until they meet user requirements and then become the final product.
Every cycle gives the opportunity to revise the design and to introduce new requirements which might affect the artifacts that have been set in former development phases.
Keeping the consistency of requirements in such artifacts along the development process is a cumbersome and time-consuming activity, especially if it is done manually.
Nowadays, some software development frameworks implement Behavior-Driven Development (BDD) and User Stories as a means of automating the test of interactive systems under construction.
Automated testing helps to simulate user's actions on the user interface and therefore check if the system behaves properly and in accordance with the user requirements.
However, current tools supporting BDD requires that tests should be written using low-level events and components that only exist when the system is already implemented.
As a consequence of such low-level of abstraction, BDD tests can hardly be reused with more abstract artifacts.
In order to prevent that tests should be written to every type of artifact, we have investigated the use of ontologies for specifying both requirements and tests once, and then run tests on all artifacts sharing the ontological concepts.
The resultant behavior-based ontology we propose herein is therefore aimed at raising the abstraction level while supporting test automation on multiple artifacts.
This thesis presents this ontology and an approach based on BDD and User Stories to support the specification and the automated assessment of user requirements on software artifacts along the development process of interactive systems.
Two case studies are also presented to validate our approach.
The first case study evaluates the understandability of User Stories specifications by a team of Product Owners (POs) from the department in charge of business trips in our institute.
With the help of this first case study, we designed a second one to demonstrate how User Stories written using our ontology can be used to assess functional requirements expressed in different artifacts, such as task models, user interface (UI) prototypes, and full-fledged UIs.
The results have shown that our approach is able to identify even fine-grained inconsistencies in the mentioned artifacts, allowing establishing a reliable compatibility among different user interface design artifacts.
In the rise of the internet, user-generated content from social networking services is becoming a giant source of information that can be useful to businesses on the aspect where users are viewed as customers or potential customers for companies.
Exploitation of user-generated texts can help identify their feelings, intentions, or reduce the effort of the agents who are responsible for collecting or receiving information on social networking services.
As part of this thesis, the content of texts such as speeches, statements, conversations from interactive communication on social media platforms become the main data object of our study.
We propose a method for extracting a CCG tree from the dependency structure of the sentence, and a general architecture to build a bridge of relationship between syntaxes and semantics of French sentences.
This dissertation explores how embedding small data-driven contextual visualizations can complement text documents.
More specifically, I identify and define important aspects and relevant research directions for the integration of small data-driven contextual visualizations into text.
This integration should eventually become as fluid as writing and as usable as reading a text.
I define word-scale visualisations as small data-driven contextual visualizations embedded in text documents.
These visualizations can use various visual encodings including geographical maps, heat maps, pie charts, and more complex visualizations.
They can appear at a range of word scales, including sizes larger than a letter, but smaller than a sentence or paragraph.
Word-scale visualisations can help support and be used in many forms of written discourse such as text books, notes, blog posts, reports, stories, or poems.
As graphical supplements to text, word-scale visualisations can be used to emphasize certain elements of a document (e.g. a word or a sentence), or to provide additional information.
For example, a small stock chart can be embedded next to the name of a company to provide additional information about the past trends of its stocks.
In another example, game statistics can be embedded next to the names of soccer teams or players in daily reports from the UEFA European Championship.
These word-scale visualisations can then for example allow readers to make comparison between number of passes of teams and players.
The main benefit of word-scale visualisations is that the reader can remain focused on the text as the visualization are within the text rather than alongside it.
I investigate placement options to embed word-scale visualisations and quantify their effects on the layout and flow of the text.
I also explore how word-scale visualisations can be combined with interaction to support a more active reading by proposing interaction methods to collect, arrange and compare word-scale visualisations.
Finally, I propose design considerations for the authoring of word-scale visualisations and conclude with application examples.
In summary, this dissertation contributes to the understanding of small data-driven contextual visualizations embedded into text and their value for Information Visualization.
Geometric data analysis, beyond convolutionsTo model interactions between points, a simple option is to rely on weighted sums known as convolutions.
Over the last decade, this operation has become a building block for deep learning architectures with an impact on many applied fields.
We should not forget, however, that the convolution product is far from being the be-all and end-all of computational mathematics.
To let researchers explore new directions, we present robust, efficient and principled implementations of three underrated operations: 1.
Optimal transport, which generalizes sorting to spaces of dimension D &gt; 1.3.
Hamiltonian geodesic shooting, which replaces linear interpolation when no relevant algebraic structure can be defined on a metric space of features.
Our PyTorch/NumPy routines fully support automatic differentiation and scale up to millions of samples in seconds.
They generally outperform baseline GPU implementations with x10 to x1,000 speed-ups and keep linear instead of quadratic memory footprints.
These new tools are packaged in the KeOps (kernel methods) and GeomLoss (optimal transport) libraries, with applications that range from machine learning to medical imaging.
Documentation is available at: www.kernel-operations.io/keops and /geomloss.
While our representation of the world is shaped by our perceptions, our languages, and our interactions, they have traditionally been distinct fields of study in machine learning.
Fortunately, this partitioning started opening up with the recent advents of deep learning methods, which standardized raw feature extraction across communities.
However, multimodal neural architectures are still at their beginning, and deep reinforcement learning is often limited to constrained environments.
Yet, we ideally aim to develop large-scale multimodal and interactive models towards correctly apprehending the complexity of the world.
As a first milestone, this thesis focuses on visually grounded language learning for three reasons (i) they are both well-studied modalities across different scientific fields (ii) it builds upon deep learning breakthroughs in natural language processing and computer vision (ii) the interplay between language and vision has been acknowledged in cognitive science.
More precisely, we first designed the GuessWhat?! game for assessing visually grounded language understanding of the models: two players collaborate to locate a hidden object in an image by asking a sequence of questions.
Finally, we investigate how reinforcement learning can support visually grounded language learning and cement the underlying multimodal representation.
We show that such interactive learning leads to consistent language strategies but gives raise to new research issues.
This thesis defines a semantic model, non probabilist and predictive, for the decisional analysis of professional and institutional social networks.
The presented multidisciplinary model, in parallel to the Galam sociophysics, integrates some semantic methods of natural language processing and knowledge engineering, some measures of statistic sociology and some electrodynamic laws, applied to the economic performance and social climate optimisation.
It has been developped and experimented in line with the Socioprise project, funded by the French State Secretariat for the prospective and development of the digital economy.
Multiple Sclerosis (MS) is the most common progressive neurological disease of young adults worldwide and thus represents a major public health issue with about 90,000 patients in France and more than 500,000 people affected with MS in Europe.
In order to optimize treatments, it is essential to be able to measure and track brain alterations in MS patients.
In fact, MS is a multi-faceted disease which involves different types of alterations, such as myelin damage and repair.
Under this observation, multimodal neuroimaging are needed to fully characterize the disease.
Magnetic resonance imaging (MRI) has emerged as a fundamental imaging biomarker for multiple sclerosis because of its high sensitivity to reveal macroscopic tissue abnormalities in patients with MS.
Conventional MR scanning provides a direct way to detect MS lesions and their changes, and plays a dominant role in the diagnostic criteria of MS.
Moreover, positron emission tomography (PET) imaging, an alternative imaging modality, can provide functional information and detect target tissue changes at the cellular and molecular level by using various radiotracers.
However, in clinical settings, not all the modalities are available because of various reasons.
In this thesis, we therefore focus on learning and predicting missing-modality-derived brain alterations in MS from multimodal neuroimaging data.
This dissertation explores how misogyny may target language uses which may be perceived as feminine and centers on the "Valley Girl" stereotype.
This term was popularized in the 1980s by Frank Zappa's eponymous single and originally referred to supposedly vain and unintelligent female teenagers who belonged to the Californian middle class.
Though Valley Girls were ridiculed in the song, the impact it had launched a craze that manifested linguistically in Valspeak.
This dialect comprises markers which are mainly phonetic (the California Vowel Shift), prosodic (the High Rising Terminal contour), lexical ("fer sure," "gag me with a spoon"), or that can be found at the discourse level (LIKE).
Though some of these markers were not (solely) popularized by Valley Girls, they may nevertheless be perceived as such, and a speaker using them may trigger negative social evaluations.
This research explores how the potential stigmatizing perception of Valspeak may be linked to misogyny, which is a phenomenon we refer to as the "linguistic misogyny" of Valspeak.
To what extent may linguistic stigma be induced by the gender of the prototypical speakers of this dialect?
Three main analyses are provided.
First, a quantitative perceptual dialectology study of three Valspeak markers (the California Vowel Shift, the High Rising Terminal contour, and LIKE) is conducted with native American English speakers.
Then, qualitative interviews are carried out in order to determine what ideologies are associated with Valspeak markers and the Valley Girl persona.
The third part of the analysis focuses on three humorous representations of female characters in television programs: Parks and Recreation, Family Guy, and Ew! (a segment on The Tonight Show Starring Jimmy Fallon).
It is suggested that Valspeak markers may be recruited in order to portray intellectually-challenged female characters without explicitly referring to the Valley Girl stereotype.
With the rapid growth of digital video content, automatic video understanding has become an increasingly important task.
Video understanding spans several applications such as web-video content analysis, autonomous vehicles, human-machine interfaces (eg, Kinect).
This thesis makes contributions addressing two major problems in video understanding: webly-supervised action detection and human action localization.
Webly-supervised action recognition aims to learn actions from video content on the internet, with no additional supervision.
We propose a novel approach in this context, which leverages the synergy between visual video data and the associated textual metadata, to learn event classifiers with no manual annotations.
We show the importance of both the main steps of our method, ie,query generation and data pruning, with quantitative results.
We evaluate this approach in the challenging setting where no manually annotated training set is available, i.e., EK0 in the TrecVid challenge, and show state-of-the-art results on MED 2011 and 2013 datasets.
In the second part of the thesis, we focus on human action localization, which involves recognizing actions that occur in a video, such as "drinking" or "phoning", as well as their spatial and temporal extent.
We propose a new person-centric framework for action localization that tracks people in videos and extracts full-body human tubes, i.e., spatio-temporal regions localizing actions, even in the case of occlusions or truncations.
The motivation is two-fold.
First, it allows us to handle occlusions and camera viewpoint changes when localizing people, as it infers full-body localization.
Our tracking algorithm connects the image detections temporally to extract full-body human tubes.
We evaluate our new tube extraction method on a recent challenging dataset, DALY, showing state-of-the-art results.
This study presents the characteristics, writing systems and structure of Uyghur language by doing a linguistic study. Our approach will consist of new trial models that facilitate the development and realization of Uyghur software tools, and contribute to the Uyghur information technology.
More precisely, our study consists of four phases:
Firstly, we are going to present the main issues of the study, characteristics of the language and its writing systems, especially the unification procedure of the Latin-Script Uyghur.
Secondly, we briefly introduce some basic notions for the retrieval of information, and we will do a demonstration of named entities retrieval, using an extraction tool, in order to test concepts and theories that we are proposing.
Then, we will discuss linguistic issues – mainly on the agglutinative aspect and morphological suffixation rules – which are applied during the implementation of prototype tools proposed in this study.
Finally, we underline problems in natural language processing (NLP) created by Uyghur language and non-Uyghur supporting environments.
We will discuss the existing difficulties and we will suggest innovative solutions to resolve such problems with the following fields: Standardization of Uyghur fonts and creation of a Unicode based Uyghur font, Implementation of system-level and browser-level input methods and-reation of multi-script converting tools, Realization of an online Uyghur – English dictionary, Implementation of a lexical generator based on the morphological suffixation rules of Uyghur, Design and creation of an suffix analyzer and explorer, Demonstration of Uyghur information retrieval, Implementation of a parser and spell checker
The benefit of performing Big data computations over individual's microdata is manifold, in the medical, energy or transportation fields to cite only a few, and this interest is growing with the emergence of smart-disclosure initiatives around the world.
To regain indivuals' trust, it becomes essential to propose user empowerment solutions, that is to say allowing individuals to control the privacy parameter used to make computations over their microdata.
This work proposes a novel concept of personalized anonymisation based on data generalization and user empowerment.
Moreover, we propose a decentralized computing infrastructure based on secure hardware enforcing these personalized privacy guarantees all along the query execution process.
Secondly, this manuscript studies the personalization of anonymity guarantees when publishing data.
We propose the adaptation of existing heuristics and a new approach based on constraint programming.
Experiments have been done to show the impact of such personalization on the data quality.
Individuals' privacy constraints have been built and realistically using social statistic studies
This thesis offers a multidimensional (sociolinguistic, phonetic, and phonological) description and study of the variety of English spoken in Greater Manchester.
We discuss the study of linguistic change and the use of corpora in linguistics from a methodological and epistemological point of view.
Our work is conducted in the framework of the PAC programme (Phonology of Contemporary English: usage, varieties and structure) and within the LVTI project (Language, Urban Life, Work, Identity), and based on the PAC-LVTI Manchester corpus, which is composed of authentic and recent fieldwork data.
Our analysis notably focuses on the phenomenon of regional dialect levelling, which has been largely documented in recent English sociolinguistic research.
In particular, we are interested in the hypothesis of the expansion of a supralocal variety in the north of England.
Our study deals mainly with the vowels of Greater Manchester English, and relies on a phonetic-acoustic analysis of our informants' realisations.
We describe the major characteristics of the Mancunian variety based on the few studies published so far, and statistically evaluate their correlation with traditional sociolinguistic factors such as age, gender or socio-economic profile.
We also explore the relevance of attitudinal factors for the study of our data.
We discuss the evolutions of the system in the light of regional dialect levelling, and question the role played by internal and external factors in these linguistic changes.
This PhD thesis is the result of my research work in the machine learning, image processing and intelligent transportation field for solving the problem of multi-task pedestrian protection system (PPS) including not only pedestrian classification, detection and tracking, but also pedestrian action-unit classification and prediction, and finally pedestrian risk estimation.
Moreover, our PPS system uses original cross-modality deep learning approaches.
The goal of our research work is to develop an intelligent pedestrian protection component-based only on single stereo vision system using an optimal cross-modality deep learning architecture in order to classify the current pedestrian action, predict their next actions and finally to estimate the pedestrian risk by the time to cross for each pedestrian.
First, we investigate the classification component where we analyzed how learning representations from one modality would enable recognition for other modalities within various deep learning, which one term as cross-modality learning.
Second, we study how the cross-modality learning improves an end-to-end the pedestrian action.
Based on a Publish/Subscribe paradigm, Web Syndication formats such as RSS have emerged as a popular means for timely delivery of frequently updated Web content.
According to these formats, information publishers provide brief summaries of the content they deliver on the Web, while information consumers subscribe to a number of RSS feeds and get informed about newly published items.
We propose a keyword-based index for user subscriptions to match it on the fly with incoming items.
We study three indexing techniques for user subscriptions.
We present analytical models to estimate memory requirements and matching time.
We also conduct a thorough experimental evaluation to exhibit the impact of critical workload parameters on these structures.
For subscriptions which are never notified, we adapt the indexes to support a partial matching between subscriptions and items.
The rise of digital media technology over the last decades has transformed the way in which organizations are evaluated.
Every day, on a plurality of platforms and websites, individuals disclose information about their interactions with organizations and their products or services.
Compared to traditional media or professional critics, digital users and customers tend to share subjective and partial experiences, have lower concerns for accuracy and balance, and often put emphasis on the emotional content.
As more customers rely on this information for their purchasing choices, firms in many industries find themselves in a position where it is hard to ignore the opinions expressed online by customers as inconsequential.
In this thesis, I study how the strategies and behaviors of organizations are affected by this “democratization” of evaluation process.
The empirical setting for my analyses is the fine-dining industry.
In the first chapter, I study online reviews as a source of information for restaurants, which may learn about problems, errors, or improvement opportunities.
I examine what features of customer feedback make it more likely to be considered by target restaurants.
With an online experiment in the French restaurant industry, I find that decision makers allocate attention to feedback that is expected to have a stronger impact on the reputation and performance of the restaurant.
In the second chapter, I analyze the effects of the interaction between amateur and expert evaluations.
In particular, I study the entry of an expert evaluator (i.e., Michelin guide) in a market, and how it pushes some organizations to make strategic choices that signal their aspirations.
Drawing on literature on organizational status, I find that restaurants better rated by Michelin make changes to their offer with the aim to self-identify with the élite group.
These changes consist in the adoption or removal of certain features displayed in their menus.
In addition, by using topic modeling techniques applied to Yelp reviews, I observe that customers' reactions to the entry of Michelin make restaurants more or less sensitive to the expert's evaluations.
In the third chapter, I focus on how organizations use public responses to customers to address criticism in online settings.
Recent studies are not conclusive on the reputational benefits of public responses to reviews.
These responses may reduce the likelihood of future negative reviews while, at the same time, draw attention to problems.
Building on existing literature on reputation and impression management, I propose that organizations may resolve this trade-off by making a strategic use of different types of verbal accounts (e.g., apology).
Although public responses to customers may be counterproductive, adapting the style of public responses to the features of customer reviews might be an optimal strategy for organizations.
For this study I analyze restaurant reviews in France and the United States using standard econometric models supported by supervised learning techniques.
This case study focuses on the integration of digital tools to the teaching and learning of English as a foreign language.
It aims to describe and analyse the different processes that contribute to the emergence of the social dimension of autonomy.
It is based on the idea that several processes come to the fore during the realization of a situated and mediated collective activity accomplished by small groups of learners.
In order to study these processes, a digital storytelling task-project was proposed to groups of learners enrolled in the first year of university of a pre-service English language-training course.
The materials designed by the learners were then tested as pedagogical resources by teachers of seven primary schools in France.
A systemic modelling of the above activity makes it possible to identify three focus areas that contribute to the emergence of social autonomy.
The first relates to the intra group operations of the small groups who accomplished the task-project.
The second is the contribution of the community of primary school teachers who tested the teaching materials.
The third constitutes the ways in which social digital tools were used by small groups to collaborate and work together.
The results from the analyses related to the three focus areas allow for a modelling of certain variables that contribute to the understanding of the emergence of social autonomy.
Bilingual corpora are an essential resource used to cross the language barrier in multilingual Natural Language Processing (NLP) tasks.
Most of the current work makes use of parallel corpora that are mainly available for major languages and constrained areas.
Comparable corpora, text collections comprised of documents covering overlapping information, are however less expensive to obtain in high volume.
Previous work has shown that using comparable corpora is beneficent for several NLP tasks.
Apart from those studies, we will try in this thesis to improve the quality of comparable corpora so as to improve the performance of applications exploiting them.
The idea is advantageous since it can work with any existing method making use of comparable corpora.
We first discuss in the thesis the notion of comparability inspired from the usage experience of bilingual corpora.
The notion motivates several implementations of the comparability measure under the probabilistic framework, as well as a methodology to evaluate the ability of comparability measures to capture gold-standard comparability levels.
The comparability measures are also examined in terms of robustness to dictionary changes.
The experiments show that a symmetric measure relying on vocabulary overlapping can correlate very well with gold-standard comparability levels and is robust to dictionary changes.
Based on the comparability measure, two methods, namely the greedy approach and the clustering approach, are then developed to improve the quality of any given comparable corpus.
The general idea of these two methods is to choose the high quality subpart from the original corpus and to enrich the low-quality subpart with external resources.
The experiments show that one can improve the quality, in terms of comparability scores, of the given comparable corpus by these two methods, with the clustering approach being more efficient than the greedy approach.
The enhanced comparable corpus further results in better bilingual lexicons extracted with the standard extraction algorithm.
Lastly, we investigate the task of Cross-Language Information Retrieval (CLIR) and the application of comparable corpora in CLIR.
We develop novel CLIR models extending the recently proposed information-based models in monolingual IR.
The information-based CLIR model is shown to give the best performance overall.
Bilingual lexicons extracted from comparable corpora are then combined with the existing bilingual dictionary and used in CLIR experiments, which results in significant improvement of the CLIR system.
Our main goal was to define a theoretical and computational framework allowing formal modeling and automatic exploration of various discursive structures involved in this textual organization.
We notably propose to describe those structures using the three elementary categories of units, relations and schemas, and outline recurrent properties of discursive patterns and clues which signal their presence: variable granularity, fuzziness, possible non-linearity and non-sequentiality, local/global interactions...
In order to give a formal description of the studied linguistic phenomena and to make their computational analysis possible, in a corpus-based approach, we propose the CDML formalism (Contraint-based Discourse Modeling Language).
A CDML parser has been implemented and may be used to apply such a formal description to a corpus and automatically detect textual structures satisfying the given constraints.
The second considers contrastive relations between various kind of textual objects, at different granularity levels.
The present dissertation is concerned with the use and interpretation of null and pronominal subjects in Brazilian Portuguese.
This investigation examines these phenomena in an attempt to disentangle the semantic and discursive factors that can be relevant for choice between these anaphoric expressions in Brazilian Portuguese and the way in which this choice is articulated with the general theory of anaphora resolution.
The starting point of this dissertation was the research looking into null and overt subjects from the perspective of Generative Grammar, specially the Parametric Theory.
Throughout the present work, however, the analyses proposed in this perspective were shown not to account for the data at stake.
The generalization that poor verbal morphology is directly related to the absence or reduced frequency of null subjects, for example, is challenged through experimental data and an investigation of the relative frequency of null subjects across discourse persons in corpora.
An alternative explanation presented in the previous literature, namely the importance of the antecedents' features of Animacy and Specificity, seems to better account for the attested distribution.
However, this explanation is not sufficient for understanding the choice between null and overt subjects in Brazilian Portuguese, since the number of animate and specific null subjects is still relatively higher than in languages with obligatory expression of subjects.
Therefore, it is argued that discourse factors seem to play a crucial role in the use of null and overt subjects in Brazilian Portuguese.
The first is a standard feature in the literature about anaphora resolution (expressed by a variety of terms, such as Salience, Familiarity, Accessibility, etc.), which is part of the reverse mapping hypothesis according to which the more obvious the subject is, the less explicit the co-referential form is allowed to be.
The second factor, Contrast, is the main finding of the present dissertation: as is the case for other levels of linguistic analyses and other phenomena in language, the choice of anaphoric expression in Brazilian Portuguese seems to be driven by efficiency.
In the present case, this means that, when the backgrounded information and the asserted (focused) information in an utterance contrast the most, it is more likely that a null subject will be used.
The design of a grammar that deals with these multiple features is sketched, specifically, a multi-layered scalar probabilistic grammar is proposed, whose semantic and discourse constraints act in parallel through a probabilistic mapping.
It is, thus, shown that null subjects are likely in discursive co-reference, since in these contexts their antecedents are more obvious and the focused information contrasts the most with the background.
An apparent counter-example to the proposal sketched here is analyzed: the generic interpretation of null subjects.
However, it is shown that the same semantic constraints cross-linguistically applied to other generic constructions can produce generic null subjects in Brazilian Portuguese, given the failure to be grounded predicted by the approach proposed here.
Finally, on-line evidence for the analysis of the use and interpretation of null and pronominal subjects is provided.
The results found in three eye-tracking while reading experiments provide striking evidence in favor of the proposal put forward here, according to which null and overt subjects and their interpretation can be accounted for in terms of constraints on interpretation rather than licensing.
Lexical frozeness is one of the main obstacles to automatic processing of natural language.
The present work intends to be a contribution to improve automatic processing applied to fossilized verbal sequences (SVF) such as casser sa pipe.
Notions of degrees of lexical frozeness and double structuration guided our study.
In order to determinate the degree of lexical frozeness of SVF, we analysed their internal structure and their external structuration.
Consequently, they can be included in predicates classes developped in L. L. I., based on G. GROSS theory of object classes.
After a survey of publications about (verbal) lexical frozeness, we identify and collect criteria to measure SVF degrees of lexical frozeness through the general structure analysis of [V SN SP] sequences.
Then, we propose formal tools for automatic recognition of metaphor through analysis of SVF coming from sports.
Lastly, we described two syntactico-semantic classes of predicats: <deplacement>; and &lt;états humains&gt;.
The thesis begins with a short historical reminder of ethnomethodology, considered as a scientific field, since the whole beginners during the 30's until the 1967 explosion in US and Europe.
The second part of the thesis is devoted to the concrete application of these theoretical concepts in the field of technological strategies which have been elaborated in France in the area of natural language processing.
Three studies successively describe the ethnomethods and rational properties of practical activities which are used in an administrative team, the elaboration of a technology policy and indexical descriptions of the language industry field.
The conclusion tries to show how the concepts and methods developped by ethnomethodology can increase, in this field, the efficacy of strategical analysis and the quality of research and development programs
With the advent and increasing popularity of Computer Supported Collaborative Learning (CSCL) and e-learning technologies, the need of automatic assessment and of teacher/tutor support for the two tightly intertwined activities of comprehension of reading materials and of collaboration among peers has grown significantly.
Whereas a shallow or surface analysis is easily achievable, a deeper understanding of the discourse is required, extended by meta-cognitive information available from multiple sources as self-explanations.
As specificity of the analysis, in terms of individual learning we have focused on the identification of reading strategies and on providing a multi-dimensional textual complexity model integrating surface, word specific, morphology, syntax and semantic factors.
Complementarily, the collaborative learning dimension is centered on the evaluation of participants' involvement, as well as on collaboration assessment through the use of two computational models: a polyphonic model, defined in terms of voice inter-animation, and a specific social knowledge-building model, derived from the specially designed cohesion graph corroborated with a proposed utterance scoring mechanism.
Our approach integrates advanced Natural Language Processing techniques and is focused on providing a qualitative estimation of the learning process.
Various cognitive validations for all our automated evaluation systems have been conducted and scenarios including the use of ReaderBench, our most advanced system, in different educational contexts have been built.
One of the most important goals of our model is to enhance understanding as a “mediator of learning” by providing automated feedback to both learners and teachers or tutors.
The main benefits are its flexibility, extensibility and nevertheless specificity for covering multiple stages, starting from reading classroom materials, to discussing on specific topics in a collaborative manner, and finishing the feedback loop by verbalizing metacognitive thoughts in order to obtain a clear perspective over one's comprehension level and appropriate feedback about the collaborative learning processes.
The contribution of this PhD belongs to the domain of Natural Language Processing, Information Extraction, Information Retrieval and Cognitive Science in a context of Geomatics.
In this context, our contribution aims at automatically processing texts telling travel stories to interpret the geographical information they contain and more particularly the itineraries they describe.
The approach we propose is an incremental method to discover the semantics of the document: starting from semantics of a simple nominal syntagm level - through proposition level semantics-
Our contribution lies in the proposition of a task driven conceptual model (for IR and the conception of educational activities), the proposition of an approach to characterise the itinerary in the text and the proposition of an operating process to link the characterization of the itinerary in the text with its conceptual model.
Our approach is clearly an experimental one, that is why we rely our works on a prototype that we developed.
This prototype (PIIR for Prototype pour l'Interprétation d'Itinéraires dans des Récits) allows us to confront our analyses and algorithms with automatic aimed treatment.
Since the 90s, Internet is at the heart of the labor market.
First mobilized on specific expertise, its use spreads as increase the number of Internet users in the population.
Seeking employment through "electronic employment bursary" has become a banality and e-recruitment something current.
This information explosion poses various problems in their treatment with the large amount of information difficult to manage quickly and effectively for companies.
We present in this PhD thesis, the work we have developed under the E-Gen project, which aims to create tools to automate the flow of information during a recruitment process.
We interested first to the problems posed by the routing of emails.
The ability of a companie to manage efficiently and at lower cost this information flows becomes today a major issue for customer satisfaction.
After, we present work that was conducted as part of the analysis and integration of a job ads via Internet.
We present a solution capable of integrating a job ad from an automatic or assisted in order to broadcast it quickly.
Based on a combination of classifiers systems driven by a Markov automate, the system gets very good results.
Thereafter, we present several strategies based on vectorial and probabilistic models to solve the problem of profiling candidates according to a specific job offer to assist recruiters.
We have evaluated a range of measures of similarity to rank candidatures by using ROC curves.
Relevance feedback approach allows to surpass our previous results on this task, difficult, diverse and higly subjective.
Our research described in this thesis is about the learning of a motif-based representation from time series to perform automatic classification.
Meaningful information in time series can be encoded across time through trends, shapes or subsequences usually with distortions.
Approaches have been developed to overcome these issues often paying the price of high computational complexity.
Among these techniques, it is worth pointing out distance measures and time series representations.
We focus on the representation of the information contained in the time series.
This framework proposes to transform a set of time series into a feature space, using subsequences enumerated from the time series, distance measures and aggregation functions.
One particular instance of this framework is the well-known shapelet approach.
The potential drawback of such an approach is the large number of subsequences to enumerate, inducing a very large feature space and a very high computational complexity.
We show that most subsequences in a time series dataset are redundant.
Therefore, a random sampling can be used to generate a very small fraction of the exhaustive set of subsequences, preserving the necessary information for classification and thus generating a much smaller feature space compatible with common machine learning algorithms with tractable computations.
We also demonstrate that the number of subsequences to draw is not linked to the number of instances in the training set, which guarantees the scalability of the approach.
The combination of the latter in the context of our framework enables us to take advantage of advanced techniques (such as multivariate feature selection techniques) to discover richer motif-based time series representations for classification, for example by taking into account the relationships between the subsequences.
These theoretical results have been extensively tested on more than one hundred classical benchmarks of the literature with univariate and multivariate time series.
Moreover, since this research has been conducted in the context of an industrial research agreement (CIFRE) with Arcelormittal, our work has been applied to the detection of defective steel products based on production line's sensor measurements.
Nigerian learners of French as a Foreign Language are generally faced with difficulties while using French Past Tenses in producing written composition.
In this thesis, we are particularly interested in the case of the Yoruba learners of French language.
The analysis of their written composition copies reveals that most of the errors committed originate from the mother tongue, Yoruba which does not know the tense-markedness of French language with her conjugation and complicated verb endings.
On the other hand, through our analysis of copies of two objective tests in which students were to produce the missing verb forms, we also found that the learners lack some theoretical linguistic knowledge which is important in understanding French past tenses: for instance, Benveniste's “Discours &amp; récit” and Weinrich's “Premier plan / Arrière-plan”.
We believe that students would better understand the use of French past tenses if they have a good grasp of the “grammatical aspect” notion and if this linguistic notion is taken into account while teaching the topic.
We brought the research to a close with different suggestions on how to improve the teaching / learning of the French tenses concerned here.
On the whole, placing oneself on the didactic perspective, we are of the opinion that all these information put together can help develop a Methodology for the teaching and learning of French past tenses; and by so doing, advance the more the cause of the teaching and learning of French in Nigeria.
Many problems in machine learning are naturally cast as the minimization of a smooth function defined on a Euclidean space.
For supervised learning, this includes least-squares regression and logistic regression.
In this manuscript, we consider the particular case of the quadratic loss.
In the first part, we are interestedin its minimization when its gradients are only accessible through a stochastic oracle.
In the second part, we consider two applications of the quadratic loss in machine learning: clustering and estimation with shape constraints.
This new framework suggests an alternative algorithm that exhibits the positive behavior of both averaging and acceleration.
The second main contribution aims at obtaining the optimal prediction error rates for least-squares regression, both in terms of dependence on the noise of the problem and of forgetting the initial conditions.
Our new algorithm rests upon averaged accelerated gradient descent.
The third main contribution deals with minimization of composite objective functions composed of the expectation of quadratic functions and a convex function.
Weextend earlier results on least-squares regression to any regularizer and any geometry represented by a Bregman divergence.
As a fourth contribution, we consider the the discriminative clustering framework.
We propose its first theoretical analysis, a novel sparse extension, a natural extension for the multi-label scenario and an efficient iterative algorithm with better running-time complexity than existing methods.
The fifth main contribution deals with the seriation problem.
We propose a statistical approach to this problem where the matrix is observed with noise and study the corresponding minimax rate of estimation.
The Artificial Intelligence (AI) field has been unprecedented growing in recent years with spectacular, high-profile successes.
In fact, AI is applied in many fields ranging from computer vision to natural language processing.
Among all the techniques of artificial intelligence, neural network based deep learning has shown very good learning capabilities and good performance in many areas.
The design and development of such networks is an arduous task that requires advanced knowledge in modern parallel architectures in order to make the best use of the computing power of such machines.
This issue is more important with the massive development of AI in both academic and industrial world.
The objective of the thesis is to define a development environment for deep learning neural networks which is capable of taking the current developments in neural network architectures into account.
This environment has to include a domain-specific language allowing the user to describe the architecture of his system without worrying about how it will be deployed.
This high-level language based on the principles of implicit parallelism provides the user with patterns or skeletons allowing to describe the neural network elements and their assembly in a simple way while giving indications that allows to define an efficient parallelization strategy.
The environment must be able to derive from the high-level description, an efficient program and deployment of the neural network on the target architecture.
In order to achieve this, it's necessary to detect and describe the relevant programming skeletons for neural network designers and to find efficient parallelization strategies for each of these skeletons.
In addition, deep learning network analysis tools to detect optimizations in terms of deployment and communications will also be essential to obtain efficient systems.
The results aim to be integrated in Huawei's MindSpore environment and contribute to Huawei's AI products and solutions, in order to explore the full potential power of its Compute Architecture for Neural Networks (CANN) as well as the hardware equipped with Huawei's Ascend chips.
Over the last few years, there has been tremendous growth in digitizing collections of cultural heritage documents.
Recently, an important need has emerged which consists in designing a computer-aided characterization and categorization tool, able to index or group historical digitized book pages according to several criteria, mainly the layout structure and/or typographic/graphical characteristics of the historical document image content.
Thus, the work conducted in this thesis presents an automatic approach for characterization and categorization of historical book pages.
The proposed approach is applicable to a large variety of ancient books.
In addition, it does not assume a priori knowledge regarding document image layout and content.
It is based on the use of texture and graph algorithms to provide a rich and holistic description of the layout and content of the analyzed book pages to characterize and categorize historical book pages.
The categorization is based on the characterization of the digitized page content by texture, shape, geometric and topological descriptors.
This characterization is represented by a structural signature.
More precisely, the signature-based characterization approach consists of two main stages.
The first stage is extracting homogeneous regions.
Then, the second one is proposing a graph-based page signature which is based on the extracted homogeneous regions, reflecting its layout and content.
Afterwards, by comparing the different obtained graph-based signatures using a graph-matching paradigm, the similarities of digitized historical book page layout and/or content can be deduced.
Subsequently, book pages with similar layout and/or content can be categorized and grouped, and a table of contents/summary of the analyzed digitized historical book can be provided automatically.
As a consequence, numerous signature-based applications (e.g. information retrieval in digital libraries according to several criteria, page categorization) can be implemented for managing effectively a corpus or collections of books.
To illustrate the effectiveness of the proposed page signature, a detailed experimental evaluation has been conducted in this work for assessing two possible categorization applications, unsupervised page classification and page stream segmentation.
In addition, the different steps of the proposed approach have been evaluated on a large variety of historical document images.
The research work presented in this thesis is a part of the general framework of work on digital humanities that seeks, among other things, to contribute to the improvement of human-machine interactions.
The objective of the study is twofold.
Firstly, it is about studying a corpus of court decisions contained in the database of the International Transport Law Institute (IDIT) in order to determine the linguistic constraints of the judicial kind.
Secondly, it is a matter of proposing interpretative paths that can help users to access to the legal information they are looking for.
The issue of the interpretation assistance is seen through the study of the modalities and modal scenarios.
The bias of this research is to consider multidisciplinarity as a theoretical and methodological asset that contributes to a better understanding of the issue of the interpretation assistance.
As a result, several approaches (semantics of modalities, textual semantics, rhetorical argumentation, textometry) are called and articulated to work together towards the objectives set.
The analysis of the corpus was conducted at two levels and in two approaches.
In the first part, the empirical analysis proposed is quantitative and contrasting.
It is conducted at the micro and mesotexual level as it focuses on the study of the lexicon.
Based on the TXM tool, this first investigation allowed a comprehensive linguistic characterization of the corpus and an initial overview of its modal profile.
It also highlighted modal expressions, concessional constructions, patterns, etc. which focus on key moments in the argumentative process and can therefore be used in the context of the interpretation assistance.
In the second part, the empirical study focuses on modal analyses realized on complete texts.
It is therefore treated in a qualitative and macrotextual approach.
This analysis leads to the formulation of a carefully described scenario model.
It can be divided into several levels, depending on how it has been constructed (foreground modalities, background modalities) and whether it characterizes a full text or a specific area of this text.
Furthermore, the proposed schematic presentation for the scenarios highlighted the role that each modal zone would play in providing the interpretation assistance.
The first part of this thesis presents a computer assisted transcription of speech method.
Every time the user corrects a word in the automatic transcription, this correction is immediately taken into account to re-evaluate the transcription of the words following it.
The latter is obtained from a reordering of the confusion networks hypothesis generated by the ASR.
In order to decrease the proper nouns error rate, an acoustic-based phonetic transcription method is proposed in this manuscript.
The use of SMT [Laurent 2009] associated with the proposed method allows a significant reduce in term of word error rate (WER) and in term of proper nouns error rate (PNER).
The goal of computational linguistics is to provide a formal account linguistical knowledge, and to produce algorithmic tools for natural language processing.
Often, this is done in a so-called generative framework, where grammars describe sets of valid sentences by iteratively applying some set of rewrite rules.
Another approach, based on model theory, describes instead grammaticality as a set of well-formedness logical constraints, relying on deep links between logic and automata in order to produce efficient parsers.
This thesis favors the latter approach.
Making use of several existing results in theoretical computer science, we propose a tool for linguistical description that is both expressive and designed to facilitate grammar engineering.
It first tackles the abstract structure of sentences, providing a logical language based on lexical properties of words in order to concisely describe the set of grammaticaly valid sentences.
It then draws the link between these abstract structures and their representations (both in syntax and semantics), through the use of linearization rules that rely on logic and lambda-calculus.
Then in order to validate this proposal, we use it to model various linguistic phenomenas, ending with a specific focus on languages that include free word order phenomenas (that is, sentences which allow the free reordering of some of their words or syntagmas while keeping their meaning), and on their algorithmic complexity.
This thesis deals with lexical unit extraction in contemporary Chinese from a corpus of specialized texts.
It addresses the task of Chinese lexicon extraction using techniques based on linguistic characteristics of the Chinese language.
The thesis also discusses how to evaluate the extraction of a lexicon in an industrial environment.
The first part of the thesis describes the context of the study.
We focus on describing the linguistic concepts of vocabulary and lexical units, and we also give a description of the construction of lexical units in contemporary Chinese.
We then make a inventory of the different techniques used by the scientific community to address the task of extracting a contemporary Chinese lexicon.
We conclude this section by describing lexicon extraction practices in industry, and we propose a formalization of the criteria used by terminologists to select the relevant lexical units.
The second part of this thesis deals with the description of a method for extracting Chinese contemporary lexicon and its evaluation.
We introduce a new numerical unsupervised method based on structural features of the lexical unit in Chinese and syntactic features of Chinese.
The method includes an optional module to interact with a user (i. E. Semi-automatic).
In the section related to the evaluation, we first evaluate the potential of the method by comparing extraction results to a reference standard and a reference method.
We then implement a more pragmatic evaluation of the method by measuring the gains using this method as opposed to manual lexicon extraction by terminologists.
The results obtained by our method are better than those produced by the reference method on the reference standard.
The pragmatic evaluation shows that the method does not significantly improve the productivity of terminologists but can extract different lexical units than those obtained manually.
Big Data for biomedicine domain deals with a major issue, the analyze of large volume of heterogeneous data (e.g. video, audio, text, image).
Ontology, conceptual models of the reality, can play a crucial role in biomedical to automate data processing, querying, and matching heterogeneous data.
Initially, ontologies were built manually.
In recent years, few semi-automatic methodologies have been proposed.
The semi-automatic construction/enrichment of ontologies are mostly induced from texts by using natural language processing (NLP) techniques.
NLP methods have to take into account lexical and semantic complexity of biomedical data: (1) lexical refers to complex phrases to take into account, (2) semantic refers to sense and context induction of the terminology.
In this thesis, we propose methodologies for enrichment/construction of biomedical ontologies based on two main contributions, in order to tackle the previously mentioned challenges.
The first contribution is about the automatic extraction of specialized biomedical terms (lexical complexity) from corpora.
New ranking measures for single- and multi- word term extraction methods have been proposed and evaluated.
In addition, we present BioTex software that implements the proposed measures.
The second contribution concerns the concept extraction and semantic linkage of the extracted terminology (semantic complexity).
The experiments conducted on real data highlight the relevance of the contributions.
However, these new features multiply by a factor 8 the amount of data to be processed before transmission to the end user.
In addition to this new format, broadcasters and Over-The-Top (OTT) content providers have to encode videos in different formats and at different bitrates due to the wide variety of devices with heterogeneous video format and network capacities used by consumers.
The objective of this thesis is thus to investigate lightweight scalable encoding approaches based on the adaptation of the spatio-temporal resolution.
The first part of this document proposes two pre-processing tools, respectively using polyphase and wavelet frame-based approaches, to achieve spatial scalability with a slight complexity overhead.
A variable frame-rate algorithm is first proposed as pre-processing.
This solution has been designed to locally and dynamically detect the lowest frame-rate that does not introduce visible motion artifacts.
The proposed variable frame-rate and adaptive spatial resolution algorithms are then combined to offer a lightweight scalable coding of 4K HFR video contents.
The quality of Volunteered Geographic Information (VGI) is currently a topic that question spatial data users as well as authoritative data producers who are willing to exploit the benefits of crowdsourcing.
This phenomenon is one the main downsides of crowsdsourcing, and despite the small amount of incidents, it may be a barrier to the use of collaborative spatial data.
This thesis follows an approach based on VGI quality -- in particular, the objective of this work is to detect vandalism in spatial collaborative data.
First, we formalize a definition of the concept of carto-vandalism.
Finally, the experiments explore the ability of learning methods to detect carto-vandalism
This thesis focuses on the integration of lexical and syntactic resources of French in two fundamental tasks of Natural Language Processing [NLP], that are probabilistic part-of-speech tagging and probabilistic parsing.
In this paper, we use these resources to give an answer to two problems that we describe briefly below: data sparseness and automatic segmentation of texts.
Through more and more sophisticated parsing algorithms, parsing accuracy is becoming higher for many languages including French.
However, there are several problems inherent in mathematical formalisms that statistically model the task (grammar, discriminant models,...).
Moreover, it is proved that spars ness is partly a lexical problem, because the richer the morphology of a language is, the sparser the lexicons built from a Treebank will be for that language.
Our first problem is therefore based on mitigating the negative impact of lexical data sparseness on parsing performance.
To this end, we were interested in a method called word clustering that consists in grouping words of corpus and texts into clusters.
These clusters reduce the number of unknown words, and therefore the number of rare or unknown syntactic phenomena, related to the lexicon, in texts to be analyzed.
Our goal is to propose word clustering methods based on syntactic information from French lexicons, and observe their impact on parsers accuracy.
Furthermore, most evaluations about probabilistic tagging and parsing were performed with a perfect segmentation of the text, as identical to the evaluated corpus.
But in real cases of application, the segmentation of a text is rarely available and automatic segmentation tools fall short of proposing a high quality segmentation, because of the presence of many multi-word units (compound words, named entities,...).
In this paper, we focus on continuous multi-word units, called compound words, that form lexical units which we can associate a part-of-speech tag.
We may see the task of searching compound words as text segmentation.
Our second issue will therefore focus on automatic segmentation of French texts and its impact on the performance of automatic processes.
In order to do this, we focused on an approach of coupling, in a unique probabilistic model, the recognition of compound words and another task.
Recognition of compound words is performed within the probabilistic process rather than in a preliminary phase.
Automated planning is a field of artificial intelligence that aims at proposing methods to chose and order sets of actions with the objective of reaching a given goal.
One then searches for plans into each component and try to assemble these local plans into a global plan for the original planning problem.
The main interest of this approach is that, for some classes of planning problems, the components considered can be planning problems much simpler to solve than the original one.
The main difference between this approach and the previous one is that it is not only modular but also distributed.
In this dissertation, we discuss how to use the human gaze data to improve the performance of the weak supervised learning model in image classification.
The background of this topic is in the era of rapidly growing information technology.
As a consequence, the data to analyze is also growing dramatically.
Since the amount of data that can be annotated by the human cannot keep up with the amount of data itself, current well-developed supervised learning approaches may confront bottlenecks in the future.
In this context, the use of weak annotations for high-performance learning methods is worthy of study.
Specifically, we try to solve the problem from two aspects: One is to propose a more time-saving annotation, human eye-tracking gaze, as an alternative annotation with respect to the traditional time-consuming annotation, e.g. bounding box.
The other is to integrate gaze annotation into a weakly supervised learning scheme for image classification.
This scheme benefits from the gaze annotation for inferring the regions containing the target object.
A useful property of our model is that it only exploits gaze for training, while the test phase is gaze free.
This property further reduces the demand of annotations.
The two isolated aspects are connected together in our models, which further achieve competitive experimental results.
Sinusoidal Modeling is one of the most widely used parametric methods for speech and audio signal processing.
The eaQHM is shown to outperform aQHM in analysis and resynthesis of voiced speech.
Based on the eaQHM, a hybrid analysis/synthesis system of speech is presented (eaQHNM), along with a hybrid version of the aHM (aHNM).
Moreover, we present motivation for a full-band representation of speech using the eaQHM, that is, representing all parts of speech as high resolution AM-FM sinusoids.
Experiments show that adaptation and quasi-harmonicity is sufficient to provide transparent quality in unvoiced speech resynthesis.
The full-band eaQHM analysis and synthesis system is presented next, which outperforms state-of-the-art systems, hybrid or full-band, in speech reconstruction, providing transparent quality confirmed by objective and subjective evaluations.
Regarding applications, the eaQHM and the aHM are applied on speech modifications (time and pitch scaling).
The resulting modifications are of high quality, and follow very simple rules, compared to other state-of-the-art modification systems.
Results show that harmonicity is preferred over quasi-harmonicity in speech modifications due to the embedded simplicity of representation.
Moreover, the full-band eaQHM is applied on the problem of modeling audio signals, and specifically of musical instrument sounds.
The eaQHM is evaluated and compared to state-of-the-art systems, and is shown to outperform them in terms of resynthesis quality, successfully representing the attack, transient, and stationary part of a musical instrument sound.
Finally, another application is suggested, namely the analysis and classification of emotional speech.
The eaQHM is applied on the analysis of emotional speech, providing its instantaneous parameters as features that can be used in recognition and Vector-Quantization-based classification of the emotional content of speech.
Although the sinusoidal models are not commonly used in such tasks, results are promising.
Thanks to the democratization of new communication technologies, there is a growing quantity of textual resources, making Automatic Natural Language Processing (NLP) a discipline of crucial importance both scientifically and industrially.
Easily available, these data offer unprecedented opportunities and, from opinion analysis to information research and semantic text analysis, there are many applications.
However, this textual data cannot be easily exploited in its raw state and, in order to carry out such tasks, it seems essential to have resources describing semantic knowledge, particularly in the form of lexico-semantic networks such as that of the JeuxDeMots project.
However, the constitution and maintenance of such resources remain difficult operations, due to their large size but also because of problems of polysemy and semantic identification.
Moreover, their use can be tricky because a significant part of the necessary information is not directly accessible in the resource but must be inferred from the data of the lexico-semantic network.
Our work seeks to demonstrate that lexico-semantic networks are, by their connexionic nature, much more than a collection of raw facts and that more complex structures such as interpretation paths contain more information and allow multiple inference operations to be performed.
In particular, we will show how to use a knowledge base to provide explanations to high-level facts.
These explanations allow at least to validate and memorize new information.
In doing so, we can assess the coverage and relevance of the database data and consolidate it.
Similarly, the search for paths is useful for classification and disambiguation problems, as they are justifications for the calculated results.
In the context of the recognition of named entities, they also make it possible to type entities and disambiguate them (is the occurrence of the term Paris a reference to the city, and which one, or to a starlet?) by highlighting the density of connections between ambiguous entities, their context and their possible type.
Finally, we propose to turn the large size of the JeuxDeMots network to our advantage to enrich the database with new facts from a large number of comparable examples and by an abduction process on the types of semantic relationships that can connect two given terms.
Each inference is accompanied by explanations that can be validated or invalidated, thus providing a learning process.
This thesis has a bi-directional approach to the domain of spoken dialog systems.
On the one hand, parts of the work emphasize on increasing the reliability and the intuitiveness of such interfaces.
On the other hand, it also focuses on the design and development side, providing a platform made of independent specialized modules and tools to support the implementation and the test of prototypical spoken dialog systems technologies.
The topics covered by this thesis are centered around an open-source framework for supporting the design and implementation of natural-language spoken dialog systems.
According to the two directions taken in this work, the natural language understanding subsystem of the platform has been thought to be intuitive to use, allowing a natural language interaction.
Finally, on the dialog management side, this thesis argue in favor of the deterministic modeling of dialogs.
However, such an approach requires intense human labor, is prone to error and does not ease the maintenance, the update or the modification of the models.
A new paradigm, the linked-form filling language, offers to facilitate the design and the maintenance tasks by shifting the modeling to an application specification formalism.
The purpose of this thesis is to prepare a French-Romanian / Romanian-French dictionary of veterinary medicine.
In order to achieve this purpose, a first step was to study the methods used over time by the masters of the language.
Our work methods have been offered by our more experienced colleagues and the ISO terminology norms.
Thus, our research consisted in an essay to modeling the terminology work, on the way to find the best methods.
In a second step, we established a research corpus, based on the methods of corpus linguistics.
We have assembled a corpus of 2193 veterinary theses from 11 years (1998-2008) of the four French veterinary schools - VeThèses, on the theme of domestic vertebrates, theses available in digitized format.
The term extraction step includes an essay to systematize the terms.
The results of this work on the corpus formed a nomenclature and the VeTerm database.
Four methods are planned to identify the underground pipelines but they have limits and depend on many factors.
Our investigation aims to solve the problem of reliable detection of underground networks by aggregation of the existing methods and reasoning at different abstraction levels.
At a time when computer science has invaded all aspects of our daily life, it is natural to see the computer field participating in human and social sciences work, and more particularly in linguistics where the need to develop computer software is becoming more and more pressing with the growing volume of analyzed corpora.
Hence our thesis which consists in elaborating a program EPL that studies the white Lebanese Arabic speech.
Starting from a corpus elaborated from two TV programs recorded then transcribed in Arabic letters, the program EPL, developed with Access software, allowed us to extract words and collocations, and to carry out a linguistic analysis on the lexical, phonetic, syntactic and collocational levels.
The EPL's functioning as well as its development code are described in the computer part.
Important annexes conclude the thesis and gather the result of the work of a team of researchers coming from different specialties.
Computer vision is the computational science aiming at reproducing and improving the ability of human vision to understand its environment.
In this thesis, we focus on two fields of computer vision, namely image segmentation and visual odometry and we show the positive impact that interactive Web applications provide on each.
The first part of this thesis focuses on image annotation and segmentation.
Many interactions have been explored in the literature to help segmentation algorithms.
The most common consist in designating contours, bounding boxes around objects, or interior and exterior scribbles.
When crowdsourcing, annotation tasks are delegated to a non-expert public, sometimes on cheaper devices such as tablets.
In this context, we conducted a user study showing the advantages of the outlining interaction over scribbles and bounding boxes.
Another challenge of crowdsourcing is the distribution medium.
While evaluating an interaction in a small user study does not require complex setup, distributing an annotation campaign to thousands of potential users might differ.
A highlights tour of its functionalities and architecture is provided, as well as a guide on how to deploy it to crowdsourcing services such as Amazon Mechanical Turk.
The application is completely opensource and available online.
In the second part of this thesis we present our open-source direct visual odometry library.
In that endeavor, we provide an evaluation of other open-source RGB-D camera tracking algorithms and show that our approach performs as well as the currently available alternatives.
The visual odometry problem relies on geometry tools and optimization techniques traditionally requiring much processing power to perform at realtime framerates.
Since we aspire to run those algorithms directly in the browser, we review past and present technologies enabling high performance computations on the Web.
In particular, we detail how to target a new standard called WebAssembly from the C++ and Rust programming languages.
Our library has been started from scratch in the Rust programming language, which then allowed us to easily port it to WebAssembly.
Thanks to this property, we are able to showcase a visual odometry Web application with multiple types of interactions available.
A timeline enables one-dimensional navigation along the video sequence.
Pairs of image points can be picked on two 2D thumbnails of the image sequence to realign cameras and correct drifts.
Colors are also used to identify parts of the 3D point cloud, selectable to reinitialize camera positions.
Combining those interactions enables improvements on the tracking and 3D point reconstruction results.
Our working hypothesis is: FSL is a language and, therefore, machine translation is relevant.
We describe the language specifications for automatic processing, based on scientific knowledge and proposals of our native FSL speaker informants.
We also expose our methodology, and do present the advancement of our work in the formalization of linguistic data based on the specificities of FSL which certain (verbs scheme, adjective and adverb modification, organization of nouns, agreement patterns) require further analysis.
We do present the application framework in which we worked on: the machine translation system and virtual characters animation system of France Telecom R&amp;D.After a short avatar technology presentation, we explain our control modalities of the gesture synthesis engine through the exchange format that we developed.
Finally, we conclude with an evaluation, researches and developments perspectives that could follow this thesis.
Our approach has produced its first results since we have achieved our goal of running the full translation chain: from the input of a sentence in French to the realization of the corresponding sentence in FSL with a synthetic character.
There is much CALL software on the Internet, designed by using authoring systems such as (Course builder, Hot Potatoes or Netquizz).
Such activities poses several problems as the rigidity of software (the data used are predetermined and can not be altered or enhanced) and the not adaptability of course to the language skills of learners (the path is independent of its response to each step, they can not evaluate).
The use of the NLP for the design of software CALL, is currently one of the method that can solve the problems.
But after more than two decades since the early work, the advanced research in the topic of CALL based on the NLP remains weak, due to two main factors: the lack of NLP from language didactic psychoanalysts or computer scientists, and the cost of resources and products of natural language processing.
The CALL work based on NLP for the Arabic language is practically not existed, in despite of a rich literature on the automatic processing of Arabic.
In addition to factors mentioned above, the deficiency of the Arabic language in this area is due to that Arabic is a language difficult to treat automatically.
Under this situation, and willing to enrich the possibilities for creating educational activities for Arabic we have: As a first step, developed a labelled dictionary of Arabic (as complete as possible), a derivative, a Conjugator and a morphological analyzer for Arabic.
In a second step, we used these tools to create a number of educational applications for Arabic learning as a foreign language for French learners by using our system SALA.
This thesis proposes linguistic and phonetic investigations of French-Algerian Arabic code-switching.
A corpus of 7h30 of speech (5h of spontaneous speech and 2h30 of read speech) has been designed with 20 males and females French-Algerian Arabic speakers.
This thesis also proposes code-switching speech data processing methods such as language segmentation, code-switching utterance segmentation and transcription of French and Algerian Arabic dialect.
Automatic speech alignment methods of the code-switching data are proposed with combined alignment of two monolingual alignments.
We conducted experiments based on language automatic identification and automatic alignment with variations that deals with the question of the influence of a phonological system of a language A on code-switching speech in phonetic productions of French and Algerian Arabic.
We performed also a variation study on vowel variation, in both French and Arabic productions.
Finally, we dealt with three types of consonant variation in the code-switching speech: gemination, emphatization and voicing consonant as variants in production.
The results shown that the code-switching French-Algerian Arabic is characterized by very short language switches witch constitute a big challenge to the code-switching languages identification.
The code-switching has an impact of the phonetic variation in both vowel and consonants.
The code-switching allows the speakers to produce less vowel and consonant variation than the monolingual speech.
Automatically generated text has been used in numerous occasions with distinct intentions.
It can simply go from generated comments in an online discussion to a much more mischievous task, such as manipulating bibliography information.
So, this thesis first introduces different methods of generating free texts that resemble a certain topic and how those texts can be used.
Therefore, we try to tackle with multiple research questions.
The first question is how and what is the best method to detect a fully generated document.
Then, we take it one step further to address the possibility of detecting a couple of sentences or a small paragraph of automatically generated text by proposing a new method to calculate sentences similarity using their grammatical structure.
The last question is how to detect an automatically generated document without any samples, this is used to address the case of a new generator or a generator that it is impossible to collect samples from.
This thesis also deals with the industrial aspect of development.
A simple overview of a publishing workflow from a high-profile publisher is presented.
From there, an analysis is carried out to be able to best incorporate our method of detection into the production workflow.
In conclusion, this thesis has shed light on multiple important research questions about the possibility of detecting automatically generated texts in different setting.
Besides the researching aspect, important engineering work in a real life industrial environment is also carried out to demonstrate that it is important to have real application along with hypothetical research.
Enterprise systems' interoperability has been identified as a significant issue faced by enterprises, which need to collaborate with other companies and participate within Networked Enterprises.
To achieve a higher quality of interoperability and ensure an effective collaboration, a certain number of Interoperability Requirements (IRs) should be satisfied.
Thus, interoperability should be verified and continuously improved.
A manner for verifying the enterprise systems' interoperability is through the Interoperability Assessment (INAS).
It also has been identified that the IEC interdependencies are not explicitly defined.
Indeed, their interdependencies should be considered as they can support the identification of impacts on the overall system.
Further, the majority of the INAS approaches are manual-conducted, which is a laborious and time-consuming process and in many times depends on the “subjective” knowledge of experts, which can be expensive in time and money when hiring external consultants.
A Design Science Research (DSR) methodology has been adopted for conducting the work.
To formally conceptualise the knowledge about the INAS (subsuming the set of IRs, interoperability problems and solutions), we proposed the Ontology of Interoperability Assessment (OIA).
A Model-Based System Engineering approach has been applied for defining and organising the concepts of the proposed ontology.
A prototype of the KBIAS using the OIA as its knowledge model has been developed in a Java platform.
The contribution proposed in this research has been evaluated through a case study based on a real Networked Enterprise
Subjective estimation and perception, complexity of the environment under study, interaction amongst subsystems, lack of precise data, missing data, limited information processing capacity and ambiguity in natural languages are major forms of uncertainty facing Decision Makers in the process of delivering strategic decisions in economic intelligent systems.
This study employs soft computing paradigm to capture and analyze uncertainty based on information risk factors via our proposed knowledge reconciliation model based on ontology and the FuzzyWatch model.
We extended this operation with fuzzy that is – what ontology captures is interpreted by fuzzy techniques (FuzzOntology).
FuzzyWatch assists in reducing missing data problems.
Future decisional process will contend with lesser information retrieval risks in Economic Intelligence process using this model.
These conversations may be found on online channels such as forums, mailing lists or chat rooms.
We want to use dialogue acts for the analysis of online written conversations.
Well-defined methods and models allowing for the fine-grained analysis of these conversations would represent a solid framework to support different user-assistance and dialogue analysis systems.
However, current conversations analysis techniques were not developed with written online conversations in mind.
It is necessary to adapt existing resources for these conversations.
Our goal is to build a dialogue act model for problem-solving online written conversations, and to offer tools for the automatic recognition of these acts.
In this thesis, we proposed a multilingual generic approach for the automatic information extraction.
Particularly, events extraction of price variation and temporal information extraction linked to temporal referential.
Our approach is based on the constitution of several semantic maps by textual analysis in order to formalize the linguistic traces expressed by categories.
We created a database for an expert system to identify and annotate information (categories and their characteristics) based on the contextual rule groups.
Two algorithms AnnotEC and AnnotEV have been applied in the SemanTAS platform to validate our assumptions.
We have obtained a satisfactory result; Accuracy and recall are around 80%.
We presented extracted knowledge by a summary file.
Automatic morphological analysis of Slovak language is the first level of an automatical analyser for Slovak's scientifical and technical texts.
Such a system could be used for different applications: automatic text indexation, automatic research of terminology or translation systems.
A rule-based description of language's regularities as well as the use of all the formal level elements of words allow to reduce considerably the volume of dictionaries.
Notably in case of inflectionally rich languages such as Slovak.
The results obtained by our morphological analyser justify such an approach and confirm the high reliability of morphological analysis based on form-recognition for all lexical categories
In recent years, hacking has become an industry unto itself, increasing the number and diversity of cyber attacks.
Threats on computer networks range from malware to denial of service attacks, phishing and social engineering.
An effective cyber security plan can no longer rely solely on antiviruses and firewalls to counter these threats: it must include several layers of defence.
Network-based Intrusion Detection Systems (IDSs) are a complementary means of enhancing security, with the ability to monitor packets from OSI layer 2 (Data link) to layer 7 (Application).
Intrusion detection techniques are traditionally divided into two categories: signatured-based (or misuse) detection and anomaly detection.
Most IDSs in use today rely on signature-based detection; however, they can only detect known attacks.
IDSs using anomaly detection are able to detect unknown attacks, but are unfortunately less accurate, which generates a large number of false alarms.
In this context, the creation of precise anomaly-based IDS is of great value in order to be able to identify attacks that are still unknown.
In this thesis, machine learning models are studied to create IDSs that can be deployed in real computer networks.
Firstly, a three-step optimization method is proposed to improve the quality of detection: 1/ data augmentation to rebalance the dataset, 2/ parameters optimization to improve the model performance and 3/ ensemble learning to combine the results of the best models.
Flows detected as attacks can be analyzed to generate signatures to feed signature-based IDS databases.
However, this method has the disadvantage of requiring labelled datasets, which are rarely available in real-life situations.
Transfer learning is therefore studied in order to train machine learning models on large labeled datasets, then finetune them on benign traffic of the network to be monitored.
This method also has flaws since the models learn from already known attacks, and therefore do not actually perform anomaly detection.
Thus, a new solution based on unsupervised learning is proposed.
It uses network protocol header analysis to model normal traffic behavior.
Anomalies detected are then aggregated into attacks or ignored when isolated.
Finally, the detection of network congestion is studied.
The bandwidth utilization between different links is predicted in order to correct issues before they occur.
A musical score is a complex semiotic object that excels at conveying musical information in a human readable format.
Nowadays, a lot of music is available exclusively as recorded performances, so systems which can automatically transform those performances into musical scores would be extremely beneficial for performers and musicologists.
This task, called automatic music transcription (AMT), comprises a large number of subtasks which generically transform the performance input into higher level representations, such as unquantized and quantized MIDI files.
We believe that a clear model of the information contained in a musical score would help the development and the evaluation of AMT systems.
In particular we advocate for a clear separation between the music which the score encodes and the set of notation symbols which is employed to represent it.
The Semantic Web extends the Web by publishing structured and interlinked data using RDF.An RDF data set is a graph where resources are nodes labelled in natural languages.
One of the key challenges of linked data is to be able to discover links across RDF data sets.
Given two data sets, equivalent resources should be identified and linked by owl:sameAs links.
This problem is particularly difficult when resources are described in different natural languages.
For this purpose, we introduce a general framework in which each RDF resource is represented as a virtual document containing text information of neighboring nodes.
This can be achieved by using machine translation or multilingual lexical resources.
Similarity between elements of this space is taken for similarity between RDF resources.
In particular, two strategies are explored: applying machine translation or using references to multilingual resources.
Overall, evaluation shows the effectiveness of cross-lingual string-based approaches for linking RDF resources expressed in different languages.
The methods have been evaluated on resources in English, Chinese, French and German.
The best performance (over 0.90 F-measure) was obtained by the machine translation approach.
The best experimental results involving just a pair of languages demonstrated the usefulness of such techniques for interlinking RDF resources cross-lingually.
This thesis deals with the dynamic adaptation of context-aware applications using information related to the social environment of users to enrich the service rendered by the applications.
To achieve this goal our contribution mobilizes multidimensional modeling of the different levels of social contexts, especially the weight of the relationship between the actors.
Particularly, we synthesize not only social contexts related to familiarity but also social contexts reasoned from the similarity of static and dynamic communities.
Two models based on respectively graphs and ontologies are proposed in order to satisfy the heterogeneity of the social networks in real life.
We use the actual data gathered on online social networking services for conducting experiments and the results are analyzed by checking the effectiveness of the models.
In parallel we consider the point of view of the application, and we present two algorithms using social contexts to improve the strategy of transmission of data in the opportunistic network, particularly countermeasure against selfish nodes.
The simulations of real scenarios confirm the advantages of introducing social contexts in terms of success rate and delay of transmission.
We carry out a comparison with other popular transmission algorithms in the literature
It is broadly accepted that 70 % of the total life cycle cost is committed during the specification phase.
Thus, we propose to methodologically integrate data science techniques into a collaborative requirement mining framework, which enables decision-makers to gain insight and discover opportunities in a massive set of requirements.
Initially, classification models extract requirements from prescriptive documents.
Requirements are subsequently analysed with natural language processing techniques so as to identify quality defects.
We conclude that the theoretical and empirical validation of our proposition corroborates the assumption that data science is an effective way to gain insight from hundreds or thousands of requirements.
Cardiovascular disease (CVD) and cancer are the leading cause of death and morbidity in men and women in France, and their annual cost is high.
The prevention of these chronic diseases is, with their early detection and their rapid and effective management, a possible way to reduce this cost.
Thus, the National Health Nutrition Program was set up in France, helping the French to have a better diet, to help reduce the incidence of these diseases.
The objective of this thesis is the construction of a system of personalized suggestions based on the profile of the individual and his cardiovascular risk.
This approach requires the establishment of an interdisciplinary approach involving researchers in the fields of computer science, epidemiology and nutrition.
The importance of this collaboration is justified by the need to produce suggestions supported by proven research in these areas.
The first contribution of this thesis is the integration of semantic web technologies into a new approach to cardiovascular risk assessment that takes into account the interactions between these factors.
The creation of the visualization tool MCVGraphViz allowed to implement this strategy.
The second contribution consists in proposing a solution to exploit the knowledge present in the health plans and the recommendations concerning the prevention of cardiovascular diseases in France.
Thus, we opted for a modular approach integrated into the tool MCVGraphViz that allows to produce recommendations (diet, physical activity, etc.) based on the assessed cardiovascular risk and the profile of the individual (sensory preferences, allergic constraints, physical capacity, etc.).
The third contribution concerns the nutritional qualification of cooking recipes for a better follow-up: the approach is based on techniques of automatic language processing and ontological reasoning to qualify a nutritional point of view of cooking recipes.
Many perspectives are exposed.
Most of them aim to improve referral systems and the expressivity of the cardiovascular disease knowledge base.
Our research is quantitative and qualitative, based on textometrics tools and discourse analysis.
Our literature review surveyed linguistic studies of advertising, through different schools of thought: descriptive linguistics, discourse analysis, semiology, rhetorics/stylistics, starting with Leech's founding work (The Language of Advertising, 1966), which led to several highlights:
1. advertising is indeed a specialty discourse, or a language for special purpose (LSP)
2. advertising shares some common features with both poetry and the language we speak everyday
3. intertextuality and active participation of the targeted reader are two key factors of advertising communication
4. the language of advertising is emotional, ambiguous, indirect and implicit
5. in a simple and playful way, advertising depicts a perfect, happy world
6. simplicity and conventionalization of the advertising discourse, with lexical constants.
To find answers to some of our questions on the advertising discourse, language, communication and translation, we selected a quantitative approach, i.e., the lexicometric methodology (statistical linguistics, more recently called textometry) developed by the Saint-Cloud (France) group, including André Salem (founder and developer of Lexico,which we used to analyze our corpus).
Our lexicometric analysis is complemented by a classic factor analysis (FCA)and Viprey's geodesical projection method (with the Astadiag software).
With our parallel corpus of roughly 800,000 tokens in English and one million in French (170 advertising brochures downloaded from 20 brands/10carmakers'Canadian websites, for a total of 229 vehicles), we found distinct lexical profiles for each of the seven carmarket segments: AB, CD, EF, MPV, PK (pickup trucks), SP (sports cars) and SUV. We also showed that overall,advertisements are translated rather literally.
Their behaviors must be adapted for each use case by experts.
Enabling the general public to teach new behaviors to robots may lead to better adaptation at lesser cost.
In open scenarios, a home social robot should learn about its environment.
The purpose of such a robot is not restricted to learning new behaviors or about the environment: it should provide entertainment or utility, and therefore support rich scenarios.
We demonstrate the teaching of behaviors in these unique conditions: the teaching is achieved by the spoken language on Pepper robots deployed in homes, with no extra device and using its standard system, in a rich and open scenario.
Using automatic speech transcription and natural language processing, our system recognizes unpredicted teachings of new behaviors, and a explicit requests to perform them.
The new behaviors may invoke existing behaviors parametrized with objects learned in other contexts, and may be defined as parametric.
Through experiments of growing complexity, we show conflicts between behaviors in rich scenarios, and propose a solution based on symbolic task planning and priorization rules to resolve them.
The development of Natural Language Processing (NLP) systems needs to determine the quality of their results.
Whether aiming to compare several systems to each other or to identify both the strong and weak points of an isolated system, evaluation implies defining precisely and for each particular context a methodology, a protocol, language ressources (data needed for both system training and testing) and even evaluation measures and metrics.
It is following these conditions that system improvement is possible so as to obtain more reliable and easy-to-exploit results.
The contribution of evaluation to NLP is important due to the creation of new language resources, the homogenisation of formats for those data used or the promotion of a technology or a system.
We have tried to reduce and delimit those manual interventions.
To do so, we have supported our work by either conducting or participating in evaluation campaigns where systems are compared to each other or where isolated systems are evaluated.
The management of the evaluation procedure has been formalised in this work and its diﬀerent phases have been listed so as to define a common evaluation framework, understandable by all.
The main point of those evaluation phases regards quality measurement through the usage of metrics.
Three consecutive studies have been carried out on human measures, automatic measures and the automation of quality computation, and the meta-evaluation of the mesures so as to evaluate their reliability.
In that context, the study of the similarities between the technologies and between their evaluations has allowed us to highlight their common features and class them.
This has helped us to show that a small set of measures allows to cover a wide range of applications for diﬀerent technologies.
Our final goal has been to define a generic evaluation architecture, which is adaptable to diﬀerent NLP technologies, and sustainable, namely allowing to reuse language resources, measures or methods over time.
Our proposal has been built on the conclusions drawn fromprevious steps, with the objective of integrating the evaluation phases to our architecture and incorporating the evaluation measures, all of which bearing in mind the place of language resource usage.
The definition of this architecture has been done with the aim of fully automating the evaluation management work, regardless of whether this concerns an evaluation campaign or the evaluation of an isolated system.
From the point of view of Natural Language Processing (NLP), the extraction of events from texts is the most complex form of Information Extraction (IE) techniques, which more generally encompasses the extraction of named entities and relationships that bind them in the texts.
The event extraction task can be represented as a complex combination of relations linked to a set of empirical observations from texts.
As a result, adaptation to a new domain is an additional challenge.
This thesis presents several strategies for improving the performance of an Event Extraction (EE) system using neural-based approaches exploiting morphological, syntactic, and semantic properties of word embeddings.
These have the advantage of not requiring a priori modeling domain knowledge and automatically generate a much larger set of features to learn a model.
More specifically, we proposed different deep learning models for two sub-tasks related to EE: event detection and argument detection and classification.
Event Detection (ED) is considered an important subtask of event extraction since the detection of arguments is very directly dependent on its outcome.
As a preliminary to the introduction of our proposed models, we begin by presenting in detail a state-of-the-art model which constitutes the baseline.
In-depth experiments are conducted on the use of different types of word embeddings and the influence of the different hyperparameters of the model using the ACE 2005 evaluation framework, a standard evaluation for this task.
We then propose two new models to improve an event detection system.
One allows increasing the context taken into account when predicting an event instance by using a sentential context, while the other exploits the internal structure of words by taking advantage of seemingly less obvious but essentially important morphological knowledge.
We also reconsider the detection of arguments as a high-order relation extraction and we analyze the dependence of arguments on the ED task.
Our work concerns systems that help users during museum visits and access to cultural heritage.
Our goal is to design recommender systems, implemented in mobile devices to improve the experience of the visitor, by recommending him the most relevant items and helping him to personalize the tour he makes.
We consider two mainly domains of application: museum visits and tourism.
We propose a context-aware hybrid recommender system which uses three different methods: demographic, semantic and collaborative.
Every method is adapted to a specific step of the museum tour.
First, the demographic approach is used to solve the problem of the cold start.
The semantic approach is then activated to recommend to the user artworks that are semantically related to those that the user appreciated.
Finally, the collaborative approach is used to recommend to the user artworks that users with similar preferences have appreciated.
We used a contextual post filtering to generate personalized museum routes depending on artworks which were recommended and contextual information of the user namely: the physical environment, the location as well as the duration of the visit.
In the tourism field, the items to be recommended can be of various types (monuments, parks, museums, etc.).
Because of the heterogeneous nature of these points of interest, we proposed a composite recommender system.
Every recommendation is a list of points of interest that are organized in a package, where each package may constitute a tour for the user.
The objective is to recommend the Top-k packages among those who satisfy the constraints of the user (time, cost, etc.).
The experimental evaluation of the system we proposed using a real world data set crawled from Tripadvisor demonstrates its quality and its ability to improve both the relevance and the diversity of recommendations.
Advances in information and communication technologies has enabled the development of collaborative work in almost all sectors of human activity.
To ensure the performance of the group and minimize the risk of errors, it is crucial that the team members share a common understanding of the situation in which they are involved.
Progress in the study of collective cognition, the heart of collaborative work, has a clear potential that must be translated into tangible applications to optimize the management and execution of collective tasks.
Real-time evaluation of the cognition of individuals and teams allows to envisage adaptive tools and systems to improve efficiency, performance and agility.
In light of these challenges, our objective, commissioned by the DGA, is to find appropriate measures that would enable an assessment of the dynamics of the sharing of situational awareness, in the very constraining context of command and control room operations, which require the lowest possible level of instrumentation of operators.
Our contribution to the field has been dual.
We have proposed the concept of situation awareness synchrony to support the theoretical development of the study of the dynamics of situation awareness sharing.
This doctoral work is presented as a demonstration of the interest and applicability of shared cognition evaluation systems in realistic collaborative work environments, and is supported by proposals concerning the future of research on C2.
As part of the fight against global warming, several countries around the world, including Canada and some European countries, including France, have established measures to reduce greenhouse gas emissions.
One of the major areas addressed by the states concerns the transport sector and more particularly the development of public transport to reduce the use of private cars.
To this end, the local authorities concerned aim to establish more accessible, clean and sustainable urban transport systems.
In order to improve transport operators'knowledge of user travel in urban areas, we are taking advantage of the development of data science (e.g., data collection, development of machine learning methods).
This thesis thus focuses on three main parts: (i) long-term forecasting of passenger demand using event databases, (ii) short-term forecasting of passenger demand and (iii) visualization of passenger demand on public transport.
The research is mainly based on the use of ticketing data provided by transport operators and was carried out on three real case study, the metro and bus network of the city of Rennes, the rail and tramway network of "La Défense" business district in Paris, France, and the metro network of Montreal, Quebec in Canada
Selection of relevant acoustic speech features is key to in the design of any system using speech processing.
For some 40 years, speech was typically considered as a sequence of quasi-stable portions of signal (vowels) separated by transitions (consonants).
Despite a wealth of studies that clearly document the importance of coarticulation, and reveal that articulatory and acoustic targets are not context-independent, the view that each vowel has an acoustic target that can be specified in a context-independent manner remains widespread.
This point of view entails strong limitations.
It is well known that formant frequencies are acoustic characteristics that bear a clear relationship with speech production, and that can distinguish among vowels.
Therefore, vowels are generally described with static articulatory configurations represented by targets in the acoustic space, typically by formant frequencies in F1-F2 and F2-F3 planes.
Plosive consonants can be described in terms of places of articulation, represented by locus or locus equations in an acoustic plane.
But formant frequencies trajectories in fluent speech rarely display a steady state for each vowel.
They vary with speaker, consonantal environment (co-articulation) and speaking rate (relating to continuum between hypo- and hyper-articulation).
User-generated content on social media, such as Twitter, provides in many cases, the latest news before traditional media, which allows having a retrospective summary of events and being updated in a timely fashion whenever a new development occurs.
However, social media, while being a valuable source of information, can be also overwhelming given the volume and the velocity of published information.
Our work falls within these frameworks and focuses on developing a tweet summarization approaches for the two aforementioned scenarios.
Nevertheless, tweet summarization task faces many challenges that stem from, on one hand, the high volume, the velocity and the variety of the published information and, on the other hand, the quality of tweets, which can vary significantly.
In the prospective notification, the core task is the relevancy and the novelty detection in real-time.
For timeliness, a system may choose to push new updates in real-time or may choose to trade timeliness for higher notification quality.
The intuition behind our proposition is that context-aware similarity measure in word2vec is able to consider different words with the same semantic meaning and hence allows offsetting the word mismatch issue when calculating the similarity between a tweet and a topic.
Second, we propose to compute the novelty score of the incoming tweet regarding all words of tweets already pushed to the user instead of using the pairwise comparison.
The proposed novelty detection method scales better and reduces the execution time, which fits real-time tweet filtering.
Third, we propose an adaptive Learning to Filter approach that leverages social signals as well as query-dependent features.
To overcome the issue of relevance threshold setting, we use a binary classifier that predicts the relevance of the incoming tweet.
In addition, we show the gain that can be achieved by taking advantage of ongoing relevance feedback.
Finally, we adopt a real-time push strategy and we show that the proposed approach achieves a promising performance in terms of quality (relevance and novelty) with low cost of latency whereas the state-of-the-art approaches tend to trade latency for higher quality.
This thesis also explores a novel approach to generate a retrospective summary that follows a different paradigm than the majority of state-of-the-art methods.
We consider the summary generation as an optimization problem that takes into account the topical and the temporal diversity.
Tweets are filtered and are incrementally clustered in two cluster types, namely topical clusters based on content similarity and temporal clusters that depends on publication time.
Summary generation is formulated as integer linear problem in which unknowns variables are binaries, the objective function is to be maximized and constraints ensure that at most one post per cluster is selected with respect to the defined summary length limit.
During the past decades 3D animation widely spread into our everyday life being for entertainment such as video games or movies, or for communication more generally.
Despite its common use, creating animations is still dedicated to skilled animators and not within reach of non-experts.
In addition, the creation process traditionally follow a strict pipeline: after the initial storyboarding and character design phases, articulated characters are modeled, rigged and roughly positioned in 3D, at which point the layout of their relative poses can be established.
Then, their actions and movements are broken down into keyframes which are interpolated to produce the final animation.
Keyframing animations is a hard process during which animators need to spend time and efforts carefully tuning animation curves for each character's degrees of freedom and for each specific action.
They also need to consistently sequence these actions over time in order to convey the desired character's intentions and personality.
Some methods such as motion capture, aim at easing the creation process by directly transferring real recorded human motions to virtual characters.
However, output animations still lack expressiveness and must be fixed by animators.
In this thesis, we propose a way of easing the process of creating animation sequences starting from a database of individual animations.
We focus on animating virtual characters reproducing a story played with props such as figurines instrumented with sensors.
To reach this goal, we propose a new animation pipeline analyzing and transcribing hands motion into a sequence of animations adapted to the user hand space-time trajectories and their motion qualities.
We also introduce a new procedural animation model inferring expressiveness fitting the narrator's hand motion qualities in terms of Laban Time and Weight Effort.
Finally, we extend our system such that it can process multiple characters at a time, detecting and transferring interactions as well as making characters act with respect to pre-defined behaviors, letting users express their narrative creativity.
We conclude with a discussion of future research directions.
In recent years, social network sites exploded in popularity and become an important part of the online activities on the web.
This success is related to the various services/functionalities provided by each site (ranging from media sharing, tagging, blogging, and mainly to online social networking) pushing users to subscribe to several sites and consequently to create several social networks for different purposes and contexts (professional, private, etc.).
Nevertheless, current tools and sites provide limited functionalities to organize and identify relationship types within and across social networks which is required in several scenarios such as enforcing users' privacy, and enhancing targeted social content sharing, etc.
Particularly, none of the existing social network sites provides a way to automatically identify relationship types while considering users' personal information and published data.
In this work, we propose a new approach to identify relationship types among users within either a single or several social networks.
We provide a user-oriented framework able to consider several features and shared data available in user profiles (e.g., name, age, interests, photos, etc.).
This framework is built on a rule-based approach that operates at two levels of granularity: 1) within a single social network to discover social relationships (i.e., colleagues, relatives, friends, etc.) by exploiting mainly photos' features and their embedded metadata, and 2) across different social networks to discover co-referent relationships (same real-world persons) by considering all profiles' attributes weighted by the user profile and social network contents.
At each level of granularity, we generate a set of basic and derived rules that are both used to discover relationship types.
To generate basic rules, we propose two distinct methodologies.
On one hand, social relationship basic rules are generated from a photo dataset constructed using crowdsourcing.
On the other hand, using all weighted attributes, co-referent relationship basic rules are generated from the available pairs of profiles having the same unique identifier(s) attribute(s) values.
To generate the derived rules, we use a mining technique that takes into account the context of users, namely by identifying frequently used valid basic rules for each user.
We present here our prototype, called RelTypeFinder, implemented to validate our approach.
It allows to discover appropriately different relationship types, generate synthetic datesets, collect web data and photo, and generate mining rules.
We also describe here the sets of experiments conducted on real-world and synthetic datasets.
The evaluation results demonstrate the efficiency of the proposed relationship discovery approach.
Large-scale collaborative systems wherein a large number of users collaborate to perform a shared task attract a lot of attention from both academic and industry.
We study the trust assessment problem and aim to design a computational trust model for collaborative systems.
We focused on three research questions.
1. What is the effect of deploying a trust model and showing trust scores of partners to users?
We designed and organized a user-experiment based on trust game, a well-known money-exchange lab-control protocol, wherein we introduced user trust scores.
Our comprehensive analysis on user behavior proved that: (i) showing trust score to users encourages collaboration between them significantly at a similar level with showing nick-name, and (ii) users follow the trust score in decision-making.
The results suggest that a trust model can be deployed in collaborative systems to assist users.
2. How to calculate trust score between users that experienced a collaboration?
We designed a trust model for repeated trust game that computes user trust scores based on their past behavior.
We validated our trust model against: (i) simulated data, (ii) human opinion, and (iii) real-world experimental data.
We extended our trust model to Wikipedia based on user contributions to the quality of the edited Wikipedia articles.
We proposed three machine learning approaches to assess the quality of Wikipedia articles: the first one based on random forest with manually-designed features while the other two ones based on deep learning methods.
3. How to predict trust relation between users that did not interact in the past?
Given a network in which the links represent the trust/distrust relations between users, we aim to predict future relations.
We proposed an algorithm that takes into account the established time information of the links in the network to predict future user trust/distrust relationships.
Our algorithm outperforms state-of-the-art approaches on real-world signed directed social network datasets
With the increasing number of novice users of computer applications, the need for efficient assistance has become critical.
To supply the need, we suggest using an Assistant Conversational Agent (ACA), an interface allowing the use of naturallanguage (used spontaneously when a problem arises) and providing a reassuring presence to the users.
A preliminary study details the constitution (combining collectÎon and the use of thesauri) of a corpus of requests, which need is justified.
This corpus of 11,626 requests is compared with others, and we show that it covers the studied domain of assistance and moreover, con tains requests regarding controlling of the application and chatting with the agent.
Ln output, requests are expressed in a formallanguage (DAFI) for which we provide the syntax and the semantics.
The analyzer is evaluated by comparing a manual annotation and the requests automatically produced, and we consider the use of sorne supervised machine learning approaches in order to identify conversationaJ acti vities.
The methodology followed is validated through the integration of an ACA into an existing Web application for cooperative music prototyping.
Finally, we describe the required architecture for the rational agent in charge of defining the reactions based on the formal requests expressed in DAFT and on the model of the assisted application, emphasizing the need for a cognitive model.
This is an attempt to show that any modelisation brings about losses in meaning.
This is demonstrated through the realisation of a factual and thorough database on a number of company agreements.
The first part presents the notions of information sciences and of modelisation.
The computer tools available for the realisation of documentary applications are then analysed (relation and object-oriented database management systems, markup languages, hypertext, computational linguistics).
This lead of a presentation of up-to-date documentary applications.
The second part of the study deals with the differents modelisations attempts.
A description of the five models realised follows a presentation of company agreements and a study of their structural problems.
The last part describes the objectives and implementation of the ACCORD database realised with PostGres and HTML.
Finally, a methodological approach for the factual treatment of corpuses made up of legal and administrative documents is presented.
The conclusion underlines the interests of collaboration in implementing such applications and the transformations induced in documentation pratices owing to computer net.
A growing number of newsrooms around the world have established fact-checking headings or rubrics.
They are dedicated to assess the veracity of claims, especially by politicians.
This practice revisits an older fact-checking practice, born in the United States in the 1920's and based on an exhaustive and systematic checking of magazines'contents before publishing.
The 'modern'version of fact-checking embodies both the willingness of online newsrooms to restore verified contents —despite the structural and economic crisis of the press— and their ability to capitalize on digital tools which enhance access to information.
Through some thirty semi-structured interviews with French fact-checkers and the study of a sample of 300 articles and chronicles from seven media, this PhD thesis examines the extent to which fact-checking, as a journalistic genre, certainly valorizes a credible method, but also —and indirectly— reveals shortcomings in professional practices.
Finally, it discusses how the promotion of more qualitative content, as well as media literacy, could place fact-checking at the heart of editorial strategies —the latter aiming at retrieving trust from the audience.
For several years now, a new phenomenon related to digital data is emerging: data which are increasingly voluminous, varied and rapid, appears and becomes available, they are often referred to as complex data.
In this dissertation, we focus on a particular type of data: complex sequence of events, by asking the following question: “how to predict as soon as possible and to influence the appearance of future events within a complex sequence of events?”.
We propose DEER: an algorithm for mining episode rules, which has the originality of controlling the horizon of the appearance of future events by imposing a temporal distance within the extracted rules.
In a second phase, we address the problem of emergence detection in an events stream.
We propose EER: an algorithm for detecting new emergent rules as soon as possible.
In order to increase the reliability of new rules, EER relies on the similarity between theses rules and previously extracted rules.
At last, we study the impact carried by events on other events within a sequence of events.
We propose IE: an algorithm that introduces the concept of “influencer events” and studies the influence on the support, on the confidence and on the distance through three proposed measures.
Our work is evaluated and validated through an experimental study carried on a real data set of blogs messages
Human beings naturally organize their space as composed of discrete units.
Those units, called "semantic places", are characterized by their spatial extend and their functional unity.
Recent works in semantic place recognition seek to endow the robot with similar capabilities.
Contrary to classical localization and mapping work, this problem is usually tackled as a supervised learning problem.
First, we combine global image characterization, which captures the global organization of the image, and visual words methods which are usually based unsupervised classification of local signatures.
Our second but closely related, contribution is to use several images for recognition by using Bayesian methods for temporal integration.
Our first model don't use the natural temporal ordering of images.
Temporal integration is very simple but has difficulties when the robot moves from one place to another.
A second model augment the classical Bayesian filtering approach by using the local order among images.
We compare our methods to state-of-the-art algorithms on place recognition and place categorization tasks.
We study the influence of system parameters and compare the different global characterization methods on the same dataset.
These experiments show that our approach while being simple leads to better results especially on the place categorization task.
Variability in Big Data refers to data whose meaning changes continuously.
For instance, data derived from social platforms and from monitoring applications, exhibits great variability.
To achieve that goal, data scientists need: (a) measures to compare data in various dimensions such as age for users or topic for network traffic, and (b) efficient algorithms to detect changes in massive data.
We propose appropriate measures for comparing user opinions in the form of rating distributions, and efficient algorithms that, given an opinion of interest in the form of a rating histogram, discover agreeing and disargreeing populations.
Difference Explanation tackles the question of providing a succinct explanation of differences between two datasets of interest (e.g., buying habits of two sets of customers).
We propose scoring functions designed to rank explanations, and algorithms that guarantee explanation conciseness and informativeness.
Finally, Difference Evolution tracks change in an input dataset over time and summarizes change at multiple time granularities.
We propose a query-based approach that uses similarity measures to compare consecutive clusters over time.
Our indexes and algorithms for Difference Evolution are designed to capture different data arrival rates (e.g., low, high) and different types of change (e.g., sudden, incremental).
The utility and scalability of all our algorithms relies on hierarchies inherent in data (e.g., time, demographic).We run extensive experiments on real and synthetic datasets to validate the usefulness of the three analytical tasks and the scalability of our algorithms.
We show that Difference Exploration guides end-users and data scientists in uncovering the opinion of different user segments in a scalable way.
Difference Explanation reveals the need to parsimoniously summarize differences between two datasets and shows that parsimony can be achieved by exploiting hierarchy in data.
Finally, our study on Difference Evolution provides strong evidence that a query-based approach is well-suited to tracking change in datasets with varying arrival rates and at multiple time granularities.
Similarly, we show that different clustering approaches can be used to capture different types of change.
Humanities challenges computer sciences since 60 years.
The 90's marks a break, announcing qualitative analysis and interpretation of interoperable data, which became «knowledge».
Since 2010, a disillusionment tarnishes the prospects, Digital Hmanities diversity increases.
This method aims at co-creating historical knowledge.
Facing the utopian modeling of qualitative knowledge in history, we designed a pragmatic process: the historian analyses quantitative data of a known corpus, this generates new hypothesis and certainties.
Most natural language processing tasks are modeled as prediction problems where one aims at finding the best scoring hypothesis from a very large pool of possible outputs.
This work aims at understanding the importance of the search space and the possible use of constraints to reduce it in size and complexity.
When information about the possible outputs of a sequence labeling task is available, it may seem appropriate to include this knowledge into the system, so as to facilitate and speed-up learning and inference.
A case study on type constraints for CRFs however shows that using such constraints at training time is likely to drastically reduce performance, even when these constraints are both correct and useful at decoding.
On the other side, we also consider possible relaxations of the supervision space, as in the case of learning with latent variables, or when only partial supervision is available, which we cast as ambiguous learning.
Word order differences between languages pose several combinatorial challenges to machine translation and the constraints on word reorderings have a great impact on the set of potential translations that is explored during search.
We study reordering constraints that allow to restrict the factorial space of permutations and explore the impact of the reordering search space design on machine translation performance.
However, we show that even though it might be desirable to design better reordering spaces, model and search errors seem yet to be the most important issues.
This study aims at the implementation and evaluation of techniques for extracting semantic relations from a multilingual aligned corpus.
Firstly, our observations will focus on the semantic comparison of translational equivalents in multilingual aligned corpus.
From these equivalences, we will try to extract "cliques", which ara maximum complete related sub-graphs, where all units are interrelated because of a probable semantic intersection.
These cliques have the advantage of giving information on both the synonymy and polysemy of units, and providing a form of semantic disambiguation.
Secondly, we attempt to link these cliques with a semantic lexicon (like WordNet) in order to assess the possibility of recovering, for the Arabic units, a semantic relationships already defined for English, French or Spanish units.
These relations would automatically build a semantic resource which would be useful for different applications of NLP, such as Question Answering systems, machine translation, alignment systems, Information Retrieval…etc.
The various stakeholders who describe study and implement a complex system require viewpoints that are dedicated to their concerns.
However, in the context of Model-Driven Engineering, approaches to define and implement those viewpoints are either too rigid and inappropriate or completely ad hoc.
In addition, those various viewpoints are rarely independent from each other.
Therefore, we must strive to identify and describe the relationships/correspondences between the viewpoints in order to be able to verify that the parts of the solution given by the various stakeholders form a consistent whole.
The work presented in this thesis provides a way to define dedicated languages based on UML for the viewpoints.
For this, a method that analyzes the semantics of the textual descriptions of the concepts of the domain we want to map to UML has been implemented to facilitate the definition of UML profiles.
To define a viewpoint based on some UML profiles, this thesis provides a method that lets the methodologist make explicit the viewpoint he/she wants.
A tool can then generate the tooling that implements this viewpoint in a modeling environment together with the corresponding dedicated language while current practice is based on an implementation essentially manual.
To assist the identification of relationships between the viewpoints, this thesis proposes again to analyze the semantics of textual descriptions of concepts of the languages used by the viewpoints.
Used in combination with existing syntactic heuristics, the proposed approach provides good results when the terminologies of the languages that are analyzed are far apart.
A theoretical framework based on category theory is provided to make explicit formally correspondences.
To use this framework, a category for languages based on UML has been proposed.
To be able to make explicit the correspondences between the models of those languages as well, the category of OWL ontologies is used.
A solution is proposed to characterize correspondences that are more complex than the simple equivalence relationship.
This theoretical framework provides a way to define formally complex relationships that can be used to verify the consistency of the architectural description.
Once the description of the architecture has been integrated according to the formal correspondences, the issue of consistency is considered.
The experiments carried out on a concrete test case to verify consistency on a syntactic perspective give satisfactory practical results.
The experiments carried on the same test case to verify consistency on a semantic perspective don't give satisfactory practical results.
Our research examines the role of online offers credibility in the customer judgment process and its determinants.
294 persons were requested to express their perception of online book offers credibility which were composed varying on their source and also on a number of reputation signals.
This quasi-experiment aims for measuring impact of these factors on offer credibility but also for investigating influence of this one on consumer interest in offer and consequently on his purchase intention.
Statistical analysis bases on a SEM model.
The most important research finding shows that credibility mediates effects of reputation signals on consumer interest in offer.
This element of interest, in turn, executes mediating effect on the causal relationship between credibility and purchase intention.
We found otherwise that credibility is also affected by consumer trust on the source and his interest manifested on the product category.
Effect of trust is positive and remarkable while interest in the category effect is negative, which reveals that people with better knowledge level may be more skeptic than others in judgment process.
According to these results, our proposal on the importance of credibility element in e-commerce activities is clear: look after the offer credibility is the nexus of merchants selling performance.
Artificial Intelligence is the field of research aiming at mimicking or replacing human cognitive abilities.
As such, one of its subfields is focused on the progressive automation of the programming process.
In other words, the goal is to transfer cognitive load from the human to the system, whether it be autonomous or guided by the user.
In this thesis, we investigate the conditions for making a user-guided system autonomous using another subfield of Artificial Intelligence: Machine Learning.
In our work, the requests are in written French, and the associated actions are represented by corresponding instructions in a programming language (here R and bash).
The learning is performed using a set of examples composed by the users themselves while interacting.
Thus they progressively define the most relevant actions for each request, making the system more autonomous.
We collected several example sets for evaluation of the learning methods, analyzing and reducing the inherent collection biases.
The proposed protocol is based on incremental bootstrapping of the system, starting from an empty or limited knowledge base.
As a result of this choice, the obtained knowledge base reflects the user needs, the downside being that the overall number of examples is limited.
To avoid this problem, after assessing a baseline method, we apply a case base reasoning approach to the request to command transfer problem: formal analogical reasoning.
We show that this method yields answers with a very high precision, but also a relatively low coverage.
We explore the analogical extension of the example base in order to increase the coverage of the provided answers.
The running delay of the simple analogical approach is already around 1 second, and is badly influenced by both the automatic extension of the base and the relaxation of the constraints.
Finally, the incremental operational assistant based on analogical reasoning was tested in simulated incremental condition in order to assess the learning behavior over time.
The system reaches a stable correct answer rate after a dozen examples given in average for each command type.
Although the effective performance depends on the total number of accounted commands, this observation opens interesting applicative tracks for the considered task of transferring from a rich source domain (natural language) to a less rich target domain (programming language).
This thesis presents a formal framework for the representation of Signed Languages (SLs), the languages of Deaf communities, in semi-automatic recognition tasks.
SLs are complex visio-gestural communication systems; by using corporal gestures, signers achieve the same level of expressivity held by sound-based languages like English or French.
However, unlike these, SL morphemes correspond to complex sequences of highly specific body postures, interleaved with postural changes: during signing, signers use several parts of their body simultaneously in order to combinatorially build phonemes.
This situation, paired with an extensive use of the three-dimensional space, make them difficult to represent with tools already existent in Natural Language Processing (NLP) of vocal languages.
A multi-modal logic was chosen as the basis of the formal language: the Propositional Dynamic Logic (PDL).
This logic was originally created to specify and prove properties on computer programs.
In particular, PDL uses the modal operators [a] and "a"; to denote necessity and possibility, respectively.
For SLs, a particular variant based on the original formalism was developed: the PDL for Sign Language (PDLSL).
With the PDLSL, body articulators (like the hands or head) are interpreted as independent agents; each articulator has its own set of valid actions and propositions, and executes them without influence from the others.
Together, the use of PDLSL and the proposed specialized data structures could help curb some of the current problems in SL study; notably the heterogeneity of corpora and the lack of automatic annotation aids.
On the same vein, this may not only increase the size of the available datasets, but even extend previous results to new corpora; the framework inserts an intermediate representation layer which can serve to model any corpus, regardless of its technical limitations.
Afterwards, a formal verification algorithm may be able to find those features in corpora, as long as they are represented as consistent LTSs.
Finally, the development of the formal framework led to the creation of a semi-automatic annotator based on the presented theoretical principles.
The final product, is an automatically generated sub-lexical annotation, which can be later corrected by human annotators for their use in other areas such as linguistics.
Lexicon is widely acknowledged as a very important component of any Natural Language Processing system, and the use of lexical resources is growing rapidly.
Resolving Prepositional Phrase Attachment Ambiguity is known as a bottleneck in automatic parsing, and nowadays most work use corpus-based lexical resources while using existing intuition-based dictionaries is not so common.
Assessing how well a lexical resource resolves Prepositional Phrase Attachment Ambiguity is mainly performed on a single corpus; therefore, very little work has been done on adapting lexical resources to the type of corpus.
In our study, we build two types of corpus: one is based on an existing dictionary (Lexicon-Grammar), the other is corpus-based (a 200 million word newspaper corpus).
We put forward some linguistic characteristics for each of the five corpora which help to understand why the performance of each lexicon varies according to the corpus.
Adapting the type of lexicon resource to be used on a given corpus is made more obvious as we assess how the corpus-based lexicon performs compared with a specialised lexicon acquired from each of the five test corpora.
This thesis addresses different aspects around the market microstructure modelling and market making problems, with a special accent from the practitioner's viewpoint.
We wish to improve the knowledge of LOB for the research community, propose new modelling ideas and develop concrete applications to the interest of Market Makers.
We would like to specifically thank the Automated Market Making team for providing a large high frequency database of very high quality as well as a powerful computational grid, without whom these researches would not have been possible.
The first chapter introduces the incentive of this research and resumes the main results of the different works.
Chapter 2 fully focuses on the LOB and aims to propose a new model that better reproduces some stylized facts.
Through this research, not only do we confirm the influence of historical order flows to the arrival of new ones, but a new model is also provided that captures much better the LOB dynamic, notably the realized volatility in high and low frequency.
In chapter 3, the objective is to study Market Making strategies in a more realistic context.
High-frequency prediction with deep learning method is studied in chapter 4.
Many results of the 1-step and multi-step prediction have found the non-linearity, stationarity and universality of the relationship between microstructural indicators and price change, as well as the limitation of this approach in practice.
The recognition of French Sign Language (LSF) as a natural language in 2005 has created an important need for the development of tools to make information accessible to the deaf public.
With this prospect, this thesis aims at linguistic modeling for a system of generation of LSF.
We first present the different linguistic approaches aimed at describing the sign language (SL).
We then present the models proposed in computer science.
In a second step, we propose an approach allowing to take into account the linguistic properties of the SL while respecting the constraints of a formalisation process.
By studying the links between semantic functions and their observed forms in LSF Corpora, we have identified several production rules.
We finally present the rule functioning as a system capable of modeling an entire utterance in LSF.
This PhD thesis takes place within the framework of the speech recognition in audio contents.
The purpose of this work is to adapt the principles of audio identification to speech recognition as well as to design and to develop robust identification techniques.
Audio identification systems by audio fingerprinting are designed to music track indexation but do not handle the specificities of the speech signal.
At first, various methods of audio identification by fingerprint are studied as well as a first work of adaptation to speech recognition.
New types of subfingerprint based on usual speech parameters are then proposed.
Secondly, the various types of variability of the speech signal are described as well as the main parameters of acoustic representation of the speech signal.
The robustness of various types of subfingerprint in extrincic variability and in intrinsic variability is estimated.
In the presence of disturbances related to the environment and to the conditions of transmission of the speech signal (CTIMIT), a type of subfingerprint stemming from the audio identification turns out then the most robust.
Ambient assisted living aims to support the aging population.
This is particularly the case with smart homes, equipped with multiple connected sensors, which enables to extend home care for the elderly.
The manuscript begins by introducing the general problem of smart homes, after presenting further the three sub-themes that are the subject of the thesis, namely the activity recognition, privacy and dialogue systems.
Activity recognition is the process of determining the day-to-day activities of a person or a group of people from the (raw) sensor data that the home is equipped with.
An example of this is the detection of a person's fall.
A smart home is typically based on the Internet of Things (IoT).
Many data are produced, which may contain private or sensitive information.
Some of this data must be shared externally, which may pose privacy issues.
Finally, a natural way of communication for the user is to use the dialogue to interact with the smart home via dialogue manager.
This thesis proposes contributions on these three sides, most of them based on deep learning.
The concept of complex IT systems includes all systems consisting of a large number of inter-connected and computer-managed components.
The configuration and management of these systems involves a multitude of tasks that are critical to their proper functioning and their evolution.
We also propose an extension to the «Planning Domain Definition Language» (PDDL) in order to modelize the knowledge of domain experts in the form of tasks decomposition methods that will be used to guide the HTN planning algorithm.
Finally, we propose a panel of evaluation criteria of mixed-initiative systems that serve as a basis for the discussion about the performances and contributions of the MIP system.
This study focuses on Wolof medical terminology under the lexical semantics context.
We talk about terminology units, particularly collocations.
This choice has a direct link with our theoretical framework of analysis, the Meaning-Text Theory (MTT), which currently offers one of the best tools for describing collocation through Lexical Functions.
Collocations constitute indices of specialization and have a singular lexico-syntactic functioning.
We analyze them, on the basis of compiled scientific corpus, in order to have a holistic perception of lexical co-occurrence.
Languages in Africa are often poorly endowed from a terminology view.
This research is based on the lexical analysis model which takes into account three key parameters: meaning, form and combinatorics, to make a description of the Wolof lexicon which, in the long run, gives principles of terminology.
The translational scope of the work lies in the interlinguistic approach we adopt to develop our list of terms.
The operative side of the study is the constitution of a beginning of trilingual medical corpus (English-French-Wolof).
The translation perspective of the terminology has revealed different processes to create and restitute medical terms (English and French) into Wolof.
Over the past 10 years, mobile phones have evolved considerably: the advent of touch screens and the disappearance of physical keyboards have changed the way we interact with these devices in our daily lives.
However, text input is still an important task though activities such as taking notes, sending text messages or communicating on social networks.
However, while using touch screen has great advantages in terms of dynamical interfaces and customization, such devices are not necessarily accessible to all.
Indeed, for 39 million people worldwide affected by blindness, there are many difficulties related to the devices that will make such interactions difficult due to the lack of tactile reference: interactions with the device are possible but they are often laborious and repetitive, which then implies a lot of cognitive load, accuracy, memory and fatigue related problems.
In this PhD thesis, we focused on the accessibility of text input interactions in the context of visual impairment.
First, we studied the currently existing solutions designed for impaired users.
The main issue of this research was to improve text input so that users have a better typing experience.
As such, we designed a deductive solution called DUCK.
This solution allows visually impaired to quickly enter text without worrying about how accurate their input is.
A basic language-based modelling system allows at the each end of a word what the user wanted to type.
This device was then tested with a sample of visually impaired people to assess the effectiveness of our solution.
Further work was subsequently focused on two main optimizations.
The first work focused on word lists.
We studied and compared different interactions to allow the user to navigate and choose words effectively and easily when faced with a list of words proposed by a predictive or deductive system.
The second optimization focuses on entering commonly used words.
We also conducted a comparative study of different interactions models to type a short word efficiently without using the deduction system, which would be too time consuming for such words.
Finally, we finish this PhD project by a longitudinal study that shows the DUCK keyboard with the integration of these optimizations.
This new system has been used by visually impaired over a period of two weeks to study the effectiveness of the keyboard over the long term.
This thesis attempts to explore the relationship between language disorders and the way they are named by speech and language therapists.
The French labels dysphasie, troubles spécifiques du langage écrit, aphasie, difficultés à l'apprentissage du langage écrit, dyslexie, retard de langage, relating to Specific Language Impairment, disorders of the written language, aphasia, dyslexia... belong to speech and language therapy (SLT) terminology and have always evolved following theoretical movements.
In order to describe the inconsistency of the link between diagnostic labels and the reality of the labelled pathology, some epistemological, lexicological and terminological issues have been explored.
Assessments enable SLT practitioners to make a diagnosis, following which they write a report (CRBO). These reports can be considered as the reflection of the representations speech and language therapists (SLTs) have of the disorders, and show how they use their own terminology.
435 authentic reports have been semi-automatically analysed descriptively using lexicological and terminological tools. The XML encoding captured the current use of the terms describing any pathology encountered by SLTs.
The results show two terminological groups (one relating to the nature of the disorder, the other relating to its form), which illustrate the necessary nuances the SLT uses with these collocational phrases.
The final phase of the analyses helped to produce a framework for a new speech and language therapy classification based on clinical practice (COFOP).
This thesis contributes to the understanding of an emergent and complex phenomenon: transformation/smartization of the urban public service system.
This involves the cross combination of different services sectors such as transport, tourism, taxes, etc.
We define three pillars of service science: Service System (SS), Service Innovation (IS) and Institutional Service Logic (LIS).
We propose a method based on Latent Semantic Analysis (LSA), Factor Analysis (FA), text mining and grounded theory to inductively reveal 30 years of evolution of interdisciplinary SS, IS and LIS intellectual structure from 1986 to 2015.
Then, we mobilize complex thinking as an integrating framework of components of the urban public service system that becomes smart, as a whole and its parts.
Our research design is based on grounded theory ; observations, longitudinal case study with a multi-level approach (i.e. local and national) ; the dialogic model (Parmentier-Cajaiba &amp; Avenier, 2013) and the pragmatic constructivism epistemological paradigm (PECP).
We define two working ontological hypotheses: the relational ontology and becoming ontology.
From the theoretical point of view, our research contributes to the theoretical refinement of the literature on SS, IS and LIS.
We propose three heuristic models from a process and content analysis (Baines et al. 2017) and grounded theory (Gioia &amp; Chittipeddi, 1991 ;Gioia et al., 2013).
The first heuristic model contributes to the understanding of the institutional work process for the creation of institutional arrangements (Standard, boundary resources, APIs) between two antagonistic and complementary collective logics (Morin, 2005 ; Smets &amp; Jarzabkowski, 2013 ; Greenwood et al., 2017): the service-dominant logic of the market (Lusch &amp; Nambisan, 2015 ; Vargo &amp; Lusch, 2016) and the public service logic (Osborne et al., 2015 ; Osborne, 2017).
The second heuristic model highlights components of a smart public service system.
The third heuristic model highlights the drivers and barriers of institutional and structural transformation.
For a long time, polysemy used to be considered as a marginal or accidental phenomenon in language. Where as today, it is well known that polysemy is being part of linguistic systems.
From the distinction established by G. Kleiber (1999), we consider two major trends in accordance with the way they conceive the link between meaning, reference and polysemy.
On one hand, polysemy is described in terms of one basic referential sense from which secondary senses derive (objectivism).
On the other hand, polysemy is analyzed as an areferential semantic potential from which senses emerge by contextual mechanisms (constructivism).
On the basis of D. Tuggy's works (1993), we propose to organize the conceptual modelings of multiple meanings words along a continuum homonymy-polysemy-multifaciality-vagueness, in function of various parameters: entrenchment, cognitive salience, possibility of accessibility and of activation of the network components (schematic or elaborated values).
So, we can highlight some organizational regularities specific to the semantic representation of polysems as well as a typology of polysemous senses.
In Cognitive Grammar, it is a non modulary, compositional and dynamic process.
The analysis of Adj-N and N-Adj noun phrases puts to the fore some regularities governing the activation of polysemic senses.
These regularities are linked to the linguistic context (position and function of the adjective towards the qualified substantive) and to the extra-linguistic context.
In the field of machine learning, deep neural networks have become the inescapable reference for a very large number of problems.
These systems are made of an assembly of layers,performing elementary operations, and using a large number of tunable variables.
Using data available during a learning phase, these variables are adjusted such that the neural network addresses the given task.
It is then possible to process new data.
To achieve state-of-the-art performance, in many cases these methods rely on a very large number of parameters, and thus large memory and computational costs.
Therefore, they are often not very adapted to a hardware implementation on constrained resources systems.
Moreover, the learning process requires to reuse the training data several times, making it difficult to adapt to scenarios where new information appears on the fly.
In this thesis, we are first interested in methods allowing to reduce the impact of computations and memory required by deep neural networks.
Secondly, we propose techniques for learning on the fly, in an embedded context.
As we enter the era of digitalization, it is important to transform data into knowledge and use it to provide avenues for industrial improvement.
The transformation of data into knowledge to optimize production is today a major industrial challenge.
We will address the issues of scheduling, planning and load balancing on and between production lines.
From a system point of view, these aspects are now considered as supported by ERP (Enterprise Resource Planning).
ERPs are tools that are highly rigid in their structure and operation, imposing this rigidity on organizations.
Our partner, iFAKT, an expert in load balancing, will support us in this thesis project.
This thesis addresses two main areas of work: one on the integration of natural language processing, machine learning and a load balancing tool and the other on the actions to be taken in relation to this integration.
For the implementation of these two major activities, we will contribute to the creation of methodologies combining the techniques and tools mentioned above within the framework of industry 4.0
Large industrial players have incorporated them into their production lines, but smaller companies hesitate due to high initial costs and the lack of programming expertise.
In this thesis we introduce a framework that combines two disciplines, Programming by Demonstration and Automated Planning, to allow users without programming knowledge to program a robot.
The user constructs the robot's knowledge base by teaching it new actions by demonstration, and associates their semantic meaning to enable the robot to reason about them.
My dissertation aims to observe and describe the acquisition of reference marks in written production among students from CE2 to CM2 (age 9 to 11) in narrative texts.
This study makes it establish a map of students' competencies in terms of referential continuity according to their class level.
Much research in linguistics have focused on the question of reference from the point of view of analysis and reception, but very little from the production point of view.
In the didactics of French as a first language, studies that analyse referential expression in student texts mainly examine a limited sample of texts.
Furthermore, psycholinguistic studies focusing on reference from a developmental point of view are more numerous for oral than for written studies and generally concern the youngest children (ages 0 to 3).
In my dissertation, I lead the study from a progressive perspective.
Indeed, I consider the development of students' writing skills by studying the processes they use to introduce and maintain referents in a narrative text.
In particular, I explore whether students from CE2 to CM2 (9 to 11 years old) favour anaphoric relations (Reichler-Béguelin, 1988) or reference chains (Schnedecker, 1997 and if age and referent characteristics influence their choice.
The pupil's linguistics marks analysis shows that class level and referent characteristics influence the number and nature of the coreferential expressions used.
XML has become a standard for representation and exchange of data across the web.
Replication of data within different sites is used to increase the availability of data by minimizing the access's time to the shared data.
However, the safety of the shared data remains an important issue.
The aim of the thesis is to propose some models of XML access control that take into account both read and update rights and that overcome limitations of existing models.
We consider the XPath language and the XQuery Update Facility to formalize respectively user access queries and user update operations.
The last part of this thesis studies the practicality of our proposals.
Firstly, we present our system, called SVMAX, that implements our solutions and we conduct an extensive experimental study, based on real-life DTD, to show that it scales well.
Many native XML databases systems (NXD systems) have been proposed recently that are aware of the XML data structure and provide efficient manipulation of XML data by the use of most of W3C standards.
Finally, we show that our system can be integrated easily and efficiently within a large set of NXD systems, namely BaseX, Sedna and eXist-db.
To the best of our knowledge, SVMAX is the first system for securing XML data in the presence of arbitrary DTDs (recursive or not), a significant fragment of XPath and a rich class of XML update operations
During the first millennium BC, the already existing libraries needed to organize texts preservation, and were thus immediately confronted with the difficulties of indexation.
The use of a title occurred then as a first solution, enabling a quick identification of every work, and in most of the cases, helping to discern works thematically close to a given one.
While in Ancient Greece, titles have had a little informative function, although still performing an identification function, the invention of the printing office with mobile characters (Gutenberg, XVth century AD) dramatically increased the number of documents, which are today spread on a large-scale.
But how some words can have a so big influence?
What functions do the titles have to perform at this beginning of the XXIth century?
How can one automatically generate titles respecting these functions?
The automatic titling of textual documents is one of the key domains of Web pages accessibility (W3C standards) such as defined in a standard given by associations about the disabled.
For a given reader, the goal is to increase the readability of pages obtained from a search, since usual searches are often disheartening readers who must supply big cognitive efforts.
For a Website designer, the aim is to improve the indexation of pages for a more relevant search.
Other interests motivate this study (titling of commercial Web pages, titling in order to automatically generate contents, titling to bring elements to enhance automatic summarization).
In this study, we use NLP (Natural Language Processing) methods and systems.
While numerous works were published about indexation and automatic summarization, automatic titling remained discreet and knew some difficulties as for its positioning in NLP.
We support in this study that the automatic titling must be nevertheless considered as a full task.
Having defined problems connected to automatic titling, and having positioned this task among the already existing tasks, we provide a series of methods enabling syntactically correct titles production, according to several objectives.
In particular, we are interested in the generation of informative titles, and, for the first time in the history of automatic titling, we introduce the concept of catchiness.
Our TIT' system consists of three methods (POSTIT, NOMIT, and CATIT), that enables to produce sets of informative titles in 81% of the cases and catchy titles in 78% of the cases.
We live in a world where a vast amount of data is being continuously generated.
Data is different than simple numerical information, it now comes in a variety of forms.
However, isolated data is valueless.
But when this huge amount of data is connected, it is very valuable to look for new insights.
At the same time, data is time sensitive.
The most accurate and effective way of describing data is to express it as a data stream.
If the latest data is not promptly processed, the opportunity of having the most useful results will be missed.
So a parallel and distributed system for processing large amount of data streams in real time has an important research value and a good application prospect.
This thesis focuses on the study of parallel and continuous data stream Joins.
We divide this problem into two categories.
Local grammars constitute a descriptive formalism of linguistic phenomena and are commonly represented using directed graphs.
Local grammars are used to recognize and extract patterns in a text, but they had some inherent limits in dealing with unexpected variations as well as in their capacity to access exogenous knowledge, in other words information to extract, during the analysis, from external resources and which may be useful to normalize, enhance validate or link the recognized patterns.
The means are twofold: on the one hand, it is achieved by adding arbitrary conditional-functions, called extended functions, which are not predefined in advance and are evaluated from outside of the grammar.
In the first part, we study the principles regarding the construction of the extended local grammars.
Then, we present a proof-of-concept of a corpus-processing tool which implements the proposed formalism.
Finally, we study some techniques to extract information from both well-formed and noisy texts.
We focus on the coupling of external resources and non-symbolic methods in the construction of our grammars and we highlight the suitability of this approach in order to overcome the inherent limitations of classical local grammars
Automatic video understanding is expected to impact our lives through many applications such as autonomous driving, domestic robots, content search and filtering, gaming, defense or security.
Video content is growing faster each year, for example on platforms such as YouTube, Twitter or Facebook.
Automatic analysis of this data is required to enable future applications.
Video analysis, especially in uncontrolled environments, presents several difficulties such as intraclass variability (samples from the same concept appear very differently) or inter-class confusion (examples from two different activities look similar).
While these problems can be addressed with the supervised learning algorithms, fully-supervised methods are often associated with high annotation cost.
Depending on both the task and the level of required supervision, the annotation can be prohibitive.
For example, in action localization, a fully-supervised approach demands person bounding boxes to be annotated at every frames where an activity is performed.
The cost of getting such annotation prohibits scalability and limits the number of training samples.
This thesis addresses above problems in the context of two tasks, namely human action classification and localization.
The former aims at recognizing the type of activity performed in a short video clip trimmed to the temporal extent of the action.
The latter additionally extracts the space-time locations of potentially multiple activities in much longer videos.
Our approach to action classification leverages information from human pose and integrates it with appearance and motion descriptors for improved performance.
Our approach to action localization models the temporal evolution of actions in the video with a recurrent network trained on the level of person tracks.
Finally, the third method in this thesis aims to avoid a prohibitive cost of video annotation and adopts discriminative clustering to analyze and combine different levels of supervision.
The Natural Language Processing (NLP) has technically improved regarding human speech vocabulary extension, morphosyntax scope, style and aesthetic.
Affective Computing also tends to integrate an “emotional” dimension with a common goal shared with NLP which is to disambiguate the natural language and increase the human-machine interaction naturalness.
Within social robotics, the interaction is modelled in dialogue systems trying to reach out an attachment dimension which effects need to an ethical and collective control.
However, the situated natural language dynamics is undermining the automated system's efficiency, which is trying to respond with useful and suitable feedbacks.
This thesis hypothesis supposes the existence of a “socio-affective glue” in every interaction, set up in between two individuals, each with a social role depending on a communication context.
This glue is so the consequence of dynamics generated by a process which mechanisms rely on an altruistic dimension, but independent of dominance dimension as seen in emotions studies.
This glue would allow the exchange of the language events between interlocutors, by regularly modifying their relation and their role, which is changing themselves this glue, to ensure the communication continuity.
A wizard of oz approach – EmOz – is used to control the vocal primitives proposed as the only language tools of a Smart Home butler robot interacting with relationally isolated elderly.
The relational isolation allows the dimensions the socio-affective glue in a contrastive situation where it is damaged.
We could thus observe the primitives' effects through multimodal language cues.
If the proposed primitives could have a real effect on the glue, the automated system will be able to train the persons to regain some unfit mechanisms underlying their relational construction, and so possibly increase their desire to communicate with their human social surroundings.
The results from the collected EEE corpus show the relation changes through various interactional cues, temporally organised.
These denoted parameters tend to build an incremental dialogue system in perspectives – SASI.
The first steps moving towards this system reside on a speech recognition prototype which robustness is not based on the accuracy of the recognised language content but on the possibility to identify the glue degree (i.e. the relational state) between the interlocutors.
Thus, the recognition errors avoid the system to be rejected by the user, by tempting to be balanced by this system's adaptive socio-affective intelligence.
Prostate cancer is the most frequent and the fourth leading cause of mortality in France.
Actual diagnosis methods are often insufficient in order to detect and precisely locate cancer.
Multiparametrics MRI is now one of the most promising method for accurate follow-up of the disease.
However, the visual interpretation of MRI is not easy and it is shown that there is strongvariability among expert radiologists to perform diagnosis, especially when MR sequences are contradictory.
Under these circumstances, a strong interest is for Computer-aided diagnosis systems (CAD) aiming at assisting expert radiologist in their final decision.
This thesis presents our work toward the conception of a CADe which final goal is to provide a cancer probability map to expertradiologist.
This thesis focuses both for cancer detection and characterization in order to provide a cancer probability map correlated to cancer aggressiveness (Gleason score).
To that end we used a dictionary learning method to extract new features to better characterize cancer aggressiveness signatures as well as image features.
Those features are then used as an input to Support Vector Machines (SVM) and Logistic Regression (LR) classifiers to produce a cancer probability map.
We then focused on discriminating agressive cancers (Gleason score &gt;6) from other tissues and provided an analysis of the correlation between cancer aggressiveness and probabilities.
Our work conclude on a strong capability to distinguish agressive cancer from other tissues but fails to precisely distinguish different grades of cancers
With the increasing awareness about the problem of climate change and the high level of energy consumption, a need for energy efficiency has emerged especially for electric power consumptions in buildings.
Monitoring and understanding the electrical consumption of appliances can also be useful for predictive maintenance, power quality analyses, demand forecasting or occupancy detection.
Thirty years ago, a method called Non Intrusive Load Monitoring (NILM) has been introduced.
It consists of estimating individual appliance energy consumptions from the measurement of the total consumption of the building.
Its main advantage over traditional sub-metering methods is to use a single electric power meter at the main breaker of the building and then use a disaggregation algorithm to separate the contributions of each appliance.
The goal of this thesis is to address the algorithmic challenge offered by NILM.
Its main difficulties are: (i) the standardization of the formulation, (ii) the ill-posedness of the problem, (iii) the lack of knowledge and (iv) the machine learning algorithm design.
All our contributions follow from the principal objective that is to solve the NILM problem for huge systems such as commercial or industrial buildings using high frequency current and voltage measurements.
However, houses and the specific equipment found inside these buildings are not excluded of the study.
This thesis is split into two parts.
In the first part, we tackle the lack of knowledge and datasets for NILM in commercial buildings.
First of all, the NILM community has mostly focused on both residential NILM application and using low frequency data provided by power meter installed by utility providers.
Our study on the rank of current matrix conducted for individual devices will serve as the base of a new device taxonomy and to prior assumptions on the rest of this thesis.
Secondly, we address the lack of datasets especially for commercial buildings by developping an algorithm for generating synthetic current data based on a modelization of the current flowing through an electrical device.
To encourage research on commercial buildings we release a synthesized dataset called SHED that can be used to evaluate NILM algorithms.
In the second part, we deal with the NILM software challenges by exploring unsupervised source separation techniques.
Motivated by the nature of the current signals, it uses a regularization term on the temporal variations of the activation matrix and a positivity constraint, and the columns of the signature matrix are constrained to lie in a specific set.
To solve the resulting optimization problem, we rely on an alternating minimization strategy involving dual optimization and quasi-Newton algorithms.
IVMF is the first proposed algorithm especially designed for high frequency NILM in huge buildings.
We finally show that IVMF outperforms competing methods (Independent Component Analysis, Semi Non-negative Matrix Factorization) on NILM datasets.
Since 2006, deep learning algorithms which rely on deep architectures with several layers of increasingly complex representations have been able to outperform state-of-the-art methods in several settings.
Deep architectures can be very efficient in terms of the number of parameters required to represent complex operations which makes them very appealing to achieve good generalization with small amounts of data.
Although training deep architectures has traditionally been considered a difficult problem, a successful approach has been to employ an unsupervised layer-wise pre-training step to initialize deep supervised models.
First, unsupervised learning has many benefits w.r.t. generalization because it only relies on unlabeled data which is easily found.
Second, the possibility to learn representations layer by layer instead of all layers at once improves generalization further and reduces computational time.
However, deep learning is a very recent approach and still poses a lot of theoretical and practical questions concerning the consistency of layer-wise learning with many layers and difficulties such as evaluating performance, performing model selection and optimizing layers.
In this thesis we first discuss the limitations of the current variational justification for layer-wise learning which does not generalize well to many layers.
We ask if a layer-wise method can ever be truly consistent, i.e. capable of finding an optimal deep model by training one layer at a time without knowledge of the upper layers.
We find that layer-wise learning can in fact be consistent and can lead to optimal deep generative models.
We prove that maximizing this criterion for each layer leads to an optimal deep architecture, provided the rest of the training goes well.
Although this criterion cannot be computed exactly, we show that it can be maximized effectively by auto-encoders when the encoder part of the model is allowed to be as rich as possible.
This gives a new justification for stacking models trained to reproduce their input and yields better results than the state-of-the-art variational approach.
Additionally, we give a tractable approximation of the BLM upper-bound and show that it can accurately estimate the final log-likelihood of models.
Taking advantage of these theoretical advances, we propose a new method for performing layer-wise model selection in deep architectures, and a new criterion to assess whether adding more layers is warranted.
As for the difficulty of training layers, we also study the impact of metrics and parametrization on the commonly used gradient descent procedure for log-likelihood maximization.
We show that gradient descent is implicitly linked with the metric of the underlying space and that the Euclidean metric may often be an unsuitable choice as it introduces a dependence on parametrization and can lead to a breach of symmetry.
To mitigate this problem, we study the benefits of the natural gradient and show that it can restore symmetry, regrettably at a high computational cost.
We thus propose that a centered parametrization may alleviate the problem with almost no computational overhead.
This thesis focuses on the synthesis of motion capture data with statistical models.
Our starting point lies in two main problems one encounters when dealing with motion capture data synthesis, ensuring realism of postures and motion, and handling the large variability in the synthesized motion.
We first describe an attempt to extend contextual Hidden Markov Models for handling variability in the data by conditioning the parameters of the models to an additional contextual information such as the emotion which which a motion was performed.
We then propose a variant of a traditional method for performing a specific motion synthesis task called Inverse Kinematics, where we exploit Gaussian Processes to enforce realism of each of the postures of a generated motion.
These preliminary results show some potential of statistical models for designing human motion synthesis systems.
Yet none of these technologies offers the flexibility brought by neural networks and the recent deep learning revolution.
The second part of the thesis describes the works we realized with neural networks and deep architectures.
It builds on recurrent neural networks for dealing with sequences and on adversarial learning which was introduced very recently in the deep learning community for designing accurate generative models for complex data.
We propose a simple system as a basis synthesis architecture, which combines adversarial learning with sequence autoencoders, and that allows randomly generating realistic motion capture sequences.
Starting from this architecture we design few conditional neural models that allow to design synthesis systems that one can control up to some extent by either providing a high level information that the generated sequence should match (e.g. the emotion) or by providing a sequence in the style of which a sequence should be generated.
The automated treatment of familiar objects, either natural or artifacts, always relies on a translation into entities manageable by computer programs.
The choice of these abstract representations is always crucial for the efficiency of the treatments and receives the utmost attention from computer scientists and developers.
However, another problem rises: the correspondence between the object to be treated and "its" representation is not necessarily one-to-one!
Therefore, the ambiguous nature of certain discrete structures is problematic for their modeling as well as their processing and analysis with a program.
Natural language, and in particular its textual representation, is an example.
The subject of this thesis is to explore this question, which we approach using combinatorial and geometric methods.
These methods allow us to address the problem of extracting information from large networks of entities and to construct representations useful for natural language processing.
Firstly, we start by showing combinatorial properties of a family of graphs implicitly involved in sequential models.
These properties essentially concern the inverse problem of finding a sequence representing a given graph.
The resulting algorithms allow us to carry out an experimental comparison of different sequential models used in language modeling.
Secondly, we consider an application for the problem of identifying named entities.
Following a review of recent solutions, we propose a competitive method based on the comparison of knowledge graph structures which is less costly in annotating examples dedicated to the problem.
We also establish an experimental analysis of the influence of entities from capital relations.
This analysis suggests to broaden the framework for applying the identification of entities to knowledge bases of different natures.
These solutions are used today in a software library in the banking sector.
Then, we perform a geometric study of recently proposed representations of words, during which we discuss a geometric conjecture theoretically and experimentally.
This study suggests that language analogies are difficult to transpose into geometric properties, and leads us to consider the paradigm of distance geometry in order to construct new representations.
Finally, we propose a methodology based on the paradigm of distance geometry in order to build new representations of words or entities.
We propose algorithms for solving this problem on some large scale instances, which allow us to build interpretable and competitive representations in performance for extrinsic tasks.
More generally, we propose through this paradigm a new framework and research leadsfor the construction of representations in machine learning.
This PhD thesis consists in jointly analyzing eye-tracking signals and multi-channel electroencephalograms (EEGs) acquired concomitantly on participants doing an information collection reading task in order to take a binary decision-is the text related to some topic or not?
Textual information search is not a homogeneous process in time-neither on a cognitive point of view, nor in terms of eye-movement.
On the contrary, this process involves several steps or phases, such as normal reading, scanning, careful reading-in terms of oculometry-and creation and rejection of hypotheses, confirmation and decision-in cognitive terms.
In a first contribution, we discuss an analysis method based on hidden semi-Markov chains on the eye-tracking signals in order to highlight four interpretable phases in terms of information acquisition strategy: normal reading, fast reading, careful reading, and decision making.
In a second contribution, we link these phases with characteristic changes of both EEGs signals and textual information.
By using a wavelet representation of EEGs, this analysis reveals variance and correlation changes of the inter-channels coefficients, according to the phases and the bandwidth.
And by using word embedding methods, we link the evolution of semantic similarity to the topic throughout the text with strategy changes.
In a third contribution, we present a new model where EEGs are directly integrated as output variables in order to reduce the state uncertainty.
This novel approach also takes into consideration the asynchronous and heterogeneous aspects of the data.
The teaching of prosody in French as a second language (FSL) classes and the use of speech software, which makes it possible to visualize the melody of learners and that of a French-speaking model, in order to compare them, are the subject of this research.
Two attempts to experiment with the correction of speech prosody in the 1960s, which took visualization into account, are mentioned as a reminder.
I was interested in the current situation of the teaching of the prosody of the FSL and the possibilities offered by digital tools, in order to understand it.
Among other things, I wanted to see how learners reacted to such tools, and especially if their use brought convincing results.
Learners worked from a manual, received independent training from a computer, were given group explanations from the teacher, sometimes with individual explanations.
The first two experiments (pilot and general) were carried out with a migrant audience, the third was carried out in a language laboratory where the students worked by themselves, and the last one, with explanations, in small groups.
The productions of the Francophone model and those of the learners were analyzed.
Surveys on the use of a digital tool and visualization with WinPitch (WP) and WinPitch Language Teaching and Learning (WP LTL) showed that most learners enjoyed working with these software.
For the last two experiments, conducted in a university context, I set up two working groups: one experimental with WP and the other control.
It turned out that the students in the experimental group had better results than those in the control group.
For the last experiment, explanations concerning "prosodic grammar", in this case the one based on Ph.
The results obtained made it possible to validate the working hypotheses.
They also show that the use of WP visualization, accompanied by the teacher's explanations, will be beneficial and justified in language classes in order to consciously improve oral expression in French.
The common thread of this thesis is the study of Hawkes processes.
These point processes decrypt the cross-causality that occurs across several event series.
Namely, they retrieve the influence that the events of one series have on the future events of all series.
For example, in the context of social networks, they describe how likely an action of a certain user (such as a Tweet) will trigger reactions from the others.
The first chapter consists in a general introduction on point processes followed by a focus on Hawkes processes and more specifically on the properties of the widely used exponential kernels parametrization.
In the following chapter, we introduce an adaptive penalization technique to model, with Hawkes processes, the information propagation on social networks.
This penalization is able to take into account the prior knowledge on the social network characteristics, such as the sparse interactions between users or the community structure, to reflect them on the estimated model.
Our technique uses data-driven weighted penalties induced by a careful analysis of the generalization error.
Next, we focus on convex optimization and recall the recent progresses made with stochastic first order methods using variance reduction techniques.
The fourth chapter is dedicated to an adaptation of these techniques to optimize the most commonly used goodness-of-fit of Hawkes processes.
Besides, such objectives include many linear constraints that are easily violated by classic first order algorithms, but in the Fenchel-dual problem these constraints are easier to deal with.
Hence, our algorithm's robustness is comparable to second order methods that are very expensive in high dimensions.
Finally, the last chapter introduces a new statistical learning library for Python 3 with a particular emphasis on time-dependent models, tools for generalized linear models and survival analysis.
Called tick, this library relies on a C++ implementation and state-of-the-art optimization algorithms to provide very fast computations in a single node multi-core setting.
Open-sourced and published on Github, this library has been used all along this thesis to perform benchmarks and experiments.
Coherence is a property that characterizes the text as a unified and interpretative whole.
Chinese learners of French as a foreign language (FLE) may have difficulty producing a coherent text in French.
This thesis therefore aims to analyze the management of the textual coherence of these learners, and more precisely, the uses of connectors as well as the resolution of anaphors and reference chains in the written productions of Chinese students who are at an intermediate and advanced level of FLE in France.
The two cohorts receive the same instructions and carry out two writing tasks (narration and argumentation) using the word processing software GenoGraphiX which records and reconstructs the writing process.
In the field of neology, different methodological approaches for the detection and extraction of semantic neologisms have been developed using strategies such as word sense disambiguation and topic modeling, but there is still not a proposal for a system for the detection of these units.
Beginning from a detailed study on the necessary theoretical assumptions required to delimit and describe semantic neologisms, in this thesis, we propose the development of an application to identify and extract said units using statistical,data mining and machine learning strategies.
The proposed methodology is based on treating the process of detection and extraction as a classification task, which consists on analyzing the concordance of topics between the semantic field from the main meaning of a word and the text where it is found.
To build the architecture of the proposed system,we analyzed five automatic classification methods and three deep learning based word embedding models.
Our analysis corpus is composed of the semantic neologisms of the computer science field belonging to the database of the Observatory of Neology of the Pompeu Fabra University, which have been registered from 1989 to 2015.
We used this corpus to evaluate the different methods that our system implements: automatic classification,keyword extraction from short contexts, and similarity list generation.
This first methodological approach aims to establish a framework of reference in terms of detection and extraction of semantic neologisms.
Stochastic systems with partial information allow one to represent numerous systems whose parameters are unknown and whose operation may depend on out-of-control factors.
In this thesis, we study several problems linked to these systems.
First one is diagnosability, that is the capacity to decide if a particular event occurred.
Second one is classification which is the capacity to decide from a trace of an execution which system produced it.
Finally, we are interested in the guarantees one can obtain by learning the probability transitions of a stochastic system.
Throughout the centuries, financial institutions have shaped the financial landscape and influenced economic activity.
The goal of this dissertation is to highlight, from a theoretical point of view, fundamental limitations of modern institutions and eventually derive implications regarding the future role of those institutions.
The first chapter provides an analysis of Central Clearing Platforms (CCP).
In the aftermath of the financial crisis of 2008, financial authorities around the world implemented regulations imposing central clearing on most derivative products.
It is shown that central clearing often requires a larger liquidity buffer than bilateral clearing.
The second chapter presents a continuous-time learning model meant to represent the learning process of an institution such as a central bank regarding a hidden information held by the market.
The last chapter introduces a specific blockchain-based payment network called the Lightning Network.
It allows users to transfer value instantly without relying on any trusted third party.
We discuss the implications regarding the structure of this network as well as its ability to become an important part of the financial landscape.
Marine traffic is the main contributor to anthropogenic underwater noise: since the 1970s, the increase in deep-sea shipping has increased the ambient noise by more than 10 dB in some areas.
In response to this concern, the Marine Strategy Framework Directive (MSFD) recommends acoustic monitoring.
Few studies are concerned with coastal activity and the noises radiated by small craft while these coastal environments are the purveyors of 41.7% of the ecosystem services produced by the oceans.
Between the academic and the industrial world, this PhD was to answer the different scientific and industrial questions on the topic of the coastal traffic in terms of the influence in the soundscape and the detection and classification of the coastal craft.
Without information on the coastal maritime traffic, a visual identification protocol is proposed using GoPro® images processing and produced the same data as the AIS (position, speed, size and type of craft); It allows to create maritime traffic maps on a disk of 1.6km radius.
The traffic is characterized by two acoustic descriptors: the SPL linked to the distance of the nearest boat and the ANL linked to the number of boats present in a 500 m radius disc.
The spatiotemporal monitoring of these descriptors allows to identify the impact on the maritime traffic on the coastal acoustic landscape.
The acoustic detection and the classification are performed after individual characterization of the noise by a set of acoustic parameters and using of supervised machine learning algorithm.
A specific protocol for the creation of the classification tree is proposed by comparing the acoustic data with the physical and contextual characteristics of each boat.
The methods are applied on the flotilla of coastal boats present in the Bay of Calvi (Corsica) during summer.
Today, a huge amount of data is made available to the research community through several web-based libraries.
Enhancing data collected from scientific documents is a major challenge in order to analyze and reuse efficiently domain knowledge.
To be enhanced, data need to be extracted from documents and structured in a common representation using a controlled vocabulary as in ontologies.
Our research deals with knowledge engineering issues of experimental data, extracted from scientific articles, in order to reuse them in decision support systems.
Experimental data can be represented by n-ary relations which link a studied object (e.g. food packaging, transformation process) with its features (e.g. oxygen permeability in packaging, biomass grinding) and capitalized in an Ontological and Terminological Ressource (OTR).
An OTR associates an ontology with a terminological and/or a linguistic part in order to establish a clear distinction between the term and the notion it denotes (the concept).
Our work focuses on n-ary relation extraction from scientific documents in order to populate a domain OTR with new instances.
More precisely, firstly, we propose to focus on unit of measure extraction which are known to be difficult to identify because of their typographic variations.
We propose to rely on automatic classification of texts, using supervised learning methods, to reduce the search space of variants of units, and then, we propose a new similarity measure that identifies them, taking into account their syntactic properties.
Hyperlinking is a way to connect texts, sounds, images or videos published on the World Wide Web.
It is now the standard way through which Internet users, content editors or search engines create and communicate information.
Prima facie, the relation between hyperlinking and European Union law does not appear to be self-evident.
Yet, hyperlinking raises a range of legal issues that challenge its apprehension by EU law.
As such, it is an important means to exercise the freedom of expression and information online.
Thus it can be expected from EU law to promote or protect hyperlinking.
Hyperlinking also contributes to the dissemination of illegal or harmful content online, in opposition with EU law.
Regarding these issues, the purpose of this study is to assess how and in what extent EU law delimits in a coherent way the freedom to link and its limits.
In order to answer this question, we will first demonstrate the emergence of a freedom to link regarding european intellectual property law.
The application of EU law to hyperlinking is however facing problems regarding the coherence of the law.
To solve this problem, proposals aimed at restoring the stability of EU law will punctuate this study.
This study thus provides a basis for understanding and thinking, common to the European Union, regarding the application of the law to hyperlinking.
Developing lexico-semantic resources is a major issue in the Natural Language Processing field.
These resources, by making explicit inter alia some knowledge possessed only by humans, aim at providing the ability of a precise and complete text understanding to NLP tasks.
Popular resources-building strategies involving crowdsourcing are flowering in NLP and are proved to be successful.
However, the resulted resources are not free of errors and lack some important semantic relations.
We designed an endogenous consolidation system for this type of networks based on inferring and annotating new semantic relations using the already existing ones, as well as extracting and proposing inference rules able to (re)generate a considerable part of the network.
The goal of this thesis, conducted within an industrial framework, is to pair textual media content.
Specifically, the aim is to pair on-line news articles to relevant videos for which we have a textual description.
The main issue is then a matter of textual analysis, no image or spoken language analysis was undertaken in the present study.
The question that arises is how to compare these particular objects, the texts, and also what criteria to use in order to estimate their degree of similarity.
We consider that one of these criteria is the topic similarity of their content, in other words, the fact that two documents have to deal with the same topic to form a relevant pair.
This problem fall within the field of information retrieval (ir) which is the main strategy called upon in this research.
The pairing system developed in this thesis distinguishes different steps which complement one another.
In the first step, the system uses natural language processing (nlp) methods to index both articles and videos, in order to overcome the traditionnal bag-of-words representation of texts.
In the second step, two scores are calculated for an article-video pair: the first one reflects their topical similarity and is based on a vector space model; the second one expresses their proximity in time, based on an empirical function.
The constraints imposed both by the data and the specific need of the partner company led us to adapt the evaluation protocol traditionnal used in ir, namely the cranfield paradigm.
We therefore propose an alternative solution for evaluating the system that takes all our constraints into account.
In recent years, there has been an actual effort to constitute and promote children's writings corpora especially in French.
The first research works on writing acquisition relied on small corpora that were not widely distributed.
Longitudinal corpora, monitoring a cohort of children's productions from similar collection conditions from one year to the next, do not exist in French yet.
Moreover, although natural language processing (NLP) has provided tools for a wide variety of corpora, few studies have been conducted on children's writings corpora.
This new scope represents a challenge for the NLP field because of children's writings specificities, and particularly their deviation from the written norm.
Hence, tools currently available are not suitable for the exploitation of these corpora.
There is therefore a challenge for NLP to develop specific methods for these written productions.
This thesis provides two main contributions.
On the one hand, this work has led to the creation of a large and digitized longitudinal corpus of children's writings (from 6 to 11 years old) named the Scoledit corpus.
Its constitution implies the collection, the digitization and the transcription of productions, the annotation of linguistic data and the dissemination of the resource thus constituted.
On the other hand, this work enables the development of a method exploiting this corpus, called the comparison approach, which is based on the comparison between the transcription of children's productions and their standardized version.
In order to create a first level of alignment, this method compared transcribed forms to their normalized counterparts, using the aligner AliScol.
It also made possible the exploration of various linguistic analyses (lexical, morphographic, graphical).
And finally, in order to analyse graphemes, an aligner of transcribed and normalized graphemes, called AliScol_Graph was created.
Working memory can be defined as the ability to temporarily store and manipulate information of any kind.
For example, imagine that you are asked to mentally add a series of numbers.
In order to accomplish this task, you need to keep track of the partial sum that needs to be updated every time a new number is given.
The working memory is precisely what would make it possible to maintain (i.e. temporarily store) the partial sum and to update it (i.e. manipulate).
In this thesis, we propose to explore the neuronal implementations of this working memory using a limited number of hypotheses.
To do this, we place ourselves in the general context of recurrent neural networks and we propose to use in particular the reservoir computing paradigm.
This type of very simple model nevertheless makes it possible to produce dynamics that learning can take advantage of to solve a given task.
In this job, the task to be performed is a gated working memory task.
The model receives as input a signal which controls the update of the memory.
When the door is closed, the model should maintain its current memory state, while when open, it should update it based on an input.
In our approach, this additional input is present at all times, even when there is no update to do.
In other words, we require our model to be an open system, i.e. a system which is always disturbed by its inputs but which must nevertheless learn to keep a stable memory.
In the first part of this work, we present the architecture of the model and its properties, then we show its robustness through a parameter sensitivity study.
This shows that the model is extremely robust for a wide range of parameters.
More or less, any random population of neurons can be used to perform gating.
Furthermore, after learning, we highlight an interesting property of the model, namely that information can be maintained in a fully distributed manner, i.e. without being correlated to any of the neurons but only to the dynamics of the group.
More precisely, working memory is not correlated with the sustained activity of neurons, which has nevertheless been observed for a long time in the literature and recently questioned experimentally.
This model confirms these results at the theoretical level.
In the second part of this work, we show how these models obtained by learning can be extended in order to manipulate the information which is in the latent space.
We therefore propose to consider conceptors which can be conceptualized as a set of synaptic weights which constrain the dynamics of the reservoir and direct it towards particular subspaces; for example subspaces corresponding to the maintenance of a particular value.
More generally, we show that these conceptors can not only maintain information, they can also maintain functions.
In the case of mental arithmetic mentioned previously, these conceptors then make it possible to remember and apply the operation to be carried out on the various inputs given to the system.
These conceptors therefore make it possible to instantiate a procedural working memory in addition to the declarative working memory.
We conclude this work by putting this theoretical model into perspective with respect to biology and neurosciences.
Speaker diarization (SD) involves the detection of speakers within an audio stream and the intervals during which each speaker is active, i.e. the determination of 'who spoken when'.
The first part of the work presented in this thesis exploits an approach to speaker modelling involving binary keys (BKs) as a solution to SD.
BK modelling is efficient and operates without external training data, as it operates using test data alone.
The presented contributions include the extraction of BKs based on multi-resolution spectral analysis, the explicit detection of speaker changes using BKs, as well as SD fusion techniques that combine the benefits of both BK and deep learning based solutions.
The SD task is closely linked to that of speaker recognition or detection, which involves the comparison of two speech segments and the determination of whether or not they were uttered by the same speaker.
Even if many practical applications require their combination, the two tasks are traditionally tackled independently from each other.
The second part of this thesis considers an application where SD and speaker recognition solutions are brought together.
The new task, coined low latency speaker spotting (LLSS), involves the rapid detection of known speakers within multi-speaker audio streams.
It involves the re-thinking of online diarization and the manner by which diarization and detection sub-systems should best be combined.
The internet of things has now entered every home and, with a society more and more focused towards wellness, these sensors measure and offer henceforth a wide variety of physiological data.
Virtual reality technologies reaching maturity, coupled with the advent of the internet of things, allow consequently new opportunities to propose improved immersive experiences.
In an attempt to overcome these limitations, this study therefore focuses on the original usage of smart wearables as substitutes for traditional sensors in immersive applications.
More precisely, the impact of biofeedback, via off-the-shelf smart wearables, on user engagement and the sense of agency.
We have thus carried out two experiments allowing us to study the impacts of the different biofeedback modalities on user experience.
Our first experiment implements a biofeedback based on heart rate in a virtual reality horror game, allowing to enhance the feeling of fear.
The results of this experiment confirm the interest of using smart wearables to capture physiological data for immersive virtual reality experiences.
They also highlight the positive impact of this biofeedback on user engagement.
The second experiment focuses on the use of cardiac activity as a mandatory interaction mechanism.
The results of this experiment demonstrate the possibility of using the said interaction mechanic for virtual reality experiences and indicate a positive impact on the sense of agency, linked with the level of competency of the participants.
On a theoretical level, this thesis proposes a synthesis of user experience models in virtual environment and submit the foundations of a model that we call "physiological immersion".
Classifiers ga (cl.16), ku (cl.17) and mu (cl.18) express respectively “contact”, “distance” and “interiority”: a) when they are prefixed by ‑úma /place/; b) when they appear in conjugated verbs or are prefixed by themes of determinants; c) when they are followed by a name in the form of a free morpheme; d) or when they are followed by a verb.
Bu (cl.14) expresses, in addition, “abstract” value when it is combined to lexical bases; when it is prefixed to the themes of determinants, it carries temporal, comparative and causal values; it serves thirdly to mark the hypothesis in the form of a free morpheme.
Since 2000, a significant progress has been recorded in research work which has proposed to learn object detectors using large manually labeled and publicly available databases.
However, when a generic object detector is applied on images of a specific scene, the detection performances will decrease considerably.
This decrease may be explained by the differences between the test samples and the learning ones at viewpoints taken by camera(s), resolution, illumination and background images.
In addition, the storage capacity evolution of computer systems, the "video surveillance" democratization and the development of automatic video-data analysis tools have encouraged research into the road-traffic domain.
The ultimate aims are the management evaluation of current and future trafic requests, the road infrastructures development based on real necessities, the intervention of maintenance task in time and the continuous road surveillance.
Moreover, traffic analysis is a problematicness where several scientific locks should be lifted.
These latter are due to a great variety of traffic fluidity, various types of users, as well multiple weather and lighting conditions.
Thus, developing automatic and real-time tools to analyse road-traffic videos has become an indispensable task.
These tools should allow retrieving rich data concerning the traffic from the video sequence and they must be precise and easy to use.
This is the context of our thesis work which proposes to use previous knowledges and to combine it with information extracted from the new scene to specialize an object detector to the new situations of the target scene.
In this thesis, we propose to automatically specialize a generic object classifier/detector to a road traffic scene surveilled by a fixed camera.
We mainly present two contributions.
This formalization approximates iteratively the previously unknown target distribution as a set of samples composing the specialized dataset of the target scene.
The samples of this dataset are selected from both source dataset and target scene further to a weighting step using some prior information on the scene.
The obtained specialized dataset allows training a specialized classifier to the target scene without human intervention.
The second contribution consists in proposing two observation strategies to be used in the SMC filter's update step.
They are used to weight the target samples.
The different experiments carried out have shown that the proposed specialization approach is efficient and generic.
We have been able to integrate multiple observation strategies.
It can also be applied to any classifier / detector.
In addition, we have implemented into the Logiroad OD SOFT software the loading and utilizing possibilities of a detector provided by our approach.
We have also shown the advantages of the specialized detectors by comparing their results to the result of Logiroad's Vu-meter method.
The discourse produced in a guided tour stems from different communicative modalities which include the visit assisted by a socio-technical device and the visit guided by an education and visitor service officer.
Several issues arise such as the genre taxonomy of the discourse linked to the specific field studied, the unit of the text segmentation which has to free itself from the scriptural or oral feature of the text production, the textual categorisation and indexation of the studied genre.
Indeed, the characterisation parameter value must introduce an essential prototype in order to categorise and index the texts of the studied genres.
Furthermore, the quantitative analysis of a selected text compilation is rooted in the discourse analysis and corpus linguistic approaches.
The method followed here, which introduces textual segmentation rules such as qualitative manual annotation and quantitative analysis suggests structural patterns of each considered genre.
In this thesis, we focus on data integration of raw data coming from heterogeneous and multi-origin data sources on the Web.
The global objective is to provide a generic and adaptive architecture able to analyze and combine this heterogeneous, informal, and sometimes meaningless data into a coherent smart data set.
In this report, we propose new models and techniques to adapt the combination and integration process to the diversity of data sources.
We focus on transparency and dynamicity in data source management, scalability and responsivity according to the number of data sources, adaptability to data source characteristics, and finally consistency of produced data (coherent data, without errors and duplicates).
In order to address these challenges, we first propose a meta-models in order to represent the variety of data source characteristics, related to access (URI, authentication) extraction (request format), or physical characteristics (volume, latency).
By relying on this coherent formalization of data sources, we define different data access strategies in order to adapt access and processing to data source capabilities.
With help form these models and strategies, we propose a distributed resource oriented software architecture, where each component is freely accessible through REST via its URI.
In order to improve the data quality of our approach, we then focus on the data uncertainty that could appear in a Web context, and propose a model to represent uncertainty in a Web context.
We introduce the concept of Web resource, based on a probabilistic model where each resource can have different possible representations, each with a probability.
This approach will be the basis of a new architecture optimization allowing to take uncertainty into account during our combination process
French population is rapidly aging.
Senior citizens ratio is increasing and our society needs to rethink its organization, taking into account this change, better knowing this fast growing population group.
Even if numerous cohorts of elderly people already exist worldly with four in France and, even as they live in growing numbers in nursing homes and out-patient treatment clinics, knowledge of this population segment is still missing.
Today several health and medico-social structures groups as Korian and Orpéa invest in big relational data bases enabling them to get real-time information about their patients/residents.
Since 2010 all Korian residents' files are dematerialized and accessible by requests.
They contain at the same time, structured medico-social data describing the residents as well as their treatments and pathologies, but also free-textual data detailing their daily care by the medical staff.
Through time and as the computerized resident file (DRI) was mainly conceived as a data base management application, it appeared essential to mine these data and build a decision-making tool intended to improve the care efficiency.
The Ageing Well Institute becoming meanwhile the Korian Ageing Well Foundation chose then, working in a private/public partnership, to finance a research work intented to better understand these datas' informative potential, to assess their reliability and response to public health threats.
This research work and this thesis were then designed in several steps:
- First, a content analysis of the data warehouse DRI, the objective being to build a research data base, with a social side and a health side.
This was the first paper subject.
- Then, by direct extraction of the residents' socio-demographic information at nursing home (NH) entry, adding hospitalizations and deaths, and finally, by an iterative textual extraction process of the transmissions data and by using the Delphi method, we created twenty-four syndromes, added hospitalizations and deaths and built a syndromic data base, the Ageing Well data base.
This information system of a new kind, allowed the constitution of a public health cohort for elderly people from the BBV residents' population and its syndromic longitudinal follow-up.
The BBV was also scientifically assessed for surveillance and public health research through present situation analysis: content, periodicity and data quality.
This cohort then gave us the opportunity to build a surveillance tool and follow the residents' population in real-time by watching their 26 daily frequency syndromic distributions.
The methodology for that assessment, Atlanta CDCs' health surveillance systems method, was used for flu and acute gastro enteritis syndroms and was the second paper subject.
- Finally, the building of a new public health tool: each syndrom's distribution through time (transmissions dates) and space (transmissions NH ids) opened the research field to new data exploration methods. I used these to study different health problems afflicting senior citizens: frequent falls, cancer, vaccinations and the end of life.
Dynamical systems exhibit temporal behaviors that can be expressed in various sequential forms such as signals, waveforms, time series, and event sequences.
Detecting patterns over such temporal behaviors is a fundamental task for understanding and assessing these systems.
Since many system behaviors involve certain timing characteristics, the need to specify and detect patterns of behaviors that involves timing requirements, called timed patterns, is evident.
However, this is a non-trivial task due to a number of reasons including the concurrency of subsystems and density of time.
The key contribution of this thesis is in introducing and developing emph{timed pattern matching}, that is, the act of identifying segments of a given behavior that satisfy a timed pattern.
We propose timed regular expressions (TREs) and metric compass logic (MCL) as timed pattern specification languages.
We first develop a novel framework that abstracts the computation of time-related aspects called the algebra of timed relations.
Then we provide offline matching algorithms for TRE and MCL over discrete-valued dense-time behaviors using this framework and study some practical extensions.
It is necessary for some application areas such as reactive control that pattern matching needs to be performed during the actual execution of the system.
For that, we provide an online matching algorithm for TREs based on the classical technique of derivatives of regular expressions.
We believe the underlying technique that combines derivatives and timed relations constitutes another major conceptual contribution for timed systems research.
Furthermore, we present an open-source tool Montre that implements our ideas and algorithms.
We explore diverse applications of timed pattern matching over several case studies using Montre.
Finally we discuss future directions and several open questions emerged as a result of this thesis.
The present dissertation focuses on the factors implied in relative clause processing, mainly subject/object relative clauses and relative clause attachment.
I propose to go beyond the notion of subject/object languages or high/low attachment languages.
This description is necessary to grasp their influence in processing, especially the differences observed across languages in experiments.
In line with the multifactorial approach, I analyse the influence of semantics and pragmatics in relative clause processing, especially in French and English.
Finally, to broaden the perspective beyond the linguistic domain, I show the influence of other non-linguistic factors on relative clause processing, by presenting visual world eye-tracking and forced choice experiments about the influence of mathematical priming on relative clause attachment in French.
In the modern economy, creating agile business processes is one of the conditions to obtain/ maintain competitive advantage on the market.
Actually, the management of business processes is supported by the BPM (Business Process Management) approach.
It addresses the management, transformation and improvement of organizational operations.
Yet, actual BPM does not feature the means to have a continuous adaptation of their business processes and quick adjustment of their models and resources allocation to meet changing environmental conditions.
To support agility at IT level, we use the SOA (Service Oriented Architecture) approach.
Indeed, the SOA can provide numerous benefits to the organization, enabling it to reduce complexity and increase flexibility through their reutilization and modularity features.
Moreover, resources which are important assets in successful process's implementation are widely supported with agile organization regarded as primordial factor for successful agility implementation.
For this reason, we propose an approach that combines management of processes with the required skills to their execution and to better enhance the process flexibility we combine BPM with SOA in a social environment.
The use of equivalent terms or semantically close is necessary to increase the coverageand sensitivity of applications such as information retrieval and extraction or semanticannotation of documents.
In the context of the adverse drug reactions identification, sensitivityis also sought to detect more exhaustively spontaneous reports and better monitordrug risk.
This is the reason that motivates our work.
In our work, we thus seek to detectsemantically close terms and the together using several methods: unsupervised algorithms, terminological resources exploited with terminological reasoning and methodsof Natural Language Processing, such as terminology structuring, where we aim to detecthierarchical and synonymous relations.
We conducted many experiments and evaluations of generated, which show that the proposed methods can efficiently contribute tothe task in question.
The aim of this dissertation is to explore the syntactic-semantic interface of 'constructions' which contain secondary predicates – either depictive or resultative.
The main problems will be to deal (i) with the selection of the subjects (or hosts) of these types of predicates and (ii) with the aspectual class of the verbs used in these sentences.
In the first part, the various patterns implied in resultative clauses will be examined, leading to the conclusion that the basic principle that governs the syntax of these clauses can be identified with the 'Restriction on direct objects' – or RDO.
After invalidating many would-be counter-examples to the RDO, its validity will be reasserted, notably as a diagnostic of unaccusativity in English.
The second part of the dissertation is devoted to depictive predicates – in particular (i) to the constraints that determine the choice of the Controller in this type of secondary predication, and (ii) to the relevant properties of depictive adjectives in contra-distinction to other types of adjuncts, often identified as 'participant-oriented'.
Finally, the distribution of those Russian adjectives which possess long and short forms, which is conditioned by specific agreement or concord properties, is examined, leading to a tentative reconstruction of a diachronic process which has led to their distribution in today's Russian.
Within the field of brain mapping, we identified the need for a tool which is grounded in the detailed knowledge of individual variability of sulci.
In this thesis, we develop a new brain mapping tool called NeuroLang, which utilises the spatial geometry of the brain.
We approached this challenge with two perspectives: firstly, we grounded our theory firmly in classical neuroanatomy.
Secondly, we designed and implemented methods for sulcus-specific queries in the domain-specific language, NeuroLang.
We tested our method on 52 subjects and evaluated the performance of NeuroLang for population and subject-specific representations of neuroanatomy.
Then, we present our novel, data-driven hierarchical organisation of sulcal stability.
To conclude, we summarise the implication of our method within the current field, as well as our overall contribution to the field of brain mapping.
The acquisition of early lexicon is very important for the development of language considering that it is the early lexicon that builds infants'first significant utterances and that it prefigures to a certain extent infants'future language skills.
It is well established that lexical acquisition presents common developmental trends and milestones, nevertheless a great amount of individual variation exists. This variation comes from linguistic, social and/or idiosyncratic factors.
Further research should be done to investigate the possible influence of evaluation procedures on the results.
Although the use of a complementary approach could limit this bias, it has rarely been used in lexical acquisition research.
This work aims at describing not only the common developmental trajectories of early lexicon in French monolingual children, but also the inter-individual differences.
More specifically, we want to show the importance of applying a complementary approach and of exploring word production during spontaneous interactions in real-life settings to better interpret inter-individual differences.
Overall, the development and the composition of individual lexicon, evaluated through the IFDC, follow the trends already reported in the literature.
As for the spontaneous vocabulary, we focused our study on 4 children at the 15-52; 50; 70-120 word linguistic stages (corpus CIBLÉ).
The integration of two complementary approaches, i.e. parental questionnaires and spontaneous observations, proved to be efficient and allowed us to reliably evaluate the lexical development and to avoid the bias linked to the use of a single method.
To better understand the results variations between the two methods, we explored the situational and interactional context on the corpus CIBLÉ.
We defined and categorized the different situations in the corpus TOTAL, then we focused on the corpus CIBLÉ to calculate their duration and we found variations between situations.
For example, the two children with the smallest lexicon had the longest duration of solitary play. During this activity, the number of produced words was generally very low.
Next, we describe the interactional context, and more particularly, the rate and the nature of the children exchanges.
The analyses revealed an important variation between measures and differences in the exchange rate among children.
To a certain degree, for some children the interactional measures provide a richer interpretation of lexical measures.
Our work clearly shows the advantages of combining several types of data to evaluate the early lexical development and the differences between individuals and encourages this approach.
The analysis of situational and interactional contexts shows that these are crucial for understanding children lexical measures and better interpreting intra-and inter-individual differences.
Domain analysis is the process of analyzing a family of products to identify their common and variable features.
In this thesis, our general contribution is to address mining and modeling variability from informal documentation.
We investigate the applicability of this idea by instantiating it in two different contexts: (1) reverse engineering Feature Models (FMs) from regulatory requirements in nuclear domain and (2) synthesizing Product Comparison Matrices (PCMs) from informal product descriptions.
In the first case study, we adopt NLP and data mining techniques based on semantic analysis, requirements clustering and association rules to assist experts when constructing feature models from these regulations.
The evaluation shows that our approach is able to retrieve 69% of correct clusters without any user intervention.
Moreover, features dependencies show a high predictive capacity: 95% of the mandatory relationships and 60% of optional relationships are found, and the totality of requires and exclude relationships are extracted.
In the second case study, our proposed approach relies on contrastive analysis technology to mine domain specific terms from text, information extraction, terms clustering and information clustering.
Overall, our empirical study shows that the resulting PCMs are compact and exhibit numerous quantitative and comparable information.
The user study shows that our automatic approach retrieves 43% of correct features and 68% of correct values in one step and without any user intervention.
We show that there is a potential to complement or even refine technical information of products.
The main lesson learnt from the two case studies is that the exploitability and the extraction of variability knowledge depend on the context, the nature of variability and the nature of text.
With the development and the expansion of connected devices in every domain, several projects on stream processing have been developed.
This thesis has been realized as part of the FUI Waves, a reasoning stream processing engine distributed.
The use case for the development was the processing of data streamed from a potable water distribution network, more specifically the detection of anomalies in the quality measures and their contextualisation using external data.
Several contributions have been realized and integrated in different stages of the project, with evaluations and publications witnessing their relevance.
These contributions use an ontology that has been designed thanks to collaboration with domain experts working for our water data management project partner.
The use of geographical data allowed to realize a profiling system aiming at improving the anomaly contextualisation process.
An ontology encoding approach, adapted to RDF stream processing, has been developed to support RDFS inferences enriched with owl: sameAs.
Conjointly, a compressed formalism (PatBin) has been designed to represent streams.
PatBin is based on the regularity of patterns found in incoming streams.
Moreover, a query language has been conceived from PatBin, namely PatBinQL.
It integrates a reasoning strategy that combines both materialization and query rewritting.
Finally, given deductions coming from a Waves machine learning component, a query generation tool has been developed.
These different contributions have been evaluated on both real-world and synthetic datasets.
This thesis investigates new clustering paradigms and algorithms based on the principle of the shared nearest-neighbors (SNN.
As most other graph-based clustering approaches, SNN methods are actually well suited to overcome data complexity, heterogeneity and high-dimensionality.
The first contribution of the thesis is to revisit existing shared neighbors methods in two points.
We first introduce a new SNN formalism based on the theory of a contrario decision.
This allows us to derive more reliable connectivity scores of candidate clusters and a more intuitive interpretation of locally optimum neighborhoods.
We also propose a new factorization algorithm for speeding-up the intensive computation of the required sharedneighbors matrices.
The second contribution of the thesis is a generalization of the SNN clustering approach to the multi-source case.
Whereas SNN methods appear to be ideally suited to sets of heterogeneous information sources, this multi-source problem was surprisingly not addressed in the literature beforehand.
The main originality of our approach is that we introduce an information source selection step in the computation of candidate cluster scores.
As shown in the experiments, this source selection step makes our approach widely robust to the presence of locally outlier sources.
This new method is applied to a wide range of problems including multimodal structuring of image collections and subspace-based clustering based on random projections.
The third contribution of the thesis is an attempt to extend SNN methods to the context of bipartite k-nn graphs.
We introduce new SNN relevance measures revisited for this asymmetric context and show that they can be used to select locally optimal bi-partite clusters.
Accordingly, we propose a new bipartite SNN clustering algorithm that is applied to visual object's discovery based on a randomly precomputed matching graph.
Experiments show that this new method outperformed state-of-the-art object mining results on OxfordBuilding dataset.
Based on the discovered objects, we also introduce a new visual search paradigm, i.e. object-based visual query suggestion.
Over the past few years, the Big Data concept has been widely developed.
In order to analyse and explore all this data, it was necessary to develop new methods and technologies.
Today, Big Data also exists in the health sector.
Hospitals in particular are involved in data production through the adoption of electronic health records.
The objective of this thesis was to develop statistical methods reusing these data in order to participate in syndromic surveillance and to provide decision-making support.
This study has 4 major axes.
First, we showed that hospital Big Data were highly correlated with signals from traditional surveillance networks.
Secondly, we showed that hospital data allowed to obtain more accurate estimates in real time than web data, and SVM and Elastic Net models had similar performances.
Then, we applied methods developed in United States reusing hospital data, web data (Google and Twitter) and climatic data to predict influenza incidence rates for all French regions up to 2 weeks.
Finally, methods developed were applied to the 3-week forecast for cases of gastroenteritis at the national, regional and hospital levels.
We introduce the task of multiclass classification and the challenge of classifying with a large number of classes.
In this domain, we introduce an asynchronous framework for performing distributed optimization.
We present application of the proposed asynchronous framework on two popular domains: matrix factorization for large-scale recommender systems and large-scale binary classification.
Whereas, in the case of large-scale binary classification we use a variant of SGD which uses variance reduction technique, SVRG as our optimization algorithm.
My research takes part in the field of Berber linguistics and precisely in phraseology addressing lexical fixation including the study of frozen sequences.
In fact this research presents a continum of my works of the two years of master.In the first place, my research of Master 1 entitled "thematic study of Berber idiomatic expressions related to the human body, variety of Ayt Ḥmad Uɛisa, Middle Atlas", and of Master 2 devoted to the study of the semantico-syntactic typology of idiomatic expressions related to the human body, and in the second place, of other works carried out in Berber studies in general.
It focuses on three different aspects of idiomatic expressions by conducting a thematic and semantico-syntactic study, and stressing the syntactic and semantic typology - study of the syntactic and semantic behaviors of each syntagm in the expression, the processes of construction of meaning - as well as on the relation between the semantism of the constituents and their syntactic fixation.
The analysis will focus on an oral corpus that il will collect using semi-direct interviews conducted with native speakers of the variety of Ayt Ḥmad Uɛisa.
I will then analyze the units of the corpus by referring to the general theoretical framework of international phraseology as well as the works done in the field of Berber phraseology in particular.
The importance of this project lies in the fact that it will provide linguistic elements explaining the syntactic and semantic functioning of Berber idiomatic expressions, and help to understand them. This will undoubtedly contribute to preserve these linguistic forms which constitute a part of Berber language through the collection of the corpus and the constitution of a database that can be used in other perspectives, such as lexicography, phraseology didactic, translation and Language Processing.
The objective is to develop machine learning and deep learning methods to extract automatically key features and information from health data, especially from diabetes patient data records in order to improve health monitoring, treatment and prediction about the pathology and the efficiency of the diagnosis and health plans efficiency.
The developed methods for diabetes will be explored for applications to other data sets, domains and sectors.
The thesis will focus on defining a methodology that can be tuned and reused for various use cases as opposed to developing specific solutions per domain.
The objective is to define a common underlying framework from which one can derive domain specific solutions.
The present thesis proposes an automatic morphological analysis of the kanji sequences in Japanese texts.
This analysis is based on the graphemic, morphological and syntactic characteristics of the Japanese language.
The first part of the thesis describes the Japanese writing system and its encoding methods.
The second part deals with the Japanese parts of speech, in particular verbs, adjectives, particles and flexional suffixes which morphosyntaxic characteristics are essential for the morphological analysis.
The third part describes the module of analysis: identification and formalization of the data necessary to the analysis, algorithm of the analysis and the related treatments, formalization of models of objects necessary to the data-processing handling of Japanese.
This thesis presents a modelisation of the main syntactical aspects of coordination using Guy Perrier's Interaction Grammars as the target formalism.
Interaction Grammars make it possible to explicitly define conjuncts'valencies.
This is precisely what our modelisation is based upon.
We also present work around this modelisation that enabled us to provide a realistic implementation: lexicalized grammar development (using our tool XMG), lexical disambiguation based on automata intersection and parsing.
We aim to obtain better computational efficiency than pure metrical mapping techniques, better accuracy as well as usability for robot guidance compared to the topological mapping.
A crucial step of any mapping system is the loop closure detection which is the ability of knowing if the robot is revisiting a previously mapped area.
Therefore, we first propose a hierarchical loop closure detection framework which also constructs the global topological structure of our hybrid map.
Using this loop closure detection module, a hybrid mapping framework is proposed in two step.
These maps are further augmented with metric information at those nodes which correspond to image sub-sequences acquired while the robot is revisiting the previously mapped area.
The second step augments this model by using road semantics.
A Conditional Random Field based classification on the metric reconstruction is used to semantically label the local robot path (road in our case) as straight, curved or junctions.
Metric information of regions with curved roads and junctions is retained while that of other regions is discarded in the final map.
Loop closure is performed only on junctions thereby increasing the efficiency and also accuracy of the map.
By incorporating all of these new algorithms, the hybrid framework presented can perform as a robust, scalable SLAM approach, or act as a main part of a navigation tool which could be used on a mobile robot or an autonomous car in outdoor urban environments.
Experimental results obtained on public datasets acquired in challenging urban environments are provided to demonstrate our approach.
This thesis, which is organized in two independent parts, presents work on distributional semantics and on variable selection.
In the first part, we introduce a new method for learning good word representations using large quantities of unlabeled sentences.
The method is based on a probabilistic model of sentence, using a hidden Markov model and a syntactic dependency tree.
We then evaluate our models on intrinsic tasks such as predicting human similarity judgements or word categorization, and on two extrinsic tasks: named entity recognition and supersense tagging.
In the second part, we introduce, in the context of linear models, a new penalty function to perform variable selection in the case of highly correlated predictors.
This penalty, called the trace Lasso, uses the trace norm of the selected predictors, which is a convex surrogate of their rank, as the criterion of model complexity.
In particular, it is equal to the ℓ 1-norm if all predictors are orthogonal and to the ℓ 2-norm if all predictors are equal.
We propose two algorithms to compute the solution of least-squares regression regularized by the trace Lasso, and perform experiments on synthetic datasets to illustrate the behavior of the trace Lasso.
Classical computer architectures are optimized to process pre-formatted information in a deterministic way and therefore struggle to treat unorganized natural data (images, sounds, etc.).
As these become more and more important, the brain inspires new, neuromorphic computer circuits such as neural networks.
In this work, we concentrate on memristors based on ferroelectric tunnel junctions that are composed of an ultrathin ferroelectric film between two metallic electrodes.
We show that the polarization reversal in BiFeO3 films can induce resistance contrasts as high as 10^4 and how mixed domain states are connected to intermediate resistance levels.
Changing the electrode materials provides insights into their influence on the electrostatic barrier and dynamic properties of these memristors.
Their analysis in combination with piezoresponse force microscopy finally allows us to establish a model describing the memristor dynamics under arbitrary voltage signals.
After the demonstration of an important learning rule for neural networks, called spike-timing-dependent plasticity, we successfully predict new, previously unexplored learning curves.
This constitutes an important step towards the realization of unsupervised self-learning hardware neural networks.
Geohistorical data extracted from various and heterogeneous sources are highly inaccurate, uncertain or inexact according to the existing terminology.
In particular, we focus on Paris historical street networks and its evolution between the end of the XVIIIth and the end of the XIXth centuries.
Our proposal is based on a merged structure of multiple representations of data capable of modelling spatial networks at different times, providing tools such as pattern detection in order to criticize, qualify and eventually correct data and sources without using ground truth data but the comparison of data with each other through the merging process.
Generally, a network reflects the interactions between many entities of a system.
These interactions have different sources, a social link or a friendship link in a social network, a cable in a router network, a chemical reaction in a protein-protein interaction network, a hyperlink in a webpage network.
Nowaday, network science is a research area in its own right focusing on describing and modeling these networks in order to reveal their main features and improve our understanding of their mecanisms.
Most of the works in this area use graphs formalism which provides a set of mathematical tools well suited for analyzing the topology of these networks.
It exists many applications, for instance applications in spread of epidemy or computer viruses, weakness of networks in case of a breakdown, attack resilience, study for link prediction, recommandation, etc.
The large majority of real-world networks depicts several levels of organization in their structure.
Because of there is a weak global density coupled with a strong local density, we observe that nodes are usually organized into groups, called communities, which are more internally connected than they are to the rest of the network.
Important information for public health is contained within Electronic Health Records (EHRs).
The vast majority of clinical data available in these records takes the form of narratives written in natural language.
Although free text is convenient to describe complex medical concepts, it is difficult to use for medical decision support, clinical research or statistical analysis.
Among all the clinical aspects that are of interest in these records, the patient timeline is one of the most important.
Being able to retrieve clinical timelines would allow for a better understanding of some clinical phenomena such as disease progression and longitudinal effects of medications.
Accessing the clinical timeline is needed to evaluate the quality of the healthcare pathway by comparing it to clinical guidelines, and to highlight the steps of the pathway where specific care should be provided.
In this thesis, we focus on building such timelines by addressing two related natural language processing topics which are temporal information extraction and clinical event coreference resolution.
Our main contributions include a generic feature-based approach for temporal relation extraction that can be applied to documents written in English and in French.
We devise a neural based approach for temporal information extraction which includes categorical features.
We present a neural entity-based approach for coreference resolution in clinical narratives.
We perform an empirical study to evaluate how categorical features and neural network components such as attention mechanisms and token character-level representations influence the performance of our coreference resolution approach.
Recognizing surrounding objects is an important skill for the autonomy of robots performing in daily-life.
Nowadays robots are equipped with sophisticated sensors imitating the human sense of touch.
In this thesis, we exploit haptic data to perform haptic recognition of daily life objects using machine learning techniques.
The main challenge faced in our work is the difficulty of collecting a fair amount of haptic training data for all daily-life objects.
This is due to the continuously growing number of objects and to the effort and time needed by the robot to physically interact with each object for data collection.
We solve this problem by developing a haptic recognition framework capable of performing Zero-shot, One-shot and Multi-shot Learning.
We also extend our framework by integrating vision to enhance the robot's recognition performance, whenever such sense is available.
The current research aims to provide a syntactic and semantic analysis of Modern Greek transitive non-locative constructions with one direct object: N0 V N1.
Our study is based on the syntactic framework of the Transformational Grammar defined by Zellig S. Harris.
Based on 16 560 morphological verbal entries, we proceeded to the classification of transitive non-locative constructions.
On the basis of formal criteria we divided them into 24 distinct classes that formed an inventory of 2 934 transitive non-locative verbal uses with one direct object.
Likewise, the passive transformation is largely blocked in the 32GNM table, while the 32GCV and 32GRA tables regroup verbs accepting a support verb transformation.
In order to hope to develop a political philosophy that immediately recognizes our interdependence, we work in a first part to establish assumptions about what we mean by reality and our access to it.
An event-based ontology seems compatible with the narrative ontogenesis which constitutes us individually by constituting a "we".
This requires imagining everyone imagining the world and learning through stories, in an inductive logic that can reconcile hermeneutic phenomenology on the one hand and statistical learning on the other.
From these stories each identifies universals, interpretable as principal components of a factorial statistical analysis of these stories that constitute us.
Time plays a key role in the dynamics of this constitution as well as in the events gathered in these stories.
The stakes are ultimately to share these universals in a common story, or, conversely, in a temporal break that may allow better access to a common world.
We then work in a second part on the question of living together with republican ideas of freedom, equality and fraternity, and with those of plurality and boundaries.
The political ecology that we see then is as republican as libertarian.
In this context, justice is expressed by rightness, fidelity, sensitivity and a “fair excess”.
The categorical imperative lies in the need to make others beautiful, free, and powerful, and to learn together.
Law appears to develop dynamically in the very time that the City is developed.
The possibility of the radically “new” worked in the first part allows articulating freedom and institutions.
The logic of a code of honor ultimately allows not to surrender to the Almighty Reason without giving up the Enlightenment.
The instrumentation and observation of learners activities by exploiting interaction traces in the EHL and the development of indicators can help tutors to monitor activities of learners and support them in their collaborative learning process.
Indeed, in a learning situation, the teacher needs to observe the behavior of learners in order to build an idea about their involvement, preferences and learning styles so that he can adapt the proposed activities.
This estimation is based on automatic analysis of students textual asynchronous conversations.
Bluecime has designed a camera-based system to monitor the boarding station of chairlifts in ski resorts, which aims at increasing the safety of all passengers.
This already successful system does not use any machine learning component and requires an expensive configuration step.
Machine learning is a subfield of artificial intelligence which deals with studying and designing algorithms that can learn and acquire knowledge from examples for a given task.
Such a task could be classifying safe or unsafe situations on chairlifts from examples of images already labeled with these two categories, called the training examples.
The machine learning algorithm learns a model able to predict one of these two categories on unseen cases.
Since 2012, it has been shown that deep learning models are the best suited machine learning models to deal with image classification problems when many training data are available.
In this context, this PhD thesis, funded by Bluecime, aims at improving both the cost and the effectiveness of Bluecime's current system using deep learning.
Febrile seizures (FS) affect 2% to 5% of children aged 6 months to 5 years of age.
Although FS are usually benign, they are associated with serious treatable neurological emergencies.
Nowadays, three factors are used to evaluate this risk: the age of the child, whether the FS is simple or complex and the features of the clinical exam.
The objective of this thesis was to investigate the hypothesis that among children experiencing a FS, only those with an abnormal clinical exam are at risk of serious, treatable neurological emergencies.
We first created an informatics tool in order to exhaustively search for cases among more than one million electronic medical records from seven pediatric emergency departments (PED) between 2007 and 2011.
Then, we identified visits of children with a FS.
Finally, we evaluated the proportion of serious, treatable neurological emergencies associated with these visits, and more specifically with visits of children with a normal clinical exam.
We found no serious treatable neurological emergencies among children visiting the ED for a FS with a normal clinical exam, whatever the age and the features of the seizure were.
The studies described in this thesis associated with the available data in the literature support our hypothesis and highlight the need of guidelines regarding the management of these children.
Finally, this thesis gives us the opportunity to discuss some considerations on the use of electronic medical records for clinical research.
The concept of “Business Rule Management System” (BRMS) has been introduced in order to facilitate the design, the management and the execution of company-specific business policies.
Based on a symbolic approach, the main idea behind these tools is to enable the business users to manage the business rule changes in the system without requiring programming skills.
It is therefore a question of providing them with tools that enable to formulate their business policies in a near natural language form and automate their processing.
Nowadays, with the expansion of intelligent systems, we have to cope with more and more complex decision logic and large volumes of data.
It is not straightforward to identify the causes leading to a decision.
There is a growing need to justify and optimize automated decisions in a short time frame, which motivates the integration of advanced explanatory component into its systems.
Thus, the main challenge of this research is to provide an industrializable approach for explaining the decision-making processes of business rules applications and more broadly rule-based systems.
This approach should be able to provide the necessary information for enabling a general understanding of the decision, to serve as a justification for internal and external entities as well as to enable the improvement of existing rule engines.
To this end, the focus will be on the generation of the explanations in themselves as well as on the manner and the form in which they will be delivered.
In this thesis, we focus on the problem of link prediction in binary tensors of order three and four containing positive observations only.
Tensors of this type appear in web recommender systems, in bio-informatics for the completion of protein interaction databases, or more generally for the completion of knowledge bases.
We benchmark our completion methods on knowledge bases which represent a variety of relationnal data and scales.
Our approach is parallel to that of matrix completion.
We optimize a non-convex regularised empirical risk objective over low-rank tensors.
Our method is empirically validated on several databases, performing better than the state of the art.
These performances however can only be reached for ranks that would not scale to full modern knowledge bases such as Wikidata.
We focus on the Tucker decomposition which is more expressive than the Canonical decomposition but also harder to optimize.
With these method, we obtain improved performances on several benchmarks for limited parameters per entities.
Finally, we study the case of temporal knowledge bases, in which the predicates are only valid over certain time intervals.
We propose a low-rank formulation and regularizer adapted to the temporal structure of the problem and obtain better performances than the state of the art.
Systems are becoming more and more complex, because to stay competitive, companies whichdesign systems search to add more and more functionalities to them.
Additionally, this competition impliesthat the design of systems needs to be reactive, so that the system is able to evolve during its conception andfollow the needs of the market.
Additionally, natural language is hard to process automatically: for example, it is hard to determine, usingonly a computer program, that two natural language requirements contradict each other.
However, naturallanguage is currently unavoidable in the specifications we studied, because it remains very practical, and itis the most common way to communicate.
We aim to complete these natural language requirements with elements which allow to make them lessambiguous and facilitate automatic processing.
These elements can be parts of models (architectural modelsfor example) and allow to define the vocabulary and the syntax of the requirements.
We experimented theproposed principles on real industrial specifications and we developped a software prototype allowing totest a specification enhanced with these vocabulary and syntax elements.
The objective of this project is to develop tools to automatically analyze the recorded speech of patients with the Huntington's disease, a serious neurodegenerative genetic disease, to obtain a marker allowing their clinical follow-up.
This project will focus on the joint modeling of cognitive and emotional deficits, using a variety of representations of speech signals.
We will examine three levels of representations on the different protocols: Intelligibility and articulatory measurements / Acoustic and prosodic representations of the vocal signal / Linguistic representations.
In particular, we will use unsupervised machine learning techniques to detect repeated patterns in the signal, as well as quantify the individual evolutionary trajectories of patients in the different dimensions of the disease.
The diversity of skills tested in patients'speech will enable us to report on the heterogeneity of their symptoms observed in the clinic (cognitive, behavioral, emotional and motor disorders).
This project will offer clinicians and researchers composite markers that faithfully reflect the evolution of the disease specific to each patient.
This research relates to the field of Spanish didactics and focuses on students who study in the St. Denis area near Paris and their social representations of and attitudes towards this language.
The effectiveness of the blended learning program on language acquisition was measured through pre-tests and post-tests to evaluate the language skills in Learning Comprehension, Written Comprehension and Short Written Expression following the Cefrl, and secondly on an analysis of a sample of students' productions.
Students uses and appropriation modes of the long distance learning platform are measured by a tracking through comments and semiannual validations and also attitude survey.
Our results highlighted a positive evolution concerning language competence and a change of attitude for two thirds of the students and a progression in fluency and accuracy according to the starting levels and targeting B1-B2 levels towards Gies 1 and 2.
While the majority of students surveyed perceived both the blended language learning program and their workgroup as a positive learning experience, one third, said that they had difficulties in taking on the imposed workload and would rather work autonomously.
The coordinated investment of the different actors required by the learning program is one of the sharing solutions asked for by the L2 teachers in the blended learning program, without which initiatives in field computer based learning are likely to remain individual and isolated.
The construction show a lack of efficiency to compare to other industries.
In answer, the world of architecture set up a new process called BIM (Building Information Modeling).
This process is based on a3D virtual mock up containing every information needed for the construction.
During the implementation of this process, difficulties of interaction has been noted by the BIM users.
BIM models are hard to observe and manage, explained by the fact that these models contain a large amount of information.
BIM process proposes the same scheme for all the construction profile.
The purpose is to offer an adapted environment, considering the profile of each BIM user, while keeping the actual design method.
Firstly, this document describes the creation of virtual reality rooms dedicated to the construction.
Secondly, it deals with the development of algorithms allowing the classification of components from BIM model, an adaptive system of visualization and a process to handle the model.
These development are based on the consideration of the profile of the user, the trade of the user.
This work presents an experimental evaluation of various voice transformation techniques based on GMM models.
These linear transforms, despite their quality obtained, they fail to some defects specially the oversmoothing effect, the problem of spectral distortion and the overfitting.
In a first part, we proposed taking these issues into account to adapt the learning strategy of the conversion functions.
The first main idea is to reduce the number of parameters describing the conversion function.
The second idea considers the solutions based on linear transform are unstable face to the lack of the training data, hence the recourse to non-linear transform model like RBF.
In a second part in some situations, we need to align non-parallel data from the source and target speakers, one solution consists to use a recursive representation of binary tree, whose depth depends on the learning data size.
Mobile devices offer measuring capabilities using embedded or connected sensors.
They are critical because the performed measurements must be reliable because possibly used in rigorous context.
Despite a real demand, there are relatively few applications assisting users with their measuring processes that use those sensors.
Such assistant should propose methods to visualise and to compute measuring procedures while using communication functions to handle connected sensors or to generate reports.
Such rarity of applications arises because of the knowledges required to define correct measuring procedures.
Those knowledges are brought by metrology and measurement theory and are rarely found in software development teams.
These premises bring the research question the presented works answer: What approach enables the conception of applications suitable to specific measurement procedures considering that the measurement procedures could be configured by the final user.
The presented works propose a platform for the development of measuring assistant applications.
The platform ensure the conformity of measuring processes without involving metrology experts.
A study of metrology enables to show the need of applications measuring process expert evaluation.
The verification is performed by confronting measuring processes to the knowledge scheme in the form of requests.
Those requests are described with a request language proposed by the scheme.
Measuring assistant applications require to propose to the user a measuring process that sequences measuring activities.
That editor uses a domain specific language dedicated to the description of measuring assistant applications.
The language is built upon concepts, formalisms and tools proposed by the metamodeling environment: Diagrammatic Predicat Framework (DPF).
Then, mobile platforms need to execute a behaviour conforming to the editor described one.
This eases their computation and the use of sensors.
The application is able to consider an implementation model and to build the corresponding agent network in order to propose a behaviour matching the end users needs.
The influence of English is evident on languages worldwide.
English is considered a global language of communication and is used by a large number of speakers worldwide for their interactions.
It is clear that English dominates many aspects of daily life, such as technology, science, the media and the Internet.
All the influences observed on the languages of the world that are due to the influence of English fall under the notion of Anglicisation, that covers all levels of linguistic analysis.
In my dissertation I study the influence of English on Modern Greek (MG), which has been particularly strong during the last two to three decades.
I aim to examine the phenomenon of Anglicisation in MG taking into account the English influence at all levels of linguistic analysis, focusing particularly on the lexical, phraseological and morphosyntactic level.
In order to analyze my data, I use dictionaries and grammars for MG, as well as MG text corpora, the Hellenic National Corpus, and the Corpus of Greek Texts, the text corpora available through the Sketch Engine platform, but also the customized text corpus that I built exclusively for my data through Sketch Engine.
Moreover, I investigate the factors responsible for the appearance and use of non-transliterated forms of the loanwords by examining their appearance in specialized vocabularies of MG, such as the vocabulary of sports and technology.
Regarding the phraseological patterns and morphosyntactic structures that calque the equivalent English ones, I compare the frequency of appearance of the calqued structure in MG to the frequency of appearance of the equivalent MG structure.
Furthermore, I try to determine the chronology of the insertion of English loanwords in MG, and finally, I draw some general conclusions, regarding Anglicisation in MG, based on the results of the research.
The prevalent deployment and usage of sensors in a wide range of sectors generate an abundance of multivariate data which has proven to be instrumental for researches, businesses and policies.
More specifically, multivariate data which integrates temporal evolution, i.e. Multivariate Time Series (MTS), has received significant interests in recent years, driven by high resolution monitoring applications (e.g. healthcare, mobility) and machine learning.
However, the explanations from the surrogate models cannot be perfectly faithful with respect to the original model, which is a prerequisite for numerous applications.
Faithfulness is critical as it corresponds to the level of trust an end-user can have in the explanations of model predictions, i.e. the level of relatedness of the explanations to what the model actually computes.
This thesis introduces new approaches to enhance both performance and explainability of MTS machine learning methods, and derive insights from the new methods about two real-world applications.
Today, learning computer assisted language is increasingly widespread in public and private institutions.
However, it is still far from expectations teachers and learners, and still does not meet their needs.
Computer-assisted language learning (CALL) today are rather test environments of learner knowledge and more like a support traditional learning.
In addition, the feedback provided by these systems remains basic and can not be adapted for independent learning, because it should be able to diagnose problems a learner with spelling, grammar, conjugation, etc., and intelligently generate adequate feedback according to the situation of learning.
This research exposes the capabilities of NLP tools to provide solutions to limitations CALL systems in order to develop a comprehensive system and CALL autonomous.
We present a complete architecture of a multilingual system learning the computer assisted language for language learners Foreign, French and Arabic.
This system could be used for learning languages by learners of the language as a second or foreign language.
The first part of our work focuses on the adaptation of tools and resources from NLP for them to be used in a language learning environment computer assisted.
These tools and resources, there are stemmers for Arabic and French corpora, electronic dictionaries, etc.
Then, in the second section presents the handwriting recognition online.
In this optical, we present a statistical approach based on neural network, then we present the design of the architecture of the recognition system as well the implementation of the recognition algorithm.
The second part of the presentation focuses on the development, integration and exploitation of NLP tools (morphological analyzers recognition system writing, dictionaries, etc.) in our learning system assisted language computer.
We also present the modules added to the platform to have a the complete architecture of a CALL system.
These modules, figure generator feedback that corrects the mistakes of learners and generate a relevant educational feedback which allows the learner to identify and faults.
Finally, we describe the tool automatic generation and automated various educational activities.
The second chapter (co-authored with Xavier Lambin) studies the impact of online reputation on ethnic discrimination.
The third chapter (co-authored with Rossi Abi Rafeh) develops and estimates a model of industry dynamics.
Faced with multiple entrants, the incumbent exploits these externalities by offering licensing deals to some entrants or by pursuing litigation in order to decrease the cost of delaying contracts offered to others.
The number of delayed entrants increases with patent strength.
The second chapter shows that reputation systems can mitigate ethnic discrimination by enabling ethnic minority sellers to accrue a high reputation quickly, leading buyers to update their beliefs.
Using data from a ridesharing platform, we find that minority drivers with no reviews make 12% less revenue relative to similar nonminority drivers.
To understand the mechanism behind this process, we construct a model of career concerns' of discriminated sellers in the presence of a reputation system.
The model's estimates show that minority drivers, who just entered the platform, face overly pessimistic beliefs about the quality of their service.
To alter these beliefs, they exert high effort and offer low introductory prices, swiftly boosting their reputation.
Counterfactual simulations reveal that the cost of incorrect prior beliefs is high and that the reputation system strictly benefits minority drivers.
The final chapter studies the entry and pricing decisions of sellers in a market with a reputation system.
We provide a model of dynamic oligopoly with heterogeneity in marginal and opportunity costs and individual reputation as a state variable.
We show that new sellers are generally less likely to reenter the platform than incumbents and sellers who have a lower chance of entering in subsequent periods set on average higher prices.
The mechanism behind these findings is selection on marginal costs.
We apply our model to a dataset on sellers on a large ridesharing marketplace.
We showcase a negative correlation of tenure on the platform, measured by the number of reviews, and prices set by drivers.
However, after accounting for drivers' unobserved characteristics, which we interpret as marginal costs, we find a positive relationship.
We provide, further, evidence of selection on unobservable by studying reentry decisions.
Finally, we calibrate our dynamic model to uncover the distribution of opportunity costs.
The main objective of the project is to investigate novel unsupervised or weakly supervised machine learning techniques for the automatic structuring of spoken conversations, given its audio recording and (manual or automatic) speech transcription.
First, one will focus on named speaker diarization.
Without any prior knowledge on the speakers involved in the conversation, this task can be divided into two subtasks: gathering the names of all speakers (thanks to named entity recognition and entity linking, for instance) followed by the attribution of each speech turns to the corresponding speaker (using natural language understanding and speaker recognition, for instance).
Then, one will address the problem of classifying conversations according to the content of the exchanges (are speakers arguing, fighting, small-talking?).
To that end, one could enrich natural language processing techniques with acoustic cues (prosody, rythm, etc.).
The proposed approaches will be applied to movies or TV series, for which a significant amount of annotations are already available.
The aim of this thesis is to develop a tool that brings together all the research launched at IDHN within the framework of the analysis through platforms, in order to combine them into a single interface and to make them work together, with a view to analyzing the phenomenon of influence.
Our choice is viky.ai, an open source platform developed by the company Pertimm to create and share linguistic agents, which analyze texts in all languages.
Creating modifiable, reusable analytics agents that can be shared collaboratively by the community, can greatly simplify the handling of textual analysis on problems that are similarly found in many areas.
viky.ai provides a unique codeless user interface.
This is a new way to create semantic modules that are easy to integrate into any system through API.
The semantic bricks, named viky.ai's agents, are multilingual assistants to find relevant data.
These bricks consist of entities and interpretations and can be combined endlessly to build models in the areas of semantic analysis.
Many NLP experts agree that NLP technologies combining rules and learning will be the best combination.
More specifically, this thesis aims to develop agents in two research fiels already outlined: detection, characterization of ideologies and links to emotional speech; detection of topic changes in political discourses.
This thesis in computer science take place in the ILE domain (Interactive Learning Environment) and was realized within the AGATE project (an Approach for Genericity in Assistance To complEx tasks) that aims at proposing generic models and unified tools to make possible the setup of assistance systems in various existing applications.
In this project, an assistance editor allows assistance designers to define assistance systems and a generic assistance engine executes these assistance systems on the various target-applications without disturbing them to help final users.
In the educational context, teachers can want to set up assistance system to complete the pedagogical or non-pedagogical software used by learners.
Pedagogical engineers therefore have the role of assistance designers and learners are end-users of such assisted applications.
In order to answer this research question, we realized this thesis in two steps: first, the study of existing assistances within applications used in the educational context, then the exploitation and enrichment of models and tools of the AGATE project to adapt them to the educational context.
In the first step, we studied the applications used by teachers in their courses as well as existing works proposing assistance system.
In the second step, we confronted the models and tools proposed previously in the AGATE project to the characteristics of the assistance identified in educational context.
The limitations of the previous models and tools led us to propose two contributions to the aLDEAS language and the SEPIA system in order to adapt them to the educational context.
Whether in an educational context or not, it is important to be able to define easily and explicitly several modes of articulation between the different elements of an assistance system.
We therefore proposed a model of articulation between aLDEAS rules in five modes: successive, interactive, simultaneous, progressive, independent.
This model and this process have been implemented in SEPIA-edu.
Then, a pedagogical guidance model allows to define different types of pedagogical guidance (free, sequential, contextualized, temporal, personalized).
This model of activity, this pedagogical guidance pattern and this process have been implemented in SEPIA-edu
Context and goal:It is almost a truism to affirm that one of the main features of speech is its variability: variability inter-gender, inter-speaker, but also variability from one context to another, or from one repetition to another for a given subject.
Variability underlies at the same time the beauty of speech, the complexity of its treatment by speech technologies, and the difficulty for understanding its mechanism.
In this thesis we study certain aspects of speech variability, our starting point being the variability characterizing the repetitions of a given utterance by a given subject, in a given condition, which we call intrinsic variability.
Models of speech motor control have mainly focused on the contextual aspects of speech variability, and have rarely considered its intrinsic component, even though it is this fundamental component of variability that gives speech it naturalness.
The main goal of this thesis is to address the contextual and intrinsic component of speech variability in an integrative computational framework.
To this aim, we postulate that the main component of the intrinsic variability of speech is not just execution noise, but that it results from a control strategy where intrinsic variability characterizes the abundance of possible productions of the intended speech item.
Methodology:
We formalize this idea in a probabilistic computational framework, Bayesian modeling, where the abundance of possible realizations of a given speech item is naturally represented as uncertainty, and where variability is thus formally manipulated.
We illustrate the pertinence of this approach with three main contributions.
Results:
Firstly, we reformulate in Bayesian terms an existing model of speech motor control, the GEPPETO model, and demonstrate that this Bayesian reformulation, which we call B-GEPPETO, contains GEPPETO as a particular case.
The somatosensory characterization of speech motor goals involved a certain number of hypotheses that we intended to evaluate with two experimental studies.
This original analysis is made possible thanks to the unified representation of knowledge in the model, which enables to account for production and perception processes in a single computational framework.
Taken together, these contributions illustrate how the Bayesian framework offers a structured and systematic approach for the construction of models in cognitive sciences.
The framework facilitates the development of models and their progressive complexification by specifying and clarifying underlying assumptions.
The thesis project is at the junction of computational linguistics and field linguistics.
In a context where linguistic diversity is threatened by the disappearance of almost half of the languages spoken on earth today, it is becoming crucial to provide field linguistics specialists with automatic or semi-automatic tools to collect linguistic data, annotate, enrich and archive them, and thereby try to preserve part of the cultural heritage of humanity.
These issues are of growing interest within the automatic language processing community, in the context of sustained collaboration with teams of linguists.
This thesis will thus take place within the framework of an international collaboration involving teams of linguists in France and Germany, with the support of the French National Research Agency.
From a methodological point of view, the first task can be approached with statistical modelling tools, such as non-parametric Bayesian models, whose applicability will be studied here in a context where the data to be annotated are small, but where there are additional resources that can potentially be mobilised (lexicons, elements of morphological description, etc.).
The development of these various types of statistical models will be validated in the languages of the project through experiments involving a close collaboration with users of these tools.
Global competitiveness has challenged manufacturing industry to rationalise different ways of bringing to the market new products in a short lead-time with competitive prices while ensuring higher quality levels.
Modern PDP has required simultaneously collaborations of multiple groups, producing and exchanging information from multi-perspectives within and across institutional boundaries.
However, it has been identified semantic interoperability issues in view of the information heterogeneity from multiple perspectives and their relationships across product development.
This research proposes a conceptual framework of an Interoperable Product Design and Manufacturing based on a set of core ontological foundations and semantic mapping approaches.
An experimental system has been performed using the Protégé tool to model the core ontologies and the Java platform integrated with the Jena to develop the interface with the user.
The conceptual framework proposed in this research has been tested through experiments using rotational plastic products.
Therefore, this research has shown that information rigorously-defined and their well-defined relationships can ensure the effectiveness of product design and manufacturing in a modern and collaborative PDP
This thesis includes three experiments and two pre-tests (N = 1135) in which three fundamental aspects of static message design on the Internet are studied: its format (infographics, audio or text), its colour and typography, on the theme of electronic waste recycling (studies 1 and 2) and then on human migration (study 3).The study of graphic aspects is relevant if we want to increase the persuasive power of a message.
The format plays a major role (study 1a), making it possible to change attitudes and to anchor this change over time.
Colours, on the other hand, do not seem to vary the persuasive force of the message or to lead readers to act in favour of recycling (study 1b).
Nor does typography seem to play a role in the persuasive dynamic, whether it is considered legible or difficult to read (study 2).
Theoretical approaches regarding the personality of typographies and their coherence with the context are developed.
The analysis of the components of the ELM revealed, in each study, the strong link between the attitude of individuals and their sense of personal responsibility towards the theme addressed as well as their a priori knowledge.
We have also seen that the levers of persuasion are not systematically the same according to the need for cognition.
We suggest that persuasive messages should adopt a format that allows for central analysis at low cognitive cost, using a main colour and typography that are both readable and consistent with the theme developed, with arguments that reinforce readers'sense of responsibility.
Development of collaborative environment is a complex process.
The complexity lies in the fact that collaborative environment development involves a lot of decision making.
Several tradeoffs need to be made to satisfy current and future requirements from a potentially various set of user profiles.
The handling of these complexities poses challenges for researcher, developers and companies.
The knowledge required to make suitable design decisions and to rigorously evaluate those design decisions is usually broad, complex, and evolving.
The results generate three models: SyCoW (synchronous collaborative work), SyCoE (synchronous collaborative environment) and SyCoEE (synchronous collaborative environment evaluation).
In Part-II of this thesis we proposed a process for selection/development of collaborative environment, where we demonstrate how SyCoW, SyCoE and SyCoEE support this process in different ways.
The results of evaluation confirmed the usability of MT-DT and provide arguments for our choices which we made during development of MT-DT.
Transfer learning with deep convolutional neural networks significantly reduces the computation and data overhead of the training process and boosts the performance on the target task, compared to training from scratch.
However, transfer learning with a deep network may cause the model to forget the knowledge acquired when learning the source task, leading to the so-called catastrophic forgetting.
Since the efficiency of transfer learning derives from the knowledge acquired on the source task, this knowledge should be preserved during transfer.
This thesis solves this problem of forgetting by proposing two regularization schemes that preserve the knowledge during transfer.
First we investigate several forms of parameter regularization, all of which explicitly promote the similarity of the final solution with the initial model, based on the L1, L2, and Group-Lasso penalties.
We also propose the variants that use Fisher information as a metric for measuring the importance of parameters.
The second regularization scheme is based on the theory of optimal transport, which enables to estimate the dissimilarity between two distributions.
We benefit from optimal transport to penalize the deviations of high-level representations between the source and target task, with the same objective of preserving knowledge during transfer learning.
With a mild increase in computation time during training, this novel regularization approach improves the performance of the target tasks, and yields higher accuracy on image classification tasks compared to parameter regularization approaches.
Nowadays, multiple actors of Internet technology are producing very large amounts of data.
Sensors, social media or e-commerce, all generate real-time extending information based on the 3 Vs of Gartner: Volume, Velocity and Variety.
The primary goal of this study is to establish, based on these approaches, an integrative vision of data life cycle set on 3 steps, (1) data synthesis by selecting key-values of micro-data acquired by different data source operators, (2) data fusion by sorting and duplicating the selected key-values based on a de-normalization aspect in order to get a faster processing of data and (3) the data transformation into a specific format of map of maps of maps, via Hadoop in the standard MapReduce process, in order to define the related graph in applicative layer.
In addition, this study is supported by a software prototype using the already described modeling tools, as a toolbox compared to an automatic programming software and allowing to create a customized processing chain of BigData
With the introduction of clinical data warehouses, more and more health data are available for research purposes.
While a significant part of these data exist in structured form, much of the information contained in electronic health records is available in free text form that can be used for many tasks.
In this manuscript, two tasks are explored: the multi-label classification of clinical texts and the detection of negation and uncertainty.
The first is studied in cooperation with the Rennes University Hospital, owner of the clinical texts that we use, while, for the second, we use publicly available biomedical texts that we annotate and release free of charge.
In order to solve these tasks, we propose several approaches based mainly on deep learning algorithms, used in supervised and unsupervised learning situations.
Our research focuses on the recommendation of new job offers that have just been posted and have no interaction history (cold start).
To this objective, we adapt well-knowns recommendations systems in the field of e-commerce by exploiting the record of use of all job seekers on previous offers.
One of the specificities of the work presented is to have considered real data, and to have tackled the challenges of heterogeneity and noise of textual documents.
The presented contribution integrates the information of the collaborative data to learn a new representation of text documents, which is required to make the so-called cold start recommendation of a new offer.
The new representation essentially aims to build a good metric.
The search space considered is that of neural networks.
Neural networks are trained by defining two loss functions.
The first seeks to preserve the local structure of collaborative information, drawing on non-linear dimension reduction approaches.
The second is inspired by Siamese networks to reproduce the similarities from the collaborative matrix.
The scaling up of the approach and its performance are based on the sampling of pairs of offers considered similar.
The interest of the proposed approach is demonstrated empirically on the real and proprietary data as well as on the CiteULike public benchmark.
Finally, the interest of the approach followed is attested by our participation in a good rank in the international challenge RecSys 2017 (15/100, with millions of users and millions of offers).
Over the past few years, neural network (NN) architectures have been successfully applied to many Natural Language Processing (NLP) applications, such as Automatic Speech Recognition (ASR) and Statistical Machine Translation (SMT).
For the language modeling task, these models consider linguistic units (i.e words and phrases) through their projections into a continuous (multi-dimensional) space, and the estimated distribution is a function of these projections.
Also qualified continuous-space models (CSMs), their peculiarity hence lies in this exploitation of a continuous representation that can be seen as an attempt to address the sparsity issue of the conventional discrete models.
In the context of SMT, these echniques have been applied on neural network-based language models (NNLMs) included in SMT systems, and oncontinuous-space translation models (CSTMs).
These models have led to significant and consistent gains in the SMT performance, but are also considered as very expensive in training and inference, especially for systems involving large vocabularies.
To overcome this issue, Structured Output Layer (SOUL) and Noise Contrastive Estimation (NCE) have been proposed; the former modifies the standard structure on vocabulary words, while the latter approximates the maximum-likelihood estimation (MLE) by a sampling method.
All these approaches share the same estimation criterion which is the MLE ; however using this procedure results in an inconsistency between the objective function defined for parameter estimation and the way models are used in the SMT application.
The work presented in this dissertation aims to design new performance-oriented and global training procedures for CSMs to overcome these issues.
The main contributions lie in the investigation and evaluation of efficient training methods for (large-vocabulary) CSMs which aim:(a) to reduce the total training cost, and (b) to improve the efficiency of these models when used within the SMT application.
On the one hand, the training and inference cost can be reduced (using the SOUL structure or the NCE algorithm), or by reducing the number of iterations via a faster convergence.
This thesis provides an empirical analysis of these solutions on different large-scale SMT tasks.
On the other hand, we propose a discriminative training framework which optimizes the performance of the whole system containing the CSM as a component model.
The experimental results show that this framework is efficient to both train and adapt CSM within SMT systems, opening promising research perspectives.
The concept Web of things (WOT) is gradually becoming a reality as the result of development of network and hardware technologies.
Nowadays, there is an increasing number of objects that can be used in predesigned applications.
The world is thus more tightly connected, various objects can share their information as well as being triggered through a Web-like structure.
However, even if the heterogeneous objects have the ability to be connected to the Web, they cannot be used in different applications unless there is a common model so that their heterogeneity can be described and understood.
In this thesis, we want to provide a common model to describe those heterogeneous objects and use them to solve user's problems.
Users can have various requests, either to find a particular object, or to fulfill some tasks.
We highlight thus two research directions.
Thus, we first study the existing technologies, applications and domains where the WOT can be applied.
We compare the existing description models in this domain and find their insufficiency to be applied in the WOT...
This PhD thesis deals with supervized machine learning and statistics.
Under assumptions, such as the Bernstein condition, such rates are attainable.
A robust estimator is an estimator with good theoretical guarantees, under as few assumptions as possible.
This question is getting more and more popular in the current era of big data.
Large dataset are very likely to be corrupted and one would like to build reliable estimators in such a setting.
We show that the well-known regularized empirical risk minimizer (RERM) with Lipschitz-loss function is robust with respect to heavy-tailed noise and outliers in the label.
When the class of predictor is heavy-tailed, RERM is not reliable.
In this setting, we show that minmax Median of Means estimators can be a solution.
Surprisingly, in large dimension, interpolating the data does not necessarily implies over-fitting.
We study a high dimensional Gaussian linear model and show that sometimes the over-fitting may be benign.
Chemical flavor analysis provides a list of the odorants contained in a food product but is not sufficient to predict the odor resulting from their mixture.
Indeed, odor perception relies on the processing by the olfactory system of many odorants embedded in complex mixtures and several perceptual interactions can occur.
Thus, the prediction of the perceptual outcome of a complex odor mixture remains challenging and two main approaches emerge from the literature review.
On the other hand, methodologies relying on recombination strategies after the chemical analyses of flavor have been successfully applied to identify those odorants that are key to the food odor.
However, the choices of odorants to be recombined are mostly based on empirical approaches.
Thus, two questions arise: How can we predict the odor quality of a mixture on the basis of the molecular structure of its odorants?
How can we improve food flavor analysis in order to predict the odor of a food containing several tens of odorants?
These two questions are at the basis of the thesis and of this manuscript which is divided in two main axes.
The first axis describes the development of a model based on the concept of angle distances computed from the molecular structure of odorants in order to predict the odor similarity between mixtures.
The results highlight the importance of taking into account the odor intensity dimension to reach a good prediction level.
Moreover, several perspectives are proposed to extend the model prediction beyond the similarity dimension and to predict more qualitative dimensions of odors.
The second axis presents an innovative strategy which allows integrating experts' knowledge in the flavor analysis procedure.
Three different types of heterogeneous data are embedded in a mathematical model: chemical data, sensory data and knowledge from expert flavorists.
Experts' knowledge is integrated owing to the development of an ontology, which is further used to define fuzzy rules optimized by evolutionary algorithms.
The final output of the model is the prediction of red wines' odor profile on the basis of their odorants' composition.
Overall, the thesis work brings original results allowing a better understanding of food odor construction and gives insights on the underlying relationships within the odor perceptual space for complex mixtures.
The crowd counting task is an important research problem.
Now more and more people are concerned about safety issues.
Considering the scenario of a crowded scene: a population density system analyzes the crowds and triggers a warning to divert the crowds when their population density exceeds a normal range.
With such a system, the incident of the Shanghai New Year's stampede will not happen again.
The most critical aspect, in some public places, is that we can not install an intelligent video surveillance system.
So how do we estimate the high-density crowd area to avoid crowd trampling accidents?
Facing these challenges, we propose implementation of real time reconfigurable embedded architecture for people counting in a crowd area.
First, our work integrates the features of HOG and LBP, which not only combines the effective identification information of multiple features, but also eliminates most of the redundant information, thereby realizing effective compression of information, saving information storage space.
Then, in terms of crowd counting, we use multiple sources of information, namely HOG, LBP and CANNY based filtering.
These sources provide separate estimates of the number of counts and other statistical measures, through the support vector Machine SVM, classification.
At the same time, in order to effectively solve the problem of extracting scale-related features in crowd counting. We propose a new framework M-MCNN based on MCNN for crowd counting on any single image.
M-MCNN not only contains the original three columns of convolutional neural networks with different filter sizes, but replaces the fully connected layers with a convolutional layer of 1*1 filters, so the input image of the model can be of any size.
Moreover, in a single individual sample, we greatly improve the learning of sample features by extracting the texture features of a single human head, and better use it for datasets.
Finally, we implement our new framework M-MCNN through FPGA, and transplant it on the drone to estimate and predict the high-density crowd area in real time.
Our model achieved good results in crowd counting.
This work is part of the "TEL Environments customization" project studying the use of activity traces in the evaluation and customization of mediated learning situations.
Analyzing traces is generally performed using analysis tools developed by researchers specifically for their needs.
Published research results can generally not be verified or compared, due to difficulties of sharing corpora and analysis tools.
The aim of this research work is to provide researchers using TEL environments in their researches with a platform to share corpora of contextualized interaction traces and analyses performed on them, and to analyse those corpora using shared analysis and visualization tools.
Heterogeneity of traces produced by TEL environments, due to the diversity of learning domains and to analysis needs makes the proposition of a common representation cannot satisfy the various needs of multidisciplinary researchers.
We propose the “proxy” approach, a participative and incremental solution based on an ontology which defines three models: a corpus model defining the structure and description metadata of the corpus and its contents, a semantic model defining generic concepts which can be retrieved in shared corpora, and an operational model defining a set of operations ensuring interoperability between shared corpora and analysis tools.
Based on this approach, we propose a platform architecture for sharing traces corpora and analysis tools allowing researchers to share their own corpora, to access to shared corpora, and to analyze them.
Since the early days of the DARPA challenge, the design of self-driving cars is catching increasing interest.
This interest is growing even more with the recent successes of Machine Learning algorithms in perception tasks.
While the accuracy of these algorithms is irreplaceable, it is very challenging to harness their potential.
Real time constraints as well as reliability issues heighten the burden of designing efficient platforms.
We discuss the different implementations and optimization techniques in this work.
We tackle the problem of these accelerators from two perspectives: performance and reliability.
We propose two acceleration techniques that optimize time and resource usage.
On reliability, we study the resilience of Machine Learning algorithms.
We propose a tool that gives insights whether these algorithms are reliable enough for safety critical systems or not.
The Resistive Associative Processor accelerator achieves high performance due to its in-memory design which remedies the memory bottleneck present in most Machine Learning algorithms.
As for the constant multiplication approach, we opened the door for a new category of optimizations by designing instance specific accelerators.
The obtained results outperforms the most recent techniques in terms of execution time and resource usage.
Combined with the reliability study we conducted, safety-critical systems can profit from these accelerators without compromising its security.
In recent years, deep learning has lead to groundbreaking developments in the image and natural language processing fields.
However, in many domains, input data consists in neither images nor text documents, but in time series that describe the temporal evolution of observed or computed quantities.
In this thesis, we study and introduce different representations for time series, based on deep learning models.
Firstly, in the autonomous driving domain, we show that, the analysis of a temporal window by a neural network can lead to better vehicle control results than classical approaches that do not use neural networks, especially in highly-coupled situations.
Thirdly, in the human pose motion generation domain, we introduce 2D convolutional generative adversarial neural networks where the spatial and temporal dimensions are convolved in a joint manner.
Finally, we introduce an embedding where spatial representations of human poses are sorted in a latent space based on their temporal relationships.
Once devices are connected to the local network, applications deployed for example on a smart phone, a PC or a home gateway, discover the plug-n-play devices and act as control points.
The aim of such applications is to orchestrate the interactions between the devices such as lights, TVs and printers, and their corresponding hosted services to accomplish a specific human daily task like printing a document or dimming a light.
Even similar devices supporting the same services represent their capabilities in a different representation format and content.
Additionally, the deployed application must use multiple protocols stacks to interact with the device.
To accomplish interoperability between plug-n-play devices and applications, we propose a generic approach which consists in automatically generating proxies based on an ontology alignment.
The alignment contains the correspondences between two equivalent devices descriptions.
Consequently, the UPnP printing application will interact transparently with the generated proxy which adapts and transfers the invocations to the real DPWS printer.
Despite the typological differences between French and Turkish, we make an assumption that comparison between these two languages can be made in terms of meaning and meaning construction processes.
This approach, based on the establishment of a repertoire of meanings, could be possibly extended to other languages that do not have such kinds of tools.
Computer vision is a field that includes methods for the acquisition, processing, analysis and understanding of images to produce numerical or symbolic information.
A research contributing to the development of this area is to replicate the capabilities of human vision to perceive and understand images.
Our thesis is part of this research axis.
We propose several original contributions belonging to the context of graphics recognition and spotting context.
The originality of the proposed approaches is the proposal of an interesting alliance between the Formal Concept Analysis and the Computer Vision fields.
We face the study of the FCA field and more precisely the adaptation of the structure of concept lattice and its use as the main tool of our work.
The main feature of our work lies in its generic aspect because the proposed methods can be combined with various other tools keeping the same strategies and following a similar procedure.
It is a concise, accurate and flexible search space thus facilitating decision making.
Our contributions are recorded as part of the recognition and localization of symbols in graphic documents.
We propose to recognize and spot symbols in graphical documents (technical drawings for example) using the alliance between the bag of words representation and the Galois lattice formalism.
We opt for various methods belonging to the computer vision field
Resources like terminologies or ontologies are used in a number of applications, including documentary description and information retrieval.
Different methodologies have been proposed to build such resources, which range from experts'interviews to the use of textual corpora.
This thesis focuses on the use of natural language processing methodologies, meant to help the building of ontologies from textual corpora, to build the particular type of resource, namely differential ontologies.
Extracting reliable source catalogs from images is crucial for a broad range of astronomical research topics.
However, the efficiency of current source detection methods becomes severely limited in crowded fields, or when images are contaminated by optical, electronic and environmental defects.
Performance in terms of reliability and completeness is now often insufficient with regard to the scientific requirements of large imaging surveys.
In this thesis, we develop new methods to produce more robust and reliable source catalogs.
We leverage recent advances in deep supervised learning to design generic and reliable models based on convolutional neural networks (CNNs).We present MaxiMask and MaxiTrack, two convolutional neural networks that we trained to automatically identify 13 different types of image defects in astronomical exposures.
We also introduce a prototype of a multi-scale CNN-based source detector robust to image defects, which we show to significantly outperform existing algorithms.
We discuss the current limitations and potential improvements of our approach in the scope of forthcoming large scale surveys such as Euclid.
Associated with preconceived images and full of prejudices, romance is a consumption product which embedies a social and cultural mirroring.
In France for several decades, romance has gained an heterogeneous but loyal readership/audience.
What are the motives of those novels which are said to be all similar?
What do they represent for the readership?
How are they perceived by those who read them?
How are they read, apprehended and interpreted?
These are some of the questions we try to give an answer to in this multidisciplinary work.
This research enabled to unveil the mecanisms involved in the evolution of romance since World War II, and to observe the different modes of reading and reception of these novels
Since twenty years ago, access and use of medical data become major issues for health professional and lay people.
In this context, multiphe health terminologies were be developped.
Theses terminologies have mostly different format and purpose: SNOMED International for clinical coding, CCAM and ISD10 used for epidemiological and medico-economic purposes, MeSH thesaurus for bibliographic databases.
Hence, and given the growing need for cooperation between health practitioners, a new "intreoprable" terminoloy base is now required.
We use also UMLS to provide a large coverage of relations between terminologies.
We benefit in the case of this PhD of an extensive experience in the Natural Language Processing field from a multiple CISMEF and LERTIM research projects.
Social sciences and Humanities research is often based on large textual corpora, that it would be unfeasible to read in detail.
Natural Language Processing (NLP) can identify important concepts and actors mentioned in a corpus, as well as the relations between them.
Such information can provide an overview of the corpus useful for domain-experts, and help identify corpus areas relevant for a given research question.
To automatically annotate corpora relevant for Digital Humanities (DH), the NLP technologies we applied are, first, Entity Linking, to identify corpus actors and concepts.
Second, the relations between actors and concepts were determined based on an NLP pipeline which provides semantic role labeling and syntactic dependencies among other information.
Part I outlines the state of the art, paying attention to how the technologies have been applied in DH.
Generic NLP tools were used.
As the efficacy of NLP methods depends on the corpus, some technological development was undertaken, described in Part II, in order to better adapt to the corpora in our case studies.
Part II also shows an intrinsic evaluation of the technology developed, with satisfactory results.
The technologies were applied to three very different corpora, as described in Part III.
Second, the PoliInformatics corpus, with heterogeneous materials about the American financial crisis of 2007-2008.
Finally, the Earth Negotiations Bulletin (ENB), which covers international climate summits since 1995, where treaties like the Kyoto Protocol or the Paris Agreements get negotiated.
For each corpus, navigation interfaces were developed.
These user interfaces (UI) combine networks, full-text search and structured search based on NLP annotations.
As an example, in the ENB corpus interface, which covers climate policy negotiations, searches can be performed based on relational information identified in the corpus: the negotiation actors having discussed a given issue using verbs indicating support or opposition can be searched, as well as all statements where a given actor has expressed support or opposition.
Relation information is employed, beyond simple co-occurrence between corpus terms.
The UIs were evaluated qualitatively with domain-experts, to assess their potential usefulness for research in the experts'domains.
First, we payed attention to whether the corpus representations we created correspond to experts'knowledge of the corpus, as an indication of the sanity of the outputs we produced.
Second, we tried to determine whether experts could gain new insight on the corpus by using the applications, e.g. if they found evidence unknown to them or new research ideas.
Examples of insight gain were attested with the ENB interface; this constitutes a good validation of the work carried out in the thesis.
Overall, the applications'strengths and weaknesses were pointed out, outlining possible improvements as future work.
This thesis describes the creation of the French FrameNet (FFN), a French language FrameNet type resource made using both the Berkeley FrameNet (Baker et al., 1998) and two morphosyntactic treebanks: the French Treebank (Abeillé et al., 2003) and the Sequoia Treebank (Candito et Seddah, 2012).
The Berkeley FrameNet allows for semantic annotation of prototypical situations and their participants.
It consists of: a) a structured set of prototypical situations, called frames. These frames incorporate semantic characterizations of the situations'participants (Frame Elements, or FEs); b) a lexicon of lexical units (LUs) which can evoke those frames; c) a set of English language frame annotations.
In order to create the FFN, we designed a “domain by domain” methodology: we defined four “domains”, each centered on a specific notion (cause, verbal communication, cognitive stance, or commercial transaction). We then sought to obtain full frame and lexical coverage for these domains, and annotated the first 100 corpus occurrences of each LU in our domains.
This strategy guarantees a greater consistency in terms of frame structuring than other approaches and is conducive to work on both intra-domain and inter-domains frame polysemy.
Our annotating frames on continuous text without selecting particular LU occurrences preserves the natural distribution of lexical and syntactic characteristics of frame-evoking elements in our corpus.
At the present time, the FFN includes 105 distinct frames and 873 distinct LUs, which combine into 1,109 LU-frame pairs (i.e. 1,109 senses).
16,167 frame occurrences, as well as their FEs, have been annotated in our corpus.
In this thesis, I first situate the FrameNet model in a larger theoretical background.
I then justify our using the Berkeley FrameNet as our resource base and explain why we used a domain-by-domain methodology.
I next try to clarify some specific BFN notions that we found too vague to be coherently used to make the FFN.
Specifically, I introduce more directly syntactic criteria both for defining a frame's lexical perimeter and for differentiating core FEs from non-core ones.
Then, I describe the FFN creation itself first by delimitating a structure of frames that will be used in the resource and by creating a lexicon for these frames.
I then introduce in detail the Cognitive Stances notional domain, which includes frames having to do with a cognizer's degree of certainty about some particular content.
Next, I describe our methodology for annotating a corpus with frames and FEs, and analyze our treatment of several specific linguistic phenomena that required additional consideration (such as object complement constructions).
Finally, I give quantified information about the current status of the FFN and its evaluation.
I conclude with some perspectives on improving and exploiting the FFN.
This thesis bends over the analysis and the constraints of elaboration of components for open domain questions answering systems.
The questions answering systems (QAs) is adapted to this task, because they associate a factual question with a precise answer.
It is suggested adapting the QAs to a task of interaction to define to them a frame of generic adaptation to the systems of dialogue.
The existing techniques for the QAs in opened domain, the various models of man-machine dialog as well as systems in the limit between the dialogue and the search for information are studied.
Then, is presented the construction of a model of structure allowing to describe the interactions between questions.
Of the formal frame, is deduct a method of calculation which is estimated.
To discuss this evaluation and see how using our model, is realized the analysis of search engines for the systems of questions answers.
In a last part, is presented a use of the data calculated for the improvement of the results of answers to the questions.
The Abjad is essential to translations, or high quality information research.
It was evident that the whole Abjad was not taken into account.
Laughter is a social vocalization universal across cultures and languages.
It is ubiquitous in our dialogues and able to serve a wide range of functions.
Laughter has been studied from several perspectives, but the classifications proposed are hard to integrate.
Despite being crucial in our daily interaction, relatively little attention has been devoted to the study of laughter in conversation, attempting to model its sophisticated pragmatic use, neuro-correlates in perception and development in children.
In the current thesis a new comprehensive framework for laughter analysis is proposed, crucially grounded in the assumption that laughter has propositional content, arguing for the need to distinguish different layers of analysis, similarly to the study of speech: form, positioning, semantics and pragmatics.
Preliminary investigations are conducted on the viability of a laughter form-function mapping based on acoustic features and on the neuro-correlates involved in the perception of laughter serving different functions in natural dialogue.
Our results give rise to novel generalizations about the placement, alignment, semantics and function of laughter, stressing the high pragmatic skills involved in its production and perception.
The development of the semantic and pragmatic use of laughter is observed in a longitudinal corpus study of 4 American-English child-mother pairs from 12 to 36 months of age.
Results show that laughter use undergoes important development at each level analysed, which complies with what could be hypothesised on the base of phylogenetic data, and that laughter can be an effective means to track cognitive/communicative development, and potential difficulties or delays at a very early stage.
We have done several studies to investigate the phenomenon between pairs of unknown people, good friends, and between people coming from the same family.
We expect that the amplitude of convergence is proportional to the social distance between the two speakers.
We found this result.
Then, we have studied the knowledge of the linguistic target impact on adaptation.
To characterize the phonetic convergence, we have developed two methods: the first one is based on a linear discriminant analysis between the MFCC coefficients of each speaker and the second one used speech recognition techniques.
Finally, we characterized the phonetic convergence with a subjective measurement using a new perceptual test called speaker switching.
The test was performed using signals coming from real interactions but also with synthetic data obtained with the harmonic plus
The actors of a company must collaborate efficiently to achieve the common purposes of the company in an environment permanently evolving, while most current information processing systems are based on the analysis of the company's organization and on the collection of the user's needs at a given time.
Whereas mediations, which permit actors'coordination, information exchanges, tasks'specifications and activity evaluation, would be real objects to conceive tools to support collaborative conception activities.
This fits undergoes almost exclusively by communication and often following nonprescriptive circuits.
The methods and tools of work organization can benefit from new technologies of communication to return these more effective adaptations.
A new type of software will be then necessary to disambiguate, to facilitate, to structure and to allow complex searches in the exchanges arising among the actors (human beings and software agents) of a cooperative socio-technical system.
Multi-Agents Systems, dynamically adaptable, can follow this evolutions and take into account each actors'specificities.
This thesis aims to develop a natural language processing tool able to analyze and reformulate arithmetic problems in French 3rd primary school cycle.
More precisely, this tool should allow:
To detect linguistic difficulties and challenges in these mathematical problems.
To classify problems according to the linguistic difficulties they present.
To generate a reformulation of these problems in order to facilitate their understanding.
To evaluate the output of the generation, or compare 2 versions of a same problem.
This thesis focuses on Arabic embedded text detection and recognition in videos.
Different approaches robust to Arabic text variability (fonts, scales, sizes, etc.) as well as to environmental and acquisition condition challenges (contrasts, degradation, complex background, etc.) are proposed.
We introduce different machine learning-based solutions for robust text detection without relying on any pre-processing.
The first method is based on Convolutional Neural Networks (ConvNet) while the others use a specific boosting cascade to select relevant hand-crafted text features.
Standing out from the dominant methodology of hand-crafted features, we propose to learn relevant text representations from data using different deep learning methods, namely Deep Auto-Encoders, ConvNets and unsupervised learning models.
Each one leads to a specific OCR (Optical Character Recognition) solution.
Sequence labeling is performed without any prior segmentation using a recurrent connectionist learning model.
Proposed solutions are compared to other methods based on non-connectionist and hand-crafted features.
Both OCR and language model probabilities are incorporated in a joint decoding scheme where additional hyper-parameters are introduced to boost recognition results and reduce the response time.
Given the lack of public multimedia Arabic datasets, we propose novel annotated datasets issued from Arabic videos.
The OCR dataset, called ALIF, is publicly available for research purposes.
Our proposed solutions were extensively evaluated.
Obtained results highlight the genericity and the efficiency of our approaches, reaching a word recognition rate of 88.63% on the ALIF dataset and outperforming well-known commercial OCR engine by more than 36%.
This doctoral thesis addresses the process of developing a subtitling software adapted to the context of higher education from the point of view of a professional subtitler.
First, it reviews the theoretical foundations of Audiovisual Translation and subtitling with the aim of defining the linguistic and semiotic characteristics of the audiovisual text and its translation.
Second, it studies the explicit extratextual norms and identifies the functionalities required for their implementation allowing subtitlers to adapt the text to the target culture's expectations.
In addition, this work deals with the professional aspect of this practice, analysing the impact that technological advances have on the subtitling process and on the tools available to translators.
It also presents the use of subtitled audiovisual material in higher education environments through the specific case of a multilingual and multicultural distance-learning environment.
This analysis of subtitling from different perspectives concludes with the creation of Miro Translate, a hybrid cloud platform specially designed for subtitling video lectures.
Finally, the quality of this tool is studied through a usability test that measures users'perceived satisfaction, efficiency and effectiveness in order to identify the necessary actions for the improvement of the platform.
1. Econometric analysis of the impact of social media movements
2. Development of new methods to analyze social media data
3. Development of new natural language processing techniques for applied economics
In the first part of the thesis, we propose a generalized optimization approach for the graph-based semi-supervised learning which implies as particular cases the Standard Laplacian, Normalized Laplacian and PageRank based methods.
Using random walk theory, we provide insights about the differences among the graph-based semi-supervised learning methods and give recommendations for the choice of the kernel parameters and labelled points.
This application demonstrates that the proposed family of methods scales very well with the volume of data.
Specifically, we propose random walk based algorithms for quick detection of large degree nodes and nodes with large values of Personalized PageRank.
Finally, in the end of the thesis we suggest new centrality measure, which generalizes both the current flow betweenness centrality and PageRank.
This new measure is particularly well suited for detection of network vulnerability.
Considered a historian who sacrifices his rigor and accuracy for the sake of rhetoric, Curtius Rufus enjoys, and with him his “fictionalized” history as well, a halftone reputation.
The historian attempts to debunk the very nature of this wonderful East, the providential fortune claimed by Macedonian, and even language.
The unbridled quest for glory pursued by the king and his dream of deification are here condemned: the East stands for a general inversion of norms and values, and fortune becomes an illusion leading to a feeling of impunity.
Implicitly, it also proposes an ideal of power mainly based on balance and on the responsibility of the prince.
The historian questions the relevance of a central myth of the Roman political imagination in the political context of the times, whose shadow looms over all ambitious men, starting with emperors or candidates for the Empire.
His well crafted narrative is a call for a reflection on the actual exercise of power, its challenges and limitations.
The extraction of images semantic is a process that requires deep analysis of the image content.
It refers to their interpretation from a human point of view.
It consists in extracting single or multiple images semantic in order to facilitate its retrieval.
These objectives clearly show that the extraction of semantic is not a new research field.
This thesis deals with the semantic collaborative annotation of images and their retrieval.
Firstly, it discusses how annotators could describe and represent images content based on visual information, and secondly how images retrieval could be greatly improved thank to latest techniques, such as clustering and recommendation.
To achieve these purposes, the use of implicit image content description tools, interactions of annotators that describe the semantics of images and those of users that use generated semantics to retrieve the images, would be essential.
In this thesis, we focus our research on the use of Semantic Web tools, in particular ontologies to produce structured descriptions of images.
Ontology is used to represent image objects and the relationships between these objects.
In other words, it allows to formally represent the different types of objects and their relationships.
Ontology encodes the relational structure of concepts that can be used to describe and reason.
This makes them eminently adapted to many problems such as semantic description of images that requires prior knowledge as well as descriptive and normative capacity.
The contribution of this thesis is focused on three main points: semantic representation, collaborative semantic annotation and semantic retrieval of images.
Semantic representation allows to offer a tool for the capturing semantics of images.
To capture the semantics of images, we propose an application ontology derived from a generic ontology.
Semantic retrieval allows to look for images with semantics provided by collaborative semantic annotation.
It is based on clustering and recommendation.
Clustering is used to group similar images corresponding to the user's query and recommendation aims to propose semantics to users based on their profiles.
It consists of three steps: creation of users community, acquiring of user profiles and classification of user profiles with Galois algebra.
Experiments were conducted to validate the approaches proposed in this work.
Cloud services offer reduced costs, elasticity and a promised unlimited managed storage space that attract many end-users.
File sharing, collaborative platforms, email platforms, back-up servers and file storage are some of the services that set the cloud as an essential tool for everyday use.
Currently, most operating systems offer built-in outsourced cloud storage applications, by design, such as One Drive and iCloud, as natural substitutes succeeding to the local storage.
However, many users, even those willing to use the aforementioned cloud services, remain reluctant towards fully adopting cloud outsourced storage and services.
Concerns related to data confidentiality rise uncertainty for users maintaining sensitive information.
There are many, recurrent, worldwide data breaches that led to the disclosure of users' sensitive information.
To name a few: a breach of Yahoo late 2014 and publicly announced on September 2016, known as the largest data breach of Internet history, led to the disclosure of more than 500 million user accounts; a breach of health insurers, Anthem in February 2015 and Premera BlueCross BlueShield in March 2015, that led to the disclosure of credit card information, bank account information, social security numbers, data income and more information for more than millions of customers and users.
A traditional countermeasure for such devastating attacks consists of encrypting users' data so that even if a security breach occurs, the attackers cannot get any information from the data.
Unfortunately, this solution impedes most of cloud services, and in particular, searching on outsourced data.
Researchers therefore got interested in the following question: how to search on outsourced encrypted data while preserving efficient communication, computation and storage overhead?
This question had several solutions, mostly based on cryptographic primitives, offering numerous security and efficiency guarantees.
While this problem has been explicitly identified for more than a decade, many research dimensions remain unsolved.
The main goal of this thesis is to come up with practical constructions that are (1) suitable for real life deployments verifying necessary efficiency requirements, but also, (2) providing good security insurances.
Throughout our research investigation, we identified symmetric searchable encryption (SSE) and oblivious RAM (ORAM) as the two potential and main cryptographic primitives' candidate for real life settings.
We have recognized several challenges and issues inherent to these constructions and provided a number of contributions that improve upon the state of the art.
First, we contributed to make SSE schemes more expressive by enabling Boolean, semantic, and substring queries.
Practitioners, however, need to be very careful about the provided balance between the security leakage and the degree of desired expressiveness.
Second, we improve ORAM's bandwidth by introducing a novel recursive data structure and a new eviction procedure for the tree-based class of ORAM constructions, but also, we introduce the concept of resizability in ORAM which is a required feature for cloud storage elasticity.
In this dissertation we address the problem of weakly supervised object detection, wherein the goal is to recognize and localize objects in weakly-labeled images where object-level annotations are incomplete during training.
To this end, we propose two methods which learn two different models for the objects of interest.
In our first method, we propose a model enhancing the weakly supervised Deformable Part-based Models (DPMs) by emphasizing the importance of location and size of the initial class-specific root filter.
We first compute a candidate pool that represents the potential locations of the object as this root filter estimate, by exploring the generic objectness measurement (region proposals) to combine the most salient regions and “good” region proposals.
We then propose learning of the latent class label of each candidate window as a binary classification problem, by training category-specific classifiers used to coarsely classify a candidate window into either a target object or a non-target class.
Furthermore, we improve detection by incorporating the contextual information from image classification scores.
Finally, we design a flexible enlarging-and-shrinking post-processing procedure to modify the DPMs outputs, which can effectively match the approximate object aspect ratios and further improve final accuracy.
Second, we investigate how knowledge about object similarities from both visual and semantic domains can be transferred to adapt an image classifier to an object detector in a semi-supervised setting on a large-scale database, where a subset of object categories are annotated with bounding boxes.
We propose to transform deep Convolutional Neural Networks (CNN)-based image-level classifiers into object detectors by modeling the differences between the two on categories with both image-level and bounding box annotations, and transferring this information to convert classifiers to detectors for categories without bounding box annotations.
We have evaluated both our approaches extensively on several challenging detection benchmarks, e.g., PASCAL VOC, ImageNet ILSVRC and Microsoft COCO.
Both our approaches compare favorably to the state-of-the-art and show significant improvement over several other recent weakly supervised detection methods.
Bayesian statistics offer a theoretically well founded framework to reason about uncertainty, and it is one of the cornerstones of modern machine learning.
Although seemingly quite remote from deep neural networks which are theoretically poorly understood, there is a great potential of cross-fertilization between deep learning and Bayesian statistics.
The first is the design of energy and memory efficient neural network models that are suitable for deployment in devices such as mobile phones or drones.
Current solutions only consider using sparsity inducing priors to suppress redundant network parameters in a given architecture, or posterior concentration to sparse deep networks.
Another line of work on neural architecture search is based on reinforcement learning and requires training of thousands of deep neural nets across hundreds of GPUs.
The second problem we want to address is the reliance of deep learning on large quantities of training data, which are typically time consuming and expensive to acquire.
Vice-versa deep learning provides a rich set of techniques that can be used to address problems in probabilistic graphical models, such as Bayesian networks and Markov random fields.
For many models of interest, the posterior is however intractable to compute exactly.
This dissertation examines the universal phenomenon of poly-divergence in archaic Chinese with five subtypes: the polygrammaticalization with the case of huò 或; the polylexicalization and the hybridization of these two with the case of rán 然; the poly-divergence under the influence of cult-culture-philosophical with the case of yī 一; the poly-divergence of the multifunctional structure with the case of "Reference + Comment"; the poly-divergence of the phrase with the case of "numeral +month".
These divergences have the effect of reducing and avoiding the opacity caused by the non-obligatory usage of function word or the lack of exclusive function word.
And these types of poly-divergence in Chinese are realized through mechanisms that are distinct from those of Western languages.
Finally, a conclusion is drawn and it is argued that the poly-divergence is the essential model of evolution in archaic Chinese
Our work demonstrated the performance of the epidemic intelligence systems used for the early detection of infectious diseases in the world, the specific added value of each system, the greater intrinsic sensitivity of moderated systems and the variability of the type information source's used.
The creation of a combined virtual system incorporating the best result of the seven systems showed gains in terms of sensitivity and timeliness that would result from the integration of these individual systems into a supra-system.
They have shown the limits of these tools and in particular: the low positive predictive value of the raw signals detected, the variability of the detection capacities for the same disease, but also the significant influence played by the type of pathology, the language and the region of occurrence on the detection of infectious events.
They established the wide variety of epidemic intelligence strategies used by public health institutions to meet their specific needs and the impact of these strategies on the nature, the geographic origin and the number of events reported.
As well, they illustrated that under conditions close to the routine, epidemic intelligence permitted the detection of infectious events on average one to two weeks before their official notification, hence allowing to alert health authorities and therefore the anticipating the implementation of eventual control measures.
Our work opens new fields of investigation which applications could be important for both users systems.
The planned Pan-European cellular digital mobile radio (DMR) system envisaged for the early 1990s intends to use medium rate encoding of about 16 kbit/s.
At least three phases of experimentation are planned, these being: (1) national pre-selection tests. (2) European selection test, and (3) the final characterisation and verification test with associated channel coding.
The first two phases intend to use listening tests only where effects due to the extra processing delay of the codecs will not be tested.
An initial maximum delay target of 65 ms was set.
The final phase will use conversion tests do evaluate the effects of quality, delay and echo and will be complemented by listening tests which will also be used for codec characterisation.
This paper describes subjective testing methodologies adopted to select suitable candidate codecs capable of being used in the proposed DMR system.
Each administration participating in the tests had to convince the European Committee (Conférence Européenne des Administrations des Postes et Télécommunications, CEPT) that their candidate codec was better than or at least as good as the speech quality of companded FM analogue 900 MHz systems currently in use in Europe.
A measure was obtained of variability in fundamental frequency (F 0) in citation forms of lexical tones.
The language selected for investigation was Thai, a tone language with five lexical tones: mid, low, falling, high and rising.
Twenty speakers participited in the experiment: 10 “young” male speakers and 10 “old” speakers, 5 male and 5 female.
High-quality tape recordings were obtained of each subject's productions of a minimal set of five monosyllabic words.
F 0 contours were extracted by a cepstral analysis.
A comparison was made of inter- and intraspeaker variability in the production of the five Thai tones.
Results of analysis of variance indicated that the degree of intersubject variability in F 0 was greater than intraspeaker across all five tones, that young and old speakers exhibited the same pattern of variability, and that variability in tone production differed depending on the lexical tone.
The falling and rising tones exhibited smaller degrees of variability than the mid, low or high.
Findings are interpreted to highlight the nature of F 0 variability, the relationsip of F 0 variability to amount of F 0 movement, and crosslinguistic differences in F 0 variability as a function of prosodic structure.
Previous research has indicated that workload can have an adverse effect on the use of speech recognition systems.
In this paper, the relationship between workload and speech is discussed, and two studies are reported.
In the first study, time-stress is considered.
In the second study, dual-task performance is considered.
Both studies show workload to significantly reduce recognition accuracy and user performance.
The nature of the impairment is shown to differ between individuals and types of workload.
Furthermore, it appears that workload affects the selection of words to use, the articulation of the words and the relationship between speaking to ASR and performing other tasks.
It is proposed that speaking to ASR is, in itself, demanding and that as workload increases so the ability to perform the task within the limits required by ASR suffers.
The Synonyma of Isidore of Seville, which were written in synonymic style but whose content is moral, could be used both as a grammar book and as a moral book.
However it is this second reading which clearly dominated in the middles ages, as it is proven by the study of the paratext and the manuscript context, of the medieval inventories, of the centos or of the literary posterity.
This publication presents two new approaches of design microstrip antennas array.
First is based on the technique of the genetic algorithms inspired from the processes of the evolution of the species and the natural genetics and the second based on the analogy between the resolution of the combinative problems of optimization and the annealing of the solids.
These two approaches permits to seek simultaneous the law of optimal feed and the space distribution of the radiant elements so that the radiation pattern is as close as possible to an optimal desired diagram specified from a function or a pattern shape.
The registration of 3-D objects is an important problem in computer vision and especially in medical imaging.
It arises when data acquired by different sensors and/or at different times have to be fused.
Under the basic assumption that the objects to be registered are rigid, the problem is to recover the six parameters of a rigid transformation.
This paper presents a novel iterative method designed for the rigid registration of 3-D objects.
Its originality lies in its physical basis: instead of minimizing an energy function with respect to the parameters of the rigid transformation (the classical approach) the minimization is achieved by studying the motion of a rigid object in a potential field.
In particular, we consider the kinetic energy of the solid during the registration process, which allows it to “jump over” some local maxima of the potential energy and so avoid some local minima of that energy.
In that particular application, we perform the matching process with the whole segmented volumes.
Different modes of prescription are detectable in the lists of terms recommended within the framework of the official terminology process in France, as it has been operating for more than two decades, following the Juppé decree in 1996.
Several dynamics are observed in these recommendations, which contain valuable remarks and numerous stylistic usage labels.
It attempts to consider these elements of official prescription as within the framework of a combined meta-lexicographic and terminological analysis.
This paper presents first the semantic web, which aims at developing a knowledge level, independently from its linguistic realization.
We show that this project is illusory and that a link must be established between formalized knowledge and texts available on the web.
The paper presents different approaches to establish this link and deal with the relationship between general knowledge sources and knowledge dynamically acquired from texts.
The French and Vietnamese languages came into contact while Vietnam was under French occupation leading to the phonological assimilation of a large number of French loanwords into the Vietnamese language.
The borrowings thus had to adjust to the Vietnamese tonal system.
Scientific literature on this issue (as well as on French-Vietnamese linguistic contacts) has been sparse, and is based on a restricted number of borrowed forms.
Using a phonological and statistical analysis of 600 Vietnamese words of French origin, the current study tackles the attribution of tones to previously toneless syllables.
It counts the occurrences of each tone and reveals some laws determining the tonal integration of the French borrowings.
The study of these songs is based on the classification of these sound units, especially to extract the song theme of the singers in a specific area during a specific season.
Recently, some approaches are proposed for automatic classification of these sound units.
This paper introduces the sparse coding as a robust unsupervised classifier to generate efficient time-frequency representation of the calls of the whale.
Secondly, the subunit shows to be interesting to analyze the evolution ofthe humpback whale songs during two years.
It is statistically shown that the shortest units are the most stable (occurring with similar time frequency shape across the two years), while the longest units are evolving from one year to one other.
The pitch filter in a low bit-rate CELP speech coder has a strong impact on the quality of the reconstructed speech.
In this paper we propose a pseudo-multi-tap pitch filter with fewer degrees of freedom than the number of prediction coefficients, but which gives a higher pitch prediction gain and a more appropriate frequency response than a conventional one-tap pitch filter.
The prediction gain of the pseudo-multi-tap pitch filter is compared to that of conventional one-tap and three-tap pitch filters with integer and non-integer pitch lags.
This relaxed test gives better results than a strict stability test.
Finally, we have incorporated the pseudo-multi-tap pitch filter into a 4.8 kbit/s CELP speech coder.
Both the objective SNR and subjective quality are better than for a conventional one-tap pitch filter.
The Stochastically Excited Linear Prediction (SELP) algorithm for speech coding offers good performance at bit rates as low as 4.8 kbit/s.
Linear Predictive Coding (LPC) techniques remove the short-term correlation from the speech.
A pitch loop removes long-term correlation, producing a noise-like residual, which is vector quantized.
Information describing the LPC filter coefficients, the long-term predictor, and the vector quantization is transmitted.
In this paper, we describe improvements to the SELP algorithm which result in better speech quality and higher computational efficiency.
In its closed-loop form, the pitch loop can be interpreted as a vector quantization of the desired excitation signal with an adaptive codebook populated by previous excitation sequences.
To better model the non-stationarity of speech we extend this adaptive codebook with a special set of candidate vectors which are transform of other codebook entries.
The second stage vector quantization is performed using a fixed stochastic codebook.
In its original form, the SELP algorithm requires excessive computational effort.
We employ a new recursive algorithm which performs a very fast search through the adaptive codebook.
In this method, we modify the error criterion, and exploit the resulting symmetries.
The same fast vector quantization procedure is applied to the stochastic codebook.
The paper presents a survey of recent finding from the authors, and their collaborators, related to the phase transition problem and relational learning.
The most compelling resuit is that commonly used heuristics such as the information gain are inevitably entrapped in the phase transition region.
Then, it is unlikely to expect relational learning scaling up to problems larger than the ones solved so far.
The distance measure is of great importance in both the design and coding stage of a vector quantizer.
Due to its complexity, however, the spectral distance which best correlates with the perceptual quality is seldom used.
Since they are in general mathematically more tractable, these weighted squared Euclidean distance measures are more commonly used.
Significant differences can be found in the performance of different distance measures suggested in previous literatures.
In this paper, a complete study and comparison of weighted squared Euclidean distance measures is given.
This paper also proposes a new weighted squared Euclidean distance measure for vector quantization of Line Spectrum Pairs (LSP) or Cosine of LSP (CLSP) parameters.
Althougn systems for the transcription of speech prosody have existed for a long time, the need to represent prosodic information in large-scale speech databases places new demands on such systems.
The ToBI system recently developed in the USA differs in many interesting ways from conventional systems such as the “Standard British” system that has been in use for several decades.
This paper discusses the differences in the context of current research on a machine-readable corpus of spoken English, and examines the possibility of converting automatically between the two types of transcription.
Two excised human larynges were used to investigate the effects of changes in supraglottal acoustics on phonation, especially fundamental frequency (F 0).
An artificial supraglottal tube was connected to the larynx.
Two different sized cylindrical blocks were inserted and moved in the tube to simulate the acoustics of neutral, front and back vowels.
The F 0 changes were greatest at the instant when the blocks were inserted in the tube, i.e. when the tube was converted from open to blocked.
F 0 changes connected with the use of the larger block did not show any systematical variation.
In many cases an F 0 drop was measured.
The results are interpreted in terms of an acoustic-mechanical feedback in vocal source-tract interaction.
It is concluded that the so-called intrinsic F 0 of vowels cannot be explained on an acoustical basis.
The acoustical conditions for the small block were simulated using a theoretical model.
A robust new algorithm for accurate endpointing of speech signals is described in this paper after an overview of the literature.
This algorithm uses simple measures based on energy and zero-crossing rate for speech/silence detection.
Instead of the usual two-state model, three states including a transitory phase are assumed.
The zero-crossing rate measure is used in a special manner in the transitory state to improve the endpointing accuracy.
The algorithm is based on classification in context and uses some knowledge-based heuristics for correction of false detections.
The approach here is the one used in the visual detection of a waveform embedded in noise.
No a priori knowledge of noise is needed and the algorithm is capable of producing good results even in cases where the signal starts with mouth noise.
One important feature of this algorithm is its ease of implementation for real-time processing.
The algorithm is also adaptive and can cope with varying signal to background noise ratios.
The algorithm was originally developed on a database collected in an acoustic chamber.
Modifications for its application to telephony speech as well as results of a preliminary test are included.
An investigation of the pharyngealization by analysis/synthesis is realized as a contribution to the development of a synthesis-by-rule system for Arabic.
The present paper describes the acoustic parametrization of the Arabic pharyngealized consonants / /, / /, / /, / / using a formant synthesizer.
Acoustical studies showed that the usual description of the Arabic vowel system as a set of 3 short vowels and their 3 long counterparts is inadequate.
A system of 12 vowels including 6 pharyngealized ones is defined according to phonetic and phonological facts.
The rules dealing with the extension of pharyngealization in connected speech are established and validated by synthesis.
We begin by considering some recent studies of institutional history, which seem to give us a better understanding of the hidden face of the elaboration of the norm in France, namely the exclusion of the Parisian Parlement as a model of good usage.
In the second half of the article, we address a thorny issue: that of the influence of normative statements on usage, focusing on three case studies from the fields of noun and verb morphology and the lexicon.
The main advances of the R&D Sample Orchestrator project are presented, aiming at the development of innovative functions for the manipulation of sound samples.
These features rely on studies on sound description, i.e. the formalization of relevant data structures for characterizing the sounds ' content and organization.
This work was applied to automatic sound indexing and to the development of new applications for musical creation - interactive corpus-based synthesis and computer-aided orchestration.
The project also included an important part on high-quality sound processing, through several enhancements of the phase vocoder model – processing by sinusoidal model in the spectral domain and automatic computation of the analysis parameters.
The Higher Pole Correction (HPC) function in analog and digital all-pole modelling of speech production is analyzed by comparing all-pole models with a Transmission Line (TL) model.
The validity of the TL model, which was chosen as a computational reference system in the study, is tested by comparing its transfer functions to acoustical measurements made on a physical vocal tract model.
The variation of effective length of the vocal tract turned out to be an important parameter in modelling the HPC.
Even if the frequency responses of the HPC in analog and digital cases differ, the relative changes in the correction, influenced by the variations in the effective length of the vocal tract, are exactly the same in both cases.
The work results in all-zero models, which can be used in analog as well as digital all-pole realizations to form a new type of pole-zero model for speech production.
This new pole-zero model is related to the PARCAS terminal analog model [Laine, 1982].
In this paper, it is tried to apply neural networks for two kinds of problems concerning articulatory motion.
They are estimation of articulatory motion from speech waves and generation of articulator movement for a sequence of phonemic symbols.
In the former problem, since estimation of articulatory parameters is regarded as a nonlinear mapping between the acoustic parameters and the articulatory ones, a neural network is expected to be a suitable method.
In the latter problem, a nonlinear control system that produces articulatory motion is successfully constructed combining neural networks.
The frame-by-frame variation of tongue profiles derived from X-ray film data is described in terms of the temporal patterns of four articulatory parameters.
The temporal variation of each parameter, i.e., movement, is assumed to be the output of a time-invariant auto-regressive filter.
Each filter is excited by a sequence of pulses, representing articulatory commands.
The filter coefficients, and the position and amplitude of the pulses are determined by applying an MLPC method.
The minimum number of pulses is determined by using an acoustic criterion.
It depends on the number of the phonetic features, in the sentence, of which their realization is crucially related to their pertinent parameters.
The influence of pitch contour, segmental durations, and spectral features on the perception of two speaking styles was studied.
For this purpose two male speakers each spoke “spontaneously” to an interviewer and afterwards read out their own literally transcribed spontaneous text.
Pairs of identical spontaneous and read utterances were selected that were fluently spoken in both speaking styles (no false starts, hesitations, etc.).
Five test conditions were constructed in which the utterances had: (1) no manipulations; (2) phoneme durations from the opposite speaking style; (3) the pitch contour from the opposite speaking style; (4) a monotonous pitch contour; (5) the original spectral features combined with the prosodic features of the opposite speaking style.
The stimuli were presented to 32 subjects in a listening experiment. Their task was to classify each utterance as either “spontaneous” or “read out” speech.
All manipulations of the test utterances had a significant effect on the classification of the speaking style.
We also analysed the original utterances with respect to several acoustic measures for intonation, duration, jitter and shimmer, and spectral vowel quality.
Overall, read speech compared to spontaneous speech had: a lower articulation rate, more F 0 variation, more F 0 declination, less shimmer, and less vowel reduction.
However, none of these acoustic features by itself can clearly discriminate between the two speaking styles. Above all it became clear that the performance of the speakers and the listeners varied enormously.
This paper proposes an augmented chart data structure with an efficient word lattice parsing scheme in speech recognition.
The augmented chart and the associated parsing algorithm can represent and very efficiently parse, without changing the fundamental principles of chart parsing, a lattice of lexically highly ambiguous word hypotheses in speech recognition.
Every word lattice can be mapped to the augmented chart, with the ordering and link between the word hypotheses being well preserved in the augmented chart.
A jump edge is defined in order to link edges representing word hypotheses physically separate, but connectable from a practical point of view.
Preliminary experimental results show that with augmented chart parsing, all the possible constituents of the input word lattice can be constructed and no constituent needs to be built more than once.
This significantly reduces computational complexity, especially when serious lexical ambiguity exists in the input word lattice as in the case of many speech recognition problems.
This augmented chart parsing is thus very a useful and efficient approach to language processing problems in speech recognition.
Problems that arise when we are comparing the vibro-acoustic signatures of high-voltage apparatus in electrical substations have led to the discovery of a new time-warp algorithm.
Such signature comparisons are made in order to monitor trends in the health of the equipment.
However, the signatures collected contain a sequence of transients generated by electromechanical events which appear with a slightly different time frame from one switching operation to the other, depending on the temperature and load, among other factors.
Despite being small, this time distortion results in a significant difference between the instantaneous amplitudes of the signatures.
The magnitude of the time difference between the last signature and a reference has a diagnostic use.
The proposed time warp algorithm makes it possible to identify the time relationship between the events of two signatures, even in the presence of discontinuities in the time frame.
We show that these discontinuities are not treated appropriately by the dynamic time warping (DTW) algorithm currently employed in speech processing.
The paper explains the way these two algorithms work and presents results that bear witness to the increased accuracy of the multi-scale correlation.
In part, it is the interpolation of the warp trace that makes such accuracy possible.
The warp trace is a function describing the time deviation between the signatures and even including the presence of inversions in the order of appearance of the transients.
A great number of application fields is concerned with multidimensional signal processing and the new possibilities of hardware allow the development of device operating to those signals.
After a presentation of the studied domain, we give some elements of terminology and present the essential models of multidimensional signals.
We present then a synthetic approach of the estimation (or measurement) methods used in the multidimensional signal characterization.
We conclude in a presentation of the principal operator (filters) used in ND signal processing in connexion with 1 D signal operators.
Nowadays, neural networks are largely used in signal and image processing.
We propose a new neuron model that uses a special coding for its output, which we will call «Scalar Distributed Representation» (SDR).
This representation is based on the idea of representing the neuron's output by a function, and not only by a scalar.
We show that SDR produces a non-linear behaviour of connections between neurons.
The SDR is described in general and then adapted on practical considerations.
We consider the use of SDR for a Multi-Layer Perceptror and we propose a learning algorithm.
Finally, we validate the model on two applications: dimensionality reduction, and prediction.
In both cases, an important benefit is obtained over the classical model.
The optimisation of a parallel distributed detection system with N sensors always leads to a set of 2 N + N non linear equations, which is only solved in particular cases, assuming statistical independence of the local observations and for systems constituted by very few sensors.
Usually, the number of equations to solve increases very quickly with the number of sensors.
The computations become unfeasible.
In this paper, a selection procedure of sensors relevant for the decision process based on the use of Shannon's conditional entropy is developed.
Then, these systems are optimized via en entropy based method.
This one determines the local thresholds and constructs a decision tree which minimises the decision error probability.
Due to the fact that the performances of distributed systems are lower than the centralised ones, the previous optimisation techniques can be applied in the distributed quantification problem taking into account a compromise between the information flow to broadcast to the fusion center, and the performances of the decision system.
A system has been developed that enables a wanted speech signal to be extracted from a background of unwanted speech and other interference under real-life conditions.
Using only two microphones 25 cm apart it exploits directional and harmonicity cues in a hybrid algorithm which takes advantages of both.
The output of the algorithm has been tested subjectively by human listeners, and objectively by a speech recognition system.
These tests show improved intelligibility of the wanted speech signal.
In one set of tests, for example, the performance of a speech recogniser after segregation was similar at a signal to noise ratio of 6 dB as without segregation at a signal to noise ratio of 20 dB.
Estimating a non-uniformly sampled function from a set of learning points is a classical regression problem.
Kernel methods have been widely used in this context, but every problem leads to two major tasks: optimizing the kernel and setting the fitness-regularization compromise.
This article presents a new method to estimate a function from noisy learning points in the context of RKHS (Reproducing Kernel Hilbert Space).
We introduce the Kernel Basis Pursuit algorithm, which enables us to build a L-i-regularized-multiple-kernel estimator.
The general idea is to decompose the function to learn on a sparse-optimal set of spanning functions.
Our implementation relies on the Least Absolute Shrinkage and Selection Operator (LASSO) formulation and on the Least Angle Regression Stepwise (LARS) solver.
The computation ofthe full regularization path, through the LARS, will enable us to propose new adaptive criteria to find an optimal fitness-regularization compromise.
This communication proposes a new Prony model with time-varying poles for modélisation of nonstationary signals.
This new model is based upon a linear combination of time-varying exponentials.
This method leads to an extension of several techniques of stationary spectral estimation to the nonstationary case: amplitude and phase are time-varying.
We show and justify a method and the corresponding algorithm to estimate the complete new parameter vector.
The determination of the time-varying parameters requires five steps: estimation of the time-varying AR parameters, estimation of the right poles of the linear time- varying system, modélisation of these poles, computation of the new poles, and least-square estimation of the amplitudes.
To validate this model, a simulation signal composed of two chirps is chosen.
Many research works combine learning from demonstration and policy improvement methods to learn the controller of a robot along a specific trajectory.
Nevertheless, a capability to learn in the whole reachable space of this robot is missing in these works.
In this paper we propose a method that consists in learning a reactive near-optimal feedback controller in two steps.
Second, the feedback controller is optimized further with direct Policy Search methods.
As a result, we obtain a controller that is executed 20 000 times faster than the original controller for a similar performance.
Our work is evaluated in simulation.
This paper presents the Dial-Your-Disc (dyd) system, an interactive system that supports browsing through a large database of musical information and generates a spoken monologue once a musical composition has been selected.
The paper focuses on the generation of spoken monologues and, more specifically, on the various ways in which the generation of an utterance at a given point in the monologue requires modeling of the linguistic context of the utterance.
The existing texture classification methods are generally based on a parameter extraction stage followed by a classifier stage.
Using this kind of method for an operational application requires to take into account the risk of classes mixture in the parameters space.
We show that a connectionnist classifier is able to deal efficiently with these parameters.
A system for the automatic recognition of sonorant consonants extracted from continuous speech is described.
The system is based on fuzzy rules.
After performing a precategorial classification in which feature extraction is carried out by modules organized in a hierarchy of levels, the speech message is segmented in pseudo-syllabic nuclei and hypotheses about vowels and consonants are emitted.
The rules which improve the classification of Italian liquids and nasals in most contexts were inferred after experiment, and account for coarticulation effects.
Results obtained for four male and two female speakers are presented together with an acoustic-phonetic motivation of the approach used.
These results show that this method gives substantially better performances than previous approaches.
Models of word recognition differ with respect to where the effects of sentential-semantic context are to be located.
Using a crossmodal priming technique, this research investigated the availability of lexical entries as a function of stimulus information and contextual constraint.
To investigate the exact locus of the effects of sentential contexts, probes that were associatively related to contextually appropriate and inappropriate words were presented at various positions before and concurrent with the spoken word.
The results show that sentential contexts do not preselect a set of contextually appropriate words before any sensory information about the spoken word is available.
Moreover, during lexical access, defined here as the initial contact with lexical entries and their semantic and syntactic properties, both contextually appropriate and inappropriate words are activated.
Contextually effects are located after lexical access, at a point in time during word processing where the sensory input by itself is still insufficiently informative to disambiguate between the activated entries.
This suggests that sentential-semantic contexts have their effects during the process of selecting one of the activated candidates for recognition.
Malicious programs, such as viruses and worms, are frequently related to previous programs through evolutionary relationships.
Discovering those relationships and constructing a phylogeny model is expected to be helpful for analyzing new malware and for establishing a principled naming scheme.
Matching permutations of code may help build better models in cases where malware evolution does not keep things in the same order.
We describe methods for constructing phylogeny models that uses features called n-perms to match possibly permuted codes.
An experiment was performed to compare the relative effectiveness of vector similarity measures using n-perms and n-grams when comparing permuted variants of programs.
The similarity measures using n-perms maintained a greater separation between the similarity scores of permuted families of specimens versus unrelated specimens.
We present a computational model that generates listening behaviour for a virtual agent.
It triggers backchannel signals according to the user's visual and acoustic behaviour.
The choice of the type and the frequency of the backchannels to be displayed is performed considering the agent's personality traits.
We link agents with a higher level of extroversion to a higher tendency to perform more backchannels than introverted ones, while linking neuroticism to less mimicry production and more response and reactive signals sent.
We run a perceptive study to test these relations in agent-user interactions, as evaluated by third parties.
A large-scale Japanese speech database has been described.
The database basically consists of (1) a word speech database, (2) a continuous speech database, (3) a database for a large number of speakers, and (4) a database for speech synthesis.
Multiple transcriptions have been made in five different layers from simple phonemic descriptions to fine acoustic-phonetic transcriptions.
The database has been used to develop algorithms in speech recognition and synthesis studies and to find acoustic, phonetic and linguistic evidence that will serve as basic data for speech technologies.
The aim of this paper is to call attention to the role played by prosodic structure in continuous word recognition.
First we argue that the written language notion of the word has had too much impact on models of spoken word recognition.
Next we discuss various characteristics of prosodic structure that bear on processing issues.
Then we present a view of continuous word recognition which takes into account the alternating pattern of weak and strong syllables in the speech stream.
A lexical search is conducted with the stressed syllables while the weak syllables are identified through a pattern-recognition-like analysis and the use of phonotactic and morphonemic rules.
We end by discussing the content word vs. function word access controversy in the light of our view.
Back-propagation has been used to train a small network for the prediction of syllable-level duration in a text-to-speech system.
Both input and output are in the form of analog values, and the net performs a multiple regression function.
Recent studies have shown that the sub-band based speech recognition approach has the potential of improving upon the conventional, full-band based model against frequency-selective noise.
A critical issue towards exploiting this potential is the choice of the method for combining the sub-band observations.
This paper introduces a new method, namely, the probabilistic-union model, for this combination.
The new model is based on the probability theory for the union of random events, and represents a new method for modeling partially corrupted observations given little knowledge about the corruption.
The new model has been incorporated into a hidden Markov model (HMM) and tested for recognizing a speaker-independent E-set, corrupted by various types of additive noise.
The results show that the new model offers robustness to partial frequency corruption, requiring little or no knowledge about the noise statistics.
We present a new algorithm for planar (XA) and tomographic (MRA) images registration.
The 2D/3D registration is defined as determining the optimal rigid transformation which enables to register the entire dataset coming from both modalities in a common three-dimensional referential.
Interests of the described method can be observed through two points.
Firstly, the use of an anatomical based registration offers the possibility of less constraining exam, more suitable for patients.
Then, after an anatomical referential has been interactively selected, registration procedure may be steered independently since the initialization step is automatic.
The tomographic dataset is used to construct a three-dimensional structure, which defined the common referential for the two modalities.
Then, the optimal structure position is obtained in the planar imaging reference through a multi-scale analysis and optimization procedures.
Finally, since the position of the anatomical structure is known in both MRA and planar imaging referential, it is possible to obtain a three-dimensional matching under condition of having at least two incidences for the planar imaging.
Low structure, scattered information, strongly symbolic representation are some of the characteristics that define this textual document type.
Then, supervised learning techniques are used in order to produce targeting models from this reduced space.
Interesting results (86% of recall and 88% of precision) are obtained by induction trees and discriminant analysis.
Even if the validation results (55% of recall and 60% of precision) could be better, this approach offers interesting perspectives, based on content, for the automatic exploitation of CV.
This paper proposes two methods for creating a phoneme- and speaker-independent model that greatly reduce the amount of calculation needed for similarity (or likelihood) normalization in speaker verification.
For each input speech, these methods only need to calculate the likelihood against a single model instead of against the models of all the reference speakers as in the conventional methods.
In addition, the new methods perform better than or equally as well as the conventional methods.
Speaker verification is tested by using separate populations of customers and impostors in order to evaluate performance under practical conditions.
The speaker (and text) verification error rates are roughly 1.5 times higher than when the same population is used for both customers and impostors.
Using 15 customers and a separate group of 15 impostors, a speaker verification error rate of 1.8% for text-independent verification and a speaker-and-text verification error rate of 1.1% for text-prompted verification were obtained after normalization.
The latter error rate was about half of that achieved by the original method.
In this paper, we review the current state of the art in stochastic modeling for spoken dialogue system design.
We discuss acoustic modeling of speech units for automatic speech recognition and language modeling of linguistic units for natural language processing.
We point out some of the emerging stochastic modeling techniques and show the similarity between language modeling and acoustic modeling.
Finally, we address search and decision issues related to the integration of knowledge sources for automatic speech recognition and natural language processing.
This article is devoted to the problem of the explanation of the result given by a decision tree (DT) when it is used as a decision aid system, to classify input data and to provide the output class as a result.
At now the end-user can rely mainly on some estimation of the error-rate or on the trace of the classification, that is the path run through the DT.
We propose here two new methods to qualify the result given by the DT for each particular case.
These methods are based on a geometric study of the inverse image of the different classes (the decision surface).
First we identify the tests in the tree that are the most relevant to explain the final class, by the way of sensitivity analysis: the vector of initial data is projected onto the decision surface.
In these days of multimodal systems and interfaces, many research teams are investigating the purposes for which novel combinations of modalities can be used.
Based on the study of particular applications, empirical investigations of speech functionality address points in a vast multi-dimensional design space.
At best, solid findings yield low-level generalisations which can be used by designers developing almost identical applications.
Furthermore, the conceptual and theoretical apparatus needed to describe these findings in a principled way is largely missing.
This paper argues that a shift in perspective can help address issues of modality choice both scientifically and in design practice.
Instead of empirically focusing on fragments of the virtually infinite combinatorics of tasks, environments, performance parameters, user groups, cognitive properties, etc., the problem of modality functionality is addressed as a problem of choosing between modalities which have very different properties with respect to the representation and exchange of information between user and system.
Based on a study of 120 claims on speech functionality from the literature, it is shown that a small set of modality properties are surprisingly powerful in justifying, supporting and correcting the claims set.
The paper analyses why modality properties can be used for these purposes and argues that their power could be made available to systems and interface designers who have to make modality choices during early design of speech-related systems and interfaces.
Using hypertext, it is illustrated how this power may be harnessed for the purpose of predictively supporting speech modality choice during early systems and interface design.
Previous studies have shown that the discriminability of a pair of stop consonants is determined by the particular consonant pair and by its position in a CVC syllable.
The hypothesis that these differences in discriminability are related to differences in the auditory representations of the stimuli, following frequency analysis by the peripheral auditory system, was tested in two experiments.
Second, the auditory representation of each consonant in the syllable-initial and syllable-final position was measured by means of a simultaneous-masking technique.
Two cues to a consonant's identity are (a) the instantaneous spectrum at the onset (syllable initial)_or offset (syllable final) of the formant transition, and (b) the formant transition pattern.
The discriminability of six pairs of consonants (three initial and three final) was correlated with both the difference between their masking patterns at onset or offset (ϱ −0.89) and with the difference between their auditory transition patterns (ϱ= 0.94).
The strength of these correlations indicates that the variations in consonant discriminability which have been reported may be predictable from a knowledge of their auditory representations at a peripheral level, with relatively little contribution from phonetic, linguistic, or cognitive factors.
The proposed framework combines machine learning techniques and Riemannian geometry-based shape analysis.
We represent facial surfaces by collections of radial curves and iso-level curves, the shapes of corresponding curves are compared using a Riemmannian framework.
We select the most discriminative curves using the well known AdaBoost algorithm.
The experiment involving FRGC v2 dataset demonstrates the effectiveness of this feature selection by achieving 98.02 % as rank-1 recognition rate.
A classification of different methods used for the assessment of TTS (Text-To-Speech) systems, according to the demands placed on the listener, is proposed and discussed.
The classification is made according to the four traditional scale levels: the Nominal, Ordinal, Interval and Ratio level.
A fifth level, the Supra-Nominal, including memory processes, is proposed.
The methods are divided into qualitative, non-metric methods and quantitative, metric methods.
The outcome is that the highest metric assessment level (Ratio) is not necessarily the level that places the highest demands on the listener.
Quite to the contrary, the Nominal level, supporting a non-metric qualitative approach, places even higher demands on the listener.
This article is concerned with the fusion of stacked images, in order to restore the 2D image of an object.
To perform this fusion, we have to estimate the transfer function between the sensitivity functions of the films on which these images have been taken.
In our context, this estimation is difficult because these images are degraded by widespread stains.
We show that it is possible to estimate both the stains and the transfer function to implement the restoration.
There are many situations where non-real-time speech enhancement is required.
For such applications, employing any available a priori knowledge can lead to more effective enhancement solutions.
In this study, a novel text-directed speech enhancement algorithm is developed for usage in non-real-time applications.
In our approach, the text of the intended dialogue is used to partition noisy speech into regions of broad phoneme classifications.
Classes considered include stops, fricatives, affricates, nasals, vowels, semivowels, diphthongs and silence.
These partitions are then used to direct a new vector quantizer based enhancement scheme in which phone-class directed constraints are applied to improve speech quality.
The proposed algorithm is evaluated using both objective as well as subjective quality assessment techniques.
It is shown that the text-directed approach improves the quality of the degraded speech over a broad range of noise sources (i.e., flat communications channel noise, aircraft cockpit noise, helicopter fly-by noise, and automobile highway noise) and over a broad range of signal-to-noise ratios (i.e., 10, 5, 0 and −5 dB).
In each case, the proposed method is shown consistently to exhibit improved objective quality over linear and generalized spectral subtraction, as well as the Auto-LSP constrained iterative enhancement method using the Itakura-Saito measure and a 100-sentence evaluation speech corpus.
Subjective quality assessment was conducted in the form of an A-B comparison test.
Results of these evaluations demonstrate that, for wideband noise distortions, the proposed algorithm is preferred over the unprocessed noisy speech more than 2 to 1, while the proposed algorithm is preferred over spectral subtraction by more than 3 to 1.
In probabilistic speech recognition it is often interesting to evaluate the contribution of the language model and that of the acoustic model.
We propose an information theoretical approach which takes into account the interaction between the two sources of information.
Experimental results are presented concerning the IBM prototype real-time recognizer of the Italian language based on a 20,000-word vocabulary.
In the framework of a phonosyntax model [1–4], the prosodic structure of the sentence is encoded by melodic contours located on stressed vowels.
These contours are described by the phonological features [±Extreme], [±Rising], and [±Ample].
Acoustic analyses of French read sentences show that, except for the final contour, intensity and duration do not play a role in differentiating the contours, which contrast effectively by their rising or falling slope and by the relative amplitude of F 0 variation.
This paper proposes a local, cooperative and real-time multi-agent approach to build adaptive and incremental profiles where a user is supposed to be represented by a set of textual documents.
These documents are sequentially parsed, which leads to the construction of a temporary terminological network (TTN).
Preliminary results of the built system as well as perspectives are then presented.
Dutch consonants, spoken in lists of two-syllable nonsense words of the type CVCVC which were embedded in short carrier phrases, were identified by listeners under various acoustic disturbance conditions.
The 28 conditions were a mixture of four reverberation times, five signal-to-noise rations, and five different noise spectra.
The identification results were summated over the six talkers and five listeners.
In this way we achieved 28 confusion matrices per constant position (initial, medial, and final).
These sets of matrices were processed by individual differences multidimensional scaling programs, and more specifically by TUCKALS (Kroonenberg and de Leeuw, [9]).
The resulting three-dimensional stimulus configuration for the initial consonants is very stable and can be represented as a tetrahedron with /z, s/, /m, n/, /p, t, k, b, d/, and /f, v, χ/ at the four corner points and /l, r, w, j, h/ in the centre.
This consonant configuration is discussed with respect to its relevance to the Dutch language given the experimental conditions.
The representation of the 28 conditions turns out to be almost exclusively one-dimensional despite the three different aspects (reverberation time, noise level, noise spectrum) of the acoustic disturbances.
Two experiments are reported on the perception of the distinction between /ba/ and /pa/ which investigate perceptual cues to the onset of voicing.
Neither the onset of periodic excitation nor a change in spectral balance appear to be the dominant cue for voicing onset.
In both experiments the overall intensity level provided the best metric for explaining the position of subjects' boundaries.
The second half of the seventeenth century is often depicted, due to the intense prescriptive activity, as a time when the arguments made were accepted and followed by effect.
In this paper, we focus on the possible limits of this activity.
After some terminological clarifications on what may be called norms and prescriptions, we study the foundations of “anti-prescriptivism” in the seventeenth century, especially in “remarks on language”, and we show how this current of resistance to prescriptions was taken up by the worldly culture of the end of the century.
In total, we consider that, far from being considered only as a period of “adjustment”, the second half of the seventeenth century, on the contrary, can be seen as a conflictual and polemical moment, characterized by an alteration of the traditional normativity of grammar, and by the establishment of social opposition that made it difficult to establish a single standard supported by a clear prescription.
Situations, the semantic interpretations of context, provide a better basis for selecting adaptive behaviours than context itself.
The definition of situations typically rests on the abilityto define logical expressions and inference methods to identify particular situations.
In this paper we extend this approach to provide for efficient organisation and selection in systems with large numbers of situations having structured relationships to each other.
We apply lattice theory to define a specialisation relationship across situations, and show how this can be used to improve the identification of situations using lattice operators and uncertain reasoning.
We demonstrate the technique against a real-world dataset.
This essay is an attempt to define the features and the value of authors' manuscripts in Middle French through the archeological and philological study of the original manuscripts of four texts by Chistine de Pizan (Cent Ballades, Debat de deux amans, Mutacion de Fortune, Advision).
Three stages in the publishing process are considered (determining the model; copying from a fair copy; revising the transcription) in order to underline the fact that an autograph transcription, certainly privileged, is less important than the initial authorization of the transcription from the exemplar, written and rewritten by the author's hand, and the final act of the manuscript's «visite», which is visible in the text corrections, the authorial modifications and the customization of certain components of the codex.
Finally, the article examines what's at stake from a methodological and editorial point of view in such a study of autograph publications (author's intention, choice of the base manuscript and relevance of an esthetical and genetic research).
The goal of this paper is to present an embedded calculator for image processing SYMPHONIE and the methodology used for its realization.
For this application, low size and low consumption will be very important and an ASIC of a million gates has been developped.
To succeed in this realization we used a VHDL model allowing simulations on the full system.
Then we used VHDL synthesis methods for the conception of the ASIC.
We will conclude this paper by a presentation of some perfomances of the system in some applications fields.
Most theories of language acquisition implicitly assume that the language learner is able to arrive at a segmentation of speech into clausal units.
The present studies employed a preference procedure to examine the sensitivity of 7-10-month-old infants to acoustic correlates of clausal units in English.
The infants oriented longer to the samples segmented at the clause boundary.
A second experiment confirmed that these preferences depended on where the pauses were inserted in the samples.
These findings have important implications for understanding how language is learnable.
The prelinguistic infant apparently possesses the means to detect important units such as clauses, within which grammatical rules apply.
This is an attempt to formulate an outline of the properties of the human voice source in connected speech.
Six aspects of the production process are considered: (1) reference data for a particular speaker; (2) segment specific values and source-tract interactions; (3) coarticulation of glottal gestures and interpolation at boundaries; (4) basic F 0 dependencies; (5) the influence of stress, accents and voice intensity; (6) the phrasal contour of source variations.
The parameterization of source data is based on the transformed LF-model and frequency domain correspondences (Fant, 1995) which allows for a maximal specificational power with a limited number of parameters.
A self-learning speaker and channel adaptation technique based on the separation of speech spectral variation sources is developed for improving speaker-independent continuous speech recognition.
Statistical methods are formulated to remove spectral biases at the acoustic level and to adapt parameters of Gaussian mixture densities at the phone unit level.
The spectral bias is estimated in two steps using unsupervised maximum likelihood estimation: the probability distributions of the speech spectral features are first assumed uniform for severely mismatched channels, and the spectral bias is then reestimated using Gaussian phone models.
The task vocabulary size was 853; the grammar perplexity was 105; the test speech data were collected under mismatched recording conditions with each test set containing 198 sentences.
Depending on the speakers and channel conditions, the two-step spectral bias removal yielded relative error reductions (RER) of 3% to 11% compared to the conventional cepstral mean removal; the USPA yielded RER of 12% to 26% after the two-step bias removal; the IPA further yielded RER of 8% to 19% after the USPA.
The structure of a synthesis system is described that uses prominence as a central parameter.
A definition of prominence suitable for this application is given.
For the empirical foundation the reliability of prominence ratings by human listeners is assessed.
These ratings were compared with acoustic data on F 0 and duration.
A linear relationship between ratings and parameter values was found.
Two algorithms to transform prominence values to prosodic parameters are briefly described and evaluated.
The application of prominence to the synthesis of focal accents is demonstrated.
The results indicate the validity of the prominence based approach as an interface between linguistics and acoustics.
Multi-agent based modelling is about considering that the global behavior of a system comes from interactions which take place between micro level entities.
Consequently, how the micro and macro levels dynamics are linked to each other is a crucial issue.
In this paper, we focus on how the Ferber and Müller's Influence/ Reaction model can help in considering such an issue.
We show that it represents an interesting solution and we then propose an adaptation of this model which is more suited to simulation: the IRM4S model (an Influence Reaction Model for Simulation).
This model clarifies the Influence/Reaction principle and highlights its capacity to actually join the micro level modeling concerns with those of the macro level.
Experts of roads and public works have been interested for a long time in the bumps on the road ways.
Such road defects with regard to a flat surface, are called the roughness of the road.
The longitudinal profile analysor (LPA) was made by the «Laboratoire Central des Ponts et Chaussées» in Nantes, in order to measure the roughness.
The signal given by this plant can be considered as the output of a linear system whose input is the unknown longitudinal section of the road.
We present in this paper two methods for solving this problem: the first one is determinist and uses a back filtering by the transfer function of the LPA.
The second one is stochastic and uses Kalman filtering.
At first, we modelize the LPA by a fifth order transfert function built with a description of its différents mechanic and electronic components and by an experimental frequency analysis.
Then the double filtering technic eliminates the phase distortions of the LPA signal, so we obtain a pseudo profile reproducing the exact profile with an attenuation for frequencies outside the analysor band pass.
The second method uses a LPA model obtained by parametric identification (maximum likelihood method) and a model of the profile type Wiener signal.
After eliminating polynomial components and low frequencies, the reconstructed signal follows accurately variations of the roughness road.
Results obtained from measurements made on a test bed and an experimental way are presented.
In this paper, we generalize relations between clean and noisy speech signal using vector Taylor series (VTS) expansion for noise-robust speech recognition.
We use it for both the noisy data compensation and hidden Markov model (HMM) parameter adaptation, and apply it for the cepstral domain directly, while Moreno used it to estimate the log-spectral parameters.
Also, we develop a detailed procedure to estimate environmental variables in the cepstral domain using the expectation and maximization (EM) algorithms based on the maximum likelihood (ML) sense.
To evaluate the developed method, we conduct speaker-independent isolated word and continuous speech recognition experiments.
White Gaussian and driving car noises added to clean speech at various SNR are used as disturbing sources.
Using only noise statistics obtained from three frames of silence and noisy speech to be recognized, we achieve significant performance improvement.
Especially, HMM parameter adaptation with VTS is more effective than the parallel model combination (PMC) based on the log-normal assumption.
A new hybrid higher-order cepstrum (HOC) and functional link network (FLN)-based blind equaliser (HOCFLN) is presented.
The system initially uses the complex cepstrum of the 1-D slice of the fourth-order cumulants of the unknown received signal to partially estimate the equaliser coefficients, then it switches to an FLN adaptive equaliser operating in the decision directed mode (DDM) to further improve the mean-squared error (MSE) convergence.
In this system two nonlinearities are used; one on the input data where the HOC is used and the other one in the FLN equaliser filter.
It is shown that in the new HOCFLN system multiple nonlinearities give significant performance improvement with less computational complexity, particularly in severe channel distortion compared to the conventional equalisation algorithms.
This method can accommodate both nonminimum phase MA and ARMA channels.
Performance results for channels exhibiting abrupt characteristic changes are also shown.
The description of 3D objects independently of their position and orientation, is an important and difficult problem in pattern analysis.
In this paper, we deal with this problem by a pseudo-Fourier transform on the group of motions of the 3D Euclidean space, which we denote by M(3).
This transform allows us to define 3D gray-levels object descriptors which are invariant and stable with respect to M(3).
This method is applied to human bones automatic classification and description.
It is attempted to reduce the phonetic quality of vowels to the positions of the peaks in their tonotopical spectra relative to the other peaks, simultaneous or preceding in context.
Synthetic two-formant vowels were identified by speakers of languages that differentiate richly among high vowels (Swedish, Turkish).
The parameters F 1 (204–801 Hz) and F′2 (509–3702 Hz) were systematicallyvaried in steps of 0.75 critical bandwidths.
f0 was kept close below F 1 in all vowels.
These were presented in two orders with subsequently rising or falling F′2.
Most subjects heard predominantly close vowels.
It is speculated that this second reference point might represent a default position of the third formant or the like.
In order to study methods of detecting a lack of vocal efficiency due to pathological changes of the larynx and of quantifying this lack at the acoustic level, we defined an acoustic parameter related to the efficiency of phonation and tested its performance in terms of its ability to discriminate between normal and dysphonic speakers.
We extracted a so-called vocal efficiency feature (VEF) from the autocorrelation function of the steady portion of the isolated French vowel /a/.
During the first stage we established by simulation the relationship between the VEF and the shape and duty cycle of the glottal waveform.
The simulation showed that the lower the VEF values were, the smaller was the glottal closure quotient and the more symmetric the waveform.
The proposed VE measure permitted discrimination between normal and dysphonic speakers.
The relevance of the resulting classification was illustrated by its interpretation in the framework of a theoretical model [20].
Research is reviewed that addresses itself to human language learning by developing precise, mechanistic models that are capable in principle of acquiring languages on the basis of exposure to linguistic data.
Such research includes theorems on language learnability from mathematical linguistics, computer models of language acquisition from cognitive simulation and artificial intelligence, and models of transformational grammar acquisition from theoretical linguistics.
It is argued that such research bears strongly on major issues in developmental psycholinguistics, in particular, nativism and empiricism, the role of semantics and pragmatics in language learning, cognitive development, and the importance of the simplified speech addressed to children.
This article develops the theory of misunderstandings by applying it to the analysis of the conversational organisation of a misunderstanding as it appears in a sequence from a tutorial dialogue.
Interlocutionary analysis (Trognon and Brassac, 1992) of the sequence containing the misunderstanding reveals that the conversational organisation of misunderstanding can take complex forms.
In the literature, the conversational organisation of misunderstanding rests on a structure with three elements: T1, the content of the turn bearing the misunderstanding, T2, the content of the turn revealing the misunderstanding, and T3, the content of the turn resolving the misunderstanding.
In the sequence analyzed, these three moments (T1, T2, and T3) do not follow each other in direct succession but are built into a complex hierarchical structure.
These three moments, more particularly T2 and T3, are reproduced several times in a row.
Furthermore, our sequence is unique as it illustrates a resolution of the misunderstanding without involving intersubjectivity.
The use of three types of vocabularies (cockpit-control words, digits, and initial consonants) was compared for the assessment of five speech recognizers.
The goal of this study is to compare various assessment methods from application oriented to carefully controlled laboratory situations.
It was found that the discrimination between various recognizer (input) conditions is improved for more difficult vocabularies.
Confusions between stimuli and responses of testwords can be used as a diagnostic tool for prediction of performance and developments.
In this paper we report our experience at LIMSI-CNRS in developing and porting a stochastic component for natural language understanding to different tasks and human languages.
The domains in which we test this component are the American ATIS (Air Travel Information Services) and the French MASK (Multimodal-Multimedia Automated Service Kiosk) applications.
The study demonstrates that for limited applications, a stochastic method outperforms a well-tuned rule-based component.
In addition we show that the human effort can be limited to the task of data labeling, which is much simpler than the design, maintenance and extension of the grammar rules.
Since a stochastic method automatically learns the semantic formalism through an analysis of these data, it is comparatively flexible and robust.
The problems that participants in conversation have, it is argued, are really joint problems and have to be managed jointly.
The participants have three types of strategies for managing them.
(1) They try to prevent foreseeable but avoidable problems.
(2) They warn partners of foreseeable but unavoidable problems.
And (3) they repair problems that have already arisen.
Speakers and addressees coordinate actions at three levels of talk: (1) the speaker's articulation and the addressees' attention to that articulation; (2) the speaker's presentation of an utterance and the addressees' identification of that utterance; and (3) the speaker's meaning and the addressees' understanding of that meaning.
There is evidence that the participants have joint strategies for preventing, warning about and repairing problems at each of these levels.
There is also evidence that they prefer preventatives to warnings, and warnings to repairs, all other things being equal.
Our research thematic deals with the representation quality and the outlier detection in supervised learning.
The prediction of a continuons value is referred to as regression learning. In this case, once constructed the neighbourhood graph resulting from the predictors, we proposed in a recent work to evaluate the representation quality using neighbourhood autocorrelation coefficient, like Moran spatial coefficient.
Extending the analogy with spatial analysis, we suggest in this paper to detect abnormal values of the variable to predictfrom local components of global neighbourhood autocorrélation coefficient and Moran scatterplot.
Experimentation led on varions bases from UCI Machine Learning repository has given interesting results.
This paper aims at giving an overview of recent advances in the domain of Speech Recognition.
The paper mainly focuses on Speech Recognition, but also mentions some progress in other areas of Speech Processing (speaker recognition, speech synthesis, speech analysis and coding) using similar methodologies.
Special emphasis centers on the improvements made possible by Markov Models, and, more recently, by Connectionist Models, resulting in progress simultaneously obtained along the above different axes, in improved performance for difficult vocabularies, or in more robust systems.
Some specialised hardware is also described, as well as the efforts aimed at assessing Speech Recognition systems.
Over the centuries Zaydīs have been called upon to respond to a series of challenges from within and without to the internal cohesion of their tradition.
Noting that Zaydīs did not commonly follow the legal opinions of their eponym Zayd b. ʿAlī (d. 112/740), Sunnī critics challenged them to justify their adoption of the label Zaydī.
The classical response was provided by the Yemeni imam al-Manṣūr ʿAbd Allāh b. Ḥamza (d. 614/1217), who explained affiliation to Zayd in theological and political terms.
Within Zaydism itself, however, disagreement among leading imams on questions of law occasioned dissent among their followers.
To counter this threat to unity within their ranks, Zaydī jurists widely adopted the theory that all qualified legal experts (muğtahids), including the Zaydī imams, were equally correct.
More technical were the questions that came to surround the character of the legal school (maḏhab) that became dominant among Yemeni Zaydīs.
These concerned both the source of the legal opinions that made up the doctrine of the school and the related question of the school's structure of authority.
These historical and theoretical questions acquired a particular urgency from the 11th/17th century and were popularized with the circulation of Isḥāq b. Yūsuf's (d. 1173/1760) short poem, ʿUqūd al-taškīk, which directly challenged Yemeni Zaydīs to clarify their legal identity.
Equally challenged was the structure of authority of all the Sunnī maḏhabs, and therefore issues raised here pertain to Islamic law more generally.
This poem evoked a variety of responses in prose and verse, including a short treatise by the poem's author, al-Tafkīk li-ʿUqūd al-taškīk.
While several respondents sought to affirm the viability of the legal school, others, notably Ibn al-Amīr al-Ṣanʿānī (d. 1182/1769) and Muḥammad b. ʿAlī al-Šawkānī (d. 1250/1834), argued that it could not be saved.
Their objections to traditional legal authority (taqlīd) within Zaydism were widely disseminated by 19th- and 20th-century Muslim reformers interested in undermining the Sunnī schools of law and continue to enjoy great currency.
One of a listener's major tasks in understanding continuous speech is segmenting the speech signal into separate words.
When listening conditions are difficult, speakers can help listeners by deliberately speaking more clearly.
In four experiments, we examined how word boundaries are produced in deliberately clear speech.
Durational measurements were taken of the pre-boundary syllable, and of pausing (if any) at the boundary, in baseline utterances and in deliberately clear repetitions.
We found that speakers do indeed attempt to mark word boundaries in clear (though not in normal) speech; moreover, they differentiate between word boundaries in a way which suggests they are sensitive to listener needs.
Previous research has suggested that in English, listeners apply heuristic segmentation strategies which make word boundaries before strong syllables easiest to perceive.
When conditions for the listener are difficult, however, speakers pay more attention to marking word boundaries before weak syllables, i.e. they mark just those boundaries which are otherwise particularly hard to perceive.
Weighted Medians with integer weights are widely used nonlinear operators in signal processing.
Weighted median filters belong to the class of stack filters.
Since each stack filter is uniquely specified by its corresponding positive Boolean function, the latter can be used to analyze the behavior of the filter.
On the other hand, weighted medians are usually represented by a given set of weights.
In this paper, weighted medians are first extended to include weighted median operators with positive real-valued weights.
It is shown that any real weighted median is equivalent to an integer weighted median.
As a consequence, a sensitivity analysis can always be performed to represent real weighted medians by finite precision weighted medians.
This result proves very useful if one realizes that optimal adaptive weighted medians are usually real-valued.
Conversion algorithms are presented to find the positive Boolean function representing a given weighted median and a set of weights of a weighted median representing a given positive Boolean function if feasible.
Approximate Value Iteration (AVI) is a method for solving a large Markov Decision Problem by approximating the optimal value function with a sequence of value representations V n processed by means of the iterations $ {V}_{n+1}=\mathcal{AT}{V}_n$ where $ \mathcal{T}$ is the so-called Bellman operator and $ \mathcal{A}$ an approximation operator, which may be implemented by a Supervised Learning (SL) algorithm.
Previous results relate the asymptotic performance of AVI to the L ∞ -norm of the approximation errors induced by the SL algorithm.
Unfortunately, the SL algorithm usually perform a minimization problem in L p -norms (p ≥ 1), rendering the L ∞ performance bounds inadequate.
In this paper, we extend these performance bounds to weighted L p -norms.
This enables to relate the performance of AVI to the approximation power of the SL algorithm, which guarantees the tightness and pratical interest of these bounds.
We illustrate the tightness of the bounds on an optimal replacement problem.
Variable selection and regularisation are two widespread methods for improving generalisation abilities in neural network models.
In this article, we show that using the L1-norm of the parameters as a regulariser allows for a simultaneous pruning ofunnecessary parameters.
Formally halfway between classical variable selection and weight-decay-style regularisation, this method reconciles both.
The result is parsimonious models with well-controlled parameters.
We first illustrate the properties of this regulariser with a one parameter analytical study.
We then present results obtained on neural networks for time series prediction.
Finally, we discuss practical aspects like parameter estimation, as well as links with a similar method proposed in the statistics literature in the context of linear regression.
Acoustic-phonetic decoding of speech recognition constitutes a major step in the process of continuous speech recognition.
This paper reminds the difficulties of the problem together with the main methods proposed so far in order to solve it.
We then concentrate on the different complementary approaches that have been investigated by our group: expert system based on spectrogram reading, recognition by phonetic triphones, connectionist model based on the cortical column unit and stochastic recognition without segmentation.
In this article, we present some development results of a system that performs mosaicing of panoramic faces.
Our objective is to study the feasibility of panoramic face construction in real-time.
This led us to conceive of a very simple acquisition system composed of 5 standard cameras and 5 face views taken simultaneously at different angles.
Then, we chose an easily hardware-achievable algorithm: successive linear transformation, in order to compose a panoramic face of 150° from these 5 views.
The method has been tested on hundreds of faces.
In order to validate our system of panoramic face mosaicing, we also conducted a preliminary study on panoramic faces recognition, based on the «eigenfaces» method.
We also are considering applying our system to other applications such as human expression categorization using movement estimation and fast 3D face reconstruction.
In this paper, we present an application of the evidence theory for the classification of physiological states in a bioprocess.
We are particularly interested by the relevance of the data sources which are here biochemical parameters measured during the bioprocess.
The evidence theory, and more particularly the notion of conflict is used to evaluate the relevance of each data source.
An other measure of conflict, based on a distance, is also used, and provides in some cases, better results than the classical notion of conflict of the evidence theory.
Results are presented for two kinds of bioprocesses: batch process (which corresponds to a supervised classification) and fed-batch process (which corresponds to an unsupervised classification).
A new approach to construct phonemic transcriptions of spoken utterances is described.
The Self-Organizing Feature Maps by Kohonen are first applied to vector-quantize speech into a sequence of phoneme labels a centisecond apart.
This code sequence is converted into a phoneme string using a multi-layered feed-forward network trained with error back propagation.
The trained network acts as a filter removing undesired transitional and coarticulatory effects from the code sequence.
This makes it almost a trivial task to convert the code sequence into a phoneme sequence.
The need for any statistical speech models, such as Hidden Markov Models, is thus eliminated.
The new approach is compared to an existing one being used in a speech recognition system, in which simple durational rules are used for the same transformation task.
The accuracy of produced phonemic transcriptions is 4.8 per cent units better using the proposed multi-layered network approach (88.4% opposed to 83.6%).
Human movement modeling can be of great interest for the design of pattern recognition systems relying on the processing of fine neuromotricity, like on-line handwriting recognition, signature verification as well as in the design of intelligent systems involving in a way or another the processing of human movements.
So far, many models have been proposed to study human movement production in general and handwriting in particular: models relying on neural networks, dynamics models, psychophysical models, kinematic models and models exploiting minimization principles.
Among the models which provide analytical representations, the Kinematic Theory of rapid human movements and its delta-lognormal model have been considered as very promising.
However, although numerous studies have shown that such a paradigm could explain most of the basic phenomena constantly reported in classical studies dealing with fine motor control, many problems, both theoretical and technical, have postponed its direct or indirect integration in the design of pattern recognizers.
In this paper, we overview these problems and report on various projects conducted by our team to overcome these difficulties.
First, we present a brief recall of the different models in the field and focus on the family of models involving lognormal functions.
Then, from a practical perspective, we describe two new parameter extraction algorithms suitable for the reverse engineering of single strokes as well as complex handwriting signals.
We show how the resulting representation can be used to improve electromyographic signal processing, opening a windows on new applications for handwriting processing, particularly in biomedical engineering and in some fields of neurosciences.
We briefly conclude by listing various potential applications of the Kinematic Theory, particularly in the fields of handwriting recognition, signature verification and biomedical signal processing.
In this article, the possible use of Kohonen's neural network to vector quantize images is investigated.
Some theoretical results on convergence of the training process are first given.
Then, results obtained for various codebook sizes and input dimensions are compared.
Tests are then performed with the best parameter values, using several images to design codebooks.
This approach is based on the concurrent use of five networks where the effect of various relevant parameters is studied, such as number of classes of vectors, vectors dimension, and number of vectors used for coding.
This article describes the architecture and the operation of the DIRA (Integrated Dialogue and Automatic Recognition) continuous speech recognition system in its present stage of development.
The DIRA system is a supervised multi-expert system.
The supervisor dynamically arranges the tasks of its expert modules, which are each attached to one of the subdomains of the speech recognition problem, i.e. the acoustic)phonetic, the lexical-, the syntactic)semantic-, the prosodic- and finally the pragmatic domain.
A blackboard serves as message interchange medium between these expert modules, as well as long-term memory for the speech recognition process as a whole.
The supervisor is an opportunistic planner: it reasons on the data present at the blackboard and «calculates» the best strategy (a scheme for the activation the expert modules) to resolve the current problem.
The operation of the individual expert modules is also addressed in this bases represented as rules controlling the transitions in ATN's (Augmented Transition Networks), the linguistic analyzers using the same A TN concept and the principle of functional lexical grammars, the comprehensive analyzer founded on the principle of lexical priming and finally the rule- based prosodic analyzer.
The operation of the speech recognition system is commented, while providing examples and test results.
In this paper, we present our work on the design, adjustment and registration of a 3D articulated model of the hand on monoscopic images sequences, without markers.
After a review of the state of the art, we describe our 3D generic model of the hand and how it is adjusted to the operator's morphology on an image of his open hand.
Then, we compare various cost functions and optimisation methods to register the 3D model of the hand.
Finally, we show results on image sequences.
Gesture capture by artificial vision could be a valuable help for applications such as human-machine interaction by gesture, for virtual humanoid animation, low bit rate coding of gesture for telepresence, or sign language recognition.
An accurate database documentation at phonetic level is very important for speech research: however, manual segmentation and labeling is a time consuming and error prone task.
This article describes an automatic procedure for the segmentation of speech: given either the linguistic or the phonetic content of a speech utterance, the system provides phone boundaries.
The technique is based on the use of an acoustic-phonetic unit Hidden Markov Model (HMM) recognizer: both the recognizer and the segmentation system have been designed exploiting the DARPA-TIMIT acoustic-phonetic continuous speech database of American English.
Segmentation and labeling experiments have been conducted in different conditions to check the reliability of the resulting system.
Satisfactory results have been obtained, especially when the system is trained with some manually presegmented material.
The size of this material is a crucial factor; system performance has been evaluated with respect to this parameter.
It turns out that the system provides 88.3% correct boundary location, given a tolerance of 20 ms, when only 256 phonetically balanced sentences are used for its training.
A new characterization of agrammatism is suggested, based on new data from Hebrew speaking agrammatic aphasics, and a reexamination of data from Russian and Italian.
This characterization is formed in relation to linguistic levels of representation.
It is then shown that agrammatic performance in a variety of tasks (including comprehension) is explained naturally as a consequence of this condition.
In the framework of phonemic speech recognition using Hidden Markov Models (HMMs) together with codebooks trained by Learning Vector Quantization (LVQ), a novel way to model context-dependencies in speech is presented.
We use LVQ to map acoustic contextual data into context independent phonemic form.
This mapping eliminates the need to employ context dependent phonemic, for example, triphone HMMs, and the difficulties associated therein.
Instead, simpler context independent discrete observation HMMs suffice.
We report excellent results for a speaker dependent task for Finnish.
This paper describes an approach for defining what individual strategies in constituting and decoding messages may be, in order to better understand and eventually automatically account for the variability of the speech signal.
The work presented here concentrates on the phonological level of French.
Two rule-based modules representing general phonological variants have been developed.
Results of tests on them are given, and the errors produced are examined.
The exceptions to the rules, taken from the rule-building data, as well as the assessment data, are examined in search of regularly produced events, common to several speakers, which are found to have common causes, for example dialect.
Speakers may then be characterised as belonging to a class of individuals who, as a result of a common cause, all make certain phonological choices in the same manner.
Speech is a natural error-correcting code.
The speech signal is full of rich sources of contextual redundancy at many levels of representation including allophonic variation, phonotactics, syllable structure, stress domains, morphology, syntax, semantics and pragmatics.
The psycholinguistic literature has tended to concentrate heavily on high level constraints such as semantics and pragmatics and has generally overlooked the usefulness of lower level constraints such as allophonic variation.
It has even been said that allophonic variation is a source of confusion or a kind of statistical noise that makes speech recognition that much harder than it already is.
In contrast, I argue that aspiration, stop release, flapping, palatalization and other cues that vary systematically with syllabic context can be used to parse syllables and stress domains.
These constituents can then constrain the lexical matching process, so that much less search will be required in order to retrieve the correct lexical entry.
In this way, syllable structure and stress domains will be proposed as an intermediate level of representation between the phonetic description and the lexicon.
My argument is primarily a computational one and will include a discussion of a prototype phonetic parser which has been implemented using simple well- understood parsing mechanisms.
No experimental results will be presented.
This article focusses on the automated synthesis of agents in an uncertain environment, working in the setting of Reinforcement Learning, and more precisely of Partially Observable Markov Decision Processes.
The agents (with no model of their environment and no short-term memory) are facing multiple motivations/goals simultaneously, a problem related to the field of Action Selection.
We propose and evaluate various Action Selection architectures.
They all combine already known basic behaviors in an adaptive manner, by learning the tuning of the combination, so as to maximize the agent's payoff.
The logical continuation of this work is to automate the selection and design ofthe basic behaviors themselves.
Image is often considered as the fundamental perceptual unit of a visualization.
In this paper, we suggest using one color image to allow an immediate and synthetic visualization of data.
The color permits to exhibit the main structures of dataset.
After reducing the dimensionality of the dataset, we generate color pixel using a transformation deduced from the work of Ohta et al.
The last step consists in sorting and arranging pixel into a squared image to provides the final color image that summurizes initial data.
The large-scale (3–3.5 Bark) spectral integration (LSI) theory derived from the work of Chistovich and colleagues and supposed to provide a basis for the computation of the F 2 parameter is not in fact supported by an actual proof, since all presumed evidence can be understood without this theory.
We believe that LSI existence cannot be demonstrated in a matching test, since if a subject is instructed to deliberately match a “complicated” spectrum with a “simplified” one, he will do it, whatever the way.
Evidence for a specific LSI mechanism can only be found if we are able to find a “trace” of it in an experiment in which no “simplification” of the spectrum is explicitly required from the subject.
In order to find a “trace of F 2” in memory, two sets of experiments (identification experiments and discrimination experiments with increasing memory load) were conducted using two sets of 4-formant synthetic vowels (around the [i]-[y] and the [e]-[ø] phonetic boundaries, respectively).
The main results of this study are the following:
• - significant differences observed in the discrimination scores cannot be explained without assuming that an LSI representation is in fact computed;
• - this LSI representation is more robust in the auditory memory than the classical critical band (i.e. 1-Bark) reprensentation.
The key argument was that stimuli associated with ambiguous F 2 determinations in matching experiments were also associated with ambiguous traces a memory — as revealed by a particularly high number of false alarms — in discrimination experiments.
These results provided us with the “trace of F 2 in memory” that we were looking for.
In this approach, the MLP is trained for phoneme classification, and then the output values of the MLP are used as the state-dependent weightings.
Applying the MLP outputs to the state-dependent weightings improves the performance of the conventional HMM without state-dependent weightings.
However, in order to further improve the discriminability of competing classes, the discriminative training of the state-dependent weightings is performed by computing the gradient of the optimization criterion for the state-weighted HMM with respect to the MLP parameters.
The proposed algorithm reduces the error rate considerably as compared with the conventional HMM in speaker-independent continuous speech recognition.
A segmental probabilistic model based on an orthogonal polynomial representation of speech signals is proposed.
Unlike the conventional frame based probabilistic model, this segment based model concatenates the similar acoustic characteristics of consecutive frames into an acoustic segment and represents the segment by an orthogonal polynomial function.
An iterative algorithm that performs recognition and segmentation processes is proposed for estimating the segment model.
This segment model is applied in the text independent speaker verification.
Tests were carried out on a 20-speaker database.
With the best version of the model, an equal error rate of 0.59% can be reached, for test utterances of 10 digits.
This corresponds to a relative error rate reduction of more than 50%, compared to the conventional frame based probabilistic model.
To classify objects located in their environment, underwater mobile robots use sequential sensory data (sonar).
These pieces of information are imperfect, that means imprecise, uncertain and incomplete.
Incompleteness is defined as the unavailability of some parameters which makes some classification criteria impossible to compute and which delays the decisions.
The paper proposes to model data in the framework of possibility theory, and to apply fuzzy calculus to evaluate criteria even in the case of incompleteness.
Results are sequentially fused by a dissymmetric combination process.
The different dissymmetric fusion rules are reviewed and a specific dissymmetric operator is proposed to solve the incompleteness problem.
This paper presents a comparative study of transformations used to compute the area of cross-sections of the vocal tract from the mid-sagittal measurements of the vocal tract.
MRI techniques have been used to obtain both mid-sagittal distances and cross-sections of the vocal tract for French oral vowels uttered by two subjects.
The measured cross-sectional areas can thus be compared to the cross-sectional areas computed by the different transformations.
The evaluation is performed with a jackknife method where the parameters of the transformation are estimated from all but one measurement of a speaker's vocal tract region and evaluated on the remaining measurement.
This procedure allows the study of both the performance of the different forms of transformation as a function of the vocal tract region and the stability of the transformation parameters for a given vocal tract region.
Three different forms of transformation are compared: linear, polynomial and power function.
The estimation performances are also compared with four existing transformations.
This paper provides a renewed perspective on the manufacturing of organizational decisions.
We approach decision making as performative praxis – a set of activities whereby actors simultaneously produce decisions and turn theories of decision-making into social reality.
We extend the performative praxis perspective – that combines the processes of conventionalization, engineering and commodification – to account for actors' direct and indirect mobilization of theoretical representations when making decisions.
This paper deals with Pseudo-QMF sub-band coding of color TV signals based on DPCM schemes with scalar quantization.
The proposed methods are based upon predictive coding system and scalar quantization and the objective is to get an excellent visual quality of the decoded images.
First, we proposed three sets of adaptive prediction functions and compared them.
Then we developed three adaptive quantization schemes with differents complexity of adaptation.
The low level of computer programs in go is not only due to the combinatorial complexity of the game but also to the difficulty to build a sound and complete evaluation function.
This paper highlights the numerous spatial concepts of a go evaluation function ; the most important are grouping, splitting, cercling and aggregating.
For such reason, computer go is an excellent example of the application spatial reasoning theories.
For each spatial concept, this paper shows the mathematical tools used for computer simulation (mathematical morphology, topology, Hausdorff distance, qualitative spatial reasoning).
This study is based on computer experiments which lead to the achievement of the international go playing program Indigo.
In this paper, we describe an efficient method for obtaining word classes for class language models.
The method employs an exchange algorithm using the criterion of perplexity improvement.
The novel contributions of this paper are the extension of the class bigram perplexity criterion to the class trigram perplexity criterion, the description of an efficient implementation for speeding up the clustering process, the detailed computational complexity analysis of the clustering algorithm, and, finally, experimental results on large text corpora of about 1, 4, 39 and 241 million words including examples of word classes, test corpus perplexities in comparison to word language models, and speech recognition results.
In this paper we present an adaptive microphone array with adaptive constraint values to suppress coherent as well as incoherent noise in disturbed speech signals.
We use a generalized sidelobe cancelling (GSC) structure implemented in the frequency domain since it allows a separate handling of determining the adaptive look-direction response to suppress incoherent noise and adjusting the adaptive filters for cancellation of coherent noise.
The transfer function in the look-direction is an adaptive Wiener-Filter which is estimated by using the short-time Fourier transform and the Nuttall/Carter method for spectrum estimation.
The experimental results demonstrate that the proposed method works well for a large range of reverberation times and is therefore able to operate independently of the correlation properties of the noise field.
Designing a visual aid for speech training of the hearing-impaired presupposes the solution of problems in various fields such as phonetics, automatic speech recognition, speech perception and production, learning theory, ergonomics, system development and computer programming.
The success of the eventual visual aid will depend on how well these problems are solved in a coherent fashion.
This paper proposes a structural description of the formal aspects of a visual aid for speech training, and presents an account of the type of problems met at different stages of developing such a device.
Together this leads to a framework for the construction of the Visual Speech Apparatus described in the second part of the paper.
The modification methods described in this paper combine characteristics of PSOLA-based methods and algorithms that resynthesize speech from its short-time Fourier magnitude only.
The starting point is a short-time Fourier representation of the signal.
In the case of duration modification, portions, in voiced speech corresponding to pitch periods, are removed from or inserted in this representation.
In the case of pitch modification, pitch periods are shortened or extended in this representation, and a number of pitch periods is inserted or removed, respectively.
Since it is an important tool for both duration and pitch modification, the resynthesis-from-short-time-Fourier-magnitude-only method of Griffin and Lim (1984) and Griffin et al. (1984) is reviewed and adapted.
Duration and pitch modification methods and their results are presented.
The digitization of mobile radio involves high demands on the system components.
It is, above all, the speech digitizer which requires particular attention.
The data rate used for the speech transmission largely determines the frequency bandwidths of the speech channels and, hence, the frequency economy of the entire system.
As a consequence, the data rate must be kept as low as possible.
In addition, it is necessary to use apropriate error protection codings in order to sufficiently compensate for disturbing influences of the radio channel on the speech quality.
Both requirements result in complex coder structures with different functional blocks complementing each other.
The structure discussed in this paper combines an optimum source coding algorithm based on Vector Quantization (VQ) with a modified multi-pulse procedure.
The paper shows possibilities of how to apply VQ to different parameter sets.
Particular attention is paid to the application of VQ to log-area coefficients split up into groups.
In this paper we present a review of some fluid mechanical phenomena involved in bilabial plosive sound production.
As a basis for further discussion, firstly an in vivo experimental set-up is described.
The order of magnitude of some important geometrical and fluid dynamical quantities is presented.
Different theoretical flow models are then discussed and evaluated using in vitro measurements on a replica of the lips and using numerical simulations.
This paper addresses the automatic cartography of sea-bottom by means of high resolution sonar images.
Many texture analysis methods have been developed since now, based on statistical, geometrical or spectral modeling [14, 45, 7, 44].
Nevertheless, few of them are robust toward image rotations.
Indeed, the property of rotation invariance is essential in our framework, particularily for obtaining good classification rates.
We present in this article five methods for the automatic classification of rotating images, corresponding to four classes of sea-floor: “sand”, “ridge”, “dune” and “wreck”.
The first one is an extension of a circular AutoRegressive method, initially proposed by Kashyap et Khotanzad [19], which allows to estimate a reduced number of rotation invariant parameters.
The four other methods are based on an original approach, consisting to apply a mathematical transform to a set of parameters describing texture features.
Two of them consist in computing the Log-Polar transform to autoregressive (AR) or correlation (COR) parameters.
The two others consist in estimating the Zernike moments of autoregressive (AR) or correlation (COR) parameters.
Classification rates obtained on sonar images and on Brodatz album are presented and allow to compare the performances of each approach.
We present a context-dependent, phoneme and function word based, Hidden Control Neural Network (HCNN-CDF) architecture for continuous speech recognition.
The system can be seen as a large vocabulary extension of the word-based HCNN system proposed by Levin in 1990.
Initially, we analysed context-independent HCNN modeling principle in the framework of the Linked Predictive Neural Network (LPNN) speech recognition system and found that it results in a 6% increase of the word recognition accuracy at perplexity 402.
Significant savings compared to the LPNN in the resource requirements and computational load for the HCNN implementation can be achieved.
In speaker-dependent recognition experiments with perplexity 111, the current versions of the LPNN and HCNN-CDF systems achieve 60% and 75% word recognition accuracies, respectively.
Between the end of the chain of waveform coders, which reaches from linear PCM to adaptive predictive coders (APC), and the class of (especially: predictive) vocoders, a “coding gap” of roughly 32-2.4 kbit/s is shown to actually define “medium-rate” speech coding.
The fundamental approaches trying to close the gap are exposed, working either in time or frequency-domain.
It is discussed, how their weaknesses may be overcome in advanced predictive or generalized “Filter-Bank” Coding (FBC) systems.
The basic ideas and problems of vector quantization (VQ) are reviewed.
Combinations of VQ with other schemes are addressed as well as other composite coding systems.
The negative peak amplitude of the differentiated glottal flow (d peak) is known to correlate strongly with the sound pressure level (SPL) of speech.
Therefore, the function between d peak and SPL has been conventionally modeled as a single line.
In this survey, the linearity of the function between d peak and SPL is revisited by analyzing glottal flows that were inverse filtered from speech sounds of largely different intensities.
It is shown that SPL–d peak-graphs can be modeled more accurately by using two linear functions, the first of which models soft phonation, and the second of which models normal and loud speech sounds.
For all of the analyzed SPL–d peak-graphs, the slope of the modeling line matching soft phonation was larger than the slope of the line for normal and loud speech.
This result suggests that vocal intensity is affected not only by the single amplitude domain value of the voice source, d peak, but also by the shape of the differentiated glottal flow near the instant of the negative peak.
This paper focuses on the partial emphasis or “prominence” of parts of Japanese sentences.
Four sets of 43 read sentences uttered by two speakers including various types of prominence (172 sentences in total) are analyzed.
This analysis shows that in 88% of the sentences prominence is produced by enhancing F 0 and increasing power.
No examples of lengthening of phoneme duration are observed in the emphasized parts of the sentences except for some special cases.
One exception is lengthening accompanied by pause insertion as a mark of prominence, and another slowing total speech rate.
The prosodic features of read natural speech are then used to develop rules for changing a reference sentence to produce prominence for rule-based speech synthesis.
Listening test results using 10 subjects do not show any significant difference in expressibility between prominence synthesized by rule (rate of correct expression: 76.9%) and prominence in natural speech (79.9%) at the 5% level.
To further improve prominence expressibility, listening tests for 10 subjects are used to clarify the conditions under which prominence expressibility becomes optimal.
These tests show that the prosodic control parameters increase the expressibility of prominence by about 20%.
Finally, prosodic features of spontaneous conversational speech are analyzed and compared with those of read sentence speech.
Speech-rate reduction in parts where prominence is placed is more conspicuous in spontaneous conversational speech.
Speaker variability in the coarticulation of the vowels /a,i,u/ was investigated in /C 1,VC2∈/ pseudo-words, containing the consonants /p,t,k,d,s,m,n,r/.
These words were read out in isolation by fifteen male speakers of Dutch.
The formants F 1–3 (in Bark) were extracted from the steady-state of each vowel /a,i,u/.
Coarticulation in each of 1200 realisations per vowel was measured in F 1–3 as a function of consonantal context, using a score-model based measure called COART.
The largest amount of coarticulation was found in /u/ where nasals and alveolars in C 1-position had the largest effect on the formant positions, especially on F 2.
Coarticulation in /a,u/ proved to be speaker-specific.
For these vowels the speaker variability of COART in a context was larger, generally, if COART itself was larger.
Studied in a speaker identification task, finally, COART improved identification results only when three conditions were combined: (a) if COART was used as an additional parameter to F 1–3; (b) if the COART-values for the vowel were high; (c) if all vowel contexts were pooled in the analysis.
The two main conclusions from this study are that coarticulation cannot be investigated speaker-independently and that COART can be contributive to speaker identification, but only in very restricted conditions.
In this paper, we tackle the problem of the extraction of non-redundant association rides using seniantics based on the Galois connection.
The presented approach is of strong interest for the visualization of the extracted rules.
The experiments carried out on real-life databases show the usefulness of the approach.
The performance levels of most current speech recognizers degrade significantly when environmental noise occurs during use.
Such performance degradation is mainly caused by mismatches in training and operating environments.
During recent years much effort has been directed to reducing this mismatch.
This paper surveys research results in the area of digital techniques for single microphone noisy speech recognition classified in three categories: noise resistant features and similarity measurement, speech enhancement, and speech model compensation for noise.
The survey indicates that the essential points in noisy speech recognition consist of incorporating time and frequency correlations, giving more importance to high SNR portions of speech in decision making, exploiting task-specific a priori knowledge both of speech and of noise, using class-dependent processing, and including auditory models in speech processing.
This article proposes an alternative rhythmic unit to the syllable: the inter-perceptual-center group (IPCG).
This group is delimited by events which can be detected using only acoustic correlates (Pompino-Marschall, 1989).
The rhythmic patterns for French are described using this characterisation: we show that realisation of accents is gradual over the trailed accentual group and that this gradual lengthening is needed for perception.
Identification experiments were performed to assess the relative importance of Fourier phase versus amplitude for intervocalic stop consonant perception.
In the first experiment, three types of stimuli were constructed from VCV signals: (1) Swapped stimuli, a swapped stimulus has the amplitude spectra of its consisting segments from one VCV signal and its corresponding phase spectra from another; (2) Phase-only stimuli; and (3) Amplitude-only stimuli.
It was shown that the perception of intervocalic stop consonants varies from amplitude dominance to phase dominance as the Fourier analysis window size increases.
The crossover lies somewhere between 192ms and 256ms.
The influence of phase at smaller window sizes was elaborated in the second experiment.
This experiment demonstrated that phonetically different signals can be constructed by combining the same short-time amplitude spectra with different phase spectra, so the short-time amplitude spectra displayed on a spectrogram cannot exclusively specify a stop consonant.
In both experiments, the perception of voicing in stops was found to rely strongly on phase information while the perception of the place of articulation was mainly determined by amplitude information.
We present a speech transmission system operating at a low bit rate (about 1000 bits/s), which is based on the following principle.
The speech signal is analysed using a LPC technique and, for each frame, the global parameters of the input signal (energy, pitch, voicing) on the one hand, and secondly the filter coefficients are separately coded.
Studies have been mainly focused on the last point. We have compared several representation spaces (autocorrelation, cepstrum, LPC analysis without and with preemphasis), in order to choose the most suitable representation for a good vector quantization.
The latter has been performed by a very simple algorithm, which we have compared to the “LBG” method.
We have shown that, in our experimental conditions, the simplicity of our algorithm is an advantage, and that it performs a good vector quantization of the spectral space.
The second part of the study is oriented towards the use of the codebook obtained as described above.
We compute it from several speakers, for a total of about 30000 frames (about six minutes) of speech: it consists of more than 1500 vectors.
We have studied how to obtain a fast coding of a vector with this codebook, losing the optimality of the nearest neighbour coding.
We have shown that the distortion is only slightly increased by using clustering techniques on the codebook, leading to a hierarchical coding decision, which allows a very fast coding of any new vector.
In conclusion, the simplification of the codebook construction (associated with a correct choice of the spectral representation space), and a fast (but suboptimal) method of coding with such a codebook lead to a system whose performances are only slightly degraded compared to reference spectral vector quantization systems for speech transmission.
In this paper we propose a new transform domain coding technique called transform trellis coded quantization (TTCQ) that is based on the trellis coded quantization technique recently proposed where Ungerboeck's amplitude modulation trellises and set partitioning ideas are used for source coding.
We combine the proposed technique with a transform domain formulation for small frame sizes obtaining a transform domain scheme suitable for low-delay speech coding.
Bellcore's CallManager system is an experimental prototype of a call screening and `follow-me anywhere' service platform that makes use of a personal appointment calendar and a personal phonebook (accessed through the customer's wireline or wireless PDA) to determine the customer's current location and the importance of each caller to the customer.
This is an example of a future class of network-provided services that utilize advanced speech processing and customer specific data to greatly enhance the value of the telephone network to the customer.
Such services allow important calls to reach the customer while screening out unwanted or unimportant calls.
These services can potentially also proactively provide information to a customer `as it happens' wherever the customer may happen to be.
The article focusses on how Jean Lemaire de Belges reflected on poetic as well as authorship issues during a period that required to take a stand against the medieval heritage - that is, all acquired knowledge from the Antiquity to the Middle ages.
In the Épîtres de l'Amant vert, the subordinate relationship between the poet and his patron results in a self-deprecating parody of the patronage tradition.
The political and moral allusions of the Illustrations de Gaule et singularitez de Troye bring into question the legitimacy of poetic discourse as well as its rhetoric, which are both influenced by their close relationship with the theme of love.
The Concorde des deux langages begins with the assumption of a possible reconciliation in the persona of the author, between the poet armed with the seductiveness of his eloquence, and the cleric submitted to the requirements of “labeur historien” (the historian's task).
We describe a model for the transduction of the basilar membrane displacements into activity of the auditory nerve fibers.
The model consits of two parts: a one-dimensional model of basilar membrane motions and a model of sensory receptors.
The first part is a linear functional model which gives the electrical analog of the displacement patterns of the basilar membrane.
The second part is a non-linear physical model which is mainly based on the assumption that information issued from the two kinds of hair cell receptors interacts electrically.
The model is shown to reproduce the excitation patterns of single auditory nerve fibers in response to simple input signals (one and two-tone stimuli) or to inferred trapezoidal displacements of the basilar membrane.
A second set of experiments is described to show how useful such a model can be for the understanding of complex sound coding.
We present results for a speech-like single formant input sound paying special attention to the fundamental frequency representation in the discharge patterns of our simulated fibers.
Moving away from constrained parametric to unconstrained flexible non parametric models is a deep trend in image processing that constant increase of computing resources makes each day more possible.
Deformable models offer a good illustration: from rigid parametric transformation to non parametric diffeomorphic matching, we show how the matching problem can be seen as search of optimal deformation paths between two objects, or equivalently determination of a minimal geodesic in a shape manifold.
This paper introduces recent research activities on speech recognition, ranging from acoustic processing to linguistic processing, at NTT (Nippon Telegraph and Telephone Corporation) Laboratories.
These include the proposal of ΔLSP parameters, hierarchical Δcepstral parameters, a new method of utilizing pitch information, automatic speaker adaptation techniques, robust HMM phoneme models, new training algorithms for neural networks, linguistic processing that uses syntactic and semantic knowledge, implementation of prototype continuous speech recognition systems, and an efficient text-independent speaker recognition algorithm.
We present a real-time approach for circular and polygonal road signs detection in still images, regardless of their pose and orientation.
Object detection is performed using a Hough like transform based on accumulation on pairs of points with their gradients.
Circular and polygonal (non triangular) traffic signs are detected by the so-called Bilateral Chinese Transform BCT.
Triangular traffic signs are processed by the Vertex Bisector Transform VBT.
These two approaches are tested on several databases of urban scene.
The visual data flow environment called V4Miner simultaneously uses a set of interactive techniques, visualization methods and machine learning algorithms to discover knowledge in an intuitive, interactive and retroactive way.
A data mining task is easily expressed by a graphical visual data flow where a method is represented by a graphic JavaBeans component, the communication between methods is based on a data bus.
The user can create and fully control the discovery process without any programming.
The visual data flow makes it possible on the one hand to reduce the complexity of an analysis task and on the other hand to improve quality and users ' comprehensibility of the obtained results.
This paper describes a rule-based data-driven (DD) method to model pronunciation variation in automatic speech recognition (ASR).
The DD method consists of the following steps.
First, the possible pronunciation variants are generated by making each phone in the canonical transcription of the word optional.
Next, forced recognition is performed in order to determine which variant best matches the acoustic signal.
Finally, the rules are derived by aligning the best matching variant with the canonical transcription of the variant.
Error analysis is performed in order to gain insight into the process of pronunciation modeling.
This analysis shows that although modeling pronunciation variation brings about improvements, deteriorations are also introduced.
A strong correlation is found between the number of improvements and deteriorations per rule.
This result indicates that it is not possible to improve ASR performance by excluding the rules that cause deteriorations, because these rules also produce a considerable number of improvements.
Finally, we compare three different criteria for rule selection.
This comparison indicates that the absolute frequency of rule application (F abs) is the most suitable criterion for rule selection.
For the best testing condition, a statistically significant reduction in word error rate (WER) of 1.4% absolutely, or 8% relatively, is found.
Jean Miélot's Contemplations sur les sept heures de la Passion, composed in 1454 for Philip the Good, duke of Burgundy, are preserved in a single copy, Paris, BnF, fr. 12441.
Miélot's main source was the anonymous Meditatio de passione Christi per septem diei horas, but a comparison of the two texts shows that he significantly modified the Latin text in order to adapt it for a lay reader.
This paper describes the design and implementation of the MATE workbench, a program which provides support for the annotation of speech and text.
It provides facilities for flexible display and editing of such annotations, and complex querying of a resulting corpus.
The workbench offers a more flexible approach than most existing annotation tools, which were often designed with a specific annotation scheme in mind.
Any annotation scheme can be used with the MATE workbench, provided it is coded using XML markup (linked to the speech signal, if available, using certain conventions).
The workbench uses a transformation language to define specialised editors optimised for particular annotation tasks, with suitable display formats and allowable editing operations tailored to the task.
The workbench is written in Java, which means that it is platform-independent.
This paper outlines the design of the workbench software and compares it with other annotation programs.
A two-part experiment on protrusion movements of the lower lip for the vowel /u/ was conducted to investigate: (1) the production of 'troughs', i.e., diminution of lip protrusion during the intervocalic consonant in /uCu/ utterances, and (2) 'time locking', that is, movement onset at a fixed interval prior to onset of voicing of /u/, regardless of the number of 'neutral' consonants in /VunroundedC n u/ utterances.
Two speakers of French, a speaker of Spanish and two speakers of English produced similar sets of nonsense utterances.
A bite block was used to eliminate mandible movements as a source of lower lip movements.
A detailed examination was made of relationships among events in movement and acoustic signals for each token of multiple repetitions of the test utterances.
Results suggest that the production of troughs may be due in part to specific articulatory targets for the intervocalic consonants /s/ and /t/.
Specific consonantal targets prevented the direct investigation of time-locking.
Experimental limitations preclude firm conclusions, but the nature of the results suggests that a complex of factors, including “co-production of adjacent segments” and “context-dependent adjustments of articulatory targets” may underlie observed variation in articulatory behaviour.
This paper reports a set of studies of some phonetic characteristics of the American English represented in the TIMIT speech database.
First we describe some relevant characteristics of TIMIT, and how we use the non-speech files on the TIMIT CD with a commercial database program.
Two studies are then described: one using only the non-audio parts of TIMIT (segmental transcriptions and durations, and speaker information), and one using the audio signal for acoustic analysis.
Results of such studies should be useful not only to linguistic phoneticians but also for speech recognition lexicons and text-to-speech systems.
We present in this paper a character recognition system using many classifiers.
Each classifier gives an answer and the final result is selected by majority-vote.
The system uses six classifiers built around first and second order hidden Markov models (TIMM) as well as nearest neighbor considerations.
The majority-vote is chosen so as to give better results than each of the other systems applied individually.
The recognition process is followed by a post-processing which employs combinations of stochastic and dictionary verification methods forword recognition and error-correction.
In this tutorial on time-scaling we follow one particular line of thought towards computationally efficient high quality methods.
We favor time-scaling based on time–frequency representations over model based approaches, and proceed to review an iterative phase reconstruction method for time-scaled magnitude spectrograms.
The search for a good initial phase estimate leads us to consider synchronized overlap-add methods which are further optimized to eventually arrive at WSOLA, a technique based on a waveform similarity criterion.
Complex emotions in real-life context have not been studied much.
We are exploring how to represent and automatically detect a subject's emotional state in humanhuman spoken interaction.
In contrast to most previous studies, conducted on artificial data, this paper addresses some of the challenges faced when studying real-life non-basic emotions.
Real-life spoken dialogs from call-center services have revealed the presence of many blended emotions.
A soft emotion vector is used to represent emotion mixtures.
This representation enables to obtain a much more reliable annotation and to select the part of the corpus without conflictual blended emotions for training models.
A correct detection rate of about 80% was obtained with SVMs and decision trees models between Negative and Neutral emotions using paralinguistic cues on a corpus of 20 hours of recording in a Medical call-center.
This paper describes a policy-based approach to firewall management.
The Policy-Based Networking (pbn) architecture proposed by the Policy Framework Group of Internet Engineering Task Force (ietf) is analysed, together with the communication protocols, policy specification languages, and the necessary information models.
An overview of policy specification languages applicability topbn architecture is presented paying particular attention to the specification of security policies through Security Policy Specification Language (spsl).
The Common Open Policy Service protocol (cops) and its variant,cops for Policy provisioning (cops-pr), both used for the transport of policy information, are also presented.
The paper continues with a description of an application of thepbn architecture to firewall management.
The proposed architecture is presented and its implementation issues are analysed with some usage examples.
The paper concludes with the evaluation of the policy-based approach to firewall management.
Two sets of experiments were performed to test the perceptual benefits of enhancing consonantal regions which contain a high density of acoustic cues to phonemic contrasts.
In the second set, corresponding regions in natural semantically-unpredictable sentence (SUS) material were annotated and enhanced in the same way.
Both sets of stimuli were combined with speech-shaped noise and presented to normally-hearing listeners.
These results demonstrate the benefits gained from enhancement techniques which use knowledge of acoustic cues to phonetic contrasts to improve the intelligibility of speech in the presence of background noise.
This study highlights a hitherto neglected trope of Muslim apocalyptic literature—namely, that in a region known as al-Ṭālaqān there awaits the future Mahdī a great treasure that will gain him a mighty army to aid him fight the final battle against evil.
Tracing the trope's origin in Zoroastrian apocalypticism and its subsequent dissemination in a wide array of Muslim apocalyptic traditions, this paper argues that this apocalyptic trope ultimately entered into Muslim apocalypticism, in particular Šīʿite apocalypticism, during a Zaydī revolt against the ʿAbbāsids led by the Ḥasanid Yaḥyā b. ʿAbd Allāh in the year 176/792.
The paper then explores how the revolt of Yaḥyā b. ʿAbd Allāh shaped the function of the 'treasures of al-Ṭālaqān' trope in Muslim apocalypticism and how Yaḥyā's personality and the revolt he inspired continued to leave an indelible imprint on Imāmī apocalypticism thereafter.
The development of dative objects by children acquiring Hebrew was investigated.
A large corpus of maternal input speech was examined, and the dative verb category was found to be extremely skewed for frequency as well as varied for semantics.
Children's development was explored through the analysis of 30 longitudinal observations.
The very first verbs produced with dative complements were indeed core verbs.
Semantic facilitation demonstrated transfer-based learning.
The effects of input frequency and obligatoriness were much smaller for the second wave of verbs learned.
The results reveal that two sequentially ordered processes operate in the development of a syntactic category which immediately recreate its structure: establishment of the core, and similarity-based transfer-learning of the periphery.
In a wide variety of applications, where the data X E A that must be processed characterize instances to which binary labels Y E {-1, +1} are randomly assigned, the goal of statistical learning does not reduce to find the likeliest label for a given instance but consists in ranking all the instances x E A in the same order as the one induced by the probability a posteriori y (x) = P {Y = +1 | X = x},, ranking rules being evaluated through ROC analysis.
In contrast to the majority of procedures used in practice, based on a preliminary estimation of the function y (x), the results described in this article propose an extension of the concept of decision tree to the ranking problem in order to optimize the ROC curve directly.
Combining the properties of both probability theories and graph theory, bayesian networks have become very popular during the past decade.
Determining the network's structure from a database of cases remains, however, a major issue.
We have developed a genetical algorithm defining a structure while ignoring the limitations usually imposed upon the search (limited number ofparents per node, knowledge of an ordering over the variables).
The algorithm searches the space ofdirected acyclic graphs and defines a set oflocal optima, eventually returning the best local optimum it has found.
This article describes a new approach for parametric analysis and synthesis of speech.
It is based on the frequency domain modelling of the residual signal of an LPC analysis, including an harmonic structure and pitch extraction, and a sub-band analysis in order to determine a «voiced-unvoiced ratio» variable with frequency.
This residue parametric representation is added to the classical LPC parameters for transmission or memorisation.
Our system presents of course the flexibility of parametrical systems and is well adapted to frame to frame transition problems encountered.in text-to-speech applications.
This paper describes and analyses algorithms for hands-free telephony which use an acoustic echo canceller combined with an additional FIR filter in the sending path of the hands-free telephone.
We describe two different methods to adapt the additional FIR filter (called “the echo shaping filter”) which are highly effective and easy to implement.
It is shown that these algorithms allow to reduce the order of the compensator significantly and still provide high echo attenuation and only little distortions of the near speech signal during double talk.
In this paper we present some issues of knowledge engineering in the field of life sciences and, as an illustration, a data integration system opened on the Web, called ONDINE (Ontology based Data INtEgration), which proposes a complete workflow to extract, to semantically annotate and to query data from tables found in scientific documents from the Web.
The core and key element of ONDINE is an Ontological and terminological Resource (OTR) allowing the modeling of n-ary relations; concepts from this OTR are used to annotate tables.
First we present the OTR model, then the semi-automatic method for semantic annotation of tables guided by this OTR, and ﬁnally our software system, @Web (Annotating Tables from the Web), designed to semantically annotate tables.
SIGNAL is a language (which definition is in progress at IRISA), intended to be the algorithms description language of a CAD system for real time signal processing applications.
Main characteristics of SIGNAL are issued from data flow principles: a program describes an oriented graph the nodes of which are processes (i. e. arithmetical and temporal primitive functions) connected through their input and output ports.
SIGNAL is able to describe and run any real time algorithm.
The presentation is illustrated by the way of an adaptive gradient lattice algorithm, treated as an example.
In an isolated testimony, Apollonius Dyscolus argues that some simple words are formed from words compounded with alpha privative, by means of the deletion of the alpha (ἀέκητι → ἕκητι).
The way Apollonius discusses this formation is paradoxical.
Indeed, the grammarian regards this formation as a pathos (morphological change).
However, a corrupted word-form, according to Apollonius himself, should always retain the meaning of the original form, whereas a simple word necessarily means the opposite of its privative compound.
This article aims to show what this conception of Apollonius, which challenges the very principle of pathology, teaches us about the particular status of the privative compound and its place in ancient grammatical theory.
Most of these systems work well only when the reference and test speech utterances are recorded under the same relatively noise-free conditions.
The goal of this study was to develop a speaker verification system based on an orthogonal linear prediction model, that adopts a unified approach by utilizing only one set of reference parameters irrespective of whether the test speech data is of high quality or noise corrupted.
However, for reference and test utterances spoken over the telephone, satisfactory verification is realized only when all the orthogonal parameters are utilized in distance computation.
This paper presents a study on finite-register-length effects in a Hidden Markov Model speech recognizer.
A statistic model is utilized to approximate the distribution of the score differences.
The range of recognition rate due to quantization noise on HMM parameters is calculated by using the statistic model.
Then the relation between the recognition rate and the quantization noise is derived.
This provides the information for determining the register length in the hardware realization of a HMM speech recognizer.
In this paper, we present a new writer independent system dedicated to the automatic recognition of on-line hand-printed texts.
This system uses a very large French lexicon (200000 words), which covers numerous fields of application.
The recognition process is based on the activation-verification model proposed in perceptive psychology.
A set of experts encodes the input signal and extracts probabilistic information at several levels of abstraction (geometrical and morphological).
We experiment several strategies of self-supervised writer-adaptation on this system.
The best one, called “dynamic self-supervised adaptation”, modifies the recognizer parameters continuously.
It gets recognition results close to supervised methods.
These results are evaluated on a database of 90 texts (5400 words) written by 38 different writers and are very encouraging as they reach a recognition rate of 90%.
This paper describes recent advances in glottal source modeling for speech synthesis.
In particular two procedures for modeling the glottal excitation waveform are described and applied to voice conversion.
One model uses a polynomial to represent the glottal excitation waveform for one pitch period.
The coefficients of the polynomial model form a vector that is used to design a glottal excitation code book with 32 entries for voiced excitation.
The codebook is designed and trained using two sentences spoken by different speakers.
In addition to the glottal excitation codebook, we use a stochastic codebook with 256 entries for unvoiced noise excitation.
Analysis techniques are described for constructing both codebooks.
The GELP synthesizer, which resynthesizes speech with high quality, provides the speech scientist with a simple speech synthesis procedure that uses established analysis techniques, that is able to reproduce all speech sounds, and yet also has an excitation model waveform that is related to the derivative of the glottal flow and the integral of the residue.
Another approach uses the LF glottal volume-velocity waveform to model the characteristics of three voice types: modal, breathy, and vocal fry (creaky).
We then convert a modal voice to sound like a breathy or vocal fry voice using the vocal tract characteristics for modal voice and the glottal volume-velocity waveform model for breathy and vocal fry voices as the excitation.
Speaker verification has been the subject of active research for many years, yet despite these efforts and promising results on laboratory data, speaker verification performance over the telephone remains below that required for many applications.
This experimental study aimed to quantify speaker recognition performance out of the context of any specific application, as a function of factors more-or-less acknowledged to affect the accuracy.
Some of the issues addressed are: the speaker model (Gaussian mixture models are compared with phone-based models), the influence of the amount and content of training and test data on performance; performance degradation due to model aging and how can this be counteracted by using adaptation techniques; achievable performance levels using text-dependent and text-independent recognition modes.
These and other factors were addressed using a large corpus of read and spontaneous speech (over 250 hours collected from 100 target speakers and 1000 imposters) in French, designed and recorded for the purpose of this study.
On these data, the lowest equal error rate is 1% for the text-dependent mode when two trials are allowed per verification attempt and with a minimum of 1.5 s of speech per trial.
Bayesian Belief Networks are a powerful tool for combining different knowledge sources with various degrees of uncertainty in a mathematical sound and computationally efficient way.
Surprisingly they have not yet found their way into the speech processing field, despite the fact that in this science multiple unreliable information sources exist.
The present paper shows how the theory can be utilized in for language modeling.
Using these extensions a language model for speech recognition based on a context-free framework is constructed.
In this model, sentences are not parsed in their entirety, as is usual with grammatical description, but only “locally” on suitably located segments.
The model was evaluated over a text data base.
In terms of test set entropy the model performed at least as good as the bi/tri-gram models, while showing a good ability to generalize from training to test data.
In this paper we describe a probabilistic fusion approach based on entropic criteria, which aims at reducing the combination space by explicitly representing the notions of source redundancy and source complementarity.
This modelling is particularly interesting to optimize the choice of measurements provided by sources in order to combine in multi-sources fusion system.
It is in agreement with the preoccupation to perform efficiently fusion processing and to minimize the hardware resources in information fusion problem.
To answer that, we made a study of the parallelization of the entropy fusion algorithm developed for its parallel implementation in the framework of an application to mobile robotics.
The algorithm specification exhibiting potential parallelism is then implemented on a network of workstations running in mode MIMD-NORMA using the parallel/distributed programming environments SynDEx which support AA-A methodology and PVM which support Hoare's CSP concept.
A simple neural network for isolated word recognition constructed under consideration of neurobiological and psychoacoustical observations is described.
The biologically motivated preprocessing of the speech signals includes transforming frequency to critical band-rate and power to loudness, contrasting the spectrograms and extracting temporal and spectral features.
It is shown that the different stages of preprocessing of the speech signal increase recognition rates significantly and are essential to achieve faultless recognition of a small vocabulary.
In addition, the network is able to recognize simultaneously spoken words without any change of its architecture.
Thus, it represents a concept to solve one of the most difficult figure-ground-problems in speech research without using conventional techniques like directional separation of stereophonically recorded speech or fundamental frequency tracking.
In this paper, we present the problem of noisy images recognition and in particular the stage of primitives selection in a classification process.
This selection stage appears after segmentation and statistical describers extraction on documentary images are realized.
We describe precisely the use of decision tree in order to harmonize and compare it with another less studied method based on a concept lattice.
This paper describes the IBM approach to Broadcast News (BN) transcription.
Typical problems in the BN transcription task are segmentation, clustering, acoustic modeling, language modeling and acoustic model adaptation.
This paper presents new algorithms for each of these focus problems.
Some key ideas include Bayesian information criterion (BIC) (for segmentation, clustering and acoustic modeling) and speaker/cluster adapted training (SAT/CAT).
Alphasic children with serious language deficits were taught to communicate via a system of visual symbols originally devised by David Premack (1969) for use with chimpanzees.
The subjects, lacking normal language, readily learned to express several language functions in this way (word, sentence, class-concept, question, negation).
The linguistic status of 'Premackese' is questioned, and it is suggested that it is better viewed as a communication system.
It may, therefore, be that the aphasic children lack some specifically linguistic ability.
In this paper, we apply a general reinforcement learning method to automatically design the behavior of non player characters of the Counter-Strike© first person shooter computer game.
The result of the learning process is a set of decision trees that represents compactly and easily readable a model of the problem itself and the decision policy of characters.
Beyond this example, we discuss the potential benefits of our method to design the decision architecture of non player characters in commercial computer games.
In the case of a “novel word” absent from a text-to-speech system's pronouncing dictionary, traditional systems invoke context-dependent letter-to-phoneme rules to produce a pronunciation.
A proposal in the psychological literature, however, is that human readers pronounce novel words not by using explicit rules, but by analogy with letter-to-phoneme patterns for words they already know.
In this paper, a synthesos-by-analogy system is presented which is, accordingly, also a model of novel-word pronunciation by humans.
It employs analogy in both orthographic and phonological domains and is applied here to the pronunciation of novel words in British (Received Pronunciation) English and German.
In implementing the system, certain detailed questions were confronted which analogy theory is at present inadequately developed to answer.
Thus, a major part of this work concerns the impact of implementational choices on performance, where this is defined as the ability of the system to produce pronunciations in line with those given by humans.
The size and content of the lexical database on which any analogy system must be based are also considered.
The better performing implementations produced useful results for both British English and German.
However, best results for each of the two languages were obtained from rather different implementations.
In this paper, we analyze the possibility of the reduction of smoothing levels in 3-D adaptive lower-upper-middle (LUM) smoother based on the fixed threshold control (FTC).
Besides the excellent noise attenuation capability with the simultaneous signal-detail preservation, recently introduced LUM FTC filter with a window size N is characterized by a relatively complex structure, where an estimate is formed according to (N +1)/2 decision rules.
This fact can constrain its possible hardware filter implementation in real motion video applications.
In order to simplify the filter complexity, however, to retain the excellent filter performance simultaneously, we propose two approaches such as linear reduction of smoothing levels and optimal reduction based on genetic algorithm.
Although most parameters in a speech recognition system are estimated from data by the use of an objective function, the unit inventory and lexicon are generally hand crafted and therefore unlikely to be optimal.
This paper proposes a joint solution to the related problems of learning a unit inventory and corresponding lexicon from data.
On a speaker-independent read speech task with a 1k vocabulary, the proposed algorithm outperforms phone-based systems at both high and low complexities.
We propose a new region-based segmentation of textured sonar images with respect to seafloor types.
We characterize sea-floor types by a set of empirical distributions estimated on texture responses to a set of different filters and we introduce a novel similarity measure between sonar textures in this attribute space.
Our similarity measure is defined as a weighted sum of Kullback-Leibler divergences between texture features.
Second, we add an additional weighting, evaluated as an angular distance between the incidence angles of the compared texture samples, to cope with the problem related to the sonar image acquisition process that leads to a variability of the backscattered (BS) value and the texture aspect with the incidence angle range.
Our segmentation method is stated as the minimization of a region-based functional that involves the similarity between region texture based statistics and prototype ones and a regularization term that imposes smoothness and regularity on region boundaries.
The proposed approach is implemented using level-set methods, and the functional minimization is done using shape derivative tools.
A hidden Markov model isolated word recogniser using full likelihood scoring for each word model can be treated as a recurrent 'neural' network.
The units in the recurrent loop are linear, but the observations enter the loop via a multiplication.
Training can use back-propagation of partial derivatives to hill-climb on a measure of discriminability between words.
The back-propagation has exactly the same form as the backward pass of the Baum-Welch (EM) algorithm for maximum-likelihood HMM training.
The use of a particular error criterion based on relative entropy (equivalent to the so-called Mutual Information criterion which has been used for discriminative training of HMMs) can have derivatives which are interestingly related to the Baum-Welch re-estimates and to Corrective Training.
A morphological method for the color segmentation of cytological images is presented.
This method is mainly based on watershed whose potential function blend local and global informations.
The method uses a priori informations for the frame of the method.
The paper is based on three parts.
In a first part, the frame of a morphological segmentation method is recalled.
Finally, the usefulness of the segmentation method is illustrated on images from serous cytology
The domain of the speech recognition and dialog system EVAR is train time table inquiry.
We observed that in real human-human dialogs when the officer transmits the information, the customer very often interrupts.
Many of these interruptions are just repetitions of the time of day given by the officer.
The functional role of these interruptions is often determined by prosodic cues only.
An important result of experiments where naive persons used the EVAR system is that it is hard to follow the train connection given via speech synthesis.
In this case it is even more important than in human-human dialogs that the user has the opportunity to interact during the answer phase.
Therefore we extended the dialog module to allow the user to repeat the time of day and we added a prosody module guiding the continuation of the dialog by analyzing the intonation contour of this utterance.
Hidden Markov Models (HMMs) have become the predominant approach for speech recognition systems.
One example of an HMM-based system is SPHINX, a large-vocabulary, speaker-independent, continuous-speech recognition system developed at CMU.
In this paper, we introduce Hidden Markov Modelling techniques, analyze the reason for their success, and describe some improvements to the standard HMM used in SPHINX.
We analyzed spontaneous speech production in semi-standardized interviews conducted with 10 patients suffering from moderate senile dementia of the Alzheimer type (SDAT), 5 Wernicke's aphasics, and 5 elderly controls without brain damage.
Data analysis revealed in both patient groups a reduction of sentence length but absence of systematic paragrammatic symptoms on the part of the demented patients.
A relatively selectively diminished use of nouns was striking in the production of both patient groups, whereas word finding ability was surprisingly well preserved in the SDAT patients.
Both patient groups exhibited marked deficits but different patterns of pathological behaviour on the discourse level of responding to the interviewer's questions.
Results are interpreted within a proposed neurolinguistic language production model.
It is argued that the formulation process may be preserved in demented patients but is disturbed in aphasia.
Language-related disturbances in senile dementia are assumed to result from pre-linguistic disorders in the formation of the conceptual structure of the intended speech act.
Articulatory models of speech production are generally controlled by kinematic parameters that govern the position and movement of tissue and air.
A four parameter model of the glottis is described with similar kinematic parameters to complement this approach.
Parameters are nondimensional zed quotients that control the static and dynamic shape of the glottis.
The model can stimulate glottal flow, glottal area, and vocal contact area simultaneously which identical parameters.
Specific features of these waveforms are discussed in terms of the glottal configuration that was used to produce them.
In particular, a detailed description of the contact area waveshape is included.
The model is expected to be applied to vocal fold imaging techniques that rely on signals gathered noninvasively on the surface of the body.
For synthesis, the model provides an alternative to flow pulse modeling because it can include some source-system interactions with relatively little computational overhead.
Psycholinguists strive to construct a model of human language processing in general.
But this does not imply that they should confine their research to universal aspects of linguistic structure, and avoid research on language-specific phenomena.
First, even universal characteristics of language structure can only be accurately observed cross-linguistically.
This point is illustrated here by research on the role of the syllable in spoken-word recognition, on the perceptual processing of vowels versus consonants, and on the contribution of phonetic assimilation phonemena to phoneme identification.
In each case, it is only by looking at the pattern of effects across languages that it is possible to understand the general principle.
Second, language-specific processing can certainly shed light on the universal model of language comprehension.
This second point is illustrated by studies of the exploitation of vowel harmony in the lexical segmentation of Finnish, of the recognition of Dutch words with and without vowel epenthesis, and of the contribution of different kinds of lexical prosodic structure (tone, pitch accent, stress) to the initial activation of candidate words in lexical access.
In each case, aspects of the universal processing model are revealed by analysis of these language-specific effects.
In short, the study of spoken-language processing by human listeners requires cross-linguistic comparison.
This research evaluated Grodzinsky's (1984, 1986a) syntactic loss and Kolk and van Grunsven's (1985) working memory impairment explanations of syntactic comprehension deficits in agrammatic aphasics.
Four aphasic patients were evaluated who showed different patterns of impairment on morphological and structural aspects of production.
The comprehension tasks compared performance on full and truncated passive sentences.
The syntactic loss hypothesis predicted worse performance on truncated than full passives, while the working memory deficit hypothesis predicted the reverse.
Neither hypothesis was supported, as the patients performed at a similar level on both types of passives.
In addition, there was little relation between the patients' production indices and their comprehension level.
The results argue against any global theory of agrammatism that attempts to attribute all agrammatic speech and co-occurring syntactic comprehension deficits to the same source.
In this paper the results of a study of objective quality measures for a broad range of coding systems are presented.
These objective measures take the linear and the nonlinear distortions of the coder into account.
A correlation analysis was performed, in order to find out those measures which are most effective in predicting perceivable parametric attributes of speech quality.
Furthermore, we describe the test signal we have used in our study, which was not natural speech but a speech-model process.
This paper deals with flexible information retrieval.
A method of gradual relevance feedback is presented.
Users give gradual preference to documents retrieved by the system.
They can express their preferences by 'this document is averagely relevant' or 'one document is very relevant'.
When designing multimodal systems, the designer faces the problem of the choice of modalities to optimize the system usability.
Based on modeling at a high level of abstraction, we propose an evaluation of this choice during the design phase, using a multicriteria principle.
The evaluation focuses on several points of view simultaneously, weighted according to the environment and the nature of the task.
It relies on measures estimating the adequacies between the elements involved in the interaction.
These measures arise from a fine decomposition of the interaction modalities.
Some considerations in the normalisation of tone are discussed, and their application demonstrated on the fundamental frequency data of seven speakers of a variety of Wu Chinese.
The derivation of a possible Linguistic-Phonetic representation for comparison with other varieties is also illustrated.
The acoustic echo canceler using an adaptive transversal filter and the least mean-square (LMS) algorithm is the most effective technique to reduce acoustic echoes in a hands-free telephone system.
However, the requirement of a very high order filter for each microphone results in difficulties in convergence and hardware implementation.
In this paper, the performance of the finite length adaptive filter is studied.
A formula which relates the echo cancellation to the filter size, N, is established.
Detailed analysis shows that this finite filter length will have better performance using speech than white noise.
Second order methods in signal processing for spatial analysis are limited by the correlation between sources.
In fact, the sources must emit independent signals in order to satisfy the exigences of theses methods as well as to obtain high resolution.
The method proposed here, increases the rank of the matrix that contains the vectors used to reform the source sub-space basis.
This is like a source decorrelation.
The advantage of this new method is that it continues to separate with a high resolution sources that are totally correlated unlike the popular method of spatial smoothing.
This article shows how the DEESE algorithm use translational invariance and directional invariance of the linear arrays for the recuperation of an orthonormal source sub-space basis.
A realistic simulations are also presented to confirm the correct position of source estimations and high resolution factors.
This paper attempts to compare the performance of two popular speech coders, namely regular pulse excitation with long-term predictor (RPE-LTP) and code-excited linear prediction (CELP), in both random and burst error environments.
In simulation, the bit stream generated by each coder is corrupted with error patterns obtained from the burst error model.
The burst error model consists of two parts: a Rayleigh fading envelope and an M-ary in differential phase shift keying reciever model in which the signal level is varied according to the Rayleigh fading envelope.
The performance of each coder is evaluated both objectively and subjectively.
The simulation results indicate that the RPE-LTP coder provides more consistent performance in the burst error environment.
This paper also discusses the comparison of four channel coding techniques designed for the CELP coder.
There are no major contradictions between Henri Ey's great plea for a “psychic body” or a “becoming conscious process” and the efforts cognitivist theoricians are displaying nowadays.
Organodynamism may be considered as the source of the latter's attempts as well as cognitivist theoricians may claim to take their inspiration from it.
This can offer advantages and enrich the two fields if the thread of history is maintained taught.
Detailed data on voicing source characteristics are of interest in both analysis and synthesis of speech.
In this study, a time-domain based method of inverse filtering was used to analyze male and female utterances produced with several voice qualities.
Fourteen mono-syllabic utterances (ten in normal voicing style, and 2 each in creaky and breathy voice) by each of 8 speakers (4 men, 4 women) were filtered by a set of zero pairs corresponding to the measured formants.
The resulting differentiated flow waveforms were analyzed in the time domain and in the frequency domain at voicing onset, near the middle of the vowel, and near the end of voicing.
Though there was variability among subjects of the same sex, the women tended to have shorter closed quotients and longer return quotients (the period between maximal rate of change of flow and minimal flow).
In the spectral domain, there tended to be less energy at higher frequencies in the middle of the vowel for the women compared to the men.
The magnitudes of the male-female differences are similar to those observed for the creaky-normal voicing differences and breathy-normal differences.
These differences may arise from a combination of biological, sociological and acoustical effects.
This paper presents a neural network methodology called «yprel networks».
After relating the main characteristics of the approach, we shall detail the incremental learning methodology used to improve the performances, which is based on the relearning phases from the classification errors.
The results obtained on a characters recognition problem are then discussed.
Among the extensive correspondence of Timothy I, Catholicos of the Church of the East, are two letters which refer to his collobaration in a translation of Aristotle's Topics into Syriac and Arabic, commissioned by the Caliph al-Mahdī.
An annotated English translation of both letters is provided.
To increase the believability and life-likeness of Embodied Conversational Agents, we introduce a behavior synthesis model for the generation of expressive gesturing. A small set of dimensions of expressivity is used to characterize individual variability of movement.
We empirically evaluate our implementation in two separate user studies.
Interaction effects between different parameters need to be studied further.
This article aims at providing a synthesis of the musical applications of digital signal processing, of related research issues, and of future directions that emerge from recent works in that field.
After introducing preliminary notions related to the music technical system and to the analysis of different digital representations of music information, it focuses on three main function types: audio synthesis and processing, sound spatialization and audio indexing and access technologies.
The Perceval printed in 1530, which collects under a single title the prose adaptations of five works related to the Conte du Graal of Chrétien de Troyes, is based on a manuscript that has since been lost.
The aim of this article is to verify, by the means of a survey of the entire text, the possible relationship with any of the existing manuscripts of the verse romances.
Although neither of these constitutes «the» source of the prose writer, the 1530 adaptation does shed light on the poems' textual tradition as well as suggest some variants which may stem back to the «originals».
This article states of the art about tools and techniques for images documents analysis and recognition.
The experience demonstrated that the implementation of an efficient knowledge management pass by the setting up of a knowledge cartography.
Thus, the cartography of knowledge is a means of «cognitive» navigation to reach resources of a knowledge heritage of an organization, that it is implicit or explicit.
Besides, it permits to have a fine understanding, by an analysis of criticity, of knowledge domains on which must be efforts made in term of capitalization, sharing or innovation.
We present in this article a methodology and tools offered to build such a cartography.
The paper describes a method for automatically segmenting a database of isolated words as required for the purpose of speech synthesis.
The phoneme-like units in the phonetic transcription of the utterances are represented by dedicated hidden Markov models (HMMs) and segmentation is performed by aligning the speech signal against the sequence of HMMs representing the words.
The specific advantage of the method presented here is that it does not need manually segmented speech material to initialize the training of the HMMs.
Therefore, it can be regarded as an improved variant of established techniques for automatic segmentation.
The problem of proper initialization of the HMMs without resorting to manually segmented material is solved by a hierarchical approach consisting of three successive steps.
In the first step a segmentation in broad phonetic classes is realized that provides anchor points for the second stage, consisting of a sequence-constrained vector quantization.
In this stage each broad phonetic class is further segmented into its constituent phonemes.
The result is a crude phonetic segmentation which is then used as initialization of the HMMs in the last stage.
Fine-tuning of the models is realized via Baum-Welch estimation.
The final segmentation is obtained by Viterbi alignment of the utterances against the HMMs.
Our basic representation of the data is a Galois lattice, i.e. a lattice in which the terms of a representation language are partitioned into equivalence classes w.r.t. their extent (the extent of a term is the part of the instance set that satisfies the term).
We propose here to simplify our view of the data, still conserving the Galois lattice formal structure.
For that purpose we use a preliminary partition of the instance set, representing the association of a type to each instance.
By redefining the notion of extent of a term in order to cope, to a certain degree (denoted as a), with this partition, we define a particular family of Galois lattices denoted as Alpha Galois lattices.
In this paper we present a solution to the nonlinear spectral estimation problem for speech enhancement.
We start from a rather simple statistical model (log-normal) for the short time spectral estimates of speech and noise.
By empirical data generation and curve fitting approaches we are able to get explicit, though simple, expressions for the MMSE estimator in function of input level and the model parameters for each frequency component.
The great advantage of our approach is that it has a sound theoretical foundation, is general by the choice of its parameters, and almost as simple to use as classical spectral subtraction.
Moreover, using a neural network as function approximator, which is found to be the best for our curve fitting problem, other model based MMSE estimators can be readily implemented with the proposed approach.
We introduce an estimate of the L2-probabilistic dependence measure constructed with the generalized Fourier series which is able to realize a linear vector feature dimensional reduction in the discriminate multi-class problem.
We compare the proposed algorithm with the well known linear discriminate analysis (LDA) and with a generalized version of a multi class recursive linear extractor based on the L2-probabilistic dependence measure (R1D L2-PMD).
For vector Gaussian mixtures such comparison is done in the mean of the probability error of classification which is estimated by a multivariate Kernel probability density function.
The corresponding smoothing parameters are optimized analytically in the sense of the Mean Integrated Square Error (MISE).
The non Gaussian case is evaluated with the error of the к nearest neighborhood classifier.
Finally we will illustrate the importance of the proposed method by testing it in the context of the face recognition
Most biologists and some cognitive scientists have independently reached the conclusion that there is no such thing as learning in the traditional “instructive” sense.
This is, admittedly, a somewhat extreme thesis, but I defend it herein the light of data and theories jointly extracted from biology, especially from evolutionary theory and immunology, and from modern generative grammar.
I also point out that the general demise of learning is uncontroversial in the biological sciences, while a similar consensus has not yet been reached in psychology and in linguistics at large.
Since many arguments presently offered in defense of learning and in defense of “general intelligence” are often based on a distorted picture of human biological evolution, I devote some sections of this paper to a critique of “adaptationism,” providing also a sketch of a better evolutionary theory (one based on “exaptation”).
Moreover, since certain standard arguments presented today as “knock-down” in psychology, in linguistics and in artificial intelligence are a perfect replica of those once voiced by biologists in favor of instruction and against selection, I capitalize on these errors of the past to draw some lessons for the present and for the future.
In this paper we propose two forms of hybridization between a metaheuristic (Ant Colony Optimization or Genetic Algorithm) and an exact method (ILP) for the solution of the car sequencing problem.
We examine whether such hybridizations can improve solution quality over what can be obtained from the alternative approach of using local search within one of these metaheuristics.
The results presented show that the use of hybrid approaches provides a significant improvement and that the performance of the heuristic or metaheuristic that the hybridization is based on greatly influences the quality of the results, however, computations time are long.
We conclude that diversification in a population-based metaheuristic plays an important role in the hybridization.
The prevailing approach to the acoustic-phonetic description oescription of the diphthong is based (1) on the two lowest, vocal-tract resonance (or formant) frequencies (F 1 and F 2) considered either individually or jointly in the F 1−F 2 plane, and (2) on a very sparse representation of the temporal course of these frequencies.
While this time-honoured approach has been particularly useful for characterising the initial and final vowels of the diphthong, there appears to be very little progress beyond the F 2−F 2 plane, as a parametric framework for elucidating the dynamic nature of the vowel-to-vowel transition.
By contrast, a more accurately spectro-temporal description of a subset of the Australian English diphthongs () is obtained in this work by considering a detailed, temporal representation of the three lowest formant-frequencies (F 1, F 2 and F 3).
In particular, certain nonlinearity features of the densely-sampled contour of the F 3 are highlighted, which appear to have hitherto been either unknown or considered inconsequential to the specification of the diphthong.
This finding is shown to contribute a new, three-dimensional (F 1−F 2−F 3) perspective on the acoustic characteristics of the vocalic transition of the diphthong.
In the history of the enunciative theories of the twentieth century, Charles Bally is the author of the first “general theory of enunciation”, although his name has rarely been mentioned by the theorists of the field.
In the context of the study of the relations between representations and operations among several enunciation theorists, in a more or less close relationship with the image of Saussure, we plan to shed light on the most revealing points that led Bally from stylistics to the theory of enunciation, pursuing a thread that goes from the distinction between “impressions” and “idées pures” (Précis 1905) to the affective / intellectual opposition (Traité 1909), then to the treatment of expressiveness as a mechanism (Le langage et la vie 1926), to arrive at the theory of enunciation, where he defines modality.
In this course, we discuss also his original interpretation of Saussurean concepts and conceptualizations, as well as Saussure's position on the status of the affectivity in linguistics.
An account is given for the evolution of strong-weak (trochaic) stress on disyllabic English nouns and weak-strong (iambic) stress on disyllabic English verbs.
This explanation draws on two claims:
(1) Language users adjust the stress patterns on words so that alternations between strong and weak beats are created (the principle of rhythmic alternation) and (2) Nouns and verbs tend to appear in different rhythmic contexts, such that verbs are more likely than nouns to be biased toward iambic stress.
Analyses of spoken and written samples of English revealed that disyllabic verbs were more likely than disyllabic nouns to receive an inflection that adds a syllable onto the word.
Because such syllables are weakly stressed, rhythmic alternation would be created if the disyllabic word received stress on the second syllable (e.g., “suggesting”) rather than the first (“promising”).
Two experiments showed that stress assignments on pseudowords such as “cortand” are in fact varied depending on the syllabic nature of inflections added to the words.
In addition, the text analyses and experiments can account for specific subpatterns within the noun-verb stress asymmetry as well as the general asymmetry itself.
Implications of these findings for theories of word stress are discussed, as well as the more general point that patterns of language change can be understood in terms of language processing at the level of the individual speaker or listener.
A 16 kbit/s speech codec with low complexity and low signal delay is presented which is a special version of the Regular-Pulse Excitation LPC approach (RPE-LPC).
This proposal is the basis for the codec standard which will be used in the future Pan-European digital mobile telephone system.
An experimental hardware model is described.
One way to lower the coding rate of CELP coders is to lengthen the excitation analysis frame size.
For enhanced speech quality in such a case it is desirable to have the CELP excitation peaky (or sharpened).
Based on this observation, we propose a new adaptive source in which samples of the source have different gains according to their amplitudes by a two-tap pitch predictor.
Simulation results show that peaky pulses at voiced onset and a burst of plosive sound are clearly reconstructed, and that in voiced sound the excitation has the desirable peaky pulse characteristic and the pitch periodicity is well reproduced.
A recursive algorithm (the recursion is on the order of model) related to the estimation of the coefficients of an autoregressive model based on maximozation of likelihood is presented.
The estimated coefficients are used to obtain the soatial power densities by means of maximum of entropy method.
Lastly, simulation results are presented for comparison with Durbin-Levinson's algorithm.
While endmembers are often extracted using a geometric approach, the abundances are usually estimated by solving an inverse problem.
In this paper, we bypass the problem of abundance estimation by using a geometric point of view.
The proposed framework shows that a large number of endmember extraction techniques can be adapted to jointly estimate the abundance fractions, with no additional computational complexity.
This is illustrated in this paper with the N-Findr, SGA, VCA, OSP, and ICE endmember extraction techniques.
A nonlinear extension is also proposed, using non linear dimension reduction methods such as MDS, LLE and ISOMAP.
These strategies maintain the geometric unmixing algorithms unchanged, for endmember extraction as well as abundance fraction estimation.
The relevance of the proposed approach is illustrated through experiments on synthesized data and real hyperspectral image.
We propose two strategies for experiment selection in the context of batch mode reinforcement learning.
The first strategy is based on the idea that the most interesting experiments to carry out at some stage are those that are the most liable to falsify the current hypothesis about the optimal control policy.
We cast this idea in a context where a policy learning algorithm and a model identification method are given a priori.
The second strategy exploits recently published methods for computing bounds on the return of control policies from a set of trajectories in order to sample the state-action space so as to be able to discriminate between optimal and non-optimal policies.
Both strategies are experimentally validated, showing promising results.
A Two-Level Time-Delay Neural Network (TLTDNN) technique has been developed to recognize all Mandarin Finals of the entire Chinese syllables.
The first level discriminates the vowel-group based on (a, e, i, o, u, v) and the nasal-group based on nasal ending (-n, -ng, -others).
The nasal-group discriminator is used to further split the large /a/ subgroup produced by the vowel-group discriminator.
The two groupings in the first level produce 8 subgroups in the second level.
Further discrimination in the second level enables the identification of all 35 Mandarin Finals.
The technique was thoroughly tested using 8 sets of 1265 isolated Hanyu Pinyin syllables, with 6 sets used for training and 2 sets used for testing.
The overall result shows that a high recognition rate of 99.4% on the training datasets and 95.6% on the test datasets, is achievable.
The top 4 recognition rate attained on the test datasets is as high as 99.1%.
In this paper, we develop different mathematical models in the framework of the multi-stream paradigm for noise robust automatic speech recognition (ASR), and discuss their close relationship with human speech perception.
Largely inspired by Fletcher's “product-of-errors” rule (PoE rule) in psychoacoustics, multi-band ASR aims for robustness to data mismatch through the exploitation of spectral redundancy, while making minimum assumptions about noise type.
Previous ASR tests have shown that independent sub-band processing can lead to decreased recognition performance with clean speech.
We have overcome this problem by considering every combination of data sub-bands as an independent data stream.
After introducing the background to multi-band ASR, we show how this “full combination” approach can be formalised, in the context of hidden Markov model/artificial neural network (HMM/ANN) based ASR, by introducing a latent variable to specify which data sub-bands in each data frame are free from data mismatch.
This enables us to decompose the posterior probability for each phoneme into a reliability-weighted integral over all possible positions of clean data.
This approach offers great potential for adaptation to rapidly changing and unpredictable noise.
When several different acoustic cues contribute to the perception of a phonetic distinction, a trading relation among the cues can be demonstrated in an identification task as long as the speech stimuli are phonetically ambiguous.
The present experiments address the question of whether the cues trade also in unambiguus stimuli, using a fixed-standard AX discrimination task with stimuli either from the vicinity of the phonetic category boundary or from within a phonetic category.
The results suggest that four of the five trading relations examined are tied to the perception of phonetic constrasts; they disappear or reverse within categories.
The one predicted exception represents a trading relation presumed to originate at a psychoacoustic level.
The data severely restrict psychoacoustic explanations for these effects and also suggest that within-category discrimination is not achieved in a phonetic mode of perception, thus affirming a dual-process view of speech discrimination.
The Multi-Layer Perceptron (PMC in French) is one of the neural networks the most widely used, particularly for supervised classification.
First, existing results on general representation capabilities enjoyed by the PMC architecture are surveyed, independently of any learning algorithm.
Then it is shown why the minimization of a quadratic error over the learning set seems an awkward optimization criterion, though some asymptotic properties are also proved.
In a second stage, the bayesian approach is analyzed when learning sets of finite size are at disposal.
With the help of certain density estimators whose basic properties are emphasized, it is possible to build a feed-forward neural network implementing the bayesian classification.
This technique of direct discrimination seems to perform better than the classical MLP in all respects despite of the similarities of the architectures.
Research has been conducted in the area of voice processing for over six decades but it has only been in the past few years that the impact of the years of research is starting to be seen in modern telecommunications systems.
Virtually every area of voice processing, including speech coding, speech synthesis, speech recognition, and even, to a small extent, speaker verification, has left the research laboratory and now appears in a product or service that is in daily use out in the marketplace, often by millions of customers per day.
This revolution in voice processing in telecommunications is fueled by algorithmic advances (which improve the quality of the voice processing systems), hardware advances (which provide high processing power and memory at low cost), and networking advances (which provide high bandwidth pipes to the home, office, and throughout the telecommunications network).
In this paper we illustrate the impact of voice processing on modern telecommunications by showing the diverse ways in which speech coding, speech synthesis, speech recognition and speaker verification have become embodied in new products and services.
This paper presents our interactive tool for knowledge acquisition. This device defines operational means to locate, acquire and develop knowledge in an image processing context.
Our pragmatic approach consist in using a terminology based on expertises to extract knowledge units.
This linguistic analysis resuit on the development of a consensual model of scenario and the development of an interface.
Then, we propose a dynamic interaction, to acquire and backup image processing knowledge in a scenarios base.
Finally, the exploitation, of aiready played scenarios, hy the naive is made in an intuitive way.
Thanks to a hyperholic graph, non-experts extract knowledge which are usefui to solve a new problem..
Among the main lip-geometry measures such as: • - frontal: width, height, area; • - lateral: upper, lower and corner protrusions; lip horn opening and depth; some relations are of interest for acoustic, articulatory and practical modeling.
In order to maximize the proper movements of the lips themselves, a corpus was designed to explore the maximum lip manoeuvre space for French vowels (5 speakers) using naturally close jaw conditions (consonantal [s] and [∫] contexts).
Proposals to calculate are from width or height alone are thus discarded.
Profile protrusions are naturally strongly correlated, and so is lip horn depth with corner protrusion.
So we can choose corner as the pivot in lip horn profile modeling.
But, for such a modeling, the most interesting relation between front and profile views is the inverse correlation between width and protrusion (on condition that corner protrusion is chosen).
This paper gives an overview of a system for phoneme-based large-vocabulary continuous-speech recognition.
The system provides the speaker dependent recognition component in the speech understanding system spicos that is designed to recognize and understand database queries spoken in natural German language.
The recognition problem then amounts to an efficient search through a huge state space such that purely local decisions can be avoided and globally optimal decisions can be taken.
The size of this state space depends primarily on the type of language model being used.
Three types of language models are studied: no language constraints, finite state network, stochastic trigram model based on word categories.
For each of the three language models, recognition experiments have been carried out on a 917-word task and 4 speakers.
For each speaker, 200 sentences totalling 1391 words had to be recognized.
In this paper we report on progress made at LIMSI in speaker-independent large vocabulary speech dictation using newspaper-based speech corpora in English and French.
The recognizer makes use of continuous density HMMs with Gaussian mixtures for acoustic modeling and n-gram statistics estimated on newspaper texts for language modeling.
Acoustic modeling uses cepstrum-based features, context-dependent phone models (intra and interword), phone duration models, and sex-dependent models.
For English the ARPA Wall Street Journal-based CSR corpus is used and for French the BREF corpus containing recordings of texts from the French newspaper Le Monde is used.
Experiments were carried out with both these corpora at the phone level and at the word level with vocabularies containing up to 20,000 words.
Word recognition experiments are also described for the ARPA RM task which has been widely used to evaluate and compare systems.
Many Linear Prediction (LP) vocoder systems use the LP residual energy method for gain matching.
Large gain errors occur however when a low frequency formant is in resonance with voiced excitation impulses.
In this paper, the errors are evaluated and their fundamental causes are discussed.
It is demonstrated that large errors are due to excessive spectral line suppression of the LP error signal, and to discrepancy between analysis windowing and continuous excitation in the synthesis.
Finally, an existing improvement of the method is discussed in the context of the statements.
We present a new image sequence analysis method for automatic and real-time extraction of transitory and complex motions in natural scenes.
We show how to extract these motions as multidimensional point clusters obtained from the temporal embedding of grey level variations, in five successive steps: embedding, fractal indexing, point chaining, cluster identification and data extraction.
We develop the two main algorithms: fractal space filling indexing and chaining in order to access directly to the relevant information.
To illustrate our method, we present an automatic system for early smoke source detection through the processing of landscape images by extracting fugitive and various movements within a small spot of pixels affected by the smoke.
We show how to modify the embedding technique used to obtain the data points coordinates to produce many other applications for the fractal embedding method, for example the recognition of complex moving or varying shapes objects.
While reporting on work done to elucidate the structuring-up process of acoustic space vs. time data-information with in the cochlea, the present article stresses the paramount importance of the phase effect within the ear.
This work is based on results secured with a peripheral-hearing model previously tested for reliability.
Here, this model is broken up into mutually independent parts for the specific function each fulfils between the external ear and the nerve fiber endings.
This model is described through a set of equations that apply to acoustic wave propagation, to the mechanicl vibration of the basilar membrane and to electro-mechanical transduction in hair cells and nerve fibers.
Representing information that runs inside the cochlea becomes a crucial problem when trying to detect pertinent acoustic cues to be used in signal analysis.
Moreover, for an improved perceptual quality, (e.g. in speech synthesis), phase is a definitely non-negligible factor.
Therefore, while the sonagram should be retained as an indispensible instrument in monitoring intensity evolution, it is equally imperative to secure a mode of sample-by-sample visualization of other relevant phenomena.
Mathematical morphology is based on the concept of ordering.
With color image process, write a valid order relation requires to using distances from standard color spaces CIELAB or CIELUV.
Since the first recommendations of the CIE (International commission on illumination), several colors distances have been proposed.
The aim of this paper is studying the impact of each color distances in the context of color mathematical morphology.
The results are developed for a new construction of morphological operators based on color distances in CIELAB space.
A criterion to evaluate methods of color ordering is then proposed to compare the main approaches in mathematical morphology with those based on a distance function.
Island-driven parsers have interesting potential applications in Automatic Speech Understanding (ASU).
Most of the recently developed ASU systems are based on an Acoustic Processor (AP) and a Language Processor (LP).
AP computes the a priori probability of the acoustic data given a linguistic interpretation.
LP computes the probability of the linguistic interpretation.
This paper describes an effort to adapt island-driven parsers to handle stochastic context-free grammars.
These grammars could then be used as Language Models (LM) by LP to compute the probability of a linguistic interpretation.
The intelligibility of initial and final consonants in monosyllabic CVC utterances was measured for rule-synthesized speech (dyadic concatenation).
In order to evaluate the intelligibility results, two additional speech conditions were tested PCM-coded speech (12 bits, 10 kHz sampling rate), and LPC-resynthesized speech (12 coefficients).
The stimulus set consisted of all possible initial and final consonants appearing in nine different CVC contexts, namely with three vowels /i, u, /, and with three nonneighboring consonants /p, t, k/.
The subjects had to identify the initial or final consonant in each word and could choose any single consonant as a response.
The overall percentage correct scores, averaged over 33 listeners, for the initial and final consonants, were 93% and 92.8% for PCM-coded speech, 86.7% and 85.4% for LPC-synthesis, and 58.2% and 73.5% for rule-synthesized speech.
Apart from these scores also the actual confusions are discussed in some detail.
More important than the scores achieved for this particular, meanwhile improved, version of the rule-synthesis system, was the insight gained in how to evaluate and improve such systems.
This paper presents a novel algorithm which generates three-dimensional face point trajectories for a given speech file with or without its text.
These codebooks consist of both acoustic and visual features.
Acoustics are represented by line spectral frequencies (LSF), and face points are represented with their principal components (PC).
During the synthesis stage, speech input is rated in terms of its similarity to the codebook entries.
Based on the similarity, each codebook entry is assigned a weighting coefficient.
If the phonetic information about the test speech is available, this is utilized in restricting the codebook search to only several codebook entries which are visually closest to the current phoneme (a visual phoneme similarity matrix is generated for this purpose).
Then these weights are used to synthesize the principal components of the face point trajectory.
The performance of the algorithm is tested on held-out data, and the synthesized face point trajectories showed a correlation of 0.73 with true face point trajectories.
In a real situation, the choice of the best representation for the implementation of a signature verification system able to cope with all types of handwriting is a very difficult task.
This study is original in that the design of the integrated classifiers is based on a large number of individual classifiers (or signature representations) in an attempt to overcome in some way the need for feature selection.
In fact, the cooperation of a large number of classifiers is justified only if the cost of individual classifiers is low enough.
This is why the extended shadow code (ESC) used as a class of shape factors tailor-made for the signature verification problem seems a good choice for the design of integrated classifiers E(x).
A sub-band multisensor structure using intermittent adaption is proposed for speech enhancement.
The convergence of the proposed method is compared with conventional LMS and frequency domain LMS and a dramatic increase in convergence rate is shown using both simulated and real data.
Preliminary investigation of sub-band filter order is also reported.
The analysis of the acoustic parameters which best summarize the cues to phone discrimination for the language under consideration should be a previous step in acoustic-phonetic decoding, regardless of the methodology to be used.
The Spanish language has not been widely analyzed from this point of view.
This work deals with the acoustic discrimination of Spanish stop consonants.
Our main goal was to find a reliable and reduced set of parameters for place of articulation identification of Spanish unvoiced stops.
On the basis of the obtained parameters, two automatic classifiers were developed and tested.
Only the acoustic features of the burst segment, automatically segmented from the speech waveform, were considered in the parameter estimation.
The analysis of these features was carried out in both the time and frequency domains over a CV context corpus uttered by 6 speakers.
In the first case, the classifier was designed as a procedural form.
Alternatively, in the second case a statistical classifier was obtained from a previous automatic discriminant analysis of the parameters.
Both classifiers were tested over a CV context corpus uttered by 40 new speakers not included in the analysis corpus, which resulted in a good rate of identification.
The proposed algorithm computes the coarse likelihood score for each word in a lexicon using the observation probabilities of speech spectra and duration information of recognition units.
With the proposed approach we could reduce the computational amount by 74% with slight degradation of recognition accuracy in an 1160-word recognition system based on the phoneme based hidden Markov modeling (HMM).
Also, we observed that the proposed coarse likelihood score computation algorithm is a good estimator of the likelihood score computed by the Viterbi algorithm.
Jitter measures are known to discriminate between normal and dysphonic speakers.
Two comparative studies were carried out.
The first showed that as far as inter-vowel quality differences were concerned, all significant differences could be related to the idiosyncratic behaviour of several preprocessing schemes with reference to vowel quality.
Intrinsic differences were canceled out by normalizing absolute jitter by the average fundamental period.
As a rule of thumb preprocessing routines were more successful, the further F 0 and F 1 were apart.
In a second experiment, jitter values extracted from connected speech did not discriminate between normal and dysphonic speakers any more efficiently than values calculated from sustained vowels.
As far as our corpora were concerned, no intrinsic superiority in the discrimination performance of connected speech as opposed to sustained vowels could be found.
In the case of running speech absolute microperturbation values appeared to be higher during inter-segment transitions and during voice onset and offset.
This paper presents a theoretical framework for environment normalization training and adaptation in the context of mixture stochastic trajectory models.
The presented approach extends, to segment based models, the currently successful technique of environment normalization used in adapting Hidden Markov models.
It also adds to the environment normalization framework a novel method for representing and combining different sources of variability.
In our approach the normalization and adaptation are performed using linear transformations.
When applied to speaker and noise adaptation in a continuous speech recognition task, our method led to up to 34% improvement in the recognition accuracy for speaker adaptation compared to unadapted models.
For noise adaptation the technique outperformed environment dependent models for some of the tested cases.
It was also observed that using environment normalization training in conjunction with transformation adaptation outperforms conventional MLLR.
In systems theory, pure anticipation seems the best mathematical solution for the problem of control, whatever the inputs are known a priori (case of the systems of regulation), or not (case of tracking systems).
From a formal point of view, this is obtained by putting the system to be controlled in cascade with its opposite system.
However, this solution is not realizable physically, because the model of the system is rarely complete.
Moreover, the opposite system is often unstable.
Lastly, the disturbances observed, either at the output, or at the input, make difficult the access to a satisfactory solution.
Last aspect: to be exploitable, anticipation has to be made in real-time.
The method described here tries to bring a fast and robust solution to the problem of prediction.
This method uses at the same time the geometrical properties of the signal to be predicted at the considered moment (local procedure), and a learning base constituted by past observations (global procedure).
The performances of this predictor are next evaluated on several significative examples and are compared with those of others predictors.
This paper addresses the impact of telephone transmission channels on automatic speech recognition (ASR) performance.
A real-time simulation model is described and implemented, which allows impairments that are encountered in traditional as well as modern (mobile, IP-based) networks to be flexibly and efficiently generated.
The model is based on input parameters which are known to telephone network planners; thus, it can be applied without measuring specific network characteristics.
It can be used for an analytic assessment of the impact of channel impairments on ASR performance, for producing training material with defined transmission characteristics, or for testing spoken dialogue systems in realistic network environments.
In the present paper, we present an investigation of the first point.
Two speech recognizers which are integrated into a spoken dialogue system for information retrieval are assessed in relation to controlled amounts of transmission degradations.
The measured ASR performance degradation is compared to speech quality degradation in human-human communication.
It turns out that ASR shows a different behavior than expected human quality judgments for some impairments.
This fact has to be taken into account in both telephone network planning as well as in speech and language technology development.
Results are presented which show that performance using the variable frame rate technique and triphone models can be better than that obtained using triphone models and full frame rate data.
The variable frame rate technique requires considerably less processing time.
The inverse problem for the vocal tract is under consideration from the viewpoint of the ill-posed problem theory.
The proposed approach, which permits overcoming the difficulties related to ambiguity and instability, is based on the variational regularization with constraints.
The work of articulators is used as a functional of regularization and a criterion of optimality for finding an approximate solution.
The measured acoustical parameters of the speech signal serve as external constraints while the geometry of the vocal tract, the mechanics of the articulation, and the phonetic properties of the language play the role of internal constraints.
An effective numerical implementation of the proposed approach is based on a local piecewise linear approximation of the articulatory-to-acoustics mapping and a polynomial approximation of the discrepancy measure.
A heuristic method named the “calibrating curves method” is applied for estimating the accuracy of the obtained approximate solution.
It was shown that in some cases the error of the inverse problem solution is weakly dependent on the errors of formant frequency measurements.
The vocal tract shapes obtained by virtue of the proposed approach are very close to those measured in X-ray experiments.
Knock is a well-known problem for spark-ignition engine manufacturers.
Knock detection helps achieve the best compromise between increasing engine efficiency, fuel consumption and present requirements with regard to exhaust emission legislation.
The ignition timing is usually controlled so that knock never occurs even with fuel quality changes.
The advantage of a knock detection method is to work within close range of knocking conditions but avoid its occurrence.
The purpose of our study consists in highlighting several knock intensities from block vibration signals provided by an accelerometer.
Our aim is to differentiate three kinds of engine cycles: absence of knock, increasing knock and heavy knock.
The developped diagnostic approach deals with fuzzy pattern recognition.
The method, experimented on a learning set, leads to several diagnoses that cooperate.
This paper investigates the impacts of standardisation and vehicularizationon linguistic diversity.
We demonstrate in the light of Tɔŋúgbe in south-eastern Ghana and vehicular Fula in Northern Cameroon that natural languages are social objects in motion, which can be institutionally and/or socially subjected to promotion or demotion.
In the first part, we show that Tɔŋúgbe, one of the dialects of the Ewe language, brings to the fore data that is not captured by standard Ewe even though such data is critical to the understanding and study of not just the Ewe language, but also, the whole Gbelanguage cluster.
We concentrate on the definite article in the dialect and demonstrate that, contrary to what pertains in standard Ewe, NP determination with the definite article in Tɔŋúgbe is an intersection between syntax and phonology.
In the second part we argue that vehicularizationas in the case of Fula Adamawa, appears to be a double- egged sword which on one hand promotes a language or a variety among others and, on the other hand, causes the demotion of minority languages or varieties.
An optimum coding of the parameters of a formant speech synthesizer is proposed.
The optimisation of these control parameters coding is based on statistical and subjective criteria.
The synthesizer used is a parallel synthesizer capable of synthesizing high quality speech whether voiced or unvoiced.
The utterances chosen for experimentation are groups of high quality synthetic French CVCV which represent French speech faithfully.
The first group of utterances consists of voiced stops (b, d and g with the vowels a, u and i) while the second group consists of voiced fricatives (З and z with the vowels a, u and i).
The proposed procedure consists of four steps.
The first one is a statistical study carried out on the control parameters of the synthetic utterances in order to find the optimum effective dynamic range of each parameter.
During the second step, the minimum number of bits necessary for quantizing each parameter independently (with no noticeable degradation) is found.
The third step finds the minimum number of bits when all the parameters are quantized simultaneously.
This is done by regrouping the parameters in sub-groups and finding the minimum number of bits when applying the quantization of the parameters of the sub-group simultaneously.
Then, we regroup the sub-groups until we find the optimum number of bits when all parameters are applied simultaneously.
The final step is to find the optimum sampling rate of each interval of each utterance.
A variable sampling interval is proposed that depends on the nature of speech events.
Applications that rely on mobile devices for user interaction must be mindful of the user's limited attention, which will typically be split between several competing tasks.
We propose a more dynamic context-driven approach to content delivery.
We demonstrate our approach using Scatterbox, a pervasive computing application we have developed which performs sensor fusion to derive a user's current situation.
Based on the user's level of interruptibility, Scatterbox prioritises and forwards relevant messages to their mobile phone.
We draw conclusions from a preliminary evaluation of the system.
This article presents several applications of the clustering criterion «K-products» introduced in [10].
This criterion is applied to the unsupervised mixture estimation, the straight lines extraction in binary images and the blind channel estimation in radiocommunications.
Some optimization algorithms are proposed and comparisons with other known methods are presented.
From a number of linear prediction parametric representations each of which furnishes equivalent information about the linear predictor, the cepstral coefficients representation is known to provide the best speech recognition performance.
Since the cepstral coefficients of a stable all-pole filter are inversely proportional to their quefrencies, these coefficients are multipled by their respective quefrencies.
The quefrency-weighted cepstral coefficients (also known as the root-power sums) are studied as to their effectiveness in a vowel recognition experiment and found to perform better than the cepstral coefficients with a Euclidean distance measure.
Our research project aims at elaborating hybrid Organizational Semantic Webs (O-SWs), based on a strong coupling between a Knowledge Base (KB) and a Documents Base (DocB).
The capitalized knowledge is hence distributed among the KB and the DocB.
The interest of modeling pieces of knowledge is that it allows the OSW to reason about this knowledge in order to assist users in their knowledge management tasks.
In counterpart the distribution of knowledge among heterogeneous sources renders its access more complicated.
In order to overcome these difficulties, we propose, on the one hand, to introduce a model of the information contained in the OSW, while making abstraction ofits mode ofspecification, and, on the other hand, to couple this model with a mechanism ofdynamic generation ofpresentations oftargeted information whose contents is adapted to the user.
In this paper, we present an overview ofthis approach.
Aerodynamic simulations of /aCa/ utterances were made using a low-frequency model for upper vocal tract airflow and a two-mass model for the voice source.
These simulations helped increase insight into the results of an empirical study of flow during running speech.
The various sources of flow, including wall compliance, were examined for their contributions to total flow from the mouth.
The two-mass model was modified to allow for more natural glottal flow during abduction and adduction.
Even with modifications the two-mass model was not sufficient to model source variations during running speech.
By means of a space-diversity technique which consists in using several identical subarrays (e.g. belonging to a single linear arrays composed of equispaced sensors), the propagator can be extracted from a single short data record and then performs goniometry of remote sources.
Thus the cases of non-stationary situations and fully correlated incident wavefronts can be handled, given that only the vector of the received signals is exploited, not their cross-spectral matrix.
In this method, a SNR per source at least equal to 0 dB is required.
Nevertheless, if a certain temporal coherence exists for the sources during a time interval corresponding to about thirty of forty snapshots, then the robustness can be greatly improved with respect to noise, using a technique which consists in applying an exponential memory to the vector of the received signals.
This superdirective method provides the phase of the incident wavefronts which can be used to estimate Doppler shifts.
The NOISEX-92 experiment and database is described and discussed.
NOISEX-92 specifies a carefully controlled experiment on artificially noisy speech data, examining performance for a limited digit recognition task but with a relatively wide range of noises and signal-to-noise ratios.
Example recognition results are given.
Research on spoken languages has shown that the durations of silent pauses in a sentence are strongly related to the syntactic structure of the sentence.
A similar analysis of the pauses (holds) in a passage in American Sign Language reveals that sequences of signs are also interspersed with holds of different lengths: long holds appear to indicate the ends of sentences; shorter holds, the break between two conjoined sentences; and the shortest holds, breaks between internal constituents.
Thus, pausal analysis is a guide to parsing sentences in ASL.
In this paper we describe a technique that we developed for enhancing speech signals degraded by additive non-stationary noise.
The performance of the technique is evaluated in the context of a speech recognition task on connected digits corrupted by different types of noise representative of military environments.
The algorithm is based upon spectral amplitude estimation of the speech signal given state-dependent parametric speech and noise models.
The spectral analysis is performed by a resonator based frequency interpolation filterbank whose parameters are selected according to the nature of the noise process.
The models are ergodic hidden Markov models (HMMs) with Gaussian multivariate distributions trained on noise and speech samples.
In this paper we propose a scheme for developing a voice conversion system that converts the speech signal uttered by a source speaker to a speech signal having the voice characteristics of the target speaker.
In particular, we address the issue of transformation of the vocal tract system features from one speaker to another.
Formants are used to represent the vocal tract system features and a formant vocoder is used for synthesis.
The scheme consists of a formant analysis phase, followed by a learning phase in which the implicit formant transformation is captured by a neural network.
The transformed formants together with the pitch contour modified to suit the average pitch of the target speaker are used to synthesize speech with the desired vocal tract system characteristics.
This paper presents the adaptation of Fujisaki's quantitative model to the analysis of German intonation and its application to F 0 synthesis by rule.
The parameter values of the model are determined by an automatic approximation of naturally produced F 0 contours.
The potential sources of variation of the parameter values are examined using statistical methods.
A set of rules is formulated that capture the effects of both linguistic and speaker-dependent features.
The rules generate artificial intonation contours which in turn can be related to linguistic features such as sentence mode or word accent.
Acceptability of the rule-generated intonation patterns as well as the adequate modelling of linguistic prosodic properties are evaluated perceptually by both phonetically trained subjects and prosodically “naive” listeners.
In general, utterances resynthesized with rule-generated F 0 contours are judged highly acceptable and natural by both groups of listeners.
The line spectrum pair (LSP) frequency representation has recently been proposed as an alternative linear prediction (LP) parametric representation.
In the context of speech coding, this representation shows better quantization properties than the other LP parametric representations.
In the present paper, the LSP representation is studied for speech recognition.
Several distance measures based on this representation are investigated on a steady-state vowel recognition task.
The weighted LSP distance measure is found to result in the best performance.
The performance of the weighted LSP distance measure is compared with that of the other popular LP distance measures (such as the Itakura, cepstral, weighted cepstral, root-power-sum, log area ratio and reflection coefficient distance measures).
The weighted LSP distance measure is found to perform significantly better than these popular LP distance measures.
The clustering methods are data mining tools, which aim at identifying groups of similar objects compared to the values they take on different attributes.
The methods known as conceptual clustering associate with the partition an interpretation of the classes.
Such a structuring can be described by a pair of partitions, called bipartition, composed of a partition of objects and a partition of attribute value pairs.
This article presents a study of the use of local search methods to produce bipartitions maximizing a quality measure defined by Goodman and Kruskal [GOO 54, GOO 59].
We propose an algorithm based on a stochastic neighborhood.
The stopping criterion is redefined in a statistical way to guaranty the quality of the obtained solution with respect to a confidence level.
Finally, a variational study of the function to be optimized leads to a complementary reduction of the computational complexity.
This contribution aims at evaluating the use of pronunciation variants for different recognition system configurations, languages and speaking styles.
To measure the need for variants we have defined the variant2+ rate which is the percentage of words in the corpus not aligned with the most common phonemic transcription.
This measure may be indicative of the possible need for pronunciation variants in the recognition system.
Pronunciation lexica have been automatically created so as to include a large number of variants (overgeneration).
In particular, lexica with parallel and sequential variants were automatically generated in order to assess the spectral and temporal modeling accuracy.
We first investigated the dependence of the aligned variants on the recognizer configuration.
Then a cross-lingual study was carried out for read speech in French and American English using the BREF and the WSJ corpora.
A comparison between read and spontaneous speech was made for French based on alignments of BREF (read) and Mask (spontaneous) data.
Comparative alignment results using different acoustic model sets demonstrate the dependency between the acoustic model accuracy and the need for pronunciation variants.
The alignment results obtained with the above lexica have been used to study the link between word frequencies and variants using different acoustic model sets.
When identifying pairs of simultaneous steady-state vowels, listeners perform well even when the two vowels start and stop at the same time, are presented monaurally, have the same fundamental frequency (f 0), and have approximately equal intensities.
The sensation described by listeners is of one dominant vowel “coloured” by a second or non-dominant vowel.
A small difference in f 0 improves performance and typically results in a sensation of two voice sources rather than of one voice coloured by another.
Dominance may reflect cognitive “decision” strategies in addition to spectral masking at the level of the peripheral auditory system.
An algorithm is proposed which automatically estimates the local signal-to-noise ratio (SNR) between speech and noise.
The feature extraction stage of the algorithm is motivated by neurophysiological findings on amplitude modulation processing in higher stages of the auditory system in mammals.
It analyzes information on both center frequencies and amplitude modulations of the input signal.
This information is represented in two-dimensional, so-called amplitude modulation spectrograms (AMS).
A neural network is trained on a large number of AMS patterns generated from mixtures of speech and noise.
After training, the network supplies estimates of the local SNR when AMS patterns from “unknown” sound sources are presented.
Classification experiments show a relatively accurate estimation of the present SNR in independent 32 ms analysis frames.
Like all products of human activity, handwriting exhibits extreme variability.
We are analysing this variability before initiating a text recognition process, a first degree of characterization for handwriting.
In the case of handwriting consisting of few words, such as the literal amount of cheques, this first degree can be obtained for each word independent of its semantic signification by extracting a small number of measures.
Based on the analysis of 989 handwritten amounts from cheques, it is shown that these measures are weakly correlated and define a variability space of non-uniform density, which suggests the possibility of classification of handwriting into a set of several families.
In this paper a new variant of HMM, named Multiple VQ HMM (MVQHMM), is presented.
Its main characteristic is the use of a separate codebook for each model.
Procedures for training and probability evaluation of these models are described.
The evaluation procedure combines the quantization distortions of the vector sequences with the discrete HMM generation probabilities.
Furthermore, the multiple VQ hidden Markov models seem to be more robust than the discrete and semi-continuous ones in relation to the inter-speaker variability of the recognition system.
A novel Content-Based Video Retrieval (CBVR) framework is presented in this paper: its purpose is to find similar video sub-sequences in videos.
By introducing temporal flexibility in the description of video sub-sequences, this framework makes the use of flexible, but slow, distance measures (such as Dynamic Time Warping) optional.
As a consequence, real-time retrieval of similar video sub-sequences, among hundreds of thousands of examples, is now possible.
The proposed method is adaptive; a fast training procedure is presented.
Performances have been successfully assessed on a dataset of 1,707 video clips (> 800,000 sub-sequences).
Ultimately, we plan to design a real-time alert (and/or recommendation) generation system for computed-aided video-guided surgery.
Does knowledge of language consist of mentally-represented rules?
Rumelhart and McClelland have described a connectionist (parallel distributed processing) model of the acquisition of the past tense in English which successfully maps many stems onto their past tense forms, both regular (walk/walked) and irregular (go/went), and which mimics some of the errors and sequences of development of children.
Yet the model contains no explicit rules, only a set of neuronstyle units which stand for trigrams of phonetic features of the stem, a set of units which stand for trigrams of phonetic features of the past form, and an array of connections between the two sets of units whose strengths are modified during learning.
Rumelhart and McClelland conclude that linguistic rules may be merely convenient approximate fictions and that the real causal processes in language use and acquisition must be characterized as the transfer of activation levels among units and the modification of the weights of their connections.
We analyze both the linguistic and the developmental assumptions of the model in detail and discover that (1) it cannot represent certain words, (2) it cannot learn many rules, (3) it can learn rules found in no human language, (4) it cannot explain morphological and phonological regularities, (5) it cannot explain the differences between irregular and regular forms, (6) it fails at its assigned task of mastering the past tense of English, (7) it gives an incorrect explanation for two developmental phenomena: stages of overregularization of irregular forms such as bringed, and the appearance of doubly-marked forms such as ated and (8) it gives accounts of two others (infrequent overregularization of verbs ending in t/d, and the order of acquisition of different irregular subclasses) that are indistinguishable from those of rule-based theories.
In addition, we show how many failures of the model can be attributed to its connectionist architecture.
We conclude that connectionists' claims about the dispensability of rules in explanations in the psychology of language must be rejected, and that, on the contrary, the linguistic and developmental facts provide good evidence for such rules.
The author discusses some recent contributions to the question of the syntactical function of the wayyiqtol form (the so-called impf. cons.) which is characteristic for Biblical Hebrew narratives (Waltke/O'Connor and Joüon/Muraoka on the one hand and Niccacci on the other).
He considers particularly the linguistic paradigms that underlie the contributions (»comparative linguistics«and»textual linguistics«).
He criticizes the etymological explanation offered by Waltke/O'Connor, at the same time arguing that Niccacci's textual linguistic model should be modified.
Finally, he tentatively suggests using the predominantly paratactic syntax of the LXX narratives as a model for modern bible translations.
Accordingly, the monotonous chains of wayyiqtol initial clauses in the Hebrew narratives should be rendered by coordinated clauses in the modern target languages.
This paper deals with the extension of the classical problem of discovering rules of the form «if a then almost b» to the search of generalized rules of the form R R' where R and R' can be rules themselves.
In the context of the statistical implicative analysis initially developed by Gras [GRA 79], [GRA 96], we recently proposed a first formalization based on the concept of “implicative hierarchy”.
Inspired from the classical hierarchical classification, an agglomerative algorithm amalgamates rules with an incremental process.
Here, we propose a new formalization which highlights the structures of the model.
And, we justified the used of the term “hierarchy” by associating it with a height which satisfies the ultrametric inequality.
The approach is illustrated on a survey from the French Public Education Mathematics Teacher Society.
This paper describes a spoken document retrieval (SDR) system for British and North American Broadcast News.
The system is based on a connectionist large vocabulary speech recognizer and a probabilistic information retrieval (IR) system.
We discuss the development of a real-time Broadcast News speech recognizer, and its integration into an SDR system.
Two advances were made for this task: automatic segmentation and statistical query expansion using a secondary corpus.
One of a listener's major tasks in understanding continuous speech is segmenting the speech signal into separate words.
When listening conditions are difficult, speakers can help listeners by deliberately speaking more clearly.
In four experiments, we examined how word boundaries are produced in deliberately clear speech.
In an earlier report we showed that speakers do indeed mark word boundaries in clear speech, by pausing at the boundary and lengthening pre-boundary syllables; moreover, these effects are applied particularly to boundaries preceding weak syllables.
In English, listeners use segmentation procedures which make word boundaries before strong syllables easier to perceive; thus marking word boundaries before weak syllables in clear speech will make clear precisely those boundaries which are otherwise hard to perceive.
The present report presents supplementary data, namely prosodic analyses of the syllable following a critical word boundary.
More lengthening and greater increases in intensity were applied in clear speech to weak syllables than to strong.
Mean F 0 was also increased to a greater extent on weak syllables than on strong.
Pitch movement, however, increased to a greater extent on strong syllables than on weak.
The effects were, however, very small in comparison to the durational effects we observed earlier for syllables preceding the boundary and for pauses at the boundary.
In conversation, speakers and addressees work together in the making of a definite reference.
In the model we propose, the speaker initiates the process by presenting or inviting a noun phrase.
Before going on to the next contribution, the participants, if necessary, repair, expand on, or replace the noun phrase in an iterative process until they reach a version they mutually accept.
In doing so they try to minimize their joint effort.
The preferred procedure is for the speaker to present a simple noun phrase and for the addressee to accept it by allowing the next contribution to begin.
We describe a communication task in which pairs of people conversed about arranging complex figures and show how the proposed model accounts for many features of the references they produced.
The model follows, we suggest, from the mutual responsibility that participants in conversation bear toward the understanding of each utterance.
When speaking to interactive systems, people sometimes hyperarticulate — or adopt a clarified form of speech that has been associated with increased recognition errors.
The goals of the present study were (1) to establish a flexible simulation method for studying users' reactions to system errors, (2) to analyze the type and magnitude of linguistic adaptations in speech during human-computer error resolution, (3) to provide a unified theoretical model for interpreting and predicting users' spoken adaptations during system error handling, and (4) to outline the implications for developing more robust interactive systems.
A semi-automatic simulation method with a novel error generation capability was developed to compare users' speech immediately before and after system recognition errors, and under conditions varying in error base-rate.
Matched original-repeat utterance pairs then were analyzed for type and magnitude of linguistic adaptation.
When resolving errors with a computer, it was revealed that users actively tailor their speech along a spectrum of hyperarticulation, and as a predictable reaction to their perception of the computer as an “at risk” listener.
During both low and high error rates, durational changes were pervasive, including elongation of the speech segment and large relative increases in the number and duration of pauses.
During a high error rate, speech also was adapted to include more hyper-clear phonological features, fewer disfluencies, and change in fundamental frequency.
The two-stage CHAM model (Computer-elicited Hyperarticulate Adaptation Model) is proposed to account for these changes in users' speech during interactive error resolution.
Within the framework of the Dempster-Shafer theory of evidence, data fusion is based on the building of single belief mass by combination of several mass functions resulting from distinct information sources.
This combination, called Dempster's rule of combination, or orthogonal sum, has several interesting mathematical properties, like commutativity or associativity.
Unfortunately, it badly manages the existing conflict between the various information sources at the normalization step.
The management of conflict is a major issue, especially during the fusion of many information sources.
Indeed, the conflict increases with the number of information sources.
That is why a strategy of conflict redistribution is essential.
In this paper, we define a formalism to describe a family of combination operators.
We propose to develop a generic framework in order to unify several operators.
We introduce, within this generic framework, traditional combination operators used within the evidence theory.
We propose other operators allowing a less arbitrary redistribution of the conflicting mass on the propositions.
These various combinations operators were tested on sets of synthetic belief masses and real data.
This paper examines the emergence of the qualifications langue morte (dead language) and langue vivante (living language) in France in the 17th century.
It is based on an extensive study of metalinguistic sources (grammars, dictionaries, collections of remarks, various treatises).
A first part deals with the evolution of the notions used to qualify Latin, from a representation in terms of alteration and corruption, in use since the 16th century, to those of life and death.
The second part shows how the development of the theory of usage is concomitant with a new valorisation of living languages.
The third part shows how dictionaries of the end of the century record the opposition between langue vivante and langue morte, paving the way for a new representation of languages that would later spread to educational contexts.
Through the study of this emerging paradigm, the question of the representation of languages as having regulated grammars or as changing vectors of human expression is addressed.
Local spectral distortion measures are commonly used to measure the similarity (or spectral distance) between two given short-time spectra.
In this study we compared several different spectral distortion measures including the Itakura-Saito distortion measure, the log likelihood ratio (LLR) distortion measure, the likelihood ratio (LR) distortion measure, the cepstral (CEP) distortion measure, and two proposed perceptually based distortion measures, the weighted likelihood ratio (WLR) and the weighted slope metric (WSM) distortion measures, in terms of their effects on the performance of standard dynamic time warping (DTW) based, isolated word, speech recognizer.
Two modifications of the basic forms of each measure were also investigated, namely a Bark-scale frequency warping and the incorporation of suprasegemental energy information.
All distortion measures and their modifications were tested on an alpha-digit vocabulary, 4-talker, telephone recording data base.
The results can be summarized as:
(1) All LPC-based distortion measures performed reasonably well.
The log likelihood ratio and weighted slope metric distortion measures gave the highest recognition accuracy, while the Itakura-Saito distortion measure gave the lowest score;
(2) Whereas the addition of suprasegmental energy information helped the recognition performance, the use of gain and absolute loudness degraded the performance;
(3) Bark-scale frequency warping did not, at least for the highly bandlimited telephone data base we tested, performed as well as its unwarped counterpart;
(4) The weighted likelihood ratio distortion measure did not perform as well as its unweighted counterpart.
A proteomic approach offers a powerful and complementary tool to genomics.
It allows to index and characterize proteins, and, for example, to compare their levels of expression between healthy and pathological states.
In mass spectrometry, the detector noise, the electronic and chemical noise, sometimes the small amount of peptides that has to be treated and finally the spectrum reduction noise (due to bad filtering and/or thresholding), can induce Parasitic Mass Peaks (PMP) and/or hide some Useful Mass Peaks (UMP) of low intensities.
The immediate consequence is that the presence of the PMP and the absence of the UMP will be detrimental to the protein identification quality.
In this article, we propose an original algorithm eliminating the PMP, detecting and amplifying those which are useful.
The preprocessing principle uses a multi-scale analysis technique coupled to a fuzzy thresholding (multi-scale fuzzy thresholding), a local amplification of the UMP, and finally an adaptive Base Line Correction.
The algorithm principle consists of dividing the frequential pass bandwidth of each masses spectrum into two subbands, a Low and High Frequency (LF,HF) subband, then each subband is in turn divided into two subbands etc.
The HF subbands are then thresholded according to the minimization criterion of the Shannon fuzzy entropy, and then amplified locally; the base line is calculated in an adaptive way and subtracted from reconstructed spectrum.
To evaluate the quality of this algorithm, we present a comparison of the results obtained by our algorithm, and those obtained by the DataExplorer software.
From a discourse perspective, utterances may vary in at least two important respects: (i) they can occupy a different hierarchical position in a larger-scale information unit and (ii) they can represent different types of speech acts.
Spoken language systems will improve if they adequately take into account both discourse segmentation and utterance purpose.
An important question then is how such discourse-structural features can be detected.
Analyses of monologues and human-human dialogues have shown that a good indicator of these factors is prosody, defined as the set of suprasegmental speech features.
This paper explores whether speakers also use prosody to highlight discourse structure in a particular type of human-machine interaction, viz., information query in a travel-planning domain.
More specifically, it investigates if speakers signal (i) the start of a new topic by marking the initial utterance of a discourse segment, and (ii) whether an utterance is a normal request for information or part of a correction sub-dialogue.
The study reveals that in human-machine interactions, both discourse segmentation and utterance purpose can have particular prosodic correlates, although speakers also mark this information through choice of wording.
Therefore, it is useful to explore in the future the possibilities of incorporating prosody in spoken language systems as a cue to discourse structure.
In this paper two methods for obtaining criteria for the perceptual evaluation of voiced/unvoiced detectors are tested and compared.
The first experiment consists of a perceptual scanning task in which subjects are asked to categorize short speech segments of 30 ms as “voiced” or “unvoiced”.
The judgments which are obtained in this task appear to be very reliable although the judgments on the onset of voiced segments are more reliable than those on the offset.
In the second experiment different versions of resynthesized speech are presented to a panel of listeners with the request to judge their relative quality.
The versions only differ in the location of the V/U-transitions.
It appears that the quality of speech in which voiced segments are determined by the SIFT pitch extraction algorithm but subsequently shortened by 20 ms at both ends is considered to be the most acceptable.
The acceptability of the versions in which the V/U-transitions are determined by listeners in a perceptual scanning task is only slightly less.
In the early 1990s, the availability of the TIMIT read-speech phonetically transcribed corpus led to work at AT&T on the automatic inference of pronunciation variation.
This work, briefly summarized here, used stochastic decision trees trained on phonetic and linguistic features, and was applied to the DARPA North American Business News read-speech ASR task.
More recently, the ICSI spontaneous-speech phonetically transcribed corpus was collected at the behest of the 1996 and 1997 LVCSR Summer Workshops held at Johns Hopkins University.
A 1997 workshop (WS97) group focused on pronunciation inference from this corpus for application to the DoD Switchboard spontaneous telephone speech ASR task.
We describe several approaches taken there.
These include (1) one analogous to the AT&T approach, (2) one, inspired by work at WS96 and CMU, that involved adding pronunciation variants of a sequence of one or more words (`multiwords') in the corpus (with corpus-derived probabilities) into the ASR lexicon, and (1+2) a hybrid approach in which a decision-tree model was used to automatically phonetically transcribe a much larger speech corpus than ICSI and then the multiword approach was used to construct an ASR recognition pronunciation lexicon.
We present here a Demand Maximizing Circuit Problem, which involves time elastic demands, and which is related to applications of Network Synthesis to the design of urban public transportation systems.
This problem consists in the optimization, on some irregular domain, of some quantity whose computation involves heavy computational costs.
We propose a specific metaheuristic Pursuit scheme, based upon the application to the original problem of a multiform rewriting process.
We present and discuss various interpretations of this scheme together with practical experimentations.
A theoretical overview and supporting data are presented about the control of the segmental component of speech production.
Findings of “motor-equivalent” trading relations between the contributions of two constrictions to the same acoustic transfer function provide preliminary support for the idea that segmental control is based on acoustic or auditory-perceptual goals.
The goals are determined partly by non-linear, quantal relations (called “saturation effects”) between motor commands and articulatory movements and between articulation and sound.
Since processing times would be too long to allow the use of auditory feedback for closed-loop error correction in achieving acoustic goals, the control mechanism must use a robust “internal model” of the relation between articulation and the sound output that is learned during speech acquisition.
Studies of the speech of cochlear implant and bilateral acoustic neuroma patients provide evidence supporting two roles for auditory feedback in adults: maintenance of the internal model, and monitoring the acoustic environment to help assure intelligibility by guiding relatively rapid adjustments in “postural” parameters underlying average sound level, speaking rate and the amount of prosodically-based inflection of F0 and SPL.
In this paper, we propose a new parameter smoothing method in the hybrid time-delay neural network (TDNN)/hidden Markov model (HMM) architecture for speech recognition.
In the hybrid architecture, the TDNN and the HMM are combined using the activations from the second hidden layer of TDNN as the outputs of a fuzzy vector quantizer (FVQ).
The HMM algorithm is modified to accommodate these FVQ outputs.
To improve the performance of the hybrid architecture, a new smoothing method has been proposed.
The average values of the activation vectors from the second hidden layer of the modular TDNN are used to generate the smoothing matrix from which smoothed output symbol observation probability is obtained.
With this proposed approach, our simulation results performed on speaker-independent Korean isolated words show the reduction of the error rate by 44.9% as compared to the floor smoothing method.
This paper describes a method of adapting a continuous density HMM recogniser trained on clean cepstral speech data to make it robust to noise.
The technique is based on parallel model combination (PMC) in which the parameters of corresponding pairs of speech and noise states are combined to yield a set of compensated parameters.
It improves on earlier cepstral mean compensation methods in that it also adapts the variances and as a result can deal with much lower SNRs.
The PMC method is evaluated on the NOISEX-92 noise database and shown to work well down to 0 dB SNR and below for both stationary and non-stationary noises.
Furthermore, for relatively constant noise conditions, there is no additional computational cost at run-time.
In this article, we propose a generic method for the automatic localisation and recognition of numerical fields (phone number, ZIP code, etc.) in unconstrained handwritten incoming mail documents.
The method exploits the syntax of a numerical field as an a priori knowledge to locate it in the document.
A syntactical analysis based on Markov models filters the connected component sequences that respect a particular syntax known by the system.
We show the efficiency of the method on a real incoming mail document database.
This paper presents a comprehensive study of continuous speech recognition in Spanish.
We have developed a semicontinuous phone-class dependent contextual modelling.
Using four phone-classes, we have obtained recognition error rate reductions roughly equivalent to the percentage increase of the number of parameters, compared to baseline semicontinuous contextual modelling.
We also show that the use of pausing in the training system and multiple pronunciations in the vocabulary help to improve recognition rates significantly.
The actual pausing of the training sentences and the application of assimilation effects improve the transcription into context-dependent units.
Multiple pronunciation possibilities are generated using general rules that are easily applied to any Spanish vocabulary.
With all these ideas we have reduced the recognition errors of the baseline system by more than 30% in a task parallel to DARPA-RM translated into Spanish with a vocabulary of 979 words.
Our database contains four speakers with 600 training sentences and 100 testing sentences each.
All experiments have been carried out with a perplexity of 979, and even slightly higher in the case of multiple pronunciations, to be able to study the acoustic modelling power of the systems with no grammar constraints.
Handling non-native speech in automatic speech recognition (ASR) systems is an area of increasing interest.
The majority of systems are tailored to native speech only and as a consequence performance for non-native speakers often is not satisfactory.
One way to approach the problem is to adapt the acoustic models to the new speaker.
Another important means to improve performance for non-native speakers is to consider non-native pronunciations in the dictionary.
The difficulty here lies in the generation of the non-native variants, especially if various accents are to be considered.
Traditional approaches to model pronunciation variation either require phonetic expertise or extensive speech databases.
They are too costly, especially if a flexible modelling of several accents is desired.
We propose to exclusively use native speech databases to derive non-native pronunciation variants.
Furthermore we combine this approach with online, incremental weighted MLLR speaker adaptation.
Using the enhanced dictionary and the speaker adaptation alone improved the word error rate of the baseline system by 5.2% and 16.8%, respectively.
When both methods were combined, we achieved an improvement of 18.2%.
Corporate terminology lies at the heart of numerous applications including corporate knowledge base, "know-how" & knowledge management, technology surveys, documentation research, etc.
Terminology acquisition is a difficult task and no unanimously accepted generic method exists.
This article demonstrates that an ontological model for the representation and meaning of terms guarantee these properties.
Such an approach defines the "Ontological Terminology", between the "Textual Terminology" and the "Conceptual Terminology".
We present a unifying framework for exact and approximate inference in Bayesian networks.
This framework is used in “ProBT”, a general purpose inference engine for probabilistic reasoning and incremental model construction.
This paper is not intended to present ProBT but to describe its underlying algorithms mainly the “Successive Restrictions Algorithm ” (SRA) for exact inference, and the "Monte Carlo Simultaneous Estimation and Maximization" (MCSEM) algorithm for approximate inference problems.
The main idea of ProBT is to use “probability expressions ” that can be “exact” or “approximate” as basic bricks to build more complex models incrementally.
In this article, a new method of classification of remote sensing images is described.
Usually, these images contain voluminous, complex, and sometimes erroneous and noisy data.
In our approach, classification rules are discovered by evolution-based process instead of applying a priori chosen classification algorithm.
During the evolution process, classification rules are created using raw remote sensing images, the expertise encoded in classified zones of images, and statistics about related thematic objects.
The discovered rules are simple to interpret, efficient, robust and noise resistant.
In the paper, the evolution-based approach is detailed and validated on remote sensing images covering not only urban zones of Strasbourg, France, but also vegetation zones of the lagoon of Venice.
The segmental quality has been and still is an important area to study and improve.
Evaluation of this aspect has been carried out at regular intervals.
We have settled for a basic VCV (vowel-consonant-vowel) test with the consonants in a symmetric vowel context.
All consonants in Swedish can occur in this position, and since we use an open response, nonsense word format, all confusions are possible.
In the most recent system, new parameter control strategies were explored.
Previously, most parameters in our system had been specified by smoothed step functions.
In the new system, the general smoothing algorithm has been replaced by a linear interpolation algorithm.
The development of the new system involved a major revision of the older rule-set.
Error rates have progressively decreased: 41.7% (1983), 26.1% (1987) and 12.8% (1989).
However, a concentrated effort on some of the consonants is necessary to improve the system further.
The use of Markov Random Field (MRF) models within the framework of global bayesian estimation has recently brought new powerful solutions to standard image analysis problems.
These models are generally associated with greedy relaxation algorithms.
This is the reason why multiresolution methods, well known in Computational Mathematics, are widely used to speed up the convergence rate of these algorithms.
But for the moment there is no real mathematical framework which associates in a simple and efficient way multigrid strategies and markovian models: most previous multiresolution markovian models have been defined using various heuristics, especially as far as the adjustment of parameters over scale is concerned.
The models we consider here are both mathematically consistent and computationally tractable and are related to a multiscale exploration of the set of solutions.
We detail the application of these new models to two basic issues in motion analysis from an image sequence: motion detection and 2D-motion estimation.
We show the advantages of the new approach: it allows the relaxation schemes to converge faster than those associated with standard multiresolution approaches, and toward better estimates (i.e. estimates of lower energy).
A new method for 2D/3D registration, applied to Magnetic Resonance Imaging (3D) and to X-Ray angiography (2D), has been adapted and used for planning treatment in radiosurgery.
The imaging flow needed for planning radiosurgery is considerable and using registration technique would make lighter the imaging protocol without restricting planning.
We describe the preliminary results of the evaluation giving criteria to compare registration technique and localization using stereotactic frame, which is the gold standard method.
Preliminary results obtained during this first step in validating registration put forward which kind of MRI sequence are more suitable to registration.
By now it should not be surprising that high performance speech recognition systems can be designed for a wide variety of tasks in many different languages.
This is mainly attributed to the use of powerful statistical pattern matching paradigms coupled with the availability of a large amount of task-specific language and speech training examples.
However, it is also well-known that such a high performance can not be maintained when the testing data do not resemble the training data.
The speech distortion usually appears as a combination of various acoustic differences but the exact form of the distortion is often unknown and difficult to model.
One way to reduce such acoustic mismatches is to adjust speech features according to some models of the differences.
Another method is to modify the parameters of the statistical models, e.g. hidden Markov models, to make the modified models characterize the distorted speech features better.
Depending on the knowledge used, this family of feature and model compensation techniques can be roughly categorized into three classes, namely: (1) training-based compensation, (2) blind compensation, and (3) structure-based compensation.
This paper provides an overview of the capabilities and limitations of the compensation approaches and illustrates their similarities and differences.
The relationship between adaptation and compensation will also be discussed.
The range of terminology based products required to satisfy emerging needs for document and knowledge management is significantly broadening.
In this paper, we assert that each application type implies to set up dedicated terminological products from texts or other domain resources.
We propose a unifying methodological framework as well as a set of software tools the use of which makes it easier to acquire knowledge from texts and to model adapted resources for a specific scope.
We mention in parallel some of the basic problems that arise and, when they exist, some technical or theoretical solutions to be considered.
We rely on three case- studies where we built up terminologies from texts for very distinct applications.
From this point of view, graph kernels provide a nice framework combining machine learning and graph theory techniques.
Among methods based on graph kernels, an major family is based on a decomposition of a graph into substructures.
In this paper, we present two extensions of a kernel previously based on unlabeled sub structures to labeled substructures and cyclic information.
We also propose selection methods which allow us to weight the set of considered sub structures in order to improve prediction accuracy.
Speaker verification experiments using discrete and semi-continuous HMMs with telephone quality isolated digits are reported.
A finite linear combination of Independent Identically Distributed (i. i.d) non Gaussian variables cannot be Gaussian.
In the case of an infinite linear combination, the Gaussian assumption is often considered by applying the limit central theorem.
The output ofARMA (possibly AR) filters is an infinite sum of independent input samples.
The aim of this paper is to study the output law of these filters and more precisely its «tendency» to the Gaussian law.
This paper addresses the issue of speaker verification system assessment.
The objective of this work is to develop a way of characterising speaker verification systems in a concise and meaningful manner.
Here a performance profile is suggested that encompasses the important aspects of a system under test, namely the actual verification performance, model storage requirements, confusability of the speakers in the test set, quality of the speech data used and the duration of speech data available for enrolment and testing.
Results are presented that show how this profile of measures can be used to give a more meaningful representation of a given system.
The aim of publishing this work is not to be prescriptive about a particular method of assessment, but rather to highlight the issues involved.
In general, better definitions of widely used terms are required before meaningful comparisons can be drawn between different speaker verification systems.
Widespread adoption of a set of standardised measures, along the lines of those suggested here, would significantly improve the value of such comparisons.
Jean Miélot plays an important part in the renewal of the knowledge of Antiquity in the late Middle Ages, conveying several texts of Italian humanism to the Burgundian court and translating Cicero.
In these circumstances, it seemed interesting to study especially the vocabulary of ancient Rome he used.
According to a very utilitarian conception of translation, the less the text is explicitly historical, the more the transpositions are numerous.
The corpus shows no intention to create or improve the vocabulary of ancient Rome.
Instead, Jean Miélot lies in the tradition of illustrious predecessors such as Simon de Hesdin and Nicolas de Gonesse.
Among the sources of information used in legal identification, fingerprints and genetic data seem to provide a high degree of reliability.
It is possible to evaluate the probability of confusing two individuals who might possess the same fingerprint characteristics or the same genetic markers, and to quantify the risk of a false alarm.
By their very nature, these data do not vary significantly over the course of time, and they cannot be modified by a suspect.
The erroneous metaphoric term “voiceprint” leads many people (not only the general public) to believe that the voice is as reliable as the papillary ridges of the fingertips.
According to present evidence, certain magistrates in France attach far too much importance to analyses of the voice which, along with other indices, should not be used except to help in directing an investigation.
In this communication, the author will detail the conditions under which, in France, voice analyses are carried out in the course of an investigation undertaken by the law, and will attempt to define the limits of this protocol, and the difficulty (and impossibility) of producing a reliable statistical test.
A historical review will then be presented of the discussions initiated by and position statements adopted by the French speech community since 1900.
Finally some ideas and proposals will be put forward in conclusion, which might be discussed by specialists in speech in collaboration with the police, the gendarmerie, and the magistrature, on a national, European, and international level, to advance the search for legal proof of identification within a scientific framework, and to end up with well-defined protocols.
The paper is concerned with the approach developed within the ANR Project StaRAC, and it gives an overview of its main results.
The objective was to reconsider the concept of stationarity so as to make it operational, allowing for both an interpretation relatively to an observation scale and the possibility of its testing thanks to the use of time-frequency surrogates, as well as to offer various extensions, especially beyond shift invariance.
Today, the need for variable bit rate coding exists to an increasing extent for, such as videoconferencing, audioconferencing, packet circuit multiplication systems and mobile radio applications.
This article extends the concept of embedded coding to include multi-stage CELP and VSELP coding at variable bit rates.
Then, the resulting orthogonalized gains are used in the derivation of a new algorithm, which is implemented off-line in order to ensure the optimization of successive codebooks in embedded multi-stage CELP/VSELP coding.
Finally, subjective test results are presented, which illustrate that 24 and 32 kbit/s embedded CELP/VSELP wideband coders provide speech quality close to that of the embedded SB/ADPCM G722 coders at 56 and 64 kbit/s.
This paper reviews and evaluates three recent stage theories of reading acquisition (Marsh, Friedman, Welch, & Desberg; Frith; Seymour) and also discusses the relationships between phonological awareness and reading, especially the direction of causality in such relationships.
Data from a longitudinal study of reading acquisition are then reported.
This study included assessments of phonological skills in children before they had begun to learn to read.
The results of the study suggest that (a) even if learning to read is conceptualised as a sequence of stages, not all children pass through the same sequence of stages, (b) phonological awareness and reading acquisition have a reciprocal interactive causal relationship, not a unidirectional one, and (c) phonological skills can play a role in the very first stage of learning to read among phonologically adept children.
Hence, it is incorrect to claim that the first stage of learning to read always involves such non-phonological procedures as “logographic” processing.
The aim of this work is to build up a common framework for a class of discriminative training criteria and optimization methods for continuous speech recognition.
A unified discriminative criterion based on likelihood ratios of correct and competing models with optional smoothing is presented.
The unified criterion leads to particular criteria through the choice of competing word sequences and the choice of smoothing.
Analytic and experimental comparisons are presented for both the maximum mutual information (MMI) and the minimum classification error (MCE) criterion together with the optimization methods gradient descent (GD) and extended Baum (EB) algorithm.
A tree search-based restricted recognition method using word graphs is presented, so as to reduce the computational complexity of large vocabulary discriminative training.
Moreover, for MCE training, a method using word graphs for efficient calculation of discriminative statistics is introduced.
Experiments were performed for continuous speech recognition using the ARPA wall street journal (WSJ) corpus with a vocabulary of 5k words and for the recognition of continuously spoken digit strings using both the TI digit string corpus for American English digits, and the SieTill corpus for telephone line recorded German digits.
For the MMI criterion, neither analytical nor experimental results do indicate significant differences between EB and GD optimization.
For acoustic models of low complexity, MCE training gave significantly better results than MMI training.
The recognition results for large vocabulary MMI training on the WSJ corpus show a significant dependence on the context length of the language model used for training.
Best results were obtained using a unigram language model for MMI training.
No significant correlation has been observed between the language models chosen for training and recognition.
This paper describes experiments on automatic speech recognition using demisyllbles as segmentation units and the consonant clusters contained therein as decision units for classification.
As compared to the large number of different demisyllables, the use of consonant clusters reduces the class inventory considerably.
In order to test the method, three experiments dealing with isolated German words were carried out.
In the third experiment a complete 1000-word recognition system was developed which performed the segmentation, the classification of consonant clusters and vowels, and a correction of recognition errors by use of a phonetix lexicon.
Demisyllable segmentation and processing have proved suitable, especially for large vocabularies.
Many problems in natural language processing (NLP) can be formulated as classification problems.
The complexity of the natural language make it difficult to select the discriminating attributes for a given task, the reliability of attribute values is often dubious and vary with the domain of the corpus.
This article argues that the bayesian networkformalism is well adapted for the classification of this type of data.
We have estimated the benefit brought by the bayesian classifior on a real NLP application: the distinction between the impersonal and anaphoric occurrences of the english pronoun it.
This paper describes a framework for time-dependent modelling of nonstationary signals, based upon autoregressive or ARM A models with time-varying coefficients.
Previous works by the author are summarized here: starting from ideas proposed by Rao, Mendel and then Liporace, it is assumed that the coefficients of the model are expressed as linear combinations of known time functions.
A class of estimators is described in this text, comprising autoregressive models, moving average models, and also lattice filters, parametrized through reflection coefficients or through Log Area Ratios.
Speech synthesis, which is one of the applications for which these models are efficients, concludes the paper.
In this paper, we describe the voice-activated interactive system MAIRIEVOX which has been developed at the CNET.
Finally, we describe some of the French industrial applications, developed from this experimental system MAIRIEVOX.
We address the definitions and synthesis of stochastic processes which possess warped scaling laws that depart from power law behaviors in a controlled manner.
We define warped infinitely divisible cascading (IDC) noise, motion and random walk.
We provide a theoretical derivation of the scaling behavior of the moments of their increments.
We provide numerical simulations of a warped log-Normal cascade to illustrate these results.
Algorithms for synthesis and Matlab functions are available from our web pages.
The participles of compound tenses have been the subject of special treatment among grammarians of Romance languages since the Renaissance.
Such forms raise a fairly specific issue since their properties do not appear to be compatible with the class of participles as the Latin tradition defines it.
Some suggest recategorizing these words by giving them a new designation or assigning them to a new class with more suitable properties.
This study brings together a series of texts (15th-18th centuries) in which theoretical options are proposed, and it highlights the importance of recategorization for dealing with data that challenge the Latin descriptive model.
Furthermore, the recurrence and commensurability of theoretical solutions in various traditions show the interest of going beyond the framework of national histories.
We present a performance evaluation of usual workstations and supercomputers for the simulation of synchronous Boltzmann machines during relaxation.
We consider the impact of several parameters, such as the network architecture, the type of relationship between layers, the coding of the neurons states, the arithmetic and the architecture of the hardware.
We give a method to forecast the average performance of different computers.
Finally, we conclude that the real time simulation of medium size Boltzmann machines will be achieved by the next generation of microprocessors.
Comprehension failures in agrammatic aphasics, as well as their difficulties in sentence construction, have been attributed to an underlying deficit involving the retrieval of syntactic structure.
In this study we show that four agrammatic patients display a remarkable sensitivity to structural information, as indicated by their performance on a grammaticality judgment task.
These results indicate significant sparing of syntactic knowledge in agrammatism, and suggest that the sentence comprehension disturbances in these patients do not reflect loss of the capacity to recover syntactic structure.
In particular, accounts of the comprehension deficit in agrammatism that implicate a failure to exploit information carried by the closed class (function word) vocabulary are called seriously into question.
Alternative explanations of the comprehension problem in agrammatism are explored.
This paper describes four experiments which have been carried out to evaluate the speech output component of the INSPIRE spoken dialogue system, providing speech control for different devices located in a “smart” home environment.
The aim is to quantify the impact of different factors on the quality of the system, when addressed either in the home or from a remote location (office, car).
Factors analyzed in the experiments include the characteristics of the machine agent during the interaction (voice, personality), the physical characteristics of the usage environment (acoustic user interface, background noise, electrical transmission path), as well as task-related characteristics (listening-only vs. interaction situation, parallel tasks).
The results show a significant impact of agent and environmental factors, but not of task factors.
Potential reasons for this finding are discussed.
The Prolog III programming language extends Prolog by redefining the fundamental process at its heart, unification.
Prolog HI integrates into this mechanism, refined processing of trees and lists, number processing, and processing of complete propositional calculus.
The capabilities thus acquired by the language are illustrated by various examples.
This article concerns the presentation of classifiers systems built to learn interactions.
Different kinds of classifiers systems are presented, using a chronological approach introducing new challenges or concepts approached by every model.
We describe used concepts: genetic algorithms and reinforcement learning.
The article presents basics systems such as ZCS or XCS, systems for anticipation, as well as hierarchicals or heterogenous classifiers.
We postulate in this paper that highly structured speech production models will have much to contribute to the ultimate success of speech recognition in view of the weaknesses of the theoretical foundation underpinning current technology.
We present two probabilistic speech recognition models with the structure designed based on approximations to human speech production mechanisms, and conclude by suggesting that many of the advantages to be gained from interaction between speech production and speech recognition communities will develop from integrating production models with the probabilistic analysis-by-synthesis strategy currently used by the technology community.
The aim of this research is to analyze the main durational changes occurring in the spoken string when a reader produces an emphatic accent (“didactic accent”) on target-words included in texts, and to verify whether these changes are organized into structured prosodic forms.
Three experiments address the following questions:
(1) Are the durational changes concentrated in the immediate vicinity of the target (last syllable and pause preceding the target, duration of enunciation of the target, pause following the target)?
(2) Are there intercorrelations among the durational variations observed?
(3) Does the typographical realization of the target produce different effects on time changes?
(4) Does the semantic weight of the targets change the way in which speakers produce didactic accents?
Two speakers had to read 16 different texts (each text once with and once without target-words).
Pre-target variables are intercorrelated, but post-target pauses vary independently.
Speakers are insensitive to typographical and semantic determinants.
The results are compatible with the hypothesis that mental representations of prosodic forms govern the temporal structure of speech in loud reading, and they show the importance of cognitive determinants during continuous reading.
This article gives a state of the art of temporal neural networks and a comparison of three recurrent neural network which are most representative for applications of dynamic monitoring and prognosis.
The criteria of selection of these networks are at two levels: a temporal criterion and an architectural criterion.
Following the application of these criteria, three recurrent networks seem relevant: the RRBF, the R2BF and the DGNN.
Tests using a benchmark of dynamic monitoring and a benchmark of prognosis enable us to evaluate the performances of the three temporal networks in term of computing and processing capacity time.
Different levels of affects are expressed in different levels of speech processing: expressions of emotions linked to an involuntarily triggered control, expressions of the speaker's attitudes and intentions and meta-linguistic expressive strategies.
C-Clone is presented as an interactive cognitive architecture of the expressive communication.
An auto-annotated subset of emotional expressions made possible the validation of the modeling of affective prosody into gradient contours.
Moreover perceptive experiments show that no prosodic dimension is specific of an emotional value.
This paper introduces a new topological clustering formalism, dedicated to categorical data arising in the form of a binary matrix or a sum of binary matrices.
The proposed approach is based on the principle of the Kohonen's model (conservation of topological order) and uses the Relational Analysis formalism by optimizing a cost function defined as a Condorcet criterion.
We propose an hybrid algorithm, which deals linearly with large data sets, provides a natural clusters identification and allows a visualization of the clustering result on a two dimensional grid while preserving the a priori topological order of the data.
The proposed approach called CRT was validated on several data sets and the experimental results showed very promising performances.
The dynamic processing of speech in the auditory system is apparently performed in parallel to spectral shape analysis and detection of spectral change.
This formulation, closely paraphrased on Chistovich et al. (1982), is a background assumption against which we examine the auditory role of diphthongs as an example of spectral change over time.
When we do this, we find a clear conflict in the literature concerning the relative perceptual weight of different components of a diphthong.
Does the listener attend to a diphthong's endpoints, or to its spectral rate-of-change?
This paper undertakes further experiments to try to resolve the conflict on this issue.
As a result of our findings, a simple cumulative model of auditory processing of the speech signal turns out to be inadequate.
So too do those models which rely on the rate-of-change itself as a trigger for the spectral comparison process.
Instead, the role of spectral change in diphthongs seems to be (a) as a flag commanding extra perceptual weighting by virtue of the fact that change (of some kind) occurs, and (b) as a pointer to adjacent temporal regions of the signal which are important and should be sampled more densely.
This paper describes a pole-zero (ARMA) modeling of speech using a recursive-least-squares (RLS) fast transversal filter (FTF) algorithm.
This ARMA FTF algorithm can estimate unknown input excitation and the estimated input is used to determined the parameters of the pole-zero model.
This algorithm is derived using geometric projections.
The geometric projection approach gives insight and useful interpretation of various filters that form the algorithm.
We give a performance evaluation of the proposed algorithm by applying to synthetic and natural speech spectral estimations.
This algorithm accurately represents spectral peaks and valleys of speech and requires less computations than RLS lattice filters and ARMA FTF algorithm of Ardalan and Faber (1988).
Additionally, this algorithm can also be applied to other signal processing areas where the input is unknown.
For a large-vocabulary speech-recognition system, such as Dragon Systems' 30,000 word DragonDictate recognizer, an efficient approach to training is to use “phonemes-in-context” (PICs) which are triphones supplemented by a code to describe prepausal lengthening.
Each PIC is in turn represented by a sequence of one to six “phonetic elements” (PELs).
For each phoneme, there may be thousands of different PICs, but there are no more than 63 PELs.
Initially all PICs and PELs are trained from a database of about 16,000 tokens recorded by a reference speaker.
When the recognizer is used by a new speaker, each word that is recognized is immediately used to adapt the PELs in its Markov models.
After about a thousand words have been recognized, most PELs have been adapted to the new speaker, so that even models for words that have not yet been spoken are appropriate for the new speaker.
The recognizer was tested with two texts that differed greatly in vocabulary and style.
Three speakers dictated each text: the reference speaker, a new male speaker and a new female speaker.
After adaptation on 1,500 words, performance for all three speakers was better than the performance for the reference speaker on unadapted models.
With an active vocabulary of 25,000 words, the fraction of words recognized correctly was 86%, with an additional 8% on a “choice list” of eight words.
This paper summerizes tools coming from estimation's theory to evaluate and design experimental device.
Theses tools are applied to the "Whale Anti Collision System" dedicated to localise Sperm Whales and to avoid collision with ships.
Relying on theoretical tools, our approach tries to be as close as possible of reality by using true sperm clicks, realistic measurement noise by allowing sensors and acoustic environmental missknowledges.
Without any acoustic environmental assesment, WaCs system shows to be a good tool for biology study but may not be enougth accurate to be included in a anti-collision network.
In this paper, a new nonlinear predictive control scheme is proposed for a five-link planar under-actuated biped walking robot.
The basic feature in the proposed strategy is to use on-line optimization to update the tracked trajectories in the completely controlled variables (actuated coordinates) in order to enhance the behavior and the stability of the remaining indirectly controlled ones (unactuated coordinates).
The whole framework is illustrated through simulation case-studies.
To attest the efficiency of the proposed scheme, robustness against model uncertainties and ground irregularities are investigated by simulation studies.
The use of the Time-Domain Pitch Synchronous OverLap-Add (TD-PSOLA) algorithm in a Text-To-Speech synthesizer is reviewed.
Its drawbacks are underlined and three conditions on the speech database are examined.
In order to satisfy them, a previously described high quality resynthesis process is developed and enhanced, which makes use of the well-known Multi-Band Excited (MBE) model.
An important by-product of this operation is that optimal Pitch Marking turns out to be automatic.
A temporal interpolation block is finally added.
The resulting Multi-Band Resynthesis Pitch Synchronous OverLap Add (MBR-PSOLA) synthesis algorithm supports spectral interpolation between voiced parts of segments, with virtually no increase in complexity.
In this paper the design of accurate Semi-Continuous Density Hidden Markov Models (SC-HMMs) for acoustic modelling in large vocabulary continuous speech recognition is presented.
Two methods are described to improve drastically the efficiency of the observation likelihood calculations for the SC-HMMs.
First, reduced SC-HMMs are created, where each state does not share all the – gaussian – probability density functions (pdfs) but only those which are important for it.
It is shown how the average number of gaussians per state can be reduced to 70 for a total set of 10 000 gaussians.
Second, a novel scalar selection algorithm is presented reducing to 5% the number of gaussians which have to be calculated on the total set of 10 000, without any degradation in recognition performance.
Furthermore, the concept of tied state context-dependent modelling with phonetic decision trees is adapted to SC-HMMs.
In fact, a node splitting criterion appropriate for SC-HMMs is introduced: it is based on a distance measure between the mixtures of gaussian pdfs as involved in SC-HMM state modelling.
This contrasts with other criteria from literature which are based on simplified pdfs to manage the algorithmic complexity.
On the ARPA Resource Management task, a relative reduction in word error rate of 8% was achieved with the proposed criterion, comparing with two known criteria based on simplified pdfs.
This paper argues that neural networks are good vehicles for automatic speech recognition not simply because they provide non-linear pattern recognition but because their architecture allows the incorporation and exploitation of existing knowledge about speech.
The first part of the paper argues that the definition of the speech recognition problem implies that prior knowledge of linguistic analysis is essential for its solution, and suggests that the currently poor exploitation of such knowledge is a consequence of contemporary pattern recognition architectures.
Criticism is made of the current emphasis on syntctic pattern recognition algorithms operating at the level of the phonetic segment.
The second part of the paper demonstrates that a network architecture for the lexicon provides a mechanism for the incorporation and exploitation of a range of phonological analyses.
Furthermore, through the explicit separation of phonological representations from phonetic ones, there exists the possibility of constructing a front-end phonetic component on purely pattern recognition principles.
Through normalisation of speaker and environment, the phonetic component may be interfaced to the network lexicon to provide a complete recognition architecture which avoids compromise in the exploitation of speech knowledge.
This paper presents the design methodology behind the floating point verification procedure for the Low-Delay Code-Excited Linear Prediction speech coder, recently selected by the CCITT as Recommendation G.728.
This procedure is based on a non bit-exact specification, which is different from previous CCITT speech coder verification procedures.
This approach gives additional freedom for the implementor of the algorithm, and will allow for more efficient implementations on various kinds of hardware.
However, this flexibility also means that different implementations will respond slightly different to verification test signals.
To cope with this, explicit objective measurements of such deviations are used in the verification process.
These measurements are simple weighted and unweighted signal-to-noise ratios (WSNR and SNR).
In addition to the objective measurements, certain restrictions had to be placed on the test sequence design.
In spite of the input restrictions, a set of test sequences giving reasonably good coverage of the LD-CELP algorithm and state space has been found.
Evaluation experiments are reported, showing that these sequences have a satisfactory error detecting capability.
A final discussion concludes that the chosen verification approach is indeed feasible as an implementor s tool.
Among data mining tasks, characterization does not attract much attention from researchers,in comparison to classification.
It seems to us an interesting task, since it does not require négative examples, which may be a strong requirement for real applications.
In this paper. we present a general framework for the characterization of a target set of objects by means of their own properties, but also the properties of objects linked to them.
According to the kinds of objects. various links can be considered.
In the case of geographic databases, spatial relations express links between geographic objects.
We propose some algorithms for mining characterization rides, and we show how they have been applied to a real goography application provided by BRGM.
French towns were very deeply transformed in a short century near 1850-1930.
Old city dwellers no longer recognized the urban life they had discovered.
The young countrymen were confused.
Perhaps integration was made easier by the development of show business.
Laughing and smiling in the city facilitated coexistence, which later became an urban identity.
We propose a model for a statistical representation of the conceptual structure of a restricted subset of spoken natural language.
The model is used for segmenting a sentence into phrases and labeling them with concept relations (or cases).
The model is trained using a corpus of annotated transcribed sentences.
An understanding system is being built around this model, allowing for unconstrained spoken input in a database retrieval task.
The scope of this paper is to give details and results concerning the new language representation model.
To that aim, the model was implemented and tested allowing a text input.
While the model parameters were estimated using 547 training sentences, the results on a test set of 148 sentences showed that almost 97% of the concepts were correctly detected and labeled by the automatic concept labeling procedure; eventually, 65% of the sentences were correctly understood.
Several studies have recently tried to provide artificial agents with cognitive capacities and psychological features.
However these studies focused on procedural approaches, which are difficult to track, and have handled only small parts of the psychological domain.
We present here a generic approach to the implementation of a principle stating that personality traits have an actual influence over the rational decision making process of cognitive agents.
Research on speech and emotion is moving from a period of exploratory research into one where there is a prospect of substantial applications, notably in human–computer interaction.
Progress in the area relies heavily on the development of appropriate databases.
This paper addresses four main issues that need to be considered in developing databases of emotional speech: scope, naturalness, context and descriptors.
The paper shows how the challenge of developing appropriate databases is being addressed in three major recent projects––the Reading–Leeds project, the Belfast project and the CREST–ESP project.
From these and other studies the paper draws together the tools and methods that have been developed, addresses the problems that arise and indicates the future directions for the development of emotional speech databases.
The problem of real-time automatic speech recognition in an adverse environment is addressed.
Though much research has been performed in the area of speech recognition, only limited success has been demonstrated for real-time recognition in noisy stressful environments.
The primary reason for this is that the performance of present day recognition algorithms are predicated on the assumptions of the environmental settings in which the algorithms have been formulated and implemented.
The speech recognition system incorporates direct processing steps to address the effects of additive noise on the speech signal and stress on the speech production system.
Performance evaluations showed an improvement in speech feature representation under stressed speaking conditions, with an average improvement in recognition rate of +17.28% across eleven noisy stressful speaking conditions.
In automatic speech recognition, the signal is usually represented by a set of time sequences of spectral parameters (TSSPs) that model the temporal evolution of the spectral envelope frame-to-frame.
Those sequences are then filtered either to make them more robust to environmental conditions or to compute differential parameters (dynamic features) which enhance discrimination.
In this paper, we apply frequency analysis to TSSPs in order to provide an interpretation framework for the various types of parameter filters used so far.
Thus, the analysis of the average long-term spectrum of the successfully filtered sequences reveals a combined effect of equalization and band selection that provides insights into TSSP filtering.
Also, we show in the paper that, when supplementary differential parameters are not used, the recognition rate can be improved even for clean speech, just by properly filtering the TSSPs.
To support this claim, a number of experimental results are presented, both using whole-word and subword based models.
The empirically optimum filters attenuate the low-pass band and emphasize a higher band so that the peak of the average long-term spectrum of the output of these filters lies at around the average syllable rate of the employed database (≈3 Hz).
First, the classification algorithms can be separated into two main categories: discriminative and model-based approaches.
While, the first approach tries to minimize the first type of error, but cannot deal effectively with outliers, the model-based approaches make the outlier detection possible, but are not sufficiently discriminant.
Thus, we propose to combine these two different approaches in a two-stage classification system embedded in a probabilistic framework.
In the first stage we pre-estimate the posterior probabilities with a model-based approach and we re-estimate only the highest probabilities with appropriate Support Vector Machine (SVM) in the second stage.
Another advantage of this combination is to reduce the principal burden of SVM: the processing time necessary to make a decision.
Finally, the first experiments on the benchmark database MNIST have shown that our dynamic classification process allows to maintain the accuracy of SVMs, while decreasing complexity by a factor 8.7 and making the outlier rejection available.
A method has been proposed for the use of optical processing techniques in the analysis and recognition of speech signals.
It was realized as an optical processor consisting of a Helium-Neon laser, optical lenses, photographic film plates and diffusers.
A personal computer system was further introduced for the total system of optical processing to manage the experimental data.
Since the optical processing method has an inherent advantage for high-speed and parallel processing of two-dimensional patterns, the frequency-time pattern of a one-dimensional signal can be obtained without shifting a window along the time axis and the template matching for speech recognition can be conducted in a short period.
Utterances of vowels and syllables were analyzed using the processor and the results showed a close aggrement with those obtained by the computer simulation.
Nonlinear warping of the time axis, indispensable for spoken word recognition, was shown to be accomplished by controlling the transmittance function of the windowing plate.
Template matching of vowel sounds gave a correct recognition of the Japanese five vowels.
These results indicate the validity of the optical processor.
The use of a liquid crystal plate was also proposed as a windowing plate, and an experiment was conducted on the analysis of vowel sounds.
A near-real-time template matching with non-linear time warping is possible by electrically controlling the liquid crystal plate using the result of template matching as a feedback signal.
For several voiced speech signals, the measurement of the instant of glottal closure was get by using the smoothed pseudo Wigner- Ville distribution and the Wong method.
Both methods give similar results when measurements are easy.
For some difficult cases, for which the Wong method becomes unaccurate, the smoothed pseudo Wigner-Ville distribution can still be used
Our research aims to instrument informal learning activities in the context of museum visits.
Various work, based on semantic characterization of artworks, have been proposed to deliver cultural content in mobility.
However, those systems take little account of the situated nature of museum visits.
We thus propose an artwork and a context model allowing to enrich the classic use of semantic distance with a contextualization step, in order to deliver document better suited to the user and his situation.
The scores returned by support vector machines are often used as a confidence measures in the classification of new examples.
However, there is no theoretical grounds sustaining this practice.
Thus, when classification uncertainty has to be assessed, it is safer to resort to classifiers estimating conditional probabilities of class labels.
Here, we focus on the ambiguity in the vicinity of the boundary decision.
We propose an adaptation of maximum likelihood estimation, instantiated on logistic regression.
The model outputs proper conditional probabilities into a user-defined interval and is less precise elsewhere.
The model is sparse, in the sense that few examples contribute to the solution.
The computational efficiency is thus improved compared to logistic regression.
Furthermore, preliminary experiments show improvements over standard logistic regression with performances similar to support vector machines.
As is well known, the acquisition of speech skills by humans involves the “simultaneous” learning of speech perception and of speech production in an environment of speakers who have already acquired these skills.
By contrast in speech processing by machines, speech recognition and speech synthesis are studied and implemented separately (and different methodologies have been developed for each).
The present paper puts forward a structure for the acquisition of speech by machines, asm, in which both recognition and synthesis are trained “simultaneously” from human training speech.
The structure consists of a synthesis chain in which a synthesiser is driven by a trainable neural network controller from a synthesis state vector and of a recognition chain comprising a trainable neural network recogniser which produces a recogniser state vector.
The recogniser alternately receives training speech from a human speaker and speech from the synthesiser.
A coupled minimisation is set up which trains the recogniser network and the synthesiser state and network necessary to classify or recognise human input speech and to produce synthetic speech which is recognised to be of the same class as the human speech.
The algorithm is demonstrated for the acquisition of steady state vowels and simple isolated words.
This paper deals with decentralized decision making in cooperative and autonomous teams of robots.
Even if DEC-MDPs describe an expressive framework for cooperative multiagent decision making, they suffer from a high complexity and fail to formalize properties of multi-robot missions.
Our purpose is therefore to propose a model that can deal with more complex time and action representations, and to develop an algorithm that efficiently solves large problems.
Thus, the OC-DEC-MDP model is defined and a polynomial algorithm is presented.
Experiments show that our approach can deal with real-world multi-robot decision problems.
Knock is a well-known problem for spark-ignition engine manufacturers.
Knock detection helps achieve the best compromise between increasing engine efficiency, fuel consumption and present requirements with regard to exhaust emission legislation.
The ignition timing is usually controlled so that knock never occurs even with fuel quality changes.
The advantage of a knock detection method is to work within close range of knocking conditions but avoid its occurrence.
The purpose of our study consists in highlighting several knock intensities from block vibration signals provided by an accelerometer.
Our aim is to differentiate three kinds of engine cycles: absence of knock, increasing knock and heavy knock.
The developped diagnostic approach deals with fuzzy pattern recognition.
The method, experimented on a learning set, leads to several diagnoses that cooperate.
This paper introduces ALTO, an interactive-graphic computer system designed to facilitate the development of routing algorithms for transportation vehicles.
This system embodies a “general heuristic”, that is, a set of templates that are instantiated by an expert user with his/her own formulas in order to create specific algorithms.
By this mean, ALTO can be used to reproduce a broad class of routing algorithms already documented in the literature or can be used to devise new resolution strategies for complex problems.
An application for a real mail pick-up problem is presented at the end of the paper, in order to emphasize the flexibility of the system.
In this paper we focus on the adaptation of boosting to grammatical inference.
We aim at improving the performances of state merging algorithms in the presence of noisy data by using, in the update rule, additional information provided by an oracle.
This strategy requires the construction of a new weighting scheme that takes into account the confidence in the labels of the examples.
We prove that our new framework preserves the theoretical properties of boosting.
Using the state merging algorithm RPNI*, we describe an experimental study on various datasets, showing a dramatic improvement ofperformances.
Autoregressive analysis regularisation is considered as a variational problem solved by calculus of variations where the autoregressive polynomial is regarded as a transformation of the unitary complex circle into a parametric closed orientated curve embedded in the complex space.
We proove that the Euler-Lagrange equation of this problem is equivalent to the classical regularized Yule-Walker equation.
Then, this regularization problem is formulated, by an intrinsic geometrical approach, as a geodesic distance minimization with respect to a metric defined by the data fitting criteria.
Then, Calculus of Variations provides, after a recall of complex function curvature definition, a «Mean Curvature Flow» Partial Differential Equation (PDE).
Its discretization by Z transform leads to a PDE acting on the vector of autoregressive parameters.
This second approach allows to set regularization free from the optimization of the additional hyperparameter, classically introduced in the Tikhonov approach, simply by stoping PDE when its evolution speed decreases.
The second advantage lies in the fact that the PDE numerical scheme is naturally adapted for on-line continuous estimation at the rate of data flow.
Extension of the way the previous problem is formulated for the estimation of Cepstrum, whose the associate distance as well as the group delay distance performances are accepted to be very efficient for signal processing applications, shows that the differential cepstrum is exactly identifiable with the Hopf-Cole transform of the autoregressive polynomial and then induces an associate according to Burgers equation with respect to data.
We conclude by using Polya's interpretation of complex function integration by means of vectors field flux and work to illustrate regularization as a process that tends to make non-divergent and non-rotational the conjugate autoregressive vectors field along the unitary complex circle.
This study aims at determining the effect of the orientation of the constraint conditions as a support for making web sites easier to use.
Study results showed that the UCC helped designers, since professionals and novices took into account an important number of user constraints without hindering the consideration of client constraints.
It was not the case for designers dealing with the MCC.
Although these results are encouraging, designers introduced again an important number of usability problems in their web sites.
The paper concludes with a presentation of studies conducted with professional and novice designers and ways to help them make web sites easier to use.
This paper describes a speech spectrum transformation method by interpolating multi-speakers' spectral patterns and multi-functional representation with Radial Basis Function networks.
The interpolation is carried out using spectral parameters between pre-stored multiple speakers' utterance data to generate new spectrum patterns.
The multiple functions' outputs are weighted-summed, using weighting values given by RBF networks.
The parameters of this multi-functional transformation are adapted by the gradient descent method.
Using ten training words, the reduction rate increased to 48% by the multi-functional transformation.
This paper presents a study in the asymmetrical semi-supervised learning framework, where only positive and unlabeled data are available, and an application to a bio-data processing problem.
We show that under very mild assumptions, the Naive Bayes classifier can be identified from positive and unlabeled data.
From this study, we derive algorithms that we experiment on artificial data.
Lastly, we present an application of this work to the problem of the extraction of local affinities in proteins for the prediction of disulfide connectivity.
We consider the automatic classification framework, using a Markov model.
We address the problem of estimating the number of classes and the associated class parameters.
We propose a method using the contextual information inherent in images to discriminate different classes in the case of mixture distributions with strongly mixed classes.
This method is validated theoretically and practically, using synthetical images and real data.
We prove that the proposed method has a validity domain larger than the methods based on a histogram analysis.
We then discuss the shape of the data driven potential induced by the detected classes in a Markovian framework.
Results are obtained by using two priors: the Potts model and the Chien-model.
This article presents an overview of several measures for speaker recognition.
These measures relate to second-order statistical tests, and can be expressed under a common formalism.
Alternative formulations of these measures are given and their mathematical properties are studied.
In their basic form, these measures are asymmetric, but they can be symmetrized in various ways.
All measures are tested in the framework of text-independent closed-set speaker identification, on 3 variants of the TIMIT database (630 speakers): TIMIT (high quality speech), FTIMIT (a restricted bandwidth version of TIMIT) and NTIMIT (telephone quality).
Remarkable performances are obtained on TIMIT but the results naturally deteriorate with FTIMIT and NTIMIT.
Symmetrization appears to be a factor of improvement, especially when little speech material is available.
The use of some of the proposed measures as a reference benchmark to evaluate the intrinsic complexity of a given database under a given protocol is finally suggested as a conclusion to this work.
We present in this paper our works on the classification of industrial parts based on «Support Vector Machine» method.
We present the practical frame in which are made the operations, flaws types to detect as well as feature extraction techniques.
Then we introduce the three classification techniques we implemented.
We explain our learning method and how we obtain optimum classifier parameters.
We compare the results obtained using feature space based on a priori knowledge and on space extracted from sequential selection algorithm.
This contribution deals with how Bar Hebræus borrowed the notion of transitivity from the grammarian of Arabic, Zamaḫšarī, and how he reformulated it within his grammar of Syriac.
I proceed by translating and commenting his text and comparing it with the text by Zamaḫšarī.
His chapter is organised into four sections:
1. First section: concerning examples of intransitive and transitive verbs;
2. Second section: on the causes of transitivity;
3. Third section: concerning the failure of the causes of transitivity;
4. Fourth section: concerning verbs which are both transitive and intransitive.
The difference between the two grammarians is manifest in the final two sections in which it appears that although Bar Hebræus borrowed the concept of transitivity from Zamaḫšarī, his treatment goes far beyond what is found in his source.
Indeed, the only concern of the grammarian of Arabic is to ensure that all the complements are in the accusative and to identify the causes of transitivity.
Finally, Bar Hebræus studies in depth the labile verbs which are both transitive and intransitive.
This paper examines air flow patterns at vowel-consonant and consonant-vowel transitions.
Oral air flow was recorded in six speakers of American English producing reiterant speech.
The air flow signal was inverse filtered to obtain an estimate of the glottal pulse.
Measurements were made of peak and minimum flow, open quotient, pulse area and fundamental frequency.
The results show that at the transitions between vowels and voiceless consonants the pulse properties show large variations.
In particular, the source is characterized by a breathy mode of phonation.
Breathiness was indexed by large values of peak and minimum flow, and an open quotient close to 1.
The observed variations can be accounted for by the laryngeal adjustments that are made for voiceless consonants, in particular the glottal opening movement and its phasing with the oral articulatory events.
Individual differences suggest that speakers vary in their use of the longitudinal tension of the vocal folds in controlling voicelessness.
This paper presents a new matrix formulation of the basic concepts governing discrete Hidden Markov Models (HMM).
Using this formulation, we show that symbol and state probabilities are exponential functions of the transition matrix of the model.
Furthermore, based on the eigenanalysis of the transition matrix, a closed form relationship is derived between the eigenvalues of this matrix and the symbol probabilities at different instants.
The matrix formulation provides a useful tool for the physical interpretation of the learning and decision process through HMM.
A better insight is obtained, and tools are also given for a design with improved learning characteristics.
Secondly, we use a set of linguistic information in the form of reduced models, based on linguistic models of texts.
In this area we aim to evaluate if using linguistic information and analysis can improve the performance of a filtering system.
Indeed, as well as using lexical characteristics, we use a range of indicators based on structure and content of the messages.
This information is independent to the application domain and reliability depends on the learning operation.
In order to evaluate the feasibility of our approach and its reliability, we have experimented with a corpus of1200 messages.
We present here the results of a set of evaluation experiments.
This paper presents some developments in query expansion and document representation of our spoken document retrieval system and shows how various retrieval techniques affect performance for different sets of transcriptions derived from a common speech source.
Modifications of the document representation are used, which combine several techniques for query expansion, knowledge-based on one hand and statistics-based on the other.
Taken together, these techniques can improve Average Precision by over 19% relative to a system similar to that which we presented at TREC-7.
These new experiments have also confirmed that the degradation of Average Precision due to a word error rate (WER) of 25% is quite small (3.7% relative) and can be reduced to almost zero (0.2% relative).
The overall improvement of the retrieval system can also be observed for seven different sets of transcriptions from different recognition engines with a WER ranging from 24.8% to 61.5%.
We hope to repeat these experiments when larger document collections become available, in order to evaluate the scalability of these techniques.
This paper examines the processing of visual information beyond the creation of the early representations.
A fundamental requirement at this level is the capacity to establish visually abstract shape properties and spatial relations.
This capacity plays a major role in object recognition, visually guided manipulation, and more abstract visual thinking.
For the human visual system, the perception of spatial properties and relations that are complex from a computational standpoint nevertheless often appears deceivingly immediate and effortless.
The perception of abstract shape properties and spatial relations raises fundamental difficulties with major implications for the overall processing of visual information.
It will be argued that the computation of spatial relations divides the analysis of visual information into two main stages.
The first is the bottom-up creation of certain representations of the visible environment.
The second stage involves the application of process called 'visual routines' to the representations constructed in the first stage.
These routines can establish properties and relations that cannot be represented explicitly in the initial representations.
Visual routines are composed of sequences of elemental operations.
Routines for different properties and relations share elemental operations.
Using a fixed set of basic operations, the visual system can assemble different routines to extract an unbounded variety of shape properties and spatial relations.
The current state of research on emotion effects on voice and speech is reviewed and issues for future research efforts are discussed.
In particular, it is suggested to use the Brunswikian lens model as a base for research on the vocal communication of emotion.
This approach allows one to model the complete process, including both encoding (expression), transmission, and decoding (impression) of vocal emotion communication.
Special emphasis is placed on the conceptualization and operationalization of the major elements of the model (i.e., the speaker's emotional state, the listener's attribution, and the mediating acoustic cues).
In addition, the advantages and disadvantages of research paradigms for the induction or observation of emotional expression in voice and speech and the experimental manipulation of vocal cues are discussed, using pertinent examples drawn from past and present research.
This article presents an original formalism, the interac-DECPOMDP, in which agents can directly interact.
On the basis of this formalism, this article proposes a decentralized learning algorithm based on a heuristic distribution of rewards during interactions.
Experiments have validated its ability to automatically build collective behaviors.
The presented techniques could then constitute a mean to operationalize self-organization in order to solve problems.
Motion estimation from image sequences is based on two assumptions ; the brightness conservation assumption and the assumption of spatial, temporal or spatio-temporal continuity (i.e. smoothness constraint) of the apparent velocity field.
The latter assumption holds locally, within the objects, but it results in blurring the boundaries between the projections, onto the image plane, of objects undergoing different motions.
These boundaries are called motion discontinuities.
The main subject of this paper is an overview of the existing techniques designed to estimate the apparent velocity fields while preserving the motion discontinuities.
The first part deals with the methods based on an assumption which states that the motion discontinuities spatially coincide with image brightness boundaries.
The second part reports the motion segmentation methods.
These discontinuities are preserved by means of a line process, a robust estimator or within an anisotropic diffusion scheme.
The last part is devoted to the occlusions.
The estimation errors in the occlusion areas are due to the violation of two basic assumptions: the continuity assumption and particularly the conservation assumption.
The major objective pursued with the deinterleaved data in as many distinct flows as emission sources is to enable the analysis of laws governing the parameters describing each radar pulse train.
However, because of the great number of disturbances and the pulse parameter evolution law complexity, the task is extremely difficult.
We show that the sequential Monte-Carlo methods are well suited to produce an adapted solution to this extraction problem, that can also be formulated like a data association problem.
The fusion of the various pulse parameters, on condition that the fusion process is realized by taking in account the model specificities, allows us to extend the algorithm application field to environments where impulse density is important.
Breathiness is used to form linguistic contrasts in some languages, but also characterizes speakers as individuals and, to an extent, gender.
Henton and Bladon (1985) claimed that breathiness diminishes intelligibility.
The experiments described in the present paper used synthetic speech to determine the effect of adding a noise source to a modal voice source and to determine the effects of the different acoustic consequences of breathiness on the intelligibility of isolated words.
No significant effects were found.
This paper describes the main retrieval problems when facing blogs.
Using the classical tf idf vector-space model together with three probabilistic and one statistical language model, we evaluate them using a TREC test-collections composed of 100 topics.
Using two performance measures, we show that ignoring a stemming approach results in a better performance than other indexing strategies (light or Porter's stemmer).
Taking account of the presence of two search words in the retrieved documents may significantly improve the retrieval performance.
This paper, based on three presentations made in 1998 at the RLA2C Workshop in Avignon, discusses the evaluation of speaker recognition systems from several perspectives.
Overall performance results of this evaluation are presented by means of detection error trade-off (DET) curves.
These show the performance trade-off of missed detections and false alarms for each system and the effects on performance of training condition, test segment duration, the speakers' sex and the match or mismatch of training and test handsets.
Several factors that were found to have an impact on performance, including pitch frequency, handset type and noise, are discussed and DET curves showing their effects are presented.
The paper concludes with some perspective on the history of this technology and where it may be going.
In this article, we devise a fidelity criterion for quantifying the degree of distortion introduced by a speech coder.
An original speech and its coded version are transformed from the time-domain to a perceptual-domain using an auditory (cochlear) model.
This perceptual-domain representation provides information pertaining to the probability-of-firings in the neural channels.
The introduced cochlear discrimination information (CDI) measure compares these firing probabilities in an information-theoretic sense.
In essence, it evaluates the cross-entropy of the neural firings for the coded speech with respect to those for the original one.
The performance of this objective measure is compared with subjective evaluation results.
Finally, we provide a rate-distortion analysis by computing the rate-distortion function for speech coding using the Blahut algorithm.
Four state-of-the-art speech coders with rates ranging from 4.8 kbit/s (CELP) to 32 kbit/s (ADPCM) are studied from the view-point of their performances (as assessed by the CDI measure) with respect to the rate-distortion limits.
It is 50 years since Stuart Piggott excavated the prehistoric complex at Cairnpapple.
At that time there were few excavated parallels in Scotland, and interpretation inevitably relied heavily on sites excavated in southern Britain.
Much more locally relevant data are now available and the sequence at Cairnpapple can now be reassessed its regional context.
Piggott identified five Periods, commencing with a stone setting, 'cove' and cremation cemetery of 'Late Neolithic date' around 'c. 2500 B.C.'.
Period II was a henge monument, consisting of a 'circle' of standing stones with ceremonial burials in association, and an encircling ditch with external bank – 'Of Beaker date, probably c. 1700 B.C.'
Period III comprised the primary cairn, containing two cist-burials 'Of Middle Bronze Age date, probably c. 1500 B.C.'
Period IV involved the doubling of the size of the cairn, with two cremated burials in inverted cinerary urns.
Of final Middle Bronze Age or native Late Bronze Age date, probably c. 1000 B.C.
Period V comprised four graves 'possibly Early Iron Age within the first couple of centuries A.D.'
The present paper, using comparable material from elsewhere in Scotland, argues for a revised phasing:
Phase 1, comprises the deposition of earlier Neolithic plain bowl sherds and axehead fragments with a series of hearths.
This is comparable to 'structured deposition' noted on other sites of this period.
Phase 2 involved the construction of the henge – a setting of 24 uprights – probably of timber rather than stone, probably followed by the encircling henge ditch and bank.
The 'cove' is discussed in the context of comparable features in Scotland.
Phase 3 saw the construction of a series of graves, including the monumental 'North Grave', which was probably encased in a cairn.
Piggott's 'Period III' cairn was then built, followed by the 'Period IV' cairn.
The urn burials seem likely to have been inserted into the surface of this mound, which may have covered a burial (since disturbed) on the top of the Period III mound, or may have been a deliberate monumentalising of it.
The four graves identified as Iron Age by Piggott seem more likely to be from the early Christian period.
The reassessment of Piggott's report emphasises the value of the writing of a clear, and sufficiently detailed account.
While no report can be wholly objective it can be seen that Piggott's striving for objectivity led him to write a paper that is of lasting value.
In this paper we describe a probabilistic fusion approach based on entropic criteria, which aims at reducing the combination space by explicitly representing the notions of source redundancy and source complementarity.
This modelling is particularly interesting to optimize the choice of measurements provided by sources in order to combine in multi-sources fusion system.
It is in agreement with the preoccupation to perform efficiently fusion processing and to minimize the hardware resources in information fusion problem.
To answer that, we made a study of the parallelization of the entropy fusion algorithm developed for its parallel implementation in the framework of an application to mobile robotics.
The algorithm specification exhibiting potential parallelism is then implemented on a network of workstations running in mode MIMD-NORMA using the parallel/distributed programming environments SynDEx which support AA-A methodology and PVM which support Hoare's CSP concept.
This article examines the extent to which word recognition is influenced by lexical, syntactic, and semantic contexts in order to contrast predictions made by modular and interactive theories of the architecture of the language comprehension system.
We conclude that there is strong evidence for lexical context effects, mixed evidence for semantic context effects and little evidence for syntactic context effects.
We suggest that top-down feedback effects in comprehension are primarily limited to situations in which there is a well-defined part-whole relationship between the two levels and the set of lower-level units that could receive feedback from a higher level is restricted.
The vowel sub-component of a speaker-independent phoneme classification system will be described.
The architecture of the vowel classifier is based on an ear model followed by a set of Multi-Layered Neural Networks (MLNN).
MLNNs are trained to learn how to recognize articulatory features like the place of articulation and the manner of articulation related to tongue position.
Experiments are performed on 10 English vowels showing a recognition rate higher than 95% on new speakers.
When features are used for recognition, comparable results are obtained for vowels and diphthongs not used for training and pronounced by new speakers.
This suggests that MLNNs suitably fed by the data computed by an ear model have good generalization capabilities over new speakers and new sounds.
Annotating images using a fixed number of concepts is a fundamental task for content based image retrieval and classification.
In practice, several modalities (visual, text...) provide information about the content of images.
We are specifically interested in the tags associated with images, usually resulting from folksonomy, that provide imperfect and partially relevant information.
We propose two tag models bearing such imperfections, one modeling the phenomenon through a threshold which is automatically set according to semantic similarity with visual concepts, and the other improving the coding scheme of bags-of-words, inspired from recent work carried out in image classification.
All this work is validated on several publicly available databases commonly used in the field of image annotation.
The experimental results show that the proposed methods are beyond the state of the art while remaining less computationally expensive.
This paper presents a POMDP approximation method, called RTBSS, which is based on a look-ahead search in order to plan in a real-time dynamic environment.
The basis of our approach is to avoid computing full policies in POMDP problems.
Our approach is especially motivated by real-time environments where the state space is too large to consider traditional offline algorithms.
We then proceed with an online approach to find at each step, the action that maximizes the agent expected utility.
To this end, we present the formalism behind our approach.
Then, we present how the approach was applied on three different environments.
Let us mention finally that this approach was successfully implemented for the RoboCupRescue 2004 international competition where we finished second.
A speech recognition system based on synthetic generation of reference prototypes is described.
The vocabulary and grammar are described in a finite-state phoneme network.
In the transformation from symbolic to spectral representation, reduction rules modify the initial phoneme target values and a coarticulation module inserts interpolated transition states at phoneme boundaries.
The phoneme templates are specified in terms of control parameters to a seriel formant synthesiser.
At each state, a 16-channel filter bank section is computed from the synthesis parameters.
The recognition process uses a time-synchronous dynamic programming technique to find the path in the network that minimises the accumulated spectral distance to the input utterance.
A technique for dynamic adaptation to the speaker's voice source spectrum is performed during recognition.
Without adaptation, the average recognition for ten male speakers was 88% on an isolated-word task using a 26-word vocabulary.
Adding voice source adaptation raised the performance to 96%.
On a vocabulary of 3 connected digits, the adaptation technique improved the recognition rate for six male speakers from 87.7% to 92.8%.
The improvement was largest for subjects with low initial recognition rate, indicating the benefit of the voice source adaptation technique for certain voices.
Changing the voice source model and optimising the adaptation time constant raised the recognition rate further to 96.1%.
Current work is directed towards speaker adaptation of phoneme parameters and modelling of the variability of the parameter dynamics at phoneme boundaries.
The effects of voice onset time (VOT), voice tail (the time between oral closure and voice offset), duration and range of formant transitions, and frication noise intensity on the perception of voicing in Dutch two-obstruent sequences (C 1 C 2) was tested in four separate experiments, employing synthetic stimuli.
The results showed that VOT and voice tail were strong cues, and that frication noise intensity was a weaker cue to the perception of voicing in C 1 C 2 sequences.
Formant transitions did not have a significant effect.
Moreover, the VOT and voice tail data showed that cues from temporally rather distant portions of the acoustic signal are integrated into a perceptual unit, and that a particular cue has a similar effect on the perceived voicing status of both component consonants.
Two experiments on unaided and cued recall of sentences presented in context are reported.
Key nouns in the sentences were arranged to have uniform surface functions, but to vary independently in deep syntactic category and semantic function.
Cued recall for sentences in which the semantic function of actor and recipient coincided with the syntactic function of deep subject and object, respectively, was better than for sentences which did not have this normal semantic-syntactic coincidence.
Unaided recall was not different for the two types of sentences.
Models of sentence processing may have to represent both types of information as available to the language user.
In this communication we apply an Information Retrieval model for the writer identification task.
Queries are handwreitten document images projected on a suitable feature set.
The handwritten document database is indexed according to the vector space model originaly used for textual information.
The approach uses both the image and textual description of handwritten documents.
Identified documents are then processed by the verification stage.
We use a mutual information criterion so as to verify that each identified document can have been written by the writer of the query.
Decision operates using an hypothesis test.
The approcah is evaluated on two different database and proves to be robust to the variability of handwriting.
Perspectives are oriented towards the use of large handwritten document database
We propose a new method to compute an approximate policy of a Dec-POMDP which outperforms state of the art approaches including PBDP and MBDP.
Our approach is based on an estimation of the probability distribution of beliefs reachable for a given horizon.
This estimation is done by simulating the execution of an heuristic policy of the Dec-POMDP.
This probability distribution over beliefs is then used to choose the candidate policy trees for the given horizon using a simple criterion which tries to minimise the error induced by pruning.
A novel acoustic modeling algorithm that generates non-uniform unit HMMs to effectively cope with spectral variations in fluent speech is proposed.
The algorithm is devised for the automatic iterative generation of long-span units for non-uniform modeling.
This generation algorithm is based on an entropy reduction criterion using text data and a maximum likelihood criterion using speech data.
The effectiveness of the non-uniform unit models is confirmed by comparing likelihood values between long-span unit HMMs and conventional phoneme-unit HMMs.
In descriptions of Tibetan grammar it is common to treat -las and -nas together in the discussion of case marking, signaling merely that -las is capable of forming comparisons whereas -nas is not.
Similarly, in the discussion of comparison most authors make no distinction between the suffixes -bas and -las.
A look at a few examples of these three morphemes demonstrates that they have quite distinct syntax and semantics.
This paper describes a development of a spoken dialogue travel guidance system, TARSAN.
TARSAN uses commercial CD-ROM guidebooks as its knowledge source, containing a large amount of travel information.
To deal with this amount of information, a large vocabulary has to be accepted by a speech recognizer without reducing its performance.
Thus, we propose two steps of active/non-active word control methods: (1) a word/grammar prediction strategy, and (2) unknown word re-evaluation algorithm.
The word/grammar prediction strategy dynamically changes a recognition network according to a conversation situation by making use of results retrieved from the CD-ROMs.
This strategy makes users to access almost all data on the CD-ROMs using a small vocabulary speech recognizer.
This algorithm enhances the ability of the word/grammar prediction.
In the experiment without Garbage Models, 80.9% of the utterances were correctly understood.
In the unknown word re-evaluation experiment using the Garbage Models, 86.4% were correctly re-evaluated, while the false alarms of 5% were found.
We combine for Monte-Carlo exploration machine learning atfour different time scales: online regret, through the use of bandit algorithms and Monte-Carlo estimates; transient learning, through the use of rapid action value estimates (RAVE) which are learnt online and used for accelerating the exploration and are thereafter neglected; offline learning, by data mining of datasets of games; use of expert knowledge coming from the old ages as prior information.
The resulting algorithm is stronger than each element separately.
We finally emphasize the exploration-exploitation dilemna in the Monte-Carlo simulations and show great improvements that can be reached with a fine tuning of related constants.
Research in large vocabulary speech recognition has been intensively carried out worldwide, in the past several years, spurred on by advances in algorithms, architectures and hardware.
In the United States, the DARPA community has focused efforts on studying several continuous speech recognition tasks including Naval Resource Management, a 991 word task, ATIS (Air Travel Information System), a speech understanding task with an open vocabulary (in practice on the order of several thousand words) and a natural language component, and Wall Street Journal, a voice dictation task with a vocabulary on the order of 20,000 words.
Although we have learned a great deal about how to build and efficiently implement large vocabulary speech recognition systems, there remain a whole range of fundamental questions for which we have no definitive answers.
Stimuli made as trains of alternating one-formant and two-formant pulses, and of two-formant stimuli with variable A 1/A 2 ratio, were used in vowel identification experiments.
Two main facts were obtained: 1) the increase in proportion of one-formant pulses in a train and the increase in amplitude of the corresponding formant in two-formant stimulus affect the identification in just the same way; 2) large variations in amplitude difference between one-formant and two-formant pulses in the train have no effect on the identification.
Neither the hypothesis of running phonemic classification nor the average spectrum hypothesis is compatible with both of these facts.
It seems possible that simple “single-peak” and “pair of peaks” patterns are used as components in spectrum shape analysis.
A series of experiments is reported in which subjects describe simple visual scenes by means of both sentential and non-sentential responses.
The data support the following statements about the lexicalization (word finding) process.
(1) Words used by speakers in overt naming or sentence production responses are selected by a sequence of two lexical retrieval processes, the first yielding abstract pre-phonological items (L1-items), the second one adding their phonological shapes (L2-items).
(2) The selection of several L1-items for a multi-word utterance can take place simultaneously.
(3) A monitoring process is watching the output of L1-lexicalization to check if it is in keeping with prevailing constraints upon utterance format.
(4) Retrieval of the L2-item which corresponds with a given L1-item waits until the L1-item has been checked by the monitor, and all other L1-items needed for the utterance under construction have become available.
A coherent picture of the lexicalization process begins to emerge when these characteristics are brought together with other empirical results in the area of naming and sentence production, e.g., picture naming reaction times (Seymour, 1979), speech errors (Garrett, 1980), and word order preferences (Bock, 1982).
Early diagnosis is the most efficient way to struggle against cancer.
Among all the existing techniques, optical methods (photodiagnosis from NUV to NIR) show important characteristics required by the physicians: high sensitivity non-ionising radiations and non-traumatic measurements.
They are particularly well suited to the detection of cancers in hollow organs, that are usually superficial and hardly visible with classical endoscopy.
This paper describes a methodological approach based on the use of tissue autofluorescence, applicable in clinical endoscopy, and leading to the definition of diagnosis indicators from the spectral parameters.
Following a state-of-the-art on autofluorescence spectroscopic (LIFS) and endoscopic imaging methods, we present the efficiency of fibered LIFS in terms of sensitivity and specificity for the diagnosis of esophagus cancerous lesions (clinical study over 25 patients).
We then present the technological characteristics of an autofluorescence endoscopic imaging prototype developed in our labs as well as its calibration.
A second part is devoted to endoscopic image registration and mosaicing and to optics aberration correction in perspective of the automatic construction of a panoramic image (cartography) of the organ's explored areas.
Finally, exploiting the fluorescence data provided by the imager, the feasibility of the superimposition of spatial and spectral information is validated with a phantom.
This paper presents a set of rules to predict phoneme durations for synthesis applications in French.
The rules use a speaker-independent Intrinsic Duration for each phoneme and a lengthening/shortening coefficient reflecting the effects of context and speaking style.
The model can thus yield different sets of phoneme durations as produced by different speakers.
The validity of the model was tested on 2 speakers.
For the test-corpora, the mean differences between predicted and measured durations were less than 18 ms.
Zhuang is the current designation for the northern and central Tai languages spoken in Guangxi in southern China.
The implications of this typology for the study of writing systems, and the Chinese writing system in particular, would seem to be considerable.
In the design of low-bit-rate (LBR) speech coding algorithms, language variability is often considered to be of secondary importance in comparison with other operational factors such as speaker variability and noise.
Given that languages differ extensively in the composition of the spectral envelope and that the quantised spectral envelope of speech represents an important part of the bit allocation in speech coding, it is surprising to find that no comprehensive studies have ever been carried out on the role of language in spectral quantisation.
This paper addresses this through a series of performance studies of spectral quantisation carried out across a set of language families typical of global mobile telecommunications.
The study considers factors of quantiser design such as the size and structure of codebooks, and the quantity of monolingual data used in codebook training.
This study found that quantisation distortion is not uniform across languages.
It is shown that a significant difference exists in the behaviour of spectral quantisation across languages, in particular the behaviour of high distortion outliers.
Detailed analysis of the spectral distortion data on a phonetic level revealed that the nature of the distribution of spectral energy in phonemes influenced the behaviour of monolingual codebooks.
Some explanations for codebook performance are presented as well as a set of recommendations for codebook design for multi-lingual environments.
On se souvient de la distinction faite par Guiette entre une poésie «formelle», qui réinvente perpétuellement ses règles, et une poésie «formaliste», qui se contente de remplir des formes fixées d'avance.
The evaluation of processed and synthesized speech is closely related to the auditory perception of complex sounds.
An understanding of the perception of complex sounds is therefore helpful to improve the quality of processed sounds.
The perceptual study of speech sounds in this paper is mainly concerned with auditory masking.
Unlike most such studies, the targets in our experiment are narrowband noise signals and the maskers are wideband harmonic complex sounds.
We show that the detection of targets at low frequencies is mainly determined by the spectral properties of the maskers.
At high frequencies, the detection of targets is predominantly determined by the temporal behaviour of maskers.
The relative contributions of spectral and temporal analysis strongly depend on the fundamental frequency of the masker.
Better temporal resolution is associated with a higher masker level.
A body-worn hearing aid has been developed with the ability to estimate formant frequencies and amplitudes in real time.
These parameters can be used to enhance the output signal by “sharpening” the formant peaks, by “mapping” the amplitudes of the formants onto the available dynamic range of hearing at each frequency, or by resynthesizing a speech signal that is suited to the listener's hearing characteristics.
The aid can also be used in a “frequency response tailoring” mode similar to a conventional hearing aid.
Initial evaluations of the peak sharpening mode produced small improvements in speech perception for three groups of subjects:
(a) Five severely-to-profoundly hearing-impaired people scored 7% higher on average when using the formant-based hearing aid combined with a multiple-electrode cochlear implant compared with their implant and their conventional hearing aid together.
(b) A hearing aid user with a severe hearing loss scored 11% higher with a “peak-sharpened” signal than with his own conventional hearing aid.
(c) Four normally hearing listeners showed a mean improvement of 19% in the perception of vowels and a decrease of 5% for consonants in background noise when the signal was processed.
These preliminary results illustrate some potential effects of formant-based processing.
This paper presents a novel approach to sinusoidal coding of speech which avoids the use of a voicing detector.
The proposed model represents the speech signal as a sum of sinusoids and bandpass random signals and it is denoted hybrid harmonic model in this paper.
The use of two different sets of basis functions increases the robustness of the model since there is no need to switch between techniques tailored to particular classes of sounds.
Sinusoidal basis functions with harmonically related frequencies allow an accurate representation of the quasi-periodic structure of voiced speech but show difficulties in representing unvoiced sounds.
On the other hand, the bandpass random functions are well suited for high quality representation of unvoiced speech sounds, since their bandwidth is larger than the bandwidth of sinusoids.
The amplitudes of both sets of basis functions are simultaneously estimated by a least squares algorithm and the output speech signal is synthesized in the time domain by the superposition of all basis functions multiplied by their amplitudes.
Experimental tests confirm an improved performance of the hybrid model for operation with noise-corrupted input speech, relative to classic sinusoidal models which exhibit a strong dependency on voicing decision.
Finally, the implementation and test of a fully quantized hybrid coder at 4.8 kbit/s is described.
This paper proposes a method to extract the emotional impact of images based on accurate and low level features.
We supposed their accuracy could also implicitly encode high- level interesting or discriminant information for emotional impact extraction.
Using this statement, our tests have been done on a new image database composed of low semantic diversified images.
The complexity of emotion modeling was considered in classification process through psycho-visual tests.
For the nature of the emotion they had the choice between "Negative", "Neutral" and "Positive" and the power ranged from "Low" to "High".
With the nature ofemotions, we made a classification in three classes of emotions.
Spoken word recognition consists of two major component processes.
First, at the prelexical stage, an abstract description of the utterance is generated from the information in the speech signal.
We review evidence which suggests that positive (match) and negative (mismatch) information of both a segmental and a suprasegmental nature is used to constrain this activation and competition process.
We then ask whether, in addition to the necessary influence of the prelexical stage on the lexical stage, there is also feedback from the lexicon to the prelexical level.
In two phonetic categorization experiments, Dutch listeners were asked to label both syllable-initial and syllable-final ambiguous fricatives (e.g., sounds ranging from [f] to [s]) in the word–nonword series maf–mas, and the nonword–word series jaf–jas.
These lexical effects became smaller in listeners' slower responses, even when the listeners were put under pressure to respond as fast as possible.
Our results challenge models of spoken word recognition in which feedback modulates the prelexical analysis of the component sounds of a word whenever that word is heard.
In the scope of gray-level image processing and understanding, thinning is certainly a central shape descriptor for image analysis and pattern recognition.
Enhancement is also an essential tool in facilitating the visual interpretation and understanding of images, especially for noisy and blurry ones.
The lack of general unified frameworks necessitates the investigation of these problems in a coherent fashion, using partial differential equations.
In this paper, we present a method for thinning and enhancing images by using a shock filter derived from our previously work introduced on enhancement.
This new filter incorporates specific diffusion fields and since each such field is characteristic of a given application, it brings a new degree of freedom to the shock filters, in order to address problems of greater practical interests.
Probative results on handwritten documents illustrate the performance and efficiency of our model.
Empiricist heritage and a combinatory conception of linguistic facts intermingle in Saussure and his immediate predecessors and this made it difficult to construct an adequate theory of meaning (Bedeutung), which was mostly understood as resulting from gathering mental representations (Vorstellungen).
As concerns this difficulty, the Saussurean sign appears to practically beg the question.
Some contemporaries however choosed another approach that rather focused on Darstellung (that is to say 'representation' as a symbolic technique, and no more as a cognitive phenomenon).
Although the grammatical phenomena they analysed were very different, these attempts share a common postulate concerning the existence of organizational properties in the sign itself, which provides arguments in favour of sign motivation.
They are however semantic oriented just as the theories focused on Vorstellung, and this feature delimits the scope of such a semiotic.
This work is in the context of the methodological development of a multispecialist architecture called MESSIE.
This paper presents the system specifications for the interpretation of man-made structures in the field of aerial imagery.
The main difficulty of such a system is the knowledge expression necessary for the interpretation: among them, strategy, scene and objects knowledges.
MESSIE is a blackboard architecture organized with four types of knowledge bases that schematically are grouped in two hierarchical levels.
The first level corresponds to the Scene and Strategy level and the second corresponds to the specialists (one for each objet).
Each level works only with certain points of view.
Errors in child speech show that some children initially formulate tense-hopping and subject-auxiliary inversion as copying without deletion.
Other errors suggest that some children may formulate other movement rules as deletion without copying.
A claim about the nature of the language acquisition device is made on the basis of our analysis of these errors: the language acquisition device formulates hypotheses about transformations in terms of basic operations.
Jean Froissart's Melyador belongs to the tradition of courtly chivalric romances.
In the most faithful fashion, this romance stages four large tournaments upon which rest the whole work's architecture and love intrigues.
Studying these conventional pieces shows that they replicate and take on the theme's inherent share of stereotypes, although they also transcend these stereotypes and turn them into a mode of aesthetic renewal.
The stereotypical nature of the tournament episodes is indeed both generalized and reduced to its essence, while it is very stylized to the point of bestowing lyrical and circular dimensions to the episodes.
Thus, in Melyador, the time of the chivalric performance meets up in a very original fashion with the poetical dimension of the work.
Artificial neural networks (ANNs) have found applications in large spectrum of fields.
Satisfactory results are obtained particularly in classification problems.
In speech recognition context, the use of ANNs is hard, this is essentially due to the absence of the temporal aspect in their structure.
On the other hand, assuming a speech recognition task, a word could be recognized and well categorized or recognized and badly categorized ; so the explanation of the decision is very important.
In this paper, we address two limitations of ANNs: the lack of explicit knowledge and the absence of temporal aspect in their implementation.
STN: is a model of a specialized temporal neuron, which includes both symbolic and temporal aspects.
To illustrate the STN utility, we consider a system for speech recognition ; we underline in this paper the explanation aspect of the system.
The use of digital signal processors is not yet commonly widespread despite their obvious advantages.
In this paper we present a certain number of ideas helping ease their use.
These ideas have been put to work by conceiving the architecture of a processor which is both optimal and easy to use.
It is our belief that speech recognition algorithms can best be handled through the effective an efficient cooperation of multiple knowledge sources.
For the development of various types of such algorithms, also called hybrid speech algorithms and, more generally, for speech processing, we need some advanced architectures and speech processing environments.
There is also a need to manipulate speech knowledge, through the use of abstract data structures, to process data bases, and to help the modeling, simulation, and evaluation of automatic speech recognizers.
To tackle these problems, we propose the new concept of an artificial laboratory for speech processing.
Such a system simulates a real laboratory and allows analysis of data.
It also provides a large range of computing facilities which can be used with ease to perform modeling and simulation.
In this correspondence, the main concepts of the system are briefly described.
Ensemble methods have been successfully used as a classification scheme.
The reduction of the complexity of this popular learning paradigm motivated the appearance of ensemble pruning algorithms.
This paper presents a new efficient ensemble pruning method which not only highly reduces the complexity of ensemble methods but also performs better than the non-pruned version in terms of classification accuracy.
This algorithm consists in ordering all the base classifiers with respect to their entropy which exploits a new version of the margin of ensemble methods.
Confrontation with both the naive approach of randomly pruning base classifiers and another ordered-based pruning algorithm turned out convincing in an extensive empirical analysis.
In earlier research the perception of voicing in Dutch two-obstruent sequences (C1C2) was shown to be affected by each of the following parameters: closure duration of the second consonant in the sequence, voice onset time (VOT), voice termination time (VTT — the period between oral closure and voice offset), duration of the preceding vowel resonance, position of stress, and frication noise intensity.
The aim of the experiment reported here was twofold: to establish whether the effects of the six cues were additive or interactive and to assess the relative perceptual importance of the six cues.
The results (measured in terms of voiced-voiced, voiceless-voiced, voiceless-voiceless, and voiced-voiceless responses) indicated that the effects of the parameters are additive and that, although presence/absence of periodicity (VOT and VTT) is the most important determinant of perceived voicing, perception is also to a large extent affected by “C2”-duration and “preceding vowel” duration.
Middle Indian languages belong to the same linguistic family as Sanskrit.
But their grammarians offer a surprising contrast: literary Prakrits are described by grammarians who use Sanskrit, the most famous prescriptive model, which is thus extended.
Pali, on the other hand, the language of Theravāda Buddhist scriptures, is described in grammars that make use of Pali.
Possible reasons for this difference are considered here.
Is the choice of Pali more than a superficial difference?
Does the choice of Sanskrit prevent from taking into account features of linguistic reality?
An attempt is made to answer these questions through the instance of verb-description and the treatment of the verb-root in grammars of the Middle Indian languages.
How do the grammarians negotiate between the powerful Sanskrit model and the reality of verbal paradigms that tend to be based on the present stem and formed on a regular basis?
Four classification techniques - nearest neighbour, Gaussian mixtures, multi-layer perceptron and Kohonen self-organizing networks - are run on a simplified “real-world” problem and the results discussed.
The problem chosen is the identification of utterances of “yes” and “no” from many speakers, represented as simple vectors.
The results show that the multi-layer perceptron outperforms the other methods, and has a low runtime complexity.
This paper presents high performance speaker identification and verification systems based on Gaussian mixture speaker models: robust, statistically based representations of speaker identity.
The identification system is a maximum likelihood classifier and the verification system is a likelihood ratio hypothesis tester using background speaker normalization.
The systems are evaluated on four publically available speech databases: TIMIT, NTIMIT, Switchboard and YOHO.
The different levels of degradations and variabilities found in these databases allow the examination of system performance for different task domains.
Constraints on the speech range from vocabulary-dependent to extemporaneous and speech quality varies from near-ideal, clean speech to noisy, telephone speech.
Closed set identification accuracies on the 630 speaker TIMIT and NTIMIT databases were 99.5% and 60.7%, respectively.
On a 113 speaker population from the Switchboard database the identification accuracy was 82.8%.
Global threshold equal error rates of 0.24%, 7.19%, 5.15% and 0.51% were obtained in verification experiments on the TIMIT, NTIMIT, Switchboard and YOHO databases, respectively.
The approach exploits the ability of neural networks for non-linear projection of multidimensional data, and their advantages over traditional methods.
An updating rule for this network, based on the Conjugate Gradient Algorithm is used. The main advantage of this algorithm is the speedup of the convergence rate.
This paper outlines a modeling technique for digital images which relies on Markov random fields proposed by Pickard for the purpose of representing fuzzy contextual concepts such as “the uniformity of a region” or “the continuity of a contour”.
We develop a maximum likelihood estimation technique which is a straightforward generalization of an approach which is used quite extensively in speech recognition circles.
Links are needed to bridge the gap between the analysis of speech as a set of discrete, ordered but durationless linguistic unit and analyses of the continuously changing acoustic signals, defined along a time axis.
Current recognition and synthesis devices do not make good use of the structure imposed by speech production processes on the mapping between an allophone sequence and the many possible associated speech signals.
A quantitative, flexible articulatory time framework has been developed as a contribution to the new kinds of phonetic descriptions needed.
Units of articulation for allophones of the phonemes of British English and methods for linking adjacent allophones are proposed.
Tentative specifications for a sub-set are offered, based on a review of published findings for natural speech.
Articulatory schemes are taken to be organised with reference to particular events
The two events may relate to inter-articulator coordination between two different quasi-independent articulators or to the durational extent of a statically maintained state for a single articulator.
The coordination between the two events is expressed through the duration D of the time interval between them.
Six examples are given of the construction of a complete articulatory time plan for an English sequence.
This forms the first stage for a computer-implemented model of the articulatory, aerodynamic and acoustic processes of speech production.
The synthetic speech output from the model is given acoustic variations intended to mimic those arising in natural speech due to a speaker's choice of options, including a change in rate of speech.
This is achieved in the modelling by altering one or more D values in the articulatory time plan and by dispensing with some optional actions.
The variability of multiple repetitions by a real speaker can be introduced into the synthetic speech by perturbing the D values.
The model needs to be matched to specific real speakers in order to assess the extent to which it is realistic in its simulation of the variation and variability of acoustic pattern features for natural speech and the extent to which covariations can be predicted with it.
This paper explores whether a speaker's voice quality, defined as the perceived timbre of someone's speech, changes as a function of variation in speech melody.
Analyses are based on several productions of the vowel `a', provided with different intonation patterns.
It appears that in general fundamental frequency covaries with the `strength relationship' between the first two harmonics (H1–H2).
That relationship determines the voice quality to some extent, and is often claimed to reflect open quotient.
However, correlating the H1–H2 measure to parameters of the LF-model reveals that both the open quotient and the skewness of the glottal pulse have an impact on the lower part of the harmonic spectrum.
The objective of our current experiments is to examine the effect of duration on loudness in CV syllables.
A natural [ta] was manipulated with the aid of a computer so as to generate a reference stimulus of 250 ms at 70 dB SPL and a series of test stimuli with duration of 75, 100, 150, 200, and 250 ms.
The stimulus interval was 200 ms in experiment I, and 600 ms in experiment II.
A computer-assisted method of adjustment was set up.
The measured level difference in dB between the reference stimulus and the adjusted test stimulus is less than 2 dB for the shortest duration, and is equal to 0.2 dB for the longest one.
In experiment III, the Up-Down-Transformed-Response procedure was used with the same stimuli as in experiment II.
Resulting level differences values, when plotted, had the same loudness contour as in experiment I and II, but were slightly higher.
A new algorithm for the extraction of the fundamental frequency time-varying information is described.
The algorithm is based on the iterative use of a linear filter with zero phase and monotonically decreasing frequency response (low pass).
The results show that the method is both efficient and robust in noisy environments, providing an estimate for the locations of the closure and opening of the vocal chords.
In this paper, we present a novel approach to text mining that helps to build intelligent user interfaces for recommender and information retrieval systems.
The main problem for the user in information retrieval is that he must have almost perfect knowledge of the domain and the domain terminology.
Our approach eases this burden by showing a way how to encode domain knowledge so that an information retrieval system can transform the user's way to talk about the domain in the expert's way to do that.
After that transformation the system can search its data bases for appropriate information.
We demonstrate the practicability of our approach in a case study on a TV recommender system.
This critical operation consists of extracting most of the redundant information present in the signal, to produce a reduced set of symbols.
These compressed symbols being extremely vulnerable to errors, which are likely to occur on the physical channel, the channel encoder is present to protect the compressed information stream and guarantee a sufficient level of quality to allow the reconstruction of the signal by the source decoder.
These contents are characterized by a large redundancy in the data stream.
Different types of redundancy can be distinguished depending on the type of the considered content: spatial, temporal or statistical redundancy [170], and two different classes of source coders can then be employed.
More details on these source coding techniques and relative standards, as well as of channel coding principle and the corresponding Shannon theorems are presented in Section 2.
Robust joint source channel coding techniques, aim at modifying the encoder in order to introduce redundancy in the coded binary stream [49], to include in particular the addition of markers [106], unequal error protection solutions [105][78][126][21], or even a more general cross-layer approach [42][127].
Biometrics refers to technologies for measuring and analysing human body characteristics in person authentication applications.
The computational power available in today's computer and embedded systems (e.g. mobile phones, l aptop and personal digital assistant) allows the biometrics market to grow with the aim of replacing PIN codes or taped password in control access.
Among the different biometric technologies that have emerged in the last decade, automatic iris verification systems are recognised as the most reliable.
This is followed by a software implementation of the proposed EMD-based iris images processing on ARM920T core-module which demonstrates the feasibility of embedding the iris technology on future multimedia mobile platform.
To decrease the hazards of using mobile phones while driving, voice processing provides several tools that simplify their use: echo cancellation allows comfortable hands-free conversation, feedback and user guidance by voice allow to operate the phone in eyes-busy situations, and last not least speech recognition frees from keypad data entry to operate the telephone.
A comprehensive view of a device incorporating the above mentioned technologies, which has been realized as an add-on for the Philips car telephone family, will be presented.
Emphasis is placed on the speech recognition algorithms.
Robustness of the algorithms to changing acoustic environment was improved by estimating and subtracting the long-term spectrum.
We will show that, if this operation is done recursively, it is equivalent to the high-pass filtering or RASTA (Relative Spectral Approaches) methods recently proposed in the literature.
The technologies of ISDN teleconferencing, CD-ROM multimedia services, and High Definition Television are creating new opportunities and challenges for the digital coding of wideband audio signals, wideband speech in particular.
In the coding of wideband speech, an important point of reference is the CCITT standard for 7 kHz speech at a rate of 64 kbit/s.
Results of recent research are pointing to better capabilities — higher signal bandwidth at 64 kbit/s, and 7 kHz bandwidth at lower bit-rates such as 32 and 16 kbit/s.
The coding of audio with a signal bandwidth of 20 kHz is receiving significant attention due to recent activity in the ISO (International Standards Organization), with a goal of storing a CD-grade monophonic audio channel at a bit-rate not exceeding 128 kbit/s.
Prospects for accomplishing this are very good.
As a side result, emerging algorithms will offer very attractive options at lower rates such as 96 and 64 kbit/s.
The study described here investigates the perceived emotional content of “affect bursts” for German.
Affect bursts are defined as short emotional non-speech expressions.
This study shows that affect bursts, presented without context, can convey a clearly identifiable emotional meaning.
The influence of the segmental structure on emotion recognition, as opposed to prosody and voice quality, is investigated.
Agreement between transcribers is used as an experimental criterion for distinguishing between reflexive raw affect bursts and conventionalised affect emblems.
A detailed account of 28 affect burst classes is given, including perceived emotion and recognition rate in listening and reading perception tests as well as a phonetic transcription of segmental structure, voice quality and intonation.
In placing the attempts to teach language to nonhuman species in both a cultural and philosophical context, we consider evolutionary claims about the origins of human language, and Quine's indeterminacy of translation thesis.
In contrasting the natural language acquired by humans with the artificial language taught nonhumans, we propose treating the human mind as a blend of learning, hard-wiring and cognition.
We discuss the nature of each of the components, suggest how they may interact, and compare the three-component human system with the two-component system of other species.
We recently proposed an input-output model of the glottal pulse.
Mathematically speaking, the pulse is broken down into a cosinusoidal input signal and a pair of nonlinear shaping functions.
The pulse is recovered when the cosinusoid is put through the shapers.
In this article, it is shown that the cycles of a speaker's glottal waveform can be synthesized with the shaping functions of a small number of reference cycles.
Indeed, nonlinear systems are not described by a transfer function.
Therefore, it may be assumed that the nonlinear shaping functions of a glottal pulse are less variable than the shape of the pulse itself.
Two experiments were carried out to test this assumption.
In a first, the output static waveforms from a two-mass model of the vocal folds were copied.
In a second, the glottis signal that was obtained from a logatome [ama] spoken by a male speaker was analyzed and synthesized.
Each pulse was characterized by its peak amplitude, period and form factor.
In both experiments, the features of all the glottal pulses could be copied by calculating the shaper coefficients of just two reference pulse and by adjusting the control parameters of the driving cosinusoid till the output of the shaper exhibited the desired feature values.
An object-oriented analysis-synthesis coder is presented which encodes objects instead of blocks of N × N picture elements.
The objects are described by three parameter sets defining the motion, shape and colour of an object.
The parameter sets are obtained by image analysis based on source models of either moving 2D-objects or moving 3D-objects.
Known coding techniques are used to encode the parameter sets.
An object-depending parameter coding allows to introduce geometrical distortions instead of quantization errors.
Using the transmitted parameter sets an image can be reconstructed by model-based image synthesis.
Experimental results achieved with a first implementation of the coder are given and are discussed.
We propose a statistical dialogue modeling method based on the information theory and the speech act theory.
The dialogue model consists of a trigram of utterances classified by their speech act.
It can be used to rule out erroneous speech recognition candidates that are syntactically and semantically correct, but contextually incorrect, by examining whether the utterance candidates form a natural local discourse in terms of speech act sequencing.
Since it is based on the information theory, we can define objective measures for the quality of the dialogue model, such as discourse perplexity.
We show that the dialogue model can predict the speech act type of the next utterance by experiments on 100 keyboard dialogues, that include 2,722 utterances and 38,954 words.
It achieves 39.7% prediction accuracy for the top candidate and 61.7% for the top three candidates, when 90 dialogues were used for training and the remaining 10 dialogues were used for testing.
We also show that we can make a better language model by combining the dialogue model with a sentence model.
The word perplexity of word bigram with speech act type trigram is 7.27, while that of simple word bigram is 11.6, when the word perplexity of the language models is computed using the 100 keyboard dialogues.
In order to construct a robust natural language processing system, it is necessary to incorporate dialogue management.
The design of an appropriate manager requires a well-founded understanding of human-human conversation.
This paper presents an attempt to articulate the notion of conversation as collaboration in light of this need.
The argument that conversation is inherently collaborative is made based on syntactic phenomena from spontaneous English conversation.
In this paper, “collaboration” implies the simultaneous co-production of a conversation by the participants involved, not merely the construction of conversational meaning through alternating discrete contributions made by conversants.
The syntactic structures discussed in support of this view are list structures, echo questions, short answers, joint productions and what are here called “parallel structures” and “accomodations”.
Suggestions are made concerning the impact of this view on conversation analysis and on the design of human-machine interfaces.
The paper is concerned with the statistical assessment of the description of contingency tables by induction trees.
It focuses on the special case ofcategorical data.
Three topics are successively considered.
i) The nature of the fit in supervised learning where we stress the distinction between fitting individual values andfitting their cross-tabulated synthetic representation.
ii) The description of contingency tables provided by induction trees which is compared with the log-linear modeling used in statistics.
iii) The adaptation of the goodness-of-fit measures and statistics used in log-linear modeling to the case ofinduction trees.
The discussion is completed with an application to the Titanic data set.
A representation of the speech signal as a sum of elementary waveforms (Elementary Waveform Speech Model or EWSM) is introduced and some of its features for modifying localized time-frequency events are demonstrated.
The elementary waveforms model the local spectro-temporal maxima of energy within the speech signal thanks to the use of simple mathematical functions.
An automatic analysis-synthesis system allows for waveforms parameters estimation, using frame-by-frame processing: spectral modelling and segmentation using short-time Fourier transform and LPC spectrum, Fourier filtering according to this segmentation, waveform spotting in each channel, waveform modelling using simple functions.
The classical theory of speech production proves the validity of the EWSM parameters; their modifications yield well-localized time-frequency transformations, including frequency compression/expansion, pitch, formant and noise modification.
The educational system and the Académie française, a specialized institution, are part of this third role that relies on texts (government circulars, programmes) defining this organizational action.
On these three levels, the state's implication in the production of linguistic norms remains indirect and scarce ―despite resistant myths to the contrary.
The above brings out an ideological component focusing on the concept of legitimacy, along with the ideas of consensus, norm and normalization, linguistic insecurity, etc.
These debates concern the nature of the state and the cohesion of the political body.
We propose, in this paper, an original approach in a statistical framework, for fully automatic delineation of kidneys (healthy and pathological) in 2D CT images.
Our approach has two main steps: a localisation step followed by a delineation step.
The localisation step is guided by a statistically learned prior spatial model in one hand and a grey level prior model in a second hand.
The second step, utilizes the localisation results in order to precisely delineate the kidney's regions using a set of learned IF-THEN rules.
The proposed approach is tested on clinically acquired images and promising results are obtained.
This is an overview of some recent studies of voice source acoustics and glottal flow analysis and modelling performed at the KTH.
Time and frequency domain aspects of the production process are discussed with a view of relating glottal flow parameters from inverse filtering and vocal tract transfer functions to formant amplitudes and bandwidths.
Alternative methods of determining the time constant Ta = 1 (2πFa) in the return phase of glottal flow derivative after the instant of excitation, and thus of spectral tilt, are discussed.
Selective inverse filtering, removing all but one formant, is potentially useful for this purpose.
The influence of uncertainties in quantifying the vocal tract transfer function is exemplified by a calculation of the effects of introducing a finite baffle effect of the human head adding a high-frequency emphasis above the standard + 6 dB/octave.
Particular attention has been paid to temporal variations within an utterance as derived from continuous inverse filtering.
Aspects of breathy voicing and female-male differences in voice production are discussed.
It is demonstrated that the temporal profile of the excitation amplitude, E e (t), within an utterance derived from a male speaker can be approximated by the envelope of the negative part of the speech wave.
This work investigates some acoustic and perceptual characteristics of focal and postfocal accents in questions of Neapolitan Italian.
In this variety, yes/no question pitch accents are characterized by a rise–fall configuration, with a very conspicuous peak (L*+H).
When intended focus is early, a postfocal accent is produced, which aligns with the last stressed syllable of the intonation phrase (!H*).
Results from a perception study suggest that the postfocal!H* is not the nuclear accent of the intonation phrase, despite being final.
The phonetic and phonological nature of the focal L*+H and the postfocal!H* are also investigated in production through a set of yes/no questions varying in intended focus scope and focus placement.
The results of this study support the hypothesis that focal and postfocal accents are structurally different, in that postfocal accents are acoustically much reduced.
Finally, we explore the temporal alignment and melodic values of the initial rise and final fall in focus constituents varying in size.
The results suggest an effect of “tonal repulsion” (Silverman and Pierrehumbert, The timing of prenuclear high accents in English, in: J. Kingston, M.E. Beckman (Eds.), Papers in Laboratory Phonology: Between the Grammar and the Physics of Speech, Cambridge University, Cambridge, 1990, pp. 71–106) on the temporal location of the L*+H peak as well as “seeming truncation” of the focus constituent final fall in one-word focus constituents.
Multi agent based simulations (MABS) have been successfully exploited to model complex systems in different areas.
Nevertheless a pitfall of MABS is that their complexity increases with the number of agents and the number of different types of behaviours considered in the model.
For average and large systems it is impossible to validate the trajectories of single agents in a simulation.
The classical validation approaches, where only global indicators are evaluated, are too simplistic to give enough confidence on the simulation's model.
It is then necessary to introduce intermediate levels of validation.
In this paper we propose the use of data clustering and automated characterization of clusters in order to build, describe and follow the evolution of groups of agents in simulations.
The description of clusters is used to generate profiles of agents that are reintroduced in simulations in order to study the stability of the descriptions and structures of clusters over several simulations and decide their capability to describe the modelled phenomena.
These tools provides the modeller with an intermediate point of view on the evolution of the model.
They are flexible enough to be applied both offline and online, and we illustrate it with both a NetLogo and a CSV-simulation log example.
Dominant models of auditory word recognition emphasize the lexical access component of the word identification problem.
They thus cast the recognition process as a simple operation matching the input against stored lexical representations or direct activation of those representations.
Given this characterization of the problem, it is entirely unclear how the perceiver's knowledge of language structure could facilitate the recognition of words.
Yet it may be necessary to appeal to grammatical knowledge to solve many of the outstanding problems in theories of word recognition, e.g., segmentation, coping with variation in the acoustic instantiation of words, and recognizing novel words.
The current paper takes seriously the possibility that grammatical knowledge participates in word recognition.
It investigates what kinds of information would be helpful for what purposes.
It also attempts to sketch in the outlines of the general sort of recognition system which could take advantage of the kinds of regularities found in natural languages.
We present a flexible environment for the generation of word hypotheses in continuous speech.
After describing the interface to the other modules of our speech understanding system a verification algorithm and a word spotting technique both based on HMM will be discussed.
The generation of reference models for the matching procedures is done automatically using the standard pronunciation of a word and a set of phonological rules about intra word assimilation.
These alternative pronunciations are represented by graphs with labeled edges.
Some preliminary results for the matching procedures are also given.
An investigation into the use of Bayesian learning of the parameters of a multivariate Gaussian mixture density has been carried out.
In a framework of continuous density hidden Markov model (CDHMM), Bayesian learning serves as a unified approach for parameter smoothing, speaker adaptation, speaker clustering and corrective training.
The goal is to enhance model robustness in a CDHMM-based speech recognition system so as to improve performance.
Our approach is to use Bayesian learning to incorporate prior knowledge into the training process in the form of prior densities of the HMM parameters.
The theoretical basis for this procedure is presented and results applying it to parameter smoothing, speaker adaptation, speaker clustering and corrective training are given.
In the present study 20 Dutch male speakers were asked to read aloud 47 test words in a word list and in short sentences.
Part of this word set was also named by them through the presentation of pictures.
A group of 20 listeners was asked to identify an unstressed vowel in all of these test words.
The vowel responses of listeners were recoded into two broad categories: “full vowel” and “schwa”.
Our aims were (1) to find out to what extent listeners are able to unambiguously distinguish between these two categories, (2) to investigate the influence of the frequency of occurrence of words on the classification of the test vowels, (3) to investigate the influence of speaking styles on the classification of the test vowels by comparing the speech conditions “word list”, “pictures” and “sentences”.
The experimental results showed that (1) listeners often could not unambiguously classify the test vowels, especially if these occurred in interstress position, (2) the number of schwa responses was much higher for vowels in words with a relatively high frequency of occurrence, (3) the number of schwa responses increased in a more casual speaking style.
Acoustic measurements on the test vowels revealed a clear relation between the perceptual results and the acoustic features of the vowels.
Although the preconditions for the sound change “full vowel → schwa” in several Dutch words are excellent, the actual completion of the sound change is in our view to a large extent blocked by the rather close correspondence between Dutch vowel sounds and their orthographic representations.
Recently, a number of researchers reported quantitative results about the acoustic changes between normal and Lombard speech.
These results highlighted that the nature of the Lombard reflex is highly speaker-dependent.
In this paper, after briefly discussing the influence of acoustics on speech production, we summarize some important characteristics of the Lombard reflex.
Then, we review some experimental results showing how the Lombard reflex varies with the speaker gender, the language, and the environment (type of noise).
Finally, we briefly discuss the use of relational features as a way to reduce the influence of the Lombard reflex on automatic speech recognizers.
In this paper we apply a study of the structure of the English language towards an automatic syllabification algorithm and consequently an automatic foreign accent identification system.
Any word consists of syllables which can in turn be divided into its constituents.
Elements within the syllable structure are defined according to both their position within the syllable and the position of the syllable within the word structure.
Elements of syllable structure that only occur at morpheme boundaries or that extend for the duration of morphemes are identified as peripheral elements; those that can occur anywhere with regard to word morphology are identified as core elements.
All languages potentially make a distinction between core and peripheral elements of their syllable structure, however the specific forms these structures take will vary from language to language.
In addition to problems posed by differences in phoneme inventories (a detailed analysis of comparative phoneme inventories across the languages treated here is outside the scope of this paper), we expect speakers with the greatest syllable structural differences between native and foreign language to have greatest difficulty with pronunciation in the foreign language.
In this paper, we will analyze two accents of Australian English: Arabic whose core/periphery structure is similar to English and Vietnamese, whose structure is maximally different to English.
We propose a new approach to pheneme-based continuous speech recognition when a time function of plausibility of observing each phoneme is given.
We introduce a criterion for best sentence, related to the sum of plausibilities of individual symbols composing the sentence.
Based on the idea of making use of a high plausibility region to reduce the computation load while keeping optimality, our method finds the most plausible sentences relating to the input speech, given the plausibility, μ a,n of observing each phoneme a at each time slot n.
Two optimization procedures are defined to deal with the following embedded search processes: (1) find the best path connecting peaks of the plausibility functions of two successive symbols, and (2) find the best time transition slot index for two given peaks.
Dynamic programming is used in these two procedures.
Since the best path finding algorithm does not search slot by slot, the recognition is highly efficient.
Experimental results with the VINICS system show that the method gives a better recognition precision while requiring about 1/20 computing time, compared to traditional DP based methods.
The experimental system obtained a 95% sentence recognition rate on a speaker-dependent test.
We present a new search algorithm for very large vocabulary continuous speech recognition.
Continuous speech recognition with this algorithm is only about 10 times more computationally expensive than isolated word recognition.
We report preliminary recognition results obtained by testing our recognizer on books on tape using a 60 000 word dictionary.
Based on the covariance method, we have developed an analysis/synthesis system which is capable of independent manipulation of the formant frequencies and bandwidths for voiced speech.
The analysis is performed pitch synchronously and is based on the local minimum of the normalized squared error.
Once the formant frequencies and their bandwidths have been estimated, modifications are performed by altering the predictor coefficients, so that the modified formants and/or bandwidths are the solution to the new polynomial equation.
This system has applications in voice modification and speech perception as a tool for investigating voice quality and personality.
Since a few years, neural networks analysis rouses great interests.
According to this approach, the study of postulated functions in the nervous system demands some powerful simulation tools.
Taking inspiration from general features of signals processing and from present tendances toward parallelism in computer architecture, we propose an efficient array processor architecture for recursive adaptive networks analysis and more generally for data (signal, image) analysis: it's the processor named CRAS Y (a systolique calculator for adaptive networks).
We present in this paper a fast classification operator suitable for image processing, the performances of this operator as well as its implementation in the form of an ASIC.
In image segmentation and classification in view of defect detection, it is often impossible to find a reduced set of pertinent characteristic parameters which allows to distinguish the classes.
We propose herein a geometric classification method by stress polytop training which allows the use of a great number of parameters and ensures a high decision speed.
The decision operator associated with the classification has been implemented in Standard Cell and Full Custom.
In this paper, an image segmentation algorithm based on credal labelling is presented.
The main contribution of this work lies in the way in which the images are modelled by belief functions in order to represented uncertainty inherent in the labelling of a voxel to a class.
For each voxel, the basic belief assignment is derived from intrinsic features of the regions in the image.
In order to control the uncertainty in the labelling step, a decision threshold is decreased in a progressive way throughout an iterative process until its stabilization.
The methodology is applied for volumes segmentation on computed tomography images.
This study explores the intellectual legacy of the Kūfan jurist Muḥammad b. Sulaymān al-Kūfī (d. early 4th/10th century) who eventually settled in Yemen and was part of the intellectual circle surrounding al-Hādī ilā l-Ḥaqq Yaḥyā b. al-Ḥusayn (d. 298/911).
It offers an analysis of his most important work, Kitāb al-–Muntaḫab, through a singular ritual law case study that focuses on the use of the basmala in the daily prayer.
The conclusion points towards the Muntaḫab's value as a possible conduit for accessing a stream of Kūfan jurisprudence which has not survived into the modern period.
Advances in transducer technology, signal processing and computing make possible high-quality sound capture from designated spatial volumes under adverse acoustic conditions.
The techniques of multiple beamforming and matched filtering are applied to two- and three-dimensional arrays of sensors.
Array performance is assessed in a preliminary way from computer simulations of rooms and from image characterization of the multipath environment.
The results suggest that high-quality signals can be retrieved from spatially-selected volumes in severely reverberant enclosures.
Reciprocally, the same techniques can be applied to spatially-selecteve sound projection.
Grammatical and semantic constraints are effective for interpreting or understanding linguistic expressions.
However, they appear to be inadequate for selecting among several candidates, all of which may be relatively correct or inadequate grammatically or semantically.
Clearly, we humans interpret a linguistic expression contextually even if there are many potential interpretations.
This method performs calculations for similarity scores between linguistic expressions and for likelihood scores to select the most suitable expression.
Both illocutionary force-based and morpho-syntactic classifications are considered, along with the frequencies of existing sets of neighboring linguistic expressions stored in an example database.
An experimental processing unit which performs such local context analysis has been implemented in a bidirectional (English and Japanese) translation prototype system, and has shown its applicability to the selection of context-dependent translation candidates.
This local context analysis mechanism can be used with conventional translation systems without contextual processing to raise translation accuracy.
In this article, we show how the issue of grammar acquisition can be approached from the standpoint of learning heuristic rules of the language under consideration.
We describe our GASRIA system consisting of: An inductive learning module for grammar inference based on a novel method capable of parsing sentences not parsable by existing methods, called Partial Parsing Algorithm (PPA), a first-order logic environment, a knowledge base (KB) consisting of a rule base using variables.
We present a new 3D interactive method for visualizing multimedia data (numeric, symbolic, sounds, images, videos, Web sites) with Virtual reality.
We use a 3D stereoscopic display in order to let the expert easily visualize and perceive the numerical attributes.
Navigating through the data is done with a 3D sensor with six degrees of freedom that simulates a virtual caméra.
Interactive request can be formulated by the expert with a data glove that recognizes the gestures.
We show how this tool bas been applied to a real world application in the context of human skin analysis.
A single-board on-line system for speaker-independent isolated word recognition is described.
The system consists of preprocessing hardware which is a simplified model of peripheral auditory processing, and a microprocessor system.
By several tests the influence of digital word length, dynamic range and filter channel configuration on recognition performance was explored.
The results indicate that a dynamic range of 30 dB seems to be adequate and three or four bits per channel are sufficient to encode the amplitude information.
In addition, the influence of varying the filter parameters was investigated.
We present a new method for recognising handwritten characters based on pseudo-ID Markov models.
This method belongs to a class of techniques which attempt to recover the prototype shape of a character starting from a distorted image.
By contrast with popular approaches in this field, our method does not need any explicit feature extraction and it outputs recognition scores which are estimates of Bayes probabilities.
It also allows an automatic training.
One of its distinctive characteristics is the use of a truely bi—dimensional but causal model for the estimation of probabilities.
Lastly, we show how synthetic images of characters fitted to the statistical distribution of shapes in the training set can be generated by this model.
This paper addresses typical problems encountered with hands-free equipment in the context of GSM radiotelephony.
We first summarise some important characteristics of the noise field in moving vehicles, and we also describe the acoustical echo phenomenon.
We show that, in order to provide sufficiently high speech quality, these hands-free equipments should include noise reduction (NR) and acoustic echo control (AEC) devices.
We then describe two possible structures combining noise reduction and acoustic echo control.
As a conclusion, we raise the fact that the choice of a particular structure, among those proposed, is conditioned by the performance of the adaptation algorithm of the AEC solution.
A new approach to adaptive Semantic-Language modelling has recently been proposed which allows automatic learning of all the acoustic and syntactic-semantic models that are required for a given Continuous Speech Recognition task.
Recognition or Understanding is seen as a Formal Transduction procedure that exploits the set of acoustic and linguistic constraints that have been captured in the learned models to directly input raw acoustic signals and output the semantic messages that are conveyed by these signals.
In this paper, the proposed approach is reviewed and new improvements are presented.
Also, preliminary results with a large semantic-space continuous speech task (Spanish numbers in the one-million range) are presented showing the currently achieved capabilities of this approach.
A system is described which adds simulated emotion effects to synthetic speech.
The control parameters of a speech synthesizer are controlled by rule in order to simulate the features of emotion expressed in the human voice.
The system can simulate six vocal emotions and was evaluated with naïve listeners.
The results indicated that the system was producing recognizable vocal emotions, with perception rankings similar to those found by previous research on human emotional speech.
This system has been developed for use in voice prosthesis systems for non-vocal disabled persons, although it could be used to enhance any application which uses rule-based synthetic speech.
A phonetic classification scheme based on a feed forward recurrent back-propagation neural network working on audio and visual information is described.
Some results will be given for various speaker dependent and independent phonetic recognition experiments regarding the Italian plosive consonants.
In this paper, we present, in an as complete as possible way, an approach for the design of complex adaptive systems, based on adaptive multi-agent systems and emergence.
For this, in the first place, we introduce the Amas theory (Adaptive Multi-Agent Systems).
This theory gives local agent design criteria so as to enable the emergence of an organization within the system and thus, of the global function of the system.
Then, we describe an application in e-education built using this theory by explaining its technical working and some experiments, which are thereafter discussed.
Other applications (flood prediction, ecommerce, telephonic routing) are also described, illustrating other domains where the theory has also been successfully applied.
Finally, we characterize the emergent phenomena in these applications, and position our theory in relation with others.
Statistical modeling of the speech signal has been widely used in speaker recognition.
The performance obtained with this type of modeling is excellent in laboratories but decreases dramatically for telephone or noisy speech.
Moreover, it is difficult to know which piece of information is taken into account by the system.
In order to solve this problem and to improve the current systems, a better understanding of the nature of the information used by statistical methods is needed.
This knowledge should allow to select only the relevant information or to add new sources of information.
The first part of this paper presents experiments that aim at localizing the most useful acoustic events for speaker recognition.
Finally, the potential of dynamic information contained in the relation between a frame and its p neighbours is investigated.
In the second part, the authors suggest a new selection procedure designed to select the pertinent features.
Conventional feature selection techniques (ascendant selection, knock-out) allow only global and a posteriori knowledge about the relevance of an information source.
However, some speech clusters may be very efficient to recognize a particular speaker, whereas they can be non-informative for another one.
Moreover, some information classes may be corrupted or even missing for particular recording conditions.
This necessity for speaker-specific processing and for adaptability to the environment (with no a priori knowledge of the degradation affecting the signal) leads the authors to propose a system that automatically selects the most discriminant parts of a speech utterance.
The proposed architecture divides the signal into different time–frequency blocks.
The likelihood is calculated after dynamically selecting the most useful blocks.
This information selection leads to a significative error rate reduction (up to 41% of relative error rate decrease on TIMIT) for short training and test durations.
Finally, experiments in the case of simulated noise degradation show that this approach is a very efficient way to deal with partially corrupted speech.
This study compared the effectiveness of three acoustic supplements to speechreading: the low-pass-filtered output of an electroglottograph, a variable-frequency sinusoidal substitute for voice fundamental frequency (FO), and a constant-frequency sinusoidal substitute that served as a representation of voicing.
Both sinusoidal signals were synthesized at constant amplitude during periods of voicing.
The sinusoidal signals were prepared off-line by a combination of automatic and manual estimation of the FO contours of video-recorded sentences.
These signals were then resynchronized with the audio portions of the original recording.
In 12 normally-hearing adults, the electroglottograph signal and the variable-frequency sinusoidal FO substitute both increased the number of words recognized in sentences of known topic by between 30 and 35 percentage points.
The magnitude of this effect was greater for longer sentences but independent of basic speechreading ability.
The constant-frequency substitute provided a 13 percentage point increase, suggesting that approximately one-third of the FO speechreading enhancement effect could be accounted for by voicing detection alone.
In this paper, we present a general framework for supervised classification.
This framework only needs the definition of a generalisation operator and provides ensemble methods.
For sequence classification tasks, we show that grammatical inference has already defined such learners for automata classes like reversible automata or k-TSS automata.
Then we propose a generalisation operator for the class of balls of words.
Finally, we show through experiments that our method efficiently resolves sequence classification tasks.
Most published adaptation research focuses on speaker adaptation, and on adaptation for noisy channels and background environments.
In this paper, we present a study of task adaptation, where the speech recognition models are adapted to a specific application or task, giving significant performance gains.
We explore several new questions about adaptation which have not been studied before, and present novel solutions to these problems.
For example, we show that adaptation can result in increased out-of-grammar error rates.
We present an automatic confidence score mapping algorithm to correct this problem.
We show that grammar-dependent acoustic adaptation gives improved performance.
In addition, we show that in-grammar acoustic adaptation gives significantly better results.
We study acoustic and grammar task adaptation, and show that the gains are additive.
Finally we show that adaptation improves both accuracy and speed, where traditional studies have been more focused on accuracy alone.
We also study traditional adaptation modes such as supervised and unsupervised adaptation, the use of confidence thresholds for unsupervised adaptation, and the effect of the amount of data on task adaptation.
In this paper we aim to identify the underlying causes that can explain the performance of different channel normalization techniques.
To this aim we compared four different channel normalization techniques within the context of connected digit recognition over telephone lines: cepstrum mean subtraction, the dynamic cepstrum representation, RASTA filtering and phase-corrected RASTA.
We used context-dependent and context-independent hidden Markov models that were trained using a wide range of different model complexities.
The results of our recognition experiments indicate that each channel normalization technique should preserve the modulation frequencies in the range between 2 and 16 Hz in the spectrum of the speech signals.
At the same time, DC components in the modulation spectrum should be effectively removed.
With context-independent models the channel normalization filter should have a flat phase response.
Finally, for our connected digit recognition task it appeared that cepstrum mean subtraction and phase-corrected RASTA performed equally well for context-dependent and context-independent models when equal amounts of model parameters were used.
In articulatory phonetics speech is described as a sequence of distinct articulatory gestures, each of which produces an acoustic event that should approximate a phonetic target.
Due to the overlap of the gestures these phonetic targets are often only partly realized.
Atal (1983) has proposed a method for speech coding based on so-called temporal decomposition of speech into a sequence of overlapping target functions and corresponding target vectors.
The target vectors may be associated with ideal articulatory positions.
The target functions describe the temporal evolution of these targets.
This method makes no use of specific articulatory or phonetic knowledge.
We have extended and modified this method to improve the determination of the number and the location of the target functions and to overcome the shortcomings of the original method.
With these improvements temporal decomposition has become a strong tool in analysing speech, from which researchers working on speech coding, recognition and synthesis may profit.
Telephone companies in the United States handle over 6 billion Directory Assistance (DA) calls each year.
Automation of even a portion of DA calls could significantly reduce the cost of DA services.
This paper explores two factors affecting successful automation of DA: (a) the effect of directory size on speech recognition performance, and (b) the complexity of existing DA call interactions.
Speech recognition performance for a set of 200 spoken names was measured for directories ranging from 200 to 1.5 million unique names.
Recognition accuracy decreased from 82.5% for a 200-name directory to 16.5% for a 1.5 million name directory.
In part because high recognition accuracy is not easily achievable for these very large, low-context directories, it is likely that initial implementations of DA automation will focus on a small percentage of calls, requiring a smaller vocabulary.
To maximize potential savings, listings that are most frequently requested constitute the optimal vocabulary.
To identify critical issues in automating frequent DA requests, approximately 13,000 DA calls from an office near a major metropolitan area in the United States were studied.
In this sample, 245 listings covered 10% of the call volume, and 870 listings covered 20% of the call volume.
This paper describes a unified framework for continuous speech recognition (CSR) under grammatical constraints, where trellis calculations and parsing are performed by the same simple fundamental operations, namely multiplication and addition of likelihood matrices.
The matrix parser is shown to be a generalization of the CYK parser.
It also facilitates explicit supra-segmental duration control for all grammatical categories.
Preliminary results showed that improved duration control on the mora level raised the recognition accuracy for a phrase recognition task from 86.7% to 88.5%.
An original form of poetical debate is elaborated in the 12th and 13th century in relation to court lyricism.
Under the appellation of “jeux-partis” in “oil” tongue, they meet some success in the urban frame of the “puy d'Arras”.
As they formulate a sophistry of love, they intersect a number of different formalisations such as the poetical, juridical and scholastic ones.
What is at stake in the debate is expressed on the dilemmatic mode.
The argumentation is worked out at large through three basic enunciations: maxims, proverbs and images.
The maxim, inscribed as it is in the lyrical discourse that it lays down as being axiomatical for love, has an ambivalent function: on one hand it is the enunciation one intends to dispute, on the other hand it is the form taken by the demonstration.
As it combines with a syntax of demonstration it only brings out the illusion of dialectics incidentally revealing the reduddant tautology of “le jeu-parti”.
Out of a number of 120 poems, there are no less than 80 proverbial expressions which articulate themselves on the context differently — although they do so in majority through assertive formulations — yet disrupting with its isotopy in so much as they illustrate and enunciate the rule at the same time.
The process of examplarisation, in the form of imaged enunciations operate other alterations.
If the proverb, descending from the empirical universe, universalizes the situation it refers to, the image alone proceeds inversely: from a general theme it gives an example of one or several anecdotical situations out of which the universalness of the rule emerges.
Now, none of those enunciations proceeds from a demonstrative or even properly argumentative logic.
They are enclosed in themselves.
The interlocutors do not resume what has just been said, unless it be in a blunt form of refutation.
The true formalisation is polemical.
It consists in a discourse which handles irony, lightly touches insult and seeks after effect rather than reasoning.
It builds up its own truth — contradictorily though — as a game played on an audience whose complicity is to be grasped and then requested — for truthfulness does lie right in the midst of the debate and asserts itself unendlessly, such as it is in “les disputes”, those contests between scholars.
“Disputes” and “jeux-partis” promote a logic of controversy.
They are “argumentations-spectacles”.
The elaboration of truth lies elsewhere, in the “Summae” for example.
Le “jeu-parti” is aporhetical.
If ever there exists an answer to its questioning, it can be found in love poetry the form of which includes the “sic et non” of “la joy”, mirth and play on love.
Among the various methods proposed to improve the accuracy and the robustness of automatic speech recognition (ASR), the use of additional knowledge sources is a successful one.
This paper describes a method we have developed for adaptive integration of acoustic and visual information in ASR.
Each modality is involved in the recognition process with a different weight, which is dynamically adapted during this process mainly according to the signal-to-noise ratio provided as a contextual input.
We tested this method on continuous hidden Markov model-based systems developed according to direct identification (DI), separate identification (SI) and hybrid identification (DI+SI) strategies.
Experiments performed under various noise-level conditions show that the DI+SI based system is the most promising one when compared to both DI and SI-based systems for a speaker-dependent continuous-spelling of French letters recognition task.
They also confirm that using adaptive modality weights instead of fixed weights allows for performance improvement and that weight estimation could benefit from using visemes as decision units for the visual recogniser in SI and DI+SI based systems.
There has been a recent focus in reinforcement learning on addressing continuous state and action problems by optimizing parameterized policies.
In this paper, we consider PI2 as a member of the wider family of methods which share the concept of probability-weighted averaging to iteratively update parameters to optimize a cost function.
We compare PI2 to other members of the same family – the 'Cross-Entropy Method' and 'Covariance Matrix Adaptation - Evolutionary Strategy' – at the conceptual level and in terms of performance.
The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for “Path Integral Policy Improvement with Covariance Matrix Adaptation”.
PI2-CMA's main advantage is that it determines the magnitude of the exploration noise automatically.
We illustrate this advantage on a non-trivial simulated robotics experiment.
In the last decade, there has been much interest in the development of context- aware systems but few logic-based formal theories of contexts have emerged.
The use of constructive type theory with Dependently Typed Records (DTR) allows for a partial knowledge and dynamic reasoning to take place while assuming an Open World Assumption.
The paper describes experiments on noisy speech recognition, using acoustic models based on the framework of Stochastic Trajectory Models (STM).
We present the theoretical framework of 4 different approaches dealing with speech model adaptation: model-specific linear regression, speech feature space transformation, noise and speech models combination, STM state-based filtering.
Experiments are performed on a speaker-dependent, 1011 word continuous speech recognition application with a word-pair perplexity of 28, using vocabulary-independent acoustic training, context independent phone models, and in various noisy testing environments.
To measure the performance of each approach, recognition rate variation is studied under different noise types and noise levels.
Our results show that the linear regression approach significantly outperforms the other methods, for every tested noise types at medium SNRs (between 6 to 24 dB).
For the Gaussian noise, with an SNR between 6 to 24 dB, we observe a reduction of the word error rate from 20% to 59% when the linear regression is used, compared to the other methods.
This paper deals with the problem of learning radial basis function neural networks to approximate non linear L 2 function from R d to R.
Hybrid algorithms are mostly used for this task.
Unsupervised learning techniques are used to estimate the center and width parameters of the radial functions and supervised learning techniques are used to estimate the linear parameters.
Supervised learning techniques are generally based on the least squares (LS) estimates (or criterion).
This estimator is optimal when the training set (z i,y i) i =1,2,.., q is composed of noisy outputs y i, i = 1,.., q and exactly known inputs z i, i = 1,.., q.
However, when collecting the experimental data, it is seldom possible to avoid noise when measuring the inputs z i.
The use of least squares estimator produces a biased estimation of the linear parameters in the case of noisy input output training data set, which leads to an erroneous output estimation.
This paper proposes the use of an estimation procedure based on the error in variables model to estimate the linear parameters (for supervised learning) when the training set is made up of input and output data corrupted by noise.
The geometrical interpretation of the proposed estimation criterion is given in order to illustrate its advantage with respect to the least squares criterion.
The improved performances in non linear function approximation is illustrated with a simulation example.
The subject of this paper is the automatic partially immersed sea target detection using infrared images (Band III).
These data exhibit different resolutions (depending on the camera field of view), different signal to noise ratios (depending on the sea state) and different contrasts (depending on the temperature difference between the target and the sea).
In a very classical way, pattern recognition involves two steps: a pre-processing phase followed with a decision phase (detection, possibly classification).
We can qualitatively conclude that the wavelet techniques are particularly robust.
The decision consists in a correlation process which is a rough (the decision results from a threshold of the correlation plane) but very simple (a filtering) operation.
The references are preprocessed in the same manner as the scene for each data series.
We propose an algorithm for estimating the finite-horizon expected return of a closed loop control policy from an a priori given (off-policy) sample of one-step transitions.
This algorithm, named Model-free Monte Carlo (MFMC) estimator, averages cumulated rewards along a set of "broken trajectories " made of one-step transitions selected from the sample on the basis of the control policy.
Under some Lipschitz continuity assumptions on the system dynamics, reward function and control policy, we provide bounds on the bias and variance of the estimator that depend only on the Lipschitz constants, on the number of broken trajectories used in the estimator, and on the sparsity of the sample of one-step transitions.
The limited number of data points usually available does not allow us to distinguish with certainty true underlying regularities from spurious ones when the data are described using nurnerous attributes.
In this paper, a recoding method inspiredfrom sparse coding and frequent itemsets is presented and applied to the classification of natural scenes.
A subset of 1,0(X) 1% itemsets is stochastically selected, and used to code each image into a boolean vector.
The aim of this work is to guide a reinforcement learning agent with some a priori knowledge we have about a given environment.
We propose a procedural formalism which allows to design a program to introduce this knowledge.
The basic idea of our method is to propose two sets of actions for a given state; a constrained one which is used first and a less constrained one, which will be used later.
The initial constraints reduce the state space and so, reduce the learning time.
But because the initial constraints can be too tight, we define a relaxation mechanism which will gradually increase the search space.
This way of relaxing the initial constraints allows us to prove, for a wide class of programs, that the policy learnt by the agent is as good as if there were no constraints.
This paper reports on a research project concerning prosody in spontaneous spech.
Two questions inaugurate the project.
The first one concerns prosodic differences between spontaneous speech and read speech.
Evidence from Swedish shows that these differences are not fundamental.
The second question concerns the relationship between prosody and discourse categories.
A methodology has been developed in order to study this relationship.
Four different kinds of analyses are applied: (1) analysis of the discourse structure of the speech corpus without specific reference to prosodic information, (2) auditory analysis in the form of a prosody-oriented transcription, (3) acoustic-phonetic analysis and (4) analysis-by-synthesis.
Part of this analysis is illustrated with exemplification from a persuasive monologue in French political rhetoric.
Focal accent and contrast in pitch range seem to account for typical prosodic means used during political debate.
We present a learning model for categorization of structured documents that takes into account both structural information and textual information.
We first define a generative model of structured documents using belief networks.
Then we transform the generative model into a discriminative one using the Fisher kernel.
Finally, we describe an instance of this model applied to the categorization of HTML documents.
The experimental application to a classical corpus shows that the use of structural information outperforms other classical models.
This brief study presents some acoustic phonetic characteristics that reflect both the voice characteristics and motor speech behaviour of 20 pre-adolescent (6-, 8- and 10-year olds) boys and girls, and 9 adults in speech data that were elicited via a picture-naming task.
The acoustic phonetic characteristics that were investigated included formant frequency values, coarticulation (or gestural overlap) and temporal patterns.
Both voice characteristics and motor speech behaviour presented evidence of age and sex differences, and age by sex interactions.
In addition there were significant correlations between formant frequencies and their associated formant frequency changes (or excursions).
There was also evidence of individual differences in the patterns of maturation, which did not conform to chronological age.
These data are presented and discussed with reference to the sexual dimorphism of the vocal apparatus, the development of vocal characteristics, and motor speech development and behaviour.
Fault tolerance in computer systems can look back at nearly thirty years of theoretical and experimental results.
Since the 80's an important effort has been brought to integrate generic fault-tolerance mechanisms into reusable distributed middleware.
The presence or absence of a context sentence, varied across the two experiments, allowed an estimate of between-sentence effects on local and global processes.
Two experiments are reported in which subjects made rapid lexical decisions about inflected nouns preceded by inflected adjectives or pseudoadjectives that did or did not agree grammatically.
Both adjectives and pseudoadjectives were shown to affect lexical decision times for nouns, suggesting that the priming of inflected nouns by inflected adjectives occurred at the level of the inflections.
Inflected pseudonouns, however, were not affected similarly, suggesting that lexical factors were contributing to the priming in addition to grammatical factors.
This instance of grammatical priming is described as an effect that arises post-lexically, based on the outcomes of relatively independent lexical and syntactical processors.
A tutorial review of the use of discrete time transversal Volterra filters (TVF) in detection, estimation and narrow-band array processing is presented, when the data are either real or complex valued.
A unique representation of such filters allows to cope simultaneously with a wide field of problems ranging from maximizing the contrast in Hilbert spaces spanned by the outputs of TVF, to the non linear least squared estimation of an unknown stochastic parameter.
The general extended normal equations, giving the optimal TVF for all the above problems, are at first derived in a unique form and afterwards applied to each issue of concern.
Finally, a few ideas are presented on adaptive TV filtering algorithms.
This paper describes a procedure of multi-step anonymization that was developed on French narrative clinical records in cardiology.
Our approach is based upon a combination of several methods, using a rule-based approach followed by the application of a machinelearning system.
The combination of the two outperformed each of them taken separately (0.881 F-measure), with a recall (0.912) higher than precision (0.851).
We introduce an extension of the notion of Shannon conditional entropy to a more general form of conditional entropy that captures hoth the conditional Shannon entropy and a similar notion related to the Gini index.
The proposed family of conditional entropies generates a collection ofmetrics over the set of partitions affinite sets, which can be used to construct decision trees.
Experimental results suggest that by varying the parameter that defines the entropy it is possible to obtain smaller decision trees for certain data bases without sacrificing accwracy.
Over the past 2 decades, the spatial analysis methods developed were based upon restricting hypotheses that are not fulfilled in practical cases and, therefore, often yield disappointing results.
This is particularly true when uncorrelated sources, plane wavefronts and identical sensors hypotheses are introduced.
In a classical approach, source parameters estimates are obtained using the information contained in the eigenvalues and eigenvectors of the received signals cross-spectral matrix.
Making use of the properties of this matrix, an alternate version of the eigensystem method based upon the linear dependency between the source-matrix rows is shown to exist.
A deep insight into the causes of performance loss clearly shows an interest for an estimation as complete as possible of the distorted wavefronts.
Letting the modules of the sources-vectors elements being equal, a new algorithm which permits the estimation of their phases is finally adressed.
This study describes a method for the automatic detection of two-consonant clusters in French.
Four corpora were used, consisting of 603 different consonant clusters and single consonant combined with the 3 vowels /i, a, ã/, in disyllabic and trisyllabic words.
The CCV, VCC, CV and VC syllable structures were studied.
Stimuli were recorded 5 times in an anechoic room by 10 subjects.
Word length, syllable length and syllable-component length were measured by means of a signal editor.
More than 90% of the consonant clusters were correctly classified on the basis of the values of the duration parameters extracted from our data base.
Seventeen rules were used to output all the macro-classes of the consonant clusters in the test corpus.
Current algorithmes on multiagent learning are for almost limited since they cannot manage the multiplicity of Nash equilibria and thus converge to the Pareto-optimal.
We present experimental results showing convergence of such learning mechanism.
We then extend our approach to the case of non-stationarity of agents which is another important aspect of multiagent systems.
Finally, we tackle the question of non stationarity in multiagent environments in its generality and we present in this context some research avenues which can lead to improve our preliminary results on adaptation.
This article reviews current research on neural network systems for speaker recognition tasks.
We consider two main approaches, the first one relies on direct classification and the second on speaker modelization.
The potential of connectionist models for speaker recognition is first presented and the main models are briefly introduced.
We then present different systems which have been recently proposed for speaker recognition tasks.
We discuss their respective performances and potentials and compare these techniques to more conventional methods like vector quantization and Hidden Markov models.
The paper ends with a summary and suggestions for further developments.
Any realistic account of language acquisition must take into account the manner in which the child passes from pre-speech communication to the use of language proper.
For it can be shown that many of the major organizing features of syntax, semantics, pragmatics, and even phonology have important precursors and prerequisites in the prespeech communicative acts of infants.
Illustrations of such precursors are examined in four different domains: The mother's mode of interpreting the infant's communicative intent; the development of joint referential devices en route to deixis; the child's developing strategy for enlisting aid in joint activity; the transformation of topic-comment organization in prespeech to predication proper.
Finally, the conjecture is explored whether the child's knowledge of the requirements of action and interaction might provide the basis for the initial development of grammar.
The obtained sitemap is both usable by the internet user and webmaster, but it is especially adapted to visual handicapped person or impaired memory capacities.
In this paper, we present our method of sitemap generation which uses artificial ants to generate clusters of similar pages, then we use Prim algorithm on each cluster generated by ants.
They represent important steps in the constitution of a monolingual lexicography that, in varying measures and for certain aspects, thanks to the use and elaboration by humanists such as Cristoforo Scarpa, Giovanni Tortelli, Nestore Dionigi Avogadro and Niccolò Perotti, anticipate modern lexicography with its models and uses.
The paper aims at illustrating these aspects by evaluating certain Latin words whose treatment in the entries of the abovementioned works from the Middle Ages to Humanism can suggest a typological classification which can also reflect the different ways in which the relationship between grammar and lexicon has been conceived and brought to fruition.
We review some of the significant themes of research being pursued by speech researchers in Australia and which are not adequately covered in the other papers of this issue.
In addition we examine some of the local work on the development of tools and resources for speech research.
For applications needing orientation analysis, Gabor functions provide a well-known and frequently used wavelet decomposition.
Localised band-pass low frequency filters, if implemented through direct convolution, lead to costly orientation image decompositions.
To counteract this effect, corrective action must be taken during the generation of the convolution kernels.
Two examples of pyramidal decomposition illustrate the efficiency of our Gabor filter implementation.
The UMLS® (Unified Medical Language System®) is an extremely rich terminological product which was pragmatically built and can be construed in multiple ways.
We sketch the nature of the UMLS and stress two of its potentially antithetic aspects: its relation to ontologies and its relation to language.
We compare a linguistic/psycholinguistic approach of the organisation of the mental lexicon with a computational approach of the implicit lexical organisation in dictionaries taken as graphs and whose structure is a “small world”.
In this paper, we propose a complete system of analysis of images, which includes the whole sequence of treatments from the low level until the interpretation.
It uses neural networks as well as a rule-based system.
We show that the implementation of an expert-system gives useful information for the conception of the neural nets.
The mixed realisation allows us to use at best the specificities of each approach.
We also show how to make a neural network learn locally contradictory configurations.
A review of research on speech-motor perceptual and auditory processing, based particularly on studies at the I.P. Pavlov Institute of Physiology in Leningrad by V. Kozhevnikov and L. Chistovich.
The use of spoken responses to speech stimuli, i.e. shadowing and mimicking, is given particular emphasis.
Early versions of a motor theory of speech perception are also presented.
We study an approximate calculus of the Fourier Transform.
Subject to specified conditions, we define, the calculus of the different components of the spectra does not require any multiplication.
Thus, the calculus, can be done in real time on an ordinary micro-computer.
Moreover, the calculus of the different components of the spectra is done independently of one another.
We tested this method on a signal whose spectra is well known, and on a real speech signal.
The precision is of a few percent if the signal is oversampled.
In this paper, we present an on-line handwritten character recognition system which is based on structured and logical modeling of handwriting using Hidden Markov Models.
After some specific preprocessing, we extract two different classes of primitives which represent the two main aspects of handwriting: the dynamic aspect for the notion of trajectory of the pen tip and the static aspect for the notion of global geometry of the letter.
We make an initial training to adjust the probabilities of each Hidden Markov Model.
Then, the recognition system computes the probabilities of generation by each model of the letter to be interpreted.
This performs a clustering process based on similarity.
Two different Artificial Neural Network (ANN) classifiers have been compared with a traditional closest-mean-classifier, a VQ and a Kohonen-network with respect to classification of vowels extracted from continuous speech.
The result shows that the best performance is achieved with the two ANN-classifiers and indicates that a Kohonen map does not deteriorate the information presented to the second layer in the network and hence can be used instead of a first hidden layer.
This study is the first of a series investigating the relation between speech errors and intelligibility of deaf speech.
The role of temporal errors is investigated by artificially correcting temporal deviations present in spoken utterances and assessing how different corrections influence intelligibility.
In a corpus of 30 sentences spoken by ten deaf children, age 12–14 years, the temporal structure was manipulated using digital signal processing techniques (incorporating L.P.C.-analysis), such that 6 different temporally corrected versions were obtained.
These versions differed mainly in the extent to which their temporal structure approximated that of norm sentences spoken by hearing children.
All versions, including the original unmanipulated one, were subjected to intelligibility tests.
These tests show that for the majority of sentences temporal correction results in a small but significant improvement (of 4.5% to 6%) in intelligibility.
Elimination of pauses generally reduces intelligibility.
The results obtained disqualify expectations based on correlational studies suggesting a much larger effect of temporal corrections, but corroborate the results of Osberger & Levitt (1979), who employed the same method as we did.
Besides replicating Osberger & Levitt's work, the present study goes further in presenting detailed information about the temporal characteristics of deaf speech and in testing finer temporal distinctions.
This paper presents a synthesis of the subspace-based methods for direction of arrival or frequency estimation which do not require the eigendecomposition of the data covariance matrix.
These methods, referred to as linear methods because they only use linear operations on the data covariance matrix, have a potential interest for real time applications because of their low complexity and their possible adaptive implementation.
While presenting the methods which are referred to as BEWE, the Propagator Method (MP) and SWEDE, we establish the relationship between the different versions of these methods.
The complexity of each method is established and discussed.
BEWE then appears as the less costly of the linear methods.
As the asymptotical performances (for an infinite number of data) of BEWE and SWEDE has already been obtained in the literature, we here propose the derivation of the asymptotical performances of a particular version of the MP, referred to as the Propagator Method with noise elimination (MPEB).
We then show that MPEB has the best performance of the linear methods and has the same performance as MUSIC.
Simulations are given to strengthen the theoretical results established in the paper and to illustrate the comparaison between all the different methods.
In order to fit properly the a priori information about the observation process, the implementation of any linear quadratic adaptive filter requires to estimate the influence of the third order moments, by the means of the skew and the kurtosis.
If these moments are vanishing, the proper structure of such algorithms is uncoupled, that is to say, two different algorithms update on the one hand the linear kernel and on the other hand the quadratic.
If these same moments are not equal to zero, the suited structure is coupled: a unique algorithm jointly updates both linear and quadratic kernels.
These results stems from the fact that any adaptive algorithm may be viewed as a stochastic estimate of a deterministic gradient type or Newton type procedure.
Since Volterra filtering is still linear with respect to the parameters, the properties of the linear quadratic LMS algorithm are very similar to these of the classical.
In particular, the approach of the convergence based on the «Mary» indépendance theory remains mainly unchanged.
The mismatch in the structure of the LMSLQ algorithm is modelled, when the third order moments are not vanishing, by a noise called the mismatch noise.
The explicit derivation of its variance exhibits how the steady state behavior of the algorithm may be damaged by an unfitness of the structure.
We describe a bottom-up acoustic and phonetic decoding system which produces phonetic lattices by simultaneously locating and identifying the units by means of various types of spectral distances adjusted according to the phonemes, the context, and the speaker's characteristics.
The results — both for isolated words and for continuous speech — give all the phonemes that have been pronounced, with an efficiency particularly interesting for a bottom-up selection of restricted sets of elements in a large vocabulary.
Starting with a comparison between the first big French thesaurusof Lafaye which distinguishes between two lexicaly close items by opposing different co-occurences within the sentences in which these items are usaully uttered, thus pioneering the Lexicon-grammar analyses inittiated by Maurice Gross, and the first big French analogical dictionary of Boissière which indeed helps finding unknown words but with the cost of squeezing grammatical and semantic differences between lexicaly close items, the author suggests a calculus of the degree of analogy between two items through an analytic and defining description refering to the matrix from which stemmed the defining matrixes of the two compared items.
In this paper, assuming that the score of a speech utterance is a weighted sum of hidden Markov model (HMM) log state-likelihoods, we propose a new method of finding discriminative state-weights recursively using the generalized probabilistic descent method.
Compared with the previous approaches, this method does not increase complexity and can be implemented with minor modification of the conventional parameter estimation and recognition algorithms by constraining the sum of the state-weights to the number of states in a recognition unit, and further it can be applied to continuous speech recognition as well as isolated word recognition.
To evaluate the performance of the state-weighted HMM recognizer, we perform two kinds of experiments with phoneme-based and word-based state-weights using various kinds of speech databases.
Experimental results showed that the recognizers with phoneme-based and word-based state-weights achieved 20% and 50% decrease in word error rate, respectively, for isolated word recognition, and 5% decrease for continuous speech recognition.
Our approach yields recognition accuracies comparable to those of the previous approaches for continuous speech recognition, but it is much simpler to implement than others.
Today the computer is changing from a big, grey, and noisy thing on our desk to a small, portable, and ever-networked item most of us are carrying around.
This new found mobility imposes a shift in how we view computers and the way we work with them.
When interaction can occur anywhere at any time it is imperative that the system adapts to the user in whatever situation the user is in.
The second part offers automatic situation assessment through Case-Based Reasoning.
We demonstrate a multi-agent system for supplying context-sensitive services in a mobile environment.
We propose a learning method from examples situated at the junction of statistical methods and those based on Artificial Intelligence techniques.
Our modelisation is based on automatic generation of classification rules and on an original use of approximate reasoning.
The proposed learning method based on linear correlation search among the components of the training set vectors is multi-features.
The rule uncertainty is managed in the learning phase as well as in the recognition one.
A tool called SUCRAGE was implemented and confronted with a real application in the field of image processing.
The obtained results validate our approach and allow us to consider other application fields.
Those results also confirm our imperfections hypothesis: the approximate reasoning as well as the intraclasses correlation search can appropriately improve the results.
A noisy environment usually degrades the intelligibility of a human speaker or the performance of a speech recognizer.
Over the last few years, special emphasis has been placed on analyzing and dealing with the Lombard effect within the framework of Automatic Speech Recognition.
Thus, the first purpose of the work presented in this paper was to study the possible common tendencies of some acoustic features in different phonetic units for Lombard speech.
Another goal was to study the influence of gender in the characterization of the above tendencies.
Extensive statistical tests were carried out for each feature and each phonetic unit, using a large Spanish continuous speech corpus.
The results reported here confirm the changes produced in Lombard speech with regard to normal speech.
Nevertheless, some new tendencies have been observed from the outcome of the statistical tests.
We present a theory of belief reconstruction to be embedded in an agent's communication model, which accounts for both belief persistence and revision.
We analyse Cohen and Levesque (1990a)'s theories, highlight problems which arise, and show that our theory does not have these problems.
The starting point of our theory is called the observation principle.
It accounts for a distinction between what an agent observes from another agent, and the action the latter has really performed.
The theory is couched in an autoepistemic logic used objectively, along the same lines as in (Levesque, 1990).
When applied to a communication context, it is shown that it correctly predicts the changes in an observer's beliefs in test cases such as sincere assertion and (detected or non-detected) lie.
Such test cases highlight the ability of the theory to handle not only normal dialogue situations but also those where problems arise due to erroneous perception, such as misrecognition in spoken communication.
Simultaneous recording of vocal fold vibrations and speech signals were performed with three patients having diplophonia using a high-speed digital image recording system developed by the present authors.
All three cases studied (1 case of unilateral paralysis of the recurrent nerve; 2 cases of unilateral paralysis of external branch of the superior laryngeal nerve) showed a difference in the vibratory frequency between the left and right vocal folds.
The phase difference between the vocal cords varies with time.
When it reaches a certain threshold, the phase difference is reset and the vocal cord movements resumes synchrony.
When the movements of the vocal cords are in phase, glottal closure is complete and the excitation pattern in the speech waveform is strong, whereas when the movements are out of phase, glottal closure is incomplete and the excitation pattern is weak, resulting in a quasi-periodic vibration in speech waveform.
The aim of this study is to determine the acoustic properties of hiatuses (vowel-vowel sequences) and diphthongs (glide-vowel sequences) in Spanish and to observe how these properties are modified depending on communicative factors.
To do this, two groups of data were used: speech samples gathered from conversations between two speakers participating in the execution of a map task, in which the corpus items corresponded to toponyms, and the reading of the same sequences at a normal speaking rate.
The comparison was done phonetically and phonologically: first, diphthongs and hiatuses were analyzed acoustically, studying their duration and spectral dynamics, and later, an inventory of diphthongizations and monophthongizations was made.
Results show that hiatuses and diphthongs differ in the temporal and frequential domain: hiatuses have a longer duration and a greater degree of curvature in the F2 trajectory than diphthongs.
We have also found that vowel-vowel and glide-vowel sequences behave differently in the way they are phonetically reduced: a reduction axis can be drawn in which hiatuses become diphthongs, and diphthongs vowels.
It is concluded that hiatus and diphthong are two phonetic categories which can be described on the basis of their acoustic characteristics and are subject, like any other phonetic category, to modifications due to a change in the communicative situation.
Japanese speech synthesis techniques based on composite phoneme units are surveyed.
Japanese is an open-syllable language, and there are no consonant clusters in its phonemic system.
Japanese is therefore structured with simple and basic syllables of a CV-type, and these are widely used in Japanese speech synthesis-by-rule instead of single phonemes.
Other composite syllabic units such as VCV or CVC are also used in Japanese speech synthesis to achieve coarticulatory characteristics.
In this paper, a speech synthesis method using CVC units with excitation waveform elements is described as an attempt to improve the quality of synthetic speech.
In spoken dialogue systems, natural language understanding is a difficult problem for which robust parsing methods are required.
Most of the systems achieve very specific tasks: understanding is founded on detection ofkey-words or patterns in order to identify values ofpredetermined semantic frames.
LOGUS, the understanding system we are presenting in this paper, uses logical formalisms, categorial grammars and conceptual graphs, outside their usual application field.
The parsing is incremental; it builds a logical formula by gradually composing the recognized constituents of the sentence.
The paper describes and compares the first two versions of Logu S.
Their evaluations yield promising results; they show the good robustness of the parsing and its quite good ability to reconstruct the meaning of the utterances.
Future studies must be led in order to take into account the context more widely and to manage dialogue.
The performance of isolated-word speech recognizers is typically measured using error rates.
The Effective Vocabulary Capacity (EVC) is the maximum vocabulary that a recognizer can in principle handle at a given error rate.
It relies on measures that are relatively independent of the challenge vocabulary and that require only tens or hundreds of test utterances.
The EVC algorithm is tested with both synthetic and real recognizer data.
Usual visualization methods for multidimensional data sets, do not scale well to high numbers of dimensions.
This paper presents a region-based segmentation algorithm which can be applied to various problems since it does not require a priori knowledge concerning the kind of processed images.
The splitting algorithm works independently on each sector, and uses a homogeneity criterion based only on grey levels.
The merging is then achieved through assigning labels to each region obtained by the splitting step, using extracted feature measurements.
We modeled exploited fields (data field and label field) by Markov Random Fields (MRF), the segmentation is then optimally determined using the Iterated Conditional Modes (ICM).
Input data of the merging step are regions obtained by the splitting step and their corresponding features vector.
The originality of this algorithm is that texture coefficients are directly computed from these regions.
These regions will be elementary sites for the Markov relaxation process.
Thus, a region- based segmentation algorithm using texture and grey level is obtained.
Relations between emotions and multimodal behaviors have mostly been studied in the case of acted basic emotions.
In this paper, we describe two experiments studying these relations with a copy-synthesis approach.
We start from video clips of TV interviews including real-life behaviors.
A protocol and a coding scheme have been defined for annotating these clips at several levels (context, emotion, multimodality).
The first experiment enabled to manually identify the levels of representation required for replaying the annotated behaviors by an expressive agent.
The second experiment involved automatic extraction of information from the multimodal annotations.
Such an approach enables to study the complex relations between emotion and multimodal behaviors.
Models of visual word recognition differ in assumptions about the extent to which phonological information is used, and the processes by which it becomes available.
These issues were examined in two studies of word recognition in two writing systems, English and Chinese, which are structured along different principles (alphabetic and logographic, respectively).
The results indicate that in each writing system, a large pool of higher frequency words is recognized on a visual basis, without phonological mediation.
Phonology only enters into the processing of lower frequency words.
Thus, although there may be other differences among writing systems which influence processing, differences in the manner in which they represent phonology are not relevant to the recognition of common words.
The results are consistent with a parallel interactive model of word recognition in which orthographic and phonological information are activated at different latencies.
How to determine the set of relevant features according to a fixed task?
Neural Feature Selection try to solve the problem during the neural network learning.
Some frequently used methods are derived from a pruning technique, OBD, proposed in 1990 by LeCun and al.
This article review these methods and propose some enhancements by using some simple rules.
A study will then compare the previous methods and other classical ones.
At first we deal with the reasons why it is necessary to separate, within a signal, the purely deterministic parts from the non deterministic parts.
Then we present the Prony s method and give application examples for computed processing signals
When using hidden Markov models for speech recognition, it is usually assumed that the probability that a particular acoustic vector is emitted at a given time only depends on the current state and the current acoustic vector observed.
In this paper, we introduce another idea, i.e., we assume that, in a given state, the acoustic vectors are generated by a continuous Markov process.
Indeed, the time evolution of the acoustic vector is inherently dynamic and continuous, and sampling only occurs for the purpose of computation.
This allows us to assign a probability density to the time trajectory of the acoustic vector inside the state, reflecting the probability that this particular path has been generated by the continuous Markov process associated with this state.
Roughly speaking, it measures the “adequacy” of the observed trajectory with respect to an ideal trajectory, which is modelled by a vectorial linear differential equation.
As usual, the segmentation can be obtained by sampling the continuous process, and by applying dynamic programming to find the best path over all the possible sequences of states and all the possible durations.
We describe the use of spectral transformation to perform speaker adaptation for HMM based isolated-word speech recognition.
The paper describes and compares three methods, namely, minimum mean square error (MMSE), canonical correlation analysis (CCA) and multi-layer perceptrons (MLP), to compute the transformations.
Using isolated words from the TI-46 speech corpus, we found that CCA offers the best adaptation performance.
Three HMM training and adaptation strategies are also discussed.
In the “no-retraining” approach, the spectral transformation is computed from a small amount of adaptation data, and may be used, essentially, for on-line adaptation.
The “training-after-adaptation” approach computes transformations prior to off-line HMM training, but produces a better set of models.
The third approach is a novel two-stage combination of these approaches which has been found to achieve good adaptation performance while maintaining fast adaptation.
Our experiments show that, on average, only around 10% of a new speaker's training data is required for adaptation in order to achieve better recognition accuracy than that obtained using the speaker-dependent models of that new speaker, when the CCA spectral transformation estimation method is used with this two-stage approach.
This article questions the implantation of Jean Miélot and his works in the environment of the court of Burgundy.
I start from the observation that in spite of the enormous variety of interesting texts that Miélot has left us, it seems that he did not meet much success during his own life time.
Many of his texts survive only in one or two copies and few contemporary libraries contained his books.
There are two questions to be asked: in the first place if we should see Miélot as an isolated author or as a writer who was well integrated in an environment of book producers at the Burgundian court, and in the second place if his works did indeed not meet much success or if they were more widely read than we think.
Some investigations in the field of the handwriting and the decoration of the Miélot manuscripts confirm the idea that he was not an isolated writer, but on the contrary well integrated in a network of producers of books for the Burgundian court.
We can suppose close ties with colleagues, especially with David Aubert.
An overview of the fifteenth-century owners of manuscripts containing Miélot's texts shows that his readers were limited to a very small circle of the ducal family and of several members of the high nobility at the Burgundian court.
His texts were indeed as good as not spread to other social groups, nor to other geographical areas.
Jean Miélot is an original and interesting fifteenth-century figure and combined codicological, paleographical, textual, and stylistic research can offer us new insights in his place and his work.
This article presents a new algorithm used in order to convert the speech of one speaker so that it sounds like that of another speaker.
This algorithm flexibly converts voice quality using two major technical developments.
Firstly, the modification of formant frequencies and spectral intensity using piecewise linear voice conversion rules.
Secondly, this algorithm provides the ability to produce speech with the desired formant structure by controlling formant frequencies, formant bandwidths and spectral intensity.
Speech is iteratively modified in order to achieve the specified formant structure.
Listening tests prove that the proposed algorithm converts speaker individuality while maintaining high speech quality.
This article outlines a theoretical framework for the understanding of the neural basis of memory and consciousness, at systems level.
This proposal rejects a single anatomical site for the integration of memory and motor processes and a single store for the meaning of entities of events.
Meaning is reached by time-locked multiregional retroactivation of widespread fragment records.
Only the latter records can become contents of consciousness.
We are currently in the midst of a revolution in communications that promises to provide ubiquitous access to multimedia communication services.
In order to succeed, this revolution demands seamless, easy-to-use, high quality interfaces to support broadband communication between people and machines.
In this paper we argue that spoken language interfaces (SLIs) are essential to making this vision a reality.
We discuss potential applications of SLIs, the technologies underlying them, the principles we have developed for designing them, and key areas for future research in both spoken language processing and human–computer interfaces.
This paper presents hierarchical self-organizing map (som) models for phoneme classification.
The hierarchical som method uses a non supervised learning and a spatial organization of data.
This classification approach extends the Kohonen map by introducing the principle of multiple prototype vectors by means of an enrichment auxiliary information method in a map.
The case study of hierarchical som classification models is phoneme recognition in continuous speech and speaker independent context.
The proposed som models serve as tools for developing intelligent systems and pursuing artificial intelligence applications.
Whether closed-class words use the same lexical access route(s) as open-class words has been intensely debated recently.
Differences in frequency sensitivity have been suggested as one manifestation of separable access routes.
We did not find evidence to support the view that closed-class words have a different or special access route.
Neither word class showed any appreciable frequency effect for Kućera-Francis frequencies of 400/million or greater, on either reaction time or error analyses.
We did find open-class words to have somewhat faster responses than comparable closed-class words, but this may contradict some explanations of the reported word class effect (Bradley et al., 1980).
Moreover, our data also show what may be word-specific influences on lexical decision times—effects which may be impossible to factor out of the word class effect in English.
In order to accommodate the frequency insensitivity that we found, logogen-based models of lexical access have to be amended to include a floor on threshold settings.
Resonance models (Gordon, 1983), already predict this frequency insensitivity.
It should be possible to distinguish between these two accounts by comparing masked and routine lexical decisions, but the unexpected word-specific effects prevented us from doing so.
We suggest that Apollonius' system can be explained by his understanding of the noun, that is said to signify both the substance and the quality of the referent and is therefore related to the signification of the hyparxis / ousía: naming a referent implies its existence at least at linguistic level.
This article deals with rereading activity, mostly in its relationship to temporality.
After a few definitions, it analyses successively textual «programming» of rereading, various modalities of rereading of either fictional or non fictional texts, the main reasons of rereading.
It thus allows a synthesis of the rereading phenomenon in its links to temporal flow, both on poetical, pragmatical and anthropological level.
This paper examines the problem of shape-based object recognition, and proposes a new approach, the alignment of pictorial descriptions.
The first part of the paper reviews general approaches to visual object recognition, and divides these approaches into three broad classes: invariant properties methods, object decomposition methods, and alignment methods.
The second part presents the alignment method.
In this approach the recognition process is divided into two stages.
The first determines the transformation in space that is necessary to bring the viewed object into alignment with possible object models.
This stage can proceed on the basis of minimal information, such as the object's dominant orientation, or a small number of corresponding feature points in the object and model.
The second stage determines the model that best matches the viewed object.
At this stage, the search is over all the possible object models, but not over their possible views, since the transformation has already been determined uniquely in the alignment stage.
The proposed alignment method also uses abstract description, but unlike structural description methods it uses them pictorially, rather than in symbolic structural descriptions.
Designing the behaviour of non player characters (NPC), in role-playing video games, is a hard problem both from the programming point of view and from behaviour modelling considerations.
This engine is combined with an action selection mechanism based on motivations.
This mechanism provides means to define several NPC behaviours by tuning the motivation parameters without new programming code.
Our proposition constitutes a behavioural engine dedicated to the modelling of situated character behaviour.
Thanks to its genericity, this engine can be used in various environment (ie games) and for various agents, since it adapts to individual specificities and abilities while proposing a diversity in designed behaviours.
Two models for simulating the time-varying vocal tract by means of wave digital filters (WDF) are described.
In one model, the coefficients of the WDF are merely updated to simulate area changes.
In the other, a “time-varying WDF” is used which allows a physically consistent description of the time-varying vocal tract.
In order to test if the first model is sufficient for speech synthesis, the two models are compared one to another.
Furthermore, three different models for simulating a glottal source are described.
One model is based on an ordinary WDF, the second on a time-varying WDF, and the third on a model in which the glottis is assumed to be purely resistive.
The signals of these models and their interactions with the vocal tract are compared one to another.
It is shown that the differences between the two models of the vocal tract are very small in the simulation of speech, and not audible in hearing experiments.
The differences are larger between the three glottis models, and also audible in hearing experiments.
Simple simulation by means of a purely resistive source produces effects, in the same way as the other models do.
LPC speech production models often assume a two real pole function for the glottal volume velocity waveform.
This paper discusses the properties of this glottal model with respect to key time domain features and concludes that it is physiologically enrealistic for normal waveform representation.
Implications for the performance of LPC analysis schemes, and the perceptual quality of vowel sounds synthesized using this model, are discussed.
This article presents the performance results of a low-bit rate image codec for videophone and B/2B channel ISDN applications.
At the receiver, the missing frames are constructed by linear interpolation using coded frames and respective motion vectors.
The effectiveness of the codec is evaluated by employing two video sequences as input.
The resulting video sequences indicate that for videophone applications, an acceptable quality of image is obtained.
Therefore, this codec can be a good candidate for a videophone in ISDN network.
Finally, the implementation considerations of this codec are also given.
This paper summarizes results from recent studies on the role of long-term memory in speech perception and spoken word recognition.
Experiments on talker variability, speaking rate and perceptual learning provide strong evidence for implicit memory for very fine perceptual details of speech.
Listeners apparently encode specific attributes of the talker's voice and speaking rate into long-term memory.
Acoustic-phonetic variability does not appear to be “lost” as a result of phonetic analysis.
The process of perceptual normalization in speech perception may therefore entail encoding of specific instances or “episodes” of the stimulus input and the operations used in perceptual analysis.
These perceptual operations may reside in a “procedural memory” for a specific talker's voice.
Taken together, the present set of findings are consistent with non-analytic accounts of perception, memory and cognition which emphasize the contribution of episodic or exemplar-based encoding in long-term memory.
The results from these studies also raise questions about the traditional dissociation in phonetics between the linguistic and indexical properties of speech.
Listeners apparently retain non-linguistic information in long-term memory about the speaker's gender, dialect, speaking rate and emotional state, attributes of speech signals that are not traditionally considered part of phonetic or lexical representations of words.
These properties influence the initial perceptual encoding and retention of spoken words and therefore should play an important role in theoretical accounts of how the nervous system maps speech signals onto linguistic representations in the mental lexicon.
In this article, we present a multisensorial solution for road obstacle detection and tracking.
This solution is based on a mixed camera/3D sensor mounted on the front of an experimental vehicle.
The multisensor is described.
The calibration step enables the matching of the heterogeneous data.
Two capabalities of the sensor have been developped: the controlled perception making possible the acquisition of depth data in an area defined in the intensity image; the visual servoing carrying out the focusing of the laser beam on a moving target detected in the intensity image.
These two capabalities allow a feedback control on the acquisition mode of the sensor according to the environment.
The perception strategy is based on the selection of the best sensor for a given goal.
The obstacle detection is based on the segmentation and interpretation of depth data which are well suited in this context.
However, the rate of acquisition of these data is too slow in order to extract the kinematic state of the obstacle.
So, the tracking process is based on the collaboration between intensity image processing which ensures the tracking itself and a 3D process which returns the obstacle model size to search in the image.
This algorithm of heterogeneous data fusion, associated with a Kalman filtering, permits to compute the state of obstacles.
This work fits into the european project PROMETHEUS. Experimental results have been validated in real situation on the Prolab vehicle.
This paper describes a monosyllabic corpus for use in testing the consonant intelligibility of synthesized speech.
It differs from those used in other tests in that it spans a wide variety of English sounds and is thus useful for diagnosis as well as for comparative assessment.
Some “standard” tests of intelligibility use restricted phonetic material, which is possibly easier to understand than a representative sample of English; thus, the results from those tests may not reflect the intelligibility of a wider sample of speech.
For illustration, we present the results of a telephone comparison between a demisyllable synthesizer currently being developed at Bellcore (“Orator”), a commercially available phoneme-based synthesizer, and natural speech obtained from 2 talkers.
The natural speech data can be used by other laboratories wishing to compare the consonant intelligibility of other synthesis systems to natural speech.
A Voice Oriented Interactive Computing Environment (VOICE) has been implemented in the Hindi language. The system provides in interactive facility for visual and voice feedback.
The 200 isolated word recognition system is designed around a railway reservation enquiry task and uses acoustic-phonetic segments as the basic units of recognition.
Frame level classification into broad acoustic-phonetic categories is accomplished by a maximum likelihood classifier and segmentation by hierarchical clustering of the frame level likelihood vectors by use of explicit duration semi (Hidden) Markov Models.
A more detailed classification of a few categories (vowels, voice bar and nasals in the first instance) is performed by neural nets.
String matching using dynamic programming accomplishes lexical access, or conversion of the phonetic category symbol strings into words.
Distributed processing of the word recognition task enables recognition at four times real time.
A language processor disambiguates between multiple choices given by the recognizer for each word and even corrects some acoustic level recognition errors.
This, the first system working in any Indian language, gives a recognition performance of 85% at the word level.
For comparison, a purely HMM based word level recognizer has also been implemented.
The performance is expected to improve further as there is still substantial scope for refinement.
Several efficient Variable-Bit-Rate MR) methods suitable for low-delay Delayed-Decision Tree-Code (DDTC) and CELP speech coders are presented in this paper.
These methods are based on modifying the existing codebook(s) by means of block or lattice codes.
To achieve a reduced rate operation of the DDTC coder a novel technique based on a parity code is developed and up to 2 dB SNR improvement uver conventional methods is demonstrated.
For extended rate operation of the LD-CELP coder an approach based on a lattice code is used.
The modified or additional lattice codebook has a geometrical (algebraic) structure, allowing the search procedure to be performed efficiently and with minimal additional memory.
Simulation results of these VBR methods for a 16 kbit/s DDTC coder and for the 16 kbit/s LD-CELP coder proposed for the CCITT recommendation (G.728) are presented.
A method is proposed to allow the retrieval of the identity of the writer of a non-constraint handwritten text by matching it with some reference handwritten documents.
The matching is based on a metric computed on the distributions of the allograph of the letters featuring a unique writing style.
An automatic system segments the text into characters and assigns a partial membership to the different representative prototypes of the considered letter of the Roman alphabet.
Two different datasets are used to assess this system.
Online handwriting is considered by this system.
Two experiments use a procedure developed by Carter and Bradshaw (Speech Communication Vol. 3 (1984) pp. 347–360) to examine the role of syllable structure in speech production.
In the procedure, subjects exchange phonological segments in corresponding positions of a pair of visually-presented words or nonwords and to produce the resulting words or nonwords as quickly as possible.
Carter and Bradshaw have shown that the pattern of latencies mirrors that of frequencies of exchange errors in natural speech.
The first experiment of the present study shows that initial consonant exchanges are promoted by phonetic similarity of the exchanging consonants and they reflect a bias for producing real words.
With these influences controlled, Experiment 2 replicates and extends the finding of Carter and Bradshaw that initial consonant exchanges are made more rapidly than final consonant exchanges.
The discussion relates the latency difference between these conditions to a difference in the “cohesiveness” of initial and final consonants with their vowel.
In particular, in Experiment 2, more than one-third of errors made on final-consonant or vowel exchanges are exchanges of the whole syllable rhyme (VC), whereas just 10% of errors made on initial consonant or vowel exchanges are exchanges of the initial CV of the word.
Various explanations for the difference in cohesiveness are examined in post hoc analyses.
This paper proposes a review of thirty years of the development of demosaicing algorithms used in digital camera for the reconstruction of color image.
Most recent digital camera used a single sensor in front of a color filter array is placed.
This sensor sample a single chromatic value per spatial position and an interpolation algorithm is needed for the definition of a color image with three components per spatial position.
This article shows that the whole signal and image processing technics have been used for solving this problem.
Moreover, a new method proposed recently by the author and collaborators is decribed.
This method based on a model of chromatic sampling by the cones in the retina highlights the nature of spatio-chromatic sampling in digital camera with single sensor.
Nearly four decades of research in speech perception have failed to untangle the relation of sound and phone.
Is this because speech sound discrimination is so complex or unique that it resists study?
Or is it because the right questions have not been asked?
This article reviews some of the more recent speech research at the Pavlov Institute in Leningrad which suggests an affirmative answer to the second question.
This interesting possibility derives from an unique modulation-analyzing model of speech perception.
Wider consideration of the modulation approach might provide some challenging alternatives to current, spectrum-oriented models for solving the “invariance” problem.
Following a general introduction, the model is considered in the context of representative experiments and results.
In this paper, we provide an analysis of Genetic Programming (GP) from the Statistical Learning Theory viewpoint in the scope of symbolic regression.
Firstly, we are interested in Universal Consistency, i.e. the fact that the solution minimizing the empirical error does converge to the best possible error when the number of examples goes to infinity, and secondly, we focus our attention on the uncontrolled growth of program length (i.e. bloat), which is a well-known problem in GP.
Results show that (1) several kinds of code bloats may be identified and that (2) Universal consistency can be obtained as well as avoiding bloat under some conditions.
We conclude by describing an ad hoc method that makes it possible simultaneously to avoid bloat and to ensure universal consistency.
Neural networks are shown to be a class of non-linear adaptive filters, which can be trained permanently with a possibly infinite number of time- ordered examples ; this is an altogether different framework from the usual, non-adaptive training of neural networks.
This paper presents experimental results obtained with an original architecture that can do generic learning for randomly observable factored Markov decision process (ROFMDP).
First, the paper describes the theoretical framework ofROFMDP and the working of this algorithm, in particular the parallelization principle and the dynamic reward allocation process.
Then, the architecture is applied to two navigation problems (gridworld and New York Driving).
The tests show that the architecture allows to learn a good and generic policy in spite of the large dimensions of the state spaces of both systems.
In this theoretical paper, we compare the “classical” learning techniques used to infer regular grammars from positive examples with the ones used to infer categorial grammars.
To this aim, we first study how to translate finite state automata into categorial grammars and back.
We then show that the generalization operators employed in both domains can be compared, and that their result can always be represented by generalized automata, called “recursive automata”.
The relation between these generalized automata and categorial grammars is studied in detail.
Finally, new learnable subclasses of categorial grammars are defined, for which learning from strings is nearly not more expensive than from structures.
Recently proposed connectionist models of acquired linguistic behaviors have linguistic rule-based representations built in.
Similar connectionist models of language acquisition have arbitrary devices and architectures which make them mimic the effect of rules.
Connectionist models in general are not well-suited to account for the acquisition of structural knowledge, and require predetermined structures even to simulate basic linguistic facts.
Such models are more appropriate for describing the formation of complex associations between structures which are independently represented.
This makes connectionist models potentially important tools in studying the relations between frequent behaviors and the structures underlying knowledge and representations.
At the very least, such models may offer computationally powerful ways of demonstrating the limits of associationistic descriptions of behavior.
The «Traité chinois des particules et des principaux termes de grammaire», included in Stanislas Julien's Syntaxe nouvelle de la langue chinoise (1869), represents the first translation of Wáng Yǐnzhī's Jīngzhuàn shìcí 經傳釋詞 (Explanation of Particles in the Classics and the Commentaries, 1819) in a Western language.
If Wáng's work can be considered the most important dictionary of grammatical particles in the Chinese philological tradition, its translation constitutes a remarkable example of presentation of Chinese philological methodologies and linguistic terminology for a European readership.
This article presents and compares the two works, analyzing the ways of translating the entries as well as the ways of transposing linguistic categories and terminology.
This study used a multi-talker database containing intelligibility scores for 2000 sentences (20 talkers, 100 sentences), to identify talker-related correlates of speech intelligibility.
We first investigated “global” talker characteristics (e.g., gender, F0 and speaking rate).
Findings showed female talkers to be more intelligible as a group than male talkers.
Additionally, we found a tendency for F0 range to correlate positively with higher speech intelligibility scores.
However, F0 mean and speaking rate did not correlate with intelligibility.
We then examined several fine-grained acoustic-phonetic talker-characteristics as correlates of overall intelligibility.
We found that talkers with larger vowel spaces were generally more intelligible than talkers with reduced spaces.
In investigating two cases of consistent listener errors (segment deletion and syllable affiliation), we found that these perceptual errors could be traced directly to detailed timing characteristics in the speech signal.
Results suggest that a substantial portion of variability in normal speech intelligibility is traceable to specific acoustic-phonetic characteristics of the talker.
Knowledge about these factors may be valuable for improving speech synthesis and recognition strategies, and for special populations (e.g., the hearing-impaired and second-language learners) who are particularly sensitive to intelligibility differences among talkers.
The development of a high accuracy (about 99%) text-independent speaker recognition system is discussed in this paper in two stages.
The first stage deals with the evaluation of the speaker selectivity characteristics of the various parameter sets that characterize human speech.
The second stage utilizes any two parameter sets of the first stage tests and combines these logically to obtain a significantly higher recognition accuracy than is possible with any single-speaker-sensitive parameter set.
The algorithm utilizes additional utterances to resolve contradictory decisions from the two parameter sets resulting from the first test utterance.
For completeness of discussion a brief review of relevant literature in the field is also presented.
A two-level scheme for speaker identification is proposed.
The first classifier level is based on the self-organizing map (SOM) of Kohonen.
The PDMs are the input for the second classifier level.
The second level consists of multilayer perceptron (MLP) networks for each speaker.
The first level of the classifier is a preprocessing procedure for the second level, where the final classification is made.
The goal of the proposed approach is to combine the advantages of the two type of networks into one classification scheme in order to achieve higher identification accuracy.
The experiments show an increased accuracy of the proposed two-level classifier, especially in the case of noise-corrupted signals.
We define prescription as any intervention in the way another person speaks.
Long excluded from linguistics as unscientific, prescription is in fact a natural part of linguistic behavior.
We seek to understand the logic and method of prescriptivism through the study of usage manuals: their authors, sources and audience; their social context; the categories of “errors” targeted; the justification for correction; the phrasing of prescription; the relationship between demonstrated usage and the usage prescribed; the effect of the prescription.
Our corpus is a collection of about 30 usage manuals in the French tradition.
Eventually we hope to create a database permitting easy comparison of these features.
A topic of importance in reinforcement learning is online value function approximation.
Related algorithms should exhibit some features such as sample efficiency, tracking the solution rather than converging to it (especially because control and learning are interleaved) and maintaining an uncertainty information about approximated values.
A Kalman-based Temporal Differences framework is introduced to deal with all these aspects at the same time.
A form of active learning which uses the available uncertainty information is also introduced, and the proposed framework is compared to state-of-the-art algorithms on classic benchmarks.
Dynamic elastography using ultrasound radiation force is an imaging technique of biological tissues elastic properties.
In a mechanical point of view, biological tissues are supposed isotropic, so their properties are independent of the reference axis.
In these mediums, the tensor of elastic constants can be expressed as a function of two independent constants: the elastic bulk modulus K (which is linked to the compression wave propagation) and the elastic shear modulus μ (which is linked to the shear wave propagation).
The development of some cancers can result in weak variations of the bulk elastic modulus, but can considerably modify the shear elastic modulus.
Measurement of m can then help for the diagnosis of this type of tissue pathology.
A judicious mean to measure this parameter is the use of a non-linear effect called ultrasound radiation force.
This force is proportional to the attenuation and the intensity of the ultrasound beam emitted by the imaging system.
This stress source essentially generates a shear wave that propagates with a velocity proportional to the shear modulus and with a purely transverse polarisation in the far-field (far from the stress source).
Measurement of the medium displacements induced by shear wave propagation can allow to calculate the shear modulus of the medium (inverse problem resolution).
We performed these measurements from the radio-frequency (RF) lines obtained with an imaging ultrasound transducer.
This work describes precisely the signal processing realized on the RF lines.
This processing is based on the use of a delay estimation method to measure temporal delays between RF lines during the shear wave propagation.
Influence of different parameters (length of the analyse window, Signal to Noise Ratio of RF lines, sampling frequency, ultrasound transducer characteristics…) on the measurement precision has been studied.
We present displacement curves as a function of time obtained after optimisation of processing parameters.
Experimental results have been favourably compared to a physical model and allowed us to calculate the shear modulus of the medium.
To estimate the orientation angle, we propose to compute a time-frequency representation of the analytic signal x a (t) the centered squared root of the projection histogram x(t) of the document.
The projection angle corresponding to the histogram with the highest maximum value of its time-frequency representation is considered as an estimation of the document orientation.
The experiments were prepared after a manual orientation of the documents into different angles ranging from −75° to +90°.
We found that the Wigner-Ville distribution reaches the highest estimation rate (100%).
A new method for automatically acquiring Fragments for understanding fluent speech is proposed.
The goal of this method is to generate a collection of Fragments, each representing a set of syntactically and semantically similar phrases.
First, phrases observed frequently in the training set are selected as candidates.
Each candidate phrase has three associated probability distributions: of following contexts, of preceding contexts, and of associated semantic actions.
The similarity between candidate phrases is measured by applying the Kullback–Leibler distance to these three probability distributions.
Candidate phrases that are close in all three distances are clustered into a Fragment.
Salient sequences of these Fragments are then automatically acquired, and exploited by a spoken language understanding module to classify calls in AT&T's “How may I help you?” task.
These Fragments allow us to generalize unobserved phrases.
For instance, they detected 246 phrases in the test-set that were not present in the training-set.
This result shows that unseen phrases can be automatically discovered by our new method.
Experimental results show that 2.8% of the improvement in call-type classification performance was achieved by introducing these Fragments.
Articulatory trajectories of an articulatory model were recovered by means of a genetic algorithm from acoustic information using a task-dynamic model of speech articulation.
Tests on simulated utterances / əbæ/ and / ədæ/ show that the method can recover most of parts of an original trajectory, but it has trouble in obtaining precise timing.
For the recovery of articulation, formant frequency trajectories should be supplemented by additional acoustic information, such as RMS amplitude.
The distinction between what is planned and what is done is well known.
One contrasts the prescribed task to the effective task, the logic of functioning to the logic of use, procedures to practices, etc.
Indeed, there is a solution, thanks to a uniform representation of elements of reasoning and of contexts, called Contextual Graphs.
We propose in this paper a notion of contextualized task model that is an operational intermediate between prescribed and effective tasks.
Using such contextualized task models would lead to developing more robust procedures as shown in four applications.
Subjects naturally integrate auditory and visual information in bimodal speech perception.
To assess the robustness of the integration process, the relative onset time of the audible and visible sources was systematically varied.
In the first experiment, bimodal syllables composed of the auditory and visible syllables /ba/ and /da/ were present at five different onset asynchronies.
The second experiment replicated the same procedure but with the vowels /i/ and /u/.
The results indicated that perceivers integrated the two sources of information at all asynchronies.
Cluster responses (for example, /bda/ given visual /ba/ and auditory /da/) occurred primarily for the consonants but not for the vowels.
In addition, cluster responses require that both the visual and the auditory information be reasonable compatible with the physical properties of a cluster articulation.
For both vowels and consonant-vowel syllables, information from the auditory and visual sources is continuous, independent and combined in a three-stage process of feature evaluation, integration and decision.
Most of object recognition schemes fail in case of illumination changes between the color image acquisitions.
One of the most widely used solutions to cope with this problem is to compare the images by means of the intersection between invariant color histograms.
Unlike the classical invariant color histograms approach which independently analyzes each image, we consider each pair constituted by the query image and one of the target images constructed during the retrieval.
In this paper, we propose a new approach based on color histograms which are adapted to each pair constituted by the query image and one of the target images.
These adapted color histograms are determined so that their intersection is high only when the objects contained in the two images are similar.
The adapted color histograms processing is based on an original model of illumination changes based on the rank measures of the pixels within the color component images.
This paper describes a resource designed for the general study of spontaneous speech under the stress of sleep deprivation.
It is a corpus of 216 unscripted task-oriented dialogues produced by normal adults in the course of a major sleep deprivation study.
The study itself examined continuous task performance through baseline, sleepless and recovery periods by groups treated with placebo or one of two drugs (Modafinil, d-amphetamine) reputed to counter the effects of sleep deprivation.
The dialogues were all produced while carrying out the route communication task used in the HCRC Map Task Corpus.
Pairs of talkers collaborated to reproduce on one partner's schematic map a route preprinted on the other's.
Controlled differences between the maps and use of labelled imaginary locations limit genre, vocabulary and effects of real-world knowledge.
The designs for the construction of maps and the allocation of subjects to maps make the corpus a controlled elicitation experiment.
Each talker participated in 12 dialogues over the course of the study.
Preliminary examinations of dialogue length and task performance measures indicate effects of drug treatment, sleep deprivation and number of conversational partners.
The corpus is available to researchers interested in all levels of speech and dialogue analysis, in both normal and stressed conditions.
In this paper, we describe different multi-microphone noise reduction techniques as front-ends for a speaker-independent isolated word recognizer in an office environment.
Our focus lies on examining the recognition rate if the noise source is not Gaussian and stationary, but a second speaker in the same room.
In this case, standard noise reduction techniques like spectral subtraction fail, whereas multi-microphone techniques can raise the recognition rate by using spatial information.
We compare the delay-and-sum beamformer, superdirective beamformers, and two post-filter systems.
A new adaptive post-filter for superdirective beamformers (APES) is introduced.
Our results show that multi-microphone techniques can increase the recognition rate significantly and that the new APES system outperforms related techniques.
This paper explores the possibility of using automatic speech recognition as a front end to a computer for Chinese character processing.
A speech recognition experiment has been performed with the complete inventory of second-tone syllables of Standard Chinese.
It is shown that the distribution of intrasyllable distances and the distribution of intersyllable distances overlap considerably for the full inventory of 260 second-tone syllables.
The recognition rate was determined as a function of the syllabary size and is 47.3% for the complete syllable inventory.
Vowel formant target frequencies from different talkers depend on the details of the vocal tract, sex, regional accent, speaking habits and other factors.
Good vowel recognition and studies of vowels from different talkers require an accurate method for compensating for speaker differences in these frequencies.
Various methods of compensating for speaker variation in formants were studied.
Bark scaled formants and subtraction of Bark fundamental frequency from the first formant was tried first.
In spite of recent published papers on the efficacy of this technique, it was found inadequate.
The transformations were incapable of improving the clusters of the cardinal vowels, for example.
A modification of the Gerstman technique, determining the speaker's formant range and then transforming into an “ideal” talker's range, was found to account for most of the variance due to different talkers given a small amount of training data.
This technique was applied to vowel in context studies on American English.
Formant ranges were studied for 125 talkers of General American English.
Plots of formant ranges for males and females showed interesting patterns.
The lower limit of the second formant was not very different, while the lower limit of the first formant was lower for males.
Both the first and second formant maxima were larger for females.
The modified Gerstman transformation was able to superimpose the formant targets for the same vowel in the same context from different talkers into the same region of F1, F2 space.
There remained some residual variance between male and female, even after the transformation.
These trends are shown in a series of plots of vowel target frequency data.
The gain portion of a shape-gain quantizer is made adaptive, yielding a vector quantizer that can adjust itself to the time-varying amplitude of a speech signal.
The adaptive version requires negligible increase in computational effort.
Design methods for optimizing the noise-to-signal or arithmetic segmented noise-to-signal ratio are presented; both yield comparable results.
The quantizer performs best on low-frequency portions of speech, due to the smoothness of the waveform there.
Overall performance is comparable to the backward-adaptive quantizer of Chen and Gersho at rate one. and slightly inferior at rate two.
Very small spectral irregularities may be detected by the auditory system, which suggests the existence of a kind of frequency derivation-like mechanism.
On the other hand, the “center of gravity” phenomenon suggests the existence of an integration-like mechanism.
Our goal here is to study these two facts through the determination of psychophysical estimates of the internal representations of static harmonic spectra.
It is commonly agreed that the pulsation threshold technique provides estimates of the results of the peripheral spectral analysis of such signals, accounting for the lateral suppression phenomena.
The first part of this study consisted of the determination of a good experimental procedure for such a task.
We report some observations we made about the pulsation threshold test and the solutions adopted.
Then we present a preliminary set of results showing how the internal representation of a one-formant static sound is modified by the emergence of a second formant or by the value of the lateral slopes of the formant.
The main conclusion is that smoothing of the spectrum due to non-infinite frequency selectivity dominates the lateral suppression effects in both cases.
Word preselection is achieved by segmenting and classifying the input signal in terms of 6 broad phonetic classes.
In the second pass, word verification, a detailed representation of the phonemic structure of word candidates is used for estimating the most likely words.
Each word candidate is modeled by a graph of subword Hidden Markov Models.
Again, a tree-structure of the whole word subset is built online for an efficient implementation of a beam-search Viterbi algorithm that estimates the likelihood of the candidates.
The results show that a complexity reduction of about 73% can be achieved by using the two pass approach with respect to the direct approach, while the recognition accuracy remains comparable.
COMPOST consists of a specialised programing language for text-to- speech synthesis.
Each text-to-speech system describes the different steps which convert a running text towards its acoustic and/or visual synthesis by means of a main program called a scenario.
The ideas presented in this article are enlighted by concrete examples extracted from a text-to-speech system for French developped using COMPOST.
Previous research has shown that, in a phoneme detection task, vowels produce longer reaction times than consonants, suggesting that they are harder to perceive.
Another way of accounting for the findings would be to relate them to the differential functioning of vowels and consonants in the syllabic structure of words.
In this experiment, we examined the second possibility.
Targets were two pairs of phonemes, each containing a vowel and a consonant with similar phonetic characteristics.
Subjects heard lists of English words had to press a response key upon detecting the occurrence of a pre-specified target.
This time, the phonemes which functioned as vowels in syllabic structure yielded shorter reaction times than those which functioned as consonants.
This rules out an explanation for response time difference between vowels and consonants in terms of function in syllable structure.
Instead, we propose that consonantal and vocalic segments differ with respect to variability of tokens, both in the acoustic realisation of targets and in the representation of targets by listeners.
An amplitude-domain quotient for parametrization of the glottal source computed by inverse filtering is presented.
The new quotient, AQ, is determined as the ratio between the amplitude of the AC-flow of the glottal waveform and the amplitude of the minimum of the flow derivative.
This quotient can be used even though absolute flow values are not given by the recording equipment.
The behaviour of AQ was compared to conventional time-based quotients by analysing voices produced by different phonation types.
It was shown that phonation types can be quantified effectively when parametrization of the glottal flow estimated by inverse filtering is based on AQ.
The ability of two bottlenosed dolphins (Tursiops truncatus) to understand imperative sentences expressed in artificial languages was studied.
One dolphin (Phoenix) was tutored in an acoustic language whose words were computer-generated sounds presented through an underwater speaker.
The second dolphin (Akeakamai) was tutored in a visually-based language whose words were gestures of a trainer's arms and hands.
The words represented agents, objects, object modifiers, and actions and were recombinable, according to a set of syntactic rules, into hundreds of uniquely meaningful sentences from two to five words in length.
The sentences instructed the dolphins to carry out named actions relative to named objects and named modifiers; comprehension was measured by the accuracy of response to the instructions and was tested within a format that controlled for context cues, for other nonlinguistic cues, and for observer bias.
The successful processing of either a left-to-right grammar (Phoenix) or of an inverse grammar (Akeakamai) indicated that wholly arbitrary syntactic rules could be understood and that an understanding of the function of words occuring early in a sentence could be carried out by the dolphin on the basis of succeding words, including in at least one case, nonadjacent words.
The comprehension approach used was a radical departure from the emphasis on language production in studies of the linguistic abilities of apes; the result obtained offer the first convincing evidence of the ability of animals to process both semantic and syntactic features of sentences.
The ability of the dolphins to utilize both their visual and acoustic modalities in these tasks underscored the amodal dependency of the sentence understanding skill.
Some comparisons were given of the dolphins' performances with those of language-trained apes and of young children on related or relevant language tasks.
Many researches focus on the study of automatic sign language recognition.
Many of them need a large amount of data to train the recognition systems.
Our work addresses the annotation of sign language video corpus in order to collect training data.
We propose a robust tracking algorithm for hands and head, a method to segment hands during occlusions and an approach to segment gestures using motion and hand shape features.
In order to show the advantages and limitations of the proposed approaches, we have evaluated each one using international corpus.
The full sign segmentation approach shows promising results.
Therefore, in a continuous speech recognizer (CSR) it would appear profitable to train separate models for the stressed and unstressed variants of each vowel.
In the experiments reported on here, we applied stress modeling in both training and testing of the recognizer.
Recognition experiments on an independent test set showed that recognition rates did not improve by this use of stress in our CSR.
However, if we swapped the stress markers in the recognition lexicon the recognition rates did significantly deteriorate.
This demonstrated that the acoustic models for the stressed and unstressed variants of the vowels were different.
A pitfall in this experiment was that lexical stress information and phonemic context were possibly confounded.
In a follow-up experiment we controlled for context by using generalized context-dependent models.
In this experiment the recognition results were not improved either, although the vowel models were better tailored to capture lexical stress-related information.
We conclude that the mapping of lexical stress to the acoustic surface of fluent speech is not sufficiently straightforward to be of direct benefit for CSR, due to interaction of lexical stress with rhythm and sentence accent in real speech.
In this paper, we present a new approach towards high performance speech/music discrimination on realistic tasks related to the automatic transcription of broadcast news.
In the approach presented here, an artificial neural network (ANN) trained on clean speech only (as used in a standard large vocabulary speech recognition system) is used as a channel model at the output of which the entropy and “dynamism” will be measured every 10 ms.
These features are then integrated over time through an ergodic 2-state (speech and non-speech) hidden Markov model (HMM) with minimum duration constraints on each HMM state.
For instance, in the case of entropy, it is indeed clear (and observed in practice) that, on average, the entropy at the output of the ANN will be larger for non-speech segments than speech segments presented at their input.
In our case, the ANN acoustic model was a multi-layer perceptron (MLP, as often used in hybrid HMM/ANN systems) generating at its output estimators of the phonetic posterior probabilities based on the acoustic vectors at its input.
It is from these outputs, thus from “real” probabilities, that the entropy and dynamism are estimated.
The 2-state speech/non-speech HMM will take these two-dimensional features (entropy and dynamism) whose distributions will be modeled through multi-Gaussian densities or a secondary MLP.
The parameters of this HMM are trained in a supervised manner using Viterbi algorithm.
Although the proposed method can easily be adapted to other speech/non-speech discrimination applications, the present paper only focuses on speech/music segmentation.
Different experiments, including different speech and music styles, as well as different temporal distributions of the speech and music signals (real data distribution, mostly speech, or mostly music), illustrate the robustness of the approach, always resulting in a correct segmentation performance higher than 90%.
Finally, we will show how a confidence measure can be used to further improve the segmentation results, and also discuss how this may be used to extend the technique to the case of speech/music mixtures.
In this paper, an algorithm dedicated to image restoration and edge detection is addressed.
Its principle is based on synchronous stochastic relaxation and resistive fuses.
The adequacy to both image enhancement and the implementation on a cellular architecture are considered.
Results of simulations are given and demonstrate the efficiency of our algorithm even on very noisy images.
A synthesis-based method for pitch extraction of the speech signal is proposed.
The method synthesizes a number of log power spectra for different values of fundamental frequency and compares them with the log power spectrum of the input speech segment.
The average magnitude (AM) difference between the two spectra is used for comparison.
The value of fundamental frequency that gives the minimum AM difference between the synthesized spectrum and the input spectrum is chosen as the estimated value of fundamental frequency.
The voiced/unvoiced decision is made on the basis of the value of the AM difference at the minimum.
For synthesizing the log power spectrum, the speech signal is assumed to be the output of an all-pole filter.
The transfer function of the all-pole filter is estimated from the input speech segment by using the autocorrelation method of linear prediction.
The synthesis-based method is tried out on real speech data and the results are discussed.
In automatic speaker recognition tasks a situation may occur in which it cannot be assumed that a voice to be recognized belongs to a known set of voice classes (a closed set).
Thus, a problem arises with respect to working out a recognition algorithm that could operate in open sets of speakers, i.e. without assuming that a speech sample from an unknown speaker must belong to one of the speakers of a given set.
Two “key words” were examined, as were different parameter sets and large and small populations of speakers.
The methodological assumptions and experimental results show that the proposed open set voice recognition method is very flexible and makes it possible to adjust global characteristics, i.e. α and β errors, to the strategy adopted by the recognition system.
For a given set of voice patterns, it is always possible to optimize recognition by a proper selection of approximations of the ground class distribution, i.e. by an appropriate selection of decision thresholds.
We are interested in providing automated services via natural spoken dialog systems.
By natural, we mean that the machine understands and acts upon what people actually say, in contrast to what one would like them to say.
There are many issues that arise when such systems are targeted for large populations of non-expert users.
In this paper, we focus on the task of automatically routing telephone calls based on a user's fluently spoken response to the open-ended prompt of “How may I help you?”.
We first describe a database generated from 10,000 spoken transactions between customers and human agents.
We then describe methods for automatically acquiring language models for both recognition and understanding from such data.
Experimental results evaluating call-classification from speech are reported for that database.
These methods have been embedded within a spoken dialog system, with subsequent processing for information retrieval and formfilling.
Visualization methods do not scale well with high number of features.
We present an approach using a consensus theory based feature selection (CTBFS) algorithm, clustering for sampling and visualization for weight assignment in order to aggregate multivariate and multidimensional datasets.
We use datasets available in UCI and Kent Ridge Bio Medical Dataset Repositories in order to evaluate the performance of our new approach.
Over the last few years, the DARPA-sponsored Hub-4 continuous speech recognition evaluations have advanced speech recognition technology for automatic transcription of broadcast news.
In this paper, we report on our research and progress in this domain, with an emphasis on efficient modeling with significantly fewer parameters for faster and more accurate recognition.
In the acoustic modeling area, this was achieved through new parameter tying, Gaussian clustering, and mixture weight thresholding schemes.
The effectiveness of acoustic adaptation is greatly increased through unsupervised clustering of test data.
In language modeling, we explored the use of non-broadcast-news training data as well as the adaptation to topic and speaking styles.
We developed an effective and efficient parameter pruning technique for backoff language models that allowed us to cope with ever increasing amounts of training data and expanded N-gram scopes.
Finally, we improved our progressive search architecture with more efficient algorithms for lattice generation, compaction, and incorporation of higher-order language models.
Decision rules resulting from a knowledge discovery in databases process are filtered by means of automated processes.
This important stage of the analysis aims at reducing the number of rules which are presented to the expert for a "close" évaluation and is based on the use of indexes which give a quantitative évaluation of the quality of the knowledge.
The search ofa good knowledge représentation is directly linked to the search and the use of good indexes.
In this article, we present the problem of the relevant choice of an index by an expert user who has some preferences on the characteristics of the index.
We show how it is possible tofind a good compromise by means of a "multicriteria decision aiding" approach.
The relationship between different levels of sentence representation and vocabulary types was investigated.
The experiment used a word-monitoring task, which varied the target's word class (open and closed) as well as the functional role of different closed class elements (lexical prepositions, obligatory prepositions, and verb particles).
The presence of different context sentence (semantically related/unrelated) was used to estimate the effect of preceding semantic and/or syntactic information on the different types of items.
The combined results of normal and agrammatic subjects provide evidence for a computational distinction of different vocabulary types, and consequently, their attribution to different levels of sentence processing.
Furthermore, they suggest that lexical and non-lexical information is generally processed at different levels, even if both types of information are carried by one item.
We propose a new approach of contrast in digital image using a multiresolution framework.
Contrast enhancement is the main application by mean of an iterative process.
Moreover, we show that when iterated, this process ends up with a simplified, e.g. binary, image from any initial image.
Several examples are presented all over the paper showing the performance of our algorithm on synthetic as well as real scenes.
The semantic networks that are built today are called ontologies.
This choice rests not only on the tradition of Logic, but also on Metaphysics.
Unfortunately, the vocabularies of natural languages are not shaped like ontologies: semantic relations are in fact much more complex and varied than ontologic relations.
To build a " de-ontology ", we have to take into account the diversity of discourses and genres, because a unique ontology remains illusory ; we have to insist on the problem of semiotic heterogeneity of texts, on complex correlations beetween meanings and expressions, on the role of context.
These are some requirements to fulfil characterization tasks, especially in corpus linguistics.
Region oriented image representation offers several advantages over block-oriented schemes, e.g. adaptation to the local image characteristics, or object motion compensation as opposed to block-wise motion compensation.
For the task of image data compression, i.e. image coding, new algorithms are needed which work on arbitrarily shaped image regions, called segments, instead of rectangular image blocks.
Based on a generalized moment approach, the luminance function inside the segment is approximated by a weighted sum of basis functions, for example polynomials.
A set of basis functions which is orthogonal with respect to the shape of the segment to be coded can be obtained using orthogonalization schemes.
This results in the derivation of a generalized shape-adapted transform coder.
Suitable coder and decoder structures are introduced which do not necessitate the transmission of the basis functions for each segment.
Finally an application of the derived algorithms to image sequence coding at low data rates is shown, which is based on a segmentation of the motion compensated prediction error image.
This paper describes the test methodology employed for the performance evaluation of the CCITT 16 kbit/sLD-CELP coder (Recommendation G.728) with non-voice signals.
This methodology is sufficiently general that it can be employed for the evaluation of other low transmission-rate digital speech coders.
The types of non-voice signals considered in this paper include voiceband data, network signaling, circuit continuity tones and dual-tone multi-frequency signaling.
In this study a new scaling technique is presented which makes it possible to estimate the voice source including its amplitude values by inverse filtering the speech pressure waveform without applying a flow mask.
The new technique is based on adjusting the DC-gain of the vocal tract model in inverse filtering to unity.
The performance of the new method is tested by analysing correlation between the minimum peak amplitude of the differentiated glottal flow given by the new technique and the sound pressure level of speech.
The results show that the new method yields reliable information of the amplitude values of the glottal source without applying a flow mask.
The mismatch between the acoustic conditions during training and recognition often causes a performance deterioration in practical applications of speech recognition systems.
Two important effects are the presence of a stationary background noise and the frequency response of the transmission channel from the speaker to the audio input of the recognizer.
The original contribution of this work are two signal processing schemes for the estimation of the actual noise spectrum and the difference of the frequency responses between training and recognition.
The estimated noise components are taken to adapt the cepstral parameters of the recognizer's references which are described by hidden Markov models (HMMs).
The adaptation process is based on the parallel model combination (PMC) approach (M.J.F. Gales, Model based techniques for noise robust speech recognition, Dissertation at the University of Cambridge, 1995).
For speaker independent connected or isolated word recognition considerable improvements can be achieved in the presence of just one type of noise as well as in the presence of both types together.
Furthermore this adaptation scheme is integrated as part of a complete dialogue and recognition system which is accessible via the public telephone network.
The usability and the gain in recognition performance is shown for this application in a real telecommunication scenario under consideration of all real-time aspects.
The SPELL workstation is intended to be a teaching device aimed at intermediate ability foreign language learners.
Audio and visual aids will be used to help students improve their general intelligibility within a basic teaching paradigm called DELTA (Demonstrate, Evaluate Listening, Teach and Assess).
Prosodic analysis will apply to the features of intonation, stress and rhythm.
A phonological approach is used for intonation which provides a well-structured system of contrasting units that correlate with discrete linguistic functions.
A more limited approach to the prosodic phonology of stress and rhythm will be taught in the SPELL system by manipulating the relatively simple acoustic features of vowel quality and segmental duration.
The micro feature analysis will focus on the segmental class of vowels.
A distinctive feature approach is used to characterize non-native vowel pronunciation.
Acoustic properties are sought which will be speaker-independent.
This study begins to explore the importance of the physiological domain in voice transformation.
A general approach is outlined for transforming the voice quality of sentence-level speech while maintaining the same phonetic content.
Transformations will eventually include gender, age, voice quality, emotional state, disordered state, dialect or impersonation.
In this paper, only a specific voice quality, twang, is described as an example.
The basic question is: relative to pure signal processing, can voices be transformed more effectively if biomechanical, acoustic and anatomical scaling principles are applied?
At present, two approaches are contrasted, a Linear Predictive Coding approach and a biomechanical simulation approach.
Mining knowledge from structured data has been extensively addressed in the few past years.
Structure of these objects is irregular and it is clever to think that a query on documents structure is almost as important as a query on data.
Moreover, manipulated data is not static because new updates are constantly realised.
Problem of maintaining such substructures is then prior on researching them because, every time data is updated, found substructures could become invalid.
In this paper we propose a system, called AUSMS. (Automatic Update Schema Mining System), allowing data retrieval, researching frequent sub-structures and maintaining extracted knowledge after sources evolutions.
In this article I examine the correspondences found between Western Old Japanese high vowels and Eastern Old Japanese midvowels in light of the recent hypotheses concerning the Proto-Japonic vowel system.
Correspondences in both the morphology and the lexicon are established and then comparative evidence from several modem Japanese and Ryukyuan dialects is adduced to show that these are instances of retention of Proto-Japonic *e and *o.
This paper addresses the problem of regression in the case of non-uniform sampled signals.
Our method is based on supervised learning theory, we propose to use L2 estimation with wavelet kernels combined with L1 multiscale regularization.
The use of Least Angle Regression as solver enable us to propose new solutions to set the regularization parameter.
In heterogeneous databases, images often provided from different sources and belong to different topics, hence there is a need for a large description to ensure efficient representation of their content.
However, extracted features are not always adapted to the considered image database.
In this paper we propose a new image recognition approach based on two innovations, namely adaptive feature selection and Multi-Model Classification Method (MC-MM).
The adaptive selection considers only the most adapted features with the used image database content.
The MC-MM method ensures image recognition using hierarchically selected features.
Experimental results confirm the effectiveness and the robustness of our proposed approach.
Previous work has shown the ability of Srtificial Neural Networks (ANNs), and Multilayer Perceptrons (MLPs) in particular, to estimate a posteriori probabilities that can be used, after division by the a priori probabilities of the classes, as emission probabilities for Hidden Markov Models (HMMs).
The advantages of aspeech recognition system incorporating both MLPs and HMMs are the best discrimination and the ability to incorporate multiple sources of evidence (features, temporal context) without restrictive assumptions of distributions or statistical independence.
While this approach has been shown useful for speech recognition, it is still important to understand the underlying problems and limitations and to consider its consequences on other algorithms.
For example, while state of the art HMM-based speech recognizers now model context-dependent phonetic units such as triphones instead of phonemes to improve their performance, most of the MLP-based approaches are restricted to phoneme models.
After a short review, it is shown here how such neural network approaches can be generalized to context-dependent phoneme models.
Also, it is discussed how previous theoretical results can affect the development of other algorithms like nonlinear Autoregressive (AR) Models and Radial Basis Functions (RBFs).
The authors propose a novel way to implement a speech dialogue system.
The method, called Simultaneous Understanding, accomplishes speech recognition, understanding and action simultaneously with the user's utterance.
This makes the dialogue system more interactive, because of the following advantages: the user does not have to wait for the system response, unless he wants to see the action results on the screen. If he sees a mis-recognized entry, he can correct it before the database is accessed.
The authors implemented two ticket reservation speech dialog systems, using an existing speech recognition system.
One of them was based on the above method, which can accept new inputs while simultaneously analyzing the previous utterance, and the other accepts new inputs after analyzing the previous utterance.
It was found that the above method improved the total average utterance accuracy by 5.4% and the total time spent to solve tasks by 4.7%.
This shows that the new method is promising for increasing the interactiveness for the speech dialogue system.
Can supra-normal spectral contrast in an acoustical speech signal compensate for the degraded frequency selectivity accompanying hearing losses of cochlear origin?
Subjects were adults either with audiometrically-normal hearing or with moderate hearing impairments of cochlear origin.
For both groups of listeners, accuracy of identification decreased as formant bandwidth increased.
In the mean data of both groups for syllable-final identification, there was a tendency for accuracy to increase when format bandwidths were set to half their nominally normal values.
Psychoacoustical measures of frequency selectivity correlated robustly with accuracy of identification, but again only for final consonants.
These results suggest that factors in addition to reduced frequency resolution and reduced absolute sensitivity set limits on the accuracy of speech identification in cases of cochlear hearing loss, particularly for syllable-initial consonants.
Possible candidates, not explored here, are reduced temporal resolution and an increased susceptibility to backward masking.
The very limited benefits for speech identification which resulted from reducing formant-bandwidths may reflect the inability of that transformation to compensate for impairments to temporal auditory processing, as well as for extreme impairments to frequency resolution.
Although Genevan research has provided a detailed analysis of cognitive structures, our knowledge of cognitive processes remains fragmentary.
The focus is now not only on macro-development but also on changes occurring in children's spontaneous action sequences in micro-formation.
A series of experiments designed to study goal-oriented behavior is in progress.
This paper describes the action sequences of 67 subjects between 4;6 and 9;5 years in a block balancing task.
It is not a study of children's understanding of a specific notion in physics, but an attempt to pave the way towards understanding the more general processes of cognitive behavior.
The results also suggest certain functional rather than structural analogies between the acquisition of physical knowledge and the acquisition of language.
In automatic speech understanding, division of continuous running speech into syntactic chunks is a great problem.
Syntactic boundaries are often marked by prosodic means.
For the training of statistical models for prosodic boundaries large databases are necessary.
For the German Verbmobil (VM) project (automatic speech-to-speech translation), we developed a syntactic–prosodic labelling scheme where different types of syntactic boundaries are labelled for a large spontaneous speech corpus.
This labelling scheme is presented and compared with other labelling schemes for perceptual–prosodic, syntactic, and dialogue act boundaries.
The main advantage of the rough syntactic–prosodic labels presented in this paper is that large amounts of data can be labelled with relatively little effort.
The classifiers trained with these labels turned out to be superior with respect to purely prosodic or syntactic labelling schemes, yielding recognition rates of up to 96% for the two-class-problem `boundary versus no boundary'.
Simple articulatory contrasts for a phonological opposition generate a multiplicity of acoustic cues.
Knowledge of the covariations of acoustic pattern features within and across speakers is needed for automatic speech recognition.
A model of speech production processes was used to generate stimuli along an articulatory continuum, the degree of abduction of the vocal folds for the fricatives in English words “hiss” and “his”.
Transition times imposed strong constraints on the articulatory plans devised as inputs to the model.
Acoustic segment durations output from the model covaried in the same way as those produced by 5 real speakers; there was good quantitative agreement in most cases.
Listeners' responses suggest that the articulatory dimension synthesised is a suitable one for natural speech.
Future directions for the modelling of multiple articulatory dimensions and for mapping from speaker-specific patterns of articulation and their perturbations onto the stability of particular acoustic cues are discussed.
This paper describes a method for automatic annotation of prosodic events in speech, using segmental duration information.
It details a way of differentiating prominence-related lengthening from boundary-related lengthening, using durational clues alone, and discusses an anomaly in the phrasing characteristics of four speakers' readings of 200 phonetically-balanced sentences.
An algorithm is described that uses syllable-level differences in normalised segmental duration measures to detect prosodic boundaries in a speech signal.
Tests with read-speech data from four British-English RP speakers show high agreement between speakers with respect to the number of boundaries detected and the length of the phrases delimited by each pair of boundaries, but the correlation between speakers on actual boundary locations is low.
There is particular disagreement between speakers in the case of a single function word linking two groups of content words.
This discrepancy can be resolved if the boundary is taken to be at the function word location itself, rather than at one or other side of the word.
These results are taken to indicate some freedom in the placement of prosodic boundaries in such cases, sometimes being cued by a syntactic boundary, and sometimes by a rhythmic one.
Native speakers of Japanese may be unable to correctly identify the phonemes /l/ and /r/ in spoken English.
Nevertheless, in perceiving English utterances, they, like native speakers of English, respond to the different acoustic patterns which convey /l/ and /r/ as if they are sensitive to differences in the vocal tract movements that convey /l/ and /r/.
Support for this conclusion is provided by a study in which native speakers of Japanese and native speakers of English labelled stimuli along a synthetic /da/-/ga/ continuum when the stimuli were preceded by natural tokens of /s/ or /∫/, /al/ or /ar/.
Each pair of precursors had contrasting effects on the location of the category boundary between /da/ and /ga/, and neither the direction nor the extent of contrast depended on native language experience.
Significantly, /al/ gave rise to more /ga/ percepts than /ar/ for Japanese and English speakers alike, regardless of their ability to identify /al/ and /ar/, as such.
Interpretation of these results rests on previous observations that the contrasting perceptual effects of /al/ vs. /ar/ and /s/ vs. /∫/ find parallels in the acoustic structure of natural utterances of /al-da/, /ar-da/ etc., due to coarticulation of the vocal tract movements that convey the preceding consonant and those that convey the following /da/ or /ga/.
Apparently, native speakers of Japanese can be sensitive to the acoustic consequences of coarticulating /l/ or /r/ with /d/ or /g/ while being unable to categorize /l/ and /r/ as different phonemes.
Preceding a language-specific level of perception where speech sounds are represented in accordance with the constraints of a given phonological system, there may exist a universally-shared level where the representation of speech sounds more closely corresponds to the articulatory gestures that give rise to the speech signal.
An assessment of a target-based control model of speech production using Feldman's Equilibrium Point Hypothesis is presented.
It consists of simulations of articulatory movements during Vowel-to-Vowel sequences with a 2D biomechanical tongue model.
In the model the main muscles responsible for tongue movements and tongue shaping in the mid-sagittal plane are represented.
The elastic properties are accounted through a Finite-Element modeling, while force generation principles are implemented according to the non-linear force-length Invariant Characteristics proposed by Feldman.
Movement is produced through control variable shifts at rates that are constant throughout each transition.
The external contours of the model are adjusted to approximate X-ray data collected on a native speaker of French, and it is inserted in the vocal tract contours of the speaker.
Emphasis is put on the realism of synthesized formant trajectories, and on the potential influence of biomechanical tongue properties on to measurable kinematic features.
This paper provides a `snapshot' of telephony based speech technology research within European telecommunications companies.
Previous research (Pind, 1986, 1995a) has shown that the ratio of vowel to rhyme (vowel + consonant) duration is a major cue for quantity in Icelandic and serves as a higher-order invariant which enables the listener to disentangle those durational transformations of the speech signal which are due to changes in the speaking rate from those which involve a change of phonemic quantity.
For the listener to be able to calculate this ratio, both segments, vowel and consonant, need to be present in the acoustic waveform.
This paper reports two perceptual experiments using edited natural speech where a closure following the vowel is either audible or not (in the latter case the closure is unreleased).
Results support the hypothesis that the surrounding context will have a greater effect on the location of the phoneme boundaries for vowel quantity in the unreleased syllables since in that case the listener will not be able to calculate the vowel to rhyme ratio.
An experiment has been performed where various two-formant models reported in the literature were assessed as to their ability to predict the formant frequencies obtained in a vowel identification task.
An alternative model is proposed in which the auditory processing of vowel sounds is assumed to take place in two stages: a peripheral processing stage and a central processing stage.
In the peripheral stage the speech spectrum is transformed to its auditory equivalent and the formant frequencies are extracted from this spectrum using a peak-picking mechanism.
The central stage performs a two-formant approximation on the results of the first stage operation, and it is this formant pair that vowel identification is taken to operate on during vowel perception.
The first and second formant frequencies of this two-formant model are taken to be equal to the first and second formant frequencies extracted at the first stage plus a perturbation term which accounts for the interaction effects of the neighbouring formants.
The perturbation caused by each of these neighbouring formants is inversely proportional to its separation from the main formants.
This model compares favourably with previous models in its prediction of the formant frequencies obtained from the vowel identification task.
This paper is concerned mainly with the choice of a figure of merit for representing the performance of connected-word recognisers when DP word-symbol sequence matching is used for the scoring.
Properties of the DP scoring method are discussed.
Experimental tests using data from the DARPA Resource Management Task confirm a prediction made from random number simulations that DP scoring overestimates substitution errors and underestimates insertion and deletion errors.
As a result, the commonly used total error measure has a particularly large bias.
The use of an alternative measure, percent correct, results in lower bias but ignores insertion errors.
A new figure of merit, weighted total errors, takes all three kinds of errors into account and minimises bias.
Finally, some more sophisticated figures of merit are discussed briefly.
The usual request in this framework is the computation of an optimal policy that defines an optimal action for every state of the system.
In this paper, we propose a method for refining near optimal policies via online search techniques, by developping from each current state a randomly sampled look-ahead tree.
We show on a navigation problem modeled as a stochastic shortest path that this online search strategy provides good “anytime” profiles.
The method exposed in this paper represents a new edge-detection tool of a grey-level image by the cooperation of two technics: wavelet decomposition and neural networks.
The first part recalls the necessary background on mono and bidimensional wavelet decomposition and their main properties.
The difficult phase of the algorithm lies in the optimal recomposition of different resolutions, in the aim to obtain thin and noiseless edges.
This work is given to a neural network which constitutes the object of the second part.
The main interest of this new method is to give good results with images whose caracteristics are completly different, without to modify any parameters.
This paper explains how visual information from the lips and acoustic signals can be combined together for speech segmentation.
The psychological aspects of lip-reading and current automatic lip-reading systems are reviewed.
The paper describes an image processing system which can extract the velocity of the lips from image sequences.
The velocity of the lips is estimated by a combination of morphological image processing and block matching techniques.
The resultant velocity of the lips is used to locate the syllable boundaries.
This information is particularly useful when the speech signal is corrupted by noise.
The paper also demonstrates the correlation between speech signals and lip information.
Data fusion techniques are used to combine the acoustic and visual information for speech segmentation.
The principal results show that using the combination of visual and acoustic signals can reduce segmentation errors by at least 10.4% when the signal-to-noise ratio is lower than 15 dB.
In the framework of an ANN/HMM hybrid system for phone recognition three specialized ANNs were designed and evaluated.
One of these ANNs detects the manner of articulation.
The other two ANNs describe the speech signal in terms of place of articulation.
The design of these networks was inspired by acoustic-phonetic knowledge.
Input parameters, ANN topology and desired output representation have been optimized for the specific task of the network.
Experiments are reported for the TIMIT database.
Frame classification errors of 17.7% with the manner ANN (5 broad classes), 25.4% with the plosive and nasal ANN (10 phones), and 25.2% with the fricative ANN (11 phones) were obtained on a set of 616 sentences from 77 new speakers.
Experiments for a prototype ANN/HMM hybrid system are also reported.
We developed an algorithm for the global optimization of this hybrid system.
The network for the manner of articulation and one network for the place of articulation were merged to a single ANN which outputs were modeled by an HMM.
With this globally optimized hybrid system we achieved a recognition accuracy of 86% on an 8 class recognition problem (7 plosives and one class corresponding to all other phonemes).
In this article, we present a new method for the selection of the most pertinent variables to set at the input of a MLP (Multi Layer Perceptron) or RBF (Radial Basis Function) neural network.
This method relies on the statistical analysis of the derivatives of the outputs of the network with respect to the inputs.
When all the derivates of the outputs with respect to a given input are statistically zeros, the influence of this input is neglectable ans this input may be eliminated.
Illustrations will be provided on several simulated or real temporal series prediction models.
This study brings a contribution to non-gaussian textures classification thanks to signatures extracted from high order correlations and/or spectra.
The first part exhibits that most of the textures present a non gaussian statistic, well represented for the third order.
The chosen normality tests, the Skewness test and the Kurtosis test, have been calibrated for this study.
To lower the complexity of algorithms and yet to reach low estimation variances, we choose to focus on third order correlations with the stationary hypothesis: bicorrelation, bispectrum and bicorspectrum.
A Row/columns representation of images has been selected.
We propose in the second part characterization based on representations describing third order spatial correlations.
The last part concerns textures classification using the previous features.
Discrimination performances on Mine Brodatz textures are presented and comparison with the results of a coocurence matrix method are provided.
To date most theories of reading ability have emphasized a single factor as the major source of individual differences in performance.
However there has been little agreement on what that factor is.
However, candidates have included visual discrimination, phonological and semantic recoding, short-term memory, and utilization of linguistic knowledge and context.
The single- factor theories are summarized.
It is concluded that more complex, multifactor models of reading ability are required, and some recent attempts to collect data conducive to such a model are described.
Two methods of conducting component skills analysis are presented, and it is recommended that they be used as converging operations.
Finally, the results of a component skills analysis are used to construct a tentative example of a class of hierarchical models of reading ability that can be pursued developmentally.
Pioneering research by Chistovich and her colleagues used speech shadowing to study the mechanisms of immediate speech processing, and in doing so exploited the phenomenon of close shadowing, where the delay between hearing a speech stimulus and repeating it is reduced to 250 msec or less.
The research summarised here began with an extension of Chistovich's findings to the close shadowing of connected prose.
Twenty-five percent of the women tested were able to accurately shadow connected prose at mean delays ranging from 250 to 300 msec.
The other women, and all the men tested, were only able to do so at longer latencies, averaging over 500 msec.
There are called distant shadowers.
A second series of experiments established that close, just as much as distant shadowers, were syntactically and semantically analysing the material as they repeated it.
This was reflected in the ways their spontaneous errors were constrained, and in their sensitivity to disruptions of the syntactic and semantic structure of the materials they were shadowing.
A third series of experiments showed that the difference between close and distant shadowers was in their output strategy.
Close shadowers are able to use the products of on-line speech analysis to drive their articulatory apparatus before they are fully aware of what these products are.
This means that close shadowing not only provides a continuous reflection of the outcome of the process of language comprehension, but also does so relatively unaffected by post-perceptual processes.
In this sense, therefore, close shadowing provides us with uniquely privileged access to the properties of the system.
Recent advances in artificial intelligence have changed the fundamental assumptions upon which the progress of computer-aided process engineering (modeling and methodologies) during the last 30 yr has been founded.
Thus, in certain instances, numerical computations today constitute inferior alternatives to qualitative and/or semi-quantitative models and procedures which can capture and utilize more broadly- based sources of knowledge.
In this paper it will be shown how process development and design, as well as planning, scheduling, monitoring, analysis and control of process operations can benefit from improved knowledge-representation schemes and advanced reasoning control strategies.
It will also be argued that the central challenge coming from research advances in artificial intelligence is “modeling the knowledge”, i.e. modeling: (a) physical phenomena and the systems in which they occur; (b) information handling and processing systems; and (c) problem-solving strategies in design, operations and control.
Thus, different strategies require different forms of declarative knowledge, and the success or failure of various design, planning, diagnostic and control systems depends on the extent of actively utilizable knowledge.
Furthermore, this paper will outline the theoretical scope of important contributions from AI and what their impact has been and will be on the formulation and solution of process engineering problems.
In this article a new shape descriptor - based on minimal graphs - is proposed and its properties are checked through the problem of graphical symbols recognition.
Recognition invariance in front shift and multi-oriented noisy object was studied in the context of small and low resolution binary images.
The approach seems to have many interesting properties, even if the construction of graphs induces an expensive algorithmic cost.
In order to reduce time computing an alternatively solution based on image compression concepts is provided.
The recognition is realized in a compact space, namely the Cosine Discrete space.
The use of blocks discrete cosine transform is discussed and justified.
The experimental results led on the GREC2003 database show that the proposed method is characterized by a good discrimination power, a real robustness to noise with an acceptable time computing.
The book raises several questions about the very notion of punctuation that is developed in it.
This critical review draws attention to several confusions noted in some of the contributions.
It also proposes a counter-study on a paleographic problem concerning the question mark and, finally, because the term “punctuation” seems to have been used in a too broad sense, wishes to add nuances to what appears to be a terminological drift.
This paper describes an automatic formant tracking algorithm incorporating speech knowledge.
It operates in two phases.
The first detects and interprets spectrogram peak lines in terms of formants.
The second uses an image contour extraction method to regularise the peak lines thus detected.
Speech knowledge served as acoustic constraints to guide the interpretation of peak lines.
The proposed algorithm has the advantage of providing formant trajectories which, in addition to being sufficiently close to the spectral peaks of the respective formants, are sufficiently smooth to allow an accurate evaluation of formant transitions.
The results obtained highlight the interest of the proposed approach.
There is a view that the fundamental processes involved in word recognition might somehow be different for speech and print.
We argue that this view is unjustified, and that the models of lexical access developed for the written form are also appropriate for speech, provided that we allow for obvious differences due to the physical characteristics of speech signals.
Particular emphasis is given to the role of word frequency in the recognition process, since this places restrictions on the types of models that can be considered (e.g., the cohort model).
We reject the view that there are no frequency effects in spoken word recognition, and we also reject the view that frequency effects in printed word recognition can be relegated to the minor status of a post-access decision effect.
In this paper, we address a characterization problem coming from plant biology.
The multiple characterization problem consists of entities (represented by a Boolean assignment) belonging to several groups to find a characterization for each group using propositional logic.
It implies that this characterization must be exact and the main difficulty is to compute the minimal one.
We show that this problem is W[2]-Complete.
We also propose a reformulation to a linear program and describe different approaches for the resolution.
We experiment on random and real instances whose solutions are used to design a patent-protected diagnostic test.
A method of detecting the pitch frequency of voiced speech is described, which exploits the harmonic structure of the short-time spectrum.
The technique is to form a harmonic histogram.
Pitch is then detected as the frequency of the largest component.
Both linearly and logarithmically weighted histograms are considered, and it is shown that the log weighted histogram gives the clearest indication of pitch.
The spectra used in this paper are derived from a digital filter bank having 187 channels between 50 Hz and 3200 Hz, on a log frequency scale.
It is shown that a clear indiction of pitch can still be obtained with 'short' spectra covering the band 200–3200 Hz, i.e. in the telephone bandwidth.
The influence of the first formant of certain vowels on harmonic amplitudes is shown to prevent a correct indication of pitch.
A method of harmonic enhancement is proposed which effectively overcomes this problem.
Finally, pitch contours by this method are presented, which show that pitch can be successfully tracked through nasals, vowels and voiced fricatives.
We present the first generalized heuristic search formalism that is able to solve decentralized POMDPs of both finite and infinite horizon.
We present a framework that is based on classical heuristic search on the one hand, and on decentralized control theory on the other hand.
We prove that our approach is able to generate optimal deterministic controllers, and we study its performance on examples from the literature.
Microphone array systems can be effective in combating the detrimental effects of acoustic noise and reverberation in hands-free telecommunication.
This paper discusses classical delay-sum beamformers as well as the more general filter-sum beamformers.
Filter-sum beamformers add the ability to control the array beampattern as a function of frequency and a new design method for a constant beamwidth filter-sum beamformer is presented.
The delay-sum and filter-sum beamformers require array sizes that are comparable to the acoustic wavelength.
These designs can result in arrays that are large in size.
For applications that are space constrained, differential microphone array systems are presented.
Finally, two types of adaptive beamformers are presented: a broadside array and a two-element differential microphone.
We describe some experiments in voice-to-voice conversion that use acoustic parameters from the speech of two talkers (source and target).
Transformations are performed on the parameters of the source to convert them to match as closely as possible those of the target.
The speech of both talkers and that of the transformed talker is synthesized and compared to the original speech.
The objective of this research is to develop a model for (1) creating new synthetic voices, (2) studying factors responsible for synthetic voice quality, and (3) determining methods for speaker normalization.
Because of the enormous amounts of rules that can be produced by automatic extraction algorithms, knowledge validation is still one of the most problematic steps in an association rule discovery process.
It is necessary to help the user appropriate this bulk of rules and to support him in his search for the relevant knowledge by organizing a real rule exploration.
In order to do so, the virtual reality techniques can be very profitable, because they combine a strong and intuitive interactivity with immersive 3D representations capable of integrating a large amount of information while remaining intelligible.
In this article, we propose a dynamic graphical representation for the association rules, which is based on virtual reality metaphors and supports the user in his rule rummaging task.
A first prototype implementing this new representation has been developed in order to validate our approach.
In this paper, we propose an alternative keyword detection method relying on confidence measures and Support Vector Machines (SVM).
Confidence measures are computed from phone level information provided by a Hidden Markov Model based speech recognizer.
We use three types of average techniques, arithmetic, geometric and harmonic, to compute a confidence measure for each word.
We also use support vector machines which are a classification technique developed from the theory of structural risk minimization.
The acceptance/rejection decision of a word is based on the confidence measure vector which is processed by a SVM classifier.
The performance of the proposed SVM classifier is compared with those of other methods based on the averaging of confidence measures.
Polynomials having only roots with négative real parts and those having only roots inside the unit circle can be characterized in many ways.
The criterias introducing a pair of other polynomials with roots alternating on the imaginary axis or on the unit circle are extended to the complex case.
An algorithm is established for testing polynomials with real or complex coefficients having all of their roots in a given half-plane, or in a sector defined by two straight lines passing through the origin.
A complete proof of the Routh's criterion for testing the continuous-time linear system stability is proposed in both the real and complex cases.
There are familiar terms such as “contour” and “trajectory” to refer to a vowel formant frequency as a function defined on the time axis, but there is no readily understood term for the analogous idea of how a formant behaves on the “vowel axis”.
For this we introduce the concept of a vowel-formant ensemble (VFE) as the set of values realized for a given formant (e.g., F2) in going from vowel to vowel among a speaker's vowel phonemes for a fixed time frame in a fixed CVC context.
The VFE affords a simple description of our development: we observe that D.J. Broad and F. Clermont's [J. Acoust. Soc. Am. 81 (1987) 155] formant-contour model is a linear function of its vowel target and that as a consequence all its VFEs for a given speaker and formant number are linearly scaled copies of one another.
To show how this question can be addressed, we use F1 and F2 data on one male speaker's productions of 7 Australian English vowels in 7 CVd contexts, with each CVd repeated 5 times.
Our hypothesized scaling relation gives a remarkably good fit to these data, with a residual rms error of only about 14 Hz for either formant after discounting random variations among repetitions.
The linear scaling implies a type of normalization for context which shrinks the intra-vowel scatter in the F1F2 plane.
VFE scaling is also a new tool which should be useful for showing how contextual effects vary over the duration of the syllable's vocalic nucleus.
Speech produced by two speakers was manually segmented, generating two databases with 18,000 and 6,000 vowel segments.
Automatic treatment of the specialyzed texts becomes more and more necessary due to their increasy number.
Term extraction, i.e. the extraction of groups of words significant for the field, is an information commonly required in the specialized domains.
In this paper, we propose a method for an automatic extraction of spécifie tenns.
Our input is a corpus of specialized texts, upon which we carry ont pretreatments: cleaning and lahelling.
Next, we are using classical association measures to e.xtract the terminology for the field.
Our main contribution is adding varions parameters to improve the research for terms.
We investigate the enhancement of speech corrupted by unknown independent additive noise when only a single microphone is available.
We present adaptive enhancement systems based on an existing non-adaptive technique [Ephraim, Y., 19992a. IEEE Transactions on Signal Processing 40 (4), 725–735].
This approach models the speech and noise statistics using autoregressive hidden Markov models (AR-HMMs).
We develop two main extensions.
The first estimates the noise statistics from detected pauses.
The second forms maximum likelihood (ML) estimates of the unknown noise parameters using the whole utterance.
Both techniques operate within the AR-HMM framework.
We have previously shown that the ability of AR-HMMs to model speech can be improved by the incorporation of perceptual frequency using the bilinear transform.
We incorporate this improvement into our enhancement systems.
We evaluate our techniques on the NOISEX-92 and Resource Management (RM) databases, giving indications of performance on simple and more complex tasks, respectively.
Both enhancement schemes proposed are able to improve substantially on baseline results.
The technique of forming ML estimates of the noise parameters is found to be the most effective.
Its performance is evaluated over a wide range of noise conditions ranging from −6 to 18 dB and on various types of stationary real-world noises.
A novel speech analysis method which uses several established psychoacoustic concepts is applied to the analysis of vowels.
This perceptually based linear predictive analysis (PLP) models the auditory spectrum by the spectrum of the low-order all-pole model.
The auditory spectrum is derived from the speech waveform by critical-band filtering, equal-loudness curve pre-emphasis, and intensity-loudness root compression.
We demonstrate through analysis of both natural and synthetic speech that psychoacoustic concepts of spectral auditory integration in vowel perception, namely the F1, F2′ concept of Carlson and Fant and the 3.5 Bark auditory integration concept of Chistovich, are well modeled by the PLP method.
A complete speech analysis-synthesis system based on the PLP method is also described in the paper.
This paper presents the evaluations of the field trials of two information inquiry systems that use different speech technologies and different human–machine interfaces.
The first system, RAILTEL, uses isolated word recognition and system driven dialogue.
The second system, Dialogos, understands spontaneous speech and implements a mixed initiative dialogue strategy.
Both systems allow access to the Italian Railway timetable by using the telephone over the public network.
RAILTEL and Dialogos were tested in extensive field trials by inexperienced subjects.
Moreover, a comparative trial with a limited number of subjects was performed in order to gain some insights on the impact of the different speech technologies on users' behaviour.
We propose a set of characteristics to identify these gradual rules, and a classification into “direct” rules and “modulation” rules.
In neurobiology, pre-synaptic neuronal connections lead to gradual processing and modulation of cognitive information.
We propose in the field of connectionism the use of “Sigma-Pi” connections to allow gradual processing in AI systems.
In order to represent as well as possible the modulation processes between the inputs of a network, we have created a new type of connection, “Asymmetric Sigma-Pi” (ASP) units.
These models have been implemented within a pre-existing hybrid neuro-symbolic system, the INSS system.
We study the adaptation of all existing language-identification system to new languages using a limited amount of training data.
The platform used for this study is the system recently developed (Yan and Barnard 1995a, b) to exploit phonotactic constraints based on language-dependent phone recognition.
Using the proposed language model re-estimation technique based on probabilistic gradient descent, two new approaches and their combination are proposed and tested.
These approaches all modify the phonotactic language models, so that they no longer equal the conventional maximum-likelihood estimate.
The difference of these methods can be viewed as different information resampling on the same amount of data.
Experiments were conducted using the standard OGI_TS database (Muthusamy et al., 1992).
For comparison, the baseline system (with traditional model estimation) was also subjected to the same set of tests.
Systems trained with different amounts of training data in the new languages were evaluated.
Compared with the conventional model estimation, the results demonstrate that the new methods improve adaptation to new languages.
The success of the discriminative model shows that conventional model estimation is not optimal for language identification, so that improvements can be obtained by modifying the maximum-likelihood estimates of the language models.
This paper reports acoustic-phonetic data on three speaking styles: Informal conversational speech, “clear” speech and Baby Talk.
These sets of observations illustrate the fact that a speaker's pronunciation of a given linguistic form can undergo rather drastic physical transformations, particularly in the wide range of contexts presented by spontaneously produced speech.
Despite their extensive variations, vowel formant measurements showed a high degree of predictability.
The findings bring to the fore a classical issue of speech research: signal variability and phonetic invariance.
While the present results do not conclusively preclude the possibility that the investigated speech samples are organized around a core of signal invariants, the extent as well as the systematicity of the variability observed lend support to a different perspective.
The paper proposes that the variegated acoustic pattern of speech be seen as products of adaptation.
According to this interpretation, phonetic gestures and signals are modulated and tuned adaptively in accordance with on-line communicative and socio-linguistic demands (e.g., controlling the “social distance” between speakers, preserving intelligibility, performing “phatic” and “emotive” functions, etc).
Furthermore, it is argued that the linguistic task of the phonetic signal is not to encode invariants but to complement information already available to the speech processing system of the listener.
Accordingly, intra-speaker phonetic variations need not be seen as invariants embedded in linguistically irrelevant variability.
They rather represent genuine behavioral adaptations that may jeopardize or demolish signal invariance but that transform speech patterns in essentially principled ways.
The two main visual sensors in underwater robotics are sonar and video.
In a first part, we present the fundamentals of acoustic imagery.
If some technics are well known, others, like synthetic aperture antenna, interferometry and parametric array are still research topics.
In a second part, acoustic image processing techniques are presented.
They are mainly applied to sea bottom characterization and robot navigation.
The third part addresses video technology and processing.
This sensor is complementary to sonar, due to its high resolution and the ease of interpretation of the images.
This paper approaches the articulatory-to-acoustic speech production inverse case.
A framework based on an explicit combination of vocal-tract morphological and acoustic constraints is proposed.
The solution is based on a Fourier analysis of the vocal-tract log-area function: the relationship between the log-area Fourier cosine coefficients and the corresponding formants is used to formulate an acoustic constraint.
The same set of coefficients is then used to express a morphological constraint.
This representation of both acoustic and morphological constraints in the same parameter space allows an efficient solution for the inverse problem.
The basis of the acoustic constraint formulation was first proposed by Mermelstein (1967).
However, at that time, the combination with morphological constraints was not realized.
The method is tested for some vowels.
The results confirm the validity of the method, but they also show the need for dynamic constraints.
We are interested by under-determined inverse problems, and more specifically by source localization in magneto and electro-encephalography (M/EEG).
Although there is a physical model for the diffusion (or “mixing'') of the sources, the (very) under-determined nature of the problem leads to a difficult inversion.
The need for strong and physically relevant priors on the sources is one of the challenge.
For M/EEG classical sparsity prior based on the ℓ1 norm is not adapted, and gives unrealistic results.
We propose to take into account a structured sparsity thanks to the use of mixed norms, especially a mixed norm with three indices.
The method is then applied on MEG signals obtained during somesthetic stimulation.
When stimulated, hand fingers activate separate regions of the primary somatosensory cortex.
The use of a three level mixed norm allows to take this prior into account in the inverse problem in order to correctly recover the organization of associated brain regions.
We also show that classical methods fail for this task.
The problem of sources discrimination, very classical in Signal Processing field, is also an actual problem in biological systems.
Biological sensors are sensitive to many sources, so the Central Nervous System processes typically multidimensional signals, each component of which is an unknown mixture of unknown sources, assumed independent.
With regard to adaptation rules used in adaptive filtering, here the adaptive increment is achieved necessarily by the product of two non-linear functions.
Some experimental results, in Signal Processing and Image Processing fields, show the efficiency of this adaptive algorithm.
We prove also the possible generalization of this algorithm in the case of more complex (non-linear, degenerated, etc.) mixtures.
This algorithm points out a new concept of Independent Components Analysis, more powerful than this one of Principal Components Analysis, and applicable in the general flame of data analysis.
In this paper, we propose an original method for agreement error detection and correction that we apply to Arabic.
The detection is based on a global syntactic analysis of the sentence.
It consists of an “extended syntagmatic analysis” that can first locate the syntagm boundaries and, second, regroup all the sentence components that have an agreement relationship.
The correction is based on a multicriteria analysis aiming to rank the correction alternatives in order to choose the best one.
This method has the advantage of reducing the dominated alternatives and ranking the remaining ones according to different evaluation criteria.
Articulatory paths have been analysed for the frictive [s] produced by a woman speaker of General American English in connected speech and also in phonotically controlled speech-like sequencues at a shower speech rate.
Aerodynamically derived traces indicating the acoustically relevant parameter of cross-section area of the constricted region of the vocal tract suggested that, contrary to related data for the movements of the solid structures, some portions of the traces for this vocal tract indicator had the same path shape across some different vowel contexts and across different speech styles.
In the connected speech, the whole cross-section area path seemed to be invariant across different stressed vowel contexts.
The acoustic pattern features associated with the invariant portions of vocal tract articulation, in combination with appropriate respiratory and laryngeal articulations, are discussed.
In recent work on the aphasic syndrome of agrammatism, Kean (1980a) has argued that a description of the impaired versus retained elements can be provided only at the linguistic level of representation that mediates between syntactic and phonological structures.
The burāq is the beast on which the Prophet Muḥammad is said to have ridden on his night journey from Mecca to Jerusalem (the isrāʾ) and occasionally on his ascension through the heavens (the miʿrāğ).
This article examines the notion of the burāq within both Muslim and Western non-Muslim sources.
It shows the evolution of the relatively simple descriptions of the burāq as found in the early Muslim sources to the later embellished and colourful accounts.
It also investigates the role the burāq plays in more general Muslim discussions concerning the isrāʾ and the miʿrāğ.
As for Western non-Muslim approaches, the article demonstrates how the burāq was initially used in anti-Islamic polemic, subsequently became an object of fascination, and in later times has become a topic of more impartial academic study.
The cue-trading relations between the intensity of aspiration and the intensity of the following vowel for the /d/-/t/ distinction were investigated at auditory, phonetic, syllable, word and sentence levels.
The results demonstrate that the higher the linguistic level of cue processing the less categorical were the identifications and the more effective was the cue trading.
The stronger cue trading at the syllable, word, and sentence levels seems to suggest that the speech mode of perception can be regarded as a special mode.
Nevertheless a certain degree of cue trading is still evident at the auditory and phonetic levels.
The results also seem to support the interactive model of speech processing.
In general, the inter-word dissimilarity measure supplied by Dynamic Time Warping algorithms can not be assumed to be a metric because it does not fully satisfy all the required properties (the triangle inequality in particular).
In this paper, however, empirical evidence of loose satisfaction of these properties with real speech will be presented, allowing the assumption of a “loose metric space” structure in the set of parametric representations of words in a given vocabulary.
Based on this structure, a search algorithm will be introduced which eliminates the need of computing the DTW-distance between the test word and many of the prototypes in the dictionary for Isolated Word Recognition.
Experiments with vocabularies of different characteristics, have proved that the algorithm finds the nearest prototype by performing DTW computations with an average of only 30% of the words in the dictionary.
This figure has been observed to decrease with increasing dictionary size, and also if the test word is close to the corresponding prototype (“well uttered” word).
We present in this paper the integration of a analytical speech recognition system to the control of a sonar console by a human operator.
This application is really useful since it does correspond to a pratical need of the operator who has his eyes busy looking at the sonar screen.
The DIAPASON system presents two original features: the acoustic-phonetic decoding part of the system is not based on a classical phoneme labeling process but it yields for each segment of speech a set of acoustic phonetic labels that describe this segment very precisely.
This phonetic labelling is associated with a special procedure for lexical access and sentences recognition ; the DIAPASON system is moreover a genuine man-machine dialogue system and not only a system capable of understanding a single sentence as case.
The history of the dialogue is used as a special knowledge source during the understanding process. Pragmatic knowledge is thus intimately associated with the analysis of sentence ; this point highly encreas as the overall performance of the system.
The paper presents the architecture of DIAPASON and its various components.
It is also discusses experimental results obtained in a multispeaker mode and compares the voice dialog system with the without voice system for a real sonar console.
This paper deals with speech enhancement for hands-free audio terminals, including two major problems: noise reduction and acoustic echo cancellation.
Our objective is to combine a noise reduction system and an acoustic echo canceller to get a near-end speech signal with a minimum distortion and low levels of echo and noise.
We present four structures (using one or two microphones and one loudspeaker) where the operation of echo cancellation comes before that of noise reduction.
Experimental results are presented.
Finally, in the mono-channel situation, an optimized structure controlled by an echo detector is proposed and tested.
This paper describes a microphone array for speech recording in car environments.
The array is designed for hands-free radiotelephone, and is also used as a front-end for an automatic speech recognition system (this study has been realised within the european ESPRIT project ARS “adverse environment recognition of speech”).
We first summarise the adaptive beamforming techniques that we have used.
We then describe several aspects of the implementation of the array (configuration, design of fixed beamformers, adaptation, complexity reduction).
In the last section, we evaluate the performance of the array.
Two measures of performance have been retained, one is the signal-to-noise ratio, and the other is the score obtained with the speech recognition system.
In binocular stereovision, the accuracy of the 3D reconstruction depends on the accuracy of matching results.
Consequently, matching is an important task.
Our first goal is to present a state of the art of matching methods.
We define a generic and complete algorithm based on essential components to describe most of the matching methods.
Occlusions are one of the most important difficulties and we also present a state of the art of methods dealing with occlusions.
Finally, we propose matching methods using two correlation measures to take into account occlusions.
The results highlight the best method that merges two disparity maps obtained with two different measures.
In recent years, many methods of analysis and classification of data based on reproducing kernel Hilbert spaces have been developed.
Most of these methods incorporate the fundamental principle dictated by Vapnik et al. in Support Vector Machines, which consists in extending linear algorithms to the non-linear case by using kernels.
Kernel Fisher Discriminant (KFD) is one of these nonlinear methods which provides interesting results in many practical cases.
However, the use of KFD requires storage and processing of matrices whose size equals the number of available data.
This paper presents a sequential KFD algorithm which does not require the manipulation of large matrices.
Sequential algorithms that fulfil the same requirements as KFD are also presented to perform Kernel Principal Component Analysis(KPCA) and Kernel Generalized Discriminant Analysis (KGDA).
When vowels are synthesised by means of a source-filter model, a delta-pulse train is often used as a source signal.
Although breathiness can to some extent be simulated by using a sophisticated glottal-source model, a more natural simulation of breathiness requires the addition of aspiration noise.
When stationary noise is used, however, the noise is to a large extent perceived as coming from a separate sound source which hardly contributes to the breathy timbre of the vowel.
This problem can be solved by using noise with a temporal envelope of the same periodicity as the pulse train.
In a simple source-filter model, a combination of lowpass-filtered pulses and synchronous highpass-filtered noise bursts of equal energy was used as a source signal.
In this way, the noise was no longer perceived as a separate sound, but integrated perceptually with the strictly periodic part of the signal.
It will be shown that this integration consists of both a reduction of the loudness of the separate noise stream and a timbre change in the breathy vowel.
In this paper, we present a novel likelihood measure which extends the adaptation mechanism to the shrinkage of covariance matrix and the adaptation bias of mean vector.
A set of adaptation functions is proposed for obtaining the compensation factors.
Experiments indicate that the likelihood measure proposed herein can markedly elevate the recognition accuracy.
A dysgraphic patient is described whose deficit is hypothesized to arise from selective damage to the Graphemic Buffer.
The patient's roughly comparable difficulties in oral and written spelling and comparable spelling difficulties in written naming, delayed copy and spelling-to-dictation rule out the hypothesis of selective damage to either input or output mechanisms.
More importantly, the nature of the errors produced by the patient and the fact that these errors were distributed virtually identically for familiar and novel words were taken as strong evidence for the hypothesis that L.B:'s spelling disorder results from selective damage to the Graphemic Buffer.
Various aspects of the patient's performance are discussed in relation to a functional architecture of the spelling process and in terms of the processing structure of the Graphemic Buffer.
Although less applied, the Henri Ey's thinking is well-known in Romania.
The translations of Ey's works (“The Consciousness” and “Psychiatric Studies”) into Romanian, as well as the writings of the Professor Pamfil, who was one of the Ey's students, contributed to propagate his theories.
The paper's author considers the organodynamic conceptualization as a premise on his own conception of the rehabilitation of the psychiatric patient, as described in his Textbook of psychiatry, published in Bucharest in 1997.
The original contribution of Henri Ey is the theory of the mind dominated by the structures of the ego (reality reflection, self-consciousness and identity).
Therefore, Ey unifies the psychoanalysis, the psycho-organic theory and psychogenetics.
Henri Ey was a real anthropologist.
Considered in the article is a model of auditory perception by man, represented as a multilevel hierarchical automaton.
A number of heirarchical levels are defined more exactly, the conceptual aspects of each level are considered, and the procedure of signal processing on the input/output route is described.
The system of speech signal features based on the psychoacoustic masking effect is described.
Modelling the masking effect makes it possible, without loss of information, to withdraw from each speech signal spectrum all the spectral components masked by more powerful adjoining components.
Verification of the feature system so obtained was carried out on vocabularies up to 200 words with 5 speakers participating.
Recognition reliability for one speaker was about 100% and for different speakers between 90 and 99% at most.
Our goal is to obtain high dialog success rates with a very open interaction, where the user is free to ask any question or to provide any information at any point in time.
In order to improve performance with such an open dialog strategy, we make use of implicit confirmation using the callers wording (when possible), and change to a more constrained dialog level when the dialog is not going well.
In source-filter based speech coding for low bit rates an efficient representation of excitation pulses is required to attain high quality of the synthetic speech.
In this paper, we discuss a pulse waveform representation by a codebook populated with pulse shapes.
The codebook is designed from glottal derivative pulses obtained by a linear predictive inverse filtering technique.
Pulses are extracted and normalized in time and amplitude to form prototype pulses.
Design methods and performance evaluation of the codebooks are investigated in a vector quantization (VQ) framework.
The quantization gains obtained by exploiting the correlation between pulses are studied by theoretic calculations which suggest that about 2 bits per vector (in a budget of 7–10 bits) can be gained when exploiting the correlation.
Memory based VQ is a generic term for quantization schemes which utilizes previous quantized pulses.
We study traditional memory based VQ methods and an extension of memory based VQ with memoryless VQ, denoted a safety-net extension.
The experiments show that performance improves when extending memory based VQ with a safety-net.
It is found that, at the designated bit rates, a safety-net extended memory based VQ can gain about 1.5–2 bits in comparison with memoryless VQ.
Content-based image retrieval in large image data sets is a tiresome task considering the high rate of heterogeneity even within the same class as well as the high dimensionality of the descriptors space.
For that, we propose to guide the research by low level descriptors of reduced size based on sub-bands of wavelet relating to the most significant regions in each image after a fuzzy segmentation step.
Moreover, we propose a negative relevance feedback technique based on region weights.
Experiments and comparative study with other similar approaches prove the robustness of the proposed approach in terms of semantic contribution thanks to the use of the wavelet sub-band and the negative relevance feedback.
In this paper, a color image segmentation method based on a new approach called bimarginal is proposed.
To overcome the drawbacks of the classical marginal approaches, color components are considered in pairs in order to have a partial view of their inner correlation.
Working with color images, the three possible combinations are considered as three independant information sources.
Each pairwise component combination is firstly analyzed according to an unsupervised morphologic clustering which looks for the dominant colors of a 2D histogram.
This leads to obtain three segmentation maps combined by intersection after being simplified.
The intersection process itself producing an over-segmentation of the image, a pairwise region merging is done according to a similarity criterion with the Dempster-Shafer theory up to a termination criterion.
The word analogy has many interconnected senses, each of which is in turn connected with such contiguous concepts as “expression” and “metaphor”.
In this study, we aim at analyzing both the different concepts covered by the word analogy and their manifold relations with expression and metaphor.
As far as the relation between analogy and expression and metaphor is concerned, regressive analogies motivate the creation and circulation of conventional metaphors, whose expression is purely instrumental, whereas projective analogies, made possible by linguistic expression, can only be conceived of as the aim of creative interpretations of conflictual complex meanings.
The automatic récognition of typical pattern sequences (scenarios), as they are developing, is of crucial importance for computer-aided patient supervision.
However, the construction of such scenarios directly from medical expertise is unrealistic in practice.
Starting from the monitored data and clinical information available, our objective is to extract typical abstracted pattern sequences and then construct scenarios, eventually validated by clinical experts as representative of a class of situations to recognize.
In this paper, we present a methodology for data abstraction that gradually allows the construction of such scenarios.
Over the years there has been widespread controversy over the relative merits of the cascade and parallel connections of formant generators in a speech synthesizer.
This report shows that the theoretically less attractive parallel connection is able to produce closer approximations to the properties of real speech signals than is generally possible for a cascade synthesizer, both for vowels and for consonants.
However, to achieve this result it is necessary to take care over the phase characteristics of the formant generators, and to appropriately shape the skirts of the formant response curves.
Although extra amplitude information is needed for a parallel synthesizer, the form of the acoustic specification is in consequence directly related to properties of human speech that can be easily measured from a spectrum display.
This method is based on the asymptotic normality of a certain function of the Haar wavelet and scaling coefficients called the Fisz transformation.
Some asymptotic results such as normality and decorrelation of the transformed image samples are extended to the 2D case.
This Fisz-transformed image is then treated as if it was independent and gaussian variables.
Then we apply a novel Bayesian denoiser that we have recently developed.
It clearly outperforms the other denoising methods especially in the low-count setting.
Combining the Fisz transform and the BKF Bayesian denoiser yields the best performance.
A Code-Excited Linear Predictive (CELP) coder is developed for transmission purposes and its connection with other predictive coding schemes as well as vector quantization is clarified.
At a rate of one bit per sample a codebook composed of 2 L waveforms of L samples is specified taking their values in {+1, −1}.
Each waveform i deduced from the L bits index by a one-to-one correspondence between each bit of the index and each sample of the waveform.
By construction the codebook is shown to have an inherent robustness against transmission errors and its internal algebraic structure leads to efficient and fast algorithms for selecting the optimum excitation.
Both objective and subjective results confirm the high level of performances obtained by the 16 kbit/s CELP coder in different realistic transmission conditions as transmission with errors and ambient noise.
The ability to synthesize pathological voices may provide a tool for the development of a standard protocol for assessment of vocal quality.
An analysis-by-synthesis approach using the Klatt formant synthesizer was applied to study 24 tokens of the vowel /a/ spoken by males and females with moderate-to-severe voice disorders.
Both temporal and spectral features of the natural waveforms were analyzed and the results were used to guide synthesis.
Perceptual evaluation indicated that about half the synthetic voices matched the natural waveforms they modeled in quality.
The stimuli that received poor ratings reflected failures to model very unsteady or “gargled” voices or failures in synthesizing perfect copies of the natural spectra.
Several modifications to the Klatt synthesizer may improve synthesis of pathological voices.
These modifications include providing jitter and shimmer parameters; updating synthesis parameters as a function of period, rather than absolute time; modeling diplophonia with independent parameters for fundamental frequency and amplitude variations; providing a parameter to increase low-frequency energy; and adding more pole-zero pairs.
Fuzzy inference systems are likely to play a significant part in system modeling, when data and expert knowledge integration is important.
The aim of this paper is to set up some guidelines for this kind of modeling, based on practical experience in the fields of Agronomy and Environment.
We dicuss fuzzy system assets, their ability for data and expert knowledge integration in a common framework, and their position relatively to other models.
The open source FisPro implementation is presented and the approach is illustrated through two detailed case studies.
The verification of the Triangle Inequality (TI) by Dynamic Time Warping (DTW) Dissimilarity Measures (DM) seems to be a fairly important prerequisite for the application of some new and promising methods recently proposed for reducing the number of DTW computations in Isolated Word Recognition.
The degree of satisfaction of this property for real speech samples has been empirically studied by various authors, who have reported rather controversial results for the different DTW and DMs speech data utilized by each of them.
Therefore, a systematic study seemed to be necessary of the impact of the different factors upon which the DTW-based DMs depend.
DTW productions, local metrics and speech data taken into account; moreover, this property was always observed as being “loosely” satisfied.
In view of these results, a prospective study of some possible causes of TI violation was carried out.
The conclusions suggest the use of time-compressing preprocessing techniques and the application of suboptimal DTW procedures (and especially if gross end-point detection errors are involved) as the most likely causes of the TI dissatisfaction rates reported elsewere.
Jitter is the small fluctuation from one glottis cycle to the next in the duration of the fundamental period of the voice source.
Analyzing jitter requires measuring glottal cycle durations accurately.
Generally speaking, this is carried out by sampling at a medium rate and interpolating the discretized signal to obtain the required time resolution.
In this article we describe an algorithm which solves the following two signal processing problems.
Firstly, signal samples obtained by interpolation are only estimates of the original samples, which are unknown.
The quality of the reconstruction of the signal therefore has to be evaluated.
Secondly, small variations in cycle durations are easily corrupted by noise and measurement errors.
The magnitude of measurement errors therefore has to be gauged.
In our algorithm, the quality of reconstruction by signal interpolation is evaluated by a statistical test which takes into account the distribution of the corrections (which are brought about by interpolation) to the positions of the signal events which mark the beginnings of the glottal cycles.
Three different interpolation methods have been implemented.
Measurement errors are controlled by estimating independently the cycle durations of the speech and the electroglottographic signals.
When the series obtained from both signals agree, we may then conclude that they reflect vocal fold activity and that they have not been unduly corrupted by errors or noise.
The algorithm has been tested on 77 signals produced by healthy and dysphonic subjects.
Its performance was satisfactory on all counts.
The behaviour of the voice source characteristics in connected speech was studied.
Voice source parameters were obtained by automatic inverse filtering, followed by automatic fitting of a glottal waveform model to the data.
Consistent relations between voice source parameters and prosodic features were observed.
This paper serves a double purpose: to review the coding methods which have been introduced during the past decade in the 4.8–9.6 kbps range, and to discuss the most recent research trends.
The bulk of the paper is devoted to CELP-based coding, a mandatory method which is at the basis of several emerging standards.
The rest consists of a brief review of an alternative class of coders based on sinusoidal modelling of speech.
The comparison between these opposite techniques will enable us to draw some conclusions and identify areas for future research.
This paper describes a technique whereby a pole/zero function can be fitted to a system frequency response by solving a set of simultaneous equations.
The order of the pole/zero functions is defined by twice the number of spectral maxima in the response.
The function is constrained to fit the maxima and minima of the response curve.
The fitting technique permits the shape of the slopes between maxima to be varied via a single coefficient.
The problem of obtaining a good approximation to the vocal tract frequency response from short-time spectra of speech, is solved by a spectral smoothing method.
The harmonic structure in the short-time spectra obtained from a digital filter bank is removed by a cepstral truncation process.
A synthetic vowel is used as an example to illustrate the technique and indicate the degree of approximation involved.
More precisely, we propose a scheme for image classification optimization, using a joint visual-text clustering approach and automatically extending image annotations.
The proposed approach is derived from the probabilistic graphical model theory and dedicated for both tasks of weakly-annotated image classification and annotation.
We consider an image as weakly annotated if the number of keywords defined for it is less than the maximum defined in the ground truth.
Thanks to their ability to manage missing values, a probabilistic graphical model has been proposed to represent weakly annotated images.
We propose a probabilistic graphical model based on a Gaussian-Mixtures and Multinomial mixture.
The visual features are estimated by the Gaussian mixtures and the keywords by a Multinomial distribution.
Therefore, the proposed model does not require that all images be annotated: when an image is weakly annotated, the missing keywords are considered as missing values.
Besides, our model can automatically extend existing annotations to weakly-annotated images, without user intervention.
The uncertainty around the association between a set of keywords and an image is tackled by a joint probability distribution (defined from Gaussian-Mixtures and Multinomial mixture) over the dictionary of keywords and the visual features extracted from our collection of images.
Moreover, in order to solve the dimensionality problem due to the large dimensions of visual features, we have adapted a variable selection method.
Results of visual-textual classification, reported on a database of images collected from the Web, partially and manually annotated, show an improvement of about 32.3% in terms of recognition rate against only visual information classification.
Besides the automatic annotation extension with our model for images with missing keywords outperforms the visual-textual classification of about 6.8%.
Finally the proposed method is experimentally competitive with the state-of-art classifiers.
The purpose of this paper is to propose a new scheme for image compression.
First, we use a wavelet transform in order to obtain a set of orthonormal subclasses of images.
The wavelet functions are well localized both in the space and frequency domains.
The original image is decomposed on this orthonormal basis with a pyramidal algorithm architecture.
The wavelets coefficients of each classes are then vector quantized.
A separate optimal codebook is designed for each given resolution and direction using a training sequence and a MSE distortion measurment.
Then the input vector is classified (resolution and direction) and only the appropriate subclass of the codebook is then checked using the usual MSE.
The answers to L.J. Boë and P. Perrier's comments are organised under three headings - acoustics, articulation and phonetics.
An introductory rundown emphasises the model's features in order to avoid any misunderstandings as to its nature.
In this paper, we address the possibilities offered by hybrid harmonic/stochastic (H/S) models in the context of wide-band text-to-speech synthesis based on segment concatenation.
After a brief recall of the hypotheses underlying such models and a comprehensive review of a well-known analysis algorithm, namely the one provided by the multi-band excited (MBE) analysis framework, we study how H/S models allow to modify the prosody of segments and how segment concatenation can be organized, in the purpose of minimizing mismatches at the border of segments.
In this context, we introduce an original concatenation algorithm which takes advantage of some analysis errors.
Speech synthesis algorithms are then described, including an original synthesis technique based on judiciously prepared IFFTs, and the final segmental quality, in TTS synthesis, is the ability of a synthesizer to produce natural-sounding speech sounds.
More particularly, we examine the differences in the quality obtained when using the model in a narrow-band speech coding context and in a wide-band, concatenation based synthesis context.
We study three possible causes for these differences: the choice of an analysis criterion, the inadequacy of the model due to pitch variatons, and the effect of coarticulation on phases.
Information retrieval systems generally return a list of ranked documents, in which only the title and possibly a snippet that contains the words of the request allow a user to evaluate the document relevance relative to her initial request.
This kind of result leads the user to browse throught a lot of documents before satisfying her information need.
In order to improve information retrieval, we have studied text visualisation: which information has to be shown and how?
Our system RÉGAL (RÉsumé Guidé par les Attentes du Lecteur) automatically extracts the visualised information from texts by applying a thematic analysis that does not require a pre-existing structuring or a formatting of the texts, and is based on the combination of two criteria: lexical cohesion and cue phrases.
We propose a method for automatically generating a pronunciation dictionary based on a pronunciation neural network that can predict plausible pronunciations (realized pronunciations) from the canonical pronunciation.
This method can generate multiple forms of realized pronunciations using the pronunciation network.
For generating a sophisticated realized pronunciation dictionary, two techniques are described: (1) realized pronunciations with likelihoods and (2) realized pronunciations for word boundary phonemes.
Experimental results on spontaneous speech show that the automatically derived pronunciation dictionaries give consistently higher recognition rates than a conventional dictionary.
This paper is concerned with the use of a commercial large-vocabulary speech recognition system by a team of mainstream users in the course of their everyday work.
In particular, it describes use by translators working in four languages in a multilingual environment at the European Commission.
The paper begins by describing some of the differences between the point-of-view of a typical researcher in speech recognition and that of a typical mainstream user.
It points out some of the psychological barriers that must be overcome if speech recognition is to gain really widespread acceptance, and it concludes that such acceptance will depend at least as much on the sharing of experience between users as on technical advances.
The overall results of the trials at the European Commission were encouragingly positive, but several unexpected problems were encountered, many of them related to the multilingual environment.
The paper describes how most of these problems are being addressed.
This paper describes a speech technology project called SPELL (Interactive System for Spoken European Language Training), whose main aim is to design an automated system for improving the pronunciation of foreign languages by learners of English, French and Italian.
The project has just completed a two-year feasibility study which has created a prototype vehicle incorporating teaching modules in intonation, rhythm and vowel quality.
The paper highlights the speech signal processing techniques, similarity metrics and user interfaces which have been integrated to produce an initial demonstration system.
A preliminary evaluation by a group of language teaching professionals suggests that the SPELL system is an appropriate tool for exploring the automated teaching of pronunciation.
Legal fictions prevailed in Middle French Literature.
How and why did the Ballades by Charles of Orleans follow this cultural fashion?
Legal metaphors frame his lyric collection from Retenue to Departie d'Amour.
The poet however changes the tradition of loyalty in courtly love into a legislation.
By doing so, he throws light on the specific communication with his lady: love in exile and prison implies paradoxically acts, especially official documents.
The two lovers, committing themselves to each other, are bound to respect their desire as a law.
The Ballades' imaginative world often embodies the lyrical I and the allegories as lawyers, judges or attorneys.
The fictional staging evolves, from places where legal actions take place, to courts of justice, where trials and inner debates never end.
Legal fictions transform the dialogues with other poets into role-plays, games where the prince can be both judge and jury.
In the present paper, we study these weighted Burg methods for pitch-synchronous analysis of short segments (duration less than one pitch period) of non-nasalized voiced speech and make their comparative performance evaluation.
Errors in estimating the power spectrum, formant frequencies and formant bandwidths are used as criteria for performance evaluation.
It is shown that the weighted Burg method of Kaveh and Lippert results in the best performance.
These methods are also compared with the autocorrelation and covariance methods and the results are discussed.
Les modélisations utilisateurs étudiées par les informaticiens ou les psychologues cognitivistes manipulent souvent des données très abondantes et utilisent des outils de fouille visuelle.
This paper presents a theory and a computational implementation for generating prosodically appropriate synthetic speech in response to database queries.
Proper distinctions of contrast and emphasis are expressed in an intonation contour that is synthesized by rule under the control of a grammar, a discourse model and a knowledge base.
The theory is based on Combinatory Categorial Grammar, a formalism which easily integrates the notions of syntactic constituency, semantics, prosodic phrasing and information structure.
Results from our current implementation demonstrate the system's ability to generate a variety of intonational possibilities for a given sentence depending on the discourse context.
Many text-to-speech (TTS) systems under development in Europe and elsewhere — we discuss in particular the system under development at Edinburgh University's Centre for Speech Technology Research (CSTR) — generate intonational properties of synthetic utterances on the basis of an intermediate abstract phonological representation of prosodic features that is quite independent of any acoustic realisation.
For evaluating certain aspects of synthetic prosody (notably accent placement and division into domains), this abstract representation is a more appropriate object of evaluation than the final acoustic output of a system, just as word stress and grapheme-to-phoneme conversion are appropriately evaluated in terms of symbol strings rather than acoustic output.
By way of illustration we present the results of an evaluation exercise carried out on the sentence-accent assignment rules of the CSTR system, based on just such an abstract representation, which has been useful in improving our rules.
In this paper, we propose an original methodology which allows the detection and the recognition of multi-oriented and multi-scaled patterns.
The supports on which the method is applied are technical documents representing the network of the French Telephone operator France Telecom.
The adopted technique, based on the Fourier-Mellin Transform (FMT) is integrated in a global strategy that solves ambiguous situations, through the providing of contextual information.
The strategy which is applied to solve the character and symbol classification problem can be divided into two stages.
The first one consists in computing a set of invariant descriptors for each isolated pattern belonging to a characters layer detected thanks to a connected components extractor.
The second stage, based on a filtering scheme, consists in detecting and recognising the shapes which are either interconnected or connected to any other object.
The results of the application of this technique are very encouraging since the classification rate reaches excellent scores in comparison with classical techniques.
In this paper, we present a different approach to the problem of estimating the angle of arrivals (AOA's) of D targets in Frequency- Hopped signaling sensors array for active systems, with D smaller than the number of sensor elements, L
This method is based on the application of the Maximum Likelihood Estimator for a new proposed model of received data available in different channels.
The simulation results show that this approach improves the resolution in the estimation of the angle of arrivals compared with Monotone-Frequency signaling case.
Its drawback, however, is that when the Signal-to-Noise Ratio, SNR, is low the performance deteriorates and a large number of snapshots is required.
Before next-generation human language technology can be designed to function successfully in actual field settings, interface techniques will be needed that can guide users' language to coincide with current system capabilities.
The present study examines how input modality and presentation structure influence the linguistic complexity observed in people's spoken and written input to an interactive system.
Using a semi-automatic simulation technique, language was collected during speech-only, writing-only and combined pen/voice exchanges, and using presentation formats that either were structured or unconstrained.
Results indicate that both modality and presentation format substantially influence linguistic complexity, although the specific nature of their impact differs.
A comprehensive analysis is provided of how both factors affect people's observed language in terms of total words, disfluencies, utterance length, lexical variability, perplexity, syntactic ambiguity and semantic integration.
Users' preferences for modalities and formats also are analyzed, and implications are discussed for channeling people's language in a transparent way.
The long-term goal of this research is to develop interface techniques for managing difficult sources of variability in people's language, so that robust processing of human language technology can be achieved.
Our work aims at comparing and merging points of view of Madagascar farmers on their territory.
Our modelling approach is based on conceptual graphs to represent farmers' spatial knowledge, and on formal concept analysis to organize the fusion of these representations.
The results interpretation relies both on a interest measure over the obtained formal concepts and onfield expertise.
We show the relevance of the selected formal concepts for analysing the way farmers organize their territory regarding forest conservation constraints.
In order to achieve better understanding of the articulatory-acoustic relationships, more data are still very much needed.
The two-fold aim of the present study was thus (1) to provide a set of coherent midsagittal functions, area functions and formant frequencies, for a small corpus of vowels and fricative consonants produced by one subject, and (2) to derive a midsagittal profile to area function conversion model optimised for this given subject.
Simultaneous tomography and sound recording were available for the subject, as well as some complementary data such as lip geometry or casts of the hard palate.
The model is based on Heinz and Stevens' A = αd β area function model, modified so that α varies continuously along the vocal tract midline as a function of the midsagittal distance.
The coefficients of the model have been determined with the help of an optimisation algorithm based on a gradient descent technique.
The gradient of the error between actual and desired formant values was computed through a back-propagation network implementing both sagittal-to-area conversion and acoustic wave propagation.
The fact that the model should work for sounds as different as vowels and consonants and be coherent at both midsagittal and acoustic levels ensures the reliability of the area functions determined in such a way.
In this paper, the training of HMMs has been considered a general optimization problem with linear constraints.
A gradient projection method for nonlinear programming with linear constraints has been introduced and presented to solve for “optimal” values of the model parameters.
When this classic method is applied to train HMMs of discrete or Gaussian mixture observation densities, a very simple formulation can be derived due to the special structure of the constraints on the HMM parameters.
This kind of classical gradient-based optimization methods can offer an opportunity for more flexible modeling of speech signals and more sophisticated training of model parameters for speech recognition.
In this paper, we address the problem of speaker-based segmentation, which is the first necessary step for several indexing tasks.
It aims to extract homogeneous segments containing the longest possible utterances produced by a single speaker.
In our context, no assumption is made about prior knowledge of the speaker or speech signal characteristics (neither speaker model, nor speech model).
However, we assume that people do not speak simultaneously and that we have no real-time constraints.
We review existing techniques and propose a new segmentation method, which combines two different segmentation techniques.
This method, called DISTBIC, is organized into two passes: first the most likely speaker turns are detected, and then they are validated or discarded.
The advantage of our algorithm is its efficiency in detecting speaker turns even close to one another (i.e., separated by a few seconds).
The segment-based speech recognition algorithms that have been developed over the years can be divided into two broad classes.
On the one hand those using the conditional segment modeling formalism (CSM), which requires the computation of the likelihood of the sequence of acoustic vectors, conditioned on the sub-word unit sequence and corresponding segmentation.
On the other hand those using the posterior segment modeling formalism (PSM), which requires the computation of the joint posterior probability of the unit sequence and segmentation, conditioned on the sequence of acoustic vectors.
The latter probability can be written as the product of a segmentation probability and a unit classification probability.
In this paper, we focus on the role of the segmentation probability.
After having shown that the segmentation probability is not required in the CSM formalism, we motivate its importance in the PSM formalism.
Next, we describe its modeling and training.
Experiments with two PSM-based recognizers on several speech recognition tasks demonstrate that the segmentation probability is essential in order to obtain a high recognition accuracy.
Moreover, the importance of the segmentation probability is shown to be strongly correlated with the magnitudes of the unit probability estimates on segments that do not correspond with a unit.
This article is concerned with the learning by analogy in the world of sequences, based on the resolution of analogical equations.
It presents a definition of an analogical relation, based on the edit distance and studies the solving of an analogical equation on sequences.
It presents a construction with finite-state trandsducers which computes all the solutions of this equation, reducing the problem to that of solving analogical equations on a finite alphabet.
It studies also what is analogy on alphabets and describes two algebraic structures which are compatible with the computation of solutions on sequences.
Finally, it presents a direct suboptimal algorithm to compute a solution to an analogical equation on sequences.
This paper presents an operational Pitch Synchronously Excited (PSE) Formant based TtS system for the Greek language, developed at WCL.
The system uses a thesaurus comprised of 140 speech segments including the Consonant (C), Vowel (V), CV and CCV type.
Particular attention is paid to the concatenation scheme applied to these segments, as well as on their context-sensitive duration and the coarticulation rules written for the Greek language.
The formant synthesizer runs on a DSP32C board.
This article studies the notion of verbal agency and valency in contemporary French.
A trivalent construction in which a verb has a subject and both a direct and an indirect object is at the core of the reflection.
A study of such constructions as found in newspaper Le Monde allows to set apart theoretical expectations and actual properties of usage.
This paper presents a signature authentication method based on handwriting temporal feature.
We propose here a method which, after signal normalization, extracts dissimilarity measures between signatures.
Those measures are then processed by a neural network.
As we do not always have forgeries, this problem is a statistical one-class problem and the use of classical learning algorithms is forbidden.
This drawback is eliminated by the introduction of a new learning algorithm.
A parser for continuous speech has to deal with lattices where the word hypotheses of the correct sentence are not usually perfectly aligned and short function words may be missing.
To cope with these problems, a two-way interaction between the recognition module and the parser, called feedback verification procedure (FVP), has been investigated.
The best scoring solution is finally selected by the parser.
Results on a 787-word, speaker-independent, telephone-bandwidth continuous speech recognition task are presented.
Brutal and massive environmental changes, generally affecting large areas, have to be localized as rapidly as possible in order to manage the immediate impact of this type of events on ecosystems and prevent related risks.
Therefore, it is necessary to develop efficient methods for change mapping.
A quasi-unsupervised region-based method for change detection in high resolution satellite images is proposed.
An automatic feature selection optimizes image segmentation and classification via an original calibration-like procedure.
A binary classification enables then to separate altered from, intact areas thanks to a new spatio-temporal descriptor based on the level of fragmentation of obtained regions.
Both segmentation and classification involve a mean shift procedure.
The method was assessed on forest environment using a Formosat-2 mul- tispectral satellite image pair acquired before and after a major storm to identify and map the damages.
This paper addresses the problem of how the forms of information derived by the visual system can be translated into forms useable by the language capacity, so that it is possible to talk about what one sees.
The hypothesis explored here is that there is a translation between the 3D model of Marr's (1982) visual theory and the semantic/conceptual structure of Jackendoff's (1983) theory of natural language semantics.
It is shown that there are substantial points of correspondence through which the encoding of physical objects and their locations and motions can be coordinated between these two levels of representation, and that both of these levels are deeply involved in visual as well as linguistic understanding.
A test of consonant perception presented with lipreading alone, and with lipreading supplemented by the aid, showed improvements in the perception of voicing and manner of articulation in the aided condition.
Testing at the word and sentence level showed differing results for the subjects completing the tasks.
A congenitally deaf subject with a history of non-hearing-aid use showed no improvements in the aided conditions whereas another subject with a history of very successful hearing aid used evidenced improvements in the aided condition for both sets of materials.
Testing at the level of connected discourse revealed improvements in the aided condition for two subjects but not for a subject with limited hearing aid experience.
The significance of the results and future research directions are discussed.
A sentence matching task is used to assess the processing effects of the ungrammaticality produced by violations of constraints on movement rules.
Surprisingly, no effects of ungrammaticality were observed either for violations of the Specified Subject Constraint or the Subjacency constraint.
These 'overgenerated' sentences could apparently be processed with the same fluency as fully grammatical controls, despite the fact that other types of ungrammaticality produced marked increases in matching times.
It is proposed that the matching task utilizes a level of mental representation at which overgenerated sentences are indistinguishable from fully grammatical sentences.
This implies a close correspondence between formal derivational mechanisms and features of the operation of the language processor.
Appearance models yield a compact representation of shape, pose and illumination variations.
The probabilistic appearance model, proposed by Moghaddam et al. (Moghaddam and Pentland, 1997; Tipping and Bishop, 1997b), has recently shown excellent performances in pattern detection and recognition, outperforming most other linear and non-linear approaches.
Unfortunately, the complexity of this model remains high.
In this paper, we introduce an efficient approximation of this model, which enables fast implementations in statistical estimation-based schemes.
Gains in complexity and cpu time of more than 10 have been obtained, without any loss in the quality of the results.
As indicated by Bourlard et al. (1996), the best and simplest solution so far in standard ASR technology to implement durational knowledge, seems to consist of imposing a (trained) minimum segment duration, simply by duplicating or adding states that cannot be skipped.
We want to argue that recognition performance can be further improved by incorporating “specific knowledge” (such as duration and pitch) into the recognizer.
This can be achieved by optimising the probabilistic acoustic and language models, and probably also by a postprocessing step that is fully based on this specific knowledge.
The widely available, hand-segmented, TIMIT database was used by us to extract duration regularities, that persist despite the great speaker variability.
Two main approaches were used.
In the first approach, duration distributions are considered for single phones, as well as for various broader classes, such as those specified by long or short vowels, word stress, syllable position within the word and within an utterance, post-vocalic consonants, and utterance speaking rate.
The other approach is to use a hierarchically structured analysis of variance to study the numerical contributions of 11 different factors to the variation in duration.
Whether this specific use of knowledge about duration in a post-processor will actually improve recognition performance still has to be shown.
However, in line with the prophetic message of Bourlard et al.'s paper, we here consider the improvement of performance as of secondary importance.
This paper presents statistics relating to various phoneme guessing algorithms.
The N-phoneme statistics were obtained by exhaustive analysis of a lexicon of 96,998 phonetic words.
The results show that by incorporating detailed phonotactic knowledge, coupled with broad phonetic knowledge, an algorithm can be formulated which successfully guesses the correct phoneme with mean success rates of up to 67%.
This implies that, as far as the computer is concerned, spoken English is, at a minimum, 67% redundant.
The results also show that the ability to guess correctly depends on word length and position; phoneme type and the number of unknown phonemes in the word have very little effect on the final results.
Temporal decomposition of a speech utterance results in a description of speech parameters in terms of overlapping target functions and associated target factors.
The former may correspond to articulatory gestures and the latter to ideal articulatory positions.
Although developed for economical speech coding, this method also provides an interesting tool for deriving phonetic information from acoustic speech signals.
The speech parameters used by Atal (1983) is proposing this method were the log-area parameters.
Our modified temporal decomposition method (Van Dijk-Kappers and Marcus, 1987, 1989) also works with log-area parameters as input.
However, the method is not restricted to these; in principle, most commonly used parameter sets can be used.
In this paper we compare the results obtained with nine different sets of speech parametes, including log-area parameters, formants, reflection coefficients and band-filter parameters.
The main criterion for good performance will be correspondence between target functions and phonemes or sub-phonemes.
The phonetic relevance of the target vectors will also be considered, but in less detail.
Speech signal resynthesis supplies yet another criterion; for those parameters sets which are transformable into the same parameter space, a reconstruction error will be defined and evaluated.
From these experiments it can be concluded that log-area parameters from the most suitable parameter set available for temporal decomposition.
In some respects band-filter parameters yield better results, but this set is not classified as the best due to properties related to resynthesis.
The prosodic structure of speech is the result of complex interactions within and between several different levels of organization.
The intonative hierarchy, which is essentially manifested by the nature of the prosodic markers, is the product of complex interactions and constraints within and across organizational levels.
Presented here is a model for predicting and interpreting the prosodic organization of spontaneous speech utterances.
This model is a hierarchical system composed of six modules: (1) sematic-pragmatic, (2) syntactic, (3) phonotactic, (4) accentual, (5) semantic adjustment, and (6) rhythmic.
For a given utterance, the system determines (i) the levels of the boundaries and prosodic markers on the basis of semantic information, and the syntactic structure as defined by the X-bar theory, and (ii) the accentual and rhythmic structures based on phonotactic constraints.
The phonetic step, which should transform the abstract labelling into acoustic values, is not presented here.
This model can and should be further developed.
Future enhancements will pertain to (a) the nature of the rules, (b) different aspects of conversation, and (c) theoretical considerations.
Concerning the latter point, current fruitful developments in X-bar theory are likely to lead to positive modifications in the prosodic model, which should enable it to account for certain unexplained phenomena.
However, even in its current state, the model produces highly convincing results, since it predicts the number and hierarchy of intonative and stress units in an utterance with a high accuracy rate.
The various rules found have led to the formulation of a computer program labelled PHONEMIA, which is the first module of a complete text-to-speech transcription project.
This program has also been used in dealing with the diphones encountered in Greek.
This paper discusses the nature of the data that form the input to linguistic descriptions, particularly with regard to the greater or lesser artificiality of such data.
The paper argues in favour of a much stronger emphasis in linguistic work on natural speech data even including spontaneous speech.
A major part of the paper is devoted to a general discussion of some preliminaries to the study of linguistic variation in natural speech.
We present a biologically motivated method for assessing the intelligibility of speech recorded or transmitted under various types of distortions.
The method employs an auditory model to analyze the effects of noise, reverberations, and other distortions on the joint spectro-temporal modulations present in speech, and on the ability of a channel to transmit these modulations.
The effects are summarized by a spectro-temporal modulation index (STMI).
The index is validated by comparing its predictions to those of the classical STI and to error rates reported by human subjects listening to speech contaminated with combined noise and reverberation.
We further demonstrate that the STMI can handle difficult and nonlinear distortions such as phase-jitter and shifts, to which the STI is not sensitive.
There are two types of classification methods using a Galois lattice: as most of them rely on selection, recent research work focus on navigation-based approaches.
In this paper, we define the structural links between decision trees and dichotomic lattices defined from the same table of data described by binary attributes.
Under this condition, we prove both that every decision tree is included in the dichotomic lattice and that the dichotomic lattice is the merger of all the decision trees that can be constructed from the same binary data table.
The efficiency of pattern recognition algorithms is highly conditioned to a proper definition of the patterns assumed to structure the data.
The multigram model provides a statistical tool to retrieve sequential variable-length regularities within streams of data.
In this paper, we present a general formulation of the model, applicable to single or multiple parallel strings of data having either discrete or continuous values.
The model is first assessed to derive an inventory of variable-length sequences of letters from text data, where all spaces between the words have been removed.
It turns out that the sequences of letters inferred during this fully unsupervised procedure clearly relate to the morphological structure of the text.
The model is then used to infer a set of variable-length acoustic units, directly from speech data.
Speech files containing examples of acoustic units are provided along with this paper in order to illustrate their consistency from an auditory point of view.
We also report experiments using these acoustically defined units for continuous speech recognition.
Bayesian networks are very good tools for representing uncertainty thanks to their clear graphical representation and the conditional probability values defined on that graph.
The structure and the probability values are usually given by an expert.
In this paper, we are interested in learning the structure of such networks from databases.
We have adopted a Bayesian approach, which allows to find an adequacy between the structure and the data.
Our starting point was two algorithms: K2 of Cooper and Herskovits and B of Buntine.
We have generated algorithm K2B that on one side benefits from the advantages of the two previous algorithms but on the other side reduces their drawbacks.
The implementation of K2B gave birth to Alexso, which is able to find a compromise between representativity and simplicity of the structure.
The aim of this paper is to present an outline of a theory of semantics based on the analogy between natural and computer programming languages.
A unified model of the comprehension and production of sentences is described in order to illustrate the central “compile and execute” metaphor underlying prodecural semantics.
The role of general knowledge within the lexicon, and the mechanism mediating selectional restrictions, are re-analyzed in the light of the procedural theory.
This paper reports the results of three projects concerned with auditory word recognition and the structure of the lexicon.
The first project was designed to experimentally test several specific predictions derived from MACS, a simulation model of the Cohort Theory of word recognition.
Using a priming paradigm, evidence was obtained for acoustic-phonetic activation in word recognition in three experiments.
The second project describes the results of analyses of the structure and distribution of words in the lexicon using a large lexical database.
Statistics about similarity spaces for high and low frequency wordswere applied to previously published data on the intelligibility of words presented in noise.
Differences in identification were shown to be related to structural factors about the specific words and the distribution of similar words in their neighborhoods.
Finally, the third project describes efforts at developing a new theory of word recognition know as Phonetic Refinement Theory.
The theory is based on findings from human listeners and was designed to incorporate some of the detailed acoustic-phonetic and phonotactic knowledge that human listeners have about the internal structure of words and the organization of words in the lexicon, and how they use this knowledge in word recognition.
The theory relies on several novel techniques to formalize strategies for search space reduction from large vocabularies when only partial information about the phonetic content of a word is available.
Taken together, the results of these projects demonstrate a number of new and important findings about the relation between speech perception and auditory word recognition, two areas of research that have traditionally been approached from quite different perspectives in the past.
The study of memory is a great challenge, perhaps the greatest in biological sciences.
Memory involves changes in a tiny fraction of an extremely large pool of elements, a conclusion that makes the task of finding those changes using current technologies formidable.
What can be done about this roadblock to neurological investigations of learning?
One response that has become particularly productive in recent years is to study learning or learning-like phenomena in relatively simple “model” systems.
The idea is to extract basic principles from these models in which molecular and anatomical details can be studied and then to use these in analyzing learning in higher regions of the brain.
In this article we discuss current progress and emerging concepts derived from the simple system approach using animal models.
We describe a stochastic component for spoken natural language understanding in an application for train travel information retrieval in French.
Another important issue concerns the iterative semantic labeling of large data amounts used for the component training.
The parser has been evaluated on both corrected and uncorrected speech recognizer output transcriptions.
The sidescan sonar records the energy of an emitted acoustical wave backscattered by the seabed for a large range of grazing angles.
The statistical analysis of the recorded signals points out a dependence according grazing angles, which penalizes the segmentation of the seabed into homogeneous regions.
To improve this segmentation, classical approaches consist in compensating artifacts due to the sonar image formation (geometry of acquisition, gains, etc.) considering a flat seabed and using either Lambert's law or an empirical law estimated from the sonar data.
The approach chosen in this study proposes to split the sonar image into stripes in the swath direction; the stripe width being limited so that the statistical analysis of pixel values can be considered as independent of grazing angles.
Two types of texture analysis are used for each stripe of the image.
The first technique is based on the Grey-Level Co-occurrence Matrix (GLCM) and different Haralick attributes derived from.
The second type of analysis is the estimation of spectral attributes.
The starting stripe at mid sonar slant range is segmented with an unsupervised classifier based on the SOFM (Self-Organizing Feature Maps) Kohonen algorithm.
The study made in this work is validated on real data acquired by the sidescan sonar Klein 5000.
Segmentation performances of the proposed algorithm are compared with those of conventional approaches (K-means).
One of the main challenges faced by the video game industry is to give life to believable Non-Player Characters (NPCs).
Research shows that emotions play a key role in determining the behavior of individuals.
In order to improve the believability of NPCs' behavior, we propose in this article a model of the dynamics of emotions taking into account the personality and the social relations of the character.
First, we present work from the literature on emotions, personality and social relations in Computer Science and in Human and Social Sciences.
We focus on the influence of personality on the trigerring of emotions, and of emotions on the dynamics of social relations.
Based on this work, we propose a dynamic model of the socio-emotional state and its implementation as part of a tool for game programmers aiming at the simulation of the evolution of emotions and social relations of NPCs based on their personality and role.
This paper introduces some recent studies on voice quality control and conversion technologies.
After briefly summarizing some basic scientific findings on the acoustic correlates of speech individuality, we review the latest developments in speech technologies related to voice control and speaker characteristic copying.
The main focus is on a survey of non-parametric methods for spectral segmental characteristics mapping between speakers, introducing some different types of spectral mapping methods that have evolved in relation to the speaker adaptation techniques being developed in speech recognition research.
This paper is devoted to Array Processing with fourth-order statistics.
After a brief review of the cumulant properties, we introduce an algebraic formalism for easy handling of higher-order multivariate statistics.
We define the «quadricovariance», which is an exhaustive representation of the fourth-order cumulants, and its orthogonal decomposition into «eigen- matrices».
In this notation, the strong formal analogy between the quadricovariance and the usual covariance is an indication that second- order methods may be directly extended to fourth-order statistics.
We also show that the source detection limit is higher with fourth-order statistics.
It goes from N − 1 sources with 2-MUSIC to 2(N − I) with 4-MUSIC operating on a linear equispaced array.
Using a non uniform linear array, it goes up to N(N − 1) sources.
The notion of eigenmatrix is then shown to provide a direct algebraic solution to the «blind source separation problem» which may be seen as an Array Processing problem where no information is available about the array manifold.
For assessing the synthesized speech output component in a complex application system, application-oriented evaluation methods and methodologies are needed which are not supplied by standardized test batteries so far.
Many standardized tests analyze synthetic speech mainly with regard to its form (surface structure), and only to a less degree with regard to the meaning that is assigned to it (deep structure).
In turn, in order to obtain a valid assessment focus for an application system, the functional aspect of speech (which depends on its deep structure) has to be taken into account.
In the paper two case studies are presented which focus on the acceptability of the synthesis component and its constituent dimensions in different application scenarios.
In the first one synthetic speech in a car navigation and traffic information system is assessed.
The second study relates to synthetic speech in a dialogue system.
The assessment is limited to laboratory experiments and avoids costly field tests.
It turns out that different dimensions contribute to a variable degree to overall acceptability, differently depending on the application scenario.
Application-oriented testing is thus required to identify the application-specific dimensions.
It is discussed which characteristics of the application have to be modeled in the assessment, and examples are given for both applications.
The standard problem of classifying cars based on acceleration and speed measures is addressed here.
An optimal discrimination strategy is methodically devised by considering 4 decision levels: the choice of the search space (variables selection through heuristics or evolutionary algorithms, discriminant analysis), the choice of the discrimination criterion (probabilistic or maximum margin), the choice of the classification algorithm complexity and the tuning of other algorithm parameters.
In the production of stop consonants by hearing-impaired subjects, voicing errors are far more frequent than place of articulation errors.
The problem arises from the phonetic complexity of some of the various kinds of homorganic plosives.
The English-speaking deaf encounter special difficulties in the pronounciation of the aspirated/ptk/ which require a precise temporal delay between oral closure and laryngeal opening.
The data presented in this paper show that the French-speaking deaf encounter similar difficulties in the production of the prevoiced/bdg/ which require a precise temporal delay between the onset of laryngeal vibrations and the release of oral closure.
The rapid sequential delivery of these two articulatory gestures enables the speaker to sustain the voice up to the end of the closure, which has decisive importance for voicing perception.
Our data also show that the speakers who correctly produce prevoicing can generally perceive the voicing feature.
Although the place of articulation of the stops is not better perceived than their voicing category, most of the moderately-deaf subjects in this experiment can produce the place distinctions perfectly.
The role of perceptual feedbacks in the mastering of articulatory gestures is discussed.
Specifically, it will be argued that principles of structural nature and principles of perceptual nature are in conflict in languages of the SOV type, because of the relative clause construction.
The way in which a relative clause is structured in an SOV language is an obstacle to its effective perceptual processing.
It will be argued that this conflict is one of the major factors determining the diachronic change of a language from an OV to a VO typology.
It is a totally interactive and open system which allows easy integration of new parameters.
A unifying presentation of source separation methods based on higher order statistics is derived.
The approach starts from the systematic scanning of the characteristic parameters of all the methods, like: the statistical hypotheses about the data ; the different kinds of models assumed, (standard or doubly orthogonal) ; the separation criteria, (indépendance), that lead to the sources restoration, written with moments or cumulants ; the effective principles of the separation, (direct trails based on a restoration matrix, indirect or global trails that estimate other characteristics of the sources, amplitudes etc…, before the true separation).
From a general standpoint, it is shown that, these methods yield a restoration of elements that belong simultaneously to two classes, more than a unique set of sources.
The first class, named second order class is associated with all random vectors that have the same covariance matrix.
The rationale of the second class, considered as a class of indépendance, stems from the invariance of mutual indépendance by any permutation operation.
The problem addressed in this article is that of automatically designing autonomous agents having to solve complex tasks involving several -and possibly concurrent- objectives.
We propose a modular approach based on the principles of action selection where the actions recommanded by several basic behaviors are combined in a global decision.
In this framework, our main contribution is a method making an agent able to automatically define and build the basic behaviors it needs through incremental reinforcement learning methods.
This way, we obtain a very autonomous architecture requiring very few hand-coding.
This approach is tested and discussed on a representative problem taken from the “tile-world”.
In current speech research, there is a need for large databases to be able to test production and perception models at different linguistic levels.
There are considerable problems in administering databases, both to label the speech and to easily access stored material.
In order to alleviate some of the problems we have created a speech analysis system.
Speech data are stored in sentence-sized files.
These files are segmented and transcribed semi-automatically given a phonetic transcription of the utterance.
This transcription is generated by the letter-to-sound rules of our text-to-speech system.
The emphasis on the database is the use for acoustic-phonetic research rather than the use in e.g. evaluation of speech recognizers.
This makes demands on flexible and linguistically specified retrieval patterns.
Our unorthodox solution to this is to use the synthesis rule structure, similar to the notation used in generative phonology, for accessing the data.
By a brief rule statement, speech segments meeting the specified contextual conditions can be identified.
Durational data can be collected directly during the database search.
Spectral analysis programs operating with a variety of spectral representations have also been created that display the result, typically as a mean/standard deviation spectrum or as a contour histogram spectrum.
The recognition rate for fluently spoken speech achieved with beam search algorithms depends on factors as the perplexity and the recombination structure of the language model, the difficulty of the vocabulary, the quality of the acoustic phonetic decoder, and the chosen pruning strategy.
In this paper a statistical model taking these factors in account is developed.
An iterative algorithm to calculate the score distribution of beam search paths and the resulting sentence error rate is presented.
It is therefore hardly surprising that one can ultimately wonder what kind of problem it is to which the central presence of the concept of representation in the linguistics of Gustave Guillaume is meant as a response.
If “to promote language to existence is to promote it to representation − without which nothing exists for the mind”, it is not because of the nature of language that the linguist resorts to the concept of representation, but because there is a certain way of being a linguist, which consists in posing language as an object for the mind, an attitude which is distinct, among many other solutions, from that consisting, for example, in seeing it as an object or a “parameter” of social life.
This paper describes the development and evaluation of the grapheme-to-phoneme sub-system of a complete real-time synthesis system under development at Macquarie University.
Evaluation and development of this system has been facilitated by using weighted statistics which reflect the frequency of occurence of each word in the LOB and Brown corpora of English.
These statistics are derived from a test word database which includes all acceptable Australian pronunciations (as defined by the Macquarie Dictionary) of each word, as well as their LOB and Brown frequency counts.
These scores facilitate decisions to be made about which alterations to the rules or lexicon will have the greatest effect on total system accuracy in ordinary running text (as reflected by the corpora frequencies).
The focus in automatic speech recognition (ASR) research has gradually shifted from isolated words to conversational speech.
Consequently, the amount of pronunciation variation present in the speech under study has gradually increased.
Pronunciation variation will deteriorate the performance of an ASR system if it is not well accounted for.
This is probably the main reason why research on modeling pronunciation variation for ASR has increased lately.
In this contribution, we provide an overview of the publications on this topic, paying particular attention to the papers in this special issue and the papers presented at 'the Rolduc workshop'.
Subsequently, the issues of evaluation and comparison are addressed.
Particular attention is paid to some of the most important factors that make it difficult to compare the different methods in an objective way.
Finally, some conclusions are drawn as to the importance of objective evaluation and the way in which it could be carried out.
This paper argues that the feature specification of a Swiss French deficient ça accounts for its syntactic distribution.
Contrary to the ordinary accusative clitics le, la, les, this pronominal form lacks number, gender and Case feature, but has a temporal-aspectual locative feature.
The arguments that support ça's caselessness are based on the following diagnostic: 1. (i) the impossibility of doubling the deficient ça, 2. (ii) the impossibility of ça being an enclitic and 3. (iii) its interaction with topicalisation and right dislocation which differs from what can be observed with ordinary clitics in this context.
Another distinction in the feature make-up of ça vs. ordinary clitics is the ambiguous categorical status of ça as both D and DP.
As a result of its feature composition, ça requires geric event quantification.
This interpretation is always available with transitive eventive verbs but with unaccusatives and transitive statives, the reading is blocked in the present tense.
It is demonstrated that the aspectual non-ambiguity of the Swiss French deficient ça, i.e. the fact that it can only appear in contexts of generic event quantification, is responsible for ungrammatical sequences such as ∗Je ça aime ('I that like') in this grammar.
The goal of this paper is to investigate various language model smoothing techniques and decision tree based language model design algorithms.
For this purpose, we build language models for printable characters (letters), based on the Brown corpus.
We consider two classes of models for the text generation process: the n-gram language model and various decision tree based language models.
In the first part of the paper, we compare the most popular smoothing algorithms applied to the former.
We conclude that the bottom-up deleted interpolation algorithm performs the best in the task of n-gram letter language model smoothing, significantly outperforming the back-off smoothing technique for large values of n.
In the second part of the paper, we consider various decision tree development algorithms.
Among them, a K-means clustering type algorithm for the design of the decision tree questions gives the best results.
However, the n-gram language model outperforms the decision tree language models for letter language modeling.
We believe that this is due to the predictive nature of letter strings, which seems to be naturally modeled by n-grams.
`Linguistic annotation' covers any descriptive or analytic notations applied to raw language data.
The basic data may be in the form of time functions – audio, video and/or physiological recordings – or it may be textual.
The added notations may include transcriptions of all sorts (from phonetic features to discourse structures), part-of-speech and sense tagging, syntactic analysis, `named entity' identification, coreference annotation, and so on.
While there are several ongoing efforts to provide formats and tools for such annotations and to publish annotated linguistic databases, the lack of widely accepted standards is becoming a critical problem.
Proposed standards, to the extent they exist, have focused on file formats.
This paper focuses instead on the logical structure of linguistic annotations.
We survey a wide variety of existing annotation formats and demonstrate a common conceptual core, the annotation graph.
This provides a formal framework for constructing, maintaining and searching linguistic annotations, while remaining consistent with many alternative data structures and file formats.
In visual representations of the acoustical signal of speech, such as the waveform or spectrogram, speech appears as a series of concatenated sequences of acoustical events, which vary in spectrum, amplitude and duration.
The results of a variety of psychoacoustical experiments, from auditory fusion to temporal masking to studies of streaming, can be interpreted as relevant to discovering the auditory capabilities used in listening to these speech sequences.
A sampling of such results serves to illustrate the connections between the psychoacoustics of speech and nonspeech, and to suggest guidelines for future work on non-speech temporal patterns, with the goal of a more complete psychophysics of complex sounds.
Within the framework of a research on cellular networks of radio communication, it is essential to be able to predict the area which would be covered by transmitters.
To study a transmitter, the standard method consists in applying an electromagnetic wave propagation model to various positions defined according to a constant spatial step.
Yet, that method leads to a considerable computation time which might become unexploitable in complex geographical environments.
There have already been some researches studying how to reduce that computation time.
They consist in the simplification of the propagation model used.
The processes in our article is complementary to them.
Indeed, our technique is independent of the model. The idea is to reduce the number of calculation points of the model.
The method presented here is based on an hypothesis which needs two elements to be confirmed: the segmentation of the signals measured by a mobile receiver ; a software used for the electromagnetic analysis of the geographic studied area.
Thus, the purpose is to segment the received signal into intervals corresponding to particular combinations of physical phenomena.
To do that, a representation suggested by Mallat and Zhong called “Wavelet Maxima Representation” is studied.
That decomposition allows the study of the derivative of a function at different scales.
We shall present a method of signal segmentation based on the maxima chaining through the scales of the decomposition.
The chaining helps us select the largest discontinuities of the signal and thus segment it.
Considering 2D trajectories is attractive since they form computable image features which capture elaborated spatio-temporal information on the viewed actions.
If they are embedded in an appropriate modeling framework, high-level information on the dynamic scene can then be reachable.
We aim at designing a general trajectory classification method that does not exploit strong a priori information on the scene structure, the camera set-up, the 3D object motions, while taking into account both the trajectory shape (geometrical information related to the type of motion and to variations in the motion direction) and the speed changes of the moving object on its trajectory (dynamics-related information).
Appropriate local differential features combining curvature and motion magnitude are defined and robustly computed on the motion trajectories.
A robust enough non-parametric feature extraction framework is also proposed since local differential features computed on the extracted trajectories are prone to be noise corrupted.
This article describes how the performance of a Dutch continuous speech recognizer was improved by modeling pronunciation variation.
We propose a general procedure for modeling pronunciation variation.
In short, it consists of adding pronunciation variants to the lexicon, retraining phone models and using language models to which the pronunciation variants have been added.
First, within-word pronunciation variants were generated by applying a set of five optional phonological rules to the words in the baseline lexicon.
Next, a limited number of cross-word processes were modeled, using two different methods.
In the first approach, cross-word processes were modeled by directly adding the cross-word variants to the lexicon, and in the second approach this was done by using multi-words.
Finally, the combination of the within-word method with the two cross-word methods was tested.
The word error rate (WER) measured for the baseline system was 12.75%.
Compared to the baseline, a small but statistically significant improvement of 0.68% in WER was measured for the within-word method, whereas both cross-word methods in isolation led to small, non-significant improvements.
The combination of the within-word method and cross-word method 2 led to the best result: an absolute improvement of 1.12% in WER was found compared to the baseline, which is a relative improvement of 8.8% in WER.
Hierarchical clustering techniques have been shown to be a powerful tool in building speaker-independent reference models for Dynamic Time Warping (DTW) based speech recognition systems.
In this paper, we introduce a clustering algorithm based on the standard KMEANS procedure that generates reference models for continuous density Hidden Markov Model (HMM) based systems by simultaneously considering spectral and duration information.
Improved speech recognition performance using clustering is demonstrated on a digit recognition task using the TI/NBS studio quality connected digit database.
This paper retraces, in an informal way, some of the history of speech synthesis and speech research from Von Kempelen's speaking machine to linear prediction.
In this paper, we propose a preliminary framework for accounting for certain surface F 0 variations in speech.
The framework consists of definitions for pitch targets and rules of their implementation.
Pitch targets are defined as the smallest operable units associated with linguistically functional pitch units, and they are comparable to segmental phones.
The implementation rules are based on possible articulatory constraints on the production of surface F 0 contours.
Due to these constraints, the implementation of a simple pitch target may result in surface F 0 forms that only partially reflect the underlying pitch targets.
We will also discuss possible implications of this framework on our understanding of various observed F 0 patterns, including carryover and anticipatory variations, downstep, declination, and F 0 peak alignment.
Finally, we will consider possible interactions between local and non-local pitch targets.
We recall the enhancement by polynomial filtering principle of the numerical Braille relief image.
We describe the recognition method based on two orthogonal axis projection of each Braille character.
The five recognition steps are developed taking account of form defaults of relief and using a maximum likehood method.
The position axis dispersion of Braille characters permits to calculate estimated theoretical error.
The error rate, verified in practice for manual made Braille reliefs is about 1 %.
The description of endangered languages is crucial for linguistic typology, as a possible source of information about types of structures not attested in the languages described so far.
Conversely, good information about the state of knowledge on linguistic typology is particularly useful for linguists dealing with under-described languages, which is generally the case for endangered languages.
However, typologists must be conscious that, in the search for typological generalizations, putting on a par well-alive and well-documented languages, and languages whose unique description cannot be checked or completed anymore, may lead to distorted conclusions.
Moreover, it is not possible to work on moribund languages in the same way as on languages still used by a community as its usual means of communication, and one may have doubts on the representativity of data collected in such conditions, in particular in domains such as syntax.
In this paper we introduce a novel concept: the cube transversals on the cube lattice of a categorical database relation.
The problem of finding cube transversals is a sub-problem of hypergraph transversals since it exists an order-emhedding from the cube lattice to the power set lattice of binary attributes.
Based on this fact, we propose a levelwise algorithm for mining minimal cube transversals.
We applied this concept by introducing a new OLAP functionality; the emerging version space.
It is the difference between two uni-compatible datacubes or the most frequent elements in the difference.
Finally we propose a merging algorithm mining the boundary sets of emerging version spaces without computing the two related datacubes.
Subtle variations in voice (phonatory) quality may reveal aspects of the speaker's mood and attitude, and are thus an important aspect of speaking style.
This paper illustrates current research being carried out by the authors on the voice source correlates of a range of such quality differences.
The voice qualities looked at include modal, breathy, whispery, tense, lax and creaky voice, following the description of Laver (1980) and analyses presented here focus on a word extracted from a propose passage read with these qualities.
The principal method used for analysing the voice source involved inverse filtering of the speech wave.
In order to quantify the source characteristics, a four parameter voice source model (the LF-model) was matched to the inverse filtered waveform.
Frequency domain analyses of the speech waveform, based on narrow band spectral sections and on spectral averaging, were also carried out.
Detailed comparisons of the data measured directly from the glottal waveform and those measured from the speech output yield insights which could not be inferred from either alone.
Results suggest a number of important differences between the qualities as well as considerable dynamic variation within a single quality.
The data should also prove useful for resynthesis, which is an important tool for testing perceptual aspects of voice quality; e.g., the attitudinal and emotional colouring that may be associated with particular voice qualities.
This article gives a simple method to accelerate the k nearest neighbours algorithm.
Despite a sensible increase in the error rate, this method should be useful in the applications that need a classification algorithm with a speed constraint.
This paper presents a new theoretic tool based on Information Theory, the main interest of which is to acutely evaluate the classification tools.
The particular nature of real-world objects recognition involves us to design systems based on multi-points of view approaches.
We show that neural networks allow to learn the fusion function, optimized to the data and the structure of the composite system.
The performance of a composite recognition system is closed to the partition of the available information on each classification tools.
A Genetic algorithm is designed to adapt the parameters space partition with the set of classification tools among the quality of the composite system, genetic algorithm.
We present “Transcriber”, a tool for assisting in the creation of speech corpora, and describe some aspects of its development and use.
Transcriber was designed for the manual segmentation and transcription of long duration broadcast news recordings, including annotation of speech turns, topics and acoustic conditions.
It is highly portable, relying on the scripting language Tcl/Tk with extensions such as Snack for advanced audio functions and tcLex for lexical analysis, and has been tested on various Unix systems and Windows.
The data format follows the XML standard with Unicode support for multilingual transcriptions.
Distributed as free software in order to encourage the production of corpora, ease their sharing, increase user feedback and motivate software contributions, Transcriber has been in use for over a year in several countries.
As a result of this collective experience, new requirements arose to support additional data formats, video control, and a better management of conversational speech.
Using the annotation graphs framework recently formalized, adaptation of the tool towards new tasks and support of different data formats will become easier.
This article explores further the question of when the Muslims began to use suffixed invocations of God's curse on the Franks (Crusaders), a topic that we have addressed previously in this journal.
The common use of such invocations took some time to develop after the Franks' arrival in the Levant, and previously we argued that this was primarily due to the suffixed invocation used by our earliest source failing to “catch on” with contemporaries, while later forms spoke more directly to the needs of the people of the time.
Since the publication of that article we have had the opportunity to work with the original manuscript of our earliest source, which has revealed that the author did use a form of suffixed invocation that became popular later, but the manner in which he did so was sufficiently ambiguous to hinder its adoption by other writers.
If articulatory movements can be estimated, then the articulatory parameters which represent the motion of the articulatory organs would be useful for speech recognition.
This paper discusses an effective method of estimating articulatory movements and its application to speech recognition.
Firstly, what is described is a method of estimating articulatory parameters known as the model matching method, and various spectral distance measures are evaluated for this method.
The results show that the best in average is the higher order cepstral distance measure, which is one of the peak weighted measure.
Secondly, articulatory parameters are utilized for the recognition of vowels uttered by unspecified speakers.
It is shown that the adaptation of the model by the estimated mean vocal tract length is effective to normalize speaker difference.
Thirdly, the motor commands to move the articulatory organs are estimated considering articulatory dynamics, and the continuous vowels are recognized by means of these estimated commands.
It has been found that a considerable part of the coarticulation effects can be compensated for by this command estimated, and the method is useful for continuous speech recognition.
In accordance with the principles of the Tone Sequence Model the F0 contour is analyzed as a series of discrete target values that are connected by means of transitional functions.
The set of example utterances can be obtained at the address ftp kiwi.nmt.edu (or internet address ftp 129.138.1.82).
More detailed instructions can be found in the ToBI-Guidelines or in the README file.
It is also possible to get an audio tape of the utterances and a printed paper copy of the labelling guide and F0 tracks at this address:
ToBI Labelling Guide, c/o Mary Beckman, Ohio State University, Linguistics Department, 222 Oxley Hall, 1712 Neil Avenue, Columbus, OH 43210-1298, USA.
Utterances from ToBI and the Boston Radio News Corpus were used for the evaluation of the generation rules: root mean squared error (RMSE) and correlation between generated and original contour were determined, and in a perception test native speakers assessed the quality of the resynthesized contours which, in general, were judged to sound natural and show few differences to the corresponding originals.
A robust variational setting is proposed for 1D signal registration and applied to the computation of shape geodesics for shape classification issues.
This approach is extended to be applied for matching images of shape sequences.
This geometric approach is mainly addressed to poorly contrasted images where the intensity-based registration fails.
For validation purposes, experiments are carried out on real signals and images issued from marine biological archives which depict a high interindividual variability such that registration-based approaches are of particular interest.
The purpose of this work was to investigate the speech comprehension of four deaf Hebrew-speaking patients implanted with a cochlear prosthesis, the Nucleus 22-channels (N-22) system.
Experiments were performed under two conditions:
The speech tests (isolated vowels, bisyllabic words and fluent speech in closed and open sets) were first conducted using the Default Frequency Boundaries (DFBs) of the cochlear implant's speech processor.
The Default Frequency Boundaries of each electrode which are specified by the computer program of the system, are assumed to be selected on the basis of English.
The patients were then retested using the same speech material, and the results were compared with those previously obtained.
As a result of the Modified Frequency Boundaries, improvements in the patients' comprehension of the speech elements were noted.
The differences in performance between the two sets of frequency boundary distributions suggest that better speech comprehension could be achieved by implanted patients, at least partly, by adjusting the frequency-to-electrode mapping of the N-22 speech processor on a language basis.
Standard articulation tests are not always sensitive enough to discriminate between speech samples which are of high intelligibility.
One can increase the sensitivity of such tests by presenting the test materials in noise.
In this way, small differences in intelligibility can be magnified into large differences in articulation scores.
We used both a more conventional articulation test and a monosyllabic adaptive speech interference test (MASIT) to evaluate the intelligibility of nine different speech-coding techniques.
We found different patterns of responses for the articulation test and MASIT.
These differences can be explained by the fact that different speech-coding schemes code different acoustic-phonetic properties of the speech signal.
Some of these properties are more liable to masking by interfering noise than others.
Our results show that, in the case of synthetic speech, differences in intelligibility are not always magnified by adding interfering noise: they may even disappear.
Studied from the Antiquity, the art of arguing was in turn a central topic of teaching, then object of many criticisms.
It was finally rehabilitated as a fundamental support for recent developments of sciences and technology of communication.
Indeed, these last decades, the emergence of dialogue systems, of natural language generation systems, and of multi-agent systems around a core of argumentation as well as developments of modelling based on both formal and informal approaches of the argumentation representation has supported this rehabilitation.
This article proposes a synthesis of this work.
It attempts to measure the adequacy of various approaches to the modeling of the natural argumentation and underlines the difficulties which remain to be surmounted.
The assistant planning system for argumentation APLA developed by the authors with an aim of attenuating these difficulties is also presented in this article.
Speech analysis shows that the second formant transitions in vowel–vowel utterances are not always of the same duration as those of the first formant transitions nor are they always synchronised.
Moreover the formant transitions often move initially in a different direction from their final target.
In order to investigate whether these deviations from linearity and synchrony are perceptually significant a series of listening tests have been conducted with the vowel pair /a/–/i/.
It was found that delays between the first and second formant transitions of up to 30 ms are not perceived, nor are differences in duration of up to 40 ms if the first and second formants start or end simultaneously.
If the second formant transition is symmetric in time with respect to the first formant differences of up to 50 ms are tolerated.
Excursions in second formant transition shape of up to about 500 Hz are also not perceived.
These results suggest that most of the deviations from linearity and synchrony found in natural vowel–vowel utterances are not perceptually significant.
Transformational methods have made of the acceptability test the instrument of measure and classification par excellence in linguistics.
Of the various a priori possible parameters which can influence the result of this test, it is especially the factor of semantic interpretation which is considered here.
We would like to show the necessity of dissociating an extralinguistic part of this interpretation (including the universe of discourse adopted by the informant, the culture to which he belongs, and the importance of certain factors of knowledge — or of ignorance — of the world) from a linguistically pertinent hypothetical interpretation which correlates very closely with questions of syntax or morphology.
Extra-linguistic semantic concepts are 'natural', come immediately to the mind and screen the research of much more abstract, unfamiliar and probably unconscious linguistic semantic concepts.
This is a boundary which it is necessary to draw in semantics between what is linguistic and what is not, i.e., between what must and what cannot appear in a generative grammar.
We insist here on the necessity in linguistics of controlling, and thus of varying, the universes of discourse which can modify the acceptability of a given sentence.
The argument centres around several uses of the verb planter ('to plant').
In the field of scene analysis for computer vision, a trade-off must be found between the quality of the results expected, and the amount of computer resources allocated for each task.
Using an adaptive vision system provides a more flexible solution as its analysis strategy can be changed according to the information available concerning the execution context.
We describe how to create and evaluate a visual attention system tailored for interacting with a computer vision system so that it adapts its processing according to the interest (or salience) of each element of the scene.
We propose a new set of constraints named PAIRED to evaluate the adequacy of a model with respect to its different applications.
We justify why dynamical systems provide good properties for simulating the dynamic competition between different kinds of information.
We present different results that demonstrate that our results are fast and highly configurable and plausible.
The closed-class hypothesis asserts that function words play a privileged role in syntactic processes.
In language production, the claim is that such words are intrinsic to, identified with, or immanent in phrasal skeletons.
Two experiments tested this hypothesis with a syntactic priming procedure.
In both, subjects tended to produce utterances in the same syntactic forms as priming sentences, with the structures of the self-generated sentences varying as a function of differences in the structures of the primes.
Changes in the closed-class elements of the priming sentences had no effect on this tendency over and above the impact of the structural changes.
These results suggest that free-standing closed-class morphemes are not inherent components of the structural frames of English sentences.
In everyday communicative situations not all parts of the spoken message are pronounced equally clear.
Especially words bearing a high load of semantic information are put in focus by the speaker.
The question of how this is realized in natural spontaneous and read speech, and whether resulting knowledge can be applied in synthetic speech to improve naturalness and acceptability, is subject of this study.
By introducing a “peak-and-level” model we examined spectral and temporal aspects in focus and non-focus words from spontaneous speech material and from the same texts, read out after orthographic transcription.
Audio recordings were made of a professional male speaker, whose voice and pronunciation also served as a model for the diphone-based component of the Dutch national speech synthesis program.
For a number of acoustic parameters it can be concluded that there is a clear difference, both in “peak values” and in “level values”, between the two natural speech styles, but that the peak values display comparable contrasts to the level values in both styles.
The results of our measurements in natural speech were compared to the data of the same texts synthesized by the Dutch diphone text-to-speech system.
In a pilot experiment, varying temporal aspects in the synthesized speech, listeners were asked to judge the naturalness and intelligibility in order to determine the starting-point for future evaluation of text-to-speech synthesis including peak-and-level contrasts.
A simple formant-estimating speech processor has been developed to make use of the “hearing” produced by electrical stimulation of the auditory nerve with a multiple-channel cochlear implant.
Thirteen implant patients were trained and evaluated with a processor that presented the second formant frequency, fundamental frequency, and amplitude envelope of the speech (F 0 F 2).
Nine patients were trained and evaluated with a processor that presented the first and second formant frequencies, fundamental frequency, and first and second formant amplitudes (F 0 F 1 F 2).
The F 0 F 1 F 2 group performed significantly better in discrimination tasks and word and sentence recognition through hearing alone.
The F 0 F 1 F 2 group also showed a significantly greater improvement when hearing and lipreading was compared with lipreading alone in a speech tracking task.
A study of spondeerecognition in noise with hearing alone indicated that the added first formant information produced an improvement that was equivalent to a 5 dB increase in the signal-to-noise ratio.
The problem of binary classifier combination is adressed in this article.
This approach consists in solving a multi-class classification problem by combining the solutions of binary sub-problems.
We consider two strategies in which each class is opposed to each other, or to all others.
The combination is considered from the point of view of the theory of evidence.
The classifier outputs are interpreted either as conditional belief functions, or as belief functions expressed in a coarser frame.
They are combined by computing a belief function that is consistent with the available information.
The performances of the methods are compared with those of other techniques and illustrated on various datasets.
It is well-known that complexities exist in the mapping between the acoustic information in the speech signal and the phonetic categories of adult language users.
We investigated whether the same complexities exist in the mapping between the speech signal and the forerunners of these categories in infants.
For two classes of complexity, we found that the manner in which the categorization of information for speech occurs was virtually identical in infant and adult listeners.
These findings indicate that the infant possesses finely tuned linguistically-relevant perceptual abilities, which undoubtedly facilitate and shape the task of language acquisition.
An algorithm for recognition of connected words has been adapted to an application for mobile radio telephony.
For this purpose, several manners of generating feature vectors were evaluated using two databases collected in a small car moving at about 120 km/h.
The databases contain digits and digit strings uttered via handset and in hands-free mode for each of 10 speakers.
In the first phase, experiments were done using signal analysis resulting in linear scaled magnitude filterbank coefficients in the feature vector.
This approach achieved a digit error rate of 6.0% on 7-digit strings collected via handset and employing isolated word training.
The second approach replaced the magnitudes of the filterbank coefficients by coefficient energy, with the energy being scaled logarithmically.
The results derived in the simulation environment were verified in a car using the product hardware.
In this article, we propose to study a speech coding method applied to the recognition of phonemes.
The proposed model (the Neural Predictive Coding, NPC) and its two declinations (NPC-2 and DFE-NPC) is a connectionist model (multilayer perceptron) based on the non linear prediction of the speech signal.
We show that it is possible to improve the discriminant capacities of such an encoder with the introduction of signal membership class information as from the coding stage.
As such, it fits in with the category of DFE encoders (Discriminant Features Extraction) already proposed in literature.
In this study we present a theoretical validation of the model in the hypothesis of unnoised signals and gaussian noised signals.
NPC performances are compared to that obtained with traditional methods used to process speech on the Darpa Timit an Ntimit speech bases.
Simulations presented here show that the classification rates are clearly improved compared to usual methods, in particular regarding phonemes considered difficult to process.
A small vocabulary word recognition experiment is provided to show how NPC features can be used in a more conventional speech ANN-HMM based system approach.
As was reported at IVTTA-94, the NYNEX VoiceDialingSM service was first deployed in the NYNEX region in mid 1993.
It has since been deployed in several other Bell Operating Companies' regions and substantial new developments have taken place.
One of them was a transition, starting in 1995, from the hardware implementation of a DTW-based recognition algorithm to the implementation, on a general-purpose DSP, of a continuous density multi-Gaussian mixture HMM-based algorithm.
As a result, it has been possible to expand the service beyond speaker-dependent name recognition to speaker-independent continuous digit recognition (SICDR) and voice-activated network control (VANC) command recognition.
This paper highlights major efforts in this transition and provides a more detailed description of the system's speech recognition components.
A new language model is proposed to cope with the scarcity of training data.
Each multiple class is assigned by a grouping process based on the left and right neighboring characteristics.
Furthermore, by introducing frequent word successions to partially include higher order statistics, multi-class N-grams are extended to more efficient multi-class composite N-grams.
In comparison to conventional word tri-grams, the multi-class composite N-grams achieved 9.5% lower perplexity and a 16% lower word error rate in a speech recognition experiment with a 40% smaller parameter size.
It was found that the F 2 locus-nucleus differences were smaller in spontaneous speech than in reference words, in non-prominent syllables than in prominent syllables, and in given words than in new words.
These results were interpreted as reflecting differences in coarticulation, both an anticipatory effect of vowel on the preceding consonant and/or formant undershoot.
F 2 locus-nucleus patterns appeared to depend on duration, confirming Lindblom's model (1963), they may also be influenced by other factors such as speaker adaptations to the communicative situation.
In voiced speech segments, natural noise is the sum of modulation noise due to the irregularly vibratory patterns of the vocal folds, and additive noise due to the air stream in a glottal chink.
The noise content can be evaluated by the signal-to-noise ratio (SNR).
The first section of the paper reviews the various time and frequency domain SNR measurement techniques.
In the second section, a new SNR measurement technique is introduced, which approximates the harmonic content of speech wave spectra by means of a spectral synthesis-by-analysis algorithm.
In the final section, the technique presented is employed to perform SNR measurements on synthetic vowels, and normal and pathological voice signals.
The SNR measurement only allows a rough screening with respect to the disorder.
The problems involved in the measurement of noise are discussed.
We describe a method for an automatic character string understanding from scanned maps.
This method takes into account oriented strings that occur frequently on maps.
The application is split in three main modules: connected components analysis, string construction, connected character detection.
Results seem sufficient to allow an operational application if syntactic rules are added to ameliorate the warning process
This paper describes Elsag's Large Vocabulary Isolated Word Recognition system Dspell.
The system makes use of a diphone-based speech model and an extremely efficient word decoding algorithm, and is implemented on Elsag's multiprocessor emma-2.∗ dspell requires a very convenient training session and features a high recognition performance and extremely fast response on lexicons of up to 10,000 words.
This paper deals with the enhancement of noisy speech signals recorded in a car for mobile radio applications.
Our concern is the signal estimation when 1 or 2 observations are available; each one is composed of a speech signal, s i, and an additive noise n i (i = 1, 2).
In the case of one microphone, we consider new techniques that capitalize on the aspect of speech perception by focusing on enhancing only the short-time spectral amplitude.
A “modified spectral subtraction” method is proposed; it uses a frequency-dependent over-estimate of the noise and can be combined with segmentation of the observation.
When two microphones are available, in the case of uncorrelated (or slightly correlated) noises, we introduce a new technique based on the coherence function which is used to filter the observations or to determine a speech/noise classification algorithm.
Finally, listening tests have been conducted to compare the simplest methods.
In the case of stationary noise, the modified spectral subtraction is very promising and in the case of non-stationary and decorrelated noises, the method based on the coherence is more attractive.
Twenty semantically unpredictable sentences were generated in each of the five selected syntactic structures.
These basic structures were defined as a crosslinguistic methodology for corpus generation in a European environment.
Responses from twenty listeners during five sessions are also analysed.
The distribution shows a strong relationship between the proportion of correct sentences (p s) and of correct words (p w).
The ratio r = Log(p s)/Log(p w) seems to be a powerful index for measuring the complexity of a spoken message.
Data replotted from the literature confirm the hypothesis that the higher the contextual (semantic, syntactic, etc.) information in a sentence, the lower this index r.
In that perspective, the index r could be related to the number of decision units listeners must deal with when listening to a sentence.
Speech synthesizers distort the comprehension of sentences.
The distribution of omissions and mistakes does not obey the binomial law that would be expected from a simple model, where all input units have the same independent probability to be correctly identified.
Analysis of the discrepancy between the experimental distribution of word errors and the binomial distribution obtained from the simple model provides a fruitful explanation of the fact that the linguistic relations between words allow a correction of “theoretically misunderstood” words and a distortion of “theoretically understood” words.
Such a phenomenon mainly depends on linguistic content of the sentences that may be quantified by means of the suggested index r.
It also shows second order variations due to other factors such as subjects' compentence and training, or the acoustic degradation of the message.
In most speaker identification or verification systems it is assumed that all speakers have the same feature covariance matrices.
This assumption simplifies the classification algorithm since it yields a linear classifier.
Speakers, however, differ not only in their mean feature vector, but in their covariance matrix as well.
The use of a speaker's individual covariance matrix results in a quadratic classifier.
If a common covariance matrix is assumed, a common optimal feature space can be determined.
With an individual covariance matrix, each speaker is optimally recognized in an individual feature space.
The recognition scheme therefore requires the matching of an unknown speaker with the templates defined over different feature spaces.
The use of the quadratic classifier together with the individual feature space is shown to drastically improve recognition accuracy while the added memory requirements are shown to be negligible.
The suggested quadratic classifier, with individual optimal feature vectors, has been tested using a speaker identification system with six male speakers.
In terms of a given separation measure, the quadratic classifier yields improvements of about 2 times over the conventional method.
Cross-linguistic comparisons may shed light on the levels of processing involved in the performance of psycholinguistic tasks.
For instance, if the same pattern of results appears whether or not subjects understand the experimental materials, it may be concluded that the results do not reflect higher-level linguistic processing.
In the present study, English and French listeners performed two tasks - click location and speeded click detection - with both English and French sentences, closely matched for syntactic and phonological structure.
Clicks were located more accurately in open- than in closed-class words in both English and French; they were detected more rapidly in open- than in closed-class words in English, but not in French.
The two listener groups produced the same pattern of responses, suggesting that higher-level linguistic processing was not involved in the listeners' responses.
It is concluded that click detection tasks are primarily sensitive to low-level (e.g. acoustic) effects, and hence are not well suited to the investigation of linguistic processing.
This study investigated the temporal coordination of the articulators involved in French Cued Speech.
Cued Speech is a manual complement to lipreading.
It uses handshapes and hand placements to disambiguate series of CV syllables.
Hand movements, lip gestures and acoustic data were collected from a speaker certified in manual Cued Speech uttering and coding CV sequences.
Experiment I studied hand placement in relation to lip gestures and the corresponding sound.
The results show that the hand movement begins up to 239ms before the acoustic onset of the CV syllable.
The target position is reached during the consonant, well before the vowel lip target.
The results show that the handshape formation gesture takes a large part of the hand transition.
Both experiments therefore reveal the anticipatory gesture of the hand motion over the lips.
The types of control for vocalic and consonantal information transmitted by the hand are discussed in reference to speech coarticulation.
Finally the temporal coordination observed between Cued Speech articulators and the corresponding sound was used as rules to control an audiovisual system delivering Cued Speech for French CV syllables.
Biometrics, which refers to identifying an individual based on his/her physical or behavioral characteristics, has gained in popularity among researchers in signal processing during recent years.
It has also focused the attention of medias since the tragic events of September 11th, 2001.
We first introduce the notion of biometrics.
Then, we describe the architecture of biometric systems and the metrics used to evaluate their performances.
We briefly discuss the most common biometrics and the different ways to combine them to obtain multimodal systems.
Finally, we present applications of biometrics.
This paper explores how conversants co-ordinate their use and interpretation of language in a restricted context.
It revolves around the analysis of the spatial descriptions which emerge during the course of 56 dialogues, elicited in the laboratory using a specially designed computer maze game.
Two types of analysis are reported.
The first is a semantic analysis of the various types of description, which indicates how pairs of speakers develop different language schemes associated with different mental models of the maze configuration.
The second analysis concerns how the communicants co-ordinate in developing their description schemes.
The results from this study would suggest that language processing in dialogue may be governed by local principles of interaction which have received little attention in the psychological and linguistic literature to date.
A new combination of coding methods for a 64 kbit/s transmission system for typical videophone situations is investigated.
The codec structure is based on a standard hybrid discrete cosine transform (DCT) codec with temporal prediction.
The picture is divided blockwise into changed and unchanged areas.
One motion vector with subpel accuracy is computed and transmitted for each block of the changed area.
For the forward analysis, the prediction error is calculated in the whole picture.
Only the blocks with the highest prediction errors are updated by a DCT with a perception adaptive quantization.
The number of DCT update blocks depends on the remaining bits after the transmission of the overhead information.
The codec is controlled by a forward analysis of the prediction error and is not based on a buffer control.
The spatial resolution of the source signal is reduced in two steps to prevent a codec overload caused by too much activity between two frames.
It is proposed that All F are G is often given a 'structure-neutral' interpretation, as All, F, G, lacking a subject-predicate distinction.
In the first experiment, children aged 6–7–8, and 11–12, and adults, acted out instructions like “Make a building in which all the yellow blocks are square”.
The experiment demonstrated the dominance in children and decline with age of structure-neutral interpretations.
In a second experiment, with the same age groups, propositions of the form All F are G, varying as to the factual inclusion relations expressed, were presented as the major premises of syllogistic items.
The results indicated the presence of structure-neutral interpretations under some circumstances in adults as well as children, and also demonstrated the existence in all subjects of a 'pragmatic processing' mode that becomes less obligatory with age.
In pragmatic interpretations, meaning is determined by previously known factual relations between the things which the words represent, rather than by grammatical relations between the words themselves.
This paper deals with quality assessment of color images dedicated to quantization algorithms.
Two methods are described in details, one using the baker's transform and the other using the matrix of local pallets.
In this assessment campaign, the results of the described techniques are compared to those of standard algorithms such as median cut, octree and split & merge.
Both objective and subjective assessment are performed.
The need of subjective evaluation comes from the fact that the usual metrics do not integrate the HVS (Human Visual System).
While the color is more considered as perceptual property than a quantitative data, these standard metrics fail in describing image distortion.
The preliminary results of the assessment campaign show that the described methods give good results with regards to the former ones.
This article reports on a study of the capacity of prosody to predict upcoming discourse boundaries.
More specifically, it is investigated whether the approaching end of a route description can be pre-signalled by melodic and temporal characteristics.
Experiment 1 brings to light that listeners are able to estimate on the basis of such prosodic properties how far a given utterance is situated from the end of a description.
However, the scope of this prosodic prediction is relatively restricted as listeners can only estimate the absolute discourse position of the last two utterances of the monologue analyzed.
Experiment 2 is run in order to explore systematically, by means of a test with synthetic speech, to what extent melodic and durational properties are sufficient to influence finality judgments.
In recent years, both automatic speech recognition (ASR) and text-to-speech (TTS) conversion systems have attained quality levels that allow inclusion in everyday applications.
One remaining problem to be solved in both these types of applications is that alleged phone inventories of specific languages are commonly expanded with phones from other languages, a problem that becomes more acute in an increasingly internationalized world where multilingual automatic speech-based services are a desideratum.
This paper investigates the nature of phone set expansion in Swedish.
The status of these phones is discussed, and since such added phones do not have a phonemic (or allophonic) function, the term `xenophones' is suggested.
The results show that very few subjects resort to full rephonematization and that xenophonic expansion is the rule, although there is an uneven distribution depending on particular phones, spanning from phones produced by most subjects, to phones produced by almost no subjects.
Of the possible explanatory factors analyzed – regional background, gender, age and educational level – the latter is by far the most important.
In a dialogue, there are at least two sorts of boundaries between discourse units.
One type of boundary signals the end of a topical unit; another type of boundary the end of a turn at talk.
These two do not necessarily coincide, as a speaker may wish to a new topical unit without wanting to be interrupted by his interlocutor.
In order to test whether prosodic cues can differentiate unambigously between topic and turn boundaries, a series of production experiments was set up in which topic-finality and turn-finality were varied independently, and in which visual and non-prosodic verbal cues could not be used.
In the most complex condition, the speaker had to give clear cues for topic finality, while not prematurely losing the floor.
In this condition, speakers avoided using low tones at turn-internal topical boundaries, reserving them to signal turn-final topic boundaries.
When listeners were confronted with portions of the description taken out of their contexts, they could reliably differentiate between turn-final and non-turn-final topical units.
Interestingly, when the final parts of a topical unit were removed, listeners could still discriminated between turn-final and non-turn-final expressions, apparently basing themselves on other, more global, prosodic cues.
This holds similarly for both minimally and maximally incomplete units.
In the past, educators relied on classroom observation to determine the relevance of various pedagogical techniques.
Automated language learning now allows us to examine pedagogical questions in a much more rigorous manner.
We can use a computer-assisted language learning (CALL) system as a base, tracing all user responses and controlling the information given out.
We have thus used the Fluency system [Proceedings of Speech Technology in Language and Learning, 1998, p. 77] to answer the question of what voice a language learner should imitate when working on pronunciation.
In this article, we will examine whether there should be a choice of model speakers and what characteristics of a model's voice may be important to match when there is a choice.
In this paper, first, we present the problem of Non Cooperative Target Recognition (NCTR) as a supervised classification problem.
KNN algorithm has been executed initially on CPU with Matlab and then on GPU.
Arithmetic operations and memory access pattern has been studied to get the best parallelization.
Finally, we conclude with a discussion about possible perspectives for the proposed method especially by focusing on other representation spaces or other classification methods.
Current enterprise modelling approaches describe generic task designs that do not capture actual work practices.
However, several improvement areas in organisations are related to the particular ways that individuals have of implementing tasks.
This research seeks to enrich enterprise modelling with a methodological approach to capture and model work practices, using a model centered on organizational agents and contexts.
The approach allows the identification and analysis of personal and inter-personal work patterns.
Context- based representations are acquired from action repositories.
The approach is illustrated with a case study.
Results on the automatic discovery of personal contexts using clustering are reported.
This paper deals with the problem of learning radial basis function neural networks to approximate non linear L 2 function from R d to R.
Hybrid algorithms are mostly used for this task.
Unsupervised learning techniques are used to estimate the center and width parameters of the radial functions and supervised learning techniques are used to estimate the linear parameters.
Supervised learning techniques are generally based on the least squares (LS) estimates (or criterion).
This estimator is optimal when the training set (z i,y i)i=1,2,..,q is composed of noisy outputs yi, i = l,..,q and exactly known inputs z i, i = 1,.., q.
However, when collecting the experimental data, it is seldom possible to avoid noise when measuring the inputs z i.
The use of least squares estimator produces a biased estimation of the linear parameters in the case of noisy input output training data set, which leads to an erroneous output estimation.
This paper proposes the use of an estimation procedure based on the error in variables model to estimate the linear parameters (for supervised learning) when the training set is made up of input and output data corrupted by noise.
The geometrical interpretation of the proposed estimation criterion is given in order to illustrate its advantage with respect to the least squares criterion.
The improved performances in non linear function approximation is illustrated with a simulation example.
An important area of speech recognition is automatic recognition of connected digit strings (i.e., sequences composed of the digits zero through nine, and oh).
Applications of this technology include credit card authorization, catalog ordering, dialing of telephone numbers, and data entry.
For the past two years AT&T has experimented with a system for automatic recognition of 10 digit merchant identification codes, and 15 digit customer credit card numbers, for the purpose of authorizing purchases charged to a credit card.
Our evaluation used data collected from about 1000 customers who provided 2000 connected digit strings over 800-based dialed up telephone connections.
The recognizer correctly recognized 97% of the digit strings with no rejections using constraints on the validity of both merchant identifications and credit card numbers.
Several schemes for applying these task constraints in a practical implementation are discussed in this paper.
Also, recognition of the dollar amounts of the transaction are presented with some preliminary results.
With a cochlear implant, deaf people are able to understand speech under good listening conditions.
Problems occur in adverse conditions, e.g. in reverberant and/or noisy environments.
Experiments were carried out to improve the intelligibility of noisy speech using two different single-channel noise suppression techniques.
This was realized by preprocessing, i.e. by applying the resynthesized speech signal to the cochlear implant system.
Intelligibility tests were carried out in cooperation with the medical department.
Instabilities of the human voice source appear in normal voices under certain conditions (newborn cries, vocal fry, creaky voice) and are symptomatic of voice pathologies.
Vocal instabilities are intimately related to bifurcations of the underlying nonlinear dynamical system.
We analyse in this paper bifurcations in 2-mass models of the vocal folds and study, in particular, how the incorporation of the vocal tract affects bifurcation diagrams.
A comparison of a simplified model (Steinecke and Herzel, 1995) with an extended version including vocal tract resonances reveals that essential features of the bifurcation diagrams (as e.g. frequency locking of both folds and toroidal oscillations) are found in both model versions.
However, vocal instabilities appear in the extended model at lower subglottal pressures and even for weak asymmetries.
Document Analysis and Recognition consist in translating their images into an electronic form that can be reusable.
The analysis extracts the document layout structure from its image, and the recognition assigns to the layout structure components their logical functions in the document.
In this article, we present our work on recognition of a category of documents in which the logical structure is based on typographical tagging such as table of contents.
We propose a perceptual approach that extracts these typographical tagging directly from document images.
However, the structures of such documents are complex and variable.
Our goal is to consider the document structure recognition problem even though these difficulties occur.
We developed a automatic recognition system based on a hybrid model combining a bayesian classifier and a probabilistic automaton.
The classifier is responsible of drawing a correspondence between text blocks extracted from document images and basic logical entities, while the automaton deals with grouping these entities into a hierarchical logical structure.
This hybrid model is built by semi-supervised learning based on knowledge provided by the user on the one hand, and the typographical properties of our documents, on the other hand.
This system has been experimented for automatic indexing of tables of contents in periodicals and journals.
The complexity and the variability of these documents allow us to show the efficiency of the approach.
Preemphasis of the speech signal at higher frequencies is a preprocessing step employed in various speech processing applications.
In the present paper, the effect of preemphasis on vowel recognition performance is studied.
Preemphasis of the speech signal is achieved by the first-order differencing of the speech signal.
Cepstral coefficients derived through linear prediction analysis are used as recognition parameters.
A minimum distance classifier is used for vowel recognition and the recognition performance is studied for four different distance measures: Euclidean distance measure, correlation distance measure, Mahalanobis distance measure and Itakura distance measure.
It is shown that preemphasis of the speech signal brings about a deterioration in the vowel recognition performance.
Implications of this result for isolated word recognition are also discussed.
The paper presents a study of syllabic structures and their variation in a large corpus of French radio interview speech.
A further aim is to show how automatic speech recognition (ASR) systems can serve as a linguistic tool to consistently explore virtually unlimited speech corpora.
Automatically selected subsets can be manually checked to accumulate knowledge on pronunciation variants.
To focus on sequential variants, a methodology has been set up using descriptions at the phonemic, syllabic and lexical levels.
This study reports on a radio corpus composed of 30 one-hour shows of interviews.
Less well described phenomena are also observed, such as other vowels (/u/, /ε/, /i/ and /a/) being deleted in a non-final (unstressed) position.
Unstressed CV syllables, when preceded by an open syllable, are likely to undergo syllabic restructuring: vowel deletion together with backward onset-coda transfer.
Complex syllables tend to be simplified: liquid consonants have a tendancy to be deleted, more often in coda than onset position.
The most deletion-prone consonant is /v/, in both onset and coda positions.
Finally, a substantial percentage of word-final schwa syllables may completely disappear and short function words are deletion prone whatever the vowel identity.
This article deals with the problem of affective states recognition from physical and physiological wearable sensors.
Given the complex nature of the relationship between available signals and affective states to be detected we propose to use a statistical learning method.
We begin with a discussion about the state of the art in the field of statistical learning algorithms and their application to affective states recognition.
Then a framework is presented to compare different learning algorithms and methodologies.
Using the results of this pre-study, a global architecture is proposed for a real time embedded recognition system.
Instead of directly recognizing the affective states we propose to begin with detecting abrupt changes in the incoming signal to segment it first and label each segment afterwards.
The interest of the proposed method is demonstrated on two real affective state recognition tasks.
Two central assumptions of current models of language acquisition were addressed in this study: (1) knowledge of linguistic structure is “mapped onto” earlier forms of non-linguistic knowledge; and (2) acquiring a language involves a continuous learning sequence from early gestural communication to linguistic expression.
The acquisition of the first and second person pronouns ME and YOU was investigated in a longitudinal study of two deaf children of deaf parents learning American Sign Language (ASL) as a first language.
Personal pronouns in ASL are formed by pointing directly to the addressee (YOU) or self (I or ME), rather than by arbitrary symbols.
Thus, personal pronouns in ASL resemble paralinguistic gestures that commonly accompany speech and are used prelinguistically by both hearing and deaf children beginning around 9 months.
This provides a means for investigating the transition from prelinguistic gestural to linguistic expression when both gesture and language reside in the same modality.
The results indicate that deaf children acquired knowledge of personal pronouns over a period of time, displaying errors similar to those of hearing children despite the transparency of the pointing gestures.
The children initially (ages 10 and 12 months) pointed to persons, objects, and locations.
Both children then exhibited a long avoidance period, during which one function of the pointing gesture (pointing to self and others) dropped out completely.
During this period their language and cognitive development were otherwise entirely normal, and they continued to use other types of pointing (e.g., to objects).
When pointing to self and others returned, it was marked with errors typical of hearing children; one child exhibited consistent pronoun reversal errors, thinking the YOU point referred to herself, while the other child exhibited reversal errors inconsistently.
Evidence from experimental tasks conducted with the first child revealed that pronoun errors occurred in comprehension as well.
Full control of the ME and YOU pronouns was not achieved until 25–27 months, around the same time when hearing children master these forms.
Thus, the study provides evidence for a discontinuity in the child's transition from prelinguistic to linguistic communication.
It is argued that aspects of linguistic structure and its acquisition appear to involve distinct, language-specific knowledge.
We have developed a spontaneous speech dialogue system TOSBURG II, employing keyword-based spontaneous speech understanding and multimodal response generation, with adaptive speech response cancellation.
Since in multimodal interaction, the user understands the system's response by a visual output before its speech response is completed, the user often interrupts the system's speech response.
Therefore, our adaptive speech response cancellation serves to facilitate natural human-computer interaction by allowing the user's interruption.
We have also developed an evaluation environment for dialogue data collection and the performance of TOSBURG II.
Unlike conventional data collection systems, TOSBURG II collects in this environment not only speech data and the final results of speech understanding but also its intermediate results as dialogue data, to use them for the evaluation and improvement of the system.
Speech recognition was used to automate directory assistance in a 6-month trial with Bell Canada's public customers.
The bilingual application gave the caller a choice at the beginning of the dialog to continue in English or French.
Over 89% of calls were either partially or fully automated.
Customer and operator reactions to the system were positive.
Bell-Northern Research's flexible vocabulary recognizer, using a vocabulary of 1,700 city names and synonyms, performed well under real world conditions.
Economically significant operator work time savings were demonstrated.
This article describes a speech-based user interface to a wide range of entertainment, navigation and communication applications in mobile environments by means of human-machine dialogues.
The system has been developed in the framework of the EU-project SENECA.
It uses noise reduction, speech recognition, and dialogue processing techniques.
One interesting aspect relies in the fact that low speech recognition confidence and word-level ambiguities are compensated by engaging flexible clarification dialogues with the user.
The SENECA system demonstrator has been evaluated by means of user tests.
With speech input, road safety, especially for complex tasks is significantly improved.
Compared to manual input, the feeling of being distracted from driving is less important with speech.
This paper presents the results of a speaker-independent, isolated word speech recognition system developed for information access over Australian public switched telephone network (PSTN).
The recognition system is based on Continuous Density Hidden Markov Modelling (CDHMM).
The speech database was collected over the PSTN from a large variety of speakers and different geographical locations.
The database contained a vocabulary of 55 words consisting of 41 country names and their variations plus a few control words.
The recognition performance, tested on 100 other speakers (50 males and 50 females) with no grammar constraint, resulted in an overall recognition rate of 97.3%.
This paper describes the HMM training methodology, which consisted of three stages: hand segmented seed model training, automatic word segmentation and reestimation.
To facilitate the future implementation of the recognition system in a DSP environment, a fast frame synchronous Viterbi algorithm was implemented with no degradation in recognition performance.
The end-point detection is performed by the combination of the silence/noise model with the word models.
For confusable word pairs, sub-word models are used to improve the recognition rate.
A post-processing approach is used to enhance the performance of the recognition system, in which all ranked candidates from the Viterbi decoding are subject to the tests of the minimum word duration and the likelihood difference between the first candidate and the second candidate.
For practical applications speech recognition systems need to be insensitive to differences between training and test acoustic conditions.
Differences in the acoustic environment may result from various sources, such as ambient background noise, channel variations and speaker stress.
These differences can dramatically degrade the performance of a speech recognition system.
A wide range of techniques have been proposed for achieving noise robustness.
This paper considers one particular approach to model-based compensation, predictive model-based compensation, which has been shown to achieve good noise robustness in a wide range of acoustic environments.
The characteristic of these schemes is that they combine a speech model with an additive noise model, a channel model and, in the general case, a speaker stress model, to generate a corrupted-speech model.
The general theory of these predictive techniques is discussed.
Various approximations for rapidly performing the model combination stage have been proposed and are reviewed in this paper.
The advantages and the limitations of such a predictive approach to noise robustness are also discussed.
In addition, methods for combining predictive schemes with schemes which make use of speech data in the new environment, adaptive schemes, are detailed.
This combined approach overcomes some of the limitations of the predictive schemes.
The growing size of the web and the heterogeneity of accessible information made relevant information extraction (IE) more and more complex from web pages.
In the context of information gathering on restricted domains of the web, this work is focused on web page adaptive IE (AIE) systems that can be adapted to new domains through training on annotated corpora as input.
WEPAIES performances are evaluated on three standard corpora, more or less structured, and compared to performances of other adaptive information extraction systems.
Using a vocabulary of 12 nonsense words, the authors have developed a method for the objective study of the auditory performance of patients fitted with cochlear implants.
The random repetition of the 12 words in the vocabulary produced a list of 129 items which were read out to the patients. Each reading simultaneously tested several features in the vocabulary.
The statistical analysis of the results covered the recognition of the words, consonants and basic features; several recognition percentage associations were also considered.
The results are given for 10 sessions held with two patients fitted with Chorimac cochlear prostheses.
They show that: (1) the different tests are not equivalent (even for the associations of percentages). (2) some important features of the Chorimac prosthesis were met such as a good discrimination of the plosive-fricative opposition and a bad distinction of voicing, and (3) in this work, there is no evidence of help being derived from lip reading; this leads to a discussion of the sensitivity of the evaluation method.
The topic of this paper is the approximation of the Value Function in Reinforcement Learning.
We present a method for modeling the Value Function that allocates memory resources while the agent explores its environment.
The model of Value Function is based on a Radial Basis Functions Network, with Gaussian units.
The model is build incrementally by creating new units on-line, when the system enters unknown regions of the space.
The parameters of the model are adapted using gradient descent and the Sarsa(X) algorithm.
The method do not require a model of the environment neither does it involve the estimation of a model.
The performance of the proposed method is demonstrated on two well-known benchmark problems: the Acrobot and the Bioreactor.
Both problems are simulated in real-valued state space and discrete time.
English undergraduate students, studying in London, consistently misspelled Gandhi as Ghandi despite intensive exposure to the correct spelling of the name.
This is an exemplary misspelling in a number of ways that we detail in this paper.
We conclude that, even with minuscule lexical knowledge of 'Indian' words and names, English readers use 'rules' in such tasks.
This gives us hope that, once a correct spelling has been achieved it will be maintained, for new rules (presumably) replace old ones.
In this paper we propose an agent communication approach based on social commitments and arguments.
In this approach, agents must use their reasoning capabilities to reason about their mental states before acting on commitments or on their contents in a relevant way.
In order to enable them to choose the most relevant arguments at each step of the dialogic interaction, this approach allows the agents to use two types of reasoning: strategic and tactical.
The strategic reasoning allows an agent to choose its global communication plan in terms and to select the constraints which it decides to satisfy.
In addition, tactical reasoning allows agents to locally choose, at each turn, the most relevant argument according to the adopted strategy.
The performance of speech enhancement algorithms deteriorates rapidly with decreasing signal-to-noise ratio (SNR).
At a low SNR, high-intensity phonemes such as vowels are therefore more likely to be enhanced than low-intensity speech segments such as many consonants.
Although the selective enhancement of vowels enhances transitional cues for consonant recognition, it simultaneously degrades relative amplitude cues.
Experiments with normal-hearing subjects were performed to determine the overall effect of selective enhancement of vowels on the intelligibility of consonants in consonant–vowel–consonant utterances.
In quiet, a 12-dB enhancement of the vowels did not significantly reduce consonant intelligibility compared with an unenhanced control condition at 65 dB (A).
When unenhanced utterances were presented in background noise with an average SNR of −6 dB at the vowel segments, 50.1% of the consonants were correctly identified while 69.8% of consonants were recognised in a condition where the consonant SNR remained unchanged but where the vowels were selectively amplified by 12 dB.
Equal enhancement of the vowels and consonants by 12 dB, however, led to 91.5% consonant recognition.
We conclude that speech enhancement algorithms should enhance all speech segments to the greatest possible extent, even if this leads to selective enhancement of some phoneme categories over others.
In order to evaluate this faithfulness, we propose to visualize any measure associated to the data by coloring the corresponding Voronoï cells in the projection space, and we define specific measures.
We experiment these techniques with the Principal Component Analysis and the Curvilinear Component Analysis applied to artificial and real databases.
The present tutorial paper is addressed to a wide audience with different discipline backgrounds as well as variable expertise on intonation.
The paper is structured into five sections.
In Section 1, “Introduction”, basic concepts of intonation and prosody are summarised and cornerstones of intonation research are highlighted.
In Section 2, “Functions and forms of intonation”, a wide range of functions from morpholexical and phrase levels to discourse and dialogue levels are discussed and forms of intonation with examples from different languages are presented.
In Section 3, “Modelling and labelling of intonation”, established models of intonation as well as labelling systems are presented.
In Section 4, “Applications of intonation”, the most widespread applications of intonation and especially technological ones are presented and methodological issues are discussed.
In Section 5, “Research perspective” research avenues and ultimate goals as well as the significance and benefits of intonation research in the upcoming years are outlined.
Spectral analysis of non-stationary signals calls for specific tools which permit one to describe a time evolution of frequency characteristics.
Such tools, referred to as time-frequency representations, can be defined in an objective way when imposing a priori constraints.
Within a stochastic and non-parametric framework, two main approaches are offered, which either emphasize a doubly orthogonal decomposition, or preserve the usual concept of frequency.
After having established the corresponding definitions and emphasized the importance of the Wigner-Ville transform, estimation problems are addressed and a discussion is provided for supporting the usefulness of time-frequency representations in processing operations which go beyond a mere description.
Several translations of the Speculum humanae salvationis have been written and copyied in the Burgundy during the second half of the fifteenth Century.
In this context, the translation of Miélot, ordered by the Duke Philip the Good, and its two manuscripts, in which we count a minute, has been thought as a failure.
The comparison of the textual and codicological features of the Miélot's translation with the three other contemporary french translations in prose permits to arise the specificity of Miélot's art of translation and of mise en livre.
This paper illustrates the advantages of using the Discrete Cosine Transform (DCT) as compared to the standard Discrete Fourier Transform (DFT) for the purpose of removing noise embedded in a speech signal.
The derivation of the Minimum Mean Square Error (MMSE) filter based on the statistical modelling of the DCT coefficients is shown.
Also shown is the derivation of an over-attenuation factor based on the fact that speech energy is not always present in the noisy signal at all times or in all coefficients.
This over-attenuation factor is useful in suppressing any musical residual noise which may be present.
The proposed methods are evaluated against the noise reduction filter proposed by Y. Ephraim and D. Malah (1984), using both Gaussian distributed white noise as well as recorded fan noise, with favourable results.
Cooperation between agents, whatever their nature may be, requires a set of representations to be implemented.
According to its own capacity in perceiving and possibly reasoning, each agent constructs a more or less rich representation of the situation in which he or she is led to cooperate.
The decisions he makes strongly depend on the quality of this representation.
It is shown how perception, reasoning and action are dependent within the general framework of the cooperative activity.
For that, we observe the relation between the co-operative activity and the individual one.
Then, we examine the case of the man/machine co-operation seen as particular case of the co-operation between agents, for which the machine must be equipped with adapted representations and must make it possible to the user to build powerful representations.
Modelling for the design of knowledge based systems brings a base ofprimarily conceptual representation.
Two points of view must indeed cohabit in co-operative systems; that of the organization in which the co-operation intervenes and that of the user who cooperates with the system to carry out a task.
In addition to the conceptual model of the application, we will thus find in the co-operative systems a model of co-operation (point of view of the organization) and a model of user.
Many writers have argued that dialogue should be regarded as a joint activity (see for example (Clark and Wilkes-Gibbs, 1986; Grosz and Sidner, 1990; Schegloff, 1981; Suchman, 1987)), something that agents do together, rather than simply as a product of the interaction of plan generators and recognizers working in synchrony and harmony, as plan-based theories propose.
Such plan-based approaches do not explain why addresses ask clarification questions, why they confirm, or even, why they do not walk away.
Rather, the joint action model claims that both parties to a dialogue are responsible for sustaining it.
Participating in a dialogue requires the conversants to have at least a joint commitment to understand one another.
The key questions to be answered include how to formalize such general commitments precisely, and to show how they predict the fine-grained synchrony so apparent in ordinary conversation.
To begin to answer these questions, we sketch here how a formal theory of joint action explains confirmations that arise in task-oriented telephone dialogues.
A more formal account is given in (Cohen and Levesque, 1991a).
Then we argue that extensions of this analysis to dialogue more generally will be difficult.
In particular, it will force us to give up our simplistic analyses of propositional content and literal meaning.
The system fits into a 386-based personal computer and runs under MS-DOS.
Among its most appealing features are the very quick and application-independent procedure for training the system to the user's voice and the easy way in which application dictionaries can be created.
What makes short training possible is the fact that the system extracts target spectral distributions of phonemes from the training words and combines this speaker-dependent information with speaker-independent information concerning other aspects of the speech signal such as energy profile, phoneme durations, and phoneme similarity.
The system accepts natural language without any restriction and can be used to dictate documents of any kind, provided the dictionary of the application domain is specified.
Dictation, however, is not the only application we envisage.
In this paper we describe our approach to speech recognition and provide some information on the actual implementation and performance of our system.
We describe a new stochastic model for generating speech signals suitable for coding at low bit rates.
In this model, the speech waveform is represented as a zero mean Gaussian process with slowly-varying power spectrum.
The optimum innovation sequence is obtained by minimizing a subjective error criterion based on properties of human auditory perception.
Each block of 40 samples (representing 5 ms of the speech signal sampled at 8 kHz) of the innovation signal is coded into one out of 1024 randomly generated Gaussian sequences of length 40.
The chosen sequence minimizes a spectrally weighted error criterion.
The innovation signal is thus encoded at 2 kbits/s.
A time-varying linear filter whose parameters are determined directly from the speech signal is used to produce the desired power spectrum.
Even at this low bit rate the resynthesized speech is barely distinguishable from the original.
A new explanatory and formal theory in speech production is presented.
This theory is founded on perturbation theory and supported by computer simulation.
Sensitivity functions which relate variations in formant frequency to small variations in area functions are revised in a manner that allows for their application to large variations in area functions.
Available speech data are shown to confirm this theory, and known phonetic, articulatory and acoustic phenomena can be explained and embedded in a simple, global and formal model.
The theory provides new insights into some principles of articulatory-acoustic relations and the application of these principles to phonetic theory.
In this paper, several implications are studied: the articulatory-acoustic relation is simplified, the quantal nature of speech is confirmed, phonetic universals and phonetic systems are considered from a new point of view, formant transitions are explained and normalized, and an easy to control 9-parameter model proposed.
We also consider other speech phenomena as interpreted by this theory such as compensatory and symmetry effects.
The theory can also be used to formulate and calculate basic speech dynamic patterns.
Several applications and research perspectives are proposed.
A familiar aspect of English pronunciation is the occurrence of syllabic consonants.
It is common to treat consonantal syllabicity as a consequence of vowel elision, implying that, for example, the pronunciation of “button” as /bλtn/ is the realisation of underlying /bλtən/ (or even /bλt⊃n/).
Since elision is a phenomenon that is subject to the influence of speaking style, it would seem to follow that in rapid or casual speech we should expect to find more cases of syllabic consonants and fewer cases of unstressed vowels followed by continuant consonants.
This paper sets out to show that the phenomenon is not this simple: we look at problems that confront our attempts at the automatic recognition of syllables and other sub-word units, and consider phonotactic and phonetic factors that may help to resolve them.
This article presents a critical edition and study of a 17th/18th-century poetry collection that had previously been mistaken for al-Ṯaʿālibī's lost Kitāb al-Ġilmān.
It provides a codicological analysis of Berlin MS Wetzstein II 1786 in which the poetry collection is contained and also explains and corrects long-held misconceptions regarding al-Ṯaʿālibī's connection with the text.
This paper describes the Speech Synthesis Markup Language, SSML, which has been designed as a platform independent interface standard for speech synthesis systems.
The paper discusses the need for standardisation in speech synthesizers and how this will help builders of systems make better use of synthesis.
Next the features of SSML (based on SGML, standard generalised markup language) are discussed, and details of the Edinburgh SSML interpreter are given as a guide on how to implement an SSML-based synthesizer.
The performance of a speech recognizer is often degraded by noise.
Part of the reason for this performance degradation is due to the fact that there is often a strong mismatch between the training and the testing conditions, i.e. the recognition features used in the training case are vastly different from the features used in the testing condition because of the effect of the noise.
One way to circumvent this mismatch problem is to use features which are less susceptible to changing noise conditions.
In this paper, we propose the use of a family of signal limiters for recognition of noisy speech.
The signal limiter, when properly scaled, is equivalent to performing an arcsin transformation on the autocorrelation functions of the original signal.
The effect of using the signal limiter as preprocessor is to reduce the variability of the feature vector, so that the mismatch between training and testing conditions in noise is reduced.
Testing on a 39-word English alpha-digit vocabulary, in a speaker trained mode, indicates that the recognition performance of a template-based, dynamic time-warping (DTW) recognizer can be significantly improved in noisy conditions when the robust signal limiter is used as a pre-processor to reduce the variability of the features in strong mismatch conditions.
Microarrays allow monitoring of thousands ofgenes over time periods.
However, due to the low number of time points of the gene expression series, taking the temporal dependences into account when clustering the data is an hard task.
Moreover, classes very interesting for the biologist, but sparse with regard to all the other genes, can be completely omitted by the standard approaches.
We propose a Bayesian approach for this problem.
A mixture model is used to describe and classify the data.
The parameters of this model are constrained by a prior distribution defined with a new type of model that expresses our prior knowledge.
These knowledge allow to take the temporal dependences into account in natural way, as well as to express rough temporal profiles about classes of interest.
We review factors that have affected the synthesis of high-quality speech by analysis-synthesis.
The influence of a selected subset of these factors on the quality of synthesized speech was evaluated through listener preference judgements by comparing natural speech to the synthetic speech of two synthesizers: linear prediction coding (LPC) and formant.
Several synthesizer excitation waveforms were considered.
These waveforms included critical parameters that replicated selected glottal timing events, e.g., the instants of glottal closure and glottal opening.
In addition, identifying voiced/unvoiced/mixed excitation and silent intervals in the speech waveform and measuring the fundamental frequency of voicing contributed to the synthesis of high-quality speech.
A two-channel approach to speech analysis is recommended to aid the automatic processing of speech, where one channel is the conventional acoustic signal, while the other channel is the electroglottogram (EGG).
To obtain an accurate phone sequence from a continuous speech signal, we suggest a novel approach consisting of tightly coupled bottom-up and top-down processing.
The bottom-up path consists of segmentation, recognition and labeling.
Also the top-down path consists of labeling, speech generation and segmentation.
In this manner, the four processes form a closed feedback loop achieving an optimal interpretation efficiently for a given noisy observation of speech signal and a priori knowledge.
The major goal of this paper is to identify the system model using both the stochastic estimation theory and the mean field theory.
Experimental results are obtained in terms of the TIMIT database.
As a result, the overall system can transform the incoming continuous signal into one of the 61 phone classes at the rate of 73.7%.
One should notice here that the system forces the user to provide 5 signatures that are roughly of the same style (with similar total length and total duration).
In fact, we used clustering algorithms and we studied the impact on the performances of the system of: the number of classes; the clustering feature space; the algorithm used.
The process to determine the classes consists in using a clustering algorithm on a learning dataset that contains the biometric profiles of several signers.
In fact, observing the result of the clustering, it seems that for too many signers, the different signatures in their profile end up in different classes, not in a unique one, which indicates that the clustering is not stable, despite our constraint on the signatures of the profile.
An algorithm is presented which is designed to yield reference pitch contours of speech signals for a comparative evaluation of pitch determination algorithms.
For the task of pitch determination the question of error analysis in general and measurement accuracy in particular is discussed.
It is shown that the inaccuracy of the algorithm must be less than 0.5% in order to render fine measurement errors inaudible.
The algorithm uses the output signal of a laryngograph which measures the laryngeal vibrations via the changes of the electric conductance of the larynx.
The point of inflection during the rapid rise of the laryngogram is taken as the estimate for the instant of glottal closure.
Using a local interpolation filter the quantization error of the measurement is kept below 0.5%.
This paper argues that human cognition employs two modes of learning, s-mode and u-mode.
S-mode learning takes place by means of abstract working memory and is delective and reportable.
U-mode learning occurs outside abstract working memory and is unselective and unavailable for verbal report.
Three experiments are described, employing two tasks which have previously been shown to give rise to two modes of learning consistent with the conceptualisations of s-mode and u-mode learning.
The experiments explore the effects of introducing a secondary verbal task on learning, performing and relearning these tasks.
The secondary task can be expected to interfere with s-mode learning.
It was found that adding the secondary task interfered with performing and relearning one of the tasks, the one for which subjects could verbalise what they had learnt.
In contrast, for the other task, for which subjects could not verbalise their learning, performance and relearning were facilitated by adding the secondary task.
These data argue for two modes of learning, only one of which involves the verbal system.
The basic hypothesis is that cry vocalizations of hearing-impaired infants differ from those of their counterparts with normal hearing abilities due to the lack of auditory feedback.
The listening experiment shows that it is possible for experts to auditorily classify cries for both infant groups, based on the voice related and melodic cry features.
The cries of profoundly hearing-impaired infants are different regarding their perceived sound, rhythm and melody.
The sound may well be correlated to spectral characteristics, and melodic and rhythmic parameters are extracted which differ significantly for the two infant groups.
The findings are discussed in the context of a cry production model.
The extracted signal parameters enable an automatic classification of the cries by means of topological feature maps, which may later be used as the basis for an early supplementary diagnostic tool.
In this paper we address the segmentation problem in a Bayesian framework.
Of the three stages (modelling, estimation, optimisation), we consider modelling and optimisation.
We consider modelling by Markov random fields.
We demonstrate the limitations of the Potts model currently employed, and propose a new model (the chien model) which allows us to control the boundary length and lines in the segmented images.
We also preserve fine structures in the data.
Then, we compare the MPM and MAP criteria when used with the algorithms discussed above.
Results are presented on synthetic images and SPOT data.
The classification problem is tackled in a second part.
Hypermedia such as web sites and CD-Rom are very rich but poorly structured resources where a user may easily get lost.
It is then necessary to propose a user-adapted help system that allows a user to use efficiently such hypermedia.
This is a problem of user modelling where we want to model the user's browsing behaviour, that is related to his goals.
This paper deals with learning and recognition of user behaviours in hypermedia through the use of Markov models.
A crucial step in processing speech audio data for information extraction, topic detection, or browsing/playback is to segment the input into sentence and topic units.
Speech segmentation is challenging, since the cues typically present for segmenting text (headers, paragraphs, punctuation) are absent in spoken language.
We investigate the use of prosody (information gleaned from the timing and melody of speech) for these tasks.
Results show that the prosodic model alone performs on par with, or better than, word-based statistical language models – for both true and automatically recognized words in news speech.
Across tasks and corpora, we obtain a significant improvement over word-only models using a probabilistic combination of prosodic and lexical information.
Inspection reveals that the prosodic models capture language-independent boundary indicators described in the literature.
For example, pause and pitch features are highly informative for segmenting news speech, whereas pause, duration and word-based cues dominate for natural conversation.
Automatic speech recognition of real-live broadcast news (BN) data (Hub-4) has become a challenging research topic in recent years.
This paper summarizes our key efforts to build a large vocabulary continuous speech recognition system for the heterogenous BN task without inducing undesired complexity and computational resources.
These key efforts included: • automatic segmentation of the audio signal into speech utterances; • efficient one-pass trigram decoding using look-ahead techniques; • optimal log-linear interpolation of a variety of acoustic and language models using discriminative model combination (DMC); • handling short-range and weak longer-range correlations in natural speech and language by the use of phrases and of distance-language models; • improving the acoustic modeling by a robust feature extraction, channel normalization, adaptation techniques as well as automatic script selection and verification.
The starting point of the system development was the Philips 64k-NAB word-internal triphone trigram system.
On the speaker-independent but microphone-dependent NAB-task (transcription of read newspaper texts) we obtained a word error rate of about 10%.
Now, at the conclusion of the system development, we have arrived at Philips at an DMC-interpolated phrase-based crossword-pentaphone 4-gram system.
This system transcribes BN data with an overall word error rate of about 17%.
This paper shows how an articulatory model, able to produce acoustic signals from articulatory motion, can learn to speak i.e. coordinate its movements in such a way that it utters meaningful sequences of sounds belonging to a given language.
This complex learning procedure is accomplished in four major steps:
(a) a babbling phase, where the device builds up a model of the forward transforms i.e. the articulatory-to-audio-visual mapping; (b) an imitation stage, where it tries to reproduce a limited set of sound sequences by audio-visual-to-articulatory inversion; (c) a “shaping” stage, where phonemes are associated with the most efficient available sensori-motor representation; and finally, (d) a “rhythmic” phase, where it learns the appropriate coordination of the activations of these sensori-motor targets.
This paper introduces a new tool for web usage mining that relies on an automatic analysis of the web user activity and on an interactive session visualization.
The analysis step summarises the information before the visualization step.
It clusters the web user logs using the bio-mimetic relational clustering algorithm Leader Ant and produces a set of representative profiles for each cluster, whose definition relies on the computation of typicality degrees.
The proposed tool was tested and evaluated on real web log files from the museum of Bourges website, showing that it can easily produce meaningful visualizations of typical user navigation.
Automatic Selection of clinical Trial based on Eligibility Criteria (ASTEC) project is to automate, so as to make it systematic, the search of cancer clinical trials, by reusing the patient data contained into an oncologic electronic health record.
ASTEC project tackles two major scientific challenges for medical informatics:
1) the syntactic and semantic interoperability between information systems.
The oncologic electronic medical records and the recruitment decision system must be interoperable.
The ASTEC project proposes a framework of syntactico-semantic interoperability based on international standards.
Generic methods of mediation and reasoning based on ontologies are developed to match data from the electronic medical records to the inclusion/exclusion criteria of clinical trials;
2) a decision support system for recruitment. We have developed inference methods on the electronic medical records adapted to the data structure as well as the eligibility criteria.
this paper, we present and justify our choices, concerning the medical process in oncology and the scientific and technical aspects.
This paper gives an overview of the Philips research system for phoneme-based, large-vocabulary, continuousspeech recognition.
The system has been successfully applied to various tasks in the German and (American) English languages, ranging from small vocabulary tasks to very large vocabulary tasks.
Here, we concentrate on continuousspeech recognition for dictation in real applications, the dictation of legal reports and radiology reports in German.
We describe this task and report on experimental results.
We also describe a commercial PC-based dictation system which includes a PC implementation of our scientific recognition prototype.
In order to allow for a comparison with the performance of other systems, a section with an evaluation on the standard Wall Street Journal task (dictation of American English newspaper text) is supplied.
The recognition architecture is based on an integrated statistical approach.
We describe the characteristic features of the system as opposed to other systems:
1. the Viterbi criterion is consistently applied both in training and testing; 2. continuous mixture densities are used without tying or smoothing; 3. time-synchronous beam search in connection with a phoneme look-ahead is applied to a tree-organized lexicon.
Vowels occurring in a constant context and spoken by male and female adults were analysed and transformed to obtain perceptually weighted spectral representations.
The amount of talker, talker-sex and phoneme information in the spectra was investigated by means of machine classifications under various conditions using appropriate average spectra as classificatory categories.
The high percentages of correct classifications obtained indicate that a single vowel token is not as ambiguous with respect to phoneme identity as formant measurements suggest.
The results agree with those of experiments investigating the identifiability of the vowels of unknown talkers which show that prior familiarity with a talker's voice is not crucial to correct identification.
Some degree of calibration to the talker may nevertheless be necessary, especially as regards talker sex, but it is shown that much of the information need for this normalization is available in the spectrum of the token to be normalized.
A preliminary algorithmic normalization procedure is presented which is based on spectral information contained in the token.
This theoretical article aims to draw up an inventory of the latest advances in medical knowledge engineering in the specific area of ontologies and knowledge based systems design.
Echoing the debates that animated the landscape of Artificial Intelligence (AI) from the 1970s under the impetus of Dreyfus HL, it aims to show that most of the difficulties currently faced by medical knowledge engineering are inherent in the nature of AI, whose project is the mechanization of cognitive activity.
As such it promotes the idea that only a fair understanding of what machines can do, given their machinic character itself, and remains, despite its cognitive finitude, a property of human being, may offer to balancing the scales between tasks that can be allocated to machines and those that have to be left in charge of humans.
Current, state-of-the-art speaker-independent continuous-speech recognizers are able to achieve word recognition rates in excess of 94 percent using lexicons of 1000 words or less and grammars or language models with perplexity 60 or less.
Performance of these systems decreases rapidly as the perplexity of the grammar increases.
As we allow users the flexibility to speak naturally, using constructions of their own choosing, perplexities increase more than an order of magnitude.
Fortunately, knowledge of the domain and of communicative and problem solving behaviors can be used to dynamically decrease perplexity and allow more natural interaction given the current state of speech recognition technology.
The perplexity reduction from knowledge results in speech performance equal to that demonstrated by speech recognizers using an equivalently low perplexity language model in the same or different domains.
This paper addresses how knowledge of domain semantics, dialog, communication conventions and problem solving behavior are used to enhance automatic speech recognition and understanding.
Included is a discussion of the system's basic principles and descriptions of the important knowledge sources and heuristics employed by the minds system.
This is followed by a brief analysis of some of the heuristics which do not have to be reimplemented across domains.
Specifically addressed are why the heuristics are effective, and how much each can be expected to reduce entropy and average branching factor in any possible application domain.
In real-life applications, errors in the speech recognition system are mainly due to inefficient detection of speech segments, unreliable rejection of Out-Of-Vocabulary (OOV) words, and insufficient account of noise and transmission channel effects.
In this paper, we review a set of techniques developed at CNET in order to increase the robustness to mismatches between training and testing conditions.
These techniques are divided in two classes: preprocessing techniques and Hidden Markov Models (HMM) parameters adaptation.
The results of several experiments carried out on field databases, as well as on databases collected over PSN and GSM networks are presented.
The main sources of errors are analyzed.
We show that a blind equalization scheme significantly improves the recognition accuracy regarding both field and GSM data.
Speech detection allows a system to delimit the boundaries of the words to be recognized.
We also use preprocessing techniques to increase the robustness of such detectors to noisy GSM speech.
We show that spectral subtraction improves speech detection under noisy GSM conditions.
Our experiments show an equivalent performance obtained with both Bayesian and linear regression adaptation of HMM parameters.
The results obtained also prove that HMM adaptation and preprocessing techniques can be advantageously combined to improve Automatic Speech Recognition (ASR) robustness.
This paper presents the work done at the CNET in speech recognition during the last few years.
The authors present the recent generation of speaker-independent systems, based on statistical modeling using the Markov models (PHIL86 software).
Several applications of these systems in the Telecommunications area are described, as well as the lessons drawn from them.
In this paper, we describe our progress during the last four years (1995–1999) in automatic transcription of broadcast news from radio and television using the BBN Byblos speech recognition system.
Overall, we achieved steady progress as reflected through the results of the last four DARPA Hub-4 evaluations, with word error rates of 42.7%, 31.8%, 20.4% and 14.7% in 1995, 1996, 1997 and 1998, respectively.
This progress can be attributed to improvements in acoustic modeling, channel and speaker adaptation, and search algorithms, as well as dealing with specific characteristics of the real-life variable speech found in broadcast news.
Besides improving recognition accuracy, we also succeeded in developing several algorithms to achieve close-to-real-time recognition speed without a significant sacrifice in recognition accuracy.
In this paper, we are dealing with the problem of facial features segmentation (mouth, eyes and eyebrows).
A specific parametric model is defined for each feature, each model being able to take into account all the possible deformations.
In order to initialize each model, some characteristic points are extracted on each image to be processed (for example, the corners of the eyes, mouth and eyebrows).
In order to fit the model with the contours to be extracted, a gradient flow (of luminance or chrominance) through the estimated contour is maximized because at each point of the searched contour, the gradient (of luminance or chrominance) is normal.
The advantage of the definition of a model associated to each feature is to be able to introduce a regularisation constraint.
However, the chosen models are flexible enough in order to produce realistic contours for the mouth, the eyes and eyebrows.
This facial features segmentation is the first step of a set of multi-media applications.
This communication addresses multilingual aspects in speech recognition and tries to link them to the concept of interoperability.
After a tentative definition of multilingual interoperability, the speech recognition components are discussed with a view towards separating language-specific from language-independent elements.
An overview gives examples of previous multilingual speech recognition research and developments across different speaking styles (read, prepared and conversational).
The problem of adaptation across languages is addressed.
In particular there exist language-independent and cross-language acoustic modeling techniques to port recognition systems from one language to another without language-specific acoustic data.
However these data remain valuable for acoustic model adaptation.
At this time pronunciation dictionaries and text material appear to be the most crucial language-dependent resources.
In our opinion fast porting, enabled by the existence of these language-dependent resources, is a step towards multilingual interoperability.
On-going efforts to produce multilingual pronunciation dictionaries and to collect multilingual text corpora including speech transcripts, should be extended to the largest possible number of languages.
These efforts could be shared with initiatives aiming at the support of minority languages.
CAD softwares are intended to become real tools for physical objects design aid.
But the preliminary design remains a widely open research field.
This survey tries to show that a constraint approach of design process is unavoidable to reach that goal.
Design process can be seen as an elaboration and constraint satisfaction cyclic process.
The associated conceptual model is a geometric constraints object model.
It is a multilevel model, integrating geometric and non geometric knowledge.
Following are the different approaches of geometric modeling: parametric, variational, feature-based and declarative modeling.
It ends up with a brief presentation of solving and decomposition methods of CAD constraints systems.
In this paper, in the case of spatially uncorrelated (or slightly correlated) noises, we introduce a new technique based on the coherence function which is used to determine a speech/noise classification algorithm.
We combine it with a noise reduction technique based on the spectral subtraction and evaluate its influence.
We report on results obtained on the performance of the algorithm and conclude that they are quite comparable to those obtained using a manual labelling.
This paper presents a review on the use of time-frequency representations in the fields of speech analysis and automatic speech processing.
Three main groups of methods are considered: speech production based methods, general signal analysis methods, auditory-based methods.
After this review, some short conclusions on their current use, and on some possible future evolutions are proposed.
This paper presents a multi-classifier system design controlled by the topology of the learning data.
Our work also introduces a training algorithm for an incremental self-organizing map (SOM).
This SOM is used to distribute classification tasks to a set of classifiers.
Thus, the useful classifiers are activated when new data arrives.
Comparative results are given for synthetic problems, for an image segmentation problem from the UCI repository and for a handwritten digit recognition problem.
We have developed a multi-channel pitch determination algorithm (PDA) that has been tested on three speech databases (0 dB SNR telephone speech, speech recorded in a car and clean speech) involving fifty-eight speakers.
Our system has been compared to a multi-channel PDA based on auditory modelling (AMPEX), to hand-labelled and to laryngograph pitch contours.
Our PDA is comprised of an automatic channel selection module and a pitch extraction module that relies on a pseudo-periodic histogram (combination of normalised scalar products for the less corrupted channels) in order to find pitch.
Our PDA excelled in performance over the reference system on 0 dB telephone and car speech.
The automatic selection of channels was effective on the very noisy telephone speech (0 dB) but performed less significantly on car speech where the robustness of the system is mainly due to the pitch extraction module in comparison to AMPEX.
This paper reports in details the voiced/unvoiced, unvoiced/voiced performance and pitch estimation errors for the proposed PDA and the reference system while utilising three speech databases.
This paper describes a framework for optimising the structure and parameters of a continuous density HMM-based large vocabulary recognition system using the Maximum Mutual Information Estimation (MMIE) criterion.
To reduce the computational complexity of the MMIE training algorithm, confusable segments of speech are identified and stored as word lattices of alternative utterance hypotheses.
An iterative mixture splitting procedure is also employed to adjust the number of mixture components in each state during training such that the optimal balance between the number of parameters and the available training data is achieved.
Experiments are presented on various test sets from the Wall Street Journal database using up to 66 hours of acoustic training data.
These demonstrate that the use of lattices makes MMIE training practicable for very complex recognition systems and large training sets.
Furthermore, the experimental results show that MMIE optimisation of system structure and parameters can yield useful increases in recognition accuracy.
In this paper, we present an algorithm designed for the stéréovision matching problem and 3D objetcs identification.
We use a simulated Hopfield neural network to solve the problem of matching a pair of stereoscopic images.
This model is helpful in optimization and it can be implemented on parallel machines easily.
Ageing at home is nowadays a matter of public health in western societies.
The presented work proposes to process the huge amount of data collected at elders homes to provide a consolidated view which describes their general behaviors.
To achieve this processing, several classification algorithms are presented.
These algorithms rely on a multi-agent paradigm and deal with the heterogeneity among the collected data.
Based on the resulting profiles, changes in health status can be anticipated for a specific elder but it is also possible to use these profiles to detect some global events such as an epidemic or the impact of high temperature on an ageing population.
Since simultaneous optimization of source and channel coding is often not practical for speech-coding algorithms, it is useful to develop channel codes optimal for a particular speech coding algorithm operating under a specified range of channel-error conditions.
Such source-dependent channel-error codes can be obtained by minimizing an appropriate speech distortion criterion.
To carry out this minimization, we use a simulated-annealing procedure.
The resulting channel codes are efficient, since they provide error correction of non-uniform accuracy (highly probable quantization levels receive more accurate correction) and/or non-uniform error detection (errors which greatly impact the speech quality are more likely to be detected).
An optimal trade-off between error correction and error detection can be obtained.
Source-dependent channel codes aimed at counteracting low, random error rates (up to 2%) are applied to the CELP algorithm and the resulting performance is reported.
It is found that a small allocation of codewords (equivalent to less than one bit) for the protection of a particular parameter often results in a large performance improvement.
This article presents an overview of the research activities carried out in the European CAVE project, which focused on text-dependent speaker verification on the telephone network using whole word Hidden Markov Models.
It documents in detail various aspects of the technology and the methodology used within the project.
In particular, it addresses the issue of model estimation in the context of limited enrollment data and the problem of a posteriori decision threshold setting.
Experiments are carried out on the realistic telephone speech database SESP.
State-of-the-art performance levels are obtained, which validates the technical approaches developed and assessed during the project as well as the working infrastructure which facilitated cooperation between the partners.
Our study deals with the parameter estimation problem of Hidden Markov Chain models and with unsupervised Bayesian image segmentation.
We propose two new estimation algorithms obtained from Iterative Conditional Estimation (ICE) and Stochastic Estimation Maximisation (SEM), denoted by MICE and MSEM respectively, and show their competitiveness with respect to the Estimation Maximisation (EM) algorithm in different situations of chain homogeneity and noise.
We then study three unsupervised chain restauration algorithms, obtained by adding EM, MICE and MSEM respectively to the Maximum Posterior Mode (MPM) restauration method.
The transformation of bi-dimentional process to mono-dimentional ones using Peano curves makes possible the application of these three methods to the problem of unsupervised statistical image segmentation.
Doing so, we obtain faster methods than those obtained by models using hidden Markov random fields and we show that the loss of effectiveness, due to the poorer adequacy of the model, is acceptable in general.
On the other hand, the flexibility of our modeling allows the conception of numerous unsupervised spatio-temporal segmentation methods.
We propose three of them and present results showing their application to the segmentation of a sequence of real images.
This paper introduces a Recurrent Radial Basis Function network (RRBF) for nonlinear system prognosis.
The training process is divided in two stages.
In the second stage, a multivariable linear regression supervised learning technique is used to determine the weights of the connections between the hidden and output layer.
The FuzzyMinMax technique makes the K-means more stable.
Time-scale and, to a lesser extent, pitch-scale modifications of speech and audio signals are the subject of major theoretical and practical interest.
Applications are numerous, including, to name but a few, text-to-speech synthesis (based on acoustical unit concatenation), transformation of voice characteristics, foreign language learning but also audio monitoring or film/soundtrack post-synchronization.
To fulfill the need for high-quality time and pitch-scaling, a number of algorithms have been proposed recently, along with their real-time implementation, sometimes for very inexpensive hardware.
This contribution reviews frequency-domain algorithms (phase-vocoder) and time-domain algorithms (Time-Domain Pitch-Synchronous Overlap/Add and the like) in the same framework.
More recent variations of these schemes are also presented.
Automatic speech recognition by computers can provide the most natural and efficient method of communication between humans and computers.
While in recent years high performance speech recognition systems are beginning to emerge from research institutions, scientists unequivocally agree that the deployment of speech recognition systems into realistic operating environments will require many hours of speech data to help us model the inherent variability in the speech signal.
This database is particularly valuable as a source of spontaneous utterances elicited in a realistic goal-oriented environment.
The relationship between speech and language processing is an important problem to be solved in order to achieve continuous large-vocabulary speech recognition for a speech translation system or the human interface of a man-machine system.
For the recognition of large-vocabulary continuous speech, first the phonemes are recognized by HMM (Hidden Markov Model).
A generalized LR parser is introduced to predict next words/phonemes.
The Japanese utterance is successfully recognized by the combined HMM-LR parser (HMM-LR).
Many phrase candidates are filtered out of the speech recognition system through the use of linguistic information.
An experimental system which translates spoken Japanese into English (SL-TRANS) has been implemented.
The translation method consists of analysis, transfer and generation processes.
A new method incorporated in the system analyzes the expressions of linguistic intention meanings in utterances.
This paper presents a brief overview of current uses, in 1994–1995, for CNET speech recognition and text-to-speech technologies in Interactive Voice Response Services (IVR) in France.
It describes several operational and experimental services, and analyzes field evaluations of some of them.
Finally, this paper summarizes recent developments in the CNET speech recognition and text-to-speech technology.
These methods can not be applied directly to speech analysis where the input signal (the excitation source of the vocal tract) is unknown.
In this paper, after clarifying the mechanism, in the case of unknown input signal, that limits the estimation precision, we describe a solution based on the impuls detection and suppression in the residue.
This approach can improve the parameter estimation accuracy mainly for the case where the residue is close to an impuls train.
It has been shown that this method improves the spectrum estimation of speech signals.
In this paper, we propose a new synthesis unit learning method aiming at multi-lingual speech synthesis and describe its application to English speech synthesis.
The method termed Multi-Layered Context Oriented Clustering (ML-COC) is a generalized framework of the COC method which has been applied to Japanese speech synthesis.
The conventional COC method produces a set of phonetic context dependent units through a cluster splitting process.
In ML-COC, the notion of context is generalized and the factors other than phonetic context, such as stressing and syntactical boundaries, are taken into account to capture the richer phoneme variations of English.
A synthesis unit generation experiment shows that ML-COC produces about three times as many synthesis units as the conventional COC (Single-Layered COC: SL-COC) method, and the average intra-cluster variance of ML-COC units is 20% lower than that of SL-COC.
These results suggest that the ML-COC synthesis units reflect the phonological structure of English much more appropriately than do the SL-COC units.
To validate the effectiveness of the ML-COC method, we conducted preference experiments using synthesized speech.
The preference test exposed 10 subjects to 52 sentences.
The ML-COC method was preferred over the conventional SL-COC method by a score of 70% to 30%.
This work deals with the reconstruction of Positron Emission Tomographic (PET) three-dimensional (3D) images for the detection of small tumors and metastases in oncology.
In PET, tumors appear as areas of hyperfixation of the injected tracer compared to regions with normal uptake.
We model the 3D distribution of activity by a mixture of laws, which describes the fact that each point in the 3D volume contains either a normal or a high activity concentration.
We solve this model using a Maximum Entropy on the Mean (MEM) approach.
Results obtained with our approach are compared with those obtained using two methods that are conventionally used for 3D PET reconstruction.
Using simulated data, results obtained with the MEM approach are significantly better than those obtained using the two other methods, when considering an evaluation criterion which characterizes the quality of reconstructed images in terms of lesion detectability.
The feasibility of the method is also illustrated on clinical data.
In conformal radiotherapy, beam set up and dose calculations are achieved using images obtained from computed tomography (CT) or magnetic resonance (MR).
These images are taken before the treatment which is performed on several sessions on several weeks.
At the beginning of each session, the patient has to be positioned on the treatment couch under the linear accelerator in the same position as during MR or CT imaging and planning, and the organs are assumed to be in the same place.
Currently, the methods used for this repositioning are based on the external anatomy of the patient and suppose an immobility of the internal structures.
In this study, we present a new approach, suited to the clinical practice, for the automatic repositioning of patient in prostate cancer radiotherapy.
It is based on localisation by ultrasound images and optical stereolocalisation and on a matching with images regenerated in the planning volume.
The method exploits a statistical model of the prostate to automatically extract its contours.
The first tests in conditions of a radiotherapy session show that the method is able to obtain a patient setup with an accuracy of about 1.4 mm.
Stop consonants are produced by forming a closure in the vocal tract, building up pressure in the mouth behind this closure, and releasing the closure.
At the consonantal release, these components of the sound include an initial transient, a burst of frication noise, and an interval in which there is a sound source at the glottis and transitions in the formants.
The models predict the absolute levels of these components for different places of articulation for the consonants.
This paper presents a new approach to equalize the telephone line effects in the transmitted signal aiming at improving the performance of speech recognition systems.
Measurements carried out on actual telephone data confirm that telephone lines introduce disturbing convolved components in speech signals.
Line effects are almost constant for a given call but vary with the calls.
The proposed adaptive filtering of the telephone line effects is compared to two conventional techniques, namely subtraction of the long-term cepstrum and highpass filtering of cepstral trajectories.
Recognition experiments are carried out on several telephone databases in a speaker-independent mode.
The results show that reducing the channel effects significantly improves the recognition performance.
Regarding the obtained error rates, the proposed adaptive filter yields better performance than the conventional highpass filters.
However, this adaptive filtering is not as good as the off-line cepstral subtraction technique where the long-term cepstrum is estimated on several recordings of a call.
Experiments were also conducted to measure the amount of speech data necessary to obtain a reliable estimate of channel effects.
Averaging cepstra vectors on a few seconds of speech produces a reliable estimate of the constant convolved perturbation.
In the context of a discussion on Petrarchism as a process of imitatio within different literary genres, we study the modalities of transmission of petrarchist schemes and images by comparing the preliminary sonnets (I and II) of Pontus de Tyard's 1549 Erreurs amoureuses, which are analysed in parallel with Voi ch'ascoltate in rime sparse il suono and with Ronsard's Amours first sonnet.
The two first sonnets of the Erreurs amoureuses represent a splitting of the traditional preliminary apostrophe, addressed to a public that is at once a spectator and an actor of the love suffering.
These sonnets take from Petrarch the theme of the giovenile errore, all the whilst interpreting the polysemic erreur in the light of its two principal meanings: erreur as a journey (wandering), and erreur as an error.
As far as Pontus de Tyard is concerned, Petrarch is quite evidently the fundamental model, both in terms of the structure and of the vocabulary.
In fact, it is indeed the lexical level that provides information in this field: the use of the same lexemes errore/erreur demonstrates to what extent the tendency toward variatio of the petrarchist archetype becomes first and foremost a rhetorical fact.
'Root' and 'affix' as linguistic terms, and later 'suffix' and 'prefix', do not belong to the large lexical technical lore inherited from the Greco-Latin world.
Like 'scheva, shewa', they appear during the Renaissance, when grammarians from the Latin tradition discover Hebrew grammar.
For the first time, 'Latin grammarians' have to tackle with a foreign but prestigious tradition.
The history of these new words is here described with some detail, because they also reflect a new look at languages.
The confrontation between the two grammatical traditions will result in the development of new concepts and a new, more analytical, approach to language morphology.
This article presents a method of formant-to-area mapping consisting of the direct calculation of the time derivatives of the cross-sections and length of a vocal tract model so that the time derivatives of the observed formant frequencies and the model's eigenfrequencies match.
The vocal tract model is a concatenation of uniform tubelets whose cross-section areas and lengths can vary in time.
Time derivatives of the tubelet parameters are obtained by solving a linear algebraic system of equations.
The derivatives are then numerically integrated to arrive at cross-section and length movements.
Since more than one area function is compatible with the observed formant frequencies, pseudo-energy constraints are made use of to determine a unique solution.
We study heuristic methods for a classification problem encountered in cork industry.
More precisely, we want to optimize the parameters of the classification rule that is daily used to classify corks.
We experimented several metaheuristics and compared their performance on a real test case.
The obtained results improve the current classification rate.
We propose also, from our observations, new directions that may lead to further improvement for a still better classification.
We present an automatic recognition system applied to handwritten numeral check amounts which is based on a segmentation-by-recognition probabilistic model
This system is descridedfrom the amount field localization on the document image to the generation of hypotheses.
An explicit segmentation algorithm determines potential cuts between characters and provides a spatial representation of segmented components.
The best path for the segmentation is determined by the combination of recognition scores, of segmentation likelihoods and of a priori probabilities of amounts.
The robustness of the system was assessed on a database of 10,000 real cheques images.
This paper describes the RailTel system developed at LIMSI to provide vocal access to static train timetable information in French, and a field trial carried out to assess the technical adequacy of available speech technology for interactive services.
The data collection system used to carry out the field trials is based on the LIMSI Mask spoken language system and runs on a Unix workstation with a high quality telephone interface.
The spoken language system allows a mixed-initiative dialog where the user can provide any information at any point in time.
Experienced users are thus able to provide all the information needed for database access in a single sentence, whereas less experienced users tend to provide shorter responses, allowing the system to guide them.
The RailTel field trial was carried out using a common methodology defined by the consortium.
100 novice subjects participated in the field trials, each calling the system one time and completing a user questionnaire.
Of the callers, 72% successfully completed their scenario.
The subjective assessment of the prototype was for the most part favourable, with subjects expressing an interest in using such a service.
We consider adaptive prediction with an HR moving average (MA) part, when controlled either by the recursive LMS algorithm or by an extended LS (ELS) algorithm based on a posteriori errors.
The predictor input is either the sum of band-pass components or a nonstationary speech sentence.
We show on one hand that using the a posteriori error algorithm smoothes the oscillations due to the selfstabilization phenomenon, compared to the LMS algorithm case.
On the other hand we show that with a nonstationary input the LMS algorithm can be unstable due to power jumps in the speech signal.
Finally the a posteriori error algorithm ensures BIBO stability (Bounded Input-Bounded Output) even for a nonstationary input.
Text-dependent speaker identification performance is investigated for small groups of speakers in which each speaker in a group is assigned the same sentence-long password utterance.
Several model construction conditions are studied.
Baseline maximum likelihood estimate (MLE) models are constructed from three same-session training utterances.
Minimum classification error (MCE) models are constructed using the training utterances of all speakers in a group.
In addition, models are constructed using additional test utterances from speakers in the group or additional utterances from speakers outside the group.
Results show that error rates approximately double from 5-speaker groups to 10-speaker groups.
MCE models provide about 25% improvement in closed- and open-set identification error rates, but less improvement, about 10% in imposter accept rates.
The greatest improvements are obtained, for both MLE and MCE models, when customer test utterances augment the training utterances.
For MCE models closed-set identification error rates are approximately 0.4% and 0.6% for 5- and 10-speaker groups, respectively, while imposter accept rates are approximately 4% and 10%, respectively, when customer reject rates are 5%.
This paper presents a microphone array adaptive beamformer with a dual function.
The noise enhanced output is suited to transmission as well as to use as input to speech recognition systems.
The areas of use envisaged are the car. the factory floor and noisy offices.
The underlying structure is a steered Griffiths-Jim beamformer, with an added speech detection switch for the selective adaptation of both sections.
This beamformer is effective in suppressing both stationary and non-stationary interference and is therefore a preprocessor for a wider range of speech recognition applications than any single channel noise suppression scheme could handle.
Experiments were performed in a reverberant room with a 4-microphone array.
Typical SNR improvements for communication purposes range from 4 to 12 dB.
The effective SNR improvement for speech recognition purposes ranges from 4 to 8 dB.
Speech synthesis now profitably automates services by speaking information from computer databases.
Some of these services provide driving directions, traffic and timetable information, stock quotes and related financial services, and catalog ordering.
One particularly successful telecommunication service, Automated Customer Name and Address (ACNA), sometimes called Reverse Directory Assistance (RDA), requires synthesis with high intelligibility and name pronunciation accuracy, both of which are achieved by current synthesis technology.
However, even the best of current technology is not good enough to mindlessly `drop' into complex services.
Customized directory preprocessing is necessary to transform listing data, which commonly contains unconventional abbreviations, acronyms unknown to the synthesizer, and scrambled word ordering, into a sentence suitable for synthesis.
We describe our directory preprocessing programs used in successful implementations of synthesis in two major US telephone companies.
It is also necessary that locality terms, which have considerable geographical variability, are pronounced in accordance with local customs; otherwise, the service will have an outsider's feel to the customers.
We also describe an experiment that determined whether the naturalness of recorded speech for prompts and other fixed messages offsets the undesirable discontinuity between natural and synthesized utterances.
In this paper, we present a multi-stream approach for off-line handwritten word recognition.
The multi-stream formalism presents many advantages: it can combine several kinds of independent features.
Significant experiments have been carried out on two publicly available word databases: IFN/ENIT benchmark database (Arabic script) and IRONOFF database (Latin script).
Moreover, the proposed recognition system provides significant results comparable to the best results reported in the literature on both databases.
This paper discusses recent advances in and perspectives of research on speaker-dependent-feature extraction from speech waves, automatic speaker identification and verification, speaker adaptation in speech recognition, and voice conversion techniques.
Speaker-dependent information exists both in the spectral envelope and in the supra-segmental features of speech.
This individual information can be further classified into temporal and dynamic features.
Speaker identification/verification methods can be divided into text-dependent and tect-independent methods.
Although text-dependent speaker verification techniques have almost reached the level suitable for practical implementation, text-independent techniques are still in the fundamental research stage.
Both supervised and unsupervised speaker adaptation algorithms for speech recognition have recently been proposed, and remarkable progress has been achieved in this field.
Improving synthesized speech quality by adding natural characteristics of voice individuality, and converting synthesized voice individuality from one speaker to another, are as yet little exploited research fields to be studied in the near future.
Research on speaker-dependent information is one of the most important future directions for achieving advanced speech information processing systems.
Interethnic communication in Cameroon is sometimes characterized by a derogatory discourse on the ethnicity of others.
This discourse generally appears in the form of insults, jokes, teases, etc. built into narrations, songs, fiction and telecasts, and is transported from one generation to the other.
This article describes some of the strategies used to denigrate the ethnic identity of others in Cameroon.
The analyses, based on data (questionnaires, participant-observations, interviews) collected in Yaoundé and other regions of the country, show how Cameroonians use borrowings, nominal compositions, metaphors, semantic shifts, metonymies, etc. to denigrate, downgrade, dehumanize, or demonize members of certain ethnic groups and/or to erase, minimize or contest the ethnicity of others and their ethnic groups.
The recognition of Mandarin syllables is a key problem in large vocabulary Mandarin speech recognition.
Conventionally, the tone and base syllable corresponding to a syllable are separately recognized by using a tone recognizer and a base syllable recognizer, respectively.
In this paper, we propose a framework for Mandarin syllable recognition based on the classification of sub-syllabic units such as initials, finals and transitions.
The final units are classified in accordance with the variations of tones to enhance the capability of tone discrimination.
By using hidden Markov models (HMM) based on LPC-derived cepstral parameters, we develop a Mandarin syllable recognizer in which base syllables and their corresponding tones are jointly recognized.
Experimental results indicate that the proposed syllable recognizer yields higher recognition rates than the conventional syllable recognizer does when sufficient amount of training data is used.
We also show that the performance of the proposed syllable recognizer can be further improved with the incorporation of a tone recognizer.
This paper first surveys and classifies applications of voice humancomputer dialogue.
The advantages and limits of speech as a means of communication between users and software are then considered.
A major problem in the development of user interfaces with a voice component, besides the choice of appropriate applications, is speech recognition, especially continuous speech recognition.
As approaches differ according to the type of application, we first summarize problems and techniques specific to voice data input ; as an example, we briefly describe the speech recognition approach that we have adopted for the dictation machine that we are developing in our Laboratory.
Then, the case of voice dialogue understanding and management is considered.
To illustrate the discussion, the architecture and functionalities of some prototypes that we have implemented are presented: for instance, an E-mail system and a Sonar control software.
Finally, we present a dialogue manager DIAL capable of helping ¡guiding the user in complex cognitive activities that we are currently developing and implementing.
I present and discuss the SAPHO (Segmentation by Acoustico-Phonetic knowledge) model implemented in Awk language under the Unix system on a MASSCOMP computer.
The system is devised as a speaker independent ASS (automatic speech segmentation), by a previous recognition of the phonetic articulation manner.
In all the ASR systems the phonetic knowledge is at least implicitely used.
It has to be explicitely referred to.
The phonemic units cannot be directly built from the acoustic signal and are not available at the output of SAPHO.
According to the Level Building procedure SAPHO supplies a hierarchized set of acoustic properties and segments, and phonetic properties and segments which fit the phonetic parsing of the acoustic wave.
The amenability of this system is entailed by its modularity which allows a possible further architecture as distributed tasks.
The suitability and the reliability of SAPHO are corroborated by the accuracy of the results.
400-channels constant bandwidth (12.5 Hz) Long Term Spectra (LTS) delivered by a BK 2033 analyser have been drawn from French utterances.
The cross-correlation coefficient was used to investigate LTS residual intra-speaker variability both in inter- and intra-text conditions.
Significant subject-dependent differences habe been revealed in both conditions.
They indicate voice coherence variability in the speakers.
Correlations lowering in inter-text condition moreover reveals the content-dependent nature of LTS.
The need for basic LTS research prior to further application is therefore emphasized.
The pervasive use of clinical categories of aphasia in neurolinguistic and cognitive neuropsychological research reflects the assumption that these patient groupings represent disruptions of the normal language processing system along theoretically significant lines.
This premise is examined here with particular reference to the status of 'agrammatism'.
It is argued that there are compelling reasons to question the coherence of agrammatism as a psychological entity.
To overcome these objections, the clinical intuitions on which this aphasic category is based must be replaced by objective criteria for selecting a theoretically significant patient grouping.
To reach this goal, it is especially important that a theoretically motivated distinction be made between within-and across-category variation.
It is argued that in the case of categories like agrammatism, there are serious methodological obstacles which make such goals unreachable.
It is argued, therefore, that our theories should not take categories like agrammatism as psychological givens, especially if the purpose of our research is to reach an understanding of the mechanisms of language processing or of individual aphasic deficits themselves.
Our conception of what it is that the speech production mechanism is attempting to implement in speaking comes from linguistics.
But linguistics first developed its methods for other purposes.
In the early 19th century linguistics produced a method for tracing the family relationship between suspected cognate words and their constituent sounds.
This, the comparative method, involved establishing an optimal path between these forms via a reconstructed parent form.
20th century structuralist linguistics (including generative phonology), essentially grafted the same method onto the task of finding the underlying phonemic constituents of words.
Although the underlying structure found in this way may be a good hypothesis as to the mental elements determining actual spoken utterances, there are reasons to suspect that it is too simple.
Too much emphasis is placed on the simplicity of the system and on the purely lexical (as opposed to the demarcative and attidudinal) function of the elements in speech.
This paper presents some initial attempts to differentiate between phonetic variants in speech which stem from single underlying forms as opposed to those which arise from separate underlying forms (though they may have had a common source historically).
Data or information is only useful when it answers one's information needs, and when it is delivered in a way that facilitates understanding and use.
We have all experienced trying to find information about a specific topic, issuing a number of queries on the web using a search engine in an attempt to obtain appropriate links, and, finally, trying to pull together the information we wanted from all the results.
In this paper, we present the Virtual Document Planner, a platform designed to support tailored information delivery.
The VDP addresses the problem of delivering information that may come from a variety of sources, and that needs to be delivered in a form that facilitates comprehension and use.
To illustrate that approach, we describe an application where the VDP is applied to the generation of information about a specific topic in the context of a company web site.
We also report on a preliminary study investigating the effect of such information delivery.
This paper presents a comparison of sequential selection processes in order to identify the relevant indices for the early syncopes prediction during a diagnosis test.
An hybrid approach, combining a sequential process and a genetic algorithm, has allowed to increase the prediction performance, to reduce the number of selected variables and also to reduce the total number of evaluated subsets during the selection process.
Finally, the obtained results allow to predict the apparition of syncope with a sensitivity of 100% and a specificity of 94%.
There is controversy over the role of auditory scene analysis in speech perception and in particular whether listeners form perceptual streams of formants.
The role of vowel formant frequencies in the perception of synthetic vowel–nasal syllables and the importance of formant continuity between vowel and nasal were examined in three experiments.
When no explicit transitions were present between the vowel and nasal, the perception of each nasal changed from /m/ to /n/ as the vowel F2 increased.
Introducing explicit formant transitions removed this effect, and listeners heard the appropriate percepts for each nasal prototype.
However, if the transition and the nasal prototype were inconsistent, the percept was determined by the transition alone.
In each experiment, therefore, the target frequency of the vowel F2 transition into the nasal consonant determined the percept, taking precedence over the formant structure of the nasal prototype.
The results do not show strong evidence for formant streaming and are more consistent with pattern matching processes.
This paper presents an adaptation method of speech hidden Markov models (HMMs) for telephone speech recognition.
Our goal is to automatically adapt the HMM parameters so that the adapted HMM parameters can match with the telephone environment.
In this study, two kinds of transformation-based adaptations are investigated.
One is the bias transformation and the other is the affine transformation.
A Bayesian estimation technique which incorporates prior knowledge into the transformation is applied for estimating the transformation parameters.
Experiments show that the proposed approach can be successfully employed for self adaptation as well as supervised adaptation.
Besides, the performance of telephone speech recognition using Bayesian adaptation is shown to be superior to that using maximum-likelihood adaptation.
The affine transformation is also demonstrated to be significantly better than the bias transformation.
A theoretical account of stuttering is presented in which an inadequacy of neuronal resources for sensory-motor information processing is seen as the basis of the disorder.
It is proposed that stutterers are deficient in the processing resources normally responsible for determining and adaptively maintaining the internal models which subserve speech production.
A general description of such computational processes is detailed in the form of circuitry for an adaptive controller which can calibrate itself to control any variable, nonlinear, dynamic, multiple input, multiple output system.
There are often sufficient cues which allow the auditory system to determine whether sound components continue through such occlusions.
This paper reviews the situations where an assumption of continuity is warranted and demonstrates how the principles governing the so-called “continuity illusion” can be used within a computational system for segregating acoustic sources.
To deal with large lexica (more than 2000 words) automatic speech recognition systems (ASR) use an internal phonetic representation of the speech signal and phonemic models of pronunciation from the lexicon to search for the spoken word chain or sentence.
Therefore it is possible to model different pronunciations of a word in the lexicon.
In German we observed that individual speakers pronounce words in a typical way that depends on several factors as sex, age, place of living, place of birth, etc.
Our goal is to enhance speech recognition by automatically adapting the models of pronunciation in the lexicon to the unknown speaker.
Another method presented in this paper is speaker adaptation by re-estimating the a posteriori probabilities of the phonetic units used in a “bottom up” ASR system.
A word hypothesis is evaluated by the product of the a posteriori probabilities of the phonetic units produced by the classification to the phonetic units belonging to the word hypothesis.
Normally these probabilities are estimated during the training of the ASR system and stay fixed during the test.
We propose an algorithm which observes the typical confusions of phonetic units of the unknown speaker and adapts the a posteriori probabilities continuously.
Underwater magnetic signals are affected by a nonstationnary noise from different sources.
In order to detect the distortion in the magnetic field caused by the displacement of a ferromagnetic mass we try to eliminate as much of the noise as possible by measuring the magnetic field with four sensors when the transient has been received by one of them.
We propose an interactive graphical tool, CA Viz, which allows to visualize and to extract knowledge from FCA results on images.
Originally the FCA is for the analysis of contingency tables.
For adapting FCA on images, we first define the "visual” words in images.
These words are constructed from local descriptors (SIFT, Scale Invariant Feature Transform) in images.
CAViz projects clouds ofpoints in factorial plans and allows viewing and extracting interesting information such as: characterizing words, important factors using FCA relevant indicators (representation quality and contribution to the inertia).
An application to the Caltech4 database shows the interest of CAViz for image mining.
Mryati et al. use the properties of the sensitivity functions of a cylindrical tube to divide it into eight specific regions associated to the eight possible combinations of the sensitivity of the first three resonances.
Bringing out acoustic properties of symmetry and compensation, they claim that the production of vowels and consonants is based on these geometric and acoustic properties, since the eight regions can be linked to morphological and articulatory properties of the vocal tract.
The authors formulate a new vowel production theory and they propose a universal phonological system for consonants.
We are critical of the New Theory on several counts: • - the limitations of sensitivity functions have been overlooked; • - the generalizations are anthropomorphically weak; • - the predictions fail to match known acoustic facts; • - the universal classification is in contradiction with basic phonetic knowledge.
More generally this kind of approach seems intrinsically very limited: the vocal tract is not a series of tubes whose dimensions can be manipulated independently, without referring to an underlying articulatory model integrating articulatory constraints.
The New Theory will retain all of its value once it has been returned to its natural context i.e. the simple acoustic description of the vocal tract around the neutral position and as a tool for speech synthesis.
As part of a system for the automatic recognition of isolated words in a large vocabulary on the basis of an analytical approach, we considered the automatic speaker-adaptation of the system.
This was carried out by means of an automatic learning procedure of the speakers' reference patterns, and by automatically adjusting the parameters of the system.
This learning relies on a time alignment algorithm using acoustic-phonetic features which are little speakerdependent.
The learning session was successfully tested on 18 speakers out of 20 (10 women and 10 men) and the reference patterns thus obtained yielded good results during the recognition phase.
We have now undertaken an analysis of the vowels uttered by 15 speakers based upon descriptive statistics and statistical interpretation in order to design procedures of normalization and of automatic generation of a speaker's vowel reference patterns.
In the STAP domain, modeling the interferences as an autoregressive (AR) process with the detector called Parametric Adaptive Matched Filter (PAMF), provides an estimation of the clutter-rejection filter with few training data.
The main difficulty of this approach is the estimation of the AR matrices by using the training data.
Thus, we propose an on-line estimation based on the Kalman filter and its variants.
A comparative study is carried out and illustrates the relevance of such approaches with data provided by the DGA.
This paper presents a software architecture, based on web services, that enables the creation and evaluation of interactive visual applications.
Web services are a standard for data exchange in a distributed system, such as the web.
They are mainly used for data publishing (via API), but can also be used for data processing.
We show that web services composition permits the creation of data visualization, by reconstructing the reference model.
Generated visualizations can be made interactive once coupled with a program (such as a web browser) to let the user perform visual exploration and data analysis tasks.
We also present a service composition interface, and applications to graph and word cloud visualization.
Finally, we show how generated server-side logs allow the representation and evaluation of users' activity.
Designing a Voice-Activated Typewriter in French necessitates a study both on how to design the acoustic level recognition, and on how to obtain a model of the French language.
Such a project was initiated at LIMSI 15 years ago.
This paper presents the different steps that have been completed since the beginning of this project.
First, a study on the phoneme-to-grapheme conversion, for continuous, error-free phonemic strings, using a large vocabulary and a natural language syntax was completed in 1979.
The corresponding results were then improved, with attempts to convert phoneme strings containing (simulated) errors, while the methodology was adapted to the case of stenotype-to-grapheme conversion.
In the ESPRIT project 860 “Linguistic Analysis of the European Languages”, our approach for language modeling was compared with other approaches on 7 different European languages.
The link between the acoustic recognition and the language model resulted in a complete system (“Hamlet”), for a limited vocabulary (2,000 words), pronounced in isolation, which was then extended to a vocabulary of 5,000 words, taking advantage of a specialized DTW chip (MuPCD), also designed at LIMSI.
This study resulted in the conclusion that dictation in an isolated mode was not acceptable.
A speaker-independent continuous speech recognition system is now developed for vocabularies of 5 to 20 KWords.
Steganography has been known and used for a very long time, as a way to exchange information in an unnoticeable manner between parties, by embedding it in another, apparently innocuous, document.
This usually leads to very high dimensional spaces for which many problems arise (in comparison to low dimensional spaces): mainly, the required number of images to have an appropriate filling of the space in which the classifier is trained, is never reached.
In this article, some of the problems encountered because of the high dimensionality of the problem usually met in steganalysis, are presented, along with possible solutions.
With this sufficient number of images, feature selection is then performed, with a forward algorithm, in an attempt to decrease the dimensionality and also to gain interpretability over which features have been reacting the most.
In this contribution, a new system for voice conversion is described.
The proposed architecture combines a PSOLA (Pitch Synchronous Overlap and Add)-derived synthesizer and a module for spectral transformation.
Prosodic modifications are applied on the excitation signal using the TD-PSOLA scheme; converted speech is then synthesized using the transformed spectral parameters.
Two different approaches to derive spectral transformations, borrowed from the speech-recognition domain, are compared: Linear Multivariate Regression (LMR) and Dynamic Frequency Warping (DFW).
Vector-quantization is carried out as a preliminary stage to render the spectral transformations dependent of the acoustical realization of sounds.
A formal listening test shows that the synthesizer produces a satisfyingly natural “transformed” voice.
Hands-free interaction represents a key-point for increase of flexibility of present applications and for the development of new speech recognition applications, where the user cannot be encumbered by either hand-held or head-mounted microphones.
When the microphone is far from the speaker, the transduced signal is affected by degradation of different nature, that is often unpredictable.
Special microphones and multi-microphone acquisition systems represent a way of reducing some environmental noise effects.
Robust processing and adaptation techniques can be further used in order to compensate for different kinds of variability that may be present in the recognizer input.
The purpose of this paper is to re-visit some of the assumptions about the different sources of this variability and to discuss both on special transducer systems and on compensation/adaptation techniques that can be adopted.
In particular, the paper will refer to the use of multi-microphone systems to overcome some undesired effects caused by room acoustics (e.g. reverberation) and by coherent/incoherent noise (e.g. competitive talkers, computer fans).
The paper concludes with the description of some experiments that were conducted both on real and simulated speech data.
This paper examines the intonational characteristics of a number of types of non-word, e.g. numbers, dates, times and other abbreviations, which occur in text and are readily identifiable.
Examples of such phenomena are presented, and heuristics for their treatment by an automatic system are proposed.
A formal evaluation of these heuristics is presented, showing a success rate of over 94%.
A final discussion outlines the advantages and disadvantages of such a treatment, and suggests lines of future research.
This article provides the editio princeps of a previously unknown maqāma attributed to Badīʿ al-Zamān al-Hamad̠ānī (d. 398/1008).
It begins with a review of the scholarship on the manuscripts of Hamad̠ānī's Maqāmāt and discusses how the text of this lost maqāma was uniquely preserved in one manuscript, Yale University, Beinecke Library, Salisbury collection no. 63.
This manuscript, copied in 603/1206, was well-known to European scholarship, having been in the possession of Everard Scheidius (1742-1794), Silvestre de Sacy (1775-1838), and Edward Eldridge Salisbury (1814-1901).
The maqāma, preserved therein, describes a fraudulent doctor's sale of medicinal compounds allegedly composed of rare materia medica.
The text of this maqāma, which the editors have entitled al-Maqāma l-Ṭibbiyya, is then provided in facsimile, a critical edition, and a fully-annotated English translation.
A detailed analysis of the maqāma follows, in which the form, subject matter, language, and style of this maqāma are discussed in relation to the known corpus of Hamad̠ānī's other maqāmāt.
The article concludes with several hypotheses about the possible authenticity of this lost work.
Formal Learning Theory may be conceived as a means of relating theories of comparative grammar to studies of linguistic development.
After a brief review of relevant concepts, the present paper surveys formal results within Learning Theory that suggest correponding constraints on linguistic theory.
Particular attention is devoted to the question: How many possible natural languages are there?
Synthetic aperture radar (SAR) is a microwave imagery system capable of producing high resolution images by processing properly data collected by a relatively small antenna.
In this paper, the bi-dimensionnal received signal, using spatial coordinates, is formulated.
The image quality is determined by that of the ambiguity function.
This latter is analyzed and optimized for two performance criteria.
First, for a matched filter receiver (maximal signal-to-noise ratio receiver), the optimal waveform is shown to be a non linear FM pulse which autocorrelation function is a Taylor.
The optimal azmiuth weighting function is related to that of Taylor by a Fourier transform.
Second, for a Wiener filter (least mean-squares receiver), we show that the optimal waveform is the first prolate spheroidal function.
The single-hit measurement of the scattering matrix by mean of two optimal orthogonal waves is discussed.
Object extraction systems performances are not homogeneous over different corpora because objects can take many different aspects within such sets.
An adaptation of these systems is thus required in order to maintain equal performances over every kind of object the system may be applied on.
Focusing on the issue of parameters optimization, a method has been developed to restrict optimization to parameters of operators which compose the system, responsible for the different categories of errors produced by the system.
Two stages are involved in our method.
The first one is dedicated to the analysis of the system performances and leads to the extraction of the different error categories already mentionned.
The second one relates to the analysis of the behavior of the different operators, leading to extract a single operator responsible for each error category.
Experiments have been carried out over a video text detection system.
What is mirrored from such a methodological exercice is that the philological work surrounding an edition is only the beginning of any book's history… a mere yet indispensable pretext.
There is a strong consensus that the sounds and sound patterns of babbling and early speech are basically the same.
The common state is one of “Frame Dominance” — a syllabic frame produced by an open-close mandibular oscillation dominates both stages, with limited ability of other articulators, including the tongue to produce active intrasyllabic and intersyllabic changes.
The question of whether the first words are similar to babbling in all respects was evaluated in 4 subjects, using a database consisting of 152 hours of audio recording.
Progress in words took the form of an increase in variegation of utterances, mainly due to vowel variegation, much of which derived from an increase in the use of high vowels and mid back yowels, especially in wordfinal position.
The presence of regression and the limited nature of the progress were taken as evidence of the strength of the Frame Dominance pattern and the consequent difficulty of escaping from it.
A set of phonetic studies based on analysis of the TIMIT speech database is presented which addresses topics relevant to the linguistic and speech recognition communities.
First, the advantages and shortcomings of using TIMIT for linguistic research are considered, and a database methodological approach is outlined.
Next, several small studies are presented which detail new results on the effect of speakers' sex and dialect region on pronunciation.
The goal of this paper is to use the database to explore sex and dialect related variation thereby ascertaining differences which may merit further experimental study.
This report concerns speaker-dependent effects on certain phonetic characteristics often involved in reduction such as speech rate, stop releases, flapping, central vowels, laryngeal state, syllabic consonants, and palatalization processes.
Specifically, it is suggested that the phonetic characteristics found more commonly with male speakers are also those typical of reduction in speech.
With the development and the availability of large textual corpora, appeared a need for structuring and organizing these corpora in a way that reflects some semantic relations between documents.
For now, in Information Retrieval, these relations are indicated mainly via hyperlinks or by organizing documents into concept hierarchies, both being manually developed.
We propose here an algorithm for automatically inferring concept and document hierarchies from a corpus.
We also present numerical criteria for measuring the relevance of these automatically generated hierarchies and discuss some experiments performed on data from the Looksmart web site.
Our present work concerns Swedish prosody in a speech synthesis framework.
Two main problem areas are examined: prominence and phrasing.
In a model for Swedish prosody, prominence levels (stress, accent, focus) are represented as layered and multidimensional for different domains (syllable, foot, word).
Phrasing involves both coherence in the form of specific combinations of existing accentual gestures and separate boundary gestures.
The main features of the intonation model are given in outline.
Experiments on prominence include modelling of durations in a combined speech data base and rule synthesis framework, where the stressed-unstressed alternation appears to be the most important duration factor.
Other experimentation concerns typical differences in the timing characteristics of the tonal gesture for focal accent between compound words and simplex accent II words.
Experiments on phrasing include both production data from a varied speech material as well as synthesis and perception.
Our experiments demonstrate that both coherence and boundary cues are effective as phrasing signals and that a combination of F 0 and duration is typically used to signal phrasing.
Our future plans include working with prosodic modelling of Swedish in a dialogue context and in a concept-to-speech framework.
In this paper we compare two different methods for automatically phonetically labeling a continuous speech database, as usually required for designing a speech recognition or speech synthesis system.
Both systems have been evaluated on read utterances not part of the training set of the HMM systems, and compared to manual segmentation.
This study outlines the advantages and drawbacks of both methods.
The speech synthetic system has the great advantage that no training stage (hence no large labeled database) is needed, while HMM systems easily handle multiple phonetic transcriptions (phonetic lattice).
The importance of such segmentation tools is a key point for the development of improved multilingual speech synthesis and recognition systems.
Vérard published his editio princeps of the prose Merlin in 1498, in three handsome folio volumes ; the third and final volume contains the Prophéties.
This difficult, 'unreadable', text – a torrent of predictions relating largely to 12th- and 13th-century Italy – is presented in all the surviving manuscripts with a mise en page offering no assistance to a 'proper' reading: few paragraphs, no rubrics.
The 1498 edition of the Prophéties, by contrast, is endowed with an elaborate and abundant programme of rubrics, which, it seems plausible, were the contribution of Vérard and his workshop.
This, allied to the Table at the beginning of the volume, seems to constitute something like a primitive 'information retrieval system'.
We review in a common framework several algorithms that have been proposed recently, in order to improve the voice quality of a text-to-speech synthesis based on acoustical units concatenation (Charpentier and Moulines, 1988; Moulines and Charpentier, 1988; Hamon et al., 1989).
These algorithms rely on a pitch-synchronous overlap-add (PSOLA) approach for modifying the speech prosody and concatenating speech waveforms.
The modifications of the speech signal are performed either in the frequency domain (FD-PSOLA), using the Fast Fourier Transform, or directly in the time domain (TD-PSOLA), depending on the length of the window used in the synthesis process.
We also discuss the different kinds of distortions involved in these different algorithms.
Research on proper names in French allows us to use the results of rhetoricians, grammarians and linguists for bilingual lexicographical processing of lexical units and phraseologisms containing proper names used figuratively in French and Macedonian.
In this article, we try to distinguish relevant criteria allowing the lexicographer to select and list those units, put forward solutions for their processing and identify problems and characteristics.
We present a swarm intelligence algorithm that solves a discrete foraging problem.
We describe simulations and provide a complete convergence analysis: we show that the population computes the solution of some optimal control problem and that its dynamics converges.
We discuss the rate of convergence with respect to the number of agents: we give experimental and theoretical arguments that suggest that this convergence rate is superlinear with respect to the number of agents.
Furthermore, we explain how this model can be extended to the case where the state space is continuous, and in order to solve optimal control problems in general.
We argue that such an approach can be applied to any problem that involves the computation of the fixed point of a contraction mapping.
This allows to design a large class of formally well understood swarm intelligence algorithms.
The time-domain harmonic-scaling (TDHS) algorithm provides a computationally efficient method (suitable for real-time implementation) for speech bandwith compression and expansion.
Pitch estimation is an important operation in the TDHS process.
In the present paper, we study a TDHS/sub-band coding system for speech operating at 16 kbits/s and investigate the relative effectiveness of five different pitch estimation methods (the autocorrelation method, the cepstrum method, the simplified inverse filtering technique, the average magnitude difference function method and the maximum likelihood method).
A formal listening test using 17 human listeners is conducted for their comparative performance evaluation.
The average magnitude difference function method was found to be the best pitch estimation method for TDHS/sub-band coding.
Word shadowing was used in order to obtain evidence concerning the relevance of the uniqueness point (UP, i.e., the moment at which the acoustic-phonetic information already presented remains compatible with a single lexical entry) as a determinant of the point where spoken word recognition occurs.
Taken as an average, response latencies showed a minimal but significant effect of the UP.
However, there were important differences between items with early and late UPs.
A multiple regression analysis, taking UP position, word length and word frequency into account, showed that UP position is the best predictor of the shadowing latencies of early-UP items, but that it does not contribute at all to the shadowing of words with late UP.
We conclude that the UP strongly mediates the recognition of spoken words with early UP.
In addition, the shadowing of late-UP items is best predicted by word length in slower, and by word frequency in faster subjects; this suggests the intervention of different mechanisms.
Adaptation in games and serious games is an important feature that allows to individualize the game experience.
It can also manage the players-learners ' frustration while increasing their motivation.
This article presents the state of the art of works dealing with adaptation in games and serious games.
These works are then described according to an evaluation framework that determines the scope, parameters, model of adaptation and whenever the game is single or multiplayer.
The analysis of the state of the art shows that building adaptive multiplayer serious games raises many challenges such as managing multiple views without breaking the game design logics.
This leads us to consider development of adaptive multiplayer serious games as an important challenge to be addressed.
This paper questions the phonetic bases of the theories dealing with segments' internal structure.
It takes a look at the rise and the development of the idea that the phoneme is not the ultimate constituent of the speech stream which cannot be broken down into smaller units, but the association product of a finite inventory of universal parameters called differently by different frameworks: features, elements, gestures.
The break with the “atomic” conception of the phoneme and the elaboration of the phonological primitives theories go, however, hand in hand with a complicated formalism and with the hermetization of phonology by its estrangement from phonetics.
The lexicon of a speech recognizer is supposed to contain pronunciation models describing how words can be realized as sequences of subword units (usually phonemes).
In this contribution we present a method for upgrading initially simple pronunciation models to new models that can explain several pronunciation variants of each word.
Since the presented strategy is capable of producing pronunciation variants and cross-word dependencies completely automatically, it is an attractive alternative to the manual encoding of multiple pronunciations in the lexicon.
By learning pronunciation rules rather than pronunciation variants from the data, one can combine the advantages of data-driven and rule-based approaches.
Important properties of the proposed methodology are that it incorporates dependencies between the rules from the very beginning (during the training), that it supports exception rules not producing pronunciation variants but affecting the production of such variants by other rules (called production rules), and that it has a sound probabilistic basis for the attachment of likelihoods to the word pronunciation variants.
Experiments showed that the introduction of such variants in a segment-based recognizer significantly improves the recognition accuracy: on timit a relative word error rate reduction of as high as 17% was obtained.
This paper presents the results of a statistical and deterministic analysis of two phonemic lexicons, with respect to the storage and generation of spelling rules using graphemes.
The aim of this paper is to demonstrate the feasibility of generating correctly spelled words for the English language using phoneme-to-grapheme rules.
An algorithm for generating the rules is presented.
A set of spelling rules were identified by the analysis of two differently sized lexicons, 96, 939 words and 11, 638 words, the smaller lexicon being a subset of the larger.
These rules were then tested for their general usability.
62.3% of all words in the 96, 939 word lexicon could be spelled correctly utilising rules alone.
A smaller lexicon which consisted of many of the more frequently occurring words plus a selection of less common words showed that 84.5% of this lexicon could be spelled correctly using rules generated by the analysis of its own lexicon.
However, only 62.3% of this dictionary could be spelled correctly using rules generated from the lexicon of 96, 939 words.
It was also shown that phoneme-to-grapheme mappings are between 63% and 69% alphabetic, depending on the size of dictionary used.
59 general default rules were identified, unfortunately only 22.6% of the smaller dictionary could be spelled correctly by using these rules.
Including speech knowledge in automatic speech recognition (ASR) systems is a good way to improve the performance of recognizers.
In this paper, we propose the orion system which deals with speaker-independent ASR for isolated-words.
orion is a two-pass hybrid system which uses several types of knowledge. This knowledge applies to psychoacoustics, physiology and phonetics.
During the first pass, an auditory model, the perceptually-based linear prediction analysis (PLP), combines static and dynamic features to provide a set of parameters to the dynamic programming algorithm.
After this stage 98% recognition accuracy was obtained for a digit vocabulary and 12 templates per word.
In the case of a confusable vocabulary (E-SET), the introduction of phonetic knowledge in the second pass decreases the error rate by more than 60% (compared to the results of the first pass).
This paper reports on activites at LIMSI over the last few years directed at the transcription of broadcast news data.
We describe our development work in moving from laboratory read speech data to real-world or `found' speech data in preparation for the DARPA evaluations on this task from 1996 to 1999.
Two main problems needed to be addressed to deal with the continuous flow of inhomogenous data.
These concern the varied acoustic nature of the signal (signal quality, environmental and transmission noise, music) and different linguistic styles (prepared and spontaneous speech on a wide range of topics, spoken by a large variety of speakers).
The problem of partitioning the continuous stream of data is addressed using an iterative segmentation and clustering algorithm with Gaussian mixtures.
The speech recognizer makes use of continuous density HMMs with Gaussian mixture for acoustic modeling and 4-gram statistics estimated on large text corpora.
Word recognition is performed in multiple passes, where current hypotheses are used for cluster-based acoustic model adaptation prior to the next decoding pass.
The overall word transcription error of the LIMSI evaluation systems were 27.1% (Nov96, partitioned test data), 18.3% (Nov97, unpartitioned data), 13.6% (Nov98, unpartitioned data) and 17.1% (Fall99, unpartitioned data with computation time under 10× real-time).
The design ofspatial coordination mechanisms for dynamical and continuous multiagent setting is a difficult challenge.
While the top-down decomposition approach is inefficient on such problems, the bottom-up approach is more promising, but requires a tedious manual parameter tuning which raises scaling-up issues.
Our own approach consists in replacing the manual tuning by a specially designed multicriteria evolutionary algorithm devoted to the tuning of our spatial coordination formalism.
In this paper, through a quantitative comparison on a complex spatial coordination problem treated previously by Balch and Hybinette, we show that our system, GACS, finds a population of solutions as efficient as this predecessor though our approach requires less involvement from the designers and can find simpler solutions.
We describe a procedure for acquiring intonational phrasing rules for text-to-speech synthesis automatically, from annotated text, and some evaluation of this procedure for English and Spanish.
Rules generated by this method have been implemented in the English version of the Bell Laboratories Text-to-Speech System and have been developed for the Mexican Spanish version of that system.
These rules currently achieve better than 95% accuracy for English and better than 94% for Spanish.
This paper describes the design principles of a large vocabulary, free text, dictation system, employing advanced speech recognition technology.
It can be used for a wide variety of tasks, and is rapidly customisable to new domains.
A case study of the application of the technology in the creation of a Pathology reporting workstation is described.
The objectives of using large vocabulary speech recognition technology to implement a system for the dictation of free text are described.
The process of creating text, from draft to final version is discussed in terms of the pre-requisites on both the technology and its implementation.
The constraints on the performance and user acceptability of a system using this technology are noted.
The design of a flexible system component supporting user adaptation is shown.
The use of a general purpose, standalone dictation system for a wide variety of potential applications is noted.
The customisation of the system to a new task by the rapid rebuilding of its parameter tables is described.
The application of a customised version of the system to an actual end user environment is exemplified by a case study of a Pathology reporting workstation.
In this paper, we describe sphinx, the world's first accurate large-vocabulary speaker-independent continuous speech recognition system.
We will present current results of sphinx, compare its performance against similar systems, and account for its high accuracy.
A microprocessor system has been designed to convert ordinary French text into audible speech in real-time.
The highly-modular Pascal software translates an input text into phonemes, and assigns duration and pitch via a simple syntactic analysis.
The speech is generated at 10 000 samples/sec with a programmable DSP integrated circuit.
Of two printed circuit cards in the system, one contains an Intel 8086 microprocessor and memory, and the other has the DSP chip and an associated interface, D/A converter and amplifier.
An earlier version of the system, for a VAX-11/780 entirely in software with floating-point arithmetic, required eight times real-time calculation.
The current real-time system shows the practicability of generating high quality French synthetic speech on a printed circuit board.
Interspeaker variability is a major source of errors in automatic speech recognition.
This paper describes a series of experiments, conducted at TELECOM Paris by the «Pattern Recognition and Speech Processing» Group, for controlling some aspects of this variability, thus allowing for the adaptation of speech recognition systems to new users.
The first experiments are based on a linear data analysis technique: multiple linear regression (MLR).
The second set uses multilayer perceptrons, and yields slightly better results, because non linear phenomena are taken into account, versus 15 % with the first one.
Those techniques can also be used for the adaptation of recognizers to new acoustical environments and recording conditions.
In this paper we report on a series of user trials carried out to assess the performance and usability of the Multimodal Multimedia Service Kiosk (Mask) prototype.
The aim of the Esprit Mask project was to pave the way for advanced public service applications with user interfaces employing multimodal, multimedia input and output.
The prototype kiosk was developed after analyzing the technological requirements in the context of users performing travel enquiry tasks, in close collaboration with the French Railways (SNCF) and the Ergonomics group at the University College of London (UCL).
In addition to meeting or exceeding the performance goals set at the project onset in terms of success rate, transaction time, and user satisfaction, the Mask kiosk was judged to be user-friendly and simple to use.
Two new methods are presented here for the detection of the glottal closure instant from the speech waveform.
Both detect abrupt changes in the short-term spectral characteristics of the speech signal within a pitch period caused by glottal events.
The same statistical approach to the sequential detection of events by hypothesis testing is used for this purpose.
The first method is based on the maximization of the likelihood ratio, while the second uses a divergence convexity test.
Experiments on real speech data demonstrate the robust of these methods.
In this paper, we compare two alternative approaches for speaker verification based on hidden Markov model (HMM) technology: single Gaussian HMMs and different types of tied multi-Gaussian HMMs.
In order to assess the performance under real-world constraints, we tested each system using a database of connected digit strings recorded over local and long-distance telephone lines.
According to our experiments, tied-mixture models were able to perform better than the single Gaussian approach provided that sufficient training data were available.
However, our experiments indicate that the single Gaussian HMM approach is to be preferred for real-world speaker verification when only limited amounts of training data are available.
Results will be discussed for both text-dependent and text-independent speaker verification.
In pursuance of better performance, current speech recognition systems tend to use more and more complicated models for both the acoustic and the language component.
Cross-word context dependent (CD) phone models and long-span statistical language models (LMs) are now widely used.
In this paper, we present a memory-efficient search topology that enables the use of such detailed acoustic and language models in a one pass time-synchronous recognition system.
Characteristic of our approach is (1) the decoupling of the two basic knowledge sources, namely pronunciation information and LM information, and (2) the representation of pronunciation information – the lexicon in terms of CD units – by means of a compact static network.
The LM information is incorporated into the search at run-time by means of a slightly modified token-passing algorithm.
The decoupling of the LM and lexicon allows great flexibility in the choice of LMs, while the static lexicon representation avoids the cost of dynamic tree expansion and facilitates the integration of additional pronunciation information such as assimilation rules.
Moreover, the network representation results in a compact structure when words have various pronunciations, and due to its construction, it offers partial LM forwarding at no extra cost.
The voice source is an important factor in the production of different voice qualities.
These different voice qualities are used in speech to convey, among other things, different suprasegmental aspects, e.g., emphasis, phrase boundaries and also different speaking styles such as an authorititave or a submissive voice.
Voice source variations are also an important means of conveying extralinguistic information of various kinds in ordinary speech.
In the present study, voice source variations in normal speech by female speakers have been investigated using inverse filtering.
The results of the inverse filtering are given in voice source parameters appropriate for controlling speech synthesis.
Accordingly, the resulting descriptions have been utilized to produce voice variations in our new synthesis system.
This paper outlines four novel methods for the task of speaker verification.
The first model, a Hybrid Multi-Layer Perception (MLP)-Radial Basis Function (RBF) model, is an MLP predictor whose weights are then used as inputs to an RBF classifier for the verification process.
The second model uses an array of linear predictors to model the true speaker where each predictor is associated with a particular sub-unit of the test utterance.
The third, a Neural Prediction Model, consists of an array of MLP predictors and the fourth, a Hidden Control Neural Network, is a single MLP predictor with added control inputs.
These control inputs modulate the MLP mapping and allow a single MLP to model a complete utterance.
Each method was trained and tested on a modest database and each performs well with verification rates of 100% for the first three models and of 90% for the Hidden Control Neural Network.
A cross-cultural study of Japanese and American children has examined the development of awareness about syllables and phonemes.
Using counting tests and deletion tests, Experiments I and III reveal that in contrast to first graders in America, most of whom tend to be aware of both syllables and phonemes, almost all first graders in Japan are aware of mora (phonological units roughly equivalent to syllables) but relatively few are aware of phonemes.
This difference in phonological awareness may be attributed to the fact that Japanese first graders learn to read a syllabary whereas American first graders learn to read an alphabet.
For most children at this age, awareness of phonemes may require experience with alphabetic transcription, whereas awareness of syllables may be facilitated by experience with a syllabary, but less dependent upon it.
To further clarify the role of knowledge of an alphabet in children's awareness of phonemes, Experiments II and IV administered the same counting and deletion tests to Japanese children in the later elementary grades.
Here the data reveal that many Japanese children become aware of phonemes by age whether or not they have received instruction in alphabetic transcription.
Discussion of these results focuses on some of the other factors that may promote phonological awareness.
A systematic study on a speaker-independent vowel recognition model has been performed.
Karhunen-Loève Transformation (KLT), or Principal Component Analysis, technique was applied subsequent to a spectral analysis of the speech signal by 18 non-overlapping critical-band filters.
Four experiments have been conducted using selected segments of 8 isolated Putonghua (Mandarin) vowels, spoken twice in 5 tones by 38 females and 13 males.
The first experiment uses the same speech sample in training and testing to evaluate the effects of KLT, speaker normalization, distance metric and number of vowel classes.
A modified Mahalanobis distance coupled with a 7-class condition was found to give the best performance.
In the next experiment, one sample was used to train the model, and another trial of the same speech, spoken by the same group of speakers, was used to test it.
It was found that, in general, a sex-specific and tone-specific procedure could be avoided without significant loss in performance.
The third experiment repeatedly trained the model with 50 speakers and tested it wiht the remaining one until all 51 speakers had been tested.
Under this stringent condition, an average recognition rate of 88.2% was achieved using only 4 classificatory dimensions.
In the last experiment, all segments of a vowel were labelled using the most stringent conditions.
The model was confirmed to perform well for one male and one female speaker selected at random.
Also, the vowel that had caused the greatest confusion was found to be well recognized when treated as an allophone of another vowel.
Finally, the possibility of extending the present technique to diphthong recognition is discussed together with some preliminary results.
English and Italian provide some interesting contrasts that are relevant to a controversial problem in psycholinguistics: the boundary between grammatical and extra-grammatical knowledge in sentence processing.
Although both are SVO word order languages without case inflections to indicate basic grammatical relations, Italian permits far more variation in word order for pragmatic purposes.
Hence Italians must rely more than English listeners on factors other than word order.
In this experiment, Italian and English adults were asked to interpret 81 simple sentences varying word order, animacy contrasts between the two nouns, topicalization and contrastive stress.
Italians relied primarily on semantic strategies while the English listeners relied on word order—including a tendency to interpret the second noun as subject in non-canonical word orders (corresponding to word order variations in informal English production).
Italians also made greater use of topic and stress information.
Finally, Italians were much slower and less consistent in the application of word order strategies even for reversible NVN sentences where there was no conflict between order and semantics.
This suggests that Italian is 'less' of an SVO language than English.
Semantic strategies apparently stand at the 'core' of Italian to the same extent that word order stands at the 'core' of English.
It is suggested that these results pose problems for claims about a 'universal' separation between semantics and syntax, and for theories that postulate a 'universal' priority of one type of information over another.
Results are discussed in the light of the competition model, a functionalist approach to grammar that accounts in a principled way for probabilistic outcomes and differential 'weights' among competing and converging sources of information in sentence processing.
A real-time speech synthesis system for unrestricted German text is described.
This system is based on the concatenation of stored single-sound and diphone transition elements.
Input occurs (presently) in phonetic text with pitch information.
The output signal is generated by a computer-controlled log-area-ratio (LAR) vocoder.
The stationary sounds are coded by single frames of vocoder parameters and the transitions by frame pairs, taken from real speech with slight modifications.
Intermediate frames are interpolated during synthesis.
The preliminary experiments concerning interpolation and the choice of frames to be stored are described together with the synthesis procedure (two variants) including table structure and treatment of pitch and sound duration.
Intelligibility measurements and quality comparisons of both variants have been carried out.
Research was conducted to determine if alterations in the acoustical characteristics of voice occur over periods of sustained operations.
Twelve male United States Air Force B-1B bomber aircrewmen participated in the study.
The participants served in crews of four and performed three 36-hour experimental periods (missions) in a high-fidelity simulator.
The missions were interspersed with 36-hour rest breaks.
Data were lost from two members of the third team due to a communication malfunction.
Speech, cognitive and subjective fatigue data were collected approximately every three hours for 11 trials per mission.
Fundamental frequency and word duration were both found to vary significantly over trials (fundamental frequency F(10,90) = 2.63, p = 0.0076, word duration F(10,90) = 2.5, p = 0.0106).
Speech duration results also showed a significant main effect of mission (F(2,18) = 6.91, p = 0.0082).
The speech data follow the same trend as the data from the cognitive tests and subjective measures.
A strong diurnal pattern is reflected in nearly all of the dependent measures.
Overall, the results support the proposition that voice may be a valid indicator of a speaker's fatigue state.
The syntactic and phonotactic structure of the sentences are systematically varied in order to understand how two functions can be carried out in parallel in the prosodic continuum: (1) enunciative: demarcation of constituents; (2) illocutory: speaker's attitude.
The statistical analysis of the corpus demonstrates that global prototypical prosodic contours characterise each attitude.
Such a global encoding is consistent with gating experiments showing that attitudes can be discriminated very early in utterances.
These results are discussed in relation to a morphological and superpositional model of intonation.
This model proposes that the information specific to each linguistic level (structure, hierarchy of constituents, semantic and pragmatic attributes) is encoded via superposed multiparametric contours.
An implementation of this model is described that automatically captures and generates these prototypical prosodic contours.
This implementation consists of parallel Recurrent Neural Networks each responsible for the encoding of one linguistic level.
The identification rates of attitudes for both training and test synthetic utterances are similar to those for natural stimuli.
We conclude that the study of discourse-level linguistic attributes such as prosodic attitudes is a valuable paradigm for comparing intonation models.
We attempted multi-talker, connected recognition of the spoken American English letter names b, d, e and v, using a recurrent neural network as the speech recognizer.
Network training was based on forward-propagation of unit potentials, instead of back-propagation of unit errors in time.
The target function was based on an input speech parameter which turns on and off at each onset of a spoken letter name.
The network was trained to copy that input speech parameter to the output unit assigned to the correct letter name.
Letter name discrimination was as high as 85% on test utterances.
Definitional accounts of language structure are explored in this paper.
Several classes of arguments for definitions are reviewed; those which connect to: classical theories of reference, theories of informal validity, theories of sentence comprehension, and theories of concept learning.
We suggest that, for each of these areas, accounts which rely upon definition are, in fact, not to be preferred on evidential grounds to plausible non-definitional alternatives.
We also present a series of experimental observations bearing on one of these areas — that of sentence comprehension.
Such subject judgements are independently demonstrated to be sensitive to structural relations of comparable type for other linguistically non-problematic types.
The presence ofa reduced visibility distance on a road network (thick fog, heavy rain, etc.) affects its safety.
We designed a roadside system on which aims to detect critical situations such as dense fog or heavy rain with a simple CCTV camera.
Different image processing are presented, particularly the estimation of visibility distance, the detection of fog, and the detection of rain.
Based on the principles underlying these algorithms, a camera is specified to meet the needs expressed by the standard NF P 99-320 on highway meteorology.
Experimental results are presented as well as prospective validation at a bigger scale.
A new glottal wave analysis method, Pitch Synchronous Iterative Adaptive Inverse Filtering (PSIAIF) is presented.
The algorithm is based on a previously developed method, Iterative Adaptive Inverse Filtering (IAIF).
In the IAIF-method the glottal contribution to the speech spectrum is first estimated with an iterative structure.
In the new PSIAIF-method the glottal pulseform is computed by applying the IAIF-algorithm twice to the same signal.
The first IAIF-analysis gives as a result a glottal excitation that spans over several pitch periods.
This pulseform is used in order to determine positions and lengths of frames for the pitch synchronous analysis.
The final result is obtained by analysing the original speech signal with the IAIF-algorithm one fundamental period at a time.
The PSIAIF-algorithm was applied in glottal wave analysis using both synthetic and natural vowels.
The results show that the method is able to give a fairly accurate estimate for the glottal flow excluding the analysis of vowels with a low first formant that are produced with a pressed phonation type.
This paper presents an analysis of a corpus of grammars written for learning French in England from 1660 to 1820, a period sometimes referred to euphemistically as the “long century” which saw language teaching evolve in response to broader social and epistemological developments, namely the increased codification of vernacular grammar against a backdrop of scientific rationalism and, in England, the greater institutionalisation of school-based pedagogies.
The aim of the analysis is twofold: firstly, to identify some key shifts in the formulation of content, specifically changes in overall structure and distribution of sections, including differences in grammatical nomenclature, and, secondly, to contextualise these developments by considering the changing role of the grammarian-teachers as demonstrated in the way they position themselves as authors to different publics.
This paper describes the application of a one-stage Dynamic Programming (DP) algorithm to the acoustic-phonetic decoding stage in a speech recognition system.
Essentially two methods are compared: (1) the recognition of demisyllable. and (2) the recognition of consonant cluster and syllabic nuclei (vowels or diphthongs).
Demisyllables, consonant clusters and vowels or diphthongs have proved their usefulness in the recognition of continuous speech.
Furthermore, the method of Dynamic Programming is a well established principle for the recognition of connected words.
In this paper a stage of acoutic-phonetic decoding is presented which applies Dynamic Programming to these phonetic units.
The algorithm used is essentially the same as the one commonly known for connected word recognition, which does not need explicit segmentation.
However, for the recognition of demisyllables, consonant clusters and vowels, some specific modifications were necessary, including the introduction of a special internal syntax between the units.
The derived algorithm was tested using fluently spoken sentences from 75 word lexicon in which the most frequent consonant clusters and vowels of the German language were represented.
Different tests were made in order to get a comparison of the recognition accuracy using a context-dependent application (concerning neighbouring units) and a context-independent application.
These results and a comparison to a similar acoustic-phonetic stage using an explicit segmentation are presented.
Participants in a discourse sometimes fail to understand one another, but, when aware of the problem, collaborate upon or negotiate the meaning of a problematic utterance.
To address non-understanding, we have developed two plan-based models of collaboration in identifying the correct referent of a description: one covers situations where both conversants know of the referent, and the other covers situations, such as direction-giving, where the recipient does not.
In the models, conversants use the mechanisms of refashioning, suggestion and elaboration, to collaboratively refine a referring expression until it is successful.
To address misunderstanding, we have developed a model that combines intentional and social accounts of discourse to support the negotiation of meaning.
The approach extends intentional accounts by using expectations deriving from social conventions in order to guide interpretation.
Reflecting the inherent symmetry of the negotiation of meaning, all our models can act as both speaker and hearer, and can play both the role of the conversant who is not understood or misunderstood and the role of the conversant who fails to understand.
Including speech knowledge in automatic speech recognition (ASR) systems is a good way to improve the performance of recognizers.
In this paper, we propose the ORION system which deals with speaker-independent ASR for isolated-words.
During the first pass an auditory model, PLP (perceptually-based linear prediction analysis) combines static and dynamic features to provide a set of parameters to the dynamic programming algorithm.
After this stage 98 % recognition accuracy was obtained for a digit vocabulary and 12 templates per word.
The introduction of phonetic knowledge in the second pass decreases the error rate by more than 60 % (compared to the results of the first pass) for a confusable vocabulary (E-SET).
Experience with the use of the X-ray microbeam system has confirmed that a substantial reduction in X-ray dosage can be achieved.
The movements of 6 pellets on the tongue and teeth are tracked at a rate of more than 100 frames per second, with an effective exposure area of about 1 cm2 per frame, and with an exposure rate of 120 mR per minute in this area.
Approximation of the movements of the pellets on the tongue, jaw and velum by means of the step response of a linear second order system has revealed that there are considerable differences in the values of the time constant among these speech organs.
The duration of the step command also varies according to the type of vowel.
These differences are reflected on the undershoot pattern and on the pattern of the coarticulation between consonant and vowel.
The investigation of the movements of the velum has showed that a simultaneous observation of EMG is important for interpreting the pattern of the underlying motor control.
The current article discusses the problem of appropriate intonation selection in Person-Machine dialogues, such as those expected in intelligent information systems when, for example, information retrieval is required.
An approach is proposed which integrates the previously mostly separate paradigms of automatic natural language generation and speech synthesis in a Person-Machine dialogue scenario.
The paper argues that such an approach removes some of the well-known gaps in both text-to-speech and concept-to-speech systems.
In the current context of a growing internationalization of scientific exchanges, the issue of the language of scientific publications – settled in the natural sciences since the 1980s – has now become a central issue for the Social Sciences.
Our paper discusses this topical issue through a detailed analysis of the linguistic strategies adopted by two major French social science journals, Population and Revue française de sociologie (RFS), which have chosen to translate into English a selection (RFS) or all (Population) of their articles.
In view of the measured effects in terms of visibility in the international scientific field – increasing visibility for the journal Population at the expense of its French edition and a marginal effect for the RFS – we raise questions about the role of national social sciences journals and recall that the specific intellectual mission of these journals lies beyond the pursuit of internationalization.
In this article we review strategies used in the design of two-folded rejection-based classifiers.
Beside the so-called classical “accept-first” strategy we have recently proposed very general families built on two different approaches, namely the “reject-first” [Fré98a, MF01b] and “mixture-first” [SFM02] reject schemes.
These three approaches differ by the kind, as well as the order, of the tests leading to the classifier final output.
While the first one starts by testing for distance rejection and, if necessary, finishes by testing for exclusive classification or ambiguity rejection respectively, the two others start respectively by testing for exclusive classification and ambiguity rejection, and then finish by the remaining alternatives.
We unify the three schemes by defining fuzzy operators built on De Morgan operators (t-norms, t-conorms, complement).
Behaviours of such different classifiers are illustrated on artificially generated examples.
When the reference speakers are represented by Gaussian mixture model (GMM), the conventional approach is to accumulate the frame likelihoods over the whole test utterance and compare the results as in speaker identification or apply a threshold as in speaker verification.
In this paper we describe a method, where frame likelihoods are transformed into new scores according to some non-linear function prior to their accumulation.
We have studied two families of such functions.
First one, actually, performs likelihood normalization – a technique widely used in speaker verification, but applied here at frame level.
The second kind of functions transforms the likelihoods into weights according to some criterion.
We call this transformation weighting models rank (WMR).
Both kinds of transformations require frame likelihoods from all (or subset of all) reference models to be available.
For this, every frame of the test utterance is input to the required reference models in parallel and then the likelihood transformation is applied.
The new scores are further accumulated over the whole test utterance in order to obtain an utterance level score for a given speaker model.
We have found out that the normalization of these utterance scores also has the effect for speaker verification.
The study of synthesizing Chinese is faced with the pressing task of improving sound quality.
This article presents the structure and features of our synthesis system for Standard Chinese.
This system was built on the basis of an acoustic-phonetic analysis of Chinese syllables.
Several original models and rules are employed in the system.
All the 1268 syllables in Standard Chinese have been synthesized by this system, which produces a sound quality close to that of natural speech with respect to both intelligibility and naturalness.
This paper describes the results of a joint project funded by two French research laboratories, LIMSI-CNRS and INSERM-CREARE, and one end-user organisation, INJA (National Institute for Young Blind People).
This project applies multimodal interfaces including speech recognition and synthesis to provide improved computer access for the blind.
A multimodal text editor designed to provide enriched texts, direct manipulation and immediate feedback for text editing tasks is described.
Promising results are presented, but combining speech with other modalities in the same interface also reveals some new technological problems that are hidden when speech is used in an isolated way.
These problems are discussed, and user needs and expectations are presented.
It has been developpedfor different applications which are also described.
An innovative method of coding Line Spectrum Pair (LSP) parameters for transmission over noisy channels is presented.
Typically, low bit-rate speech coders use these parameters to convey perceptually important spectral information.
Thus it is necessary that these parameters are not only efficiently quantized but preserved during transmission.
The scheme uses a joint source and channel coding technique applied to a concatenated trellis structure.
Operating as a source coder, the system encodes below the 1 dB spectral distortion limit.
It is shown that by modifying the encoder cost function to include the expected channel distortion, the code exhibits improved robustness to channel errors.
This is accomplished with minimal increase in complexity and without increase in bit-rate.
It is noted that the scheme performs well over a wide range of channel bit error rates and compares favourably with a standard scalar LSP quantizer tandemed with a channel coder both in terms of bit-rate and channel noise immunity.
In traditional accounts on speech prosody, fundamental frequency, duration and intensity have been described as the most important attributes.
Among these, intensity has attracted the least attention.
In perceptual studies both F 0 and duration have had an undisputable role in signalling prosodic categories, but the role of intensity has been less clear.
This has resulted in an emphasis on the former attributes in current speech synthesis schemes.
We are in this study exploring the use of speech intensity and also other segmental correlates of prosody.
Intensity has a dynamic aspect, discriminating emphasized and reduced stretches of speech.
A more global aspect of intensity must be controlled when we try to model different speaking styles.
Specifically, we have been trying to model the continuum from soft to loud speech.
Many groups have investigated the relationship of word error rate and perplexity of language models.
This issue is of central interest because perplexity optimization can be done independent of a recognizer and in most cases it is possible to find simple perplexity optimization procedures.
Moreover, many tasks in language model training such as the optimization of word classes may use perplexity as target function resulting in explicit optimization formulas which are not available if error rates are used as target.
This paper first presents some theoretical arguments for a close relationship between perplexity and word error rate.
Thereafter the notion of uncertainty of a measurement is introduced and is then used to test the hypothesis that word error rate and perplexity are correlated by a power law.
There is no evidence to reject this hypothesis.
Derivatives in -(cu)-lus, -(cu)-la, -(cu)-lum in technical Latin texts are difficult to translate.
In some cases, the suffix is used as a diminutive: the object is smaller than the object without a suffix.
In other cases, it is used to denote differences in time or in the thematic environment: both are used to denote the same object in the same context but at different times or they denote two different (or similar) objects in different contexts without regard to size.
Rare examples of doublets used by technical authors in the same chapter, same paragraph and even in the same sentence to describe an object by its name or by its derivative are analysed in this article.
In this article, we describe an automatic system for train timetable information over the telephone that provides accurate connections between 1200 German cities.
The caller can talk to it in unrestricted, natural, and fluent speech, very much like he or she would communicate with a human operator, and is not given any instructions in advance.
The system's four main components, speech recognition, speech understanding, dialogue control, and speech output, are separated into independent modules that are executed sequentially.
In an ongoing field trial, this system has been made available to the general public, both to gather speech data and to evaluate its performance.
This field test was organized as a bootstrapping process: initially, the system was trained with just the developers' voices, then the telephone number was passed around within the department, the company and, finally, the outside world.
After each step, the newly collected material was used for retraining, as well as for general improvements.
Several computational methods based on mathematical tools already exist, but most of the time their implementation is complex and takes longer execution time.
In this article we propose another learning and anticipation method intended to user assistance in dynamic situations.
To do so, manipulated knowledge is especially structured to limit our solution's complexity and to facilitate learning and anticipation.
A rich variety of factors have been proposed as possible determinants of differences in the ease of processing of relative clauses.
These determinants include the grammatical role of the head, the shape of surface order configurations the occurence of interruptions of the main clause, and the presence or absence of morphological cues.
The strict SVO word order of English makes it so that subject-modifying relatives necessarily interrupt the main clause, thus confounding the effects of role and interruption determinants.
Hungarian, with its variable word order, allows us to achieve a somewhat better understanding of the separate effects of roles, configurations, interruptions, and morphological cues.
A study using 144 different restrictive relative clause patterns in Hungarian provided evidence for the importance of three determinants of relative clause processing.
First, the importance of perspective maintenance was indicated by the fact that SS sentences were the easiest to process and that SO were the most difficult.
Second, the extreme difficulty subjects had in processing NNV sentences with a relative clause modifying the second noun indicated the importance of limits on fragment construction of chunks in a bottom-up parsing process.
The use of antecedent tagging to mark extraposed relatives in SOV languages with variable order such as Hungarian and Georgian also indicated the importance of limits on fragment construction.
Third, the conflict between focusing in the relative clause and focusing in the main clause indicated the importance of focus maintenance.
A variety of other proposed determinants were found to be of little importance in accounting for the processing of relative clauses in Hungarian.
High quality low-delay speech coding at 8–16 kbit/s can be obtained with backward adaptive analysis-by-synthesisalgorithms, such as Low-Delay CELP (the new CCITT 16 kbit/s standard), Low-Delay Vector Excitation Coding (LD-VXC), and backward adaptive tree/trellis codecs.
The paper examines and reviews some of the basic techniques underlying low delay coding algorithms and presents design and performance trade-offs for low-delay analysis-by-synthesis codecs at rates of 8–16 kbit/s.
A number of approaches for improving the speech quality at 8 kbit/s are discussed.
Backward pitch prediction is compared to a closed-loop forward configuration similar to that used in conventional CELP codecs for the adaptive codebook.
Finally, robustness to transmission errors is discussed and a number of trade-offs for reducing transmission error sensitivity are presented.
A motor theory of speech perception, initially proposed to account for results of early experiments with synthetic speech, is now extensively revised to accommodate recent findings, and to relate the assumptions of the theory to those that might be made about other perceptual modes.
According to the revised theory, phonetic information is perceived in a biologically distinct system, a 'module' specialized to detect the intended gestures of the speaker that are the basis for phonetic categories.
In consequence, the module causes perception of phonetic structure without translation from preliminary auditory impressions.
Thus, it is comparable to such other modules as the one that enables an animal to localize sound.
Peculiar to the phonetic module are the relation between perception and production it incorporates and the fact that it must compete with other modules for the same stimulus variations.
The main purpose of this paper is to present recent results of urban studies about the first urban révolution: two case studies are particularly developed, Uruk and Mari.
Building analysis and landscape analysis are the two principles tools used in tis study, based on old and latest field results.
Stress induced by various types of situation leads to vocal signal modifications.
Previous studies have indicated that stressed speech is associated with a higher fundamental frequency and noticeable changes in vowel spectrum.
This paper presents pitch- and spectral-based analyses of stressed speech corpora drawn from both artificial and real situations.
The laboratory corpus is obtained by means of the Stroop test, the real-case corpus is extracted from the Cockpit Voice Recording of a crashed aeroplane.
Analyses relative to pitch are presented and an index of microprosodic variation, μ, is introduced.
Spectrum-related indicators of stress are issued from a cumulative histogram of sound level and from statistical analyses of formant frequencies.
Distances to the F1-F2-F3 centre are also investigated.
All these variations, throughout the two different situations, show the direct link between some new vocal parameters and stress appearances.
The results confirm the validity of laboratory experiments on stress, but emphazise quantitative as well as qualitative differences between the situations and the speakers involved.
Both isolated phonemes and continuous speech experiments are presented.
Rennes CITH has a double objective innovative and evaluative functions of health technologies.
More specifically, multivariate monitoring system and active prosthesis allowing the exploration, the evaluation and the treatment of the cardiovascular, nervous and respiratory functions.
This paper describes briefly its origin, the partnerships and some technological activities developed since 2001.
The aim of this paper is to derive the spectral density of signals with periodically missing data, and to establish their model when they are generated from ARMA processes.
Recognition performance decreases when recognition systems are used over the telephone network, especially wireless network and noisy environments.
It appears that non-efficient speech/non-speech detection (SND) is an important source of this degradation.
Therefore, speech detection robustness to noise is a challenging problem to be examined, in order to improve recognition performance for the very noisy communications.
Several studies were conducted aiming to improve the robustness of SND used for speech recognition in adverse conditions.
The present paper proposes some solutions aiming to improve SND in wireless environment.
Speech enhancement prior detection is considered.
Then, two versions of SND algorithm, based on statistical criteria, are proposed and compared.
Finally, a post-detection technique is introduced in order to reject the wrongly detected noise segments.
The Two Stage Fuzzy Decision Classifier (TSFDC) consists of an Artificial Neural Network (ANN) providing a first stage of discrimination and a second post-processing stage.
The latter stage uses reference fuzzy set information, for each class of data considered.
The ANN isolates the two most likely classes for each test vector.
Post-processing selects the final class from amongst the two.
The TSFDC applies post-processing only to those classes which its ANN has difficulty recognising.
Three text-independent Automatic Speaker Identification (ASI) experiments are conducted with emphasis on forensic needs.
In these experiments, the signal is degraded by a range of factors affecting communication channels.
The TSFDC increases the percentage of correctly identified speech frames, for those speakers poorly identified by its ANN, by a mean of 3.27% over the three experiments.
Concurrently, the difference in number of identified frames between true and corresponding runner-up speakers improves by a mean of 5.27%.
Post-processing better than halves the number of speakers misclassified by the ANN.
Many symbols in music scores are linear segments.
In this context, we designed an extractor of segments, which modelizes the characteristics of binary images (scale factor, curvature, bias and noises).
We were able to recognize music symbols (staves, stems, slurs, beams, bar lines, black note heads, quavers and note groups) by applying both this extractor and simple rules of classification for the detected segments, to the defined layers.
In this paper we suggest an improved algorithm for filtering multiplicative noise.
We opted for a simple method to evaluate local statistics and we supposed that the local distribution of the original image was uniform in order to express our Jack of knowledge about the original probability function.
Predictive web usage modeling has undergone an intense period of investigation until the late 90's.
However, two features of web browsing have rarely been taken into account: the presence of noise and parallel browsing.
In this paper, we propose a new model, the SBR model (Skipping-Based Recommender) which uses a technique called skipping, and is able to take into account these features.
In a series of experimental studies, we put forward the various contributions that this model possesses and show that its quality surpasses that of the state-of-the-art.
In spite of superficial similarities, the nominal sentence (jumla ismiyya) according to the Arabic grammatical tradition has little if anything to do with what is usually called by this name in general linguistics since Meillet (1906).
It is not characterized by the lack of any verb or copula, but by a topic + predicate (mubtadaˀ + ḫabar) structure, and covers a substantial set of facts, while explaining them in a self-consistent way.
This paper discusses how these facts are presented and analyzed in a set of Arabic grammars written in Europe from the 17th century to the present day, raising the issue of the inter-translatability of linguistic categories from one tradition to another.
This article presents an approach to Web Engineering which aims to account for context-awareness in a comprehensive and integrated fashion, thus enabling an enhanced adaptation of the application to the end-user.
A conceptual model, permitting the combination of a domain ontology with context-relevant parameters and a degree of relevance, is presented.
Subsequently, the use of such a model in a Web Engineering process is discussed, including appropriate modeling software, and requirements for a runtime system.
This paper is an overview of the NYNEX VoiceDialingSM service — the first introduction of speech recognition based technology to a mass market of residential and business customers.
This is a network based service which allows telephone users to make calls by simply saying the name of the person or place they wish to reach.
VoiceDialingSM is compatible with both touch-tone and rotary dial service and is designed to work on all existing telephone sets and, therefore, on any extensions in the customer's home.
We describe the network architecture, user interface, and speech recognition technology, with special focus on the analysis of success in service deployment and customer acceptance.
The paper opens with an overview of speech research and development at NYNEX Science and Technology.
We explore the use of features derived from multiresolution analysis of speech and the Teager energy operator for classification of drivers' speech under stressed conditions.
We apply this set of features to a database of short speech utterances to create user-dependent discriminants of four stress categories.
In addition we address the problem of choosing a suitable temporal scale for representing categorical differences in the data.
This leads to two modeling approaches.
In the first approach, the dynamics of the feature set within the utterance are assumed to be important for the classification task.
These features are then classified using dynamic Bayesian network models as well as a model consisting of a mixture of hidden Markov models (M-HMM).
In the second approach, we define an utterance-level feature set by taking the mean value of the features across the utterance.
This feature set is then modeled with a support vector machine and a multilayer perceptron classifier.
We compare the performance on the sparser and full dynamic representations against a chance-level performance of 25% and obtain the best performance with the speaker-dependent mixture model (96.4% on the training set, and 61.2% on a separate testing set).
We also investigate how these models perform on the speaker-independent task.
Although the performance of the speaker-independent models degrades with respect to the models trained on individual speakers, the mixture model still outperforms the competing models and achieves significantly better than random recognition (80.4% on the training set, and 51.2% on a separate testing set).
This paper challenges the major theoretical motivation underlying a stage-model for language development (Gleitman, 1981), namely, that early grammars are exclusively of a semantic nature.
Data concerning the development of gender systems in a variety of languages is presented.
Particularly, the development of the use of referential pronouns and inflected verb forms and the role of the animate/inanimate distinction in the development of linguistic gender are seen to involve strictly formal nonsemantic generalizations from their first appearances in children's language, ages 2 years and on.
Early two-word stage grammar cannot then be exclusively 'semantic'.
Since it already involves semantic as well as non-semantic generalizations, the more highly developed grammars of later phases need not trigger any qualitative changes that will warrant a stage model for language development.
Some experiments with voice modelling using recent developments of the KTH speech synthesis system will be presented.
It contains an improved glottal source built on the LF voice source model, some extra control parameters for the voiced and noise sources and an extra pole/zero-pair in the nasal branch.
Furthermore, the present research versions of the KTH text-to-speech system include possibilities for interactive manipulations at the parameter level with on-screen reference to natural speech.
The synthesis system constitutes a flexible environment for voice modelling experiments.
The new synthesis tools and models were used for synthesis-by-analysis experiments.
A sentence uttered by a female speaker was analysed and a stylized copy was made using both the old and the new synthesis system.
With the new system the synthetic copy sounded very similar to the natural utterance.
From this point of view morphological criteria do not appear to provide a decisive answer and in the end it is those of frequency and the linguistic sense of the speakers which seem to give the most reliable indications.
The study of these processes should take into account not only the structure of the loanwords themselves (the 'signifiants'), but also their precise meanings (the 'signifiés')—the words being examined with reference to the phonological and morphological characteristics of the languages involved, and the meanings with reference to the meanings of the Arabic originals and of possible corresponding loans in other Fula dialects and in the intermediary languages.
Articulatory parameters are determined from the first five formant frequencies and the first three formant amplitudes by the minimization of the root mean square error in the acoustic space.
Beforehand, the nonlinear properties of the articulatory-to-acoustic transformation are investigated by use of a parametric representation of the area function and an electric analogue of the lossy vocal tract.
A global analysis of the transformation is performed to locate the excessive nonlinearities that induce errors during the minimization procedure.
A table of reference articulatory-acoustic pairs is generated from that analysis.
The articulatory parameter identification procedure consists of an initial guess obtained from the reference table and a least-squares algorithm.
Tests on formant values produced by the model itself indicate the efficiency of the method in solving the inverse problem.
Another test on formant values corresponding to Fant's area function data provides qualitatively acceptable results, but the lack in precision indicates the limitations of the actual articulatory model.
The aim of this paper is to consider the classical pitch determination method based on the short-time autorcorrelation analysis of the speech signal.
Two commonly used estimators and the effect of signal windowing are taken into account.
It is shown that in both periodic and purely random cases a similar decomposition holds for the estimated autocorrelation.
Such a decomposition makes it possible to foresee the relative merits of the estimators considered, at least as far as gross pitch determination errors and voicing-errors are concerned.
The expected behaviour is found to be in good agreement with results obtained through applying the SIFT algorithm to real speech.
This paper describes some of the implications of Ludmilla Chistovich's “spectral center of gravity” (SCG) effect for a model of the auditory representation of American English vowels.
Chistovich's work on defining a critical distance for the SCG effect is closely related to two of the most fundamental problems in speech communication research: the relation between acoustic attributes and phonemic features and the problem of invariance despite large acoustic differences between speakers.
Firstly, experimental findings relating to the SCG effect are reviewed.
Secondly, a model incorporating these perceptual effects is described, and thirdly, three aspects of the model are discussed: (1) the resulting feature analysis, (2) normalization, and (3) acoustic variability as represented in the model and its relation to Steven's quantal theory of speech production.
Such a simplification allows to turn the learning problem into an optimization problem which relies on an hill-climbing strategy requiring only one pass over the database.
Our strategy penalizes tree depth by using adjusted R2.
The experiments show that generalization error rate of our trees is as good as the one of C4.5, while maintaining the intelligibility of the results.
He also pronounces a small vocabulary called the adaptation vocabulary.
Each new speaker then merely pronounces the adaptation vocabulary.
We have compared two adaptation methods, establishing a correspondence between the codebooks of the reference and the new speakers, on a 20-speaker database with a 104-word application vocabulary.
Method I uses a transposed codebook to represent the new speaker during the recognition process, whereas Method II uses a codebook which is obtained by clustering analysis on the NS's pronunciation of the adaptation vocabulary.
The adaptation vocabulary contains 136 words.
Comparison of the performance of the two methods shows that a new speaker's codebook is not necessary to represent the new speaker.
Consequently we have used the first method to perform tests with a 5000-word application vocabulary, and a 4-speaker database.
The adaptation is still efficient (the mean improvement is about 14%), even if the relative improvement is 30% compared to 56% obtained in the 104-word application experiment.
Further experiments show that the recognition accuracy can be improved by increasing the adaptation vocabulary size and the codebook size.
Pitch detection remains one of the most difficult problems in speech analysis.
Therefore, to detect pitch contours of speech, we have developed a new method which is quite different from conventional ones.
The method utilizes a bank of bandpass filter-pairs;
it is a fully continuous method in the time domain.
This paper describes the parameter optimization of the filter-pair for a database of enlarged vocabulary and the integration of the filter-pair method by adding a voicing detector.
Compared with conventional pitch detection methods, the proposed method produced a low Gross Pitch Error rate.
We describes here a general gaming model that can represent the rules of almost any computable game.
We first develop the model with a game's classification and then we expose the computable one.
Then we briefly describes an application that allow the confrontation of multiple kind of decision engines on games concerned by this model.
To conclude, we expose a short protocol that allow to manage both the application and the decision engine's competitions around the general gaming model.
In this paper, we will consider the problem of speech recognition under noisy conditions.
Local statistical information of the speech and the noise are estimated online and used as inputs to the estimators.
The results are comparable to those obtained in clean condition at a moderate signal-to-noise ratio (SNR) of 20 dB.
Substantial improvement is also obtained at lower SNRs.
By carefully studying the results, it is noted that the MLP based estimators appear to perform poorly when there is virtually no detectable significant speech activity.
As a result, a modified gain function is introduced.
This paper presents a study on the optimization of the set of rules that can be extracted from a set of data using the requent itemset search methodology.
The association rules have been extracted using standard frequent itemsets level-wise search.
A discussion holds on the pruning of the set of rules and on the possible optimization of the pruning of this rule set.
As usual, support and confidence of rules are taken into account.
In parallel, other rule quality criteria are introduced and discussed (referring mainly to statistics criteria).
We propose a graphical environment using the new radial tree layout, zoom/pan techniques and some existing methods, including explorer-like, hierarchical visualization, interactive techniques to represent large decision trees in a graphical mode more intuitive than the results in output of usual decision tree algorithms.
The user can easily extract inductive rules and prune the tree in the post-processing stage.
The numerical test results with real datasets show that the proposed methods have given an insight into decision tree results.
It can guide the user towards for evaluating the models and also making more accurate decisions.
This paper proposes a normalization scheme for HMM-based text-dependent speaker verification in which the claimed-speaker model score and the background model score are computed for a common alignment made on the speaker-independent model of the password.
It is shown that such a normalization preserves some speaker-specific information contained in the alignment and makes the normalization score more consistent in emphasizing remarkable parts of the claimed-speaker model.
A special training procedure is proposed.
Experiments on a large-scale and realistic telephone database are reported.
Finally, first experiments about the integration of an information based on alignment in the decision making are presented.
All these results show the interest of the method and encourage further investigation on the speaker modeling in such an approach.
In this paper we present a new direction finding algorithm for non circular sources based on the polynomial rooting technique.
Using polynomial rooting instead of a searching technique limites the method to linear uniformly spaced arrays.
Polynomial rooting however reduces computation cost and enhances resolution power significantly.
Computer simulations are used to show the performance of the algorithm compared with that given by some known algorithms.
The paper describes an experimental-phonetic approach to the study of speech melody, developed at the Institute for Perception Research (IPO).
The advocated method leads to intonation models which are helpful for the interpretation of acoustic and physiological data on pitch in natural speech.
It is also a powerful framework for the design of rules for intonation synthesis in a variety of languages.
This paper reviews what is currently known about the sensory and perceptual input that is made available to the word recognition system by processes typically assumed to be related to speech sound perception.
In the first section, we discuss several of the major problems that speech researchers have tried to deal with over the last thirty years.
In the second section, we consider one attempt to conceptualize the speech perception process within a theoretical framework that equates processing stages with levels of linguistic analysis.
This framework assumes that speech is processed through a series of analytic stages ranging from peripheral auditory processing, acoustic-phonetic and phonological analysis, to word recognition and lexical access.
Finally, in the last section, we consider several recent approaches to spoken word recognition and lexical access.
We examine a number of claims surrounding the nature of the bottom-up input assumed by these models, postulated perceptual units, and the interaction of different knowledge sources in auditory word recognition.
An additional goal of this paper was to establish the need to employ segmental representations in spoken word recognition.
This study presents a new frequency domain parameter, Parabolic Spectral Parameter (PSP), for the quantification of the glottal volume velocity waveform.
PSP is based on fitting a parabolic function to the low-frequency part of a pitch-synchronously computed spectrum of the estimated glottal flow.
PSP gives a single numerical value that describes how the spectral decay of an obtained glottal flow behaves with respect to a theoretical limit corresponding to maximal spectral decay.
By analyzing speech signals of different phonation types the performance of the new parameter is compared to three commonly used time-based parameters and to one previously developed frequency domain method.
Various types of knowledge can be extracted from the data stemming from a questionnaire.
They depend on the questioning of the analyst but also the methods of data processing which are used.
And so one can obtain the refusal of a null hypothesis but also a typology of the items of the questionnaire, the subjects which answered it, but still a graphic structure of inferential filiation, a hierarchy of behavioral rules, etc.
In this paper, we present several possible approaches of treatment of a questionnaire aiming at structuring features of personality derived from behavior of answer to the questionnaire.
Instead, a large speech corpus is created per emotion to synthesize speech with the appropriate emotion by simple switching between the emotional corpora.
The acoustic characteristics of each corpus are different and the emotions identifiable.
The acoustic characteristics of each emotional utterance synthesized by our method show clear correlations to those of each corpus.
Perceptual experiments using synthesized speech confirmed that our method can synthesize recognizably emotional speech.
We further evaluated the method's intelligibility and the overall impression it gives to the listeners.
The results show that the proposed method can synthesize speech with a high intelligibility and gives a favorable impression.
With these encouraging results, we have developed a workable text-to-speech system with emotion to support the immediate needs of nonspeaking individuals.
This paper describes the proposed method, the design and acoustic characteristics of the corpora, and the results of the perceptual evaluations.
In this paper, a joint process estimation algorithm is presented that simultaneously estimates the smooth spectral structure of the speech signal and the pulse-like driving function used in Multi-Pulse Linear Predictive Coding (MPLPC).
Experimental results indicate that while the resulting excitation function differs from the normal multi-pulse excitation, the difference between the optimized and non-optimized linear predictive coding parameters is minimal in both a subjective and a numerical sense.
This paper presents an original method to assess the quality of a grey level image.
An example of application is presented on images compressed according to the JPEG standard.
Contrary to most of the existing methods, this quality assessment is univariant, i.e. it doesn't require any reference image.
The quality is represented by a mark whose variation depends on what use the image is for: visual, mathematical, data processing.
A neural network is used to learn the marking way with a pool of known examples.
The method is then compared to classical bivariant methods to make sure that it is reliable.
The anticipated results arrived at have a less than 7 % precision.
The aim of this study is to propose a new approach to Automatic Language Identification: it is based on rhythmic modelling and fundamental frequency modelling and does not require any hand labelled data.
First we need to investigate how prosodic or rhythmic information can be taken into account for Automatic Language Identification.
A new automatically extracted unit, the pseudo syllable, is introduced.
Rhythmic and intonative features are then automatically extracted from this unit.
Elementary decision modules are defined with gaussian mixture models.
These prosodic modellings are combined with a more classical approach, a vocalic system acoustic modelling.
Experiments are conducted on the five European languages of the Multext corpus: English, French, German, Italian and Spanish.
The relevance of the rhythmic parameters and the efficiency of each system (rhythmic model, fundamental frequency model and vowel system model) are evaluated.
The influence of these approaches on the performances of automatic language identification system is addressed.
We obtain 91 % of correct identification with 21 s. utterances using all the information sources
Recent work ([CHI 98], [DEN 98]) has shown the interest of using Metropolis procedures in a Bayesian framework to search for good classification trees.
For a particular class ofprior distributions on the trees, we introduce a new algorithm for MCMC sampling oftrees, similar to a Gibbs sampler, based on Willems et al. 's “tree weighting” algorithm [WIL 95], which results in a dramatic increase of the number of models actually taken into account.
The sampled tree models are then averaged to produce an aggregate estimator or classifier.
We present results ofsimulations on three benchmark datasets, that show the practical interest of this procedure.
In this study we present a cross-linguistic analysis of the strategies used by Korean, Japanese and English-speaking children in processing sentences with relative clauses.
The results of an experiment on the comprehension of relative clauses in Korean are reported and compared with prior research on the acquisition of relative clauses in Japanese and English.
In our experiment on Korean, 6-year-old children acted out sentences with left-branching and center-embedded relative clauses in two matrix word orders, SOV and OSV, and two intonation conditions, “clear” (syntactically motivated) and “list” intonation.
The findings provide strong evidence for a basic left-to-right processing strategy and significant roles for a canonical sentence strategy and for a parallel function strategy in the comprehension of relative clauses in Korean.
We propose that an adequate cross-linguistic account of relative clause comprehension must be based upon an integrated view of multiple universal processing strategies, whose application will depend upon language-specific structural properties of relative clauses and upon the developmental stage of the child.
Volterra models are widely used in various application areas.
Their usefulness is mainly due to their ability to approximate with an arbitrary precision any fading memory nonlinear system, and their property of linearity with respect to parameters, the kernels coefficients.
The main drawback of these models is their parametric complexity needing to estimate a huge number of parameters.
Considering Volterra kernels as symmetric tensors, we use their PARAFAC decomposition to derive the Volterra-Parafac models inducing a substantial parametric complexity reduction.
We show that these models can be viewed as a set of Wiener models in parallel.
Then, we apply the extended Kalman filter for recursively identifying such Volterra-Parafac models.
Some simulation results illustrate the effectiveness of the proposed identification method, in the case of cubic Volterra systems.
KEAL is a continuous speech recognition system developed at the CNET laboratory in Lannion (France).
Part of the laboratory's current work aims at extending it in the direction of a speech-understanding and man-machine dialog system.
A question-answer-type dialog is set in motion in order to provide the user with information (the current application consists in simulating a directory inquiries service).
This paper describes how syntactic, semantic and pragmatic knowledge is used for implementing such a dialog, and the main advantages and drawbacks of the methods chosen are discussed.
Sentence recognition is performed by a left-to-right bottom-up parser by means of a semantic context-free grammar.
Using a method analogous to that of semantic attributes, the parse-tree is then interpreted in order to obtain a semantic structure which represents the information relevant to the subsequent dialog.
The dialog manager uses the semantic structure for instantiating a model graph, which represents the state of the dialog at any instant; it indicates the next message to be sent to the user, and how to analyse his answer.
An example derived from the directory inquiries service is described.
This article proposes a principle of knowledge's integration for the improvement of a system of defects recognition by vision on wooden boards.
We locate the problem of vision which is at the base of this study, then we clarify the expert knowledge, as well in the field of the wood profession as in the field of vision.
We use for that a symbolic model based on the NIAM/ORM method, formalizing the expert knowledge expressed in natural language.
Then we present the way we exploit these expert knowledge to generate nodes of an arborescent structure for the defects identification of wooden boards.
Each node represents an engine of inference based on fuzzy linguistic rules.
The results obtained prove the interest of this principle.
This paper reports the results of the development, deployment and testing of a large spoken-language dialogue application for use by the general public.
We built an automated spoken questionnaire for the US Bureau of the Census.
In the project's first phase, the basic recognizers and dialogue system were developed using 4000 calls.
In the second phase, the system was adapted to meet Census Bureau requirements and deployed in the Bureau's 1995 national test of new technologies.
In the third phase, we refined the system and showed empirically that an automated spoken questionnaire could successfully collect and recognize census data, and that subjects preferred the spoken system to written questionnaires.
Our large data collection effort and two subsequent field tests showed that, when questions are asked correctly, the answers contain information within the desired response categories about 99% of the time.
Jean Marot's rebus poem La vraye disant Advocate des Dames (1506), was composed in the form of a “neuvain picard.”
Even though “Grands Rhétoriqueurs” are known to be “jongleurs de syllables” (“jugglers of syllables”), the literary device used in this poem proves to be problematic.
By analyzing the different reading levels of the poem (literary game, historical poem, controversial poem, etc.), the author attempts to demonstrate how La vraye disant Advocate des Dames influenced his future career.
It has most certainly had more influence than his earlier “rebus-rondeau”, which had been analyzed by Adrian Armstrong and where specific spatial relationships proved to be significant in the layout.
Language involves both structure and process.
Giving each its due, we present a cognitive process model and show how its empirical success is related to claims about syntactic structure.
However, what is directly observed in the experiments is not a syntactic structure but the execution of a plan.
We present a language of process for representing such plans and thereby provide a unified explanation of several developmental phenomena, including the results of the above experiments and of new experiments suggested by our approach.
The explanation is in terms of the cognitive resources required to formulate and execute a plan.
Since the explanation is based on nonsyntactic processing, the children's syntax need no longer be held faulty.
This conclusion invigorates the claim that the range of phrase structures available to children is biologically constrained.
Understanding the neural bases of cognition has become a scientifically tractable problem, and neurally plausible models are proposed to establish a causal link between biological structure and cognitive function.
In the course of development and in the adult this internal evolution is epigenetic and does not require alteration of the structure of the genome.
A selective stabilization (and elimination) of synaptic connections by spontaneous and/or evoked activity in developing neuronal networks is postulated to contribute to the shaping of the adult connectivity within an envelope of genetically encoded forms.
At a higher level, models of mental representations, as states of activity of defined populations of neurons, are discussed in terms of statistical physics, and their storage is viewed as a process of selection among variable and transient pre-representations.
Theoretical models illustrate that cognitive functions such as short-term memory and handling of temporal sequences may be constrained by “microscopic” physical parameters.
Finally, speculations are offered about plausible neuronal models and selectionist implementations of intentions.
The aim of this paper is to support the idea that the study of complex cooperative systems needs a new conceptual and methodological framework in order to be understood and ultimately reorganised for better efficiency.
We will then stress the role of this new approach as a deep renewal in the field of social sciences.
This paper reports on experiments of porting the ITC-irst Italian broadcast news recognition system to two spontaneous dialogue domains.
Porting was investigated by applying state-of-the-art adaptation methods on acoustic and language models, and by evaluating the trade-off between performance and required amount of task specific annotated data.
The use of different levels of supervision for acoustic model adaptation was also studied.
By employing 2 h of manually annotated speech, word error rates of 26.0% and 28.4% were achieved by the adapted systems.
These results are to be compared with the performance of two domain specific baseline systems, 22.6% and 21.2%, respectively, which were developed on much more training data.
Finally, a robust method is presented that allows to tune the insertion of spontaneous speech phenomena by the speech decoder.
A significant stake of shape query in databases of images is to define unsupervised thresholds making it possible to avoid a flood of false detections, or, on the contrary, rejections of shapes which should have been recognized.
By taking as example a method of shape recognition introduced by Lisani [15, 16], we show that one can answer the following question: given a query shape and a database of images, below which distance between the query and a detected shape is one sure that the shape is recognized?
This insurance is quantified by the number of false alarms associated with the pair query shape – candidate shape.
Although this method only considers for the moment pieces of shape, it already leads to sure detections based on a single piece of shape.
Speech sounds, as heard by listeners, contain phonetic, personal, and transmission information.
The differences between the formant frequencies of vowels spoken by men, women, and children show a fairly uniform tendency in several studies and languages, and they are regarded as personal quality differences.
The differences between the sexes are mainly due to the descent of the larynx in males during puberty.
The observed tendency in female/male formant frequency ratios is reproduced in a calculation taking into account the physiological consequences of larynx descent and assuming that the vowel specific neural commands to the articulators remain unchanged.
The perception of phonetic quality is seen as a process of tonotopic gestalt recognition.
The tonality (= critical-band rate) distances between the formants in phonetically identical vowels are claimed and shown to be invariant as long as they are smaller than 6 Bark.
The absolute position of the formants allows personal variation.
The tonality distance between the first formant and the fundamental is smaller in most vowels spoken by women than in those by men and children.
As for the role of the fundamental in this connection, some alternative hypotheses are discussed.
Image's experts use different kind of attributes to represent texture information.
We propose a methodology to automatically choose the best texture models using a feature selection algorithm.
Therefore we compare the efficiency of several recent algorithms.
The algorithms evaluation is performed using classification error rates and heuristics.
We demonstrate the interest ofsuch a methodology on Brodatz and satellite images.
The familiar Arabic pronominal object marker iyyā- performs other functions within the language.
One of these, the demonstrative, has been recognized in spoken Egyptian Arabic but passes virtually unremarked in written Arabic.
Nevertheless, it is so used by writers from the eastern Arabophone world more often than by those from the west.
As such, it usually performs four roles in structuring information: expressing contrast, emphatic reflexivity, and two degrees of distal deixis.
While modern Arab writers appear to use it demonstratively more often than did those of medieval and classical Arabic, that earlier writers were using it suggests that its demonstrative property is an inherent feature.
This is confirmed by comparing object markers in other Semitic languages, which may function as demonstratives in Hebrew and Aramaic, reflexives in Syriac, and in remote deixis in Amharic.
Working on a feasibility study of wheatears counting, a colour component texture's analysis method was developed.
The agronomic goal is yield prediction before harvest evaluating mean number of wheatears per squared meter according to the field variation knowledge.
To this counting system, we evaluate six textural parameters (two statistical parameters and four Haralick features from co-occurrence matrix) on the main colour systems and vegetation indices used in agronomic applications.
A new hybrid system provides a representation of wheatears' pictures taken under natural conditions with a better extraction of wheat.
A method based on distances measurements (Euclidian, Mahalanobis) allows to extract wheatears with few errors corrected by mathematical morphology.
Although we encounter difficulties from light intensity's variation and high entropy in the scene (ears' covering and shadows), results allow to extract disturbed wheatears and last recent images give an higher accuracy in segmentation.
The use of automatic speech recognition (ASR) technology to automate telephone-service transactions provides opportunities for significant operational cost savings, as well as for improvements in the quality of customer service.
At GTE, our focus has been on developing speech-interactive systems, named OASIS, for service-order transactions.
In this paper, we discuss our methodology for developing dialogs, describe the dialog developed for our service-disconnect application, and present data from a field trial of the disconnect system.
Our design methodology addresses development of a transaction model, specification of dialog flow, design of language structures, and construction of system speech.
In addition, following our approach, dialog flow and recognition vocabularies are developed in the context of the system recognition and interpretation capabilities.
Primary attributes of our methodology include the following: structured representation of conversational transactions as a progressive flow of information elements; query language that follows the style of discourse to elicit predictable responses; use of recognition outcomes and corresponding system actions to define dialog flow; and generation of scalable solutions.
To evaluate the effectiveness of our approach, we present results from the service-disconnect field trial.
In general, users responded cooperatively, adhered to the structured interaction, rarely anticipated future information, and provided on-target answers to system queries.
We present in this paper a new approach to Pattern Recognition based on the Transferable Belief Model, a non probabilistic interpretation of Dempster and Shafer's theory of belief functions.
This method uses the formalism of belief functions to represent the information provided by the training set concerning the class of a new pattern.
Various decision strategies generalizing Bayes decision theory are presented and demonstrated using a real example.
The Speech Under Simulated and Actual Stress (SUSAS) database is a collection of utterances recorded under conditions of simulated or actual stress, the purpose of which is to allow researchers to study the effects of stress and speaking style on the speech waveform.
The aim of the present investigation was to assess the perceptual validity of the simulated portion of the database by determining the extent to which listeners classify its utterances according to their assigned labels.
Seven listeners performed an eight-alternative, forced-choice response, judging whether monosyllabic or disyllabic words spoken by talkers from three different regional accent classes (Boston, General American, New York) were best classified as, clear, fast, loud, neutral, question, slow, or soft.
Mean percentages of “correct” judgments were analysed using a 3 (regional accent class)×2 (number of syllables)×8 (speaking style) repeated measures analysis of variance.
Results indicate that, overall, listeners correctly classify the utterances only 58% of the time, and that the percentage of correct classifications varies as a function of all three independent variables.
In the present paper the most commonly occurring phoneme sequences in German are investigated with regard to their structural rules.
Knowledge of all possible phoneme sequences and their structural rules is important e.g. for the context-dependent evaluation of acoustic-phonetic cues in a syllable-based automatic speech recognition system.
Since the main coarticulation effects are limited to syllable initial and final region, the presented investigations regard the phonemes with respect to their position within the syllable.
The results take the form of two graphs for the temporal ordering of the manner of articulation, one applying to all 45 IC's and the other to al 160 FC's.
A multi-agent system is composed of numerous entities, called agents, interacting in various ways between them and their common environment.
This technology is applied in many domains like computer vision, robotics, system simulation or electronic commerce.
We consider that problems occuring in signal processing could also be tackled by this technology.
We present first the basic tools available for multi-agent systems designers: models, platforms and methodologies.
Two projects illustrate our purpose: SCALA in the management of aerospace fighter patrol, and goods routing.
We focus then on the adaptation ability of these systems considered as an emergent problem solving question.
We detailed in this field the AMAS (Adaptive Multi-Agent System) theory allowing a MAS design where the global fonction is derived from the cooperative self-organisation of its components.
An example on flood forecast gives implementation information of this theory.
A model for the identification of one- and two-formant steady-state vowels is proposed.
The model consists of a formant-picking algorithm and a classification algorithm.
A signal is represented in the model as a combination of several patterns of three types.
Each pattern is characterized by its position on the tonality axis, and by a “type coefficient”.
The pattern positions are determined by the positions of the spectral peaks detected.
The type coefficients depend on the distances between the detected peaks and the center of gravity of the auditory spectrum.
the output of the model is a “response distribution”; a normalized vector of the similarities of a signal to given classes (phonemes).
An experiment on the identification of two-formant signals with varying amplitude relations between the formants is reported on, and the results are compared to the “response distributions” of the model.
The results of the second experiment on the identification of one-formant signals are used to verify the parameters of the formant-picking algorithm.
A Brain-Computer Interface (BCI) is a new type of human-machine interface that allows direct communication between user and machine by decoding brain activity.
The ERPs as the P300 can be obtained through the odd ball paradigm, where targets are selected by the user.
A new method for reducing the number of sensors that record electroencephalography (EEG) signals is proposed.
Reducing the number of sensors allows reducing the time required for the installation of sensors and therefore increases user's comfort.
The proposed approach is based on a recursive elimination where the cost function is based on the signal to signal plus noise ratio (SSNR), after spatial filtering.
We show that this cost function is more robust and less costly in computing time than other functions based on evaluation of the detection of P300 or targets, thus avoiding a step of classification.
We also propose a decision function to better categorize the importance of a sensor based on the number of desired sensors.
The proposed approach is tested and validated on 20 subjects over several sessions.
Genetic algorithms, genetic programming, evolution strategies, and what is now called evolutionary algorithms, are stochastic optimisation techniques inspired by Darwin's theory.
We present here an overview of these techniques, while stressing on the extreme versatility of the artificial evolution concept.
Their applicative framework is very large and is not limited to pure optimisation.
Artifical evolution implementations are however computationally expensive: an efficient tuning of the components and parameter of these algorithms should be based on a clear comprehension of the evolutionary mechanisms.
Moreover, it is noticeable that the killer-applications of the domain are for the most part based on hybridisation with other optimisation techniques.
As a consequence, evolutionary algorithms are not to be considered in competition but rather in complement to the “classical ” optimisation techniques.
We report a new experimental approach to studying Spoonerisms, whereby subjects are required to deliberately exchange phonemes at specified positions in word pairs presented in list form, with speed and accuracy measurements.
With CVC words, initial phoneme transpositions were performed best, and final phoneme transpositions worst.
The ease with which medial phonemes were transposed depended upon whether or not the medial vowel was a dipthong, and also upon whether or not a Spoonerism exchange would result in a major orthographic change were the purely articulatory response to be actually transcribed.
Thus orthographic variables appear to influence processing even in a task presumably performed at a purely acoustic/articulatory level.
While performance was not affected by the word/nonword status of the response, the articulatory environment of surrounding phonemes did influence performance in exchanging a target medial phoneme.
Finally sinistrals proved better able to produce such Spoonerisms on demand, possibly reflecting their superior ability at mirro reading and writing.
The findings generated new hypotheses for the study of spontaneous Spoonerisms.
This paper describes the attenuation and group-delay distorsion approximation for IIR digital filters.
The developped method is characterized by the use of two successive steps: the attenuation approximation by an iterative algorithm asking the solution of a set of linear equations (very fast), followed by the simultaneous approximation based on the solution of a linear programming problem at each iteration.
The convergence of the two algorithms is guaranteed and their efficiency is proved by solving a classical example.
Various kinds of non-text symbols appear in texts.
The oral expressions of these symbols may vary with their senses.
This paper proposes a three-layer classifier (TLC) which can disambiguate the senses of these symbols effectively.
The layers within TLC are employed in sequence.
The 1st layer is composed of two components: pattern table and decision tree.
If this layer can disambiguate the sense of the target symbol, the disambiguation task stops.
Otherwise the next two layers will be triggered.
According to the algorithm confidence of sense disambiguation, the 3rd layer may exploit an alternative model to enhance the performance.
Experiments show that our approaches can learn well even with only a small amount of data.
The overall accuracies of training and testing sets are 99.8% and 97.5%, respectively.
This paper describes the approach taken at Chalmers University of Technology in building up an integrated multilevel speech database for the purpose of speech research and the development of speech coding techniques.
The material comprises today isolated speech sounds (phones and diphones) as well as short, semantically unrelated sentences and coherent texts.
Data collection is, to start with, restricted to Swedish material and read speech.
Registration of the speech samples was carried out under optimal conditions (sound-insulated, anechoic studio) using digital recording equipment (SONY PCM-F1).
Segmentation, classification and labeling is performed at eight interlacing levels of linguistic (including acoustic, phonetic and prosodic) analysis.
The Discriminative Feature Extraction (DFE) method provides an appropriate formalism for the design of the front-end feature extraction module in pattern classification systems.
In the recent years, this formalism has been successfully applied to different speech recognition problems, like classification of vowels, classification of phonemes or isolated word recognition.
The DFE formalism can be applied to weight the contribution of the components in the feature vector.
This variant of DFE, that we call Discriminative Feature Weighting (DFW), improves the pattern classification systems by enhancing those components more relevant for the discrimination among the different classes.
This paper is dedicated to the application of the DFW formalism to Continuous Speech Recognizers (CSR) based on Hidden Markov Models (HMMs).
Two different types of HMM-based speech recognizers are considered: recognizers based on Discrete-HMMs (DHMMs) (for which the acoustic evaluation is based on an Euclidean distance measure) and Semi-Continuous-HMMs (SCHMMs) (for which the acoustic evaluation is performed making use of a mixture of multivariated Gaussians).
We report how the components can be weighted and how the weights can be discriminatively trained and applied to the speech recognizers.
We present recognition results for several continuous speech recognition tasks.
The experimental results show the utility of DFW for HMM-based continuous speech recognizers.
Traditionnally case-based reasoning is conducted on experiences that are represented in a well structured format such as objects or database records.
However different models have been proposed to overcome some of the limitations imposed by this structural approach and to undertake new applications domains.
In this paper, we review some of the extensions proposed to apply CBR principles to experiences contained in textual documents, commonly refered to as textual CBR.
Following a short presentation of some case-based reasoning principles, we present the main research works in Textual CBR and provide a synthesis of current state of the art in the field.
We finally discuss some shortcomings of the current approaches and propose some directions for future research.
Two experiments investigated the role of syntactic presupposition in sentence comprehension.
In Experiment I subjects verified cleft, pseudocleft and factive complement sentences with respect to preceding context paragraphs, which contradicted either the assertion or the presupposition of the target sentence.
Subjects took significantly longer to verify sentences with false presuppositions than sentences with false assertions.
In Experiment II subjects verified cleft and pseudocleft sentences with respect to subsequently presented pictures.
Once again, verification times for sentences with false presuppositions were significantly longer than verification times for sentences with false assertions.
It was argued that these findings are more adequately explained by a “structural” hypothesis, than in terms of strategies designed to locate given and new information.
Variation in pronunciation observed in speakers today parallels in many details the documented variation in pronunciation over the centuries (sound change).
It is reasonable to conclude that there is some necessary link between the two.
I argue that diachronic variation emerges for the most part from synchronic variation thus: universal and timeless physical constraints on speech production and perception leads listeners to misapprehend the speech signal.
Any such misapprehension that leads the listener to pronounce things in a different way is potentially the beginning of a sound change.
If we study sound change we can gain insights into how speech is produced and perceived.
I exemplify this point by considering a variety of sound changes that involved voiceless fricatives: so-called spontaneous nasalization, s-aspiration, and nasal effacement.
They suggest that one cue to this class of sounds is a special voice quality on that portion of vowels immediately abutting the fricative.
Three lexical decision experiments were concerned with the separability of syntactic and semantic processing in spoken word perception.
An additional experiment examined the problem of measuring reaction times to a spoken stimulus.
Words in the Serbo-Croatian language were used; each stimulus consisted of a noun stem (which was either a meaningful root or a pseudoword stem) plus an inflectional suffix which conveyed information about the noun's grammatical case.
Speed of identifying the inflectionally related forms of a noun was a function of differences in their syntactic meanings rather than differences in their physical forms or their actual frequencies of occurrence.
In addition, identification of a noun was facilitated when it was preceded by a stimulus carrying predictive inflectional information whether that stimulus was a real adjective or pseudoadjective.
The results echo previous findings for word perception in print and provide evidence of essential structural uniformity in the processing of inflection for both spoken and printed words.
For both, there is evidence that inflectional processing is modular, at least to the extent that it is independent from semantic processing for the initial portion of its time course.
Several activities have been undertaken in Italy in the field of Digital Mobile Radio (DMR).
In particular, concerning speech coding two codecs were developed to be compared on experirental Single Carrier Per Channel (SCPC) systems.
These two codecs are described in the paper and their performances are compared.
The main feature of both schemes is the use of Vector Quantization (VQ) to compact the side information.
In addition the SB-APC scheme uses the technique of post-filtering to shape the quantizing noise of each sub-band.
The main result reported in the paper is that the performances of the two schemes are almost equivalent although their structure is very different.
We propose a new glottis signal model.
Most glottal pulse shape models have been obtained so far by concatenating a small number of curved segments.
We propose here an alternative approach which consists of expanding the signal into a combination of a finite set of basic time functions.
These functions are chosen taking into account the point-like and non-linear character of the acoustic voice source.
We establish a mathematical relationship between the weights of the individual time functions and the Fourier coefficients of the signal being modelled.
The control parameters are the frequency and amplitude of a cosinusoidal driving function.
The spectral envelope of the output signal (i.e. the glottal pulses) changes with the fundamental frequency, and its spectral content evolves with the amplitude of the driving function.
The first one performs low level processing and extracts characteristic points.
The second one processes a state space transformation of the input picture, tries to recognize the learning objects and proposes a reconstruction to confirm the recognition
During the training, the robot extracts characteristic points of one object and makes a transformation from each of these points.
During the interpretation, the robot focuses its eye on a characteristic point, processes a complex logarithmic transformation and performs a mental rotation to match the present transformation with the learned representation.
To complete its interpretation or to remove ambiguity, the robot focuses on other characteristic points used during learning.
Objects can be recognized in real scene even if they are partially occluded or if the picture is noisy.
The performance of the existing speech recognition systems degrades rapidly in the presence of background noise.
A novel representation of the speech signal, which is based on Linear Prediction of the One-Sided Autocorrelation sequence (OSALPC), has shown to be attractive for noisy speech recognition because of both its high recognition performance with respect to the conventional LPC in severe conditions of additive white noise and its computational simplicity.
The aim of this work is twofold: (1) to show that OSALPC also achieves a good performance in a case of real noisy speech (in a car environment), and (2) to explore its combination with several robust similarity measuring techniques, showing that its performance improves by using cepstral liftering, dynamic features and multilabeling.
The paper addresses the problem of the temporal integration of information in vowel perception and the nature of this information.
One-formant stimuli with, two-formant F1, F2 stimuli and stimuli with a jumping formant produced as trains of alternating F1 - and F2 - pulses occurring on different proportions were used in vowel identification experiments.
The response distributions corresponding to two-formant stimuli and to stimuli with a jumping formant were approximated as weighted sums of two 'basic' distribution. 1 and 2, corresponding to one-formant stimuli.
An increase in the proportion of F2-pulses in stimuli with a jumping formant was accompanied by a systematic increase in the 2 component in the response distribution.
Stimuli with a jumping formant did not elicit the phonemic responses typical for two-formant stimuli with the same F1 and F2 frequencies.
The data suggest that the running identification of the stimulus is integrated temporally.
This paper describes an attempt at capturing segmental transition information for speech recognition tasks.
The slowly varying dynamics of spectral trajectories carries much discriminant information that is very crudely modelled by traditional approaches such as HMMs.
In approaches such as recurrent neural networks there is the hope, but not the convincing demonstration, that such transitional information could be captured.
The method presented here starts from the very different position of explicitly capturing the trajectory of short time spectral parameter vectors on a subspace in which the temporal sequence information is preserved.
This was approached by introducing a temporal constraint into the well known technique of Principal Component Analysis (PCA).
On this subspace, an attempt of parametric modelling the trajectory was made, and a distance metric was computed to perform classification of diphones.
Using the Principal Curves method of Hastie and Stuetzle and the Generative Topographic map (GTM) technique of Bishop, Svensen and Williams as description of the temporal evolution in terms of latent variables was performed.
On the difficult problem of /bee/, /dee/, /gee/ it was possible to retain discriminatory information with a small number of parameters.
Experimental illustrations present results on ISOLET and TIMIT database.
A common problem in the field of speech synthesis is the lack of a quantitative description of speech sounds applicable to both vowels and consonants.
Usually only vowels are specified in terms of formant frequencies.
This paper proposes a parametric description of the steady-state zones of all non-plosive phones based on centroids in the log area ratio parameter space.
For each phone, a centroid is computed as the center of gravity over an ensemble of realizations produced by a single speaker in different contextual situations.
The centroids are quantitatively compared to one another by means of an objective distance measure.
In addition some information relating to the dynamic behaviour of the speech signal is obtained by signal destruction and construction experiments.
All these data then serve to discuss several phonoacoustic problems encountered in the delimitation of the German phones, with respect to vowel quality and quantity, for example.
This paper proposes Strada, a novel learning approach to the automatic design of adaptive strategies for modern strategy games.
Strada combines new ideas with state-of-the-art techniques from several areas of machine learning.
Solutions to these issues are combined into an efficient learning system, whose performance is demonstrated on a commercial war-game.
Speech analysis and synthesis methods developed at ECL in NTT are described.
Starting from the LPC method the PARCOR and LSP schemes based on all pole filter are explained.
The principles and physical interpretations of these methods are presented in comparison with each other.
The characteristics of the feature parameters in these methods are clarified by means of several experiments.
Synthesized speech quality is also shown by objective and subjective experiments.
High quality synthesized speech can be achieved by these methods at a low bit rate transmission under 9600 bps.
In this article, we propose a method of characterization of images of old documents based on a texture approach.
This characterization is carried out with the help of a multi-resolution study of the textures contained in the images of the document.
Thus, by extracting five features linked to the frequencies and to the orientations in the different areas of a page, it is possible to extract and compare elements of high semantic level without expressing any hypothesis about the physical or logical structure of the analysed documents.
Experimentations demonstrate the performance of our propositions and the advances that they represent in terms of characterization of content of a deeply heterogeneous corpus.
An important procedure in many prosodic analysis systems is locating syllables.
The location of syllables is used in the identification of stress and of pitch accents, which in turn form the basis for the analysis of rhythm and intonation.
This paper presents a novel syllabification system utilising recurrent neural networks which operates on speaker-independent continuous speech.
It is trained and tested on dialect region 1 of the TIMIT database, and finds 94% of syllables and places most syllable boundary points within 20 msec of the desired location.
Methods for optimising the performance and training of the recurrent neural networks are investigated.
This paper describes the real time implementation of the Klatt speech formant synthesizer (cascade and parallel) on the SGS-Thomson DSP Processor.
The determination of optimal functional conditions of the synthesis algorithm is performed by means of the floating point simulation.
Fixed point simulation is realized to estimate the world length and to define the adequate scale of the used variables.
The realization of these procedures allows the programming, in assembly language, of the algorithm on the DSP.
This is a first step towards a realization of a PC-compatible speech synthesis board based on the DSP processor.
In this paper we deal with the statistical grey-level segmentation, without any reference to texture.
Adding a previous model parameter estimation step, which is a mixture estimation, all these methods can be rendered automated, or unsupervised.
On the one hand, results obtained with unsupervised methods differ little from results obtained with true parameter based methods.
Although global methods can give excellent results when data are well suited to the underlying model, in other situations local methods can ensure clearly better performance.
We deduce the choice from two factors: class image homogeneity and spatial correlation of the noise.
The good behaviour of our algorithm is validated with simulations and real-world image segmentation results.
It is argued that theories of semantic memory have diverged in a manner that parallels current linguistic controversy concerning the representation of meaning.
The feature-comparison model (Smith, Shoben & Rips, 1974) applies the linguistic theory of Lakoff (1972) to predict people's reaction times to verify sentences, while the marker-search model, described here, uses the type of semantic representation outlined by Katz (1972) to explain a similar range of data.
The two models are described and the evidence for each is reviewed.
Available evidence supports the marker-search model, but disconfirms a major prediction of the feature-comparison model.
It is argued that the feature-comparison model is in principle inadequate as a model of semantic representation, unless its conception of semantic components is substantially alatered.
In recent years, the sustainable management of agricultural and ecological systems has become a major challenge.
Sustainable management has to solve crucial environmental problems linked, in part, to rapid changes in context: climatic changes, agricultural policy objectives changes, etc.
Solving this challenge involves the joint development of researches in modelling, simulation and virtual experimentation.
In this article, we present some recent work devoted to the modelling and simulation of complex systems involved in agro-ecosystem management.
Then, we present new formalisms for management strategies design, based on the Weighted Constraint Satisfaction Problems or the Markov Decision Processes frameworks.
We also how how simulation and conception of strategies can be integrated.
Finally, we illustrate the use of the presented approaches on several case studies in Agroecosystems management, jointly tackled with research teams in Agronomy.
Piaget's theory of space perception in infancy is presented in the format of a hypothetico-deductive system.
Eleven hypotheses are defined, regarding the perception of the agent of visual change; shape and size constancy; depth; and the perception of higher-order relationships among spatial elements.
The proof Piaget offers for each hypothesis is presented in the following steps: behavioral evidence, interpretation in terms of inner states; inferences and generalizations.
Some general conclusions are briefly discussed.
The paper investigates the acoustic measures of pitch perturbations and vocal noise in pathological voice signals in terms of their ability to discriminate between normal and pathological voice status.
A brief review of the related work done by other researchers in Japan is also given.
This chapter proposes to the discussion a set of principles for a design-oriented analysis of technico-organisational systems as dynamic, living, social and cultural systems, through eleven questions.
After developing an ontological notion of complexity adequate to technico-organisational systems, these questions deal with the different theoretical, epistemological and methodological (data collecting, analysis and modelling) aspects of the knowledge of this complexity and their relation to design.
The discussion appeals to many different disciplinary contributions, in empirical science, ergonomics and philosophy, and relates to ideas developed by other authors in other chapters of this book.
Grammatically incorrect sentences (paragrammatisms) are characteristic of the spontaneous speech of fluent aphasics.
The paragrammatisms produced by five neologistic jargon aphasics are analysed and compared to the paragrammatisms of four normal control subjects.
We show that the paragrammatisms of the aphasics are qualitatively identical to the grammatical errors of normal subjects, although they are much more frequent.
The reason for this is discussed in terms of models of speech production; we argue that paragrammatisms are a consequence of a breakdown in the control processes.
This paper is an analytical study of ten new Thamudic inscriptions written in the so-called Thamudic E script, collected by the authors during a survey in the region of al-Jafr (southeast Jordan).
The study aims at analyzing the inscriptions, meanings and structure of the words and the proper nouns contained therein.
This group of inscriptions emphasizes some new personal names mentioned in Thamudic inscriptions.
A multilayer perceptron has been trained to perform an analogue mapping from the power spectra of vowels and nasal consonants, spoken by a single speaker, to the control parameters of a speech synthesiser based on an acoustic tube model.
The outputs of the neural network control these eleven areas, while its inputs are samples of the power spectrum which the synthesised speech spectrum is intended to copy.
After training, natural speech, with this restricted phoneme set and by the same speaker, can be synthesised with good intelligibility.
A reduced complexity multipulse coder is proposed in this paper.
A 33% complexity reduction affects the overall speech output quality only slightly as is demonstrated in both informal listening and SNR computation.
On the other hand, it makes possible the implementation of the coder with cheap commercial signal processing chips.
The TMS32020 has been used for real time implementation of the proposed algorithm at 16 kbit/s with only fixed point computations.
The method studied here can be considered as an alternative to the regular pulse excitation technique.
We consider the design of a bot for the Tetris game.
After a review of the literature, we emphasize the fact that comparing the performances must be done with great care, as the game scores have a huge standard deviation, and as subtle implementation details have a significant effect on the resulting performance.
We then consider the cross-entropy method to tune a ratingbased one-piece bot as suggested by Szita et al. (2006).
In this context, we discuss the influence of the noise, and we make experiments with several sets offeatures such as those introduced by Bertsekas et al. (1996), by Dellacherie (Fahey, 2003) and some original ones.
This approach leads to a bot that outperforms the previous known results.
On a simplified version of Tetris considered by most research works, it achieves 35,000,000 ±20% lines per game on average.
In this paper, we propose an original approach for sequential non-parametric density estimation defined in high-dimensional state spaces using the particle filtering framework.
By exploiting conditional independences in the state space, we propose to swap independent sub-particle sets to generate new sets that better sample this space.
We integrate this approach into two versions of particle filter, i.e., partition sampling and annealed particle filter, to prove its efficiency.
We compare it to classical approaches on synthetic articulated object density estimation problems, and show that our approach reduces both estimation errors and computation times.
It is shown that the maximum of entropy rate corresponds to the maximum of prediction error.
Using the reflection coefficients, the equivalence with the autoregressive method is directly established.
An interpretation in terms of whitening is given and the minimum of entropy rate is also discussed.
In spoken document retrieval (SDR), speech recognition is applied to a collection to obtain either words or subword units, such as phonemes, that can be matched against queries.
We have explored retrieval based on phoneme n-grams.
The use of phonemes addresses the out-of-vocabulary (OOV) problem, while use of n-grams allows approximate matching on inaccurate phoneme transcriptions.
Our experiments explored the utility of word boundary information, stopword elimination, query expansion, varying the length of phoneme sequences to be matched and various combinations of n-grams of different lengths.
Our experiments show that there is some deterioration in effectiveness, but the particular form of matching is less vital if the sequence of phonemes was correct.
When phone sequences are recognised directly, with higher error rates than for words, it was more important to select a good matching approach.
Varying gram length trades precision against recall; combination of n-grams of different lengths, in particular 3-grams and 4-grams, can improve retrieval.
The manuscript BnF, fr. 6449 constitutes the unique known copy of Jean Miélot's Vie de sainte Katherine (1457), a complete biography of this Saint, native of the Egyptian town of Alexandria, which the Burgundian canon set into the context of the Roman history of the fourth century.
This article highlights the interest of this work through a presentation of the manuscript, an investigation surrounding the Latin sources of the text, as well as an examination of the language and translation techniques by Miélot ; it provides so a preliminary study to the critical edition of the Vie.
This article presents an overview of the POLYCOST database dedicated to speaker recognition applications over the telephone network.
The main characteristics of this database are: medium mixed speech corpus size (>100 speakers), English spoken by foreigners, mainly digits with some free speech, collected through international telephone lines, and minimum of nine sessions for 85% of the speakers.
This paper illustrates the importance of various cognitive factors involved in perceiving and comprehending synthetic speech.
However, and more importantly, this difficulty can and does decrease with the subjects' exposure to said synthetic voices.
Furthermore, greater workload demands are associated with synthetic speech and subjects listening to synthetic passages are required to pay more attention than those listening to natural passages.
Face recognition is the process of the automatic recognition of the person's identity based on individual informations that are included in face image.
This document demonstrates how a face recognition system can be designed by a conventional artificial neural network and by another more recent neural network, which is called Spike neural network.
The latter is developed to capture the important characteristics of the face, to simulate the human visual system and to optimise the computational time, this last characteristic has been one of the driving forces behind the development of spike neural networks.
Note that the training process of the two networks, on different sets of noisy images, forced the two networks to learn how to deal.
This paper describes the use of a multiple codebook semi-continuous hidden Markov model (SCHMM) automatic speaker verification (ASV) system, which uses a novel technique for discriminative hidden Markov modelling known as discriminative observation probabilities (DOP).
DOP is not a discriminative training technique, it is a method of constructing what is effectively a discriminating model by contrasting two standard HMMs so as to improve discrimination between the classes that those models represent.
This paper experimentally evaluates the use of DOP HMMs for ASV.
The experimental evaluation is based on a text-dependent task using isolated digits.
The database contains 24 true (client) speakers and 100 casual impostors, recorded over the public telephone network in the United Kingdom.
In this study we present approaches to multilingual speech recognition.
We first define different approaches, namely portation, cross-lingual and simultaneous multilingual speech recognition.
We will show some experiments performed in the fields of multilingual speech recognition.
In recent years we have ported our recognizer to other languages than German (Italian, Slovak, Slovenian, Czech, English, Japanese).
We found that some languages achieve a higher recognition performance with comparable tasks, and are thus easier for automatic speech recognition than others.
Furthermore, we present experiments which show the performance of cross-lingual speech recognition of an untrained language with a recognizer trained with other languages.
The substitution of phones is important for cross-lingual and simultaneous multilingual recognition.
We compared results in cross-lingual recognition for different baseline systems and found that the number of shared acoustic units is very important for the performance.
With simultaneous multilingual recognition, performance usually decreases compared to monolingual recognition.
In few cases, like in the case of non-native speech, however, the recognition can be improved.
In our work we attempted to answer the following question: How do listeners identify two sentences as spoken by one or by two persons when they can take into consideration only acoustic speaker characteristics and intonation?
We used successive utterance portions forming a single turn or two subsequent turns in a dialogue, the lexical structure of which was held constant.
The experiments are based on sentence combinations varying with respect to speakers, sentence types, and types of communicative coherence.
Pairs of sentences with and without constant intervening pauses were presented to subjects with the task of ascribing them to one of three categories: “dialogue”, “monologue” and “metadialogue” (i.e., the imitation of dialogue by a single voice).
The results showed that the following factors influence the decision of the listeners: voice properties, presence or absence of pause, and the communicative types of the sentences.
Two principles involved in processing the input signals are supposed.
If the sentences are separated by a pause, a comparison of the auditory voice properties of the two portions may take place.
We present in this paper a new bio-inspired algorithm that dynamically creates and visualizes groups of data.
This algorithm uses the concepts of flying insects that move together in complex manner with simple local rules.
Each insect represents one data.
The insect moves aim at creating homogeneous groups of data that evolve together in a 2D environment.
These created groups are visualized in real time and help the domain expert to understand the underlying class structure of the data set, like for instance a realistic number of classes, clusters of similar data, isolated data.
We present several extensions of this algorithm like reducing its computational time and the use of a 3D display.
This algorithm has been tested on artificial and real-world data.
A heuristic algorithm can be used to evaluate the relevance of the obtained classification.
An improved, phoneme-based IWSR system is described, which employs a robust reference data extraction procedure and achieves increased recognition accuracy.
Furthermore, a novel method for the adaptation of the IWSR-system to continuous speech is presented.
The IWSR system employs a multisection codebook design technique and the LVQ algorithm, which provide well-defined and accurate codebooks, minimize the influence of the within-word coarticulation and allow the use of time-sequence information at the recognition stage.
The adaptation method is based on modifications of the system's reference data codebook using a small amount of representative continuous speech data on linear transformations of the main prosodic parameters (i.e. energy and duration).
Extensive testing under different conditions (speaker dependent versus speaker independent reference data, single versus multisection codebooks, adapted versus unadapted codebooks, phoneme versus word recognition accuracy, etc.) has shown the efficiency of the proposed methods.
This article is about the two earliest sources of the Cent Nouvelles Nouvelles: the manuscript of Glasgow (University Library, Hunter 252), and the edition by Vérard (Paris, BnF, Rés. Y2-174).
A summary of philological problems concerning the transmission of the text is followed by a contrastive analysis of the iconographic programs in the two volumes (illuminations and woodcuts), in particular for nouvelles 9, 12, 27, 33, 46.
Chomsky and Halle (1968) claim that the stress and intonation of an utterance are not determined solely by physical properties of the acoustic signal but are also influenced by the syntactic organization of the utterance.
Strong support for their contention has been obtained by presenting listeners with a continuously repeated string of monosyllabic words.
spicos II enables an on-going dialogue to take place with an office data-bank.
This represents a further development of spicos I that, linguistically, was confined to a simple question-answer scheme.
spicos II is speaker-adaptable and incorporates a vocabulary of some 1,200 words.
The present article gives an overview of the elements connected with linguistic analysis.
These elements are governed by a dialogue module that controls the dialogue and, by means of appropriate follow-up questions, avoids possible communications difficulties.
The syntactic analysis is based on an augmented phrase-structure grammar.
At the same time, semantic constraints are checked by means of a semantic network.
Syntactic structure, together with semantic features and the results of the resolution of anaphoric bindings and a separate formal representation of discourse referents, is used to build formal-logic semantic representations of utterances.
User presuppositions are also represented.
From these formal representations date-bank queries are generated.
In this study coarticulatory effects on the formant frequencies and on the duration of the Dutch schwa were investigated, both in open syllables and in closed syllables, by using nonsense words of the form C1əC2V and VC1əC2.
In these nonsense words C1 and C2 could be any of the consonants /p, t, k, f, s, χ, m, n, η, r, l, j, ν/ and V was taken from the vowel set /i, a:, u/.
Consonants and vowels were systematically varied in all possible combinations which gave a total of 897 test words that were read aloud by three male speakers.
It appeared that the coarticulatory effects on the schwa could be successfully described with a simple linear model.
Especially for F 2-tracks of schwas, the model fit turned out to be very good.
The model for F 2-tracks could also be successfully applied to schwas in meaningful words.
We believe that the schwa should be interpreted as a vowel without articulatory target that is completely assimilated with its phonemic context.
The widespread view of a schwa position in the centre of the vowel triangle, that the formant patterns of reduced vowels are shifting to, is not very accurate.
In our interpretation vowel reduction results in a shift of formant frequencies to a schwa position that can be almost anywhere in the vowel plane, dependent on the phonemic context.
This paper deals with real time face detection and tracking by a video camera.
The method is based on a simple and fast initializing stage for learning.
The transferable belief model is used to deal with the prior model incompleteness due to the lack of exhaustiveness of the learning stage.
The algorithm works in two steps.
To deal with the colour information dependence in the fusion process, we propose a compromise operator close to the Denœux cautious rule.
As regards the tracking phase, the pignistic probabilities from the face model guarantee the compatibility between the believes and the probability formalism.
They are the inputs of a particle filter which ensures face tracking at video rate.
The optimal parameter tuning of the evidential model is discussed.
A recently proposed pitch detection algorithm (Dologlou and Carayannis, Speech Communication, Vol. 8, 1989) used iterative low-pass, zero-phase filtering followed by pulse-to-pulse measurements on the filtered signal.
The iterative low-pass filtering was terminated when the signal had a sufficiently sinusoidal appearance.
The halting criterion used was that frequencies derived from an autocorrelation analysis and a second order LPC analysis should be sufficiently close to each other.
A claim is made by the authors that unless the input was a pure sinusoid, the two frequencies would never be equal.
We discuss the proposed halting criterion and give an example with a speech-like signal consisting of two sinusoids of comparable amplitudes; this example violates the claim concerning the uniqueness of the two frequencies and where the proposed algorithm would halt prematurely.
Errors were analyzed with reference to: (i) general structural quantitative parameters (numeral and number lengths…); (ii) general behavioral disturbances (perseveration, serial ordering disorders…); (iii) specific cognitive processes implied in this particular transcoding task.
The third approach proved the most powerful; it indicated that systematic errors were produced resulting from the partial and/or inappropriate use of transcoding strategies such as the transcription of each numeral element into its lexical value or the systematic coding of 'MILLE' (thousand) and 'CENT' (hundred) into one digit, irrespective of their particular lexical/ multiplicand role in the syntactical structure underlying the word order in numerals.
The results are relevant to the study of cognitive processes in normal subjects.
An algorithm which estimates both the input excitation and the parameters of a time-varying autoregressive moving-average (ARMA) speech production model is proposed.
Here, the AR and MA coefficients are estimated by two independent recursive least squares (RLS) lattice joint process estimators, respectively, and the input excitation is estimated by a bootstrap from one of the two estimators.
From the experimental spectral envelope estimations for the natural speech signals, it is shown that the proposed algorithm yields accurate parameter estimation of a slowly time-varying ARMA speech model.
This paper provides a new interpretation of the so-called “delta-cepstrum” and extends the formulation of the conventional delta-cepstrum towards an optimal design of the filter, which extracts important spectral dynamics from a cepstrum sequence.
The algorithm to obtain new feature parameters is unified to a formulation using a matrix coefficient filter and is tested through Japanese speech recognition experiments.
The average recognition error rate in a Japanese 24 phoneme recognition experiment for four speakers was reduced from 12.2% to 10.3%.
Control of unregulated animal diseases depends on farmers but is often encouraged by professional organizations to enhance the health status or the economy in the area.
The challenge is to be able to offer tools for making decisions and assessing a priori the impact ofproposed decisions on the spread of a pathogen in epidemiological (prevalence) and in economic terms.
In this paper, we evaluate the contribution of Markov Decision Processes (MDP):
We propose a model of between-herds spread when a control action is prompted by a collective decision maker to optimize the cost of the disease and of its control at the group level.
We suppose the efficiency of the advice is known.
Then, an epidemiological MDP model can suggest at each time whether advice should be given or not.
Resulting strategy is non-systematic.
Although the goal was to decrease costs, we observe the prevalence decreases also.
Here, an adaptive strategy is an advantage of our approach because these strategies are often not studied.
Speech is normally heard against a background of other sounds.
This paper reviews recent work on listeners' ability to separate speech from other sounds.
Evidence is presented that both low-level grouping mechanisms and knowledge specific to speech are deployed in solving this diffucult problem.
With the rise of significant speech recognition and text-to-speech applications, the activity of our lab encompasses now a broader set of activities, from new algorithmic approaches to speech product engineering and application development.
In particular, the paper gives an overview of the products originated from our speech technology research.
This paper presents an approach based on the properties of group delay functions for extracting formants from speech signals.
The algorithm is similar to the cepstral smoothing approach for formant extraction using homomorphic deconvolution.
The significant differences are (i) the logarithmic operation is replaced by () r operation and (ii) the additive and high resolution properties of group delay function are expolited to emphasize formant peaks.
The group delay function (or the negative derivative of the Fourier transform phase) is derived for a signal which in turn is derived from the Fourier transform magnitude of the speech signal.
If a suitable value of r is used, this method gives highly consistent estimates of formants compared to both the cepstral approach and the model-based linear prediction (LP) approach for smoothing the magnitude spectrum.
The effects of the parameters, exponent r and window width p, on the proposed technique for formant extraction are studied.
This paper deals with the estimation of a speech signal disturbed by acoustical additive noises when two noisy observations are available.
The useful signals received on two microphones are issued from the same speech signal and noises are assumed to be decorrelated.
Our concern is the improvement due to the second channel in comparison with the estimation obtained using only one channel.
The vectorial Wiener filtering and two structures, called PIS (Preprocessing + Signal Identification), are presented.
A theoretical study based on optimal filters is first presented.
The different systems as well as the mono-channel Wiener filtering are compared in terms of distortion, residual noise and global error.
The interest of the proposed structures is proved for low input signal to noise ratios on the first channel.
The four algorithms — mono-channel and two-channel Wiener filterings and the two structures PIS – are tested on real noisy signals recorded in a moving car.
They are compared using objective and subjective measures.
Results confirm the improvement brought by our methods on real signals.
A novel speech recognizer is described which capitalizes on multi-dimensional articulatory structures and incorporates key ideas from autosegmental phonology and articulatory phonology.
The novelty has been in the design of the atomic units of speech so as to arrive at a unified and parsimonious way to account for the context-dependent behavior of speech acoustics.
At the heart of the recognizer is a procedure developed to automatically convert a probabilistic overlap pattern over five articulatory feature dimensions into a finite-state automaton which serves as the phonological construct of the recognizer.
The phonetic-interface component of the recognizer, based on the nonstationary-state hidden Markov model or the trended HMM, is also described.
Some phonetic recognition results using the TIMIT database are reported.
Utterance verification (UV) is a process by which the output of a speech recognizer is verified to determine if the input speech actually includes the recognized keyword(s).
The output of the speech verifier is a binary decision to accept or reject the recognized utterance based on a UV confidence score.
In this paper, we extend the notion of utterance verification by presenting an utterance verification method that will be utilized to perform three tasks: (1) detect non-keyword strings (false alarms), (2) detect keyword substitution errors, and (3) selectively correct substitution errors when N-best string hypotheses are available.
The utterance verification method presented here employs a set of verification-specific models that are independent of the models used in the recognition process.
The verification models are trained using a discriminative training procedure that seeks to minimize the verification error by simultaneously maximizing the rejection of non-keywords and misrecognized keywords while minimizing the rejection of correctly recognized keywords.
The error correction is performed by reordering the hypotheses produced by an N-best recognizer based on a UV confidence score.
In order to detect laryngeal pathologies by acoustic analysis of the speech wave, several acoustic features have been proposed in literature.
The evaluation of their individual performance in discriminating between normal and pathological speakers has been done either qualitatively by inspection or by classical statistical analysis.
It seems that the evaluation of a whole set of acoustic features has never been done without explicit reference to a decision model.
On the other hand there are strong indications that the distributions underlying the features space are multimodal.
In order to evaluate quantitatively their discrimination power without reference to a statistical model, we performed a clustering analysis on a set of six well-known acoustic features.
The features were computed from the acoustic signal of the steady vowel /a/ uttered by 37 normal speakers and by 24 dysphonic speakers.
The results confirm the good performance of the pitch and amplitude perturbation measures reported elseewhere.
On the contrary, the acoustic features solely defined on the residue signal show poor discrimination ability.
In this paper, we propose an MLP/HMM hybrid model in which the input feature vectors are transformed by nonlinear predictors using multilayer perceptrons (MLPs) assigned to each state of a Hidden Markov Model (HMM).
The prediction error vectors in the states are modeled by Gaussian mixture densities.
The use of a hybrid model is motivated from the need to model the prediction errors in the conventional neural prediction model (NPM) where the prediction errors are variable due to the effect of varying contexts and speaker identity.
The MLP/HMM hybrid model is advantageous because frame-correlation in the input speech signal is exploited by employing the MLP predictors, and the variabilities in the prediction error signals are explicitly modeled.
We present the training algorithms based on the maximum likelihood (ML) criterion and discriminative criterion for minimum error classification.
Experiments were done on speaker-independent continuous speech recognition.
By ML training of the hybrid model, we obtained a much better performance than a conventional NPM which does not explicitly model the prediction error signals.
By training with the discriminative criterion, confusion among different models was significantly reduced and word error rate was reduced by 56% compared with the ML training.
In this paper, methods are proposed to detect objects in complex scenes using statistical global appearance based models.
In our approach, the standard eigenspace representation of a training image database and a priori non- Gaussian hypotheses are brought together in a Bayesian framework.
This work unifies standard (appearance- based) detection methods already proposed in the literature and leads naturally to the definition of a new family of probabilistic detectors.
It allows the use of more general a priori assumptions about the distribution on the eigenspace and its orthogonal.
Experimental results are illustrated with ROC (Receiver Operating Characteristic) curves and show the major improvement of our Bayesian approach in comparison to the standard methods that have been the reference up to now [2, 14].
A paradigm for automatic speech recognition using networks of actions performing variable depth analysis is presented.
The paradigm produces descriptions of speech properties that are related to speech units through Markov models representing system performance.
Results in the speaker-independent recognition of isolated letters and digits are presented.
This paper presents a work on a comparison between a user model and user's behavior based on three premises.
First, any system includes a representation of its users.
Second, the external representation of users in a system is related to how the system is used by users.
Third, knowing how to use the system depends on the task context.
For making context explicit in order to use it, we use contextual graphs to capture the effective behaviors of users in an activity of information retrieval on a scientific website.
This approach allows dealing with a system that is able to incrementally acquire new knowledge from the user and learn new practices when the system is in a situation of failure.
As is widely known, most people who suffer from a defect in their auditory system perceive a valuable amount of speech information by lip-reading.
This entire paper presents some results of a structural analysis of lip contours for different speakers during the articulation of different isolated vowels.
The method used is based on the Fourier analysis of contourfunctions.
Such an analysis is of great interest with respect to the artificial generation and real-time recognition of visual speech patterns.
Psychological theories of natural language processing have usually assumed that the sentence processor resolves local syntactic ambiguities by selecting a single analysis on the basis of structural criteria such as Frazier's (1978) “minimal attachment.”
According to such theories, alternative analyses will only be attempted if the initial analysis subsequently proves inconsistent with the context. (See also Ferreira & Clifton, 1986; Ford, Bresnan, & Kaplan, 1982; Rayner, Carlson, & Frazier, 1983).
An alternative hypothesis exists, however: If sentences are understood incrementally, more or less word-by-word (Marlsen-Wilson, 1973, 1975), then syntactic processing can in principle exploit the fact that interpretations are available, using them “interactively” to select among alternative syntactic analyses on the basis of their plausibility with respect to the context.
The present paper considers possible architectures for such incremental and interactive sentence processors, and argues for an architecture in which alternative analyses are initially offered in parallel, and are then discriminated among by immediate appeal to the comprehension process under a selective or "weak" interaction, as opposed to directive or "strong" interaction.
We note that such an architecture does not compromise the modularity hypothesis of Fodor (1983) in any way.
We review experimental evidence which has been claimed to show that human sentence processing is non-interactive and mediated by purely structural criteria.
New results are presented which appear incompatible with the structuralist proposal, and which support the interactive hypothesis.
We suggest reasons why the earlier contrary results may be discounted, and conclude that the human sentence processing mechanism resolves modifier-attachment ambiguities by recourse to higher-level contextual and referential information under the weak interaction.
This paper first describes recent trends of ASR and TTS telecommunications applications in Japan.
ASR applications focus on public services such as operator automation, operator assistance, voice-activated information retrieval, and voice dialing.
Major TTS applications include information service by voice and e-mail reading.
The usage of ASR and TTS functions is expected to dramatically increase in the near future with the penetration of handy and mobile telephone terminals; hot topics are text broadcasting and digital communication.
Secondly this paper describes NTT's experimental interactive system featuring (1) highly accurate speaker independent and large vocabulary speech recognition based on context-dependent accurate acoustic phoneme HMM models trained with speech data from more than 10,000 speakers collected over telephone network, (2) high quality text-to-speech synthesis that generates speech by concatenating triphone-context-dependent waveform segments, (3) software-based configuration that requires no special hardware except a PC equipped with a sound board and a voice modem, and (4) easy and rapid prototyping which enables the developer to build a system by writing some types of service scenarios.
In this paper, a general class of “single-hidden layered, feedforward” Artificial Neural Network (ANN) based adaptive non-linear filters is proposed for processing band-limited signals in a multi-microphone sub-band adaptive speech-enhancement scheme.
Initial comparative results achieved in simulation experiments using both simulated and real automobile reverberant data demonstrate that the proposed speech-enhancement system employing ANN-based sub-band processing is capable of outperforming conventional noise cancellation schemes.
This paper is devoted to the problem of the selection of those speech signal features which are relevant for automatic speech recognition.
A comparison of the features was performed using a non parametric (template matching) model of an isolated word recognition system.
The experiments were based on Serbo-Croat digits spoken by 109 speakers.
The recognition accuracy of each of the speech signal features was established.
The structure of recognition errors and their causes was also studied in relation to the test vocabulary.
Planning under uncertainty techniques are hard to apply to autonomous robotics problems: they require a tedious modeling effort when dealing with large state spaces.
Factored representations, using state variables, are more compact, but they cannot cope very efficiently with variables that can take a large number of values, e.g. the robot localization variables.
Search and rescue problems combine these with mission variables.
We propose a generic hierarchical abstract model in order to simplify the modeling stage for robotics planning problems with action uncertainties.
An algorithm automatically instanciates our abstract model, which is shown and assessed on several instances of search and rescue autonomous rotorcraft problems.
The primary goal of this paper is to propose a new learner for boosting algorithms, namely least general generalization.
First experiments conducted on benchmarks show that ADABOOST boosting least general generalization obtains smaller error than reference systems.
This paper examines the psychological reality of feature representations.
Given the non-cognitive linguistic and the low-level phonetic approaches, it is a priori an open question whether the psycholinguistic representation borrows from both, from either or from neither of these.
As a test case, a point of divergence between phoneticians and linguistics has selected for scrutiny.
While the former consider [f] and [v] to be labiodental fricatives, the latter regard them as underlying bilabials or simply labials.
To determine if [f] and [v] behave psycholinguistically like bilabial or labiodental segments, the interactions between the fricatives at issue and the other phonemes are analyzed in two large corpora of slips of the tongue in English and German.
The results indicate that [f] and [v] are indistinguishable from bilabials, that is, they are as likely to substitute for other bilabials and as unlikely to replace non-bilabials in the way that [p], [m] or [w] do.
Three hypotheses are put forward in an attempt to come to grips with this finding.
According to the bilabialness-only and the labialness-only hypotheses there is no node for labiodentalness in the processing network.
The rival account starts from the assumption that features are represented as vectors in a space coordinate system.
In this concept, there is a vector for bilabialness as well as one for labiodentalness.
It is claimed that these vectors are closer to each other than their actual phonetic distance would imply.
There is less empirical support for the vector hypothesis than for the bilabialness-only one.
It is concluded that phonetic and psycholinguistic feature representations need not match.
Dynamic Time Warping (DTW) and Vector Quantisation (VQ) techniques have been applied with considerable success to speaker verification.
It is standard practice to use these techniques to calculate a single distance score, and threshold this value to produce a verification decision.
In this paper we examine applying a statistical weighting to a number of parameters extracted using the DTW warp path and VQ decision mechanisms.
Results are presented which show that the additional parameters extracted encode further speaker specific information, and can be used to improve upon the speaker verification performance of the baseline systems.
The application of a distance normalisation technique, which involves comparing DTW or VQ scores for the claimed identity against other speakers, is also investigated.
Speaker verification results for baseline and enhanced DTW and VQ systems are reported for a population of 42 speakers.
The problem of speech modeling for generating stressed speech using a source generator framework is addressed in this paper.
In general, stress in this context refers to emotional or task induced speaking conditions.
Throughout this particular study, the focus will be limited to speech under angry, loud and Lombard effect (i.e., speech produced in noise) speaking conditions.
Source generator theory was originally developed for equalization of speech under stress for robust recognition (Hansen, 1993, 1994).
It was later used for simulated stressed training token generation for improved recognition (Bou-Ghazale, 1993; Bou-Ghazale and Hansen, 1994).
The objective here is to generate stressed perturbed speech from neutral speech using a source generator framework previously employed for stressed speech recognition.
The approach is based on (i) developing a mathematical model that provides a means for representing the change in speech production under stressed conditions for perturbation, and (ii) employing this framework in an isolated word speech processing system to produce emotional/stressed perturbed speech from neutral speech.
A stress perturbation algorithm is formulated based on a CELP (code-excited linear prediction) speech synthesis structure.
The algorithm is evaluated using four different speech feature perturbation sets.
The stressed speech parameter evaluations from this study revealed that pitch is capable of reflecting the emotional state of the speaker, while formant information alone is not as good a correlate of stress.
However, the combination of formant location, pitch and gain information proved to be the most reliable indicator of emotional stress under a CELP speech model.
Results from formal listener evaluations of the generated stressed speech show successful classification rates of 87% for angry speech, 75% for Lombard effect speech and 92% for loud speech.
The problem of quantizer design for detection or classification has a long history, with classical contributions by Kassam, Poor, Picinbono, Bucklew and others.
The goal was to design a quantizer such that a detection rule based on the quantized information was optimized.
During recent years an alternative approach has been developed which seeks to jointly optimize quantization and classification by incorporating the Bayes risk resulting from the quantizer into the quantizer optimization.
In this paper the general classical approach of Picinbono and Duvaut is compared contrasted with the joint approach and illustrated by a simple example.
This paper introduces MAS4AT, a cooperative and self-adaptive multi-agent system for abnormal behaviour detection and alert triggering in maritime surveillance.
MAS4AT is designed and implemented in the context of the I2C project, a FP7 European project in which we are involved, which aims at implementing a new generation of maritime surveillance system able to permanently track and monitor all type of ship tracks in vulnerable trading lanes in order (i) to detect abnormal ships behaviours, (ii) to analyse it and, (iii) to trigger alerts if these behaviours correspond to threatening situations.
This paper presents I2C and then focuses on MAS4AT and its learning abilities by reinforcements.
A series of experiments was performed to determine how the duration of a word spoken in a sentence is influenced by: (1) the grammatical category to which it belongs, and (2) the position of the word in a constituent.
Experiment I contained Noun-Verb homophones (e.g. “I saw the coach…” — noun; “I saw him coach…” — verb), in sentences matched for phonetic environment and stress pattern.
Results support the notion that Nouns are longer than Verbs in typical sentences.
The results of Experiments II and III provided support for a lengthening account based on constituent-final position, while ruling out an account based on a debatable deletion site.
Results of Experiment IV generalized the account of constituent-final lengthening for Nouns and Verbs to additional major categories, by comparing the duration of the phrase-initial Adjective two with the phrase-final Adverb too.
Finally, Experiment V tested the distinction between minor and major categories.
Results demonstrated that the Preposition to is shorter than the Adjective two by approximately 50 percent.
Taken in sum, these findings indicate that it is sufficient to make a binary distinction between major and minor categories for purposes of a theory of speech timing and speech synthesis.
Durational effects traditionally ascribed to differences within the class of major categories can be accounted for solely in terms of constituent boundaries, already required in the theory to account for three other classes of phenomena.
The problem of singing voice extraction from mono audio recordings, i.e., one microphone separation of voice and music, is studied.
The approach is based on a priori probabilistic models for two sources, more precisely on Gaussian Mixture Models (GMM).
A method for model adaptation to the characteristics of the mixed sources is developed and a comparative study of different models and estimators is performed.
We show that the adaptation of the model of music from the non-vocal parts of songs yields good results in realistic conditions.
The selection of the “best” transcriptions is accomplished according to different criteria.
The Frequency criterion chooses the k most frequent transcriptions in the set of all phonetic decoding of that word, while the Maximum Likelihood (ML) criterion chooses the k most likely ones.
With the two criteria k is the same whatever the word is, and each of the k variants “describes” all the training utterances of the word.
A partition procedure, which determines the “optimal” number of transcriptions for each word, is then investigated.
This procedure assumes that, in the set of the selected transcriptions, each transcription must “describe” a subset of the utterances of the word.
So, the goal is to find the “suitable” transcriptions and to associate each of them to a subset of the pronunciations (utterances).
Two iterative algorithms are developed and evaluated, and a compromise between the likelihood and the number of elements of the “optimal” set of transcriptions is studied.
Speaker-independent speech recognition experiments showed that the ML criterion outperforms the Frequency criterion and that the performance obtained with the former criterion is comparable to that obtained with reference transcriptions.
This study presents a new predictive method, Separated Linear Prediction (SLP), for spectral estimation of speech.
The prediction of the signal value x(n) is computed from its p + 1 preceding samples by emphasising among the previous samples the one which is located next to sample x(n).
All the p samples from x(n − 2) to x(n − (p + 1)) are linearly extrapolated with x(n − 1) to obtain p new values, which are used for prediction.
Optimisation of the filter coefficients is computed using the autocorrelation criterion as in conventional LP.
The SLP-analysis yields an all-pole filter of the order p + 1 with p unknowns in the normal equations.
The performance of SLP was compared to conventional LP by analysing vowels produced by two female and four male speakers.
Results showed that the proposed method yielded in general more accurate modelling of higher formants, residuals with smaller energy and increased flatness.
In a semi-spontaneous conversational setting, subjects were made to repeat the same correction of one digit in a three-digit sequence consisting of “five” or “nine” followed by “Pine Street”.
Articulatory and acoustic signals were recorded by the University of Wisconsin Microbeam Facility for four speakers of American English.
By analyzing jaw movements, syllable magnitude and time values were evaluated, to represent the rhythmic organization of the utterance by a linear string of syllable pulses.
Preliminary results suggest that not only does the magnitude of the corrected syllable increase by the correction of a digit, but also, in most cases, there is some systematic increase of syllable magnitude both in the corrected digit and other digits in the same utterance, as the same correction is repeated.
Considerable difference among different speakers is observed and discussed in terms of syllable magnitude and timing patterns.
The Information Visualization scientific community has solutions to facilitate the navigation within the knowledge bases.
The usual object of these actions of visualization and navigation is the graphs.
Our goal is to compare techniques of displaying and of handling graphs, on the cognitive contribution which get to the users.
We developed a graph (3000 nodes, 10000 edges), bearing on common nouns.
This structure is declined according to three handling and views: two graphic modes (coloured and monochrome), and a textual mode (with hyperlinks).
This material is proposed to users for an experimental study, observing their preferences and performances on tasks of directed navigation (search for ways in a graph).
If the users prefer to browse on a graphic and coloured display, their performances (measured time and number of actions) are not significantly better in this view.
This reveals that a combination of two views would be an interesting solution: a global and graphic representation of the handled structure, coupled with a local, textual and more detailed representation of the zone of interest of this same structure.
The use of colour in computer vision has received growing attention.
This paper gives the state-of-art in this subfield, and tries to answer the questions: What is color?
Which are the adequate representations?
How to compute it?
What can be done, using it?
Towards that goal, we make a deep and up-to-date review of the existing littérature on this subject, we outline the important research directions and issues, and we attempt to evaluate them.
We present in this article a self-calibration method for uncalibrated array of sensors.
The proposed algorithm estimates the unknown sensors gain and phase.
The originality of our approach is in the consideration of non circularity of the impinging signals.
We show that this method works even in the case where the number of sources is larger than the number of sensors.
Simulation examples are processed in order to show the behaviour of our algorithm, in terms of convergence speed and estimation accuracy.
We describe the broad phonetic classification and segmentation of continuous speech.
This paper presents a generic approach for designing on-line handwritten shapes recognizers.
We present in detail our system and make the link between our models and more standard statistical models such as Hierarchical Hidden Markov Models and Dynamic Bayesian Networks.
We then evaluate fundamental properties of our approach: learning from scratch any symbol, learning from very few training sample.
We show experimentally that, using our approach, one can learn both a state-of-the-art writer- independent recognizer for alphanumeric characters, and a writer-dependent recognizer working with any twodimensional shapes that learns a new symbol with a few training samples and requires very few machines resources.
If natural rules in phonology, such as the rule which deletes a word final consonant before a consonant, are frequently found in unrelated languages, it must be because they tap universal features of production and/or perception.
The present experiment employed a learning task to see whether naive subjects have a predisposition for the natural rule as opposed to its converse (consonant deletion before a vowel).
Then three novel adjectives were combined with each of the four nouns, following the natural rule for one group of Ss, the unnatural rule for the other.
The Ss learning the unnatural corpus had a strong tendency to give natural responses, whereas the converse was not true.
Consequently they made many more errors en route to mastery than their natural counterparts, even when the operative rule was displayed on the first trial by presenting in turn each adjective with its four following nouns.
It appears that our Ss had implicit knowledge of the natural rule, even though it does not operate to any significant extent in English.
This paper deals with neural network modeling for time series analysis or regression.
Based on recent results about the least-square estimation for non-linear time series, we propose a complete andfeasible methodology for both parameter estimation (learning process) and model selection (architecture selection).
In particular, we solve the pruning problem for multilayer perceptron models with a stepwise search method by using a BIC criterion which is proved to be consistent.
In this paper, we present the concept of speaker-specific mapping for the task of speaker recognition.
The speaker-specific mapping is realized using a multilayer feedforward neural network.
In the mapping approach, the aim is to capture the speaker-specific information by mapping a set of parameter vectors specific to linguistic information in the speech, to a set of parameter vectors having linguistic and speaker information.
In this study, parameter vectors suitable for speaker-specific mapping are explored.
Background normalization for score comparison and network error criterion for frame selection are proposed to improve the performance of the basic system.
It is shown that removing the high frequency components of speech results in loss of performance of the speaker verification system.
For all the 630 speakers of the TIMIT database, an equal error rate (EER) of 0.5% and 100% identification is achieved by the mapping approach.
On a set of 38 speakers of the dialect region “dr1” of NTIMIT database, an EER of 6.6% is obtained.
The system uses diphones and a formant synthesizer chip for speech synthesis.
Input to the system is in pseudo-phonetic notation.
Intonation contours using a declination line and various rises and falls are generated starting from an input consisting of punctuation and accent marks.
The hardware design has resulted in a small, portable and battery-powered device.
A short evaluation with users has been carried out, which has shown possibilities for such a device but has also indicated some problems with the current pseudo-phonetic input.
As an introduction to the Special Issue, a tutorial examination of recent developments important to understand current research on reading acquisition is offered.
The accent is put on the interrelations between studies of skilled adult performance, of effects of neurological damage and of early reading.
The central puzzle of reading research is to identify the causes of the specific difficulties which acquiring literacy appears to entail.
The problem has generally been attacked through correlational methods, based on the comparison of better and poorer achievers.
The merits and shortcomings of that approach are examined and the need for linking differential studies to a general theoretical conception of the reading process and of its development is emphasized.
The line of studies stemming from the hypothesis that a major difficulty in acquiring alphabetic literacy is to manipulate language at the level of phonemic segments is examined, and also the way the results of these studies can be related to current theories of lexical access.
The limitations of the approach consisting of deriving hypotheses about development from theories of the adult stage are discussed and illustrated by data from studies of the reading performance of both children with normal reading achievement and developmental dyslexics.
Finally, the possibility that sources of acquisition difficulties might be found at levels beyond that of word recognition is discussed.
This paper presents a new approach to automatic recognition of spoken words.
After discussing the demands upon appropriate subword units and reporting some experiments in using phone superclasses for word recognition,
we will develop technique of robust classification, segmentation, and lexical access, utilizing binary phonetic features as processing units.
Speakers in conversations between humans continually adapt the prosodic and structural aspects of their speech to the perceived needs of their listeners, in terms of judgments about the potentially masking effects of transient and ambient noise levels, and in response to explicit requests by listeners for repetition.
Adaptive strategies for repetition include changing such prosodic aspects of utterances as pitch range and mean, intensity mean and overall tempo of speaking, together with intonational re-structuring.
Such repetition also deploys re-start strategies based on structural linguistic knowledge.
Bayesian networks are well suited tools for diagnosis tasks.
In this paper, we focus on classical algorithms used to build diagnosis systems based on bayesian networks, and more particularly, medical diagnosis systems.
We review some methodological questions concerning the representation of probability densities (discretization? use of gaussian models?) and the choice of the adequate structure (naive Bayes structure? learning the structure with the help of an expert or from data?).
A case study, thyroid cancer diagnosis, will illustrate those considerations and some implemented algorithms.
The recent research trend towards the use of harmonies/sinusoid based methods, in order to exploit the fine spectral structure of voiced speech, cannot be questioned.
This paper discusses the state of the art in this area, both in terms of analysis-synthesis methods and of their application to coding.
The key points are:
• - Harmonic modelling is a very efficient tool for voiced regions, producing synthetic speech of very high quality, but being simultaneously prone to pitch and voicing errors.
The main disadvantage of harmonic coding is the need for an alternative method for unvoiced regions.
ATC is a natural choice.
In this paper, an 8 kbit/s simulation is presented, using hard switching between harmonic coding and ATC.
• - Sinusoid based modelling extends the basic analysis-synthesis framework to unvoiced and transition regions, by removing the constraint that the sinusoids be harmonically related.
When it comes to coding, however, it still has many unsolved problems.
As a conclusion, some guidelines for future research are discussed.
The work described in the paper was carried out in the SPEAK! project (Speech Generation in Multimodal Information Systems).
The aim of the project was to improve the quality of synthesised speech output to be used in dialogue systems as an additional element of multimodal man-machine interfaces.
German text and dialogue interaction analysis (theoretical research) has been carried out to predict the tone groups (TGs), the phrase boundaries in sentences and the place of the focus in the phrase.
Tone groups represent the general intonation structure of the phrase not taking into account word level intonation.
The results of this research are the intonation markers described in (Teich et al., 1997).
The CTS synthesiser constructs the main intonation patterns from texts containing these additional markers.
This paper describes the research results on German intonation, including the construction of intonation rules, combined with the study on timing adjustments, pause generation for rhythm (both for segmental and suprasegmental levels) for the MULTIVOX-SPEAK! system.
Detailed rules and a new tone-group based prosody generation module are also introduced: these have been integrated into the MULTIVOX TTS system.
Preliminary evaluation results are also given.
The letter-to-sound transcription rule system is built up taking into account the implicit phonotactic constraints of the Italian language.
The system follows the mathematical model called Finite States Automata (FSA), which is generalized and augmented to obtain a simple syntax-directed translation schema.
These rules are entirely based on the pure orthographic form of the words, and no level of grammatical knowledge of their classes is considered.
Finally, the advantages and disadvantages of the graph-oriented approach will be shown and discussed, together with future trends and improvements.
An algorithm is described which abstracts acoustic parameters of a speech waveform to automatically transcribe sentential stress and pitch movements.
The waveform acoustics used are duration, energy and fundamental frequency.
The abstractions described aim to isolate the prosodically imposed variations in these parameters.
A method of syllabification from acoustic parameters is presented.
The prominence of each syllable is determined using the automatic process described and the resultant transcription is compared with a hand-labelled prosodic transcription.
The agreement level of 61.6% suggests that acoustic parameters other than those already used by the algorithm may be available to the human labeller.
The performance of speech recognition systems is significantly degraded in the presence of noise.
To solve the noise problem, there is a need to reconsider standard approaches by taking into account this new constraint.
We first envisage two well-known cepstral representations (parametric and non-parametric) of speech signals and propose a unifying view of both schemes.
We introduce a pseudo-autocorrelation domain, which can be interpreted as a “Root-cepstral domain”, and we show how non-parametric cepstral and linear predictive analyses converge to the same optimal solution.
Experiments are carried out using an HMM-based isolated word recogniser for speaker-dependent and speaker-independent tasks in car noise environments.
Although Tamil has an indigenous tradition of «grammatical» description — in an extended sense of «grammatical» which encompasses poetics and metrics, along with phonetics, morphology and syntax — which goes back to the first half of the first millenium AD, it was described once again from the 16th century onwards from a new angle (which included an interest for ordinary language) by Christian missionaries who brought with them a Latin model of grammatical description, which they tried to apply, by creative extension.
I shall therefore refer to the corpus of their productions as the Grammatici Tamulici, although the earliest among them make use of Portuguese as a metalanguage for the description of Tamil.
Not being in the situation of some field linguists, who have to start from scratch when confronted with a language which has never been written, those missionaries progressively discovered that the Latin alphabet, although enriched by several extensions developed for the representation of European vernaculars, was a less efficient tool than the local (Tamil) syllabary for noting down Tamil sounds and words.
They also discovered that their capacity to be convincing, in front of their converts, depended in a great part on their adopting the language hierarchies which governed (and still govern) the diglossic Tamil society, as we shall see here in this preliminary exploration covering five authors who were active during a period of ca. 200 years, up to the year 1739.
Edited and Condensed Nearest Neighbor Rules are used in various applications in Pattern Recognition problems.
In this study, modified versions of these algorithms are applied to speaker-independent isolated word recognition to select the word templates, as opposed to the clustering techniques.
It is shown that the approach improves the recognition rate when compared with clustering, with the disadvantage of being more costly.
A new system for the automatic segmentation and labelling of speech is presented.
The system is capable of labelling speech originating from different languages without requiring extensive linguistic knowledge or large (manually segmented and labeled) training databases of that language.
Due to the limited size of the neural networks, the segmentation and labelling strategy requires but a limited amount of computations, and the adaptation to a new task can be accomplished very quickly.
The system was first evaluated on five isolated word corpora designed for the development of Dutch, French, American English, Spanish and Korean text-to-speech systems.
The results show that the accuracy of the obtained automatic segmentation and labelling is comparable to that of human experts.
In order to provide segmentation and labelling results which can be compared to data reported in the literature, additional tests were run on TIMIT and on the English, Danish and Italian portions of the EUROM0 continuous speech utterances.
The performance of our system appears to compare favourably to that of other systems.
The behavior of real ants while resolving complex problems, that they encounter in their daily life, have inspired attracting algorithms for the resolution of the clustering problem.
Most ant clustering algorithms extend the basic model developed by Lumer and Faieta (Lumer et al., 1994) which was inspired by cemetery organization and larval sorting phenomena detected on some ant species.
Other algorithms were developed by taken other inspirations from real ants like self-assembling behavior and chemical recognition.
In this paper we take inspiration from further biological properties of real ants mainly sensorial communication.
After some preliminary information concerning the success of Dante's Commedia in France during the XVthand XVIthcenturies, this article focuses on the anonymous translation of the Inferno, in alexandrines and in terza rima, now held at the Biblioteca Nazionale Universitaria in Turin (ms. L. III. 17).
This translation is currently considered as the most ancient French version of the Commedia.
Unfortunately, its transcription, which dates back to the end of the XIXthcentury, is extremely inaccurate, and needs to be replaced by a more reliable critical edition.
After a brief summary of previously conducted studies, some preliminary questions are tackled in the next few pages.
Firstly, this article provides both textual and iconographical elements that should help date the manuscript whilst identifying the source of the Italian text featured in the Turin codex, which is most likely the starting point for the translation.
Secondly, this article deals with the possibility of determining the translator's origins on the basis of some lexical items. Subsequently, its attention focuses on the analysis of a number of translation strategies deployed, which may hopefully pave the way for further contributions on the literary aspects of this work.
A new low delay speech coder is proposed.
In order to achieve low delay, the coder is based on the memory dependent vector quantization of the synthesis filter.
The transformed samples are considered to be a sequence of Laplacian random variables that are vector quantized in an efficient manner using a geometrical lattice VQ.
The coder was tested at a bit-rate of 8625 bit/s and a delay of 8 ms; a good reproduced voice quality was obtained.
F0 (fundamental frequency) control of human speech production is studied by using both a stochastic time series model and a system analysis with a vector autoregressive (VAR) model.
We use two-dimensional time series data of F0 (ff0 and sf0) obtained through the transformed auditory feedback (TAF) experimentation system developed by one of the authors.
sf0 is extracted from speech data that includes prolonged phonation of the vowel /a/, and the signal ff0 is extracted from frequency-modulated feedback speech by white Gaussian noise.
Most of the data had mean-nonstationary characteristics.
The stochastic procedure decomposes the mean-nonstationary and other components in one stage without pre-manufacturing the data.
The cyclical components around the mean-nonstationary components are assumed to be generated by the VAR model.
We can execute a stochastic system analysis using the estimates of the VAR model to analyze the physical characteristics of the data.
We also performed a simulation study using the estimates obtained by the model to discover the role of F0 control under the situation in which we completely lost our hearing ability.
The results clearly indicate the dynamic properties of F0 control for each segment which occur with each breath taken during a sustained tone.
It has been shown previously that spoonerisms (such as barn door → darn bore) can be elicited by having subjects attempt to articulate a target barn door) preceded by bias items which contain at least the initial phoneme (/d/) of the desired error outcome.
Since certain linguistic characteristics of the error outcomes differ from those of their targets, variables which affect only these 'outcome' properties in a systematic way can be shown to be the result of prearticulatory output processes, independent of perceptual 'target' properties.
The present study shows that the base-rate of errors produced by the phonetic bias technique can be increased dramatically by adding, to the word-pairs preceding the target, some items which are semantically synonymous to the error outcomes of the target.
In this way, it is demonstrated rigorously that semantic bias increases the likelihood of slips of the tongue; which is one of the defining properties of so-called 'Freudian slips'.
Implications are discussed.
The study presented in this paper deals with the modelling of extreme emotions occurring in abnormal situations.
The aimed application is civil safety and surveillance in the public places in particular.
A corpus of fiction (SAFE Corpus) is selected illustrating rich and varied contexts with the presence of extreme emotions, mainly fear.
An annotation strategy adapted to the application is then developed, with both generic and specific descriptors.
Finally, a detection system of fear emotions based on acoustic cues is implemented to carry out an evaluation.
On the one hand the system is robust to context changes.
On the other hand, the influence of multimodal annotation is minor.
Results obtained with the various protocols are similar: fear is recognized with 67% of success.
The effect of sentence accent, word stress, and word class (function words versus content words) on the acoustic properties of 9 Dutch vowels in fluent speech was investigated.
A list of sentences was read aloud by 15 male speakers.
Each sentence contained one syllable of interest.
This could be a monosyllabic function word, an unstressed syllable of a content word, or a stressed syllable of a content word.
A total number of 3465 vowels were segmented from the syllables and analysed.
It was found that all three factors mentioned above had a significant effect both on the steady-state formant frequencies (F 1 and F 2) and on the duration of the vowels.
Word stress and word class had a stronger effect on the vowels than sentence accent.
A listening experiment showed the perceptual significance of the acoustic measurements.
It appeared that spectral vowel reduction could be better interpreted as the result of an increased contextual assimilation than as the tendency to centralize.
We also studied changes in the dynamics of the formant tracks due to the experimental conditions.
It was found that formant tracks of reduced vowels became flatter which supports the view of an increased contextual assimilation.
Three simple models of vowel reduction are discussed.
We present in this article a Partial Connection Multilayered Network (PCMN), based on a technique of partial connection between layers.
This network can be trained automatically by the gradient back-propagation (GBP) algorithm.
A general network based on GBP has been implemented on a ring configuration of the F.P.S. T20 Hypercube.
We have made a special effort to ensure an efficient parallelization of this algorithm.
This implementation provides us with a large number of possible configurations for our network.
In this experiment the network is used to effect isolated word recognition.
Results show the advantages of partial connection as against full connection.
This technique permits the efficient treatment of temporal information, which is very important in speech processing, unlike image processing.
This experiment also permits us to gain a better understanding of network characteristics important for speech processing.
Partial connection can introduce both temporal context constraints and some implicit knowledge into the network and may also lead to efficient learning on a small data base.
Good recognition results have been obtained.
This corresponds to situations that occur frequently in areas like geophysics, ultrasonic imaging or nondestructive inspection.
ARMA representations yield a non-standard state driving noise detection-estimation problem whose resolution is complex and requires great computational efforts.
AR representations and the use of multi-pulse coding techniques cannot account for nonminimal phase systems and exhibit the disadvantages of output-error type methods.
None of these approaches provide any on-line processing ability.
Furthermore, fast modified Chandrasekhar equations can be used for the implementation of this procedure and produce significant savings in computational requirements.
Simulation results are satisfactory, and are obtained with less computations than other existing methods.
This paper describes a real time vision system, allowing to localize faces in video sequences and to recognize their identity.
These processes are based on combining techniques of image processing and neural network approach.
The robustness of this system has been evaluated quantitatively on 8 video sequences.
We have tested our model using the ORL database in order to compare performances with other systems.
The system has also been implanted on electronic architectures based on dedicated chip ZISC and FPGA.
We analyse the algorithm complexity and we present results of hardware implementations in terms of used resources and processing speed.
One of the most highly developed human abilities is communication by speech.
Throughout the years, research on speech perception has demonstrated that humans are well adapted to extract highly encoded linguistic information from the speech signal.
The sophisticated nature of these capacities and their early appearance during development suggest the existence of a rich biological substrate for speech perception.
In the present paper, we describe some of these important capacities and examine research from different domains that may help illuminate the nature of their biological foundations.
Recently, a lot of algorithms minimizing a non-convex energy function have been proposed to solve low level vision problems.
Different kinds of relaxation methods are available.
The stochastic techniques, such as simulated annealing, asymptotically converge to the global minimum but require a high computational cost.
Deterministic relaxation methods which are sub-optimal, give good results and are faster than the stochastic ones.
In this paper, we focus on the parallel implementation of two deterministic algorithms for edge detection and image smoothing: the graduated non convexity (GNC) originally proposed by Blake & Zisserman and the mean field annealing (MFA) introduced by Geiger & Girosi and extended to anisotropic compound Gauss-Markov random fields by Zerubia & Chellappa.
Both methods are based on a weak-membrane model and both algorithms are inherently serial: each step produces a pixel map which is taken as an input for the next step.
For the GNC, we implement a checkerboard version of the successive over-relaxation (SOR) method to minimize the energy.
For the MFA, we use an optimal step conjugate gradient descent.
A compensating ability of the articulatory control system for laryngectomized patients was studied.
X-rays of the vocal tract and acoustic measurements were carried out on three patients before and after the operation, using the trachea – esophagus bypass.
Within two weeks of the operation, the patients produced the Russian vowels /a, u, i/ with formant frequencies closer to the phonetic norm than before the operation. After two years, two patients produced the vowels with normal formant parameters.
The acoustical characteristics of speech after the operation were measured on 14 patients.
1 to 2 years after the operation, four patients were able to make voicing–unvoicing distinction.
One patient has recovered complete control of the vocal source.
The results obtained imply that the adaptation of the articulatory control system to the distorted conditions of articulation and voice generation can be governed, not only by acoustical parameters like formant frequencies, but also by such a complex phonetic element as the voicing cue.
The control system has demonstrated its ability to reorganize the activity of articulatory muscles and to transfer the functions of the excised laryngeal muscles to the muscles that had never been used for voice control.
The implication of the observed phenomena for the concept of internal model is being discussed.
Learning Classifier Systems (LCSs) are rule-based systems that automatically build their ruleset.
Initially, LCSs were dedicated to the modelling of the emergence of cognitive abilities thanks to adaptive mechanisms, particularly evolutionary processes.
After a renewal of the field more focused on learning, LCSs have been reconsidered as sequential decision problem solving systems endowed with a generalization property.
Finally, much more recently, LCSs have proved very efficient at solving classification tasks, which boosted the field.
In this context, the aim of this contribution is to present the state-of-the-art of LCSs, insisting on recent developments, and focusing more on the sequential decision domain than on automatic classification.
Most approaches developed or used in the present work for seabed characterization are based on the use of texture analysis methods.
Indeed, sonar images have different homogeneous areas of sediment that can be viewed as texture entities.
Generally, texture features are numerous and not all are relevant; an extraction-reduction of these features seems necessary before the classification phase.
We present in this work a complete chain for sonar images classification while optimizing the chain steps.
We use the Knowledge Discovery in Databases (KDD) process for the chain development.
The underwater environment is uncertain, which is reflected on the images obtained from the sensors used for their acquisition.
Therefore, it is important to develop robust methods to these imperfections.
We solve this problem in two different ways: a first solution is to make robust traditional classification methods, such as support vector machines or ^-nearest neighbors, to these imperfections.
A second solution is to model these imperfections to be taken into account by belief or fuzzy classification methods.
We present the results obtained using different texture analysis approaches and classification approaches.
We use other approaches based on the uncertain theories to overcome sonar images imperfections problem.
In North America, people call the directory assistance operator to find the phone number of a business or residential listing.
The directory assistance service is generally maintained by telcos, and it represents a significant cost to them.
Partial or complete automation of directory assistance would result in significant cost savings for telcos.
Nortel Networks has a product called Automated Directory Assistance System (ADAS) Plus which partially automates this directory assistance function through the use of speech recognition.
The system has been deployed all across Quebec, through most of US West and BellSouth.
ADAS Plus primarily automates the response to the question “for what city?” through speech recognition.
We give details of this speech recognition system and outline its performance in the deployed regions.
Cooperation in work settings goes through communicative interactions where colleagues need to reach a certain level of mutual understanding for coordinating their actions or finding negotiated decisions.
Mutual understanding is complex because of the heterogeneity of the participants which makes the interpretations uncertain and unpredictable.
We argue here that chronic cooperation is still more complex because the memory of the previous cooperative interactions is an additional source of difference between the cognitive contexts of the participants.
We studied the memory of cooperative interactions in four different collaborative work settings.
The analysis indicates a massive forgetting of the verbal content and a greater remembering of the relational positionings, interactional structures and emotions.
An overview of a statistical paradigm for speech recognition is given where phonetic and phonological knowledge sources, drawn from the current understanding of the global characteristics of human speech communication, are seamlessly integrated into the structure of a stochastic model of speech.
A consistent statistical formalism is presented in which the submodels for the discrete, feature-based phonological process and the continuous, dynamic phonetic process in human speech production are computationally interfaced.
This interface enables global optimization of a parsimonious set of model parameters that accurately characterize the symbolic, dynamic, and static components in speech production and explicitly separates distinct sources of the speech variability observable at the acoustic level.
The formalism is founded on a rigorous mathematical basis, encompassing computational phonology, Bayesian analysis and statistical estimation theory, nonstationary time series and dynamic system theory, and nonlinear function approximation (neural network) theory.
Two principal ways of implementing the speech model and recognizer are presented, one based on the trended hidden Markov model (HMM) or explicitly defined trajectory model, and the other on the state-space or recursively defined trajectory model.
Both implementations build into their respective recognition and model-training algorithms a continuity constraint on the internal, production-affiliated trajectories across feature-defined phonological units.
The continuity and the parameterized structure in the dynamic speech model permit a joint characterization of the contextual and speaking-style variations manifested in speech acoustics, thereby holding promises to overcome some key limitations of the current speech recognition technology
Much current work in the field of context-aware systems focuses on application- specific solutions and ad-hoc approaches, and the lack of conceptual models for the design of context-aware systems hinders the development of more general and complex systems.
Furthermore, it is often unclear what the consequences of early design decisions are for the quality of the final implementation, making the design difficult and leading to mistakes only to be discovered when the system is already implemented.
In this article we present a classification of the architectural aspects of developing context-aware systems.
We furthermore give a quality framework that describes the consequences of the architectural aspects on the quality of the context-aware system.
We demonstrate the usefulness of the framework by modeling the architecture of a context-aware system.
This paper investigates the problem of automatic segmentation of speech recorded in noisy channel corrupted environments.
Using an HMM-based speech segmentation algorithm, speech enhancement and parameter compensation techniques previously proposed for robust speech recognition are evaluated and compared for improved segmentation in colored noise.
Speech enhancement algorithms considered include: Generalized Spectral Subtraction, Nonlinear Spectral Subtraction, Ephraim–Malah MMSE enhancement, and Auto-LSP Constrained Iterative Wiener filtering.
In addition, the Parallel Model Combination (PMC) technique is also compared for additive noise compensation.
In telephone environments, we compare channel normalization techniques including Cepstral Mean Normalization (CMN) and Signal Bias Removal (SBR) and consider the coupling of channel compensation with front-end speech enhancement for improved automatic segmentation.
Compensation performance is assessed for each method by automatically segmenting TIMIT degraded by additive colored noise (i.e., aircraft cockpit, automobile highway, etc.), telephone transmitted NTIMIT, and cellular telephone transmitted CTIMIT databases.
Experiments and results of the application of the Approximating and Eliminating Search Algorithm (aesa) to multi-speaker data are reported.
Previous (single-speaker) results had already shown that the performance (speed) of the aesa remains greatly insensitive to increasing the size of the dictionary, while a very strong (exponential)_tendency to higher performance is exhibited as the test utterances are close to their corresponding prototypes.
Following these results we show in this paper that, by increasing the number of tokens included in dictionaries with multiply represented words, a simultaneous reduction can be achieved in both the error-rate and the number of distance computations required.
Training algorithms for natural speech recognition require very large amounts of transcribed speech data.
Commercially distributed books on tape constitute an abundant source of such data, but it is difficult to take advantage of it using current training algorithms because of the requirement that the data be hand-segmented into chunks that can be comfortably processed in memory.
In order to address this problem we have developed a training algorithm which is capable of handling unsegmented data files of arbitrary length; the computational requirements of the algorithm are linear in the amount of data to be processed and the memory requirements are constant.
This paper reviews the fundamental concepts of Linear Prediction (LP) and Maximum Entropy (ME) spectral analysis, and elucidates the reasons for their practical importance in the world of real signals.
Subsequently, the powerful principle of Minimum Cross-Entropy (MCE) spectral analysis is introduced.
MCE permits the incorporation of prior information into signal analysis.
In a new approach to speech signal analysis, application of the MCE principle reduces the average number of predictor coefficients (poles) that have to be specified per time frame for a given spectral resolution by relying on prior spectral information.
Such prior spectral information may be given by glottal source and lip radiation characteristics, microphone and transmission frequency responses, and spectral information from preceding time frames — particulary during steady-state or slowly-varying portions of a speech utterance.
This paper stresses general principles rather than computational details.
This article presents a comparative study of four approaches of school enrolment of allophone pupils into French colleges.
Our problematic consists in examining the elements that facilitate the success of allophone pupils, including how the way they are integrated affects their ability to earn.
We shall analyze their management in the target devices from two different standpoints: the intrinsic interest of each device and how they articulate with the 'ordinary class'.
This qualitative research, which consists in semi-directive interviews carried on with allophone students and teachers, points to the conclusion that the only really personalized and adapted approach for allophones students lies in the specific reception devices
In order to analyse and interpret speech signals, different time-frequency representations are used (e.g. spectrogram, Wigner-Ville distribution, wavelets).
In this paper we construct within Cohen's class of time-frequency distributions the distribution that is optimally suited for the representation of speech signals.
Thereby we take advantage of the special time-frequency structure of speech expressed in the Elementary Waveform Speech Model (EWSM, d'Alessandro, 1990).
As an application we present an algorithm that extracts a point pattern in the time-frequency plane out of the speech signal using the optimized distribution.
Thus we get a very simple representation of the speech signal that is well interpretable both for non-stationary and for stationary speech segments.
Furthermore this representation could serve as a base for further analysis (e.g classification).
In many speech research institutes, work is in progress which aims at the collection and annotation of a large number of speech utterances.
As the complexity of these databases increases, it becomes important to pay attention to certain aspects of database management and specifically to database access.
In this paper, a user-friendly formalism is presented which may be used to phrase queries for speech databases.
This formalism is domain-specific in the sense that the speech researcher may phrase a query in acoustic-phonetic terms, without first having to become a computer programmer.
This paper explores the dynamics of stock market from a behavioral perspective using a multi-agent simulation.
The aim of this paper is to study the behavior of investors in the stock market to find a model as close as possible to reality.
The main problem is to understand, through a novel model which includes behavioral and cognitive attitudes of the investor, the running of the market and determine the sources of his complexity.
Simulation experiments are being performed to observe stylized facts of the financial time series.
These experiments show that representing a behavioral model allows to observe emergent socioeconomic phenomena.
The use of hypothesis verification is recurrent in the model-based recognition literature.
When data involved in the computation of the pose are noisy, the pose is inaccurate and difficult to verify, especially when the objects are partially occluded.
It consists in a recursive multi resolution exploration of the pose space, discarding outliers in the match data while the search is progressing.
Numerous experimental results are described.
Herbert Simon was the inventor of the “Scientific Discovery”.
This domain of research whose main figures are Pat Langley, Jan Zytkow and Douglas Lenat, aims at a rational reconstruction of old scientific discoveries in a way which could be reproduced with a computer.
The advantages are twofold.
On the one hand, in the epistemological field, it models the scientific activity up to its more enigmatic appearances.
On the other hand, from a practical point of view, it opens an exciting area in which the computer helps men in their quest of knowledge.
Here is, in memory of Herbert Simon, a glimpse of the scientific discovery.
The mirror for princes known as the Naṣīḥat al-mulūk of al-Māwardī, probably a tenth-century text, is replete with references to sources identified by the author as “Indian”.
A large number of these texts also appear in the so-called Waṣiyyat Arisṭāṭālīs li-l-Iskandar; some examples find parallels in Kalīla wa-Dimna and Bilawhar wa-Būḏāsaf.
These coincidences raise several possibilities: first, that the author's “Indian” source represents a work of Indic background, translated from Sanskrit or another Indian language into Arabic, probably at the time when the Barmakids were sponsoring such translations in significant numbers; secondly, that it was rendered from an Indian language into Middle Persian in the Sasanian period and from that language into Arabic in the early centuries of the Islamic era; thirdly, that the text was composed in a non-Indian language, probably Middle Persian, and acquired a “forged” Indian genealogy in a parallel to the numerous spurious Greek attributions (a category that would subsequently include the pseudo-Aristotelian testament).
The article addresses these three possibilities, and, on the basis of textual and contextual considerations, suggests that at the present stage of research, it is the second that seems most likely.
This paper deals with the problem of detecting multiple narrow-band sources and estimating their angles of arrival using the signals received on a large moving array in underwater acoustic passive listening.
This result provides the antenna with its maximum gain and permits to reconstruct the line-array antenna underlying most array processing methods.
Afterwards we can estimate the number of secondary sources and their location.
After recalling the AIC and MDL detection criteria, we briefly describe the MUSIC algorithm that we use to identify the sources supposed narrow-band and far from the reception array.
Finally, we apply this processing to experiment data.
The results are compared with those, provided by the two-dimensional- Fourier Transform.
This spatio-temporal representation points out that knowledge of the geometry of the receving antenna is very important.
In Middle French, the referential continuity and the choice of anaphoric expressions in the anaphoric chain - nominal, pronominal forms or zero anaphora supported by verbal morphology - are determined, in narrative and discourse, by five syntacticalsemantic rules: one, of “referential competition”, three, “valentiello-référentielles” and one, “synctactico-valentielle”.
This study examines their frequency of application on a Middle French prose translation, namely that of Boccaccio's Decameron translated by L. de Premierfait (1411–1414), which may allow us to observe the influence the source language has on its medieval translation.
In this paper, we present a brief survey on the use of different types of Markov models in writing recognition.
Recognition is done by a posteriori pattern class probability calculus.
This computation implies several terms which, according to the dependency hypotheses akin to the considered application, can be decomposed in elementary conditional probabilities.
Under the assumption that the pattern may be modeled as a uni- or two-dimensional stochastic process (random field) presenting Markovian properties, local maximisations of these probabilities result in maximum pattern likelihood.
We have studied throughout the article several cases of subpattern probability conditioning.
Each case is accompanied by practical illustrations related to the field of writing recognition.
This paper provides an overview of an approach for both predicting the performance of a previously characterised speech recogniser for particular applications and also for analysing the effect of speech variability on performance.
The method also potentially provides a principles way of developing a database for assessment purposes.
The method, Recogniser Sensitivity Analysis (RSA) was developed by Logica within the UK Alvey Speech Technology Assessment (STA) project MMI/132.
The method relies principally on the characterisation of the many sources of variability in speech recogniser performance by a small number of measurable parameters.
The experiments carried out to determine the validity of this approach and the results obtained are described.
The applicability of the method for predicting likely field performance of a recogniser in a particular application as well as the influence of particular within- and between-speaker variations on recogniser performance are discussed.
Preliminary results indicate a significant correlation between certain speech parameter values and recogniser performance.
The paper concludes with a discussion on future directions for extending RSA.
Four speech coders in the CELP (Code Excited Linear Predictive) family are described.
By replacing the long term prediction with a self excitation sequence (adaptive codebook) as well as substituting a stochastic multipulse, or sparse codebook for the commonly used codebook of Gaussian noise, the speech quality is improved.
The coders are fully quantised at 7.0 and 5.0 kbit/s, interesting bit-rates for such possible applications as the half rate GSM system and the INMARSAT-M service.
The performance of the coding schemes are evaluated by a formal listening test and presented by their Mean Opinion Scores (MOS).
For the coder with maximum performance with respect to quality and complexity, a set of tolerable bit error rates (BER) are given.
It is shown that the presence of acoustic background noise does not influence the coder quality, and that bit errors in this case will be partially masked by the background noise giving the coder a high degree of robustness.
Considering the performance in the presence of bit errors and background noise, the coder seems to be suitable for use in a mobile communication system using satellite links.
When Coptic grammars appeared for the first time (middle of the thirteenth century), only the dominant Arabic linguistic tradition could serve as a point of reference.
However, Coptic, which was losing its status of vernacular, does not belong to the same family as Arabic.
Through some typical samples related to script, phonology, and morphology, this paper tempts an insight into the way the Arabic conceptual and terminological apparatus was applied and adapted to Ancient Egyptian in its final stage. We show that in general medieval Coptic grammarians did in fact succeed in their undertaking.
Whatever deficiency one may find, this should not necessarily be imputable to the linguistic tradition that acts as a model, but rather to the external conditions affecting the very intellectual activity of the protagonists.
Since 1976 work has been carried out towards the development of a microcomputer-controlled set of equipment for displaying pitch contours of sentences.
It was found that, for different groups of learners (Dutch learning English, Turks learning Dutch), subjects receiving audio-visual feedback performed better than those receiving only auditory feedback.
No differential learning effects were found for L2-proficiency level or age.
The paper describes a simulation methodology for radar targets in marine environment.
Our approach is based on a new model, called "Scattering Center Set Unified Representation", which is able to approximate the backscattered radar target echo for any aspect angle.
This model is fast to calculate and has the advantage to take into account the partial or total target concealing due to the sea waves.
It associates to any target aspect a scattering center set.
An amplitude map accounts for each scattering center anisotropy and geometrical visibility.
A virtual model is then used for describing the target motion and its concealing by the sea waves.
The influence of the sea clutter is also taken into account.
The radar signatures used in our simulations have been measured in the anechoic chamber of ENSIETA for four scale-reduced naval targets.
The paper also presents some imagery and classification results, which are aimed to illustrate the other side of the naval target characterization problem.
This paper describes the essential speech processing techniques for interactive voice applications in the telecommunications field.
These techniques include speech recognition and speech synthesis, both of which aim to make interactive speech communications between man and machine more natural.
Keyword spotting, background noise effects reduction, and speakers and/or telephone adaptation techniques are considered essential in speech recognition in order to allow a more natural voice input as well as an adequate robustness against environmental variabilities.
In the area of text-to-speech synthesis, we propose a rule-based synthesis method applicable to the Japanese language, aiming to produce high quality speech.
The commercial system ANSER of a former project is also described as an example of an interactive speech processing system.
Finally, a recently developed speech recognition server which includes a vocabulary-flexible recognition function is described.
This paper describes two algorithms for separating two overlapping speech signals.
Both rely heavily on accurate measurements of the pitch of the target voice.
The first uses a single microphone, and an important feature is the exploitation of the onset of a voiced sound as an aid to the extraction of its pitch in the presence of interference.
The second uses two microphones, and an important feature is that it also makes use of the direction of the target voice.
Making a self-repair in speech typically proceeds in three phases.
The first phase involves the monitoring of one's own speech and the interruption of the flow of speech when trouble is detected.
From an analysis of 959 spontaneous self-repairs it appears that interrupting follows detection promptly, with the exception that correct words tend to be completed.
Another finding is that detection of trouble improves towards the end of constituents.
The second phase is characterized by hesitation, pausing, but especially the use of so-called editing terms.
Speech errors induce other editing terms than words that are merely inappropriate, and trouble which is detected quickly by the speaker is preferably signalled by the use of 'uh'.
The third phase consists of making the repair proper.
The linguistic well-formedness of a repair is not dependent on the speaker's respecting the integrity of constituents, but on the structural relation between original utterance and repair.
A bi-conditional well-formedness rule links this relation to a corresponding relation between the conjuncts of a coordination.
It is suggested that a similar relation holds also between question and answer.
In all three cases the speaker respects certain structural commitments derived from an original utterance.
It was finally shown that the editing term plus the first word of the repair proper almost always contain sufficient information for the listener to decide how the repair should be related to the original utterance.
Speakers almost never produce misleading information in this respect.
It is argued that speakers have little or no access to their speech production process; self-monitoring is probably based on parsing one's own inner or overt speech.
Les entrées et les sorties ne sont pas des phénomènes indépendants dans les systèmes interactifs en général et plus particulièrement lorsque l'on considère le cas de l'interaction multimodale.
Nous présentons les résultats issus d'une expérimentation de type «Magicien d'Oz» qui suggère que les modalités de sortie, utilisées par un système multimodal, ont une influence sur les modalités d'entrée effectivement employées par une large catégorie d'utilisateurs du grand public.
Certains sujets ont cependant une modalité favorite et ne sont donc pas influençables.
En effet ce type d'environnement ne requiert pas de connaissances en informatique et peut donc être utilisé par des personnes du grand public.
Les expériences ont aussi fait ressortir que la modalité d'expression orale est la favorite lorsqu'on se situe dans une pièce intelligente, ceci pour une large proportion des sujets, sauf si la modalité graphique est utilisée par le système lui-même.
The creation and evolution of the 16 kbit/s Low-Delay CELP (LD-CELP) speech coding algorithm (CCITT Recommendation G.728) represents an intensive research effort from 1988 to 1992.
In this paper, we give a historical overview of this four-year effort, with emphasis on discussions of technical merits of many alternative algorithmic techniques we investigated.
As a part of such discussions, we explain why we put sume of these techniques in the final G.728 coder and left the others out.
It is hoped that this paper illustrates how the G.728 algorithm was created out of very simple initial concepts, and then modified and improved little by little under real-time implementation constraints, until the final algorithm fully met the performance requirements that were seemingly impossible at the outset.
The Philips automatic telephone switchboard and directory information system PADIS provides a natural-language user interface to a telephone directory database.
Using speech recognition and language understanding technologies, the system offers phone numbers, fax numbers, e-mail addresses, and room numbers as well as direct call completion to a desired party.
In this paper, we present the underlying probabilistic framework, the system architecture, and the individual modules for speech recognition, language understanding, dialogue control, and speech output.
In addition, we report results on performance and user behaviour obtained from a field test in our research lab with a 600-entry database.
We derive a new maximum-a-posteriori decision rule which incorporates database knowledge and dialogue history as constraints in speech recognition and language understanding.
It has improved speech understanding accuracy by 19% (in terms of concept error rate), and reduced attribute substitution errors (e.g. recognition of a wrong name) by 38%.
The decision rule is implemented in a multi-stage approach as a combination of state-of-the-art speech recognition, partial parsing with an attributed stochastic context-free grammar, and an N-best algorithm which is also described in this paper.
The project of a of «mises en prose» database meant to update Georges Doutrepont's famous work (1939) was launched at 3rd International Congress of the AIEMF (Gargnano, May 2008);
this paper outlines the initative in question, presents the form which will serve as a template to our collaborators, as well as two analytic subject entries: Jean Wauquelin's Manequine and anonymous Belle Hélène de Constantinople.
Many real-world networks can be represented as large graphs.
Reducing the complexity of a graph so that it can be easily interpreted by the human eye is a valuable help to understand and analyze this type of data.
We compare two approaches to grouping of nodes in communities and propose a multi-scale interactive visualization of large graphs based on these hierarchical classifications of nodes that allow us to represent these graphs in a legible and interpretable way.
We then apply our methodology to a network of French-speaking weblogs to quickly illustrate the advantages and disadvantages of this approach.
On the background of mobile, ubiquitous, and pervasive applications, context determination and assignment is a necessary factor to provide IT solutions suited to a user and the user's current situation.
In this paper, context is seen as n-ary relationship.
Context gets embedded into ontologies, which are used to structure application specific knowledge.
We present an integrative, case-based modelling approach for context and context management.
We discuss the incorporation of context-based reasoning and explanation.
And finally, we show how to apply our approach for trust management.
This is an introductory study and critical edition of a short treatise on Latin morphosyntax, as contained in MS London, B.L., Add. 10352.
As a handbook meant for young students supposed to have studied Donatus' Ars minor, this anonymous Latin 'catechism' addresses three main subject areas: constructio, regimen and congruitas.
The treatise is especially interesting for its linguistic terminology (first attestations and/or technical meanings).
Modeling and simulation have long been dominated by equation-based approaches, until the recent advent of agent-based approaches.
To curb the increasing complexity of models resulting from this new approach, the trend has been to oversimplify the models.
Some more descriptive models have still been developed for various phenomenons, but the cognition of agents is too often neglected, despite its great importance in some fields, such as Social and Human Sciences.
The solution that we put forward in this paper is to use BDI agents.
We will show that this is an expressive, realistic yet simple paradigm that thus offers numerous benefits to agent-based simulation.
We propose a robust recursive procedure, based on a weighted recursive least squares (WRLS) algorithm with variable forgetting factor (VFF) and a quadratic classifier with sliding training data set, for identification of non-stationary autoregressive (AR) model of speech production system.
Experimental evaluation is done using the results obtained by analyzing speech signal with voiced and mixed excitation frames.
Experimental results have shown that the proposed robust recursive procedure achieves more accurate AR speech parameter estimates and provides improved tracking performance.
This paper proposes a new approach for the color display of multispectral/hyperspectral images.
The color representation of such data becomes problematic when the number of bands is higher than three, i.e. the basic RGB (Red, Green, Blue) representation is not straightforward.
Here we employ a technique that uses a segmentation map, like an a priori information, and then compute a Factorial Discriminant Analysis (Fischer analysis) in order to allow, at best, a distribution of the information in the color space HSV (Hue, Saturation, Value).
The information collected from the segmentation map (where each pixel is associated with class) has been shown to be advantages in the representation of the images through the results obtained on increasing size image collections in the framework of astronomical images.
This method can easily be applied to other domains such as polarimetric or remote sensing imagery.
Information extraction is driven by an Ontological and Terminological Resource and uses patterns and identification of domain terms and their variations.
It was integrated into an assistant, which helps experts to populate the database.
The initial conception of a model-based analysis synthesis image coding (MBASIC) system is described and a construction method for a three-dimensional (3-D) facial model that includes synthesis methods for facial expressions is presented.
The proposed MBASIC system is an image coding method that utilizes a 3-D model of the object which is to be reproduced.
An input image is first analyzed and an output image using the 3-D model is then synthesized.
A very low bit rate image transmission can be realized because the encoder sends only the required analysis parameters.
Output images can be reconstructed without the noise corruption that reduces naturalness because the decoder synthesizes images from a similar 3-D model.
In order to construct a 3-D model of a person's face, a method is developed which uses a 3-D wire frame face model.
A full-face image is then projected onto this wire frame model.
For the synthesis of facial expressions two different methods are proposed; a clip-and-paste method and a facial structure deformation method.
In the present study the temporal aspects of syllables and words in spoken Italian are investigated.
As for the syllables we tested the effects of syllable composition on the acoustic duration of vowels and consonants.
The effects of word structure on segmental durations were tested by varying the word size and the position of lexical stress.
The results indicate that both syllable structure and word structure have systematic effects on the duration of vowels and consonants.
The effects of syllable structure and word size are primarily anticipatory, and are in the direction required for preserving the total duration of these units.
Two points are made in the present paper: 1. the syllable data suggest that the unit tending to be constant in duration is the temporal interval from vowel onset. 2. The data relative to syllable and word would indicate that the durational variations due to these variables are realized by two different articulatory strategies.
A few observations are made on syllable- and stress-timing.
A major deficiency in state-of-the-art automatic speech recognition (ASR) systems is the lack of robustness in additive and convolutional noise.
The model of auditory perception (PEMO), developed by Dau et al. (T. Dau, D. Püschel, A. Kohlrausch, J. Acoust. Soc. Am. 99 (6) (1996) 3615–3622) for psychoacoustical purposes, partly overcomes these difficulties when used as a front end for automatic speech recognition.
To further improve the performance of this auditory-based recognition system in background noise, different speech enhancement methods were examined, which have been evaluated in earlier studies as components of digital hearing aids.
Monaural noise reduction, as proposed by Ephraim and Malah (Y. Ephraim, D. Malah, IEEE Trans. Acoust. Speech Signal Process. ASSP-32 (6) (1984) 1109–1121) was compared to a binaural filter and dereverberation algorithm after Wittkop et al. (T. Wittkop, S. Albani, V. Hohmann, J. Peissig, W. Woods, B. Kollmeier, Acustica United with Acta Acustica 83 (4) (1997) 684–699).
Both noise reduction algorithms yield improvements in recognition performance equivalent to up to 10 dB SNR in non-reverberant conditions for all types of noise, while the performance in clean speech is not significantly affected.
Even in real-world reverberant conditions the speech enhancement schemes lead to improvements in recognition performance comparable to an SNR gain of up to 5 dB.
This effect exceeds the expectations as earlier studies found no increase in speech intelligibility for hearing-impaired human subjects.
The systematic errors children make in the course of phonological development, like adult production errors and adult phonological processes, can provide evidence of language production mechanisms.
A detailed investigation of the environments in which velar stops are fronted by a phonologically delayed child reveals that fronting is dependent on both word stress and word boundaries; that it shows lexical exceptions; and that it occurs in output only.
This distribution suggests that the child has output lexical representations which are independent of input lexical representations, and that the fronting error occurs in these output representations.
It also suggests that prosodic features are crucial to the identification of articulatory features within these representations.
Such an analysis has implications for theories of lexical access, and for the development of lexical access in children.
The inverse problem for vocal tract shape, area function and articulatory parameters was solved for steady-state vowels by means of an optimization procedure requiring the conditional minimum of work on the part of the articulatory organs.
One to four formant frequencies were used as references.
The shape of the tongue was measured with an X-ray microbeam system for male and female speakers.
The shapes of the vocal tract calculated for the experiments are very similar to the measured shapes.
This paper aims at evaluating the contribution of semantic networks to the computational terminology.
It focuses on the conceptual graph formalism and, at a lower degree, on the description logics.
We nevertheless argue that such formalisms can play a role in the modeling process, for building an intermediate model between the natural language semantics and the operational model to be used in an automatic process.
The purpose of time series analysis is to take into account the fact that glottal cycles are produced sequentially and that relations between neighbouring perturbations exist.
The jitter time series model statistically represents the present perturbation as a weighted sum of past perturbations and random noise.
The model is fitted to observed jitter time series by means of conventional linear methods.
A discriminant analysis of jitter time series extracted from 279 sustained vocoids [a] [i] [u] shows that the jitter features which separately describe the predictable and random components better characterise healthy and dysphonic speakers than a traditional jitter feature.
The conclusion is that the relations between neighbouring cycle length perturbations are an aspect of jitter independent of the scatter of the cycle lengths which is described by conventional jitter features.
In this work we present a new shape from texture algorithm applied to natural scenes analysis.
The originality of this approach is based on the modeling of the structure of the primary visual cortex (V1).
The algorithm is able to deal with a large variety of textures presenting different types of irregularities.
First to sample the amplitude spectra, we present new filters, called log-normal filters, inspired from the complex cells of V1, in replacement of the classical Gabor filters.
These filters appear to be suitable for pattern analysis techniques due to their different theoretical properties, notably their radial frequency profile (adapted to the 1/f frequency profile of natural scenes) and their separability in orientation and frequency.
We then use an estimation method of the local mean frequency applied to natural signals.
This one does not imply the search for the adapted scale for the analysis and takes advantage of the frequencies of the used bank of filters.
Finally, from a local estimation, the orientation and shape are extracted using the geometrical properties of the perspective projection.
The precision of the method is evaluated on different types of textures, both regular and irregular, and on natural scenes.
The presented method allows to obtain favorably comparable results to existing best known methods with a low computational cost.
Finally the model can be adapted to other applications like texture analysis, characteristic points extraction or content-based image indexation.
One of the most striking characteristics typical of the speech of Broca's aphasics is its agrammatism — the omission of 'function' words and inflectional morphemes.
Agrammatism is generally viewed as being symptomatic of a syntactic deficit.
We argue here that such an account lacks grammatical systematicity, and that the only uniform and systematic interpretation of this deficit is in terms of phonological structure.
A natural class consisting of 'function' words and some bound morphemes can be defined with reference to the junctural properties of sentences which characterize phonological words.
It is this class of elements which tend to be omitted in agrammatic speech.
The goal of this paper is not only to provide an hypothesis for the interpretation of one aphasic syndrome, but also to test and illustrate the efficacy of paying close attention to substantive universals of grammatical structure in proposing accounts of linguistic deficits.
A modified SEARMA method is proposed for estimating the speech spectrum in the presence of colored background noise.
The following assumptions are used in developing the analysis.
The speech production process is represented by an autoregressive moving-average (ARMA) model.
Following these assumptions, the process during speech activity can be represented by an extended ARMA model.
In this formulation, unique estimation of AR parameters of the vocal tract transfer function is always possible if the MA parameters of the noise process can be estimated separately, but the estimation of MA parameters of the speech production process requires further assumption of a high SNR.
The validity of the proposed method is demonstrated by spectral estimation of both synthetic and natural speech sounds in the presence of additive colored noise, and by comparing the results with those obtained by the LPC method.
A method is presented to provide a useful searchable index for spoken audio documents.
The task differs from the traditional (text) document indexing, because large audio databases are decoded by automatic speech recognition and decoding errors occur frequently.
The idea in this paper is to take advantage of the large size of the database and select the best index terms for each document with the help of the other documents close to it using a semantic vector space.
First, the audio stream is converted into a text stream by a speech recognizer.
Then the text of each story is represented in a vector space as a document vector which is the normalized sum of the word vectors in the story.
A large collection of such document vectors is used to train a self-organizing map (SOM) to find latent semantic structures in the collection.
As the stories in spoken news are short and will include speech recognition errors, smoothing of the document vectors using the semantic clusters determined by the SOM is introduced to enhance the indexing.
The application in this paper is the indexing and retrieval of broadcast news on radio and television.
Test results are given using the evaluation data from the text retrieval conference (TREC) spoken document retrieval (SDR) task.
This paper proposes three noise adaptation algorithms which allow improvements in the performance of speech recognition systems under noisy conditions.
They are VQ-based feature mapping techniques which hierarchically transform noisy feature vectors into clean feature vectors.
The first algorithm was originally used for unsupervised speaker adaptation.
It is based on hard clustering and iteratively adapts the noisy input data to a small set of codebooks created from clean data.
The second algorithm is a modified version of the first one.
It redefines the mapping function using the notion of cluster scope.
The last algorithm proposes a fuzzy clustering technique as a substitute to the original hard clustering technique.
In the NATO digit task, these algorithms significantly improve the performance of CRIM's speech recognition system.
EXPULSE method lifts the main drawback of the classical high resolution spectral analysis methods (MUSIC, MINORM, …) which have a poor robustness with respect to an unreliable knowledge of the number of sources.
Its novelty stems from the modeling of the periodogram of complex sinusoids embedded in an additive noise, as the convolution of a perfectly known kernel (depending upon the window) and a compound Bemoulli—Gaussian process, plus a noise.
The discrete frequencies where the Bemoulli process takes 1 values locate the sinusoids; the gaussian process describes the amplitudes.
Since climate change impact studies have shown that the marine environment could be greatly weakened by the disappearance of some flora and fauna species and the rapid aging of underwater infrastructure, new, efficient and robust observation tools are required.
In this paper we propose acoustic cameras as innovative tools for acquiring underwater data along with a conceptual framework that allows an accurate and complete three-dimensional reconstruction implementation of the underwater environment using image sequences acquired by those acoustic cameras.
Examples of information extracted from the acquired acoustic data that participate in such a 3D reconstruction are also provided as well as some preliminary results.
A data base of around 6500 syllables and their component segments were analysed to describe and develop a model of segment and syllable duration for Australian English.
Segment duration was analysed according to prosodic context.
Syllables were labelled and analysed according to their prosodic context, length (number of segments), and nature of syllabic peak.
Syllable duration was modelled using a three-layer neural network that was trained and tested on different portions of the database.
Segment durations were stretched or compressed to fit the network-assigned syllable duration frame.
This relatively simple syllable model was able to account for nearly 80% of syllable-level durational variance observed in the database.
The performance of present-day automatic speech recognition (ASR) systems is seriously compromised by levels of acoustic interference (such as additive noise and room reverberation) representative of real-world speaking conditions.
Studies on the perception of speech by human listeners suggest that recognizer robustness might be improved by focusing on temporal structure in the speech signal that appears as low-frequency (below 16 Hz) amplitude modulations in subband channels following critical-band frequency analysis.
A speech representation that emphasizes this temporal structure, the “modulation spectrogram”, has been developed.
Visual displays of speech produced with the modulation spectrogram are relatively stable in the presence of high levels of background noise and reverberation.
Using the modulation spectrogram as a front end for ASR provides a significant improvement in performance on highly reverberant speech.
When the modulation spectrogram is used in combination with log-RASTA-PLP (log RelAtive SpecTrAl Perceptual Linear Predictive analysis) performance over a range of noisy and reverberant conditions is significantly improved, suggesting that the use of multiple representations is another promising method for improving the robustness of ASR systems.
This paper investigates a weighted LPC analysis of voiced speech.
In view of the speech production model, the weighting function is either chosen to be the short-time energy function of the preemphasized speech sample sequence with certain delays or is obtained by thresholding the short-time energy function.
In this method, speech samples are selectively weighted based on how well they match the speech production model.
Therefore, the estimates of the LPC coefficients obtained by this novel LPC analysis are more accurate than those obtained from the conventional LPC analysis.
They are also less sensitive to the values of the fundamental frequency than conventional LPC.
The accuracy of speech recognition systems is known to be affected by fast speech.
If fast speech can be detected by means of a measure of speaking rate, the acoustic as well as language models of a speech recognition system can be adapted to compensate for fast speech effects.
We have studied several measures of speaking rate which have the advantage that they can be computed prior to speech recognition.
The proposed measures have been compared with conventional measures, viz., word and phone rate on the TIMIT database.
Some of the proposed measures have significant correlations with phone rate and vowel duration.
We have shown that the mismatch between actual and expected durations of test vowels reduces if the vowel duration models are adapted to speaking rate, as estimated by the proposed measures.
These measures can be computed from features commonly employed in speech recognition, do not entail significant additional computational load and do not need labeling or segmentation of unknown utterance in terms of linguistic units.
A statistical image model is used for segmenting polarimetric synthetic aperture radar (SAR) data into regions of homogeneous polarimetric backscatter characteristics.
Optimal region labels of the data are those which maximize the a posteriori distribution of the region labels given the polarimetric complex data.
Implementation of the MAP (Maximun A Posteriori) technique is accomplished either by the modified deterministic ICM (Iterative Conditional Modes) algorithm or by the stochastic SA (Simulated Annealing) algorithm.
Unsupervised parameter estimation procedures are obtained either by the CMFMAP-NSO (unsupervised fuzzy partition - optimal number of classes) and CMF-NS or by SEM (Stochastic Estimation Maximization) algorithm.
Results, using fully polarimetric SAR forest data, obtained by the CMFMAP-NSO following by the ICM algorithm with a K_ distribution model are quite satisfactory.
Six types of speech synthesis were evaluated for comprehensibility: standard linear predictive coding analysis/ resynthesis; pitch synchronous analysis/resynthesis; pitch synchronous multi-pulse analysis/resynthesis; and three PSOLA (pitch synchronous overlap-and-add) techniques.
The relative comprehensibility of the synthesis types was tested by using the synthesised speech to convey information that subjects needed in order to perform a diagram-based multiple-choice task.
The application of learning theory to bayesian networks is still uncomplete and we propose a contribution, especially through the use of covering numbers.
We deduce multiple corollaries, among which a nonfrequentist approach for parameters learning and a score taking into account a measure of structural entropy that has never been taken into account before.
We then investigate the algorithmic aspects of our theoretical solution, based on BFGS and adaptive refining of gradient calculus.
In order to facilitate the access and the usage to the general public of the rapidly expanding applications and services, particularly on the internet, new assisting tools are needed with two main requirements: naturalness and acceptability.
Conversational agents are a promising approach but assisting agents cannot rely only on rational reasoning over the structure and the functioning of the assisted applications.
They must also express behavioral reactions that involve social relationships, character traits and affects.
In the first part of this paper, we propose a flexible framework for modeling the relationships between the rational and behavioral reactions of an assisting agent.
Then this framework is used to support a first case-study, based on cognitive biases.
To date, speech recognition systems have been applied in real world applications in which they must be able to provide a satisfactory recognition performance under various noise conditions.
However, a mismatch between the training and testing conditions often causes a drastic decrease in the performance of the systems.
The viability of the suggested technique was verified in various experiments using different background noises and microphones.
In a multi-environment speaker-independent connected digit recognition task, the proposed method reduced the error rates by over 16%.
This paper reviews past work comparing modern speech recognition systems and humans to determine how far recent dramatic advances in technology have progressed towards the goal of human-like performance.
Comparisons use six modern speech corpora with vocabularies ranging from 10 to more than 65,000 words and content ranging from read isolated words to spontaneous conversations.
Error rates of machines are often more than an order of magnitude greater than those of humans for quiet, wideband, read speech.
Machine performance degrades further below that of humans in noise, with channel variability, and for spontaneous speech.
Humans can also recognize quiet, clearly spoken nonsense syllables and nonsense sentences with little high-level grammatical information.
These comparisons suggest that the human-machine performance gap can be reduced by basic research on improving low-level acoustic-phonetic modeling, on improving robustness with noise and channel variability, and on more accurately modeling spontaneous speech.
This paper is a contribution to automatic speaker recognition.
It considers speech analysis by linear prediction and investigates the recognition contribution of its two main resulting components, namely the synthesis filter on one hand and the residue on the other hand.
This investigation is motivated by the orthogonality property and the physiological significance of these two components, which suggest the possibility of an improvement over current speaker recognition approaches based on nothing but the usual synthesis filter features.
Specifically, we propose a new representation of the residue and we analyse its corresponding recognition performance by issuing experiments in the context of text-independent speaker verification.
Experiments involving both known and new methods allow us to compare the recognition performance of the two components.
First we consider separate methods, then we combine them.
Each method is tested on the same database and according to the same methodology, with strictly disjoint training and test data sets.
The results show the usefulness of the residue when used alone, even if it proves to be less efficient than the synthesis filter.
However, when both are combined, the residue shows its true relevance.
It achieves a reduction of the error rate which, in our case, went down from 5.7% to 4.0%.
The possibility to vary speaker type and speaking style will be a feature of the next generation text-to-speech systems.
Already today, the need for these possibilities is apparent in dialogue systems and when speech synthesis is used as prostheses for persons with a communication handicap.
Much of the information needed is not yet available.
In this contribution we argue that speech synthesis itself is an efficient tool to study and understand the variability in speech.
Some different methods are reviewed, representing both analysis/synthesis techniques and signal manipulations as well as text-to-speech.
The main emphasis is, on the work at KTH, to study the variation of speaker and speaking styles in the context of our text-to-speech system.
This paper proposes an original method for robotic programming based on bayesian inference and learning.
This method formally deals with problems of uncertainty and incomplete information that are inherent to the field.
Indeed, the principal difficulties of robot programming comes from the unavoidable incompleteness of the models used.
We present the formalism for describing a robotic task as well as the resolution methods.
We illustrate it by programming a surveillance task with a mobile robot: the Khepera.
In order to do this, we use generic programming resources called “descriptions”.
We show how to define and use these resources in an incremental way (reactive behaviors, sensor fusion, situation recognition and sequences of behaviors) within a systematic and unified framework.
This paper presents a new frequency-domain approach to implement an adaptive postfilter for enhancement of noisy speech.
The postfilter is described by a set of DFT coefficients which suppress noise in the spectral valleys and allow for more noise in formant regions which is masked by the speech signal.
First, we perform an LPC analysis of the noisy speech and calculate the log magnitude spectrum of the input speech.
After identifying the formants and valleys (by a new method), the log magnitude spectrum is modified to obtain the postfilter coefficients.
The filtering operation is also done in the frequency domain through an FFT and an overlap-add strategy to get the postfiltered speech.
Experimental results on 8-kHz-sampled speech show that this new frequency-domain approach results in enhanced speech of better perceptual quality than obtained by a time-domain method.
This new method is especially efficient in eliminating high frequency noise and in preserving the weaker, high frequency formants in sonorant sounds.
Previous research suggested that rhythmic expectations could play a role in languages contrasting stressed syllables with unstressed ones, whereas languages without such a contrast and with a clear syllabic structure, such as French, would be processed according to a syllable-based procedure.
Lexical parsing of bisyllabic words composed of two monosyllabic words are studied.
Two experiments examine the effects of usual and reverse metrical patterns on segmentation.
The usual iambic pattern produces, more often than not, recognition of bisyllables whereas lexical parsing is not influenced by monosyllable frequency and syllabic structure.
The trochaïc pattern strongly increases the amount of segmentation.
In Experiment 2, focusing subjects' attention on the timing structure strengthens these effects.
Consequently, French subjects use a metrical segmentation strategy.
By contrast, the processing of spondees (Experiment 3) shows an effect of structural parameters on parsing and suggests the use of a syllable-based segmentation procedure when rhythmic information is absent.
Implications for speech recognition models are discussed.
These last years, there were many studies on the problem of conflict coming from information fusion, especially in evidence theory.
We can summarize the solutions for managing the conflict in three different approaches: first, you can try to suppress or reduce the conflict before the combination step; secondly, you can manage the conflict in order to give no influenence to the conflict in the combination step, and then take into account the conflict in the decision step; thirdly, you can take into account the conflict in the combination step.
The first approach is certainly the better, but not always feasible.
It is difficult to say which approach is the best between second and third.
However, the most important is the produced results in applications.
We propose here a new combination rule that distributes the conflict proportionally on the elements giving this conflict.
We compare these different combination rules on real data in Sonar imagery and Radar target classification.
This paper reports the design of a command-based speech interface for an answering machine or a voice mail system.
Automatic speech recognition was integrated in order to facilitate the remote control and the retrieval of voice messages from any telephone in a speech-only dialogue.
The design goal was that consumers would perceive the speech interface as a benefit compared with the common touch-tone interface.
In this paper we will first describe the speech technology underlying the system.
Then it will be shown how, based on this technology, the user interface was designed in a top-down approach.
We started with the development of a concept and tested it by means of a Wizard-of-Oz simulation.
After refining the concept in parallel design, it was implemented in a high-fidelity prototype.
By means of qualitative user testing the design was improved in three iteration steps.
The achievement of the design goal was finally verified with user tests in two countries.
It is well known that the introduction of acoustic background distortion and the variability resulting from environmentally induced stress causes speech recognition algorithms to fail.
In this paper, several causes for recognition performance degradation are explored.
It is suggested that recent studies based on a Source Generator Framework can provide a viable foundation in which to establish robust speech recognition techniques.
This research encompasses three inter-related issues: (i) analysis and modeling of speech characteristics brought on by workload task stress, speaker emotion/stress or speech produced in noise (Lombard effect), (ii) adaptive signal processing methods tailored to speech enhancement and stress equalization, and (iii) formulation of new recognition algorithms which are robust in adverse environments.
An overview of a statistical analysis of a Speech Under Simulated and Actual Stress (SUSAS) database is presented.
This study was conducted on over 200 parameters in the domains of pitch, duration, intensity, glottal source and vocal tract spectral variations.
These studies motivate the development of a speech modeling approach entitled Source Generator Framework in which to represent the dynamics of speech under stress.
This framework provides an attractive means for performing feature equalization of speech under stress.
In the second half of this paper, three novel approaches for signal enhancement and stress equalization are considered to address the issue of recognition under noisy stressful conditions.
The first method employs (Auto:I,LSP:T) constrained iterative speech enhancement to address background noise and maximum likelihood stress equalization across formant location and bandwidth.
The second method uses a feature enhancing artificial neural network which transforms the input stressed speech feature set during parameterization for keyword recognition.
The final method employs morphological constrained feature enhancement to address noise and an adaptive Mel-cepstral compensation algorithm to equalize the impact of stress.
Recognition performance is demonstrated for speech under a range of stress conditions, signal-to-noise ratios and background noise types.
In this paper, we present a reinforcement learning approach for multi-agent communication in order to learn what to communicate,when and to whom.
This method is based on introspective agents, that can reason about their own actions and data so as to construct appropriate communicative acts.
We propose an extention of classical reinforcement learning algorithms with multi-agent communication.
We show how communicative acts and memory can solve non-markovity and asynchronism issues MAS.
The motion-compensated hybrid DCT/DPCM algorithm has been successfully adopted in various video coding standards, such as H.261, H.263, MPEG-1 and MPEG-2.
However, its robustness is challenged in the face of an inadequate bit allocation, either globally for the whole video sequence, or locally as a result of an inappropriate distribution of the available bits.
In either of these situations, the trade-off between quality and the availability of bits results in a deterioration in the quality of the decoded video sequence, both in terms of the loss of information and the introduction of coding artifacts.
These distortions are an important factor in the fields of filtering, codec design, and the search for objective psychovisual-based quality metrics; therefore, this paper presents a comprehensive analysis and classification of the numerous coding artifacts which are introduced into the reconstructed video sequence through the use of the hybrid MC/DPCM/DCT video coding algorithm.
Artifacts which have already been briefly described in the literature, such as the blocking effect, ringing, the mosquito effect, MC mismatch, blurring, and color bleeding, will be comprehensively analyzed.
Additionally, we will present artifacts with unique properties which have not been previously identified in the literature.
The next generation of text-to-speech systems will have to be more sensitive to sociolinguistic 'style' variables.
In order to assist in the adaptation of synthesis to a wider range of contexts, this article examines several sociolinguistic parameters which have been shown to influence the realization of negatives in actual discourse, analyzing their effects on the realization of negatives in English prose readings.
Consistent with the results found in an earlier study, the analysis shows that pitch prominence on negatives is not common in read prose passages, and is even less common in read dialogue.
The results show a surprising absence of conformity with 'theoretical' linguistic expectations, highlighting the necessity for consideration of register as an important variable for speech synthesis.
When we communicate through (natural) languages, we do not explicitly say everything.
Both the speaker and the hearer utilize information available from the utterance situation, which includes the mental states of the speaker and the hearer.
Interesting cases are frequently observed in the use of Japanese (in dialogue situations).
Syntactic (or configurational) constraints of Japanese are weaker than those of English, in the sense that the speaker may omit almost any element in a sentence.
In this paper we present a mechanism of the hearer in the light of situated reasoning and show how the missing information can be supplied from the situation.
Although we believe that the model captures the essential nature of human communication, it may be too naive as a model of human cognition.
Rather, the model is intended to be used in the design of software agents that communicate with each other in a mechanical but flexible and efficient way.
Evidence of syntactic control on the prosodic structure of an utterance in French has been exploited in various generative models of prosody.
Nevertheless models without syntactic knowledge based only on rhythmic constraints can generate acceptable prosodic structures.
The model presented here shows that syntax-driven and rhythm-driven strategies could be extreme cases of a more complex model which integrates both syntactic and rhythmic constraints.
This generative model has been integrated into various text-to-speech systems.
Preliminary perceptual comparative tests published elsewhere have shown an improvement in quality over a well-known syntax-driven model.
This paper describes the implementation of a new parametric model of the glottal geometry aimed at improving male and female speech synthesis in the framework of articulatory analysis synthesis.
It is embedded in an articulatory analysis synthesis system (articulatory speech mimic).
To introduce naturally occurring details in our synthetic glottal flow waveforms, we modelled two different kinds of leakage: a “linked leak” and a “parallel chink”.
While the first is basically an incomplete glottal closure, the latter models a second glottal duct that is independent of the membranous (vibrating) part of the glottis.
Characteristic for both types of leaks is that they increase de-flow and source/tract interaction.
A linked leak, however, gives rise to a steeper roll-off of the entire glottal flow spectrum, whereas a parallel chink decreases the energy of the lower frequencies more than the higher frequencies.
In fact, for a parallel chink, the slope at the higher freqencies is more or less the same as in the no-leakage case.
Environmental robustness for automatic speech recognition systems based on parameter modification can be accomplished in two complementary ways.
One approach is to modify the incoming features of environmentally-degraded speech to more closely resemble the features of the (normally undegraded) speech used to train the classifier.
The other approach is to modifying the internal statistical representations of speech features used by the classifier to more closely resemble the features representing degraded speech in a particular target environment.
This paper attempts to unify these two approaches to robust speech recognition by presenting several techniques that share the same basic assumptions and internal structure while differing in whether they modify the features of incoming speech or whether they modify the statistics of the classifier itself.
We present the multivaRiate gAussian-based cepsTral normaliZation (RATZ) family of algorithms which modify incoming cepstral features, along with the STAR (STAtistical Reestimation) family of algorithms, which modify the internal statistics of the classifier.
Both types of algorithms are data driven, in that they make use of a certain amount of adaptation data for learning compensation parameters.
The algorithms were evaluated using the SPHINX-II speech recognition system on subsets of the Wall Street Journal database.
While all algorithms demonstrated improved recognition accuracy compared to previous algorithms, the STAR family of algorithms tended to provide lower error rates than the RATZ family of algorithms as the SNR was decreased.
The hardware described and test results presented correspond to the equipment tested in the European contest at Turin, Italy.
The codec, whilst giving good speech quality, also offers several important features including low delay, low computational complexity and a good tolerance to transmission errors.
The codec employs an 8-band subband coding algorithm incorporating backward-adaptive quantisation and backward-adaptive prediction.
The hardware implementation described uses a pair of digital signal processing devices, thereby giving a very compact realisation.
The results of both subjective and objective tests are also presented.
Prosodic relations in prose, poetry and music are discussed with an emphasis on durational properties.
In order to gain a deeper understanding of speech prosody, we are presently engaged in a comparison of the timing relations in such activities as the reading of poetry and music performance, where there usually is a strong and obvious rhythmic patterning of the produced sound sequences.
Also there are interesting parallels to be drawn by comparing the formal notations of prose, poetry and music.
Generally, there are no simple relations between abstract notations and performance, and moreover, notations have varied with tradition and particular needs.
However, it is a challenge to tie descriptive systems closer to common human constraints in production and perception.
In this paper the harmonic features based on the harmonic decomposition of the Hildebrand–Prony line spectrum are introduced.
A Hildebrand–Prony method of spectral analysis was applied because of its high resolution and accuracy.
Comparative tests with the LP and LP-cepstral features were made with 50 speakers from the Slovene database SNABI (isolated words corpus) and 50 speakers of the German database BAS Siemens 100 (utterances of sentences).
With both databases the advantages of the harmonic features were noticed especially for the speaker identification while for the speaker verification the harmonic features have performed better on the SNABI database and as good as the LP cepstral features on the BAS Siemens 100 database.
This article introduces a methodology for quantifying the distortion introduced by a low or medium bit-rate speech coder.
Since the perceptual acuity of a human being determines the precision with which speech data must be processed, the speech signal is transformed onto a perceptual-domain (PD).
This is done using Lyon's cochlear (auditory) model whose output provides the probability-of-firing information in the neural channels at different clock times.
In our present approach, we use a hidden Markov model to describe the basic firing/non-firing process operative in the auditory pathway.
We consider a two-state fully-connected model of order one for each neural channel; the two states of the model correspond to the firing and non-firing events.
Assuming that the models are stationary over a fixed duration, the model parameters are determined from the PD observations corresponding to the original signal.
Then, the PD representations of the coded speech are passed through the respective models and the corresponding likelihood probabilities are calculated.
These probability scores are used to define a cochlear hidden Markovian (CHM) distortion measure.
This methodology considers the temporal ordering in the neural firing patterns.
The CHM measure which utilizes the contextual information present in the firing pattern shows robustness against coder delays.
We present here a whole operational prototype for the compression and recognition of dance gestures from contemporary ballet.
Our input data are motion trajectories followed by the joints of a dancing body provided by a motion-capture system.
We propose a suitable tool for nonuniform sub-sampling of spatio-temporal signals.
The key of our approach is the use of polygonal approximation to provide a compact and efficient representation of motion trajectories.
Our dance gesture recognition method involves a set of Hidden Markov Models (HMMs), each of them being related to a motion trajectory followed by the joints.
We have validated our recognition system on 12 fundamental movements from contemporary ballet performed by 4 dancers.
Information Retrieval techniques make use of terms that are automatically extracted from documents; these terms are used to give information access.
In this paper we propose an approach to enrich semantically this extraction by adding knowledge from thesauri.
More specifically, the methodology we promote in this paper aims at transforming a thesaurus into a domain ontology which will then be used to semantically index documents (indexes are concepts rather than terms).
We also propose techniques that implement this transformation as well as an evaluation in the field of astronomy.
The articulatory pattern observed by electropalatography cannot be interpreted simply as the concatenation of assimilated segments.
Between 1399 and 1400 Christine de Pizan wrote her Epistre Othea.
Sixty years later Jean Miélot rewrote and revised this successful work for the court of the Dukes of Burgundy.
As well as pointing up the particularity of Jean Miélot's revision over his contemporaries like Jean Wauquelin or David Aubert the present article attempts to contextualize both the textual source and the textual target in order to answer the question of the emergence of Miélot's revision but also seeks possible explanation for its absence of dissemination.
Finally we note that the traditional textual and philological approach does not entirely explain the reception of a text within a context and that this traditional approach should also be coupled with a factual and contextualized approach.
Transitional information, represented by first-order orthogonal coefficients which result from the orthogonal polynomials expansion of the cepstral contours, describes the speed of variation of the speech spectra.
Cepstral and first-order orthogonal coefficients vectors were employed separately as well as joined into a single feature vector, in classical DTW-based verification algorithms.
Investigations on a population of 22 speakers (high-quality speech) showed that the elimination of the time-invariant spectral components from the speech features, taking place when performing cepstral normalization or computing first-order orthogonal coefficients, brings a substantial reliability improvement.
Furthermore, transitional information is practically as effective as instantaneous information, whereas combining both kinds of information does not lead to further improvement.
This paper describes a system for speech analysis based on known aspects of human auditory processing.
A goal of this work is to provide signal transformations which preserve and enhance perceptually important speech features.
A computer model of the peripheral auditory system incorporating phase-locking, two-tone suppression and additive adaptation effects is presented.
Recent research in auditory physiology has highlighted the important contribution which these phenomena make to normal speech perception.
The proposed model is able to emphasise dynamic spectral regions of the kind found throughout normal conversational speech.
In this paper quality evaluation procedures for hands-free telephones are described with reference to some examples.
Instrumental measurements, showing procedures for instrumental evaluation, are shown using the examples of the different hands-free telephones.
We present in this paper a genetic search strategy for a search engine.
We begin by showing that important relations exist between Web statistical studies, search engines based on agent approach, and standard techniques in optimization: the web is a graph which can be searched for relevant information with an evaluation function and with operators based on création or local exploration.
It is then straightforward to define an évaluation function that is a mathematical formulation of the user request and to define a steady State genetic algorithm that evolves a population of pages with spécifie operators.
The creation of individuals is performed bv querving standaid search engines.
The mutation operator consists in exploring the neighborhood of a page thanks to the hyperlinks.
We present a comparative evaluation which is performed with the same protocol as used in optimization.
The paper presents a model for the construction of an artificial agent that can express performatives through facial expression.
The performative of a speech act or communicative act is the particular communicative intention a Sender has to one's Addressee, the way one wants to socially relate oneself to the interlocutor.
Performatives are decomposed both on the meaning and on the signal side: on the meaning side, a performative is represented as a cluster of cognitive units, that in turn include subclusters mentioning the Sender's general goal (informing, asking, requesting), the power relationship between Sender and Addressee, the Sender's affective state, and further information peculiar of specific performatives; on the signal side, facial expressions are decomposed into Action Units.
The proposed system computes the appropriate performative of one's communicative acts through consideration of the context of communication, particularly of the Addressee's cognitive capacity, social relationship and personality traits, and then expresses the computed performatives through 3D facial displays.
The paper is focussed on the vocalic differences between spontaneous and laboratory speech in Spanish.
The first and second formants of 954 vowel utterances (477 in laboratory and 477 in spontaneous speech) have been measured.
They constitute clusters in the F 1/F 2 space.
The paper describes inter- and intra-cluster variabilities caused by communication situations changes.
In spontaneous speech, the formants values show (1) a marked schwa-tendency; (2) increasing intra-cluster variability.
Both phenomena result in lowered differenciation of the sounds in spontaneous speech.
An automatic text-independent speaker recognition system suitable for identification and verification purposes is presented.
This database consists of L prototypes for every speaker, representing the vowels of the language, which are estimated from L vowel clusters.
These are formed by applying a modified k-means algorithm on the patterns extracted from the vowels of training utterances.
The patterns of the training utterances are stored in a training database to be used for updating the reference data of the system.
The system was tested over a period of four months with a population of 15 male and female speakers with non-correlated training and test data.
Its accuracy proved to be satisfactory (91.39% for verification, 90.19% for closed-set identification, 95.28% for open-set identification), considering that the training utterances per speaker do not exceed 50 sec and the test utterances have a duration of 1.3 sec on the average.
The accuracy is substantially increased when increasing the length of the test utterance (e.g. 93.75% verification accuracy for test utterances having an average duration of 4 sec).
Additional advantages of the system are the small memory requirements and the fast response.
The goal is to determine the effective membership of points of contour obtained by active contour method to the cortical.
Several parameters associated to the points are taken into account (gray level, mean of gray leveland standard deviation on a neighbourhood, outdistances between points belonging to close slices).
The architecture is based on the formalism of the evidence.
We discuss the results obtained, the validity of them and we propose further objectives of this work.
The time-frequency analysis of magnetic signals, generated by ferromagnetic objects, is used to extract a robust discriminant parameter set for their classification.
After the feature selection phase, an extensive study is performed in order to validate the most appropriate classifier structure, in terms of the correct classification rate and the generalization ability.
If SVM (Support Vector Machines) are now considered as one of the best learning methods, they are still considered as slow.
We chose to implement this algorithm with Matlab environment since it is user-friendly and efficient - it uses the ATLAS library.
The comparison to the state of the art in this field, SMO shows that in some cases, our solution is faster and less complex.
In this paper we describe a new method of medical image registration.
We formulate the registration problem as a minimization problem involving robust estimators.
We propose an efficient hierarchical optimization framework which is both multiresolution and multigrid in order to accelerate the algorithm and to improve the quality of the estimation.
The benefits of this method are demonstrated on real data and its performances are objectively evaluated on simulated data.
From a corpus of CVCVCV nonsense words where the consonant was [b] or [m] and the vowel [a i u], the behavior of the orbicularis oris superior and levator veli palatini muscles was studied.
The importance of preplanning of the utterance (the role of the initial position functioning as a reference) and of intrasegmental timing reorganization have been assessed within the framework of a motor encoding theory based on the concepts of “sequencing” and “phasing”.
Furthermore, our data confirm the idea that muscular synchronization constitutes a fairly basic rule of speech production.
In many domains (biology, medicine, psychology, etc.), efficient text mining tools could help the expert.
In order to obtain a usable tool, an expert of the domain must control the varions text mining steps.
The approach proposed in this paper consists in extracting the association rides specifie to the field starting from a set of specialized and homogeneous texts.
Our approach is made up of varions stages in which the expert's rôle is essential.
The first stage consists in extracting the ternis in the texts and associating them to a concept, i.e. a set of terms having the saine semantic.
Using this new spécifie knowledge, the initial corpus is transformed into a matrix.
At the last stage of our approach, this matrix is discretised in order to extract association rides.
This paper adresses the inverse reinforcement learning (IRL) problem, that is inferring a reward for which a demonstrated expert behavior is optimal.
We introduce a new algorithm, SCIRL, whose principle is to use the so-called feature expectation of the expert as the parameterization of the score function of a multiclasse classifier.
This approach produces a reward function for which the expert policy is provably near-optimal.
Contrary to most of existing IRL algorithms, SCIRL does not require solving the direct RL problem.
Moreover, with an appropriate heuristic, it can succeed with only trajectories sampled according to the expert behavior.
This is illustrated on a car driving simulator.
This paper introduces a new approach of watermarking for copyright protection.
The goal of the method is to hide signatures composed of w segments of r bits in digital images.
The framework itself is founded upon a wavelet transformed domain, and an additive embedding rule using products of orthogonal basis functions.
We will show how the choice of different kinds of orthogonal functions allows to improve the robustness of the watermarking scheme to signal processing or geometric attacks.
As a basic coder structure, the long-term predictor is implemented as an adaptive codebook, while a sparse Gaussian codebook with non-overlapping vectors is used for the stochastic excitation.
In order to meet the complexity requirements, several methods for efficient codebook search are adopted.
With these methods, it is shown that the computational effort for the basic coder structure can be reduced to 12.4 MIPS with a 7 bit stochastic codebook.
A two-stage hierarchical search through the adaptive codebook is investigated.
This search method reduces the computational effort further although at the cost of a small degradation in coder performance.
The speech quality with the basic CELP structure is judged to be comparable to the G.722 coder at 48 kbit/s.
Sacadeau-Software is a decision support tool for agronomists working on water pollution in catchments, and for people in charge of the management of these catchments.
This tool focuses on water contamination by pesticides applied on corn.
It builds on a biophysical model representing the transfer processes ofpesticides through the catchment and on a decisional model representing the farming techniques for maïs culture.
Sacadeau-Software allows to run simulations throughout a watershed and obtain the transfer rate of pollutants through the catchment.
Classiﬁcation rules characterizing the sub-parts of the watershed with pollution, and the sub-parts without pollution, are automatically learned from the simulations.
A visualization tool enables to relate the learned rules to the examples characterized by these rules.
Finally, a action recommendation tool analyzes the learned rules and proposes actions that improve a situation of pollution.
Speech production variations due to perceptually induced stress contribute significantly to reduced speech processing performance.
One approach for assessment of production variations due to stress is to formulate an objective classification of speaker stress based upon the acoustic speech signal.
This study proposes an algorithm for estimation of the probability of perceptually induced stress.
It is suggested that the resulting stress score could be integrated into robust speech processing algorithms to improve robustness in adverse conditions.
First, results from a previous stress classification study are employed to motivate selection of a targeted set of speech features on a per phoneme and stress group level.
Analysis of articulatory, excitation and cepstral based features is conducted using a previously established stressed speech database (Speech Under Simulated and Actual Stress (SUSAS)).
Stress sensitive targeted feature sets are then selected across ten stress conditions (including Apache helicopter cockpit, Angry, Clear, Lombard effect. Loud, etc.) and incorporated into a new targeted neural network stress classifier.
Second, the targeted feature stress classification system is then evaluated and shown to achieve closed speaker, open token classification rates of 91.0%.
Finally, the proposed stress classification algorithm is incorporated into a stress directed speech recognition system, where separate hidden Markov model recognizers are trained for each stress condition.
An improvement of +10.1% and +15.4% over conventionally trained neutral and multi-style trained recognizers is demonstrated using the new stress directed recognition approach.
Listeners are able to tell apart read-aloud and spontaneously produced speech.
Prosody appears to be important for this perceptual distinction.
In this paper, the importance of the distribution and realization of prosodic boundaries is investigated.
Recordings were made of five male speakers, spontaneously producing so-called instruction monologues.
Transcripts of these monologues were read aloud by the same speakers.
A perception experiment was carried out to obtain classification scores for isolated utterances selected from the spontaneous and read material.
Auditory prosodic transcriptions were made of the entire spontaneous and read monologues, assessing the distribution and realization of underlying prosodic boundaries in both speech types.
The underlying prosodic structure was assessed by means of an automatic text-to-speech system.
Observed differences in the production of prosodic boundaries in the spontaneous and read material are related to the perceptual classification scores by means of a multiple regression analysis.
This paper describes an accurate and efficient algorithm for very-large-vocabulary continuous speech recognition.
It is based on a two-stage LR parser with hidden Markov models (HMMs) as phoneme models.
To improve recognition accuracy, it uses the forward and backward trellis likehood.
To improve search efficiency, it uses adjusting windows and merges candidates that have the same allophonic phoneme sequences and grammatical state, and then merges candidates at the meaning level.
This algorithm was applied to a telephone directory assistance system that contains more than 70,000 subscribers (about 80,000 words) to evaluate its speaker-independent speech recognition capabilities.
For eight speakers, the algorithm achieved a speech understanding rate of 65% for spontaneous speech.
The results show that the system performs well in spite of the large word perplexity.
This paper also describes a multi-modal dialog system that uses our large-vocabulary speech recognition algorithm.
The use of signal transformations is a necessary step for feature extraction in pattern recognition systems.
These transformations should take into account the main goal of pattern recognition: the error-rate minimization.
In this paper we propose a new method to obtain feature space transformations based on the Minimum Classification Error criterion.
The goal of these transformations is to obtain a new representation space where the Euclidean distance is optimal for classification.
The proposed method is tested on a speech recognition system using different types of Hidden Markov Models.
The comparison with standard pre-processing techniques shows that our method provides an error-rate reduction in all the performed experiments.
We review different methods for the second task, emphasizing the advantages and disadvantages of the linear predictive (LPC) diphone approach.
Diphones require more memory represent all possible spectral transitions between pairs of phonemes, but they directly capture many of the coarticulation effects that must otherwise be modeled in phonemic synthesis.
Relatively simple interpolation is allowed due to the similarity of spectra at diphone boundaries.
This paper addresses the problem of predicting the quality of telephone speech.
Starting from a definition of quality, which takes communicative as well as service-related factors into account, a new classification scheme for prediction models is proposed.
It considers input and output parameters, the network components and application area the model is used for, as well as the psychoacoustic and judgment-related bases.
According to this scheme, quality prediction models can be classified into signal-based comparative measures, network planning models and monitoring models.
Whereas signal-based approaches have been described extensively in literature, this paper discusses the latter two approaches in detail.
The underlying psychoacoustic properties of two network planning models, the E-model and the SUBMOD model, are analyzed, and combined approaches for monitoring models are developed.
Possible future extensions to the models are pointed out, including wide-band scenarios and speech sound quality, non-stationary impairments as well as speech technology devices.
We propose a nonparametric classification method designed to support the inter- pretability of the prediction.
On the one hand, the use of generalized additive models makes it possible to represent the effect of each input variable on the output variable graphically.
On the other hand, parameters of this model are estimated via penalized likelihood, where the term of regularization generalizes the h-penalization to the splines functions.
This penalization favors parsimonious solutions selecting one part ofthe set of input variables, while allowing a flexible modeling of the dependence on the selected variables.
We study the adaptation of various analytical model selection criteria to these models, and we evaluate them on two real data sets
This paper presents an overview of Japanese research on individuality information in speech waves, which have been performed from various points of view.
Whereas physical correlates having perceptual voice individuality have been investigated from the psychological viewpoint, research from the engineering viewpoint is related to automatic speaker recognition, speaker-independent speech recognition, and training algorithms in speech recognition.
Speaker recognition research can be classified into two classes, depending on whether or not the text is predetermined.
However, it has been made clear that even if the text is not predetermined, text-dependent individual information can be used that is based on explicit or implicit phoneme recognition.
Various examples of speaker recognition methods are classified into these variations, and their performances are presented in this paper.
In particular, this paper focuses on the long-term intra-speaker variability of feature parameters as on the most crucial problems in speaker recognition.
Additionally, it presents an investigation into methods for reducing the effects of long-term spectral variability on recognition accuracy.
This paper presents a learning algorithm using hidden Markov models (HMMs) and genetic algorithms (GAs).
Two standard problems to be solved with HMMs are how to determine the probabilities and the number of hidden states of the learned models.
Generally, this number of states is determined either by the trial-error method that needs experimentation, or by the background knowledge available.
The presented algorithm uses a GA in order to determine at the same time both the number of states and the probabilities of learned HMMs.
This hybrid algorithm uses the Baum-Welch algorithm to optimise precisely the probabilities of HMMs.
Several algorithms, either hybrid or not, are compared in a face recognition task.
The obtained results highlight the strength of our approach for the concerned problem.
A hybrid coding algorithm, consisting of motion compensated interframe prediction and adaptive gain/shape vector quantization, is proposed for low bit rate video coding.
A video coding system using the proposed coding algorithm and its coding characteristics are also described.
Motion compensated interframe prediction is an efficient technique for reducing temporal redundancy contained in moving pictures.
In the proposed scheme, constant amplitude blocks (“gray level blocks”) are also included in search vectors in order to increase coding efficiency for quick object movement or scene change.
Adaptive gain/ shape vector quantization with a tree search codebook is employed to encode motion compensated interframe difference signals.
It can respond to changes in source statistics because its codebook is independent of the gain components of the input vectors.
The proposed coding algorithm has been evaluated by computer simulations and shown to be effective for low bit rate video transmission.
A 64 kbit/s video coding system based on this coding algorithm has been implemented.
The developed coding system can transmit multiplexed video, audio and digital data at the basic ISDN rate, and can be applied to videophone as well as videoconferencing.
This introduction sets the stage for the papers making up this special issue.
Its focus is on two major problems in the study of lexical processing—determining the phases involved in recognising a spoken word and identifying the nature of different types of contextual influences on these phases.
An attempt is made to decompose the process of recognising a word into phases which have both theoretical and empirical consequences.
A similar analytic approach is taken in the discussion of the problem of context effects by distinguishing qualitatively different types of context (lexical, intra-lexical, syntactic, semantic, and interpretative).
We argue that such an approach is necessary to make explicit the relationship between a particular type of contextual information and the phase(s) of processing at which it has its impact.
The general framework of this paper is speech analysis and synthesis.
The speech signal may be separated into two components: (1) a periodic component (which includes the quasi-periodic or voiced sounds produced by regular vocal cord vibrations); (2) an aperiodic component (which includes the non-periodic part of voiced sounds (e.g. fricative noise in /v/) or sound emitted without any vocal cord vibration (e.g. unvoiced fricatives, or plosives)).
This work is intended to contribute to a precise modelling of this second component and particularly of modulated noises.
Firstly, a synthesis method, inspired by the “shot noise effect”, is introduced.
This technique uses random point processes which define the times of arrival of spectral events (represented by Formant Wave Form (FWF)).
Based on the theoretical framework provided by the Rice representation and the random modulation theory, an analysis/synthesis scheme is proposed.
Perception tests show that this method allows to synthesize very natural speech signals.
The representation proposed also brings new types of voice quality modifications (time scaling, vocal effort, breathiness of a voice, etc.).
Our study links a linguistic and psycholinguistic approach to the “metaphor” phenomenon based on the spontaneous utterances of 2-4-year-old children as well as on adult productions in scientific texts for the general public.
We would like to show in what way these utterances, generally considered to be “deviant” or “ordinary”, can bring a new manner to look at both the structuring of the verb lexicon in young children and the organisation of the verb lexicon in adults.
Concerning the utterances produced by young children we develop arguments which go against the notions, which they are usually given, of “error” or “metaphor”.
We also propose an extension of this formulation in order to achieve optimal modelling of pronunciation variations.
Since different words will not in general exhibit the same amount of pronunciation variation, the procedure allows words to be represented by a different number of baseforms.
The methods improve the subword description of the vocabulary words and have been shown to improve recognition performance on the DARPA Resource Management task.
The automatic recognition of acquired texts by a digitalizing tablet, opens the door to a new generation of computers, keyboardless and mouseless, that communicate with the user through an instrument that he is used to control since his youth: the pen.
In this work, we studied various methods ofcooperation between a context analyzer and two classifier-experts that process the data in two different ways.
The experimental results on a database of7000 letters and 12 writers are promising because they allowed a global improvement of efficiencies of 20%, leading to the general recognition rates of 84.2 % for letters and 64 % for words.
This article presents the various linguistic procedures used to create new forms of address (language creativity) and gives an account of some functions of the new address patterns in the management of social relations.
Among speech enhancement methods, the Ephraı̈m and Malah suppression rule (EMSR) has proven to be efficient in reducing the background noise while preventing from a common artefact: the musical noise.
From psychoacoustic motivation, an implementation of the EMSR with a perceptually relevant frequency partition is proposed.
This implementation is based on non-uniform oversampled filter-banks.
The frequency resolution is nevertheless uniform on the equivalent rectangular bandwidth (ERB)-scale.
Objective and subjective comparison with classical EMSR and uniform critically decimated filter-banks implementations has been achieved.
New techniques for automatic speaker recognition from telephone speech are described.
The recognition is based on spectral analysis of fixed sentence-long utterances.
From the whole utterance the form the time-dependent spectrogram.
Normalization procedures are applied to the spectrogram to take account of spectral distortions introduced by the telephone transmission system and of amplitude variations in the utterance.
The decision on the speaker's identity is arrived at by comparing the spectrogram of a sample utterance with stored reference spectrograms and calculating a measure of dissimilarity between the spectrograms.
To perform this spectrogram comparison, it is necessary to take into account differences in speaking rate and to bring corresponding speech events of the utterances into exact time synchronisation.
Time alignment is based on a measure of dissimilarity between speech events and is carried out by a dynamic programming algorithm which minimizes timing differences between corresponding speech events.
A set of utterances spoken by cooperative speakers and transmitted via conventional dialed-up telephone lines was used for the evaluation of the system.
Different versions of the recognition procedure were evaluated and compared.
For identification as well as for verification, error rates of 2% or less have been obtained.
The talk gives an overview of prosodic research at the Department of Linguistics and Phonetics at Lund University since 1950.
It shows how the word accents have been a prime motor in these activities.
The interest first centered on local Fo configurations caused by the distinctive accents, their contextual and dialectal variation and perceptual cues for their identification.
Further analyses of global aspects led to a compositional model of Swedish prosody in which the local accent shapes are seen as superposed on a global intonation.
Some neglected areas of research are pointed out, e.g. perception and possible effects of general rules of economy.
The talk ends with a plea for a more unified framework in prosodic analysis.
Some 550 vowel segments have been excised from a text read by a Dutch speaker, both at normal rate and at fast rate.
The duration of each segment is measured, as well as static and dynamic formant characteristics, such as midpoint formant frequencies, and descriptions of the formant tracks in terms of 16 equidistant points per segment, or Legendre polynomial functions.
We examined these formant characteristics as a function of vowel duration, but found no indication for duration-dependent undershoot.
Instead, this speaker showed very consistent consonant-specific coarticulatory behavior and adapted his speaking style to the speaking rate in order to reach the same midpoint formant frequencies.
Various (parabolically stylized) formant tracks, at various durations, in isolation or in CVC contexts, were synthesized and presented to listeners for identification.
Net shifts in vowel responses, compared to stationary stimuli, showed no indication of perceptual overshoot.
A weighted averaging method with the greatest weight to formant frequencies in the final part of the vowel tokens, explained the results best.
The processing scheme applies the Least Mean Squares (LMS) adaptive algorithm in frequency delimited sub-bands to process speech signals from simulated and real room acoustic environments with various realistic signal to noise ratios (SNR).
The processing scheme aims to take advantage of binaural input channels to perform noise cancellation.
The two wide-band signals are split into linear or cochlear distributed sub-bands, then processed according to their sub-band signal characteristics.
The results of a series of intelligibility tests are presented in which speech and noise data, generated in simulated and real room conditions, was presented to human volunteer subjects at various SNRs, sub-band distributions and sub-band spacings.
The results from both simulated and real room acoustical environments show that the MMSBA processing scheme significantly improves both SNR and intelligibility.
The paper proposes a technique to acquire morphological constructional (derivational) relations from dictionaries of synonyms in order to create morphological databases semi-automatically.
It exploites the paradigmatic structure of the lexicons in several ways.
The technique is based on the discovery of analogical (morpho-synonymic) quadruplets where semantic contraints defined as synonymic relations are combined with morphographical constraints.
It is robust and language independent: it has been successfully applied to French and English dictionnaries ofsynonyms.
The technique can be enhanced by typing the morpho-synonymic quadruplets in order to make explicit the fact that some pairs of lexemes are submitted to more constraints than others.
Smoothly bent paper-like surfaces are developable.
They are however difficult to minimally parameterize since the number of meaningful parameters is intrinsically dependent on the actual deformation.
Previous generative models are either incomplete, i.e. limited to subsets of developable surfaces, or depend on huge parameter sets.
Our first contribution is a generative model governed by a quasi-minimal set of intuitive parameters, namely rules and angles.
More precisely, a flat mesh is bent along guiding rules, while a number of extra rules controls the level of smoothness.
The second contribution is an automatic multi-camera 3D reconstruction algorithm.
First of all, the cameras and a sparse structure are reconstructed from the images using Structure-from-Motion method.
A 2D parametrization of the reconstructed points is computed by dimensionality reduction.
This parameterization is used to initialize the proposed model since it easily allows us to estimate the surface curvature.
The initial model parameters are eventually tuned through model-based bundle-adjustment.
Some experiments have been carried out to study and compensate for within-speaker variations in speaker verification.
To induce speaker variation, a speaking behaviour elicitation software package has been developed.
A 50-speaker database with voluntary and involuntary speech variation has been recorded using this software.
The database has been used for acoustic analysis as well as for automatic speaker verification (ASV) tests.
The voluntary speech variations are used to form an enrolment set for the ASV system.
This set is called structured training and is compared to neutral training where only normal speech is used.
Both sets contain the same number of utterances.
It is found that the ASV system improves its performance when testing on a mixed speaking style test without decreasing the performance of the tests with normal speech.
This paper describes the experimental set-up used by the SAM (ESPRIT-BRA Project no. 2589: Multilingual Speech Input/Output: Assessment, Methodology and Standardisation) group for evaluating the intelligibility of text-to-speech systems at sentence level.
The SUS test measures overall intelligibility of Semantically Unpredictable Sentences which can be automatically generated using five basic syntactic structures and a number of lexicons containing the most frequently occurring mini-syllabic words in each language.
The sentence material has the advantage of not being fixed, as words can be extracted from the lexicons randomly to form a new set of sentences each time the test is run.
Various text-to-speech systems in a number of languages have been evaluated using this test.
Results have demonstrated that the SUS test is effective and that it allows for reliable comparison across synthesisers provided guidelines are followed carefully regarding the definition of the test material and actual running of the test.
These recommendations are the result of experience gained during the SAM project and beyond.
This paper addresses the problem of optimal feature extraction from a wavelet representation.
For solving such a problem, we introduce a novel multiple kernel learning algorithm based on active constraints methods.
When applied to wavelet kernel learning, our experimental results show that the approaches we propose are competitive with respect to the state of the art on Brodatz texture datasets.
This paper will challenge the simple notion of analogy, which relies entirely on phonological associations, and the assumption that frequency effects are incompatible with symbolic rules as they are applied in research on morphological processing.
It will discuss the results of three experiments investigating the processing of Russian complex verbal morphology by adult native speakers of Russian, and American learners of Russian— two experiments on novel verb generation, and a lexical decision task.
This paper examines the question of whether and how the grammars proposed by linguists may be said to be 'realized' in adequate models of human sentence processing.
We first review the assumptions guiding the so-called Derivational Theory of Complexity (DTC) experiments.
Recall that the DTC experiments were taken to show that the theory of transformational grammar (TG) known as the Standard Theory was only a partially adequate model for human parsing.
In particular, it was assumed (see Fodor et al., 1974) that the DTC experiments demonstrated that while the parser actually used the structural descriptions implicit in a transformational derivation, the computations it used bore little resemblance to the transformations proposed by a TG.
The crucial assumptions behind the DTC were that (1) the processing model (or 'parser') performs operations in a linear, serial fashion; and (2) the parser incorporates a grammar written in more or less the same format as the competence grammar.
If we assume strict seriality, then it also seems easier to embed an Extended Lexical Grammar, such as the model proposed in Bresnan (1978) (as opposed to a TG), into a parsing model.
Therefore, this assumption plays an important role in Bresnan's critique of TG as an adequate part of a theory of language use.
Both Fodor, Bever and Garrett (1974) and Bresnan (1978) attempt to make the grammatical rules compatible with the psycholinguistic data and with assumption (1) by proposing models that limit the amount of active computation performed on-line.
They do this by eliminating the transformational component.
However, we show that on-line computation need not be associated with added reaction time complexity.
That is, we show that a parser that relates deep structure to surface structure by transformational rules (or, more accurately, by parsing rules tailored very closely after those of a transformational model) can be made to comport with the relevant psycholinguistic data, simply by varying assumption (1).
In particular, we show that by embedding TG in a parallel computational architecture—an architecture that can be justified as a reasonable one for language use—one can capture the sentence processing complexity differences noted by DTC experimenters.
Assumption (2) is also relevant to the evaluation of competing grammars as theories of language use.
First we show that Bresnan (1978) must relax this assumption in order to make Extended Lexical Grammar compatible with the psycholinguistic results.
Secondly, we analyze Tyler and Marslen-Wilson's (1977) and Tyler's (1980) claim that their experiments show that one cannot instantiate a TG in a model of parsing without varying assumption (2).
This is because they insist that their experiments support an 'interactive model' of parsing that, they believe, is incompatible with the 'Autonomy of Syntax' thesis.
We show that the Autonomy Thesis bears no relation to their 'interactive model'.
Therefore, adopting this model is no barrier to the direct incorporation of a TG in a parser.
Moreover, we show why meeting assumption (2), a condition that we dub the 'Type Transparency Hypothesis', is not an absolute criterion for judging the utility of a grammatical theory for the construction of a theory of parsing.
We claim that the grammar need not be viewed as providing a parsing algorithm directly or transparently (assumption 2 above).
Nevertheless, we insist that the theory of grammar figures centrally in the development of a model of language use even if Type Transparency is weakened in the ways that we suggest.
Taken together, these considerations will be shown to bear on the comparative evaluation of candidate parsing models that incorporate transformational grammar, extended-lexical grammar, or the Tyler and Marslen-Wilson proposals.
Variations in rate-of-speech (ROS) produce variations in both spectral features and word pronunciations that affect automatic speech recognition systems.
To deal with these ROS effects, we propose to use a set of parallel rate-specific acoustic and pronunciation models.
Rate switching is permitted at word boundaries, to allow within-sentence speech rate variation, which is common in conversational speech.
Because of the parallel structure of rate-specific models and the maximum likelihood decoding method, our approach does not require ROS estimation before recognition, which is hard to achieve.
We evaluate our models on a large vocabulary conversational speech recognition task over the telephone.
Experiments on the NIST 2000 Hub-5 development set show that word-level ROS-dependent modeling results in a 2.2% absolute reduction in word error rate over a rate-independent baseline system.
Relative to an enhanced baseline system that models cross-word phonetic elision and reduction in a multiword dictionary, rate-dependent models achieve an absolute improvement of 1.5%.
Furthermore, we introduce a novel method to modeling reduced pronunciations that are common in fast speech based on the approach of skipping short phones in the pronunciation models while preserving the phonetic context for the adjacent phones.
This method is shown to also produce a small additional improvement on top of ROS-dependent acoustic modeling.
The debate between Emmanuel Macron and Marine Le Pen during the 2017 French presidential election campaign reveals the ambiguities of fact-checking in its claims to denounce lies propagated by public figures.
While putting forward the principle of veracity, fact-checking seeks to identify falsehood rather than to expose truth.
Fact-checkers delegate responsibility for determining truth to reliable sources.
Analysis reveals that these sources are primarily institutional and are collectively considered to be legitimate.
Fact-checking thus deploys a conservative approach to journalistic information, applied to all that is said in the public sphere.‪
This paper proposes a method of extracting the desired signal from a noisy signal, addressing the problem of segregating two acoustic sources as a model of acoustic source segregation based on Auditory Scene Analysis.
Since the problem of segregating two acoustic sources is an ill-posed inverse problem, constraints are needed to determine a unique solution.
The proposed method uses the four heuristic regularities proposed by Bregman as constraints and uses the instantaneous amplitude and phase of noisy signal components that have passed through a wavelet filterbank as features of acoustic sources.
Then the model can extract the instantaneous amplitude and phase of the desired signal.
Simulations were performed to segregate the harmonic complex tone from a noise-added harmonic complex tone and to compare the results of using all or only some constraints.
The results show that the method can segregate the harmonic complex tone precisely using all the constraints related to the four regularities and that the absence of some constraints reduces the accuracy.
In this paper we consider a complex of language-related problems that research has identified in children with reading disorder and we attempt to understand this complex in relation to proposals about the language processing mechanism.
The perspective gained by considering reading problems from the standpoint of language structure and language acquisition allows us to pose specific hypotheses about the causes of reading disorder.
The hypotheses are then examined from the standpoint of an analysis of the demands of the reading task and a consideration of the state of the unsuccessful reader in meeting these demands.
The remainder of the paper pursues one proposal about the source of reading problems, in which the working memory system plays a central part.
This proposal is evaluated in the light of empirical research which has attempted to tease apart structural knowledge and memory capacity both in normal children and in children with notable reading deficiencies.
This paper describes a new model of intonation for English.
The paper proposes that intonation can be described using a sequence of rise, fall and connection elements.
Pitch accents and boundary rises are described using rise and fall elements, and connection elements are used to describe everything else.
Equations can be used to synthesize fundamental frequency (F 0) contours from these elements.
An automatic labelling system is described which can derive a rise/fall/connection description from any utterance without using prior knowledge or top-down processing.
Synthesis and analysis experiments are described using utterances from six speakers of various English accents. An analysis/resynthesis experiment is described which shows that the contours produced by the model are similar to within 3.6 to 7.3 Hz of the originals.
An assessment of the automatic labeller shows 72% to 92% agreement between automatic and hand labels.
The paper concludes with a comparison between this model and others, and a discussion of the practical applications of the model.
This paper describes the first step of our research, a system which recognizes two speakers in each of Spanish and English and is limited to some four hundred words.
The key new idea is that the speech recognition and the language analysis are tightly coupled by using the same language model, an augmented phrase-structure grammar, for both.
In this paper, we describe several objective and subjective methods to measure the performance of spoken language dialogue systems and their components.
The focus is on the evaluation of spontaneous Human-Machine interaction, notably supported by information retrieval systems.
In this paper, we describe our contribution to the CEPT effort for the definition of a European standard for the cellular mobile radio.
We propose a descriptive model of forms of dyadic cooperative problem-solving activity, with associated learning mechanisms.
Cooperative problem-solving is analysed in terms of three fundamental and gradual dimensions: symmetry, alignment and agreement.
The first dimension relates to distribution of transactional roles, the second to action coordination, and the third to expression and resolution of disagreements.
Combination of the three dimensions produces a space of eight main forms of cooperation, within which we situate "collaboration ".
Illustrative analyses of face-to-face and computer-mediated interactions are presented, revealing relations between the dimensions of cooperation.
Finally, we discuss related cooperative learning mechanisms, and possible extensions to the model required for analysing larger groups.
Articulatory parameters, vocal tract shape and cross-sectional area function were determined from fricative spectra.
A model of fricative generation was used for providing acoustical constraints for an optimization procedure with muscles work as the criterion of optimality.
A distance between spectra was measured with the use of the Cauchy-Bounjakovsky non-equality.
A proper initial approximation of articulatory parameters is required to obtain an accurate and stable solution of the inverse problem.
In the field of Automatic Speech Recognition (ASR) research, it is conventional to pursue those approaches that reduce the word error rate.
However, it is the authors' belief that this seemingly sensible strategy often leads to the suppression of innovation.
The leading approaches to ASR have been tuned for years, effectively optimizing on test data for a local minimum in the space of available techniques.
In this case, almost any sufficiently new approach will necessarily hurt the accuracy of existing systems and thus increase the error rate.
However, if progress is to be made against the remaining difficult problems, new approaches will most likely be necessary.
In this paper, we discuss some research directions for ASR that may not always yield an immediate and guaranteed decrease in error rate but which hold some promise for ultimately improving performance in the end applications.
Issues that will be addressed in this paper include: discrimination between rival utterance models, the role of prior information in speech recognition, merging the language and acoustic models, feature extraction and temporal information, and decoding procedures reflecting human perceptual properties.
Inferring gene regulatory networks tends to use several biological information.
Here we use data from genetic markers and expression data in the framework of discrete static bayesian networks.
We compare several scores and also the impact of a network connectivity a priori.
We propose and compare two models with existing approaches of gene regulatory network inference.
On simulated data one of our models reached better results in the case ofsmall sample size.
We use this model on real data in Arabidopsis thaliana.
It is well known that speaker variability caused by accent is one factor that degrades performance of speech recognition algorithms.
If knowledge of speaker accent can be estimated accurately, then a modified set of recognition models which addresses speaker accent could be employed to increase recognition accuracy.
In this study, the problem of language accent classification in American English is considered.
A database of foreign language accent is established that consists of words and phrases that are known to be sensitive to accent.
Next, isolated word and phoneme based accent classification algorithms are developed.
The feature set under consideration includes Mel-cepstrum coefficients and energy, and their first order differences.
It is shown that as test utterance length increases, higher classification accuracy is achieved.
Isolated word strings of 7–8 words uttered by the speaker results in an accent classification rate of 93% among four different language accents.
A subjective listening test is also conducted in order to compare human performance with computer algorithm performance in accent discrimination.
The results show that computer based accent classification consistently achieves superior performance over human listener responses for classification.
It is shown, however, that some listeners are able to match algorithm performance for accent detection.
Finally, an experimental study is performed to investigate the influence of foreign accent on speech recognition algorithms.
It is shown that training separate models for each accent rather than using a single model for each word can improve recognition accuracy dramatically.
The history and sociology of a special group of civil servants are presented, namely: stenographers in parliament, an occupation closely tied to the history of parliamentary government in most democracies.
Based on original material drawn from sociology and history, this inquiry seeks to see how institutions are shaped, produced and reproduced through the knowledge and deeds that incarnate values and shape institutions.
Besides the publication of parliamentary proceedings, the technical, material and social conditions are discussed that led the National Assembly to become a political institution.
This paper is dedicated to a floating block FFT error analysis.
A statistical model of the error propagation in a pass is developped which considers separately the three following parts: the input, arithmetic roundoff and coefficient quantization errors.
This is largely due to the complexity of identifying and categorising the emotion factors in natural human speech, and implementing these factors within synthetic speech.
Such models could also be used as practical tools in the investigation and validation of models of emotion and other speech-altering Stressors.
Achieving reliable performance for a speech recogniser is an important challenge, especially in the context of mobile telephony applications where the user can access telephone functions through voice.
The breakthrough of such a technology is appealing, since the driver can concentrate completely and safely on his task while composing and conversing in a “full” hands-free mode.
This paper addresses the problem of speaker-dependent discrete utterance recognition in noise.
Special reference is made to the mismatch effects due to the fact that training and testing are made in different environments.
A novel technique for noise compensation is proposed: nonlinear spectral subtraction (NSS).
Robust variance estimates and robust pdf evaluations (projection) are also introduced and combined with NSS into the HMM framework.
We show that the lower limit of applicability of the projection (low SNR values) can be loosened after combination with NSS.
The performance of an HMM-based recogniser rises from 56% (no compensation) to 98% after speech enhancement.
More than 3300 utterances have been used to evaluate the systems (three databases, two European languages).
This result is achieved by the use of robust training/recognition schemes and by preprocessing the noisy speech by NSS.
We provide experimental results that compare the propagation algorithm developed for the possibilistic product-based networks and the inference algorithm developed in this paper.
The history of social control applications of psychology and the likelihood of a future increase in their importance are assessed.
The effects of military funding of psychological research and the social consequences of very widespread unemployment are specifically considered.
It is argued that psychology and related areas of cognitive science and neuroscience may well become increasingly relevant in the development of the technical components of such techniques rather than in providing ideological justifications for their use.
Assuming that the pixel values in the images are proportional to some conserved quantity, a new penalty function is defined for motion estimation of a deforming body.
We use the theory of linear elasticity and the conservation laws of continuum medium to propose new constraining terms.
The introduction of a deformation model gives a new interpretation of the Song and Leahy's solution [Song 91].
Examples of experiments using simulated and real images of deforming body are presented.
The method is able to take into account compressible or incompressible motion according to the parameter values.
This article describes fully Bayesian algorithms to unmix hyperspectral images.
Each pixel of the hyperspectral image is decomposed as a combination of pure endmember spectra according to the linear mixing model.
In a supervised context, the endmembers are assumed to be known.
The unmixing problem consists of estimating the mixing coefficients under positivity and additivity constraints.
An appropriate distribution is chosen as prior distribution for these coefficients, that are estimated from their posterior distribution.
A Markov chain Monte Carlo (MCMC) algorithm is developed to approximate the estimators.
In a semi-supervised framework, the spectra involved in the mixtures are assumed to be unknown.
They are supposed to belong to a known spectral library.
A reversible-jump MCMC algorithm allows one to solve the resulting model selection problem.
Finally, in a final step, the previous algorithms are extended to handle the unsupervised unmixing problem, i.e., to estimate the endmembers and the mixing coefficients jointly.
This blind source separation problem is solved in a lower-dimensional space, which effectively reduces the number of degrees of freedom of the unknown parameters.
In this paper, we are interested in how to structure the terms of a domain, i.e acquiring relations between terms.
The terminologies can no more just make a list of the terms used in a domain and succinctly organize them in a hierarchie.
The terminologies have to be an adapted answer to the needs of the applications and propose a variety of relations which better reflects the knowledge of the domain.
We constrast the place that traditionally the theory gives to the relations in the terminologies with the real needs of the constitution of terminologies and their use and recycling in applications.
Then, we present existing approaches to acquire relations between terms from specialized corpora.
The process of spoken word-recognition breaks down into three basic functions, of access, selection and integration.
Access concerns the mapping of the speech input onto the representations of lexical form, selection concerns the discrimination of the best-fitting match to this input, and integration covers the mapping of syntactic and semantic information at the lexical level onto higher levels of processing.
This paper describes two versions of a “cohort”-based model of these processes, showing how it evolves from a partially interactive model, where access is strictly autonomous but selection is subject to top-down control, to a fully bottom-up model, where context plays no role in the processes of form-based access and selection.
Context operates instead at the interface between higher-level representations and information generated on-line about the syntactic and semantic properties of members of the cohort.
The new model retains intact the fundamental characteristics of a cohort-based word-recognition process.
It embodies the concepts of multiple access and multiple assessment, allowing a maximally efficient recognition process, based on the principle of the contingency of perceptual choice.
This paper presents a segmentation system that automatically splits into letters a set of handwritten words.
This is made in order to built a learning base for an on-line recognition system dedicated to handwritten words.
The drawn words to be segmented are entered on a digitizing tablet before being converted into a sequence of vectors.
At last, the corresponding alphabetic word is associated to each of these drawn words.
The description of our segmentation system is followed by an experimental test of letter segmentation applied on a database of 10000 words coming from 10 different scriptors.
A computational simulation was used to generate impulse responses between points in a rectangular room and two points on opposite sides of a spherical “head”.
Sounds were convolved with the impulse responses to generate stimuli with which to study the effects of reverberation on the ability of listeners to use differences in fundamental frequency (Δ Fos) to separate concurrent vowels.
Experiment 1 verified the suitability of the simulation by showing that it produced (i) appropriate percepts of lateralization, (ii) a larger contribution to lateralization from interaural differences in timing than level, and (iii) no effects of reverberation on lateralization.
Experiments 2–5 measured masked identification thresholds for synthetic harmonic “target” vowels in the presence of masking sounds.
In Experiment 2, listeners identified targets against pink-noise maskers.
The experiment established a spatial geometry and a degree of reverberation for which listeners did not benefit from binaural cues arising from the spatial geometry of the sources.
Experiment 3 demonstrated that the same arrangement did not undermine the ability to use Δ Fos to separate targets from vowel-like maskers when both had static Fo contours, but did prevent listeners from using Δ Fos carried on coherently changing Fo contours.
Experiment 4 showed that a modulation width of ±1.45% was sufficient to reduce the benefits of Δ Fos, but that the benefits were not eliminated until the width of modulation exceeded the Δ Fo.
It is argued that these results are compatible with existing models of the ability to use Δ Fos to separate concurrent vowels and that reverberation undermines the ability when the Fos are changing by diffusing the periodicities of the competing sources.
Finally, Experiment 5 demonstrated that reverberation had no effect on the ability to separate a modulated vowel from pink noise.
Thus, reverberation may have its detrimental effects in these experiments by diffusing the periodicity of the masking sounds rather than the targets.
Overall, the experiments demonstrate that Δ Fos can be more robust cues for separating concurrent sounds than binaural cues.
The relevance of these results to the perception of natural continuous speech is discussed.
The first contains neural networks for transformation of ordinary signal processing cepstral parameters into a set of continuously valued acoustic-phonetic features for each frame of the speech signal and for its division into acoustic phonetic segments.
The output from the first stage, which is a combination of the results of segmentation and acoustic-phonetic segments, gives a first estimate of the sequence of phonemes.
The second stage contains an expert system consisting of allophonic rules, a lexicon of transcriptions of the words belonging to the selected application vocabulary, syntax rules and the overall control structure.
In this stage the incoming string of phonemes is processed using the lexicon and an island-driven parsing system.
The current vocabulary consists of 35 words given by a CAD-like application.
This paper presents a comparative study investigating the relation between the timing of a rising or falling pitch movement and the temporal structure of the syllable it accentuates for three languages: Dutch, French and Swedish.
In a perception experiment, the five-syllable utterances /mamamamama/ and / a a a a a/ were provided with a relatively fast rising or falling pitch movement.
The timing of the movement was systematically varied so that it accented the third or the fourth syllable.
Subjects were asked to indicate which syllable they perceived as accented.
The accentuation boundary (AB) between the third and the fourth syllable was then defined as the moment before which more than half of the subjects indicated the third syllable as accented and after which more than half of the subjects indicated the fourth syllable.
The results show that there are significant differences between the three languages as to the location of the AB.
In general, for the rises, well-defined ABs were found.
They were located in the middle of the vowel of the third syllable for French subjects, and later in that vowel for Dutch and Swedish subjects.
For the falls, a clear AB was obtained only for the Dutch and the Swedish listeners.
This was located at the end of the third syllable.
For the French listeners, the fall did not yield a clear AB.
This corroborates the absence of accentuation by means of falls in French.
By varying the duration of the pitch movement it could be shown that, in all cases in which a clear AB was found, the cue for accentuation was located at the beginning of the pitch movement.
In this paper a Walsh linear coding algorithm (WLC) is developed which yields a compressed approximation to the Walsh power spectrum of a time frame.
This algorithm is interpolative, rather than predictive.
It minimizes the mean square interpolative error, and it yields WLC model spectra.
The effectiveness of utilizing WLC model spectra in speech recognition systems is explored.
Each of two unity gain distortion measures is embedded in a dynamic time warp based speaker-dependent isolated word recognition system and is tested by means of a 10-digit vocabulary.
Trần Đức Thảo and Jean Piaget both aimed to revolutionize epistemology by combining scientific discoveries about the evolution of human intelligence and the development of our cognitive abilities.
This point calls to mind the recapitulation theory in biology, which is false and obsolete, and therefore seems to condemn their efforts.
We shall demonstrate, however, that this notion of recapitulation was reelaborated by the two scholars.
A time-domain simulation method of the vocal-tract system is described.
The system, composed of a constant air pressure source, a time-varying narrow section representing the glottis and a tube corresponding to the vocal tract coupled with the nasal cavity was assumed as a vocal-tract model.
The acoustic equations that govern the generation and the propagation of acoustic waves inside the model were transformed into the discrete variable representation by applying certain rules; the rectangular rule in space and the trapezoid rule in time.
This particular manner of discretization causes spectral distortion due to the frequency warping.
A theoretical analysis indicated that this warping can be interpreted as the manifestation of the frequency dependent phase velocity of waves in the discrete system.
The magnitude of the warping depends on both the sampling frequency (f s) and the sampling interval in space (X).
Eleven French vowels synthesized with f s = 20 kHz and X = 1 cm sounded very natural and highly intelligible, even though a trace of the frequency warping was noticeable at the third formant frequency region on their spectra.
When f s = 40 kHz was used, the effect of the spectral distortion became practically negligible for frequencies below 4 kHz.
This paper reports on the speech synthesis module used to present spoken traffic messages through the car radio, as part of the “Traffic Message Control” system RDS-TMC.
One of the basic ideas of this intended Pan-European service is its ability to provide traffic information in the driver's native language, independent of the language used in the geographical area or used for broadcasting.
To accomplish this for an unlimited set of location names, speech synthesis is a must.
A prototype has been developed for German.
Other countries and languages will follow in 1998.
This paper promotes an individual-based approach for solving a well-known problem, the stable marriage problem.
In this approach, a solution is the output of an emergent phenomena due to the negotiation between the agents.
The agentification of the seminal Gale-Shapley algorithm is based upon two distinct behaviours (the proposer and the responder) negotiating to reach a stable matching which is not fair.
The CASANOVA agent behaviour we propose in this paper relies on a society of agents that play at the same time both of these roles in multi bi-lateral negotiations.
The agents applies the minimal concession strategy for negotiation to ensure their individual welfare.
The emergent solutions are fair and they cannot be reached by classical multi-agent methods, whereas CASANOVA is decentralized and privacy-preserving.
A three-dimensional (3D) tongue model has been developed using MR images of a reference subject producing 44 artificially sustained Swedish articulations.
Based on the difference in tongue shape between the articulations and a reference, the six linear parameters jaw height, tongue body, tongue dorsum, tongue tip, tongue advance and tongue width were determined using an ordered linear factor analysis controlled by articulatory measures.
The first five factors explained 88% of the tongue data variance in the midsagittal plane and 78% in the 3D analysis.
The six-parameter model is able to reconstruct the modelled articulations with an overall mean reconstruction error of 0.13 cm, and it specifically handles lateral differences and asymmetries in tongue shape.
In order to correct articulations that were hyperarticulated due to the artificial sustaining in the magnetic resonance imaging (MRI) acquisition, the parameter values in the tongue model were readjusted based on a comparison of virtual and natural linguopalatal contact patterns, collected with electropalatography (EPG).
Electromagnetic articulography (EMA) data was collected to control the kinematics of the tongue model for vowel-fricative sequences and an algorithm to handle surface contacts has been implemented, preventing the tongue from protruding through the palate and teeth.
SAS (Synthetic Aperture Sonar) has been used in sea bed imagery.
Indeed, high resolution images provided by SAS are of great interest, especially for the detection, localization or eventually classification of objects lying on sea bed.
But, SAS images are highly corrupted by a granular multiplicative noise, called speckle noise, which reduces spatial and radiometric resolutions.
For this reason, an automatic analysis of these images is not so evident.
A solution can consist on the use of a filtering before process, without a spatial resolution degradation.
The purpose of this article is to present a new process consisting on the jointly use of the stochastic matched filter and an autoadaptive mean filter.
Furthermore, in order to well preserve the spatial resolution, we propose to use as a criteria for the stochastic matched filter the minimization between the speckle noise local statistics with the removal signal ones, allowing a subimage size adaptation.
Results obtained on real SAS data are proposed and compared with those obtained using other stochastic matched filtering based denoising methods.
The emergence of the lexicon for events is rather late in L1 acquisition, compared to the lexicon for entities, but the same holds for subsequent L2 acquisition.
The semantic processing of verbs by children and adults shows a greater flexibility than the processing of nouns: explanations to that are the relational nature of verbs, and the fact that the lexical categorization of events is less determined by perception than structured by language-specific lexicalisation patterns.
When a child's variety – or a learner variety – contains few verbs, their semantic flexibility will be maximally used, notably by analogical processes.
In this paper, the present situation in Japan regarding development and application of the electro-palatography is reviewed.
Then, data on lingual articulation with respect to personal differences and child characteristics are reported.
These data were obtained by combining the three-dimensional measurements of the plaster casts of the hard palates of fifteen adults, thirty children, and two children at different dental stages, with the electro-palatographic observation of the lingual contact patterns with the hard palates of the adult subjects and four children during utterances of the Japanese consonants.
In this paper, we propose a technique for detection and segmentation of skin color areas.
Our method is using data mining techniques in order to produce classification rules, followed by segmentation phase in coherent regions of skin using that rules for decision making.
Experimental results on the representative database of images show effectiveness and high reliabilty of our approach.
A method is described for designing speaker recognition features that are robust to telephone handset distortion.
The approach transforms features such as mel-cepstral features, log spectrum, and prosody-based features with a non-linear artificial neural network.
The neural network is discriminatively trained to maximize speaker recognition performance specifically in the setting of telephone handset mismatch between training and testing.
The algorithm requires neither stereo recordings of speech during training nor manual labeling of handset types either in training or testing.
Results on the 1998 National Institute of Standards and Technology (NIST) Speaker Recognition Evaluation corpus show relative improvements as high as 28% for the new multilayered perceptron (MLP)-based features as compared to a standard mel-cepstral feature set with cepstral mean subtraction (CMS) and handset-dependent normalizing impostor models.
In this paper is presented the DESAM project which was divided in two parts.
Most aspects that have been considered in this project have led to the proposal of new methods which have been grouped together into the so-called DESAM Toolbox, a set of Matlab® functions dedicated to the estimation of widely used spectral models for music signals.
Although those models can be used in Music Information Retrieval (MIR) tasks, the core functions of the toolbox do not focus on any specific application.
It is rather aimed at providing a range of state-of-the-art signal processing tools that decompose music recordings according to different signal models, giving rise to different “mid-level” representations.
This paper proposes an algorithm based on F0 patterns to hypothesize word boundaries and function words in continuous speech in Hindi.
It makes use of the properties of F0 contour such as declination tendency, resetting and fall-rise patterns in Hindi.
The syllabic units are identified by using the energy contour, pitch and the first order LP coefficient.
Each syllabic unit is assigned an accent value L (Low), H or h (High) by (i) comparing the F0 value at the mid point of each syllabic nucleus with that of the previous syllabic unit and (ii) comparing the F0 values at two different points within each syllabic unit in a sequence having an accent value L.
Word boundaries are placed between the adjacent syllabic units (i) H and L, (ii) h and L, (iii) L and L, (iv) L and h and (v) H and h.
An evaluation conducted on a corpus of 50 sentences in Hindi read aloud by five native speakers in an ordinary office environment showed that about 74 percent of the word boundaries and about 28 percent of the function words were correctly identified.
The results of the word boundary hypothesization can be used to improve the performance of the acoustic-phonetic, lexical and syntactic modules in a speech-to-text conversion system.
Robustness of the algorithm in handling noisy speech input conditions and telephone speech are also discussed.
This paper describes PEGASUS, a spoken dialogue interface for on-line air travel planning that we have recently developed.
PEGASUS leverages off our spoken language technology development in the ATIS domain, and enables users to book flights using the American Airlines EAASY SABRE system.
The input query is transformed by the speech understanding system to a frame representation that captures its meaning.
The tasks of the System Manager include transforming the semantic representation into an EAASY SABRE command, transmitting it to the application backend, formatting and interpreting the resulting information, and managing the dialogue.
Preliminary evaluation results suggest that users can learn to make productive use of PEGASUS for travel planning, although much work remains to be done.
The last several years have been an exciting time at AT&T in the field of advanced speech applications for telecommunications: technical progress and platform/processor advances have enabled the identification, development and testing of a range of new services.
During this period, prior to the divestiture of AT&T of Lucent Technologies and NCR, AT&T brought together, under a single corporate `roof', a research laboratory committed to advancing speech technology, business organizations building platforms to leverage this technology for telecommunications applications, and yet other business organizations with responsibility for deploying speech-enabled services to facilitate the use and reduce the cost of telecommunications services for both consumers and businesses.
While this period of our corporate history has drawn to a close, we can look back to provide an overview of how technical progress, platform advances and network services needs and opportunities interacted to make speech technology an everyday experience for millions of people – and some of the lessons we learned along the way.
Previous work has shown that it is possible to train a multi-layer perceptron to estimate the voice fundamental period (Tx) for multiple speakers in the presence of high levels of background noise.
The algorithm has been implemented in real-time on a TMS320C25 based development system.
A prototype pocket-sized portable device has been constructed and the real-time software transferred to it.
This will provide the basis for a new generation of signal processing hearing aids for the profoundly and totally deaf.
Power supply current is sufficiently low for battery operation for periods of 12 hours between charges.
The basic algorithm has been adapted to provide the higher time resolution which will make it applicable to a wide range of other applications.
This paper addresses the implementation of a dynamic bayesian network, with exogenous continuous inputs, for the classification of discrete irregularly spaced events sequences.
Mixture models, estimated using the EM algorithm, are used to model the conditional probability tables, which depends on the vector of distances between events.
The application framework of this study is the classification of railway trucks singular points for the Paris metro.
The proposed approach reveals also good parsimony properties as soon as the model order is greater or equal two.
This implementation may improve decisions provided by a specific rail defect sensor.
Recently, techniques motivated by human auditory perception are being applied in main-stream speech technology and there seems to be renewed interest in implementing more knowledge of human speech communication into a design of a speech recognizer.
The paper discusses the author's experience with applying auditory knowledge to automatic recognition of speech.
It advances the notion that the reason for applying of such a knowledge in speech engineering should be the ability of perception to suppress some parts of the irrelevant information in the speech message and argues against the blind implementation of scattered accidental knowledge which may be irrelevant to a speech recognition task.
The following three properties of human speech perception are discussed in some detail: • limited spectral resolution, • use of information from about syllable-length segments, • ability to ignore corrupted or irrelevant components of speech.
It shows by referring to published works that selective use of auditory knowledge, optimized on and in some cases derived from real speech data, can be consistent with current stochastic approaches to ASR and could yield advantages in practical engineering applications.
This study examined the production of English consonants by native speakers of Italian.
The 240 adult native Italian speakers of English who participated had begun learning English when they emigrated to Canada between the ages of 2 and 23 years.
Word-initial, word-medial and word-final tokens of English stops and fricatives were assessed through forced-choice judgments made by native English-speaking listeners, and acoustically.
The native Italian subjects' ages of learning (AOL) English exerted a systematic effect on their production of English consonants even though they had lived in Canada for an average of 32 years, and reported speaking English more than Italian.
In all but two instances, one or more native Italian subgroup defined on the basis of AOL differed significantly from subjects in a native English (NE) control group.
The AOL of the first native Italian subgroup to differ from the NE subjects varied across consonant and syllable position.
The results are discussed in terms of hypotheses proposed in the literature concerning the basis of segmental errors in L2 speech production.
Normalization of formant frequencies have frequently been used to eliminate inter-speaker differences in vowel recognition.
However, estimation of formant frequencies becomes difficult under certain circumstances, such as for telephone speech.
This paper presents an approach to vowel normalization based on frequency warped spectral matching.
A frequency normalized distance between test and reference spectra is defined on the basis of the minimum mean square difference over all possible choices of frequency warping functions under certain nonlinearity constraints and boundary conditions.
After adaptively eliminating spectral slope differences due to the individual glottal characteristics, the spectral distance is computed by means of dynamic programming.
The vowel identification experiments were conducted on the nine American English vowels in /hvd/ utterances spoken by 12 male and 12 female speakers.
The results indicated that the frequency warping method substantially increased the identification scores for female vowels when the male vowels were used as reference.
They also indicated that although the improvement in identification was attributed mainly to the linear frequency scaling, an additional improvement for vowel /ae/ was obtained by a slight nonlinear frequency warping.
In addition, an application to speaker normalization for word detection in connected speech is discussed.
Nevertheless, this model has two drawbacks: first, the number of parameters grows exponentially with the order of the chain, secondly, the parameters are difficult to estimate.
The proposed solutions for improving the model are probability smoothing or variable memory length Markov models, which can be represented by a tree known as Prediction Suffix Tree.
In this paper, we propose an other improvement, based on a statistical test, which aim is to decide if the model of the sequence to classify includes subsequences (domains, patterns...) which are not in the model of its class and conversely.
As illustration, we compare the results given by different models on the E. coli DNA sequences from the UCI repository of machine learning database and we show how the choice of parameters of these models influences the performances.
This paper proposes a new methodology to automatically build semantic hierarchies suitable for image annotation and classification.
The aim is to provide a measure that best represents image semantics.
We then propose rules based on this measure, for the building of the final hierarchy, and which explicitly encode hierarchical relationships between different concepts.
Therefore, the built hierarchy is used in a semantic hierarchical classification framework for image annotation.
Our experiments and results show that the built hierarchy improves significantly the classification accuracy.
This paper discusses some important topics in current speech synthesis research.
Modeling of speaker characteristics and emotions are used as an example of new trends in the speech synthesis field.
The relation to speech recognition research is emphasized.
New methods such as automatic learning and the use of new analysis techniques are also discussed.
The stop /p,t,k/ recognition part of a speaker-independent speech recognition system is described in this paper.
This work is based on the conclusions of several perceptual experiments and on the results of an acoustic investigation with stop consonants.
These experiments allowed us to evaluate the discrimination power of the burst regarding the stop place of articulation, and how the vocalic information may help stop identification, which could not be done efficiently without taking into account the nature of the following vowel.
Thus, a novel system architecture is proposed which is made up of two stages: first, an automatic detector of reliable cues regarding stop and vowel features, and, then, a context sensitive multilayered perceptron (ODWE) fed by the previous acoustic cues.
The results show a recognition rate of 90% over the stop consonants.
Most difficulties arising in the development of speech understanding systems come from the great uncertainty inherent in the acoustic signal.
The inaccuracy of acoustic-phonetic decoders makes it necessary to use higher level information: lexical, syntactic and semantic.
We present here a possible solution for integrating some of this information in the particular case of an administrative database questioning system.
First, we try to sketch how contextual knowledge can be extracted from local or global representations of an utterance, such as partial recognition or the dialogue history in the case of man-machine interaction.
Discussions at the ESCA-NATO Workshop on Speech Under Stress (Lisbon, Portugal, September 1995) centred on definitions and models of stress and its effects.
Based on the Workshop discussions, in this paper we attempt to produce a definition of stress, and propose a number of stress models which clarify issues in this field, and which might be adopted by the speech community.
The concept of stress is very broad and used differently in a number of domains — a definition of stress has thus remained elusive.
Greater separation of Stressors (the causes of stress) and strain (the effects of stress) is proposed, and methods for relating Stressors to strains are presented.
Suggestions for future research directions in this field are also made.
In this paper, methods are proposed for facial feature detection (eyes, brows, nose, mouth, chin) and for facial expression recognition.
The methods are based on modified versions of the standard Active Appearance Model proposed by Cootes et al. [11] to control both the shape and the texture of a given face.
The detection algorithm makes use of an active appearance model computed on hierarchical Gabor descriptions a set of training faces.
In a second part, two expression models are proposed, based on the standard AAM, and used to recognize and then to cancel or modify the facial expression of a given unknown face.
The hippocampus and the amygdala are two brain structures which play a central role in several fundamental cognitive processes.
Their segmentation from Magnetic Resonance Imaging (MRI) scans is a unique way to measure their atrophy in some neurological diseases, but it is made difficult by their complex geometry.
Their simultaneous segmentation is considered here through a competitive homotopic region growing method.
It is driven by relational anatomical knowledge, which enables to consider the segmentation of atrophic structures in a straightforward way.
For both structures, this fast algorithm gives results which are comparable to manual segmentation with a better reproducibility.
Its performances regarding segmentation quality, automation and computation time, are amongst the best published data.
In this paper, we develop data driven registration algorithms, relying on pixel similarity metrics, that enable an accurate rigid registration of dissimilar single or multimodal 2D/3D medical images.
Gross dissimilarities are handled by considering similarity measures related to robust M-estimators.
Fast stochastic multigrid optimization algorithms are used to minimize these similarity metrics.
The proposed robust similarity metrics are compared to the most popular standard similarity metrics on real MRI/MRI and MRI/SPECT image pairs showing gross dissimilarities.
Our robust similarity measures compare favourably with all standard (non robust) techniques.
This study aims at analysing, modelling and simulating human capabilities of planning and interaction.
The experimental protocols have been analyzed from the planning point of view and from the interaction point of view.
The planning model and the interaction model are integrated homogeneously into an architecture called BDIggy.
The human interaction model is twofold: 1) it is based on the speech act theory to model the utterances, with a set of performatives applied to beliefs and desires 2) it uses a discourse model, represented by timed automata, to describe the dynamics of human conversations.
In BDIggy, interaction and planning are linked thanks to the BDI concepts.
The BDIggy architecture is validated with the use of a "Turing-like" test.
In the field of telemedicine, eHealth aims at remotely monitoring the health status of the patient living independently at home.
In this paper we describe some principles of such systems with some running examples.
This paper is dedicated to the study of the main properties of the so called 'Relational Principal Components Analysis' (RPCA), that achieves the analysis of a random vector, with respect to the prior knowledge of one binary relationship upon the underlying probabilistic space.
We detail the relational covariance and expectation properties that are the grounds of this technique, which whilst not being novel, remains scarcely studied.
The paper presents with didactic examples for the properties we previously addressed and throw some light on interpretations in RPCA.
Often, decision-making processes are embedded in “official” procedures to address focuses in any case.
However, procedures lead often to sub-optimal solutions for any specific decision making, and actors are obliged to develop practices to address the specificity of the context in which a decision is made.
This opposition between procedure and practices is well known in different domains (prescribed and effective tasks, logic of functioning versus logic of use, etc.).
We have shown what the differences were in a context-based formalism called Contextual Graphs.
In this paper, we discuss the possibility to use of “good” and “bad” practices for the training of human actors, thanks to an incremental acquisition of knowledge and the learning of new practices by a system.
We discuss these aspects in the framework of a real-world application in road safety (modeling of drivers' behaviors), but ideas can be easily reused in other domains.
In this paper, the AM–FM modulation model is applied to speech analysis, synthesis and coding.
The AM–FM model represents the speech signal as the sum of formant resonance signals each of which contains amplitude and frequency modulation.
Multiband filtering and demodulation using the energy separation algorithm are the basic tools used for speech analysis.
First, multiband demodulation analysis (MDA) is applied to the problem of fundamental frequency estimation using the average instantaneous frequency as estimates of pitch harmonics.
The MDA pitch tracking algorithm is shown to produce smooth and accurate fundamental frequency contours.
Next, the AM–FM modulation vocoder is introduced, which represents speech as the sum of resonance signals.
A time-varying filterbank is used to extract the formant bands and then the energy separation algorithm is used to demodulate the resonance signals into the amplitude envelope and instantaneous frequency signals.
Efficient modeling and coding (at 4.8–9.6 kbits/sec) algorithms are proposed for the amplitude envelope and instantaneous frequency of speech resonances.
Finally, the perceptual importance of modulations in speech resonances is investigated and it is shown that amplitude modulation patterns are both speaker and phone dependent.
The results of a recognition experiment, conducted on a speaker independent continuous Italian speech database, are reported in this paper.
The recognition system is based on mixture density hidden Markov models of phonetic units.
Different sets of units were tested, beginning with the most general context independent phones and ending with the most specific triphones; function word dependent units were also investigated.
The recognition, based on a vocabulary of 979 words, was performed with no linguistic constraints, i.e., with a word branching factor of 979.
The results confirm the effectiveness of context dependent units, for which a word accuracy of nearly 80% was obtained on a set of 300 sentences.
The problem of association rides mining is of particular interest in data mining research.
Many knowledge is represented in the form of association rules and the exhaustive search for these rules is often very expensive in time when the data are dense and highly correlated.
We propose a new approach based on a non-exhaustive search of association rules hidden in the data.
The proposed algorithm (i) extracts the best association rules given a measure of quality, (ii) extracts "nuggets" of knowledge from the data (i.e., the association rules showing a weak support and a strong confidence).
In this paper a selective overview is given of methods used for the evaluation of text-to-speech (TTS) systems, with some comments on their advantages and disadvantages.
The overview is confined to subjective methods of evaluation, i.e. methods which make use of human listeners.
Objective methods, which try to assess quality by means of signal processing techniques, are not considered.
In Section 1, four factors affecting the form of the evaluation, i.e. TTS-system component, text level, aspect of speech, and function, are discussed.
In Sections 2 and 3 the methods themselves are presented, related to linguistic and phonetic/acoustic TTS-modules, respectively.
Finally, in Section 4, some general conclusions are drawn.
Two experiments are reported concerning the detection of lexical stress in isolated English words.
A non-statistical method using rules based on pitch variations is proposed.
Better results are obtained with this method than when using static values of pitch as stress correlates.
To avoid the problem of syllable segmentation, two methods based on pitch and energy function contours are presented to find stressed regions in words.
These regions can be considered as regions of phonetic reliability.
The second experiment analyzes lexical stress in words pronounced with a rising pitch.
Maximum energy is reported to be the best correlate of stress, followed by duration.
Finally, it is proposed that the study of lexical stress in continuous speech be accompanied by the study of prosodics and their general use in sentences.
Interfaces are direct communication devices between a person and a machine that do not rely on the activation of peripheral nerves or muscles.
Brain-Machine Interfaces are based on the acquisition and analysis of the brain activity which is then converted to control signals for the device.
Then, we present a state of the art of the present BMI devices specifically designedfor helping severely handicapped people to communicate and control systems.
Mutual dependence of articulatory parameters allows the reducing of codebook volume and helps to improve conditions for global optimum search.
The initial approximations of articulatory vectors for the inverse problem solving are sampled along the trajectories of articulatory parameters in synthesized syllables.
Piece-wise linear mapping of the space of articulatory parameters onto the space of acoustic parameters, the minimal value of cross-sectional area of the vocal tract and the Reynolds number accelerate the process of optimization over 100 times.
In this paper a new criterion, named HVS (heuristic for Variables Selection), to evaluate a set ofcandidate features and select an informative subset to be used as input data for a connectionist model, is presented.
HVS criterion allows analysis of influence input variables have on the output of a connectionist model.
The primary aim of our method is the selection of an appropriate subset ofinput variables to estimate the best and more parsimonious model.
This paper deals with the use of image processing techniques for tiling the time-frequency plane.
This technique is applied on seismic wave separation.
We consider data recorded by a linear array of sensors.
For each recorded signal, the application of a time-frequency transform allows a two dimensional representation where the different seismic events are well localized and isolated.
The segmentation by the watershed algorithm applied on each representation enables the definition of the time-frequency filters leading to the separation of the different waves.
Then, in order to apply the separation algorithm to all the different recorded signals, we use the continuity from one signal to the other to perform the tracking of the different waves from one image to the next.
After an initialisation step, this leads to an automatic algorithm.
This algorithm is validated on a real data set and compared with a classical method.
In comparison, the proposed method has the advantage to separate all the different waves simultaneously and without introducing artefact in the spatial domain.
The limit of the algorithm is reached when the patterns associated to the different waves are not correctly separated in the time-frequency representation.
We present a new algorithm that extends the Reinforcement Learning framework to Partially Observed Markov Decision Processes (POMDP).
The main idea of our method is to build a state extension, called exhaustive observable, which allow us to define a next processus that is Markovian.
We bring the proof that solving this new process, to which classical RL methods can be applied, brings an optimal solution to the original POMDP.
We apply the algorithm built on that proof to several examples to test its validity and robustness.
The goal of the Langues et Civilisations à Tradition Orale (LACITO) Linguistic Archive project is to conserve and disseminate recorded and transcribed oral literature and other linguistic materials, mainly in unwritten languages, giving simultaneous access to sound recordings and text annotation.
The project uses XML markup for the kinds of annotation traditionally used in field linguistics.
Transcriptions are segmented into sentences (roughly) and words.
Annotations are associated with different levels: metadata at the text level, free translation at the sentence level, interlinear glosses at the word level, etc.
The project makes maximum use of standard, generic software tools.
Over 100 texts in 20 languages have been processed at the time of writing; some of these are available on the Internet for browsing and simple querying.
We present an environment and methodology for the representation and processing of acoustic, phonetic and lexical knowledge for speech recognition.
The tools suggested enable the encoding and processing of numerical data (signals, parameters, shapes, etc.) and symbolic informations (words, phonemes, syllables, features, cues, etc.) to be carried out in a uniform, uninterrupted and dynamic manner.
The application of this methodology is described with reference to a task involving the multi-speaker recognition of the names of the 26 letters of the alphabet given in French.
Despite the widely acknowledged difficulty of this vocabulary, the results attained provide a clear validation of the approach, particularly in the case of acoustically very similar words.
This paper first gives an overview of Simon's research in psychology, and then focuses on his work, on the psychology of expertise.
In this paper, a new microphone array processing technique is proposed for blind dereverberation of speech signals affected by room acoustics.
It is based on the separate processing of the minimum-phase and all-pass components of delay-steered multi-microphone signals.
The minimum-phase components are processed in the cepstrum-domain, where spatial averaging followed by low-time filtering is applied.
The all-pass components, which contain the source location information, are processed in the frequency-domain by performing spatial averaging and by retaining only the all-pass component of the resulting output.
The underlying motivation for the new processor is to use spatio-temporal processing over a single set of synchronous speech segments from several microphones to reconstruct the source speech, such that it is applicable to practical time-variant acoustic environments.
Simulated room impulse responses are used to evaluate the new processor and to compare it to a conventional beamformer.
Significant improvements in array gain and important reductions of reverberation in listening tests are observed.
We report the results of a cognitive investigation of the language deficits of a single Wernicke's aphasic patient.
The patient, R.D., showed poor speech comprehension but good reading comprehension.
His spontaneous speech and his attempts at reading aloud contained many neologisms and some verbal paraphasias.
Following Butterworth (1979) we interpret the neologisms as due to problems with retrieval of the phonological specifications of words from a speech output lexicon, and we present evidence showing that success in lexical retrieval was affected by word frequency and not by any syntactic distinction between content (open-class) and function (closed-class) words.
R.D.'s spelling was better preserved than his spoken naming and he could spell many words he was unable to say correctly.
His spelling errors appeared to be attempts at a target word based on retrieval of partial orthographic information (just as his neologisms seem to be based on partial retrieval of phonological information).
We argue that normal subjects may make similar speech and writing errors under certain circumstances.
Garrett's (1982) model of speech production is presented and we discuss how neologistic jargonaphasia and other forms of Wernicke's aphasia may be explained in terms of it.
We also argue that when R.D.'s deficits are analyzed and compared with those of other patients in the literature a number of implications emerge for theories of normal language processing.
These include 1) that the comprehension and production of familiar written words do not involve obligatoruy phonological mediation, 2) that there are distinct phonological and orthographic lexicons, 3) that morphemes are seperately represented in the phonological lexicon, 4) that ease and speed of retrieval of items from the phonological lexicon is affected by frequency of usage, and 5) that retrieval from both lexicons is not all-or-nothing.
Vector quantizers have traditionally been used in speech recognition systems as a pre-processor for sophisticated algorithmes such as Hidden Markov Modelling (HMM) or Dynamic Time Warping (DTW).
Recently simpler systems based more directly on Vector Quantization (VQ) have been proposed for recognizing isolated words with small vocabularies.
The major problem with such VQ-based systems is the lack of temporal information in the recognition algorithm.
Recent and new variations of the VQ-based systems incorporating temporal information are described.
The principal new variation introduced here is a conditional histogram technique which incorporates relative likelihoods of successive codewords into the distortion measure used in the VQ recognition algorithm.
Several VQ-based recognition algorithms are applied to the recognition of spoken letters of the English alphabet, a subset of the IBM Spellmode vocabulary.
Simulation results highlight the relative merits of the algorithms.
The understanding module of a spoken dialogue system must extract, from the speech recognizer output, the kind of request expressed by the caller (the call type) and its parameters (numerical expressions, time expressions or proper-names).
Such expressions are called Named Entities and their definitions can be either generic or linked to the dialogue application domain.
Detecting and extracting such Named Entities within a mixed-initiative dialogue context like How May I Help You?
sm,tm (HMIHY) is the subject of this study.
After reviewing standard methods based on hand-written grammars and statistical tagging, we propose a new approach, combining the advantages of both in a 2-step process.
We also propose a novel architecture which exploits understanding to improve recognition accuracy: the output of the Automatic Speech Recognition module is now a word lattice and the understanding module is responsible for transcribing the word strings which are useful to the Dialogue Manager.
All the methods proposed are trained and evaluated on a corpus comprising utterances from live customer traffic.
This paper describes a bi-directional letter/sound generation system based on a strategy combining data-driven techniques with a rule-based formalism.
Our approach provides a hierarchical analysis of a word, including stress pattern, morphology and syllabification.
Generation is achieved by a probabilistic parsing technique, where probabilities are trained from a parsed lexicon.
Our training and testing corpora consisted of spellings and pronunciations for the high frequency portion of the Brown Corpus (10,000 words).
The phonetic labels are augmented with markers indicating morphology and stress.
We will report on two distinct grammars representing a historical perspective.
Our early work with the first grammar inspired us to modify the grammar formalism, leading to greater constraint with fewer rules.
We evaluated our performance on letter-to-sound generation in terms of whole word accuracy as well as phoneme accuracy.
For the unseen test set, we achieved a word accuracy of 69.3% and a phoneme accuracy of 91.7% using a set of 52 distinct phonemes.
While this paper focuses on letter-to-sound generation, our system is also capable of generation in the reverse direction, as reported elsewhere (Meng et al., 1994a).
We believe that our formalism will be especially applicable for entering unknown words orally into a recognition system.
The proposed method is based on the cooperative use of a conventional n-gram constraint and additional grammatical constraints which take deviations from the grammar into account with a multi-pass search strategy.
The partial utterance segments are obtained with high confidence as the segments that satisfy both n-gram and grammatical constraints.
For improved efficiency, the context-free grammar expressing the grammatical constraints is approximated by a finite-state automaton.
We consider all kinds of deviations from the grammar such as insertions, deletions and substitutions when applying the grammatical constraints.
As a result, we can achieve a more robust application of grammatical constraints compared to a conventional word-skipping robust parser that can only handle one type of deviation, that is, insertions.
Our experiments confirm that the proposed method can recognize partial segments of utterances more reliably than conventional continuous speech recognition methods using only n-grams.
In addition, our results indicate that allowing more deviations from the grammatical constraints leads to better performance than the conventional word-skipping robust parser approach.
The conic fitting from image points is a very old topic in estimation and pattern recognition.
Systematically, these works have been based on the algebraic representation of the conic to establish the optimization criteria.
Less studied, the polar representation of the ellipse is costlier because it needs the optimization of the parametrization.
Yet, we propose in this paper some new ideas about this question.
First, we show that the estimation of the parameters and the parametrization separated permit to make the problem easier leading to a direct inversion and the search of the roots of a four degree polynomial respectively.
We also show that the parametrization carries the dimensional characteristics of the ellipse and when it is correctly disrupted in the minimization process, we constraint the ellipse search space.
This new result gives an estimate without dimensional bias in a noised and incomplete context.
A confidence envelope is then estimated to direct the search for continuations of the ellipse.
At last, we propose a hierarchical grouping and fitting stage following with a fuzzy decision step to detect automatically the elliptic shapes in the images.
Autism is characterized by profound difficulties in emotional interactions and should therefore be an appropriate field of research for emotion based human-computer interfaces.
This article focuses on emotionally charged facial expressions when they occur during a dialogue.
We designed a human-computer interface that matches an emotional facial expression to each reply in the dialogue.
Facial expressions are displayed using two different graphical designs, one being more cartoon-like than the other.
We describe an experimental protocol used to evaluate the impact of this interface on 10 teenagers with autism and 10 children without autism.
Results show that subjects without autism are more successful than subjects with autism in using facial expressions jointly with speech to disambiguate a dialogue.
The present study investigates on the basis of which units and strategies children initially organize their speech.
Transcribed longitudinal data from children acquiring German as their L1 are presented.
The data were obtained in weekly recording sessions, which began when the subjects were between seven and thirteen months old.
All the material was collected within the framework of the Kiel Project on Early Phonological Development.
The evidence presented in this paper suggests that a limited inventory of articulatory patterns functioning as underlying organizational units may determine the phonetic structure of the large majority of a child's first words.
The patterns are most probably constructed on the basis of a child's preferred articulations as well as on the basis of the acoustically and auditorily most salient features of adult model words.
There are basically five ways in which the early or original patterns change over time.
The linguistics of enunciation formulated by Antoine Culioli is a theory of predicative and enunciative operations.
Thus, the concepts of operation and representation are central to the epistemological model and the analysis method which aims to disentangle the links in play in the construction of utterances to relate them to the symbolic activity of representation and to the mental process that it presupposes.
A critical examination of the model of the three levels of representation leads us to distinguish the function of representing from the mode of representing.
The return to Saussure enables us to re-examine the model by integrating, at the level of notional representations, what Saussure names figures.
In conclusion, a brief case study exemplifies the theoretical development.
This paper presents an experimental study of “stress shift” in category-ambiguous and non-ambiguous material.
Ambiguous sequences such as Chinese fan exhibit phonological evidence for two structural analyses.
If the sequences is a syntactic phrase, with Chinese an adjective modifying the noun fan, then fan has greater relative prominence.
If Chinese is a noun and the sequence is a compound, then fan is deaccented and Chinese has greater relative prominence.
Additionally, since Chinese is a “stress shift” item, stress shift may apply in the phrasal interpretation.
Thus, category-ambiguous words with a potential for stress shift might contain earlier cues to syntactic category, in the form of a modified stress pattern, than non-stress shift items.
Production data show that stress shift patterns do indeed map onto syntactic categories, but only if the second element in the sequence is not right-branching.
A comprehension experiment with category-ambiguous material suggests that compound or phrasal prominence patterns and stress shift facilitate syntactic processing.
A second comprehension experiment replicates this effect and extends the investigation to non-ambiguous material such as Torquay College.
In non-ambiguous material, again, phrasal and compound stress appear to affect processing, but stress shift does not.
We address the problem of detecting slow moving target within ground clutter with a non side looking monostatic airborne radar using either a uniformly spaced and linear antenna (ULA) or a uniformly curved or circular antenna (UCuA and UCA respectively).
With a monostatic radar using an ULA in side looking configuration, the clutter has specific properties: the space-time repartition of the clutter spectral power which is called “Clutter Ridge'' is range independent.
But in case of a monostatic radar using an ULA in a non side looking configuration or using a UcuA or a UCA, the clutter properties are changed.
The clutter ridges are range dependent and specific methods must be applied to compensate the range dependency [4], [5], [6].
A novel speech coding algorithm, named pitch synchronous multi-band (PSMB), is proposed.
The new coding algorithm uses the multi-band excitation (MBE) model to generate a representative pitch-cycle waveform (PCW) for each frame.
The representative PCW of a frame is encoded by two out of three codebooks depending upon whether the frame is related or unrelated to the previous frame.
When a frame is unrelated to its previous frame, it is encoded by a bandlimited single pulse excitation (BSPE) codebook and the stochastic codebook.
The new speech coder introduces a pitch-period-based coding feature.
It overcomes some weaknesses existing in the improved MBE (IMBE) speech coder.
The PSMB coder operating at 4 kbps outperforms the Inmarsat 4.15 kbps IMBE coder by a clear margin.
Our listening tests also indicate that it is slightly better than the FS1016 4.8 kbps code excited linear predictive (CELP) coder in terms of perceptual quality.
Fast search algorithms for the three codebooks used in PSMB are also developed.
The fast algorithms render the new speech coder comparable to the FS1016 CELP coder, in terms of computational complexity.
This paper describes our recent work in developing multilingual spoken language systems that support human-computer interactions.
Our approach is based on the premise that a common semantic representation can be extracted from the input for all languages, at least within the context of restricted domains.
In our design of such systems, language dependent information is separated from the system kernel as much as possible, and encoded in external data structures.
The internal system manager, discourse and dialogue component, and database are all maintained in a language transparent form.
Our description will focus on the development of the multilingual MIT Voyager spoken language system, which can engage in verbal dialogues with users about a geographical region within Cambridge, MA in the USA.
The system can provide information about distances, travel times or directions between objects located within this area (e.g., restaurants, hotels, banks, libraries), as well as information such as the addresses, telephone numbers or location of the objects themselves.
Evaluations for the English, Japanese and Italian systems are reported.
Other related multilingual research activities are also briefly mentioned.